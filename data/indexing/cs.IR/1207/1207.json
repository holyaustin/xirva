[{"id": "1207.0246", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara, Pasquale De Meo, Giacomo Fiumara, Robert Baumgartner", "title": "Web Data Extraction, Applications and Techniques: A Survey", "comments": "Knowledge-based Systems", "journal-ref": "Knowledge-Based Systems, 70, 301-323. 2014", "doi": "10.1016/j.knosys.2014.07.007", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Data Extraction is an important problem that has been studied by means of\ndifferent scientific tools and in a broad range of applications. Many\napproaches to extracting data from the Web have been designed to solve specific\nproblems and operate in ad-hoc domains. Other approaches, instead, heavily\nreuse techniques and algorithms developed in the field of Information\nExtraction.\n  This survey aims at providing a structured and comprehensive overview of the\nliterature in the field of Web Data Extraction. We provided a simple\nclassification framework in which existing Web Data Extraction applications are\ngrouped into two main classes, namely applications at the Enterprise level and\nat the Social Web level. At the Enterprise level, Web Data Extraction\ntechniques emerge as a key tool to perform data analysis in Business and\nCompetitive Intelligence systems as well as for business process\nre-engineering. At the Social Web level, Web Data Extraction techniques allow\nto gather a large amount of structured data continuously generated and\ndisseminated by Web 2.0, Social Media and Online Social Network users and this\noffers unprecedented opportunities to analyze human behavior at a very large\nscale. We discuss also the potential of cross-fertilization, i.e., on the\npossibility of re-using Web Data Extraction techniques originally designed to\nwork in a given domain, in other domains.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2012 21:14:39 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 15:47:10 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2014 19:07:24 GMT"}, {"version": "v4", "created": "Tue, 10 Jun 2014 03:58:11 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Ferrara", "Emilio", ""], ["De Meo", "Pasquale", ""], ["Fiumara", "Giacomo", ""], ["Baumgartner", "Robert", ""]]}, {"id": "1207.0446", "submitter": "Zakaria Elberrichi", "authors": "Zakaria Elberrichi, Belaggoun Amel, Taibi Malika", "title": "Medical Documents Classification Based on the Domain Ontology MeSH", "comments": null, "journal-ref": "The International Arab Journal of e-Technology, Vol. 2, No. 4,\n  June 2012, page 210-215", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of classifying web documents using domain\nontology. Our goal is to provide a method for improving the classification of\nmedical documents by exploiting the MeSH thesaurus (Medical Subject Headings)\nwhich will allow us to generate a new representation based on concepts. This\napproach was tested with two well-known data mining algorithms C4.5 and KNN,\nand a comparison was made with the usual representation using stems. The\nenrichment of vectors using the concepts and the hyperonyms drawn from the\ndomain ontology has significantly boosted their representation, something\nessential for good classification. The results of our experiments on the\nbenchmark biomedical collection Ohsumed confirm the importance of the approach\nby a very significant improvement in the performance of the ontology-based\nclassification compared to the classical representation (Stems) by 30%.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 17:00:51 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Elberrichi", "Zakaria", ""], ["Amel", "Belaggoun", ""], ["Malika", "Taibi", ""]]}, {"id": "1207.0833", "submitter": "Fr\\'ed\\'eric Blanchard", "authors": "Fr\\'ed\\'eric Blanchard and Michel Herbin", "title": "Relational Data Mining Through Extraction of Representative Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing interest on Network Analysis, Relational Data Mining is\nbecoming an emphasized domain of Data Mining. This paper addresses the problem\nof extracting representative elements from a relational dataset. After defining\nthe notion of degree of representativeness, computed using the Borda\naggregation procedure, we present the extraction of exemplars which are the\nrepresentative elements of the dataset. We use these concepts to build a\nnetwork on the dataset. We expose the main properties of these notions and we\npropose two typical applications of our framework. The first application\nconsists in resuming and structuring a set of binary images and the second in\nmining co-authoring relation in a research team.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2012 20:48:36 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Blanchard", "Fr\u00e9d\u00e9ric", ""], ["Herbin", "Michel", ""]]}, {"id": "1207.1414", "submitter": "Eerika Savia", "authors": "Eerika Savia, Kai Puolamaki, Janne Sinkkonen, Samuel Kaski", "title": "Two-Way Latent Grouping Model for User Preference Prediction", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-518-524", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel latent grouping model for predicting the relevance of a\nnew document to a user. The model assumes a latent group structure for both\nusers and documents. We compared the model against a state-of-the-art method,\nthe User Rating Profile model, where only users have a latent group structure.\nWe estimate both models by Gibbs sampling. The new method predicts relevance\nmore accurately for new documents that have few known ratings. The reason is\nthat generalization over documents then becomes necessary and hence the twoway\ngrouping is profitable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:23:52 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Savia", "Eerika", ""], ["Puolamaki", "Kai", ""], ["Sinkkonen", "Janne", ""], ["Kaski", "Samuel", ""]]}, {"id": "1207.1847", "submitter": "Ted Dunning", "authors": "Ted Dunning", "title": "Finding Structure in Text, Genome and Other Symbolic Sequences", "comments": "~ 176 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical methods derived and described in this thesis provide new ways\nto elucidate the structural properties of text and other symbolic sequences.\nGenerically, these methods allow detection of a difference in the frequency of\na single feature, the detection of a difference between the frequencies of an\nensemble of features and the attribution of the source of a text. These three\nabstract tasks suffice to solve problems in a wide variety of settings.\nFurthermore, the techniques described in this thesis can be extended to provide\na wide range of additional tests beyond the ones described here.\n  A variety of applications for these methods are examined in detail. These\napplications are drawn from the area of text analysis and genetic sequence\nanalysis. The textually oriented tasks include finding interesting collocations\nand cooccurent phrases, language identification, and information retrieval. The\nbiologically oriented tasks include species identification and the discovery of\npreviously unreported long range structure in genes. In the applications\nreported here where direct comparison is possible, the performance of these new\nmethods substantially exceeds the state of the art.\n  Overall, the methods described here provide new and effective ways to analyse\ntext and other symbolic sequences. Their particular strength is that they deal\nwell with situations where relatively little data are available. Since these\nmethods are abstract in nature, they can be applied in novel situations with\nrelative ease.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 07:02:13 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Dunning", "Ted", ""]]}, {"id": "1207.2232", "submitter": "Reshmy Krishnan", "authors": "P. C. Sherimon, Reshmy Krishnan, P. V. Vinu", "title": "Effective Enabling of Sharing and Reuse of Knowledge On Semantic Web by\n  Ontology in Date Fruit Model", "comments": "International Journal of Computer Science Issues, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Organizations have recognized that knowledge constitutes a valuable\nintangible asset for creating and sustaining competitive advantages, knowledge\nsharing has a vital role in present society. It is an activity through which\ninformation is exchanged among people through different media. Many problems\nface the area of knowledge sharing and knowledge reuse. Currently, knowledge\nsharing between entities is achieved in a very ad-hoc fashion, lacking proper\nunderstanding of the meaning of the data. Ontologies can potentially solve\nthese problems by facilitating knowledge sharing and reuse through formal and\nreal-world semantics. Ontologies, through formal semantics, are\nmachine-understandable. A computer can process data, annotated with references\nto ontologies, and through the knowledge encapsulated in the ontology, deduce\nfacts from the original data. The date fruit is the most enduring symbol of the\nSultanate's rich heritage. Creating ontology for dates will enrich the farming\ngroup and research scholars in the agro farm area.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 06:45:59 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Sherimon", "P. C.", ""], ["Krishnan", "Reshmy", ""], ["Vinu", "P. V.", ""]]}, {"id": "1207.2615", "submitter": "Florian B\\\"aurle", "authors": "Hannah Bast, Florian B\\\"aurle, Bj\\\"orn Buchhold, Elmar Haussmann", "title": "Broccoli: Semantic Full-Text Search at your Fingertips", "comments": "10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Broccoli, a fast and easy-to-use search engine for what we call\nsemantic full-text search. Semantic full-text search combines the capabilities\nof standard full-text search and ontology search. The search operates on four\nkinds of objects: ordinary words (e.g., edible), classes (e.g., plants),\ninstances (e.g., Broccoli), and relations (e.g., occurs-with or native-to).\nQueries are trees, where nodes are arbitrary bags of these objects, and arcs\nare relations. The user interface guides the user in incrementally constructing\nsuch trees by instant (search-as-you-type) suggestions of words, classes,\ninstances, or relations that lead to good hits. Both standard full-text search\nand pure ontology search are included as special cases. In this paper, we\ndescribe the query language of Broccoli, the main idea behind a new kind of\nindex that enables fast processing of queries from that language as well as\nfast query suggestion, the natural language processing required, and the user\ninterface. We evaluated query times and result quality on the full version of\nthe English Wikipedia (40 GB XML dump) combined with the YAGO ontology (26\nmillion facts). We have implemented a fully functional prototype based on our\nideas and provide a web application to reproduce our quality experiments. Both\nare accessible via http://broccoli.informatik.uni-freiburg.de/repro-corr/ .\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 12:29:35 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2012 16:00:43 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2013 14:49:52 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Bast", "Hannah", ""], ["B\u00e4urle", "Florian", ""], ["Buchhold", "Bj\u00f6rn", ""], ["Haussmann", "Elmar", ""]]}, {"id": "1207.2900", "submitter": "Rajesh Pasupuleti", "authors": "P. Rajesh, G. Narasimha, N. Saisumanth", "title": "Privacy Preserving MFI Based Similarity Measure For Hierarchical\n  Document Clustering", "comments": "6 pages,1 table,1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing nature of World Wide Web has imposed great challenges for\nresearchers in improving the search efficiency over the internet. Now days web\ndocument clustering has become an important research topic to provide most\nrelevant documents in huge volumes of results returned in response to a simple\nquery. In this paper, first we proposed a novel approach, to precisely define\nclusters based on maximal frequent item set (MFI) by Apriori algorithm.\nAfterwards utilizing the same maximal frequent item set (MFI) based similarity\nmeasure for Hierarchical document clustering. By considering maximal frequent\nitem sets, the dimensionality of document set is decreased. Secondly, providing\nprivacy preserving of open web documents is to avoiding duplicate documents.\nThere by we can protect the privacy of individual copy rights of documents.\nThis can be achieved using equivalence relation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 10:01:41 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Rajesh", "P.", ""], ["Narasimha", "G.", ""], ["Saisumanth", "N.", ""]]}, {"id": "1207.3583", "submitter": "Mahyuddin K. M.  Nasutiion", "authors": "Mahyuddin K. M. Nasution, Shahrul Azman Noah", "title": "Information Retrieval Model: A Social Network Extraction Perspective", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future Information Retrieval, especially in connection with the internet,\nwill incorporate the content descriptions that are generated with social\nnetwork extraction technologies and preferably incorporate the probability\ntheory for assigning the semantic. Although there is an increasing interest\nabout social network extraction, but a little of them has a significant impact\nto infomation retrieval. Therefore this paper proposes a model of information\nretrieval from the social network extraction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 06:07:47 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Nasution", "Mahyuddin K. M.", ""], ["Noah", "Shahrul Azman", ""]]}, {"id": "1207.3628", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Sukanta Sinha, Rana Dattagupta, Debajyoti Mukhopadhyay", "title": "Identify Web-page Content meaning using Knowledge based System for Dual\n  Meaning Words", "comments": "4 pages, 2 figures; International Journal of Engineering Research and\n  Applications, ISSN: 2248-9622, Vol. 2, Issue 4, July-August 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meaning of Web-page content plays a big role while produced a search result\nfrom a search engine. Most of the cases Web-page meaning stored in title or\nmeta-tag area but those meanings do not always match with Web-page content. To\novercome this situation we need to go through the Web-page content to identify\nthe Web-page meaning. In such cases, where Webpage content holds dual meaning\nwords that time it is really difficult to identify the meaning of the Web-page.\nIn this paper, we are introducing a new design and development mechanism of\nidentifying the Web-page content meaning which holds dual meaning words in\ntheir Web-page content.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2012 11:20:23 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Sinha", "Sukanta", ""], ["Dattagupta", "Rana", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "1207.4146", "submitter": "Rong Jin", "authors": "Rong Jin, Luo Si", "title": "A Bayesian Approach toward Active Learning for Collaborative Filtering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-278-285", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is a useful technique for exploiting the preference\npatterns of a group of users to predict the utility of items for the active\nuser. In general, the performance of collaborative filtering depends on the\nnumber of rated examples given by the active user. The more the number of rated\nexamples given by the active user, the more accurate the predicted ratings will\nbe. Active learning provides an effective way to acquire the most informative\nrated examples from active users. Previous work on active learning for\ncollaborative filtering only considers the expected loss function based on the\nestimated model, which can be misleading when the estimated model is\ninaccurate. This paper takes one step further by taking into account of the\nposterior distribution of the estimated model, which results in more robust\nactive learning algorithm. Empirical studies with datasets of movie ratings\nshow that when the number of ratings from the active user is restricted to be\nsmall, active learning methods only based on the estimated model don't perform\nwell while the active learning method using the model distribution achieves\nsubstantially better performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:55:41 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Jin", "Rong", ""], ["Si", "Luo", ""]]}, {"id": "1207.4152", "submitter": "Lawrence Zitnick", "authors": "Lawrence Zitnick, Takeo Kanade", "title": "Maximum Entropy for Collaborative Filtering", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-636-643", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the task of collaborative filtering two challenges for computing\nconditional probabilities exist. First, the amount of training data available\nis typically sparse with respect to the size of the domain. Thus, support for\nhigher-order interactions is generally not present. Second, the variables that\nwe are conditioning upon vary for each query. That is, users label different\nvariables during each query. For this reason, there is no consistent input to\noutput mapping. To address these problems we purpose a maximum entropy approach\nusing a non-standard measure of entropy. This approach can be simplified to\nsolving a set of linear equations that can be efficiently solved.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:59:15 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Zitnick", "Lawrence", ""], ["Kanade", "Takeo", ""]]}, {"id": "1207.4157", "submitter": "Ben Wellner", "authors": "Ben Wellner, Andrew McCallum, Fuchun Peng, Michael Hay", "title": "An Integrated, Conditional Model of Information Extraction and\n  Coreference with Applications to Citation Matching", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-593-601", "categories": "cs.LG cs.DL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although information extraction and coreference resolution appear together in\nmany applications, most current systems perform them as ndependent steps. This\npaper describes an approach to integrated inference for extraction and\ncoreference based on conditionally-trained undirected graphical models. We\ndiscuss the advantages of conditional probability training, and of a\ncoreference model structure based on graph partitioning. On a data set of\nresearch paper citations, we show significant reduction in error by using\nextraction uncertainty to improve coreference citation matching accuracy, and\nusing coreference to improve the accuracy of the extracted fields.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:00:28 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Wellner", "Ben", ""], ["McCallum", "Andrew", ""], ["Peng", "Fuchun", ""], ["Hay", "Michael", ""]]}, {"id": "1207.4169", "submitter": "Michal Rosen-Zvi", "authors": "Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, Padhraic Smyth", "title": "The Author-Topic Model for Authors and Documents", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-487-494", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the author-topic model, a generative model for documents that\nextends Latent Dirichlet Allocation (LDA; Blei, Ng, & Jordan, 2003) to include\nauthorship information. Each author is associated with a multinomial\ndistribution over topics and each topic is associated with a multinomial\ndistribution over words. A document with multiple authors is modeled as a\ndistribution over topics that is a mixture of the distributions associated with\nthe authors. We apply the model to a collection of 1,700 NIPS conference papers\nand 160,000 CiteSeer abstracts. Exact inference is intractable for these\ndatasets and we use Gibbs sampling to estimate the topic and author\ndistributions. We compare the performance with two other generative models for\ndocuments, which are special cases of the author-topic model: LDA (a topic\nmodel) and a simple author model in which each author is associated with a\ndistribution over words rather than a distribution over topics. We show topics\nrecovered by the author-topic model, and demonstrate applications to computing\nsimilarity between authors and entropy of author output.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:05:53 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Rosen-Zvi", "Michal", ""], ["Griffiths", "Thomas", ""], ["Steyvers", "Mark", ""], ["Smyth", "Padhraic", ""]]}, {"id": "1207.4180", "submitter": "Pradeep Ravikumar", "authors": "Pradeep Ravikumar, William Cohen", "title": "A Hierarchical Graphical Model for Record Linkage", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-454-461", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of matching co-referent records is known among other names as rocord\nlinkage. For large record-linkage problems, often there is little or no labeled\ndata available, but unlabeled data shows a reasonable clear structure. For such\nproblems, unsupervised or semi-supervised methods are preferable to supervised\nmethods. In this paper, we describe a hierarchical graphical model framework\nfor the linakge-problem in an unsupervised setting. In addition to proposing\nnew methods, we also cast existing unsupervised probabilistic record-linkage\nmethods in this framework. Some of the techniques we propose to minimize\noverfitting in the above model are of interest in the general graphical model\nsetting. We describe a method for incorporating monotinicity constraints in a\ngraphical model. We also outline a bootstrapping approach of using\n\"single-field\" classifiers to noisily label latent variables in a hierarchical\nmodel. Experimental results show that our proposed unsupervised methods perform\nquite competitively even with fully supervised record-linkage methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 19:48:03 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Ravikumar", "Pradeep", ""], ["Cohen", "William", ""]]}, {"id": "1207.4259", "submitter": "Mohammad  Nabil Almunawar Dr", "authors": "Mohammad Nabil Almunawar", "title": "Content Based Multimedia Information Retrieval to Support Digital\n  Libraries", "comments": "15 pages, conference paper", "journal-ref": "International Conference on New Information Technologies, 23-26\n  July, 2001, Brunei Darussalam", "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based multimedia information retrieval is an interesting research\narea since it allows retrieval based on inherent characteristic of multimedia\nobjects. For example retrieval based on visual characteristics such as colour,\nshapes or textures of objects in images or retrieval based on spatial\nrelationships among objects in the media (images or video clips). This paper\nreviews some work done in image and video retrieval and then proposes an\nintegrated model that can handle images and video clips uniformly. Using this\nmodel retrieval on images or video clips can be done based on the same\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 04:11:55 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Almunawar", "Mohammad Nabil", ""]]}, {"id": "1207.4328", "submitter": "Zeno Toffano", "authors": "Zeno Toffano and Bich-Lien Doan", "title": "Quantum-like Tests for Contextual Querying", "comments": "11 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests are essential in Information Retrieval (IR), in order to evaluate the\neffectiveness of a query. Tests intended to exhibit the sense of words in\ncon-text were undertaken and linked with Quantum Mechanics (QM). Poll tests\nwere undertaken on heterogeneous media such as music and polysemy in foreign\nlanguages. Interference effects are shown in the results. Bell inequality was\nused leading to a significant spread in the results of the poll tests but\nwithout violating the classical limit. Then an automatic pertinence measure\ntool on texts has been developed using the HAL algorithm using an orthonormal\nvector decomposition model. In this case the spread in the values can lead to\nthe violation of the Bell inequality even beyond Cirel'son bound.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 10:23:37 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Toffano", "Zeno", ""], ["Doan", "Bich-Lien", ""]]}, {"id": "1207.4371", "submitter": "Klaus Berberich", "authors": "Klaus Berberich and Srikanta Bedathur", "title": "Computing n-Gram Statistics in MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics about n-grams (i.e., sequences of contiguous words or other tokens\nin text documents or other string data) are an important building block in\ninformation retrieval and natural language processing. In this work, we study\nhow n-gram statistics, optionally restricted by a maximum n-gram length and\nminimum collection frequency, can be computed efficiently harnessing MapReduce\nfor distributed data processing. We describe different algorithms, ranging from\nan extension of word counting, via methods based on the Apriori principle, to a\nnovel method Suffix-\\sigma that relies on sorting and aggregating suffixes. We\nexamine possible extensions of our method to support the notions of\nmaximality/closedness and to perform aggregations beyond occurrence counting.\nAssuming Hadoop as a concrete MapReduce implementation, we provide insights on\nan efficient implementation of the methods. Extensive experiments on The New\nYork Times Annotated Corpus and ClueWeb09 expose the relative benefits and\ntrade-offs of the methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 13:21:10 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Berberich", "Klaus", ""], ["Bedathur", "Srikanta", ""]]}, {"id": "1207.4525", "submitter": "Simon Lacoste-Julien", "authors": "Simon Lacoste-Julien, Konstantina Palla, Alex Davies, Gjergji Kasneci,\n  Thore Graepel, Zoubin Ghahramani", "title": "SiGMa: Simple Greedy Matching for Aligning Large Knowledge Bases", "comments": "10 pages + 2 pages appendix; 5 figures -- initial preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet has enabled the creation of a growing number of large-scale\nknowledge bases in a variety of domains containing complementary information.\nTools for automatically aligning these knowledge bases would make it possible\nto unify many sources of structured knowledge and answer complex queries.\nHowever, the efficient alignment of large-scale knowledge bases still poses a\nconsiderable challenge. Here, we present Simple Greedy Matching (SiGMa), a\nsimple algorithm for aligning knowledge bases with millions of entities and\nfacts. SiGMa is an iterative propagation algorithm which leverages both the\nstructural information from the relationship graph as well as flexible\nsimilarity measures between entity properties in a greedy local search, thus\nmaking it scalable. Despite its greedy nature, our experiments indicate that\nSiGMa can efficiently match some of the world's largest knowledge bases with\nhigh precision. We provide additional experiments on benchmark datasets which\ndemonstrate that SiGMa can outperform state-of-the-art approaches both in\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 00:15:05 GMT"}], "update_date": "2012-07-20", "authors_parsed": [["Lacoste-Julien", "Simon", ""], ["Palla", "Konstantina", ""], ["Davies", "Alex", ""], ["Kasneci", "Gjergji", ""], ["Graepel", "Thore", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1207.5409", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, Manjeet Singh, Seema Shukla", "title": "FST Based Morphological Analyzer for Hindi Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hindi being a highly inflectional language, FST (Finite State Transducer)\nbased approach is most efficient for developing a morphological analyzer for\nthis language. The work presented in this paper uses the SFST (Stuttgart Finite\nState Transducer) tool for generating the FST. A lexicon of root words is\ncreated. Rules are then added for generating inflectional and derivational\nwords from these root words. The Morph Analyzer developed was used in a Part Of\nSpeech (POS) Tagger based on Stanford POS Tagger. The system was first trained\nusing a manually tagged corpus and MAXENT (Maximum Entropy) approach of\nStanford POS tagger was then used for tagging input sentences. The\nmorphological analyzer gives approximately 97% correct results. POS tagger\ngives an accuracy of approximately 87% for the sentences that have the words\nknown to the trained model file, and 80% accuracy for the sentences that have\nthe words unknown to the trained model file.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 14:24:13 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Kumar", "Deepak", ""], ["Singh", "Manjeet", ""], ["Shukla", "Seema", ""]]}, {"id": "1207.5425", "submitter": "Ana Cerdeira-Pena", "authors": "Nieves R. Brisaboa, Ana Cerdeira-Pena, Gonzalo Navarro, Oscar Pedreira", "title": "Ranked Document Retrieval in (Almost) No Space", "comments": "This is an extended version of the paper that will appear in Proc. of\n  SPIRE'2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked document retrieval is a fundamental task in search engines. Such\nqueries are solved with inverted indexes that require additional 45%-80% of the\ncompressed text space, and take tens to hundreds of microseconds per query. In\nthis paper we show how ranked document retrieval queries can be solved within\ntens of milliseconds using essentially no extra space over an in-memory\ncompressed representation of the document collection. More precisely, we\nenhance wavelet trees on bytecodes (WTBCs), a data structure that rearranges\nthe bytes of the compressed collection, so that they support ranked conjunctive\nand disjunctive queries, using just 6%-18% of the compressed text space.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 15:34:39 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Brisaboa", "Nieves R.", ""], ["Cerdeira-Pena", "Ana", ""], ["Navarro", "Gonzalo", ""], ["Pedreira", "Oscar", ""]]}, {"id": "1207.5745", "submitter": "Swathi Rajasurya", "authors": "Swathi Rajasurya, Tamizhamudhu Muralidharan, Sandhiya Devi, S.\n  Swamynathan", "title": "Semantic Information Retrieval Using Ontology In University Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's conventional search engines hardly do provide the essential content\nrelevant to the user's search query. This is because the context and semantics\nof the request made by the user is not analyzed to the full extent. So here the\nneed for a semantic web search arises. SWS is upcoming in the area of web\nsearch which combines Natural Language Processing and Artificial Intelligence.\nThe objective of the work done here is to design, develop and implement a\nsemantic search engine- SIEU(Semantic Information Extraction in University\nDomain) confined to the university domain. SIEU uses ontology as a knowledge\nbase for the information retrieval process. It is not just a mere keyword\nsearch. It is one layer above what Google or any other search engines retrieve\nby analyzing just the keywords. Here the query is analyzed both syntactically\nand semantically. The developed system retrieves the web results more relevant\nto the user query through keyword expansion. The results obtained here will be\naccurate enough to satisfy the request made by the user. The level of accuracy\nwill be enhanced since the query is analyzed semantically. The system will be\nof great use to the developers and researchers who work on web. The Google\nresults are re-ranked and optimized for providing the relevant links. For\nranking an algorithm has been applied which fetches more apt results for the\nuser query.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2012 16:51:43 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Rajasurya", "Swathi", ""], ["Muralidharan", "Tamizhamudhu", ""], ["Devi", "Sandhiya", ""], ["Swamynathan", "S.", ""]]}, {"id": "1207.6033", "submitter": "Emilio Ferrara", "authors": "Giovanni Quattrone, Licia Capra, Pasquale De Meo, Emilio Ferrara,\n  Domenico Ursino", "title": "Effective Retrieval of Resources in Folksonomies Using a New Tag\n  Similarity Measure", "comments": "6 pages, 2 figures, CIKM 2011: 20th ACM Conference on Information and\n  Knowledge Management", "journal-ref": "Proceedings of the 20th ACM international conference on\n  Information and knowledge management, pp. 545-550, 2011", "doi": "10.1145/2063576.2063657", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social (or folksonomic) tagging has become a very popular way to describe\ncontent within Web 2.0 websites. However, as tags are informally defined,\ncontinually changing, and ungoverned, it has often been criticised for\nlowering, rather than increasing, the efficiency of searching. To address this\nissue, a variety of approaches have been proposed that recommend users what\ntags to use, both when labeling and when looking for resources. These\ntechniques work well in dense folksonomies, but they fail to do so when tag\nusage exhibits a power law distribution, as it often happens in real-life\nfolksonomies. To tackle this issue, we propose an approach that induces the\ncreation of a dense folksonomy, in a fully automatic and transparent way: when\nusers label resources, an innovative tag similarity metric is deployed, so to\nenrich the chosen tag set with related tags already present in the folksonomy.\nThe proposed metric, which represents the core of our approach, is based on the\nmutual reinforcement principle. Our experimental evaluation proves that the\naccuracy and coverage of searches guaranteed by our metric are higher than\nthose achieved by applying classical metrics.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 15:46:58 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Quattrone", "Giovanni", ""], ["Capra", "Licia", ""], ["De Meo", "Pasquale", ""], ["Ferrara", "Emilio", ""], ["Ursino", "Domenico", ""]]}, {"id": "1207.6037", "submitter": "Emilio Ferrara", "authors": "Giovanni Quattrone, Emilio Ferrara, Pasquale De Meo, Licia Capra", "title": "Measuring Similarity in Large-scale Folksonomies", "comments": "7 pages, SEKE '11: 23rd International Conference on Software\n  Engineering and Knowledge Engineering", "journal-ref": "SEKE '11: Proceedings of the 23rd International Conference on\n  Software Engineering and Knowledge Engineering, pp. 385-391, 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social (or folksonomic) tagging has become a very popular way to describe\ncontent within Web 2.0 websites. Unlike taxonomies, which overimpose a\nhierarchical categorisation of content, folksonomies enable end-users to freely\ncreate and choose the categories (in this case, tags) that best describe some\ncontent. However, as tags are informally defined, continually changing, and\nungoverned, social tagging has often been criticised for lowering, rather than\nincreasing, the efficiency of searching, due to the number of synonyms,\nhomonyms, polysemy, as well as the heterogeneity of users and the noise they\nintroduce. To address this issue, a variety of approaches have been proposed\nthat recommend users what tags to use, both when labelling and when looking for\nresources.\n  As we illustrate in this paper, real world folksonomies are characterized by\npower law distributions of tags, over which commonly used similarity metrics,\nincluding the Jaccard coefficient and the cosine similarity, fail to compute.\nWe thus propose a novel metric, specifically developed to capture similarity in\nlarge-scale folksonomies, that is based on a mutual reinforcement principle:\nthat is, two tags are deemed similar if they have been associated to similar\nresources, and vice-versa two resources are deemed similar if they have been\nlabelled by similar tags. We offer an efficient realisation of this similarity\nmetric, and assess its quality experimentally, by comparing it against cosine\nsimilarity, on three large-scale datasets, namely Bibsonomy, MovieLens and\nCiteULike.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 16:01:22 GMT"}], "update_date": "2012-07-26", "authors_parsed": [["Quattrone", "Giovanni", ""], ["Ferrara", "Emilio", ""], ["De Meo", "Pasquale", ""], ["Capra", "Licia", ""]]}, {"id": "1207.6083", "submitter": "Alex Kulesza", "authors": "Alex Kulesza, Ben Taskar", "title": "Determinantal point processes for machine learning", "comments": "120 pages", "journal-ref": "Foundations and Trends in Machine Learning: Vol. 5: No 2-3, pp\n  123-286", "doi": "10.1561/2200000044", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Determinantal point processes (DPPs) are elegant probabilistic models of\nrepulsion that arise in quantum physics and random matrix theory. In contrast\nto traditional structured models like Markov random fields, which become\nintractable and hard to approximate in the presence of negative correlations,\nDPPs offer efficient and exact algorithms for sampling, marginalization,\nconditioning, and other inference tasks. We provide a gentle introduction to\nDPPs, focusing on the intuitions, algorithms, and extensions that are most\nrelevant to the machine learning community, and show how DPPs can be applied to\nreal-world applications like finding diverse sets of high-quality search\nresults, building informative summaries by selecting diverse sentences from\ndocuments, modeling non-overlapping human poses in images or video, and\nautomatically building timelines of important news stories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2012 18:45:43 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 16:10:06 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2012 14:05:25 GMT"}, {"version": "v4", "created": "Thu, 10 Jan 2013 20:43:53 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Kulesza", "Alex", ""], ["Taskar", "Ben", ""]]}, {"id": "1207.6379", "submitter": "Jose Bento", "authors": "Jos\\'e Bento, Nadia Fawaz, Andrea Montanari, Stratis Ioannidis", "title": "Identifying Users From Their Rating Patterns", "comments": "Winner of the 2011 Challenge on Context-Aware Movie Recommendation\n  (RecSys 2011 - CAMRa2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on our analysis of the 2011 CAMRa Challenge dataset (Track\n2) for context-aware movie recommendation systems. The train dataset comprises\n4,536,891 ratings provided by 171,670 users on 23,974$ movies, as well as the\nhousehold groupings of a subset of the users. The test dataset comprises 5,450\nratings for which the user label is missing, but the household label is\nprovided. The challenge required to identify the user labels for the ratings in\nthe test set. Our main finding is that temporal information (time labels of the\nratings) is significantly more useful for achieving this objective than the\nuser preferences (the actual ratings). Using a model that leverages on this\nfact, we are able to identify users within a known household with an accuracy\nof approximately 96% (i.e. misclassification rate around 4%).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 19:27:03 GMT"}], "update_date": "2012-07-27", "authors_parsed": [["Bento", "Jos\u00e9", ""], ["Fawaz", "Nadia", ""], ["Montanari", "Andrea", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "1207.6448", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Dhaval Chandarana, Rutvi Dave, Sharyu Page,\n  Shikha Gupta", "title": "Query Optimization Over Web Services Using A Mixed Approach", "comments": "10 pages, 1 figure", "journal-ref": "Advances in Computing & Inf. Technology, AISC 178, pp.381-389,\n  2012", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Web Service Management System (WSMS) can be well-thought-out as a\nconsistent and a secure way of managing the web services. Web Service has\nbecome a quintessential part of the web world, managing and sharing the\nresources of the business it is associated with. In this paper, we focus on the\nquery optimization aspect of handling the \"natural language\" query, queried to\nthe WSMS. The map-select-composite operations are piloted to select specific\nweb services. The main aftermath of our research is ensued in an algorithm\nwhich uses cost-based as well as heuristic based approach for query\noptimization. Query plan is formed after cost-based evaluation and using Greedy\nalgorithm. The heuristic based approach further optimizes the evaluation plan.\nThis scheme not only guarantees an optimal solution, which has a minimum\ndiversion from the ideal solution, but also saves time which is otherwise\nutilized in generating various query plans using many mathematical models and\nthen evaluating each one.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 04:55:07 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Chandarana", "Dhaval", ""], ["Dave", "Rutvi", ""], ["Page", "Sharyu", ""], ["Gupta", "Shikha", ""]]}, {"id": "1207.6600", "submitter": "Rama  Badrinath", "authors": "Rama Badrinath, C. E. Veni Madhavan", "title": "Diversity in Ranking using Negative Reinforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of diversity in ranking of the nodes\nin a graph. The task is to pick the top-k nodes in the graph which are both\n'central' and 'diverse'. Many graph-based models of NLP like text\nsummarization, opinion summarization involve the concept of diversity in\ngenerating the summaries. We develop a novel method which works in an iterative\nfashion based on random walks to achieve diversity. Specifically, we use\nnegative reinforcement as a main tool to introduce diversity in the\nPersonalized PageRank framework. Experiments on two benchmark datasets show\nthat our algorithm is competitive to the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2012 17:16:59 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Badrinath", "Rama", ""], ["Madhavan", "C. E. Veni", ""]]}]