[{"id": "1905.00052", "submitter": "Grigor Aslanyan", "authors": "Grigor Aslanyan, Aritra Mandal, Prathyusha Senthil Kumar, Amit\n  Jaiswal, Manojkumar Rangasamy Kannadasan", "title": "Personalized Ranking in eCommerce Search", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of personalization in the context of eCommerce search.\nSpecifically, we develop personalization ranking features that use in-session\ncontext to augment a generic ranker optimized for conversion and relevance. We\nuse a combination of latent features learned from item co-clicks in historic\nsessions and content-based features that use item title and price.\nPersonalization in search has been discussed extensively in the existing\nliterature. The novelty of our work is combining and comparing content-based\nand content-agnostic features and showing that they complement each other to\nresult in a significant improvement of the ranker. Moreover, our technique does\nnot require an explicit re-ranking step, does not rely on learning user\nprofiles from long term search behavior, and does not involve complex modeling\nof query-item-user features. Our approach captures item co-click propensity\nusing lightweight item embeddings. We experimentally show that our technique\nsignificantly outperforms a generic ranker in terms of Mean Reciprocal Rank\n(MRR). We also provide anecdotal evidence for the semantic similarity captured\nby the item embeddings on the eBay search engine.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 18:29:28 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Aslanyan", "Grigor", ""], ["Mandal", "Aritra", ""], ["Kumar", "Prathyusha Senthil", ""], ["Jaiswal", "Amit", ""], ["Kannadasan", "Manojkumar Rangasamy", ""]]}, {"id": "1905.00075", "submitter": "Colin B Clement", "authors": "Colin B. Clement, Matthew Bierbaum, Kevin P. O'Keeffe, Alexander A.\n  Alemi", "title": "On the Use of ArXiv as a Dataset", "comments": "7 pages, 3 tables, 2 figures, ICLR 2019 workshop RLGM submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The arXiv has collected 1.5 million pre-print articles over 28 years, hosting\nliterature from scientific fields including Physics, Mathematics, and Computer\nScience. Each pre-print features text, figures, authors, citations, categories,\nand other metadata. These rich, multi-modal features, combined with the natural\ngraph structure---created by citation, affiliation, and co-authorship---makes\nthe arXiv an exciting candidate for benchmarking next-generation models. Here\nwe take the first necessary steps toward this goal, by providing a pipeline\nwhich standardizes and simplifies access to the arXiv's publicly available\ndata. We use this pipeline to extract and analyze a 6.7 million edge citation\ngraph, with an 11 billion word corpus of full-text research articles. We\npresent some baseline classification results, and motivate application of more\nexciting generative graph models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 19:43:53 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Clement", "Colin B.", ""], ["Bierbaum", "Matthew", ""], ["O'Keeffe", "Kevin P.", ""], ["Alemi", "Alexander A.", ""]]}, {"id": "1905.00185", "submitter": "Elisa Claire Alem\\'an Carre\\'on", "authors": "Elisa Claire Alem\\'an Carre\\'on, Hirofumi Nonaka, Toru Hiraoka, Minoru\n  Kumano, Takao Ito and Masaharu Hirota", "title": "Emotional Contribution Analysis of Online Reviews", "comments": null, "journal-ref": "In proceedings of the 2018 International Conference on Artificial\n  Life and Robotics (ICAROB2018). pp. 259 - 362. Beppu, Japan (2018, February\n  1-4)", "doi": "10.5954/ICAROB.2018.OS5-3", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the constant increase in population and tourism worldwide,\nthere is a need for the development of cross-language market research tools\nthat are more cost and time effective than surveys or interviews. Focusing on\nthe Chinese tourism boom and the hotel industry in Japan, we extracted the most\ninfluential keywords in emotional judgement from Chinese online reviews of\nJapanese hotels in the portal site Ctrip. Using an entropy based mathematical\nmodel and a machine learning algorithm, we determined the words that most\nclosely represent the demands and emotions of this customer base.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 04:54:41 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Carre\u00f3n", "Elisa Claire Alem\u00e1n", ""], ["Nonaka", "Hirofumi", ""], ["Hiraoka", "Toru", ""], ["Kumano", "Minoru", ""], ["Ito", "Takao", ""], ["Hirota", "Masaharu", ""]]}, {"id": "1905.00453", "submitter": "Thanh Tran", "authors": "Thanh Tran, Xinyue Liu, Kyumin Lee, Xiangnan Kong", "title": "Signed Distance-based Deep Memory Recommender", "comments": null, "journal-ref": "Proceedings of the 2019 World Wide Web Conference", "doi": "10.1145/3308558.3313460", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized recommendation algorithms learn a user's preference for an item\nby measuring a distance/similarity between them. However, some of the existing\nrecommendation models (e.g., matrix factorization) assume a linear relationship\nbetween the user and item. This approach limits the capacity of recommender\nsystems, since the interactions between users and items in real-world\napplications are much more complex than the linear relationship. To overcome\nthis limitation, in this paper, we design and propose a deep learning framework\ncalled Signed Distance-based Deep Memory Recommender, which captures non-linear\nrelationships between users and items explicitly and implicitly, and work well\nin both general recommendation task and shopping basket-based recommendation\ntask. Through an extensive empirical study on six real-world datasets in the\ntwo recommendation tasks, our proposed approach achieved significant\nimprovement over ten state-of-the-art recommendation models.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:07:22 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Tran", "Thanh", ""], ["Liu", "Xinyue", ""], ["Lee", "Kyumin", ""], ["Kong", "Xiangnan", ""]]}, {"id": "1905.00470", "submitter": "Swagata Duari", "authors": "Swagata Duari and Vasudha Bhatnagar", "title": "Semi-automatic System for Title Construction", "comments": "12 pages, 2 figures, conference paper, accepted for publication", "journal-ref": "Gani A., Das P., Kharb L., Chahal D. (eds) Information,\n  Communication and Computing Technology. ICICCT 2019. Communications in\n  Computer and Information Science, vol 1025, 216-227. Springer, Singapore", "doi": "10.1007/978-981-15-1384-8_18", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a semi-automatic system for title construction from\nscientific abstracts. The system extracts and recommends impactful words from\nthe text, which the author can creatively use to construct an appropriate title\nfor the manuscript. The work is based on the hypothesis that keywords are good\ncandidates for title construction. We extract important words from the document\nby inducing a supervised keyword extraction model. The model is trained on\nnovel features extracted from graph-of-text representation of the document. We\nempirically show that these graph-based features are capable of discriminating\nkeywords from non-keywords. We further establish empirically that the proposed\napproach can be applied to any text irrespective of the training domain and\ncorpus. We evaluate the proposed system by computing the overlap between\nextracted keywords and the list of title-words for documents, and we observe a\nmacro-averaged precision of 82%.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:49:41 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Duari", "Swagata", ""], ["Bhatnagar", "Vasudha", ""]]}, {"id": "1905.00522", "submitter": "Paul Sheridan", "authors": "Paul Sheridan, Mikael Onsj\\\"o, Janna Hastings", "title": "The Literary Theme Ontology for Media Annotation and Information\n  Retrieval", "comments": "12 pages, 2 figures, 1 tables, minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary theme identification and interpretation is a focal point of literary\nstudies scholarship. Classical forms of literary scholarship, such as close\nreading, have flourished with scarcely any need for commonly defined literary\nthemes. However, the rise in popularity of collaborative and algorithmic\nanalyses of literary themes in works of fiction, together with a requirement\nfor computational searching and indexing facilities for large corpora, creates\nthe need for a collection of shared literary themes to ensure common\nterminology and definitions. To address this need, we here introduce a first\ndraft of the Literary Theme Ontology. Inspired by a traditional framing from\nliterary theory, the ontology comprises literary themes drawn from the authors\nown analyses, reference books, and online sources. The ontology is available at\nhttps://github.com/theme-ontology/lto under a Creative Commons Attribution 4.0\nInternational license (CC BY 4.0).\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:33:16 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 03:25:25 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Sheridan", "Paul", ""], ["Onsj\u00f6", "Mikael", ""], ["Hastings", "Janna", ""]]}, {"id": "1905.00579", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Wenyuan Gao, Xiaojie Zhou, Weijia Jia, Shaohua Zhang,\n  Yutao Luo", "title": "Herding Effect based Attention for Personalized Time-Sync Video\n  Recommendation", "comments": "ACCEPTED for ORAL presentation at IEEE ICME 2019", "journal-ref": null, "doi": "10.1109/ICME.2019.00085", "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-sync comment (TSC) is a new form of user-interaction review associated\nwith real-time video contents, which contains a user's preferences for videos\nand therefore well suited as the data source for video recommendations.\nHowever, existing review-based recommendation methods ignore the\ncontext-dependent (generated by user-interaction), real-time, and\ntime-sensitive properties of TSC data. To bridge the above gaps, in this paper,\nwe use video images and users' TSCs to design an Image-Text Fusion model with a\nnovel Herding Effect Attention mechanism (called ITF-HEA), which can predict\nusers' favorite videos with model-based collaborative filtering. Specifically,\nin the HEA mechanism, we weight the context information based on the semantic\nsimilarities and time intervals between each TSC and its context, thereby\nconsidering influences of the herding effect in the model. Experiments show\nthat ITF-HEA is on average 3.78\\% higher than the state-of-the-art method upon\nF1-score in baselines.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 05:55:53 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Yang", "Wenmian", ""], ["Gao", "Wenyuan", ""], ["Zhou", "Xiaojie", ""], ["Jia", "Weijia", ""], ["Zhang", "Shaohua", ""], ["Luo", "Yutao", ""]]}, {"id": "1905.00758", "submitter": "Kan Ren", "authors": "Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie\n  Bian, Guorui Zhou, Jian Xu, Yong Yu, Xiaoqiang Zhu and Kun Gai", "title": "Lifelong Sequential Modeling with Personalized Memorization for User\n  Response Prediction", "comments": "SIGIR 2019. Reproducible codes and datasets:\n  https://github.com/alimamarankgroup/HPMN", "journal-ref": null, "doi": "10.1145/3331184.3331230", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User response prediction, which models the user preference w.r.t. the\npresented items, plays a key role in online services. With two-decade rapid\ndevelopment, nowadays the cumulated user behavior sequences on mature Internet\nservice platforms have become extremely long since the user's first\nregistration. Each user not only has intrinsic tastes, but also keeps changing\nher personal interests during lifetime. Hence, it is challenging to handle such\nlifelong sequential modeling for each individual user. Existing methodologies\nfor sequential modeling are only capable of dealing with relatively recent user\nbehaviors, which leaves huge space for modeling long-term especially lifelong\nsequential patterns to facilitate user modeling. Moreover, one user's behavior\nmay be accounted for various previous behaviors within her whole online\nactivity history, i.e., long-term dependency with multi-scale sequential\npatterns. In order to tackle these challenges, in this paper, we propose a\nHierarchical Periodic Memory Network for lifelong sequential modeling with\npersonalized memorization of sequential patterns for each user. The model also\nadopts a hierarchical and periodical updating mechanism to capture multi-scale\nsequential patterns of user interests while supporting the evolving user\nbehavior logs. The experimental results over three large-scale real-world\ndatasets have demonstrated the advantages of our proposed model with\nsignificant improvement in user response prediction performance against the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:10:10 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 16:30:26 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ren", "Kan", ""], ["Qin", "Jiarui", ""], ["Fang", "Yuchen", ""], ["Zhang", "Weinan", ""], ["Zheng", "Lei", ""], ["Bian", "Weijie", ""], ["Zhou", "Guorui", ""], ["Xu", "Jian", ""], ["Yu", "Yong", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "1905.00805", "submitter": "Wenhui Yu", "authors": "Wenhui Yu, Zheng Qin", "title": "Spectrum-enhanced Pairwise Learning to Rank", "comments": "11 pages; submitted to World Wide Web Conference (WWW 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the performance of the recommender system, side information is\nextensively explored with various features (e.g., visual features and textual\nfeatures). However, there are some demerits of side information: (1) the extra\ndata is not always available in all recommendation tasks; (2) it is only for\nitems, there is seldom high-level feature describing users. To address these\ngaps, we introduce the spectral features extracted from two hypergraph\nstructures of the purchase records. Spectral features describe the\n\\textit{similarity} of users/items in the graph space, which is critical for\nrecommendation. We leverage spectral features to model the users' preference\nand items' properties by incorporating them into a Matrix Factorization (MF)\nmodel. In addition to modeling, we also use spectral features to optimize.\nBayesian Personalized Ranking (BPR) is extensively leveraged to optimize models\nin implicit feedback data. However, in BPR, all missing values are regarded as\nnegative samples equally while many of them are indeed unseen positive ones. We\nenrich the positive samples by calculating the similarity among users/items by\nthe spectral features. The key ideas are: (1) similar users shall have similar\npreference on the same item; (2) a user shall have similar perception on\nsimilar items. Extensive experiments on two real-world datasets demonstrate the\nusefulness of the spectral features and the effectiveness of our\nspectrum-enhanced pairwise optimization. Our models outperform several\nstate-of-the-art models significantly.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:25:56 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Yu", "Wenhui", ""], ["Qin", "Zheng", ""]]}, {"id": "1905.00829", "submitter": "Niels Dalum Hansen", "authors": "Niels Dalum Hansen", "title": "Web data mining for public health purposes", "comments": "PhD thesis (2017), Univ Copenhagen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a long time, public health events, such as disease incidence or\nvaccination activity, have been monitored to keep track of the health status of\nthe population, allowing to evaluate the effect of public health initiatives\nand to decide where resources for improving public health are best spent. This\nthesis investigates the use of web data mining for public health monitoring,\nand makes contributions in the following two areas: New approaches for\npredicting public health events from web mined data, and novel applications of\nweb mined data for public health monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 16:04:21 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Hansen", "Niels Dalum", ""]]}, {"id": "1905.00957", "submitter": "A\\'ecio Solano Rodrigues Santos", "authors": "Sonia Castelo, Thais Almeida, Anas Elghafari, A\\'ecio Santos, Kien\n  Pham, Eduardo Nakamura, Juliana Freire", "title": "A Topic-Agnostic Approach for Identifying Fake News Pages", "comments": "Accepted for publication in the Companion Proceedings of the 2019\n  World Wide Web Conference (WWW'19 Companion). Presented in the 2019\n  International Workshop on Misinformation, Computational Fact-Checking and\n  Credible Web (MisinfoWorkshop2019). 6 pages", "journal-ref": null, "doi": "10.1145/3308560.3316739", "report-no": null, "categories": "cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news and misinformation have been increasingly used to manipulate\npopular opinion and influence political processes. To better understand fake\nnews, how they are propagated, and how to counter their effect, it is necessary\nto first identify them. Recently, approaches have been proposed to\nautomatically classify articles as fake based on their content. An important\nchallenge for these approaches comes from the dynamic nature of news: as new\npolitical events are covered, topics and discourse constantly change and thus,\na classifier trained using content from articles published at a given time is\nlikely to become ineffective in the future. To address this challenge, we\npropose a topic-agnostic (TAG) classification strategy that uses linguistic and\nweb-markup features to identify fake news pages. We report experimental results\nusing multiple data sets which show that our approach attains high accuracy in\nthe identification of fake news, even as topics evolve over time.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 20:50:22 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Castelo", "Sonia", ""], ["Almeida", "Thais", ""], ["Elghafari", "Anas", ""], ["Santos", "A\u00e9cio", ""], ["Pham", "Kien", ""], ["Nakamura", "Eduardo", ""], ["Freire", "Juliana", ""]]}, {"id": "1905.01053", "submitter": "Wenmian Yang", "authors": "Wenmian Yang, Kun Wang, Na Ruan, Wenyuan Gao, Weijia Jia, Wei Zhao,\n  Nan Liu, Yunyong Zhang", "title": "Time-sync Video Tag Extraction Using Semantic Association Graph", "comments": "Accepted by ACM TKDD 2019", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), Volume\n  13 Issue 4, July 2019", "doi": "10.1145/3332932", "report-no": "TKDD-1304-37", "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-sync comments reveal a new way of extracting the online video tags.\nHowever, such time-sync comments have lots of noises due to users' diverse\ncomments, introducing great challenges for accurate and fast video tag\nextractions. In this paper, we propose an unsupervised video tag extraction\nalgorithm named Semantic Weight-Inverse Document Frequency (SW-IDF).\nSpecifically, we first generate corresponding semantic association graph (SAG)\nusing semantic similarities and timestamps of the time-sync comments. Second,\nwe propose two graph cluster algorithms, i.e., dialogue-based algorithm and\ntopic center-based algorithm, to deal with the videos with different density of\ncomments. Third, we design a graph iteration algorithm to assign the weight to\neach comment based on the degrees of the clustered subgraphs, which can\ndifferentiate the meaningful comments from the noises. Finally, we gain the\nweight of each word by combining Semantic Weight (SW) and Inverse Document\nFrequency (IDF). In this way, the video tags are extracted automatically in an\nunsupervised way. Extensive experiments have shown that SW-IDF (dialogue-based\nalgorithm) achieves 0.4210 F1-score and 0.4932 MAP (Mean Average Precision) in\nhigh-density comments, 0.4267 F1-score and 0.3623 MAP in low-density comments;\nwhile SW-IDF (topic center-based algorithm) achieves 0.4444 F1-score and 0.5122\nMAP in high-density comments, 0.4207 F1-score and 0.3522 MAP in low-density\ncomments. It has a better performance than the state-of-the-art unsupervised\nalgorithms in both F1-score and MAP.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 07:30:14 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Yang", "Wenmian", ""], ["Wang", "Kun", ""], ["Ruan", "Na", ""], ["Gao", "Wenyuan", ""], ["Jia", "Weijia", ""], ["Zhao", "Wei", ""], ["Liu", "Nan", ""], ["Zhang", "Yunyong", ""]]}, {"id": "1905.01152", "submitter": "Murali Karthick Baskar", "authors": "Murali Karthick Baskar, Shinji Watanabe, Ramon Astudillo, Takaaki\n  Hori, Luk\\'a\\v{s} Burget, Jan \\v{C}ernock\\'y", "title": "Semi-supervised Sequence-to-sequence ASR using Unpaired Speech and Text", "comments": "INTERSPEECH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.IR cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence automatic speech recognition (ASR) models require large\nquantities of data to attain high performance. For this reason, there has been\na recent surge in interest for unsupervised and semi-supervised training in\nsuch models. This work builds upon recent results showing notable improvements\nin semi-supervised training using cycle-consistency and related techniques.\nSuch techniques derive training procedures and losses able to leverage unpaired\nspeech and/or text data by combining ASR with Text-to-Speech (TTS) models. In\nparticular, this work proposes a new semi-supervised loss combining an\nend-to-end differentiable ASR$\\rightarrow$TTS loss with TTS$\\rightarrow$ASR\nloss. The method is able to leverage both unpaired speech and text data to\noutperform recently proposed related techniques in terms of \\%WER. We provide\nextensive results analyzing the impact of data quantity and speech and text\nmodalities and show consistent gains across WSJ and Librispeech corpora. Our\ncode is provided in ESPnet to reproduce the experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:13:41 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 08:54:20 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Baskar", "Murali Karthick", ""], ["Watanabe", "Shinji", ""], ["Astudillo", "Ramon", ""], ["Hori", "Takaaki", ""], ["Burget", "Luk\u00e1\u0161", ""], ["\u010cernock\u00fd", "Jan", ""]]}, {"id": "1905.01257", "submitter": "Stefano Marchesin", "authors": "Maristella Agosti, Giorgio Maria Di Nunzio, Stefano Marchesin,\n  Gianmaria Silvello", "title": "A Relation Extraction Approach for Clinical Decision Support", "comments": "4 pages, 1 figure, DTMBio-KMH 2018, in conjunction with ACM 27th\n  Conference on Information and Knowledge Management (CIKM), October 22-26\n  2018, Lingotto, Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate how semantic relations between concepts\nextracted from medical documents can be employed to improve the retrieval of\nmedical literature. Semantic relations explicitly represent relatedness between\nconcepts and carry high informative power that can be leveraged to improve the\neffectiveness of retrieval functionalities of clinical decision support\nsystems. We present preliminary results and show how relations are able to\nprovide a sizable increase of the precision for several topics, albeit having\nno impact on others. We then discuss some future directions to minimize the\nimpact of negative results while maximizing the impact of good results.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:22:09 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Agosti", "Maristella", ""], ["Di Nunzio", "Giorgio Maria", ""], ["Marchesin", "Stefano", ""], ["Silvello", "Gianmaria", ""]]}, {"id": "1905.01263", "submitter": "Abhishek K Das", "authors": "Abhishek K Das, Nikhil Bhat, Sukanto Guha and Janvi Palan", "title": "A Personalized Subreddit Recommendation Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to improve upon the generic recommendations that Reddit\nprovides for its users. We propose a novel personalized recommender system that\nlearns from both, the presence and the content of user-subreddit interaction,\nusing implicit and explicit signals to provide robust recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:38:05 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Das", "Abhishek K", ""], ["Bhat", "Nikhil", ""], ["Guha", "Sukanto", ""], ["Palan", "Janvi", ""]]}, {"id": "1905.01267", "submitter": "Xuehui Hu", "authors": "Xuehui Hu, Nishanth Sastry", "title": "Characterising Third Party Cookie Usage in the EU after GDPR", "comments": null, "journal-ref": null, "doi": "10.1145/3292522.3326039", "report-no": null, "categories": "cs.CR cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recently introduced General Data Protection Regulation (GDPR) requires\nthat when obtaining information online that could be used to identify\nindividuals, their consents must be obtained. Among other things, this affects\nmany common forms of cookies, and users in the EU have been presented with\nnotices asking their approvals for data collection. This paper examines the\nprevalence of third party cookies before and after GDPR by using two datasets:\naccesses to top 500 websites according to Alexa.com, and weekly data of cookies\nplaced in users' browsers by websites accessed by 16 UK and China users across\none year.\n  We find that on average the number of third parties dropped by more than 10%\nafter GDPR, but when we examine real users' browsing histories over a year, we\nfind that there is no material reduction in long-term numbers of third party\ncookies, suggesting that users are not making use of the choices offered by\nGDPR for increased privacy. Also, among websites which offer users a choice in\nwhether and how they are tracked, accepting the default choices typically ends\nup storing more cookies on average than on websites which provide a notice of\ncookies stored but without giving users a choice of which cookies, or those\nthat do not provide a cookie notice at all. We also find that top non-EU\nwebsites have fewer cookie notices, suggesting higher levels of tracking when\nvisiting international sites. Our findings have deep implications both for\nunderstanding compliance with GDPR as well as understanding the evolution of\ntracking on the web.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:50:57 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Hu", "Xuehui", ""], ["Sastry", "Nishanth", ""]]}, {"id": "1905.01386", "submitter": "Manojkumar Rangasamy Kannadasan", "authors": "Manojkumar Rangasamy Kannadasan, Grigor Aslanyan", "title": "Personalized Query Auto-Completion Through a Lightweight Representation\n  of the User Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query Auto-Completion (QAC) is a widely used feature in many domains,\nincluding web and eCommerce search, suggesting full queries based on a prefix\ntyped by the user. QAC has been extensively studied in the literature in the\nrecent years, and it has been consistently shown that adding personalization\nfeatures can significantly improve the performance of QAC. In this work we\npropose a novel method for personalized QAC that uses lightweight embeddings\nlearnt through fastText. We construct an embedding for the user context\nqueries, which are the last few queries issued by the user. We also use the\nsame model to get the embedding for the candidate queries to be ranked. We\nintroduce ranking features that compute the distance between the candidate\nqueries and the context queries in the embedding space. These features are then\ncombined with other commonly used QAC ranking features to learn a ranking\nmodel. We apply our method to a large eCommerce search engine (eBay) and show\nthat the ranker with our proposed feature significantly outperforms the\nbaselines on all of the offline metrics measured, which includes Mean\nReciprocal Rank (MRR), Success Rate (SR), Mean Average Precision (MAP), and\nNormalized Discounted Cumulative Gain (NDCG). Our baselines include the Most\nPopular Completion (MPC) model as well as a ranking model without our proposed\nfeatures. The ranking model with the proposed features results in a $20-30\\%$\nimprovement over the MPC model on all metrics. We obtain up to a $5\\%$\nimprovement over the baseline ranking model for all the sessions, which goes up\nto about $10\\%$ when we restrict to sessions that contain the user context.\nMoreover, our proposed features also significantly outperform text based\npersonalization features studied in the literature before, and adding text\nbased features on top of our proposed embedding based features results only in\nminor improvements.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 23:28:18 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kannadasan", "Manojkumar Rangasamy", ""], ["Aslanyan", "Grigor", ""]]}, {"id": "1905.01395", "submitter": "Steffen Rendle", "authors": "Steffen Rendle, Li Zhang, Yehuda Koren", "title": "On the Difficulty of Evaluating Baselines: A Study on Recommender\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical evaluations with comparisons to baselines play a central role when\njudging research in recommender systems. In this paper, we show that running\nbaselines properly is difficult. We demonstrate this issue on two extensively\nstudied datasets. First, we show that results for baselines that have been used\nin numerous publications over the past five years for the Movielens 10M\nbenchmark are suboptimal. With a careful setup of a vanilla matrix\nfactorization baseline, we are not only able to improve upon the reported\nresults for this baseline but even outperform the reported results of any newly\nproposed method. Secondly, we recap the tremendous effort that was required by\nthe community to obtain high quality results for simple methods on the Netflix\nPrize. Our results indicate that empirical findings in research papers are\nquestionable unless they were obtained on standardized benchmarks where\nbaselines have been tuned extensively by the research community.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 00:27:22 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Rendle", "Steffen", ""], ["Zhang", "Li", ""], ["Koren", "Yehuda", ""]]}, {"id": "1905.01546", "submitter": "Pan Li", "authors": "Pan Li, Alexander Tuzhilin", "title": "Latent Unexpected and Useful Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing unexpected recommendations is an important task for recommender\nsystems. To do this, we need to start from the expectations of users and\ndeviate from these expectations when recommending items. Previously proposed\napproaches model user expectations in the feature space, making them limited to\nthe items that the user has visited or expected by the deduction of associated\nrules, without including the items that the user could also expect from the\nlatent, complex and heterogeneous interactions between users, items and\nentities. In this paper, we define unexpectedness in the latent space rather\nthan in the feature space and develop a novel Latent Convex Hull (LCH) method\nto provide unexpected recommendations. Extensive experiments on two real-world\ndatasets demonstrate the effectiveness of the proposed model that significantly\noutperforms alternative state-of-the-art unexpected recommendation methods in\nterms of unexpectedness measures while achieving the same level of accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 19:45:04 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Li", "Pan", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "1905.01553", "submitter": "Elham Shaabani", "authors": "Elham Shaabani, Ashkan Sadeghi-Mobarakeh, Hamidreza Alvari and Paulo\n  Shakarian", "title": "An End-to-End Framework to Identify Pathogenic Social Media Accounts on\n  Twitter", "comments": "9 pages, 8 figures, International Conference on Data Intelligence and\n  Security. arXiv admin note: text overlap with arXiv:1905.01556", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathogenic Social Media (PSM) accounts such as terrorist supporter accounts\nand fake news writers have the capability of spreading disinformation to viral\nproportions. Early detection of PSM accounts is crucial as they are likely to\nbe key users to make malicious information \"viral\". In this paper, we adopt the\ncausal inference framework along with graph-based metrics in order to\ndistinguish PSMs from normal users within a short time of their activities. We\npropose both supervised and semi-supervised approaches without taking the\nnetwork information and content into account. Results on a real-world dataset\nfrom Twitter accentuates the advantage of our proposed frameworks. We show our\napproach achieves 0.28 improvement in F1 score over existing approaches with\nthe precision of 0.90 and F1 score of 0.63.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 20:19:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shaabani", "Elham", ""], ["Sadeghi-Mobarakeh", "Ashkan", ""], ["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1905.01556", "submitter": "Elham Shaabani", "authors": "Elham Shaabani, Ruocheng Guo, and Paulo Shakarian", "title": "Detecting Pathogenic Social Media Accounts without Content or Network\n  Structure", "comments": "8 pages, 5 figures, International Conference on Data Intelligence and\n  Security. arXiv admin note: text overlap with arXiv:1905.01553", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of harmful mis-information in social media is a pressing problem.\nWe refer accounts that have the capability of spreading such information to\nviral proportions as \"Pathogenic Social Media\" accounts. These accounts include\nterrorist supporters accounts, water armies, and fake news writers. We\nintroduce an unsupervised causality-based framework that also leverages label\npropagation. This approach identifies these users without using network\nstructure, cascade path information, content and user's information. We show\nour approach obtains higher precision (0.75) in identifying Pathogenic Social\nMedia accounts in comparison with random (precision of 0.11) and existing bot\ndetection (precision of 0.16) methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 20:51:06 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Shaabani", "Elham", ""], ["Guo", "Ruocheng", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1905.01686", "submitter": "Michael Shekasta", "authors": "Michael Shekasta, Gilad Katz, Asnat Greenstein-Messica, Lior Rokach,\n  Bracha Shapira", "title": "New Item Consumption Prediction Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems have become ubiquitous in today's online world and are\nan integral part of practically every e-commerce platform. While traditional\nrecommender systems use customer history, this approach is not feasible in\n'cold start' scenarios. Such scenarios include the need to produce\nrecommendations for new or unregistered users and the introduction of new\nitems. In this study, we present the Purchase Intent Session-bAsed (PISA)\nalgorithm, a content-based algorithm for predicting the purchase intent for\ncold start session-based scenarios. Our approach employs deep learning\ntechniques both for modeling the content and purchase intent prediction. Our\nexperiments show that PISA outperforms a well-known deep learning baseline when\nnew items are introduced. In addition, while content-based approaches often\nfail to perform well in highly imbalanced datasets, our approach successfully\nhandles such cases. Finally, our experiments show that combining PISA with the\nbaseline in non-cold start scenarios further improves performance.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 14:01:16 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 11:48:41 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Shekasta", "Michael", ""], ["Katz", "Gilad", ""], ["Greenstein-Messica", "Asnat", ""], ["Rokach", "Lior", ""], ["Shapira", "Bracha", ""]]}, {"id": "1905.01712", "submitter": "Felipe Soares", "authors": "Felipe Soares and Martin Krallinger", "title": "BVS Corpus: A Multilingual Parallel Corpus of Biomedical Scientific\n  Texts", "comments": "Accepted at the Copora conference. arXiv admin note: text overlap\n  with arXiv:1905.01715", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The BVS database (Health Virtual Library) is a centralized source of\nbiomedical information for Latin America and Carib, created in 1998 and\ncoordinated by BIREME (Biblioteca Regional de Medicina) in agreement with the\nPan American Health Organization (OPAS). Abstracts are available in English,\nSpanish, and Portuguese, with a subset in more than one language, thus being a\npossible source of parallel corpora. In this article, we present the\ndevelopment of parallel corpora from BVS in three languages: English,\nPortuguese, and Spanish. Sentences were automatically aligned using the\nHunalign algorithm for EN/ES and EN/PT language pairs, and for a subset of\ntrilingual articles also. We demonstrate the capabilities of our corpus by\ntraining a Neural Machine Translation (OpenNMT) system for each language pair,\nwhich outperformed related works on scientific biomedical articles. Sentence\nalignment was also manually evaluated, presenting an average 96% of correctly\naligned sentences across all languages. Our parallel corpus is freely\navailable, with complementary information regarding article metadata.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 16:18:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Soares", "Felipe", ""], ["Krallinger", "Martin", ""]]}, {"id": "1905.01758", "submitter": "Hamed Zamani", "authors": "Harshith Padigela, Hamed Zamani, W. Bruce Croft", "title": "Investigating the Successes and Failures of BERT for Passage Re-Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bidirectional encoder representations from transformers (BERT) model has\nrecently advanced the state-of-the-art in passage re-ranking. In this paper, we\nanalyze the results produced by a fine-tuned BERT model to better understand\nthe reasons behind such substantial improvements. To this aim, we focus on the\nMS MARCO passage re-ranking dataset and provide potential reasons for the\nsuccesses and failures of BERT for retrieval. In more detail, we empirically\nstudy a set of hypotheses and provide additional analysis to explain the\nsuccessful performance of BERT.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 22:16:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Padigela", "Harshith", ""], ["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1905.01866", "submitter": "Wen Chen", "authors": "Wen Chen, Pipei Huang, Jiaming Xu, Xin Guo, Cheng Guo, Fei Sun, Chao\n  Li, Andreas Pfadler, Huan Zhao, Binqiang Zhao", "title": "POG: Personalized Outfit Generation for Fashion Recommendation at\n  Alibaba iFashion", "comments": "Till appear in KDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing demand for fashion recommendation raises a lot of challenges for\nonline shopping platforms and fashion communities. In particular, there exist\ntwo requirements for fashion outfit recommendation: the Compatibility of the\ngenerated fashion outfits, and the Personalization in the recommendation\nprocess. In this paper, we demonstrate these two requirements can be satisfied\nvia building a bridge between outfit generation and recommendation. Through\nlarge data analysis, we observe that people have similar tastes in individual\nitems and outfits. Therefore, we propose a Personalized Outfit Generation (POG)\nmodel, which connects user preferences regarding individual items and outfits\nwith Transformer architecture. Extensive offline and online experiments provide\nstrong quantitative evidence that our method outperforms alternative methods\nregarding both compatibility and personalization metrics. Furthermore, we\ndeploy POG on a platform named Dida in Alibaba to generate personalized outfits\nfor the users of the online application iFashion.\n  This work represents a first step towards an industrial-scale fashion outfit\ngeneration and recommendation solution, which goes beyond generating outfits\nbased on explicit queries, or merely recommending from existing outfit pools.\nAs part of this work, we release a large-scale dataset consisting of 1.01\nmillion outfits with rich context information, and 0.28 billion user click\nactions from 3.57 million users. To the best of our knowledge, this dataset is\nthe largest, publicly available, fashion related dataset, and the first to\nprovide user behaviors relating to both outfits and fashion items.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 08:00:25 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 09:09:07 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 03:30:28 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Chen", "Wen", ""], ["Huang", "Pipei", ""], ["Xu", "Jiaming", ""], ["Guo", "Xin", ""], ["Guo", "Cheng", ""], ["Sun", "Fei", ""], ["Li", "Chao", ""], ["Pfadler", "Andreas", ""], ["Zhao", "Huan", ""], ["Zhao", "Binqiang", ""]]}, {"id": "1905.01973", "submitter": "Simona Maggio", "authors": "B\\'eranger Dumont, Simona Maggio, Ghiles Sidi Said, Quoc-Tien Au", "title": "Who wrote this book? A challenge for e-commerce", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern e-commerce catalogs contain millions of references, associated with\ntextual and visual information that is of paramount importance for the products\nto be found via search or browsing. Of particular significance is the book\ncategory, where the author name(s) field poses a significant challenge. Indeed,\nbooks written by a given author (such as F. Scott Fitzgerald) might be listed\nwith different authors' names in a catalog due to abbreviations and spelling\nvariants and mistakes, among others. To solve this problem at scale, we design\na composite system involving open data sources for books as well as machine\nlearning components leveraging deep learning-based techniques for natural\nlanguage processing. In particular, we use Siamese neural networks for an\napproximate match with known author names, and direct correction of the\nprovided author's name using sequence-to-sequence learning with neural\nnetworks. We evaluate this approach on product data from the e-commerce website\nRakuten France, and find that the top proposal of the system is the normalized\nauthor name with 72% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 10:13:07 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Dumont", "B\u00e9ranger", ""], ["Maggio", "Simona", ""], ["Said", "Ghiles Sidi", ""], ["Au", "Quoc-Tien", ""]]}, {"id": "1905.01986", "submitter": "Robin Burke", "authors": "Himan Abdollahpouri, Gediminas Adomavicius, Robin Burke, Ido Guy,\n  Dietmar Jannach, Toshihiro Kamishima, Jan Krasnodebski and Luiz Pizzato", "title": "Beyond Personalization: Research Directions in Multistakeholder\n  Recommendation", "comments": "64 pages", "journal-ref": "User Model User-Adap Inter 30, 127-158 (2020)", "doi": "10.1007/s11257-019-09256-1", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are personalized information access applications; they\nare ubiquitous in today's online environment, and effective at finding items\nthat meet user needs and tastes. As the reach of recommender systems has\nextended, it has become apparent that the single-minded focus on the user\ncommon to academic research has obscured other important aspects of\nrecommendation outcomes. Properties such as fairness, balance, profitability,\nand reciprocity are not captured by typical metrics for recommender system\nevaluation. The concept of multistakeholder recommendation has emerged as a\nunifying framework for describing and understanding recommendation settings\nwhere the end user is not the sole focus. This article describes the origins of\nmultistakeholder recommendation, and the landscape of system designs. It\nprovides illustrative examples of current research, as well as outlining open\nquestions and research directions for the field.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 18:50:10 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2019 14:21:25 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Adomavicius", "Gediminas", ""], ["Burke", "Robin", ""], ["Guy", "Ido", ""], ["Jannach", "Dietmar", ""], ["Kamishima", "Toshihiro", ""], ["Krasnodebski", "Jan", ""], ["Pizzato", "Luiz", ""]]}, {"id": "1905.01987", "submitter": "Fahim Faisal", "authors": "Fahim Faisal (1), Shafkat Ahmed Bhuiyan (1), Dr. Abu Raihan Mostofa\n  Kamal (1) ((1) Islamic University of Technology)", "title": "Disease Identification From Unstructured User Input", "comments": "This was an undergraduate research. The hypotheses it proposes is\n  based on a small number of samples and thus, can not be declared significant.\n  To declare it significant, a large number of sample testing is needed. After\n  that, it can be put through", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method to identify probable diseases from the unstructured textual input\n(eg, health forum posts) by incorporating a lexicographic and semantic feature\nbased two-phase text classification module and a symptom-disease\ncorrelation-based similarity measurement module. One notable aspect of my\napproach was to develop a competent algorithm to extract all inherent features\nfrom the data source to make a better decision.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 05:10:48 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 11:18:08 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Faisal", "Fahim", "", "Islamic University of Technology"], ["Bhuiyan", "Shafkat Ahmed", "", "Islamic University of Technology"], ["Kamal", "Dr. Abu Raihan Mostofa", "", "Islamic University of Technology"]]}, {"id": "1905.01989", "submitter": "Sahin Geyik", "authors": "Sahin Cem Geyik, Stuart Ambler, Krishnaram Kenthapadi", "title": "Fairness-Aware Ranking in Search & Recommendation Systems with\n  Application to LinkedIn Talent Search", "comments": "This paper has been accepted for publication at ACM KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330691", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for quantifying and mitigating algorithmic bias in\nmechanisms designed for ranking individuals, typically used as part of\nweb-scale search and recommendation systems. We first propose complementary\nmeasures to quantify bias with respect to protected attributes such as gender\nand age. We then present algorithms for computing fairness-aware re-ranking of\nresults. For a given search or recommendation task, our algorithms seek to\nachieve a desired distribution of top ranked results with respect to one or\nmore protected attributes. We show that such a framework can be tailored to\nachieve fairness criteria such as equality of opportunity and demographic\nparity depending on the choice of the desired distribution. We evaluate the\nproposed algorithms via extensive simulations over different parameter choices,\nand study the effect of fairness-aware ranking on both bias and utility\nmeasures. We finally present the online A/B testing results from applying our\nframework towards representative ranking in LinkedIn Talent Search, and discuss\nthe lessons learned in practice. Our approach resulted in tremendous\nimprovement in the fairness metrics (nearly three fold increase in the number\nof search queries with representative results) without affecting the business\nmetrics, which paved the way for deployment to 100% of LinkedIn Recruiter users\nworldwide. Ours is the first large-scale deployed framework for ensuring\nfairness in the hiring domain, with the potential positive impact for more than\n630M LinkedIn members.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 21:06:49 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 22:48:45 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 19:22:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Geyik", "Sahin Cem", ""], ["Ambler", "Stuart", ""], ["Kenthapadi", "Krishnaram", ""]]}, {"id": "1905.01990", "submitter": "Won-Yong Shin", "authors": "Cong Tran, Jang-Young Kim, Won-Yong Shin, Sang-Wook Kim", "title": "Clustering-Based Collaborative Filtering Using an Incentivized/Penalized\n  User Model", "comments": "12 pages, 4 figures, 6 tables, To appear in the IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Giving or recommending appropriate content based on the quality of experience\nis the most important and challenging issue in recommender systems. As\ncollaborative filtering (CF) is one of the most prominent and popular\ntechniques used for recommender systems, we propose a new clustering-based CF\n(CBCF) method using an incentivized/penalized user (IPU) model only with\nratings given by users, which is thus easy to implement. We aim to design such\na simple clustering-based approach with no further prior information while\nimproving the recommendation accuracy. To be precise, the purpose of CBCF with\nthe IPU model is to improve recommendation performance such as precision,\nrecall, and $F_1$ score by carefully exploiting different preferences among\nusers. Specifically, we formulate a constrained optimization problem, in which\nwe aim to maximize the recall (or equivalently $F_1$ score) for a given\nprecision. To this end, users are divided into several clusters based on the\nactual rating data and Pearson correlation coefficient. Afterwards, we give\neach item an incentive/penalty according to the preference tendency by users\nwithin the same cluster. Our experimental results show a significant\nperformance improvement over the baseline CF scheme without clustering in terms\nof recall or $F_1$ score for a given precision.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 06:47:36 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Tran", "Cong", ""], ["Kim", "Jang-Young", ""], ["Shin", "Won-Yong", ""], ["Kim", "Sang-Wook", ""]]}, {"id": "1905.01991", "submitter": "Sudipto Mukherjee", "authors": "Sudipto Mukherjee and Ke Jiang", "title": "A Content-Based Approach to Email Triage Action Prediction: Exploration\n  and Evaluation", "comments": "User representations, Personalization, Email response prediction,\n  Similarity features", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email has remained a principal form of communication among people, both in\nenterprise and social settings. With a deluge of emails crowding our mailboxes\ndaily, there is a dire need of smart email systems that can recover important\nemails and make personalized recommendations. In this work, we study the\nproblem of predicting user triage actions to incoming emails where we take the\nreply prediction as a working example. Different from existing methods, we\nformulate the triage action prediction as a recommendation problem and focus on\nthe content-based approach, where the users are represented using the content\nof current and past emails. We also introduce additional similarity features to\nfurther explore the affinities between users and emails. Experiments on the\npublicly available Avocado email collection demonstrate the advantages of our\nproposed recommendation framework and our method is able to achieve better\nperformance compared to the state-of-the-art deep recommendation methods. More\nimportantly, we provide valuable insight into the effectiveness of different\ntextual and user representations and show that traditional bag-of-words\napproaches, with the help from the similarity features, compete favorably with\nthe more advanced neural embedding methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 01:52:57 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mukherjee", "Sudipto", ""], ["Jiang", "Ke", ""]]}, {"id": "1905.01997", "submitter": "Hui Fang", "authors": "Hui Fang, Danning Zhang, Yiheng Shu, and Guibing Guo", "title": "Deep Learning for Sequential Recommendation: Algorithms, Influential\n  Factors, and Evaluations", "comments": "41 pages, 19 figures, 6 tables, 155 references, TOIS accepted", "journal-ref": "ACM Transactions on Information Systems, 2020", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of sequential recommendation, deep learning (DL)-based methods\nhave received a lot of attention in the past few years and surpassed\ntraditional models such as Markov chain-based and factorization-based ones.\nHowever, there is little systematic study on DL-based methods, especially\nregarding to how to design an effective DL model for sequential recommendation.\nIn this view, this survey focuses on DL-based sequential recommender systems by\ntaking the aforementioned issues into consideration. Specifically,we illustrate\nthe concept of sequential recommendation, propose a categorization of existing\nalgorithms in terms of three types of behavioral sequence, summarize the key\nfactors affecting the performance of DL-based models, and conduct corresponding\nevaluations to demonstrate the effects of these factors. We conclude this\nsurvey by systematically outlining future directions and challenges in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:13:50 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 07:34:44 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 07:11:22 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Fang", "Hui", ""], ["Zhang", "Danning", ""], ["Shu", "Yiheng", ""], ["Guo", "Guibing", ""]]}, {"id": "1905.02009", "submitter": "Wenhui Yu", "authors": "Wenhui Yu, Xiangnan He, Jian Pei, Xu Chen, Li Xiong, Jinfei Liu, Zheng\n  Qin", "title": "Visually-aware Recommendation with Aesthetic Features", "comments": "Accepted by VLDBJ. arXiv admin note: substantial text overlap with\n  arXiv:1809.05822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information plays a critical role in human decision-making process.\nWhile recent developments on visually-aware recommender systems have taken the\nproduct image into account, none of them has considered the aesthetic aspect.\nWe argue that the aesthetic factor is very important in modeling and predicting\nusers' preferences, especially for some fashion-related domains like clothing\nand jewelry. This work addresses the need of modeling aesthetic information in\nvisually-aware recommender systems. Technically speaking, we make three key\ncontributions in leveraging deep aesthetic features: (1) To describe the\naesthetics of products, we introduce the aesthetic features extracted from\nproduct images by a deep aesthetic network. We incorporate these features into\nrecommender system to model users' preferences in the aesthetic aspect. (2)\nSince in clothing recommendation, time is very important for users to make\ndecision, we design a new tensor decomposition model for implicit feedback\ndata. The aesthetic features are then injected to the basic tensor model to\ncapture the temporal dynamics of aesthetic preferences (e.g., seasonal\npatterns). (3) We also use the aesthetic features to optimize the learning\nstrategy on implicit feedback data. We enrich the pairwise training samples by\nconsidering the similarity among items in the visual space and graph space; the\nkey idea is that a user may likely have similar perception on similar items. We\nperform extensive experiments on several real-world datasets and demonstrate\nthe usefulness of aesthetic features and the effectiveness of our proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 14:35:07 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 08:52:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Yu", "Wenhui", ""], ["He", "Xiangnan", ""], ["Pei", "Jian", ""], ["Chen", "Xu", ""], ["Xiong", "Li", ""], ["Liu", "Jinfei", ""], ["Qin", "Zheng", ""]]}, {"id": "1905.02331", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, Inderjit\n  Dhillon", "title": "Taming Pretrained Transformers for Extreme Multi-label Text\n  Classification", "comments": "KDD 2020 Applied Data Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the extreme multi-label text classification (XMC) problem: given\nan input text, return the most relevant labels from a large label collection.\nFor example, the input text could be a product description on Amazon.com and\nthe labels could be product categories. XMC is an important yet challenging\nproblem in the NLP community. Recently, deep pretrained transformer models have\nachieved state-of-the-art performance on many NLP tasks including sentence\nclassification, albeit with small label sets. However, naively applying deep\ntransformer models to the XMC problem leads to sub-optimal performance due to\nthe large output space and the label sparsity issue. In this paper, we propose\nX-Transformer, the first scalable approach to fine-tuning deep transformer\nmodels for the XMC problem. The proposed method achieves new state-of-the-art\nresults on four XMC benchmark datasets. In particular, on a Wiki dataset with\naround 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial\nimprovement over state-of-the-art XMC approaches Parabel (linear) and\nAttentionXML (neural), which achieve 68.70% and 76.95% precision@1,\nrespectively. We further apply X-Transformer to a product2query dataset from\nAmazon and gained 10.7% relative improvement on prec@1 over Parabel.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 02:32:06 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 18:54:52 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 01:13:05 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 19:28:18 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Yu", "Hsiang-Fu", ""], ["Zhong", "Kai", ""], ["Yang", "Yiming", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1905.02430", "submitter": "Iva Gornishka", "authors": "Iva Gornishka, Stevan Rudinac, Marcel Worring", "title": "Interactive Search and Exploration in Online Discussion Forums Using\n  Multimodal Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel interactive multimodal learning system,\nwhich facilitates search and exploration in large networks of social multimedia\nusers. It allows the analyst to identify and select users of interest, and to\nfind similar users in an interactive learning setting. Our approach is based on\nnovel multimodal representations of users, words and concepts, which we\nsimultaneously learn by deploying a general-purpose neural embedding model. We\nshow these representations to be useful not only for categorizing users, but\nalso for automatically generating user and community profiles. Inspired by\ntraditional summarization approaches, we create the profiles by selecting\ndiverse and representative content from all available modalities, i.e. the\ntext, image and user modality. The usefulness of the approach is evaluated\nusing artificial actors, which simulate user behavior in a relevance feedback\nscenario. Multiple experiments were conducted in order to evaluate the quality\nof our multimodal representations, to compare different embedding strategies,\nand to determine the importance of different modalities. We demonstrate the\ncapabilities of the proposed approach on two different multimedia collections\noriginating from the violent online extremism forum Stormfront and the\nmicroblogging platform Twitter, which are particularly interesting due to the\nhigh semantic level of the discussions they feature.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 09:23:12 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Gornishka", "Iva", ""], ["Rudinac", "Stevan", ""], ["Worring", "Marcel", ""]]}, {"id": "1905.02623", "submitter": "Nataliya Shakhovska Prof", "authors": "Nataliya Shakhovska, Taras Cherna", "title": "The method of automatic summarization from different sources", "comments": null, "journal-ref": "ECONTECHMOD. AN INTERNATIONAL QUARTERLY JOURNAL - 2016. Vol. 5.\n  No. 1. 103-109", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article is analyzed technology of automatic text abstracting and\nannotation. The role of annotation in automatic search and classification for\ndifferent scientific articles is described. The algorithm of summarization of\nnatural language documents using the concept of importance coefficients is\ndeveloped. Such concept allows considering the peculiarity of subject areas and\ntopics that could be found in different kinds of documents. Method for\ngenerating abstracts of single document based on frequency analysis is\ndeveloped. The recognition elements for unstructured text analysis are given.\nThe method of pre-processing analysis of several documents is developed. This\ntechnique simultaneously considers both statistical approaches to abstracting\nand the importance of terms in a particular subject domain. The quality of\ngenerated abstract is evaluated. For the developed system there was conducted\nexperts evaluation. It was held only for texts in Ukrainian. The developed\nsystem concluding essay has higher aggregate score on all criteria. The\nsummarization system architecture is building. To build an information system\nmodel there is used CASE-tool AllFusion ERwin Data Modeler. The database scheme\nfor information saving was built. The system is designed to work primarily with\nUkrainian texts, which gives a significant advantage, since most modern systems\nstill oriented to English texts\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:56:14 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Shakhovska", "Nataliya", ""], ["Cherna", "Taras", ""]]}, {"id": "1905.02681", "submitter": "Armel Jacques Nzekon Nzeko'o", "authors": "Armel Jacques Nzekon Nzeko'o, Maurice Tchuente, Matthieu Latapy", "title": "A general graph-based framework for top-N recommendation using content,\n  temporal and trust information", "comments": "27 pages", "journal-ref": "Journal of Interdisciplinary Methodologies and Issues in Sciences,\n  2019", "doi": "10.18713/JIMIS-300519-5-2", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending appropriate items to users is crucial in many e-commerce\nplatforms that contain implicit data as users' browsing, purchasing and\nstreaming history. One common approach consists in selecting the N most\nrelevant items to each user, for a given N, which is called top-N\nrecommendation. To do so, recommender systems rely on various kinds of\ninformation, like item and user features, past interest of users for items,\nbrowsing history and trust between users. However, they often use only one or\ntwo such pieces of information, which limits their performance. In this paper,\nwe design and implement GraFC2T2, a general graph-based framework to easily\ncombine and compare various kinds of side information for top-N recommendation.\nIt encodes content-based features, temporal and trust information into a\ncomplex graph, and uses personalized PageRank on this graph to perform\nrecommendation. We conduct experiments on Epinions and Ciao datasets, and\ncompare obtained performances using F1-score, Hit ratio and MAP evaluation\nmetrics, to systems based on matrix factorization and deep learning. This shows\nthat our framework is convenient for such explorations, and that combining\ndifferent kinds of information indeed improves recommendation in general.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 12:52:44 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Nzeko'o", "Armel Jacques Nzekon", ""], ["Tchuente", "Maurice", ""], ["Latapy", "Matthieu", ""]]}, {"id": "1905.02851", "submitter": "Tomohide Shibata", "authors": "Wataru Sakata, Tomohide Shibata, Ribeka Tanaka, Sadao Kurohashi", "title": "FAQ Retrieval using Query-Question Similarity and BERT-Based\n  Query-Answer Relevance", "comments": "Accepted in SIGIR 2019 (short paper), camera ready, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently Asked Question (FAQ) retrieval is an important task where the\nobjective is to retrieve an appropriate Question-Answer (QA) pair from a\ndatabase based on a user's query. We propose a FAQ retrieval system that\nconsiders the similarity between a user's query and a question as well as the\nrelevance between the query and an answer. Although a common approach to FAQ\nretrieval is to construct labeled data for training, it takes annotation costs.\nTherefore, we use a traditional unsupervised information retrieval system to\ncalculate the similarity between the query and question. On the other hand, the\nrelevance between the query and answer can be learned by using QA pairs in a\nFAQ database. The recently-proposed BERT model is used for the relevance\ncalculation. Since the number of QA pairs in FAQ page is not enough to train a\nmodel, we cope with this issue by leveraging FAQ sets that are similar to the\none in question. We evaluate our approach on two datasets. The first one is\nlocalgovFAQ, a dataset we construct in a Japanese administrative municipality\ndomain. The second is StackExchange dataset, which is the public dataset in\nEnglish. We demonstrate that our proposed method outperforms baseline methods\non these datasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 00:33:37 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 00:14:18 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Sakata", "Wataru", ""], ["Shibata", "Tomohide", ""], ["Tanaka", "Ribeka", ""], ["Kurohashi", "Sadao", ""]]}, {"id": "1905.03028", "submitter": "Kan Ren", "authors": "Kan Ren, Jiarui Qin, Lei Zheng, Zhengyu Yang, Weinan Zhang and Yong Yu", "title": "Deep Landscape Forecasting for Real-time Bidding Advertising", "comments": "KDD 2019. The reproducible code and dataset link is\n  https://github.com/rk2900/DLF", "journal-ref": null, "doi": "10.1145/3292500.3330870", "report-no": null, "categories": "cs.IR cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of real-time auction in online advertising has drawn huge\nattention of modeling the market competition, i.e., bid landscape forecasting.\nThe problem is formulated as to forecast the probability distribution of market\nprice for each ad auction. With the consideration of the censorship issue which\nis caused by the second-price auction mechanism, many researchers have devoted\ntheir efforts on bid landscape forecasting by incorporating survival analysis\nfrom medical research field. However, most existing solutions mainly focus on\neither counting-based statistics of the segmented sample clusters, or learning\na parameterized model based on some heuristic assumptions of distribution\nforms. Moreover, they neither consider the sequential patterns of the feature\nover the price space. In order to capture more sophisticated yet flexible\npatterns at fine-grained level of the data, we propose a Deep Landscape\nForecasting (DLF) model which combines deep learning for probability\ndistribution forecasting and survival analysis for censorship handling.\nSpecifically, we utilize a recurrent neural network to flexibly model the\nconditional winning probability w.r.t. each bid price. Then we conduct the bid\nlandscape forecasting through probability chain rule with strict mathematical\nderivations. And, in an end-to-end manner, we optimize the model by minimizing\ntwo negative likelihood losses with comprehensive motivations. Without any\nspecific assumption for the distribution form of bid landscape, our model shows\ngreat advantages over previous works on fitting various sophisticated market\nprice distributions. In the experiments over two large-scale real-world\ndatasets, our model significantly outperforms the state-of-the-art solutions\nunder various metrics.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:22:02 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 16:27:14 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ren", "Kan", ""], ["Qin", "Jiarui", ""], ["Zheng", "Lei", ""], ["Yang", "Zhengyu", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1905.03282", "submitter": "Behrooz Razeghi", "authors": "Shideh Rezaeifar, Behrooz Razeghi, Olga Taran, Taras Holotyak, Slava\n  Voloshynovskiy", "title": "Reconstruction of Privacy-Sensitive Data from Protected Templates", "comments": "accepted at ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of data reconstruction from\nprivacy-protected templates, based on recent concept of sparse ternary coding\nwith ambiguization (STCA). The STCA is a generalization of randomization\ntechniques which includes random projections, lossy quantization, and addition\nof ambiguization noise to satisfy the privacy-utility trade-off requirements.\nThe theoretical privacy-preserving properties of STCA have been validated on\nsynthetic data. However, the applicability of STCA to real data and potential\nthreats linked to reconstruction based on recent deep reconstruction algorithms\nare still open problems. Our results demonstrate that STCA still achieves the\nclaimed theoretical performance when facing deep reconstruction attacks for the\nsynthetic i.i.d. data, while for real images special measures are required to\nguarantee proper protection of the templates.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 18:17:01 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Rezaeifar", "Shideh", ""], ["Razeghi", "Behrooz", ""], ["Taran", "Olga", ""], ["Holotyak", "Taras", ""], ["Voloshynovskiy", "Slava", ""]]}, {"id": "1905.03375", "submitter": "Harald Steck", "authors": "Harald Steck", "title": "Embarrassingly Shallow Autoencoders for Sparse Data", "comments": "In the proceedings of the Web Conference (WWW) 2019 (7 pages)", "journal-ref": null, "doi": "10.1145/3308558.3313710", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Combining simple elements from the literature, we define a linear model that\nis geared toward sparse data, in particular implicit feedback data for\nrecommender systems. We show that its training objective has a closed-form\nsolution, and discuss the resulting conceptual insights. Surprisingly, this\nsimple model achieves better ranking accuracy than various state-of-the-art\ncollaborative-filtering approaches, including deep non-linear models, on most\nof the publicly available data-sets used in our experiments.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 22:16:59 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Steck", "Harald", ""]]}, {"id": "1905.03752", "submitter": "Chenghao Liu", "authors": "Chenghao Liu, Tao Lu, Xin Wang, Zhiyong Cheng, Jianling Sun, Steven\n  C.H. Hoi", "title": "Compositional Coding for Collaborative Filtering", "comments": "SIGIR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiency is crucial to the online recommender systems. Representing users\nand items as binary vectors for Collaborative Filtering (CF) can achieve fast\nuser-item affinity computation in the Hamming space, in recent years, we have\nwitnessed an emerging research effort in exploiting binary hashing techniques\nfor CF methods. However, CF with binary codes naturally suffers from low\naccuracy due to limited representation capability in each bit, which impedes it\nfrom modeling complex structure of the data.\n  In this work, we attempt to improve the efficiency without hurting the model\nperformance by utilizing both the accuracy of real-valued vectors and the\nefficiency of binary codes to represent users/items. In particular, we propose\nthe Compositional Coding for Collaborative Filtering (CCCF) framework, which\nnot only gains better recommendation efficiency than the state-of-the-art\nbinarized CF approaches but also achieves even higher accuracy than the\nreal-valued CF method. Specifically, CCCF innovatively represents each\nuser/item with a set of binary vectors, which are associated with a sparse\nreal-value weight vector. Each value of the weight vector encodes the\nimportance of the corresponding binary vector to the user/item. The continuous\nweight vectors greatly enhances the representation capability of binary codes,\nand its sparsity guarantees the processing speed. Furthermore, an integer\nweight approximation scheme is proposed to further accelerate the speed. Based\non the CCCF framework, we design an efficient discrete optimization algorithm\nto learn its parameters. Extensive experiments on three real-world datasets\nshow that our method outperforms the state-of-the-art binarized CF methods\n(even achieves better performance than the real-valued CF method) by a large\nmargin in terms of both recommendation accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 16:57:27 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Liu", "Chenghao", ""], ["Lu", "Tao", ""], ["Wang", "Xin", ""], ["Cheng", "Zhiyong", ""], ["Sun", "Jianling", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "1905.04413", "submitter": "Hongwei Wang", "authors": "Hongwei Wang, Fuzheng Zhang, Mengdi Zhang, Jure Leskovec, Miao Zhao,\n  Wenjie Li, Zhongyuan Wang", "title": "Knowledge-aware Graph Neural Networks with Label Smoothness\n  Regularization for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs capture structured information and relations between a set\nof entities or items. As such knowledge graphs represent an attractive source\nof information that could help improve recommender systems. However, existing\napproaches in this domain rely on manual feature engineering and do not allow\nfor an end-to-end training. Here we propose Knowledge-aware Graph Neural\nNetworks with Label Smoothness regularization (KGNN-LS) to provide better\nrecommendations. Conceptually, our approach computes user-specific item\nembeddings by first applying a trainable function that identifies important\nknowledge graph relationships for a given user. This way we transform the\nknowledge graph into a user-specific weighted graph and then apply a graph\nneural network to compute personalized item embeddings. To provide better\ninductive bias, we rely on label smoothness assumption, which posits that\nadjacent items in the knowledge graph are likely to have similar user relevance\nlabels/scores. Label smoothness provides regularization over the edge weights\nand we prove that it is equivalent to a label propagation scheme on a graph. We\nalso develop an efficient implementation that shows strong scalability with\nrespect to the knowledge graph size. Experiments on four datasets show that our\nmethod outperforms state of the art baselines. KGNN-LS also achieves strong\nperformance in cold-start scenarios where user-item interactions are sparse.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 00:43:54 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 05:27:40 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 00:25:25 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Wang", "Hongwei", ""], ["Zhang", "Fuzheng", ""], ["Zhang", "Mengdi", ""], ["Leskovec", "Jure", ""], ["Zhao", "Miao", ""], ["Li", "Wenjie", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "1905.04454", "submitter": "Mingbao Lin", "authors": "Mingbao Lin, Rongrong Ji, Hong Liu, Xiaoshuai Sun, Shen Chen, Qi Tian", "title": "Hadamard Matrix Guided Online Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online image hashing has attracted increasing research attention recently,\nwhich receives large-scale data in a streaming manner to update the hash\nfunctions on-the-fly. Its key challenge lies in the difficulty of balancing the\nlearning timeliness and model accuracy. To this end, most works follow a\nsupervised setting, i.e., using class labels to boost the hashing performance,\nwhich defects in two aspects: First, strong constraints, e.g., orthogonal or\nsimilarity preserving, are used, which however are typically relaxed and lead\nto large accuracy drop. Second, large amounts of training batches are required\nto learn the up-to-date hash functions, which largely increase the learning\ncomplexity. To handle the above challenges, a novel supervised online hashing\nscheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this\npaper. Our key innovation lies in introducing Hadamard matrix, which is an\northogonal binary matrix built via Sylvester method. In particular, to release\nthe need of strong constraints, we regard each column of Hadamard matrix as the\ntarget code for each class label, which by nature satisfies several desired\nproperties of hashing codes. To accelerate the online training, LSH is first\nadopted to align the lengths of target code and to-be-learned binary code. We\nthen treat the learning of hash functions as a set of binary classification\nproblems to fit the assigned target code. Finally, extensive experiments\ndemonstrate the superior accuracy and efficiency of the proposed method over\nvarious state-of-the-art methods. Codes are available at\nhttps://github.com/lmbxmu/mycode.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 05:53:31 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 09:44:17 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 14:31:52 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Lin", "Mingbao", ""], ["Ji", "Rongrong", ""], ["Liu", "Hong", ""], ["Sun", "Xiaoshuai", ""], ["Chen", "Shen", ""], ["Tian", "Qi", ""]]}, {"id": "1905.04564", "submitter": "Amar Saini", "authors": "Amar Saini", "title": "PrivateJobMatch: A Privacy-Oriented Deferred Multi-Match Recommender\n  System for Stable Employment", "comments": "45 pages, 28 figures, RecSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordination failure reduces match quality among employers and candidates in\nthe job market, resulting in a large number of unfilled positions and/or\nunstable, short-term employment. Centralized job search engines provide a\nplatform that connects directly employers with job-seekers. However, they\nrequire users to disclose a significant amount of personal data, i.e., build a\nuser profile, in order to provide meaningful recommendations. In this paper, we\npresent PrivateJobMatch -- a privacy-oriented deferred multi-match recommender\nsystem -- which generates stable pairings while requiring users to provide only\na partial ranking of their preferences. PrivateJobMatch explores a series of\nadaptations of the game-theoretic Gale-Shapley deferred-acceptance algorithm\nwhich combine the flexibility of decentralized markets with the intelligence of\ncentralized matching. We identify the shortcomings of the original algorithm\nwhen applied to a job market and propose novel solutions that rely on machine\nlearning techniques. Experimental results on real and synthetic data confirm\nthe benefits of the proposed algorithms across several quality measures. Over\nthe past year, we have implemented a PrivateJobMatch prototype and deployed it\nin an active job market economy. Using the gathered real-user preference data,\nwe find that the match-recommendations are superior to a typical decentralized\njob market---while requiring only a partial ranking of the user preferences.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 17:58:40 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Saini", "Amar", ""]]}, {"id": "1905.04577", "submitter": "Suzan Verberne", "authors": "Suzan Verberne, Jiyin He, Gineke Wiggers, Tony Russell-Rose, Udo\n  Kruschwitz, Arjen P. de Vries", "title": "Information search in a professional context - exploring a collection of\n  professional search tasks", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search conducted in a work context is an everyday activity that has been\naround since long before the Web was invented, yet we still seem to understand\nlittle about its general characteristics. With this paper we aim to contribute\nto a better understanding of this large but rather multi-faceted area of\n`professional search'. Unlike task-based studies that aim at measuring the\neffectiveness of search methods, we chose to take a step back by conducting a\nsurvey among professional searchers to understand their typical search tasks.\nBy doing so we offer complementary insights into the subject area. We asked our\nrespondents to provide actual search tasks they have worked on, information\nabout how these were conducted and details on how successful they eventually\nwere. We then manually coded the collection of 56 search tasks with task\ncharacteristics and relevance criteria, and used the coded dataset for\nexploration purposes. Despite the relatively small scale of this study, our\ndata provides enough evidence that professional search is indeed very different\nfrom Web search in many key respects and that this is a field that offers many\navenues for future research.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 19:05:20 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Verberne", "Suzan", ""], ["He", "Jiyin", ""], ["Wiggers", "Gineke", ""], ["Russell-Rose", "Tony", ""], ["Kruschwitz", "Udo", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1905.04590", "submitter": "Yang Zhang", "authors": "Yang Zhang", "title": "Language in Our Time: An Empirical Analysis of Hashtags", "comments": "WWW 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashtags in online social networks have gained tremendous popularity during\nthe past five years. The resulting large quantity of data has provided a new\nlens into modern society. Previously, researchers mainly rely on data collected\nfrom Twitter to study either a certain type of hashtags or a certain property\nof hashtags. In this paper, we perform the first large-scale empirical analysis\nof hashtags shared on Instagram, the major platform for hashtag-sharing. We\nstudy hashtags from three different dimensions including the temporal-spatial\ndimension, the semantic dimension, and the social dimension. Extensive\nexperiments performed on three large-scale datasets with more than 7 million\nhashtags in total provide a series of interesting observations. First, we show\nthat the temporal patterns of hashtags can be categorized into four different\nclusters, and people tend to share fewer hashtags at certain places and more\nhashtags at others. Second, we observe that a non-negligible proportion of\nhashtags exhibit large semantic displacement. We demonstrate hashtags that are\nmore uniformly shared among users, as quantified by the proposed hashtag\nentropy, are less prone to semantic displacement. In the end, we propose a\nbipartite graph embedding model to summarize users' hashtag profiles, and rely\non these profiles to perform friendship prediction. Evaluation results show\nthat our approach achieves an effective prediction with AUC (area under the ROC\ncurve) above 0.8 which demonstrates the strong social signals possessed in\nhashtags.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 20:47:30 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Zhang", "Yang", ""]]}, {"id": "1905.04611", "submitter": "Tatsuki Sekino", "authors": "Tatsuki Sekino", "title": "Data description and retrieval using periods represented by uncertain\n  time intervals", "comments": "9 pages, 8 figures, 1 table; Journal of Information Processing (JIP)\n  28(2), in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time periods are frequently used to specify time in metadata and retrieval.\nHowever, it is not easy to describe and retrieve information about periods,\nbecause the temporal ranges represented by periods are often ambiguous. This is\nbecause these temporal ranges do not have fixed beginning and end points. To\nsolve this problem, basic logics to describe and process uncertain time\nintervals were developed in this study. An uncertain time interval is\nrepresented as a set of time intervals that indicate states when the uncertain\ntime interval is determined. Based on this concept, a logic to retrieve\nuncertain time intervals satisfying a given condition was established, and it\nwas revealed that retrieval results belong to three states: reliable,\nimpossible, and possible matches. Additionally, to describe data about\nuncertain periods, an ontology (the HuTime Ontology) was constructed based on\nthe logic. This ontology is characterized by the fact that uncertain time\nintervals can be defined recursively. It is expected that more data about time\nperiods will be created and released using the result of this study.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 23:48:03 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 23:59:17 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Sekino", "Tatsuki", ""]]}, {"id": "1905.04749", "submitter": "Md. Tawkat Islam Khondaker", "authors": "Junaed Younus Khan, Md. Tawkat Islam Khondaker, Sadia Afroz, Gias\n  Uddin, Anindya Iqbal", "title": "A Benchmark Study of Machine Learning Models for Online Fake News\n  Detection", "comments": "22 pages, 5 figures, to be published in Machine Learning with\n  Applications journal", "journal-ref": "Machine Learning with Applications: 4(2021).100032", "doi": "10.1016/j.mlwa.2021.100032", "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of fake news and its propagation on social media has become\na major concern due to its ability to create devastating impacts. Different\nmachine learning approaches have been suggested to detect fake news. However,\nmost of those focused on a specific type of news (such as political) which\nleads us to the question of dataset-bias of the models used. In this research,\nwe conducted a benchmark study to assess the performance of different\napplicable machine learning approaches on three different datasets where we\naccumulated the largest and most diversified one. We explored a number of\nadvanced pre-trained language models for fake news detection along with the\ntraditional and deep learning ones and compared their performances from\ndifferent aspects for the first time to the best of our knowledge. We find that\nBERT and similar pre-trained models perform the best for fake news detection,\nespecially with very small dataset. Hence, these models are significantly\nbetter option for languages with limited electronic contents, i.e., training\ndata. We also carried out several analysis based on the models' performance,\narticle's topic, article's length, and discussed different lessons learned from\nthem. We believe that this benchmark study will help the research community to\nexplore further and news sites/blogs to select the most appropriate fake news\ndetection method.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 17:15:11 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 05:30:08 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Khan", "Junaed Younus", ""], ["Khondaker", "Md. Tawkat Islam", ""], ["Afroz", "Sadia", ""], ["Uddin", "Gias", ""], ["Iqbal", "Anindya", ""]]}, {"id": "1905.04877", "submitter": "Yangyang Guo", "authors": "Yangyang Guo and Zhiyong Cheng and Liqiang Nie and Yibing Liu and\n  Yinglong Wang and Mohan Kankanhalli", "title": "Quantifying and Alleviating the Language Prior Problem in Visual\n  Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the advancement of computer vision, natural language\nprocessing and information retrieval techniques, visual question answering\n(VQA), which aims to answer questions about an image or a video, has received\nlots of attentions over the past few years. Although some progress has been\nachieved so far, several studies have pointed out that current VQA models are\nheavily affected by the \\emph{language prior problem}, which means they tend to\nanswer questions based on the co-occurrence patterns of question keywords\n(e.g., how many) and answers (e.g., 2) instead of understanding images and\nquestions. Existing methods attempt to solve this problem by either balancing\nthe biased datasets or forcing models to better understand images. However,\nonly marginal effects and even performance deterioration are observed for the\nfirst and second solution, respectively. In addition, another important issue\nis the lack of measurement to quantitatively measure the extent of the language\nprior effect, which severely hinders the advancement of related techniques.\n  In this paper, we make contributions to solve the above problems from two\nperspectives. Firstly, we design a metric to quantitatively measure the\nlanguage prior effect of VQA models. The proposed metric has been demonstrated\nto be effective in our empirical studies. Secondly, we propose a regularization\nmethod (i.e., score regularization module) to enhance current VQA models by\nalleviating the language prior problem as well as boosting the backbone model\nperformance. The proposed score regularization module adopts a pair-wise\nlearning strategy, which makes the VQA models answer the question based on the\nreasoning of the image (upon this question) instead of basing on\nquestion-answer patterns observed in the biased training set. The score\nregularization module is flexible to be integrated into various VQA models.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 06:31:33 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Guo", "Yangyang", ""], ["Cheng", "Zhiyong", ""], ["Nie", "Liqiang", ""], ["Liu", "Yibing", ""], ["Wang", "Yinglong", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1905.05044", "submitter": "Eirini Papagiannopoulou", "authors": "Eirini Papagiannopoulou and Grigorios Tsoumakas", "title": "A Review of Keyphrase Extraction", "comments": "author pre-print version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase extraction is a textual information processing task concerned with\nthe automatic extraction of representative and characteristic phrases from a\ndocument that express all the key aspects of its content. Keyphrases constitute\na succinct conceptual summary of a document, which is very useful in digital\ninformation management systems for semantic indexing, faceted search, document\nclustering and classification. This article introduces keyphrase extraction,\nprovides a well-structured review of the existing work, offers interesting\ninsights on the different evaluation approaches, highlights open issues and\npresents a comparative experimental study of popular unsupervised techniques on\nfive datasets.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 14:01:08 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 15:51:54 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Papagiannopoulou", "Eirini", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "1905.05305", "submitter": "Behzad Tabibian", "authors": "Behzad Tabibian, Vicen\\c{c} G\\'omez, Abir De, Bernhard Sch\\\"olkopf,\n  Manuel Gomez Rodriguez", "title": "Consequential Ranking Algorithms and Long-term Welfare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking models are typically designed to provide rankings that optimize some\nmeasure of immediate utility to the users. As a result, they have been unable\nto anticipate an increasing number of undesirable long-term consequences of\ntheir proposed rankings, from fueling the spread of misinformation and\nincreasing polarization to degrading social discourse. Can we design ranking\nmodels that understand the consequences of their proposed rankings and, more\nimportantly, are able to avoid the undesirable ones? In this paper, we first\nintroduce a joint representation of rankings and user dynamics using Markov\ndecision processes. Then, we show that this representation greatly simplifies\nthe construction of consequential ranking models that trade off the immediate\nutility and the long-term welfare. In particular, we can obtain optimal\nconsequential rankings just by applying weighted sampling on the rankings\nprovided by models that maximize measures of immediate utility. However, in\npractice, such a strategy may be inefficient and impractical, specially in high\ndimensional scenarios. To overcome this, we introduce an efficient\ngradient-based algorithm to learn parameterized consequential ranking models\nthat effectively approximate optimal ones. We showcase our methodology using\nsynthetic and real data gathered from Reddit and show that ranking models\nderived using our methodology provide ranks that may mitigate the spread of\nmisinformation and improve the civility of online discussions.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 22:27:59 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Tabibian", "Behzad", ""], ["G\u00f3mez", "Vicen\u00e7", ""], ["De", "Abir", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Rodriguez", "Manuel Gomez", ""]]}, {"id": "1905.05412", "submitter": "Chen Qu", "authors": "Chen Qu, Liu Yang, Minghui Qiu, W. Bruce Croft, Yongfeng Zhang and\n  Mohit Iyyer", "title": "BERT with History Answer Embedding for Conversational Question Answering", "comments": "Accepted to SIGIR 2019 as a short paper", "journal-ref": null, "doi": "10.1145/3331184.3331341", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is an emerging topic in the information retrieval\ncommunity. One of the major challenges to multi-turn conversational search is\nto model the conversation history to answer the current question. Existing\nmethods either prepend history turns to the current question or use complicated\nattention mechanisms to model the history. We propose a conceptually simple yet\nhighly effective approach referred to as history answer embedding. It enables\nseamless integration of conversation history into a conversational question\nanswering (ConvQA) model built on BERT (Bidirectional Encoder Representations\nfrom Transformers). We first explain our view that ConvQA is a simplified but\nconcrete setting of conversational search, and then we provide a general\nframework to solve ConvQA. We further demonstrate the effectiveness of our\napproach under this framework. Finally, we analyze the impact of different\nnumbers of history turns under different settings to provide new insights into\nconversation history modeling in ConvQA.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:40:38 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 14:15:47 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Qu", "Chen", ""], ["Yang", "Liu", ""], ["Qiu", "Minghui", ""], ["Croft", "W. Bruce", ""], ["Zhang", "Yongfeng", ""], ["Iyyer", "Mohit", ""]]}, {"id": "1905.05598", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento, Vera Costa", "title": "Confirmatory Factor Analysis -- A Case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confirmatory Factor Analysis (CFA) is a particular form of factor analysis,\nmost commonly used in social research. In confirmatory factor analysis, the\nresearcher first develops a hypothesis about what factors they believe are\nunderlying the used measures and may impose constraints on the model based on\nthese a priori hypotheses. For example, if two factors are accounting for the\ncovariance in the measures, and these factors are unrelated to one another, we\ncan create a model where the correlation between factor X and factor Y is set\nto zero. Measures could then be obtained to assess how well the fitted model\ncaptured the covariance between all the items or measures in the model. Thus,\nif the results of statistical tests of the model fit indicate a poor fit, the\nmodel will be rejected. If the fit is weak, it may be due to a variety of\nreasons. We propose to introduce state of the art techniques to do CFA in R\nlanguage. Then, we propose to do some examples of CFA with R and some datasets,\nrevealing several scenarios where CFA is relevant.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 19:27:22 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Costa", "Vera", ""]]}, {"id": "1905.05818", "submitter": "Sean MacAvaney", "authors": "Sean MacAvaney, Sajad Sotudeh, Arman Cohan, Nazli Goharian, Ish\n  Talati, Ross W. Filice", "title": "Ontology-Aware Clinical Abstractive Summarization", "comments": "4 pages; SIGIR 2019 Short Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating accurate summaries from clinical reports could save\na clinician's time, improve summary coverage, and reduce errors. We propose a\nsequence-to-sequence abstractive summarization model augmented with\ndomain-specific ontological information to enhance content selection and\nsummary generation. We apply our method to a dataset of radiology reports and\nshow that it significantly outperforms the current state-of-the-art on this\ntask in terms of rouge scores. Extensive human evaluation conducted by a\nradiologist further indicates that this approach yields summaries that are less\nlikely to omit important details, without sacrificing readability or accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:09:18 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["MacAvaney", "Sean", ""], ["Sotudeh", "Sajad", ""], ["Cohan", "Arman", ""], ["Goharian", "Nazli", ""], ["Talati", "Ish", ""], ["Filice", "Ross W.", ""]]}, {"id": "1905.05827", "submitter": "Yejin Kim", "authors": "Yejin Kim, Xiaoqian Jiang, Luyao Chen, Xiaojin Li, Licong Cui", "title": "Discriminative Sleep Patterns of Alzheimer's Disease via Tensor\n  Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": "PMC7153114", "categories": "q-bio.NC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep change is commonly reported in Alzheimer's disease (AD) patients and\ntheir brain wave studies show decrease in dreaming and non-dreaming stages.\nAlthough sleep disturbance is generally considered as a consequence of AD, it\nmight also be a risk factor of AD as new biological evidence shows. Leveraging\nNational Sleep Research Resource (NSRR), we built a unique cohort of 83 cases\nand 331 controls with clinical variables and EEG signals. Supervised tensor\nfactorization method was applied for this temporal dataset to extract\ndiscriminative sleep patterns. Among the 30 patterns extracted, we identified 5\nsignificant patterns (4 patterns for AD likely and 1 pattern for normal ones)\nand their visual patterns provide interesting linkage to sleep with repeated\nwakefulness, insomnia, epileptic seizure, and etc. This study is preliminary\nbut findings are interesting, which is a first step to provide quantifiable\nevidences to measure sleep as a risk factor of AD.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:21:58 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kim", "Yejin", ""], ["Jiang", "Xiaoqian", ""], ["Chen", "Luyao", ""], ["Li", "Xiaojin", ""], ["Cui", "Licong", ""]]}, {"id": "1905.05830", "submitter": "Yejin Kim", "authors": "Xiaoqian Jiang, Samden Lhatoo, Guo-Qiang Zhang, Luyao Chen, Yejin Kim", "title": "Combining Representation Learning with Tensor Factorization for Risk\n  Factor Analysis - an application to Epilepsy and Alzheimer's disease", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbi.2020.103462", "report-no": null, "categories": "stat.AP cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing studies consider Alzheimer's disease (AD) a comorbidity of epilepsy,\nbut also recognize epilepsy to occur more frequently in patients with AD than\nthose without. The goal of this paper is to understand the relationship between\nepilepsy and AD by studying causal relations among subgroups of epilepsy\npatients. We develop an approach combining representation learning with tensor\nfactorization to provide an in-depth analysis of the risk factors among\nepilepsy patients for AD. An epilepsy-AD cohort of ~600,000 patients were\nextracted from Cerner Health Facts data (50M patients). Our experimental\nresults not only suggested a causal relationship between epilepsy and later\nonset of AD ( p = 1.92e-51), but also identified five epilepsy subgroups with\ndistinct phenotypic patterns leading to AD. While such findings are\npreliminary, the proposed method combining representation learning with tensor\nfactorization seems to be an effective approach for risk factor analysis.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:28:06 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Jiang", "Xiaoqian", ""], ["Lhatoo", "Samden", ""], ["Zhang", "Guo-Qiang", ""], ["Chen", "Luyao", ""], ["Kim", "Yejin", ""]]}, {"id": "1905.05861", "submitter": "Yejin Kim", "authors": "Rui Zhang, Luca Giancardo, Danilo A. Pena, Yejin Kim, Hanghang Tong,\n  Xiaoqian Jiang (for the Alzheimer's Disease Neuroimaging Initiative)", "title": "From Brain Imaging to Graph Analysis: a study on ADNI's patient cohort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we studied the association between the change of structural\nbrain volumes to the potential development of Alzheimer's disease (AD). Using a\nsimple abstraction technique, we converted regional cortical and subcortical\nvolume differences over two time points for each study subject into a graph. We\nthen obtained substructures of interest using a graph decomposition algorithm\nin order to extract pivotal nodes via multi-view feature selection. Intensive\nexperiments using robust classification frameworks were conducted to evaluate\nthe performance of using the brain substructures obtained under different\nthresholds. The results indicated that compact substructures acquired by\nexamining the differences between patient groups were sufficient to\ndiscriminate between AD and healthy controls with an area under the receiver\noperating curve of 0.72.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 21:58:34 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Zhang", "Rui", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Giancardo", "Luca", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Pena", "Danilo A.", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Kim", "Yejin", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Tong", "Hanghang", "", "for the Alzheimer's Disease Neuroimaging Initiative"], ["Jiang", "Xiaoqian", "", "for the Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1905.05910", "submitter": "Peng Xu", "authors": "Peng Xu, Xiaofei Ma, Ramesh Nallapati, Bing Xiang", "title": "Passage Ranking with Weak Supervision", "comments": "6 pages, 1 figure", "journal-ref": "ICLR 2019 LLD workshop", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a \\textit{weak supervision} framework for neural\nranking tasks based on the data programming paradigm \\citep{Ratner2016}, which\nenables us to leverage multiple weak supervision signals from different\nsources. Empirically, we consider two sources of weak supervision signals,\nunsupervised ranking functions and semantic feature similarities. We train a\nBERT-based passage-ranking model (which achieves new state-of-the-art\nperformances on two benchmark datasets with full supervision) in our weak\nsupervision framework. Without using ground-truth training labels, BERT-PR\nmodels outperform BM25 baseline by a large margin on all three datasets and\neven beat the previous state-of-the-art results with full supervision on two of\nthe datasets.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:47:57 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 04:04:17 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Xu", "Peng", ""], ["Ma", "Xiaofei", ""], ["Nallapati", "Ramesh", ""], ["Xiang", "Bing", ""]]}, {"id": "1905.05955", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Stephan Guennemann", "title": "GhostLink: Latent Network Inference for Influence-aware Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social influence plays a vital role in shaping a user's behavior in online\ncommunities dealing with items of fine taste like movies, food, and beer. For\nonline recommendation, this implies that users' preferences and ratings are\ninfluenced due to other individuals. Given only time-stamped reviews of users,\ncan we find out who-influences-whom, and characteristics of the underlying\ninfluence network? Can we use this network to improve recommendation?\n  While prior works in social-aware recommendation have leveraged social\ninteraction by considering the observed social network of users, many\ncommunities like Amazon, Beeradvocate, and Ratebeer do not have explicit\nuser-user links. Therefore, we propose GhostLink, an unsupervised probabilistic\ngraphical model, to automatically learn the latent influence network underlying\na review community -- given only the temporal traces (timestamps) of users'\nposts and their content. Based on extensive experiments with four real-world\ndatasets with 13 million reviews, we show that GhostLink improves item\nrecommendation by around 23% over state-of-the-art methods that do not consider\nthis influence. As additional use-cases, we show that GhostLink can be used to\ndifferentiate between users' latent preferences and influenced ones, as well as\nto detect influential users based on the learned influence graph.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 05:59:25 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Guennemann", "Stephan", ""]]}, {"id": "1905.06109", "submitter": "Xiaosen Wang", "authors": "Kun He and Wu Wang and Xiaosen Wang and John E. Hopcroft", "title": "A New Anchor Word Selection Method for the Separable Topic Discovery", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separable Non-negative Matrix Factorization (SNMF) is an important method for\ntopic modeling, where \"separable\" assumes every topic contains at least one\nanchor word, defined as a word that has non-zero probability only on that\ntopic. SNMF focuses on the word co-occurrence patterns to reveal topics by two\nsteps: anchor word selection and topic recovery. The quality of the anchor\nwords strongly influences the quality of the extracted topics. Existing anchor\nword selection algorithm is to greedily find an approximate convex hull in a\nhigh-dimensional word co-occurrence space. In this work, we propose a new\nmethod for the anchor word selection by associating the word co-occurrence\nprobability with the words similarity and assuming that the most different\nwords on semantic are potential candidates for the anchor words. Therefore, if\nthe similarity of a word-pair is very low, then the two words are very likely\nto be the anchor words. According to the statistical information of text\ncorpora, we can get the similarity of all word-pairs. We build the word\nsimilarity graph where the nodes correspond to words and weights on edges stand\nfor the word-pair similarity. Following this way, we design a greedy method to\nfind a minimum edge-weight anchor clique of a given size in the graph for the\nanchor word selection. Extensive experiments on real-world corpus demonstrate\nthe effectiveness of the proposed anchor word selection method that outperforms\nthe common convex hull-based methods on the revealed topic quality. Meanwhile,\nour method is much faster than typical SNMF based method.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:16:10 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["He", "Kun", ""], ["Wang", "Wu", ""], ["Wang", "Xiaosen", ""], ["Hopcroft", "John E.", ""]]}, {"id": "1905.06112", "submitter": "Vuong M. Ngo", "authors": "T.H.H Duong, T.D. Vu, V.M. Ngo", "title": "Detecting Vietnamese Opinion Spam", "comments": "6 pages, in Vietnamese", "journal-ref": "ICTFIT 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Vietnamese Natural Language Processing has been researched by\nexperts in academic and business. However, the existing papers have been\nfocused only on information classification or extraction from documents.\nNowadays, with quickly development of the e-commerce websites, forums and\nsocial networks, the products, people, organizations or wonders are targeted of\ncomments or reviews of the network communities. Many people often use that\nreviews to make their decision on something. Whereas, there are many people or\norganizations use the reviews to mislead readers. Therefore, it is so necessary\nto detect those bad behaviors in reviews. In this paper, we research this\nproblem and propose an appropriate method for detecting Vietnamese reviews\nbeing spam or non-spam. The accuracy of our method is up to 90%.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 10:41:16 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Duong", "T. H. H", ""], ["Vu", "T. D.", ""], ["Ngo", "V. M.", ""]]}, {"id": "1905.06114", "submitter": "Vuong M. Ngo", "authors": "Ngo Minh Vuong", "title": "Semantic Search using Spreading Activation based on Ontology", "comments": "21 pages, in Vietnamese", "journal-ref": "Science Journal, special issue E-Learning Architecture and\n  Technology (ELATE), Vol.53, 2013, pp. 136-156", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the text document retrieval systems have many challenges in\nexploring the semantics of queries and documents. Each query implies\ninformation which does not appear in the query but the documents related with\nthe information are also expected by user. The disadvantage of the previous\nspreading activation algorithms could be many irrelevant concepts added to the\nquery. In this paper, a proposed novel algorithm is only activate and add to\nthe query named entities which are related with original entities in the query\nand explicit relations in the query.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 10:28:57 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Vuong", "Ngo Minh", ""]]}, {"id": "1905.06115", "submitter": "Jiangning Chen", "authors": "Jiangning Chen, Zhibo Dai, Juntao Duan, Heinrich Matzinger, Ionel\n  Popescu", "title": "Naive Bayes with Correlation Factor for Text Classification Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naive Bayes estimator is widely used in text classification problems.\nHowever, it doesn't perform well with small-size training dataset. We propose a\nnew method based on Naive Bayes estimator to solve this problem. A correlation\nfactor is introduced to incorporate the correlation among different classes.\nExperimental results show that our estimator achieves a better accuracy\ncompared with traditional Naive Bayes in real world data.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 20:27:00 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Chen", "Jiangning", ""], ["Dai", "Zhibo", ""], ["Duan", "Juntao", ""], ["Matzinger", "Heinrich", ""], ["Popescu", "Ionel", ""]]}, {"id": "1905.06134", "submitter": "Nadia Fawaz", "authors": "Nadia Fawaz", "title": "Recommending Dream Jobs in a Biased Real World", "comments": "Accepted and presented at Grace Hopper Conference, GHC 2017", "journal-ref": "Grace Hopper Conference, GHC 2017", "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models learn what we teach them to learn. Machine learning\nis at the heart of recommender systems. If a machine learning model is trained\non biased data, the resulting recommender system may reflect the biases in its\nrecommendations. Biases arise at different stages in a recommender system, from\nexisting societal biases in the data such as the professional gender gap, to\nbiases introduced by the data collection or modeling processes. These biases\nimpact the performance of various components of recommender systems, from\noffline training, to evaluation and online serving of recommendations in\nproduction systems. Specific techniques can help reduce bias at each stage of a\nrecommender system. Reducing bias in our recommender systems is crucial to\nsuccessfully recommending dream jobs to hundreds of millions members worldwide,\nwhile being true to LinkedIn's vision: \"To create economic opportunity for\nevery member of the global workforce\".\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 19:26:01 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Fawaz", "Nadia", ""]]}, {"id": "1905.06269", "submitter": "Weiqing Min", "authors": "Weiqing Min, Shuqiang Jiang, Ramesh Jain", "title": "Food Recommendation: Framework, Existing Solutions and Challenges", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing proportion of the global population is becoming overweight or\nobese, leading to various diseases (e.g., diabetes, ischemic heart disease and\neven cancer) due to unhealthy eating patterns, such as increased intake of food\nwith high energy and high fat. Food recommendation is of paramount importance\nto alleviate this problem. Unfortunately, modern multimedia research has\nenhanced the performance and experience of multimedia recommendation in many\nfields such as movies and POI, yet largely lags in the food domain. This\narticle proposes a unified framework for food recommendation, and identifies\nmain issues affecting food recommendation including building the personal\nmodel, analyzing unique food characteristics, incorporating various context and\ndomain knowledge. We then review existing solutions for these issues, and\nfinally elaborate research challenges and future directions in this field. To\nour knowledge, this is the first survey that targets the study of food\nrecommendation in the multimedia field and offers a collection of research\nstudies and technologies to benefit researchers in this field.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:12:05 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:45:01 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Min", "Weiqing", ""], ["Jiang", "Shuqiang", ""], ["Jain", "Ramesh", ""]]}, {"id": "1905.06336", "submitter": "Zhang Junlin", "authors": "Junlin Zhang, Tongwen Huang, Zhiqi Zhang", "title": "FAT-DeepFFM: Field Attentive Deep Field-aware Factorization Machine", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Click through rate (CTR) estimation is a fundamental task in personalized\nadvertising and recommender systems. Recent years have witnessed the success of\nboth the deep learning based model and attention mechanism in various tasks in\ncomputer vision (CV) and natural language processing (NLP). How to combine the\nattention mechanism with deep CTR model is a promising direction because it may\nensemble the advantages of both sides. Although some CTR model such as\nAttentional Factorization Machine (AFM) has been proposed to model the weight\nof second order interaction features, we posit the evaluation of feature\nimportance before explicit feature interaction procedure is also important for\nCTR prediction tasks because the model can learn to selectively highlight the\ninformative features and suppress less useful ones if the task has many input\nfeatures. In this paper, we propose a new neural CTR model named Field\nAttentive Deep Field-aware Factorization Machine (FAT-DeepFFM) by combining the\nDeep Field-aware Factorization Machine (DeepFFM) with Compose-Excitation\nnetwork (CENet) field attention mechanism which is proposed by us as an\nenhanced version of Squeeze-Excitation network (SENet) to highlight the feature\nimportance. We conduct extensive experiments on two real-world datasets and the\nexperiment results show that FAT-DeepFFM achieves the best performance and\nobtains different improvements over the state-of-the-art methods. We also\ncompare two kinds of attention mechanisms (attention before explicit feature\ninteraction vs. attention after explicit feature interaction) and demonstrate\nthat the former one outperforms the latter one significantly.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 12:35:37 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Zhang", "Junlin", ""], ["Huang", "Tongwen", ""], ["Zhang", "Zhiqi", ""]]}, {"id": "1905.06365", "submitter": "Jiawei Zhang", "authors": "Bowen Dong and Jiawei Zhang and Chenwei Zhang and Yang Yang and Philip\n  S. Yu", "title": "Missing Movie Synergistic Completion across Multiple Isomeric Online\n  Movie Knowledge Libraries", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online knowledge libraries refer to the online data warehouses that\nsystematically organize and categorize the knowledge-based information about\ndifferent kinds of concepts and entities. In the era of big data, the setup of\nonline knowledge libraries is an extremely challenging and laborious task, in\nterms of efforts, time and expense required in the completion of knowledge\nentities. Especially nowadays, a large number of new knowledge entities, like\nmovies, are keeping on being produced and coming out at a continuously\naccelerating speed, which renders the knowledge library setup and completion\nproblem more difficult to resolve manually. In this paper, we will take the\nonline movie knowledge libraries as an example, and study the \"Multiple aligned\nISomeric Online Knowledge LIbraries Completion problem\" (Miso-Klic) problem\nacross multiple online knowledge libraries. Miso-Klic aims at identifying the\nmissing entities for multiple knowledge libraries synergistically and ranking\nthem for editing based on certain ranking criteria. To solve the problem, a\nthorough investigation of two isomeric online knowledge libraries, Douban and\nIMDB, have been carried out in this paper. Based on analyses results, a novel\ndeep online knowledge library completion framework \"Integrated Deep alignEd\nAuto-encoder\" (IDEA) is introduced to solve the problem. By projecting the\nentities from multiple isomeric knowledge libraries to a shared feature space,\nIDEA solves the Miso-Klic problem via three steps: (1) entity feature space\nunification via embedding, (2) knowledge library fusion based missing entity\nidentification, and (3) missing entity ranking. Extensive experiments done on\nthe real-world online knowledge library dataset have demonstrated the\neffectiveness of IDEA in addressing the problem.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 18:15:44 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 17:21:16 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Dong", "Bowen", ""], ["Zhang", "Jiawei", ""], ["Zhang", "Chenwei", ""], ["Yang", "Yang", ""], ["Yu", "Philip S.", ""]]}, {"id": "1905.06452", "submitter": "Andrew Stanton", "authors": "Andrew Stanton, Akhila Ananthram, Congzhe Su, Liangjie Hong", "title": "Revenue, Relevance, Arbitrage and More: Joint Optimization Framework for\n  Search Experiences in Two-Sided Marketplaces", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sided marketplaces such as eBay, Etsy and Taobao have two distinct groups\nof customers: buyers who use the platform to seek the most relevant and\ninteresting item to purchase and sellers who view the same platform as a tool\nto reach out to their audience and grow their business. Additionally, platforms\nhave their own objectives ranging from growing both buyer and seller user bases\nto revenue maximization. It is not difficult to see that it would be\nchallenging to obtain a globally favorable outcome for all parties. Taking the\nsearch experience as an example, any interventions are likely to impact either\nbuyers or sellers unfairly to course correct for a greater perceived need. In\nthis paper, we address how a company-aligned search experience can be provided\nwith competing business metrics that E-commerce companies typically tackle. As\nfar as we know, this is a pioneering work to consider multiple different\naspects of business indicators in two-sided marketplaces to optimize a search\nexperience. We demonstrate that many problems are difficult or impossible to\ndecompose down to credit assigned scores on individual documents, rendering\ntraditional methods inadequate. Instead, we express market-level metrics as\nconstraints and discuss to what degree multiple potentially conflicting metrics\ncan be tuned to business needs. We further explore the use of policy learners\nin the form of Evolutionary Strategies to jointly optimize both group-level and\nmarket-level metrics simultaneously, side-stepping traditional cascading\nmethods and manual interventions. We empirically evaluate the effectiveness of\nthe proposed method on Etsy data and demonstrate its potential with insights.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 22:02:44 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Stanton", "Andrew", ""], ["Ananthram", "Akhila", ""], ["Su", "Congzhe", ""], ["Hong", "Liangjie", ""]]}, {"id": "1905.06482", "submitter": "Yufei Feng", "authors": "Yufei Feng, Fuyu Lv, Weichen Shen and Menghan Wang and Fei Sun and Yu\n  Zhu and Keping Yang", "title": "Deep Session Interest Network for Click-Through Rate Prediction", "comments": "Has been accepted by IJCAI-2019, 7 pages, 3 pages, 3 pics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-Through Rate (CTR) prediction plays an important role in many\nindustrial applications, such as online advertising and recommender systems.\nHow to capture users' dynamic and evolving interests from their behavior\nsequences remains a continuous research topic in the CTR prediction. However,\nmost existing studies overlook the intrinsic structure of the sequences: the\nsequences are composed of sessions, where sessions are user behaviors separated\nby their occurring time. We observe that user behaviors are highly homogeneous\nin each session, and heterogeneous cross sessions. Based on this observation,\nwe propose a novel CTR model named Deep Session Interest Network (DSIN) that\nleverages users' multiple historical sessions in their behavior sequences. We\nfirst use self-attention mechanism with bias encoding to extract users'\ninterests in each session. Then we apply Bi-LSTM to model how users' interests\nevolve and interact among sessions. Finally, we employ the local activation\nunit to adaptively learn the influences of various session interests on the\ntarget item. Experiments are conducted on both advertising and production\nrecommender datasets and DSIN outperforms other state-of-the-art models on both\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 00:31:41 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Feng", "Yufei", ""], ["Lv", "Fuyu", ""], ["Shen", "Weichen", ""], ["Wang", "Menghan", ""], ["Sun", "Fei", ""], ["Zhu", "Yu", ""], ["Yang", "Keping", ""]]}, {"id": "1905.06589", "submitter": "Yong Liu Stephen", "authors": "Qiong Wu, Yong Liu, Chunyan Miao, Yin Zhao, Lu Guan, Haihong Tang", "title": "Recent Advances in Diversified Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of recommender systems, accuracy is no longer the\nonly golden criterion for evaluating whether the recommendation results are\nsatisfying or not. In recent years, diversity has gained tremendous attention\nin recommender systems research, which has been recognized to be an important\nfactor for improving user satisfaction. On the one hand, diversified\nrecommendation helps increase the chance of answering ephemeral user needs. On\nthe other hand, diversifying recommendation results can help the business\nimprove product visibility and explore potential user interests. In this paper,\nwe are going to review the recent advances in diversified recommendation.\nSpecifically, we first review the various definitions of diversity and generate\na taxonomy to shed light on how diversity have been modeled or measured in\nrecommender systems. After that, we summarize the major optimization approaches\nto diversified recommendation from a taxonomic view. Last but not the least, we\nproject into the future and point out trending research directions on this\ntopic.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 08:20:30 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wu", "Qiong", ""], ["Liu", "Yong", ""], ["Miao", "Chunyan", ""], ["Zhao", "Yin", ""], ["Guan", "Lu", ""], ["Tang", "Haihong", ""]]}, {"id": "1905.06863", "submitter": "Farzad Eskandanian", "authors": "Farzad Eskandanian, Bamshad Mobasher", "title": "Modeling the Dynamics of User Preferences for Sequence-Aware\n  Recommendation Using Hidden Markov Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.00272", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In a variety of online settings involving interaction with end-users it is\ncritical for the systems to adapt to changes in user preferences. User\npreferences on items tend to change over time due to a variety of factors such\nas change in context, the task being performed, or other short-term or\nlong-term external factors. Recommender systems need to be able to capture\nthese dynamics in user preferences in order to remain tuned to the most current\ninterests of users. In this work we present a recommendation framework which\ntakes into account the dynamics of user preferences. We propose an approach\nbased on Hidden Markov Models (HMM) to identify change-points in the sequence\nof user interactions which reflect significant changes in preference according\nto the sequential behavior of all the users in the data. The proposed framework\nleverages the identified change points to generate recommendations using a\nsequence-aware non-negative matrix factorization model. We empirically\ndemonstrate the effectiveness of the HMM-based change detection method as\ncompared to standard baseline methods. Additionally, we evaluate the\nperformance of the proposed recommendation method and show that it compares\nfavorably to state-of-the-art sequence-aware recommendation models.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 19:56:57 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Eskandanian", "Farzad", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1905.06874", "submitter": "Qiwei Chen", "authors": "Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, Wenwu Ou", "title": "Behavior Sequence Transformer for E-commerce Recommendation in Alibaba", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based methods have been widely used in industrial\nrecommendation systems (RSs). Previous works adopt an Embedding&MLP paradigm:\nraw features are embedded into low-dimensional vectors, which are then fed on\nto MLP for final recommendations. However, most of these works just concatenate\ndifferent features, ignoring the sequential nature of users' behaviors. In this\npaper, we propose to use the powerful Transformer model to capture the\nsequential signals underlying users' behavior sequences for recommendation in\nAlibaba. Experimental results demonstrate the superiority of the proposed\nmodel, which is then deployed online at Taobao and obtain significant\nimprovements in online Click-Through-Rate (CTR) comparing to two baselines.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:51:59 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Chen", "Qiwei", ""], ["Zhao", "Huan", ""], ["Li", "Wei", ""], ["Huang", "Pipei", ""], ["Ou", "Wenwu", ""]]}, {"id": "1905.07043", "submitter": "Omer Ben-Porat", "authors": "Gal Bahar, Omer Ben-Porat, Kevin Leyton-Brown, Moshe Tennenholtz", "title": "Fiduciary Bandits", "comments": "Published in The Thirty-seventh International Conference on Machine\n  Learning (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems often face exploration-exploitation tradeoffs: the\nsystem can only learn about the desirability of new options by recommending\nthem to some user. Such systems can thus be modeled as multi-armed bandit\nsettings; however, users are self-interested and cannot be made to follow\nrecommendations. We ask whether exploration can nevertheless be performed in a\nway that scrupulously respects agents' interests---i.e., by a system that acts\nas a fiduciary. More formally, we introduce a model in which a recommendation\nsystem faces an exploration-exploitation tradeoff under the constraint that it\ncan never recommend any action that it knows yields lower reward in expectation\nthan an agent would achieve if it acted alone. Our main contribution is a\npositive result: an asymptotically optimal, incentive compatible, and ex-ante\nindividually rational recommendation algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 21:38:39 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 14:20:45 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 12:10:29 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Bahar", "Gal", ""], ["Ben-Porat", "Omer", ""], ["Leyton-Brown", "Kevin", ""], ["Tennenholtz", "Moshe", ""]]}, {"id": "1905.07075", "submitter": "Karan Sikka", "authors": "Karan Sikka and Lucas Van Bramer and Ajay Divakaran", "title": "Deep Unified Multimodal Embeddings for Understanding both Content and\n  Users in Social Media Networks", "comments": "Preprint submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an explosion of multimodal content generated on social media\nnetworks in the last few years, which has necessitated a deeper understanding\nof social media content and user behavior. We present a novel\ncontent-independent content-user-reaction model for social multimedia content\nanalysis. Compared to prior works that generally tackle semantic content\nunderstanding and user behavior modeling in isolation, we propose a generalized\nsolution to these problems within a unified framework. We embed users, images\nand text drawn from open social media in a common multimodal geometric space,\nusing a novel loss function designed to cope with distant and disparate\nmodalities, and thereby enable seamless three-way retrieval. Our model not only\noutperforms unimodal embedding based methods on cross-modal retrieval tasks but\nalso shows improvements stemming from jointly solving the two tasks on Twitter\ndata. We also show that the user embeddings learned within our joint multimodal\nembedding model are better at predicting user interests compared to those\nlearned with unimodal content on Instagram data. Our framework thus goes beyond\nthe prior practice of using explicit leader-follower link information to\nestablish affiliations by extracting implicit content-centric affiliations from\nisolated users. We provide qualitative results to show that the user clusters\nemerging from learned embeddings have consistent semantics and the ability of\nour model to discover fine-grained semantics from noisy and unstructured data.\nOur work reveals that social multimodal content is inherently multimodal and\npossesses a consistent structure because in social networks meaning is created\nthrough interactions between users and content.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 01:16:15 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 14:48:32 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 03:17:53 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Sikka", "Karan", ""], ["Van Bramer", "Lucas", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1905.07089", "submitter": "Yu Gong", "authors": "Yu Gong, Yu Zhu, Lu Duan, Qingwen Liu, Ziyu Guan, Fei Sun, Wenwu Ou,\n  Kenny Q. Zhu", "title": "Exact-K Recommendation via Maximal Clique Optimization", "comments": "SIGKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper targets to a novel but practical recommendation problem named\nexact-K recommendation. It is different from traditional top-K recommendation,\nas it focuses more on (constrained) combinatorial optimization which will\noptimize to recommend a whole set of K items called card, rather than ranking\noptimization which assumes that \"better\" items should be put into top\npositions. Thus we take the first step to give a formal problem definition, and\ninnovatively reduce it to Maximum Clique Optimization based on graph. To tackle\nthis specific combinatorial optimization problem which is NP-hard, we propose\nGraph Attention Networks (GAttN) with a Multi-head Self-attention encoder and a\ndecoder with attention mechanism. It can end-to-end learn the joint\ndistribution of the K items and generate an optimal card rather than rank\nindividual items by prediction scores. Then we propose Reinforcement Learning\nfrom Demonstrations (RLfD) which combines the advantages in behavior cloning\nand reinforcement learning, making it sufficient- and-efficient to train the\nmodel. Extensive experiments on three datasets demonstrate the effectiveness of\nour proposed GAttN with RLfD method, it outperforms several strong baselines\nwith a relative improvement of 7.7% and 4.7% on average in Precision and Hit\nRatio respectively, and achieves state-of-the-art (SOTA) performance for the\nexact-K recommendation problem.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 02:02:59 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Gong", "Yu", ""], ["Zhu", "Yu", ""], ["Duan", "Lu", ""], ["Liu", "Qingwen", ""], ["Guan", "Ziyu", ""], ["Sun", "Fei", ""], ["Ou", "Wenwu", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "1905.07356", "submitter": "Matthijs Westera", "authors": "Matthijs Westera and Gemma Boleda", "title": "Don't Blame Distributional Semantics if it can't do Entailment", "comments": "To appear in Proceedings of the 13th International Conference on\n  Computational Semantics (IWCS 2019), Gothenburg, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional semantics has had enormous empirical success in Computational\nLinguistics and Cognitive Science in modeling various semantic phenomena, such\nas semantic similarity, and distributional models are widely used in\nstate-of-the-art Natural Language Processing systems. However, the theoretical\nstatus of distributional semantics within a broader theory of language and\ncognition is still unclear: What does distributional semantics model? Can it\nbe, on its own, a fully adequate model of the meanings of linguistic\nexpressions? The standard answer is that distributional semantics is not fully\nadequate in this regard, because it falls short on some of the central aspects\nof formal semantic approaches: truth conditions, entailment, reference, and\ncertain aspects of compositionality. We argue that this standard answer rests\non a misconception: These aspects do not belong in a theory of expression\nmeaning, they are instead aspects of speaker meaning, i.e., communicative\nintentions in a particular context. In a slogan: words do not refer, speakers\ndo. Clearing this up enables us to argue that distributional semantics on its\nown is an adequate model of expression meaning. Our proposal sheds light on the\nrole of distributional semantics in a broader theory of language and cognition,\nits relationship to formal semantics, and its place in computational models.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:26:05 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Westera", "Matthijs", ""], ["Boleda", "Gemma", ""]]}, {"id": "1905.07370", "submitter": "Farhan Khawar", "authors": "Farhan Khawar and Nevin L. Zhang", "title": "Cleaned Similarity for Better Memory-Based Recommenders", "comments": "To appear in SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331310", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-based collaborative filtering methods like user or item k-nearest\nneighbors (kNN) are a simple yet effective solution to the recommendation\nproblem. The backbone of these methods is the estimation of the empirical\nsimilarity between users/items. In this paper, we analyze the spectral\nproperties of the Pearson and the cosine similarity estimators, and we use\ntools from random matrix theory to argue that they suffer from noise and\neigenvalues spreading. We argue that, unlike the Pearson correlation, the\ncosine similarity naturally possesses the desirable property of eigenvalue\nshrinkage for large eigenvalues. However, due to its zero-mean assumption, it\noverestimates the largest eigenvalues. We quantify this overestimation and\npresent a simple re-scaling and noise cleaning scheme. This results in better\nperformance of the memory-based methods compared to their vanilla counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:44:05 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1905.07471", "submitter": "Jacob Beckerman", "authors": "Jacob Beckerman and Theodore Christakis", "title": "Learning Open Information Extraction of Implicit Relations from Reading\n  Comprehension Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The relationship between two entities in a sentence is often implied by word\norder and common sense, rather than an explicit predicate. For example, it is\nevident that \"Fed chair Powell indicates rate hike\" implies (Powell, is a, Fed\nchair) and (Powell, works for, Fed). These tuples are just as significant as\nthe explicit-predicate tuple (Powell, indicates, rate hike), but have much\nlower recall under traditional Open Information Extraction (OpenIE) systems.\nImplicit tuples are our term for this type of extraction where the relation is\nnot present in the input sentence. There is very little OpenIE training data\navailable relative to other NLP tasks and none focused on implicit relations.\nWe develop an open source, parse-based tool for converting large reading\ncomprehension datasets to OpenIE datasets and release a dataset 35x larger than\npreviously available by sentence count. A baseline neural model trained on this\ndata outperforms previous methods on the implicit extraction task.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:02:35 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Beckerman", "Jacob", ""], ["Christakis", "Theodore", ""]]}, {"id": "1905.07508", "submitter": "Piper Armstrong", "authors": "Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Emily\n  Hales, and Kevin Seppi", "title": "Cross-referencing using Fine-grained Topic Modeling", "comments": "6 figures 1 table 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-referencing, which links passages of text to other related passages,\ncan be a valuable study aid for facilitating comprehension of a text. However,\ncross-referencing requires first, a comprehensive thematic knowledge of the\nentire corpus, and second, a focused search through the corpus specifically to\nfind such useful connections. Due to this, cross-reference resources are\nprohibitively expensive and exist only for the most well-studied texts (e.g.\nreligious texts). We develop a topic-based system for automatically producing\ncandidate cross-references which can be easily verified by human annotators.\nOur system utilizes fine-grained topic modeling with thousands of highly\nnuanced and specific topics to identify verse pairs which are topically\nrelated. We demonstrate that our system can be cost effective compared to\nhaving annotators acquire the expertise necessary to produce cross-reference\nresources unaided.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 00:28:37 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Lund", "Jeffrey", ""], ["Armstrong", "Piper", ""], ["Fearn", "Wilson", ""], ["Cowley", "Stephen", ""], ["Hales", "Emily", ""], ["Seppi", "Kevin", ""]]}, {"id": "1905.07689", "submitter": "Zhiqing Sun", "authors": "Zhiqing Sun, Jian Tang, Pan Du, Zhi-Hong Deng, Jian-Yun Nie", "title": "DivGraphPointer: A Graph Pointer Network for Extracting Diverse\n  Keyphrases", "comments": "Accepted to SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331219", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyphrase extraction from documents is useful to a variety of applications\nsuch as information retrieval and document summarization. This paper presents\nan end-to-end method called DivGraphPointer for extracting a set of diversified\nkeyphrases from a document. DivGraphPointer combines the advantages of\ntraditional graph-based ranking methods and recent neural network-based\napproaches. Specifically, given a document, a word graph is constructed from\nthe document based on word proximity and is encoded with graph convolutional\nnetworks, which effectively capture document-level word salience by modeling\nlong-range dependency between words in the document and aggregating multiple\nappearances of identical words into one node. Furthermore, we propose a\ndiversified point network to generate a set of diverse keyphrases out of the\nword graph in the decoding process. Experimental results on five benchmark data\nsets show that our proposed method significantly outperforms the existing\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 05:17:12 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sun", "Zhiqing", ""], ["Tang", "Jian", ""], ["Du", "Pan", ""], ["Deng", "Zhi-Hong", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "1905.07854", "submitter": "Xiang Wang", "authors": "Xiang Wang, Xiangnan He, Yixin Cao, Meng Liu, Tat-Seng Chua", "title": "KGAT: Knowledge Graph Attention Network for Recommendation", "comments": "KDD 2019 research track", "journal-ref": null, "doi": "10.1145/3292500.3330989", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To provide more accurate, diverse, and explainable recommendation, it is\ncompulsory to go beyond modeling user-item interactions and take side\ninformation into account. Traditional methods like factorization machine (FM)\ncast it as a supervised learning problem, which assumes each interaction as an\nindependent instance with side information encoded. Due to the overlook of the\nrelations among instances or items (e.g., the director of a movie is also an\nactor of another movie), these methods are insufficient to distill the\ncollaborative signal from the collective behaviors of users. In this work, we\ninvestigate the utility of knowledge graph (KG), which breaks down the\nindependent interaction assumption by linking items with their attributes. We\nargue that in such a hybrid structure of KG and user-item graph, high-order\nrelations --- which connect two items with one or multiple linked attributes\n--- are an essential factor for successful recommendation. We propose a new\nmethod named Knowledge Graph Attention Network (KGAT) which explicitly models\nthe high-order connectivities in KG in an end-to-end fashion. It recursively\npropagates the embeddings from a node's neighbors (which can be users, items,\nor attributes) to refine the node's embedding, and employs an attention\nmechanism to discriminate the importance of the neighbors. Our KGAT is\nconceptually advantageous to existing KG-based recommendation methods, which\neither exploit high-order relations by extracting paths or implicitly modeling\nthem with regularization. Empirical results on three public benchmarks show\nthat KGAT significantly outperforms state-of-the-art methods like Neural FM and\nRippleNet. Further studies verify the efficacy of embedding propagation for\nhigh-order relation modeling and the interpretability benefits brought by the\nattention mechanism.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 03:08:11 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 02:49:37 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Wang", "Xiang", ""], ["He", "Xiangnan", ""], ["Cao", "Yixin", ""], ["Liu", "Meng", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1905.07894", "submitter": "Noe Cecillon", "authors": "No\\'e Cecillon (LIA), Vincent Labatut (LIA), Richard Dufour (LIA),\n  Georges Linar\\`es (LIA)", "title": "Abusive Language Detection in Online Conversations by Combining\n  Content-and Graph-based Features", "comments": null, "journal-ref": "ICWSM International Workshop on Modeling and mining\n  Social-Media-driven Complex Networks (Soc2Net) 2019", "doi": "10.3389/fdata.2019.00008", "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, online social networks have allowed worldwide users to meet\nand discuss. As guarantors of these communities, the administrators of these\nplatforms must prevent users from adopting inappropriate behaviors. This\nverification task, mainly done by humans, is more and more difficult due to the\never growing amount of messages to check. Methods have been proposed to\nautomatize this moderation process, mainly by providing approaches based on the\ntextual content of the exchanged messages. Recent work has also shown that\ncharacteristics derived from the structure of conversations, in the form of\nconversational graphs, can help detecting these abusive messages. In this\npaper, we propose to take advantage of both sources of information by proposing\nfusion methods integrating content-and graph-based features. Our experiments on\nraw chat logs show that the content of the messages, but also of their dynamics\nwithin a conversation contain partially complementary information, allowing\nperformance improvements on an abusive message classification task with a final\nF-measure of 93.26%.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 06:15:07 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Cecillon", "No\u00e9", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Dufour", "Richard", "", "LIA"], ["Linar\u00e8s", "Georges", "", "LIA"]]}, {"id": "1905.08031", "submitter": "Farzad Eskandanian", "authors": "Farzad Eskandanian, Nasim Sonboli, Bamshad Mobasher", "title": "Power of the Few: Analyzing the Impact of Influential Users in\n  Collaborative Recommender Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3320435.3320464", "report-no": null, "categories": "cs.SI cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Like other social systems, in collaborative filtering a small number of\n\"influential\" users may have a large impact on the recommendations of other\nusers, thus affecting the overall behavior of the system. Identifying\ninfluential users and studying their impact on other users is an important\nproblem because it provides insight into how small groups can inadvertently or\nintentionally affect the behavior of the system as a whole. Modeling these\ninfluences can also shed light on patterns and relationships that would\notherwise be difficult to discern, hopefully leading to more transparency in\nhow the system generates personalized content. In this work we first formalize\nthe notion of \"influence\" in collaborative filtering using an Influence\nDiscrimination Model. We then empirically identify and characterize influential\nusers and analyze their impact on the system under different underlying\nrecommendation algorithms and across three different recommendation domains:\njob, movie and book recommendations. Insights from these experiments can help\nin designing systems that are not only optimized for accuracy, but are also\ntuned to mitigate the impact of influential users when it might lead to\npotential imbalance or unfairness in the system's outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 19:57:06 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Eskandanian", "Farzad", ""], ["Sonboli", "Nasim", ""], ["Mobasher", "Bamshad", ""]]}, {"id": "1905.08076", "submitter": "Dorien Herremans", "authors": "Dorien herremans, David Martens, Kenneth S\\\"orensen", "title": "Dance Hit Song Prediction", "comments": null, "journal-ref": "Journal of New music Research. 43:302 (2014)", "doi": "10.1080/09298215.2014.881888", "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record companies invest billions of dollars in new talent around the globe\neach year. Gaining insight into what actually makes a hit song would provide\ntremendous benefits for the music industry. In this research we tackle this\nquestion by focussing on the dance hit song classification problem. A database\nof dance hit songs from 1985 until 2013 is built, including basic musical\nfeatures, as well as more advanced features that capture a temporal aspect. A\nnumber of different classifiers are used to build and test dance hit prediction\nmodels. The resulting best model has a good performance when predicting whether\na song is a \"top 10\" dance hit versus a lower listed position.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:01:10 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["herremans", "Dorien", ""], ["Martens", "David", ""], ["S\u00f6rensen", "Kenneth", ""]]}, {"id": "1905.08108", "submitter": "Xiang Wang", "authors": "Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, Tat-Seng Chua", "title": "Neural Graph Collaborative Filtering", "comments": "SIGIR 2019; the latest version of NGCF paper, which is distinct from\n  the version published in ACM Digital Library", "journal-ref": null, "doi": "10.1145/3331184.3331267", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning vector representations (aka. embeddings) of users and items lies at\nthe core of modern recommender systems. Ranging from early matrix factorization\nto recently emerged deep learning based methods, existing efforts typically\nobtain a user's (or an item's) embedding by mapping from pre-existing features\nthat describe the user (or the item), such as ID and attributes. We argue that\nan inherent drawback of such methods is that, the collaborative signal, which\nis latent in user-item interactions, is not encoded in the embedding process.\nAs such, the resultant embeddings may not be sufficient to capture the\ncollaborative filtering effect.\n  In this work, we propose to integrate the user-item interactions -- more\nspecifically the bipartite graph structure -- into the embedding process. We\ndevelop a new recommendation framework Neural Graph Collaborative Filtering\n(NGCF), which exploits the user-item graph structure by propagating embeddings\non it. This leads to the expressive modeling of high-order connectivity in\nuser-item graph, effectively injecting the collaborative signal into the\nembedding process in an explicit manner. We conduct extensive experiments on\nthree public benchmarks, demonstrating significant improvements over several\nstate-of-the-art models like HOP-Rec and Collaborative Memory Network. Further\nanalysis verifies the importance of embedding propagation for learning better\nuser and item representations, justifying the rationality and effectiveness of\nNGCF. Codes are available at\nhttps://github.com/xiangwang1223/neural_graph_collaborative_filtering.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 13:41:16 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 15:24:44 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wang", "Xiang", ""], ["He", "Xiangnan", ""], ["Wang", "Meng", ""], ["Feng", "Fuli", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1905.08359", "submitter": "Moritz Schubotz", "authors": "Andr\\'e Greiner-Petter, Terry Ruas, Moritz Schubotz, Akiko Aizawa,\n  William Grosky, Bela Gipp", "title": "Why Machines Cannot Learn Mathematics, Yet", "comments": "Submitted to 4th Joint Workshop on Bibliometric-enhanced Information\n  Retrieval and Natural Language Processing for Digital Libraries colocated at\n  the 42nd International ACM SIGIR Conference", "journal-ref": "2019 http://ceur-ws.org/Vol-2414/paper14.pdf", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, Machine Learning (ML) is seen as the universal solution to improve\nthe effectiveness of information retrieval (IR) methods. However, while\nmathematics is a precise and accurate science, it is usually expressed by less\naccurate and imprecise descriptions, contributing to the relative dearth of\nmachine learning applications for IR in this domain. Generally, mathematical\ndocuments communicate their knowledge with an ambiguous, context-dependent, and\nnon-formal language. Given recent advances in ML, it seems canonical to apply\nML techniques to represent and retrieve mathematics semantically. In this work,\nwe apply popular text embedding techniques to the arXiv collection of STEM\ndocuments and explore how these are unable to properly understand mathematics\nfrom that corpus. In addition, we also investigate the missing aspects that\nwould allow mathematics to be learned by computers.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 21:54:26 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Greiner-Petter", "Andr\u00e9", ""], ["Ruas", "Terry", ""], ["Schubotz", "Moritz", ""], ["Aizawa", "Akiko", ""], ["Grosky", "William", ""], ["Gipp", "Bela", ""]]}, {"id": "1905.08487", "submitter": "Bang Liu", "authors": "Bang Liu, Weidong Guo, Di Niu, Chaoyue Wang, Shunnan Xu, Jinghong Lin,\n  Kunfeng Lai, Yu Xu", "title": "A User-Centered Concept Mining System for Query and Document\n  Understanding at Tencent", "comments": "Accepted by KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330727", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts embody the knowledge of the world and facilitate the cognitive\nprocesses of human beings. Mining concepts from web documents and constructing\nthe corresponding taxonomy are core research problems in text understanding and\nsupport many downstream tasks such as query analysis, knowledge base\nconstruction, recommendation, and search. However, we argue that most prior\nstudies extract formal and overly general concepts from Wikipedia or static web\npages, which are not representing the user perspective. In this paper, we\ndescribe our experience of implementing and deploying ConcepT in Tencent QQ\nBrowser. It discovers user-centered concepts at the right granularity\nconforming to user interests, by mining a large amount of user queries and\ninteractive search click logs. The extracted concepts have the proper\ngranularity, are consistent with user language styles and are dynamically\nupdated. We further present our techniques to tag documents with user-centered\nconcepts and to construct a topic-concept-instance taxonomy, which has helped\nto improve search as well as news feeds recommendation in Tencent QQ Browser.\nWe performed extensive offline evaluation to demonstrate that our approach\ncould extract concepts of higher quality compared to several other existing\nmethods. Our system has been deployed in Tencent QQ Browser. Results from\nonline A/B testing involving a large number of real users suggest that the\nImpression Efficiency of feeds users increased by 6.01% after incorporating the\nuser-centered concepts into the recommendation framework of Tencent QQ Browser.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 08:25:09 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Liu", "Bang", ""], ["Guo", "Weidong", ""], ["Niu", "Di", ""], ["Wang", "Chaoyue", ""], ["Xu", "Shunnan", ""], ["Lin", "Jinghong", ""], ["Lai", "Kunfeng", ""], ["Xu", "Yu", ""]]}, {"id": "1905.08732", "submitter": "Ting-Shuo Yo", "authors": "Chu-Ren Huang, Ting-Shuo Yo, Petr Simon, Shu-Kai Hsieh", "title": "A realistic and robust model for Chinese word segmentation", "comments": "Proceedings of the 20th Conference on Computational Linguistics and\n  Speech Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A realistic Chinese word segmentation tool must adapt to textual variations\nwith minimal training input and yet robust enough to yield reliable\nsegmentation result for all variants. Various lexicon-driven approaches to\nChinese segmentation, e.g. [1,16], achieve high f-scores yet require massive\ntraining for any variation. Text-driven approach, e.g. [12], can be easily\nadapted for domain and genre changes yet has difficulty matching the high\nf-scores of the lexicon-driven approaches. In this paper, we refine and\nimplement an innovative text-driven word boundary decision (WBD) segmentation\nmodel proposed in [15]. The WBD model treats word segmentation simply and\nefficiently as a binary decision on whether to realize the natural textual\nbreak between two adjacent characters as a word boundary. The WBD model allows\nsimple and quick training data preparation converting characters as contextual\nvectors for learning the word boundary decision. Machine learning experiments\nwith four different classifiers show that training with 1,000 vectors and 1\nmillion vectors achieve comparable and reliable results. In addition, when\napplied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall\nrates that are higher than all published results. Unlike all previous work, our\nOOV recall rate is comparable to our own F-score. Both experiments support the\nclaim that the WBD model is a realistic model for Chinese word segmentation as\nit can be easily adapted for new variants with the robust result. In\nconclusion, we will discuss linguistic ramifications as well as future\nimplications for the WBD approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 16:22:47 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Huang", "Chu-Ren", ""], ["Yo", "Ting-Shuo", ""], ["Simon", "Petr", ""], ["Hsieh", "Shu-Kai", ""]]}, {"id": "1905.08772", "submitter": "Sergio Gast\\'on Burdisso", "authors": "Sergio G. Burdisso, Marcelo Errecalde, Manuel Montes-y-G\\'omez", "title": "A Text Classification Framework for Simple and Effective Early\n  Depression Detection Over Social Media Streams", "comments": "Highlights: (*) A novel text classifier having the ability to\n  visually explain its rationale; (*) Domain-independent classification that\n  does not require feature engineering; (*) Support for incremental learning\n  and text classification over streams; (*) Efficient framework for addressing\n  early risk detection problems; (*) State-of-the-art performance on early\n  depression detection task", "journal-ref": "18 May 2019, Volume 133, Expert Systems With Applications,\n  Elsevier", "doi": "10.1016/j.eswa.2019.05.023", "report-no": null, "categories": "cs.CY cs.CL cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of the Internet, there is a growing need to build intelligent\nsystems that are capable of efficiently dealing with early risk detection (ERD)\nproblems on social media, such as early depression detection, early rumor\ndetection or identification of sexual predators. These systems, nowadays mostly\nbased on machine learning techniques, must be able to deal with data streams\nsince users provide their data over time. In addition, these systems must be\nable to decide when the processed data is sufficient to actually classify\nusers. Moreover, since ERD tasks involve risky decisions by which people's\nlives could be affected, such systems must also be able to justify their\ndecisions. However, most standard and state-of-the-art supervised machine\nlearning models (such as SVM, MNB, Neural Networks, etc.) are not well suited\nto deal with this scenario. This is due to the fact that they either act as\nblack boxes or do not support incremental classification/learning. In this\npaper we introduce SS3, a novel supervised learning model for text\nclassification that naturally supports these aspects. SS3 was designed to be\nused as a general framework to deal with ERD problems. We evaluated our model\non the CLEF's eRisk2017 pilot task on early depression detection. Most of the\n30 contributions submitted to this competition used state-of-the-art methods.\nExperimental results show that our classifier was able to outperform these\nmodels and standard classifiers, despite being less computationally expensive\nand having the ability to explain its rationale.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 15:46:38 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Burdisso", "Sergio G.", ""], ["Errecalde", "Marcelo", ""], ["Montes-y-G\u00f3mez", "Manuel", ""]]}, {"id": "1905.08775", "submitter": "Evangelos Pournaras", "authors": "David Castells-Graells, Christopher Salahub, Evangelos Pournaras", "title": "On Cycling Risk and Discomfort: Urban Safety Mapping and Bike Route\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bike usage in Smart Cities becomes paramount for sustainable urban\ndevelopment. Cycling provides tremendous opportunities for a more healthy\nlifestyle, lower energy consumption and carbon emissions as well as reduction\nof traffic jams. While the number of cyclists increase along with the expansion\nof bike sharing initiatives and infrastructures, the number of bike accidents\nrises drastically threatening to jeopardize the bike urban movement. This paper\nstudies cycling risk and discomfort using a diverse spectrum of data sources\nabout geolocated bike accidents and their severity. Empirical continuous\nspatial risk estimations are calculated via kernel density contours that map\nsafety in a case study of Zurich city. The role of weather, time, accident type\nand severity are illustrated. Given the predominance of self-caused accidents,\nan open-source software artifact for personalized route recommendations is\nintroduced. The software is also used to collect open baseline route data that\nare compared with alternative ones that minimize risk or discomfort. These\ncontributions can provide invaluable insights for urban planners to improve\ninfrastructure. They can also improve the risk awareness of existing cyclists'\nas well as support new cyclists, such as tourists, to safely explore a new\nurban environment by bike.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 20:50:31 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Castells-Graells", "David", ""], ["Salahub", "Christopher", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "1905.08865", "submitter": "Namyong Park", "authors": "Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, Christos Faloutsos", "title": "Estimating Node Importance in Knowledge Graphs Using Graph Neural\n  Networks", "comments": "KDD 2019 Research Track. 11 pages. Changelog: Type 3 font removed,\n  and minor updates made in the Appendix (v2)", "journal-ref": null, "doi": "10.1145/3292500.3330855", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we estimate the importance of nodes in a knowledge graph (KG)? A KG\nis a multi-relational graph that has proven valuable for many tasks including\nquestion answering and semantic search. In this paper, we present GENI, a\nmethod for tackling the problem of estimating node importance in KGs, which\nenables several downstream applications such as item recommendation and\nresource allocation. While a number of approaches have been developed to\naddress this problem for general graphs, they do not fully utilize information\navailable in KGs, or lack flexibility needed to model complex relationship\nbetween entities and their importance. To address these limitations, we explore\nsupervised machine learning algorithms. In particular, building upon recent\nadvancement of graph neural networks (GNNs), we develop GENI, a GNN-based\nmethod designed to deal with distinctive challenges involved with predicting\nnode importance in KGs. Our method performs an aggregation of importance scores\ninstead of aggregating node embeddings via predicate-aware attention mechanism\nand flexible centrality adjustment. In our evaluation of GENI and existing\nmethods on predicting node importance in real-world KGs with different\ncharacteristics, GENI achieves 5-17% higher NDCG@100 than the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:48:42 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 04:26:24 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Park", "Namyong", ""], ["Kan", "Andrey", ""], ["Dong", "Xin Luna", ""], ["Zhao", "Tong", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1905.08880", "submitter": "Anshul Kanakia", "authors": "Anshul Kanakia (1), Zhihong Shen (1), Darrin Eide (1), Kuansan Wang\n  (1) ((1) Microsoft Research)", "title": "A Scalable Hybrid Research Paper Recommender System for Microsoft\n  Academic", "comments": "7 pages, 7 figures. Short paper at The Web Conference 2019, San\n  Francisco, USA", "journal-ref": "In The World Wide Web Conference (WWW '19). ACM, New York, NY,\n  USA, 2893-2899", "doi": "10.1145/3308558.3313700", "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and methodology for the large scale hybrid paper\nrecommender system used by Microsoft Academic. The system provides\nrecommendations for approximately 160 million English research papers and\npatents. Our approach handles incomplete citation information while also\nalleviating the cold-start problem that often affects other recommender\nsystems. We use the Microsoft Academic Graph (MAG), titles, and available\nabstracts of research papers to build a recommendation list for all documents,\nthereby combining co-citation and content based approaches. Tuning system\nparameters also allows for blending and prioritization of each approach which,\nin turn, allows us to balance paper novelty versus authority in recommendation\nresults. We evaluate the generated recommendations via a user study of 40\nparticipants, with over 2400 recommendation pairs graded and discuss the\nquality of the results using P@10 and nDCG scores. We see that there is a\nstrong correlation between participant scores and the similarity rankings\nproduced by our system but that additional focus needs to be put towards\nimproving recommender precision, particularly for content based\nrecommendations. The results of the user survey and associated analysis scripts\nare made available via GitHub and the recommendations produced by our system\nare available as part of the MAG on Azure to facilitate further research and\nlight up novel research paper recommendation applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:46:33 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Kanakia", "Anshul", "", "Microsoft Research"], ["Shen", "Zhihong", "", "Microsoft Research"], ["Eide", "Darrin", "", "Microsoft Research"], ["Wang", "Kuansan", "", "Microsoft Research"]]}, {"id": "1905.08900", "submitter": "Shibo Yao", "authors": "Shibo Yao, Dantong Yu, Keli Xiao", "title": "Enhancing Domain Word Embedding via Latent Semantic Imputation", "comments": "ACM SIGKDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330926", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method named Latent Semantic Imputation (LSI) to transfer\nexternal knowledge into semantic space for enhancing word embedding. The method\nintegrates graph theory to extract the latent manifold structure of the\nentities in the affinity space and leverages non-negative least squares with\nstandard simplex constraints and power iteration method to derive spectral\nembeddings. It provides an effective and efficient approach to combining entity\nrepresentations defined in different Euclidean spaces. Specifically, our\napproach generates and imputes reliable embedding vectors for low-frequency\nwords in the semantic space and benefits downstream language tasks that depend\non word embedding. We conduct comprehensive experiments on a carefully designed\nclassification problem and language modeling and demonstrate the superiority of\nthe enhanced embedding via LSI over several well-known benchmark embeddings. We\nalso confirm the consistency of the results under different parameter settings\nof our method.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 23:31:45 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Yao", "Shibo", ""], ["Yu", "Dantong", ""], ["Xiao", "Keli", ""]]}, {"id": "1905.08957", "submitter": "Helia Hashemi", "authors": "Helia Hashemi, Mohammad Aliannejadi, Hamed Zamani, W. Bruce Croft", "title": "ANTIQUE: A Non-Factoid Question Answering Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the widespread use of mobile and voice search, answer passage\nretrieval for non-factoid questions plays a critical role in modern information\nretrieval systems. Despite the importance of the task, the community still\nfeels the significant lack of large-scale non-factoid question answering\ncollections with real questions and comprehensive relevance judgments. In this\npaper, we develop and release a collection of 2,626 open-domain non-factoid\nquestions from a diverse set of categories. The dataset, called ANTIQUE,\ncontains 34,011 manual relevance annotations. The questions were asked by real\nusers in a community question answering service, i.e., Yahoo! Answers.\nRelevance judgments for all the answers to each question were collected through\ncrowdsourcing. To facilitate further research, we also include a brief analysis\nof the data as well as baseline results on both classical and recently\ndeveloped neural IR models.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 05:32:03 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 07:41:43 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Hashemi", "Helia", ""], ["Aliannejadi", "Mohammad", ""], ["Zamani", "Hamed", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1905.09050", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Peter Ochs", "title": "Beyond Alternating Updates for Matrix Factorization with Inertial\n  Bregman Proximal Gradient Algorithms", "comments": "Accepted at NeuRIPS 2019. Paper url:\n  http://papers.nips.cc/paper/8679-beyond-alternating-updates-for-matrix-factorization-with-inertial-bregman-proximal-gradient-algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization is a popular non-convex optimization problem, for which\nalternating minimization schemes are mostly used. They usually suffer from the\nmajor drawback that the solution is biased towards one of the optimization\nvariables. A remedy is non-alternating schemes. However, due to a lack of\nLipschitz continuity of the gradient in matrix factorization problems,\nconvergence cannot be guaranteed. A recently developed approach relies on the\nconcept of Bregman distances, which generalizes the standard Euclidean\ndistance. We exploit this theory by proposing a novel Bregman distance for\nmatrix factorization problems, which, at the same time, allows for\nsimple/closed form update steps. Therefore, for non-alternating schemes, such\nas the recently introduced Bregman Proximal Gradient (BPG) method and an\ninertial variant Convex--Concave Inertial BPG (CoCaIn BPG), convergence of the\nwhole sequence to a stationary point is proved for Matrix Factorization. In\nseveral experiments, we observe a superior performance of our non-alternating\nschemes in terms of speed and objective value at the limit point.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:09:54 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:33:13 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Ochs", "Peter", ""]]}, {"id": "1905.09052", "submitter": "Gloria Feher", "authors": "Gloria Feher, Andreas Spitz, Michael Gertz", "title": "Retrieving Multi-Entity Associations: An Evaluation of Combination Modes\n  for Word Embeddings", "comments": "4 pages; Accepted at SIGIR'19", "journal-ref": null, "doi": "10.1145/3331184.3331366", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have gained significant attention as learnable\nrepresentations of semantic relations between words, and have been shown to\nimprove upon the results of traditional word representations. However, little\neffort has been devoted to using embeddings for the retrieval of entity\nassociations beyond pairwise relations. In this paper, we use popular embedding\nmethods to train vector representations of an entity-annotated news corpus, and\nevaluate their performance for the task of predicting entity participation in\nnews events versus a traditional word cooccurrence network as a baseline. To\nsupport queries for events with multiple participating entities, we test a\nnumber of combination modes for the embedding vectors. While we find that even\nthe best combination modes for word embeddings do not quite reach the\nperformance of the full cooccurrence network, especially for rare entities, we\nobserve that different embedding methods model different types of relations,\nthereby indicating the potential for ensemble methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 10:13:48 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Feher", "Gloria", ""], ["Spitz", "Andreas", ""], ["Gertz", "Michael", ""]]}, {"id": "1905.09086", "submitter": "Nikola Milo\\v{s}evi\\'c Dr", "authors": "Nikola Milosevic, Dimitar Marinov, Abdullah Gok, and Goran Nenadic", "title": "From web crawled text to project descriptions: automatic summarizing of\n  social innovation projects", "comments": "Keywords: Summarization, evaluation metrics, text mining, natural\n  language processing, social innovation, SVM, neural networks Accepted for\n  publication in Proceedings of 24th International Conference on Applications\n  of Natural Language to Information Systems (NLDB2019)", "journal-ref": "Preceeding of 24th International Conference on Applications of\n  Natural Language to Information Systems (NLDB2019)", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the past decade, social innovation projects have gained the attention of\npolicy makers, as they address important social issues in an innovative manner.\nA database of social innovation is an important source of information that can\nexpand collaboration between social innovators, drive policy and serve as an\nimportant resource for research. Such a database needs to have projects\ndescribed and summarized. In this paper, we propose and compare several methods\n(e.g. SVM-based, recurrent neural network based, ensambled) for describing\nprojects based on the text that is available on project websites. We also\naddress and propose a new metric for automated evaluation of summaries based on\ntopic modelling.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 11:49:37 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Milosevic", "Nikola", ""], ["Marinov", "Dimitar", ""], ["Gok", "Abdullah", ""], ["Nenadic", "Goran", ""]]}, {"id": "1905.09205", "submitter": "William La Cava", "authors": "William La Cava, Heather Williams, Weixuan Fu, Steve Vitale, Durga\n  Srivatsan, Jason H. Moore", "title": "Evaluating recommender systems for AI-driven biomedical informatics", "comments": "17 pages, 8 figures. this version fixes link to pennai in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Many researchers with domain expertise are unable to easily apply\nmachine learning to their bioinformatics data due to a lack of machine learning\nand/or coding expertise. Methods that have been proposed thus far to automate\nmachine learning mostly require programming experience as well as expert\nknowledge to tune and apply the algorithms correctly. Here, we study a method\nof automating biomedical data science using a web-based platform that uses AI\nto recommend model choices and conduct experiments. We have two goals in mind:\nfirst, to make it easy to construct sophisticated models of biomedical\nprocesses; and second, to provide a fully automated AI agent that can choose\nand conduct promising experiments for the user, based on the user's experiments\nas well as prior knowledge. To validate this framework, we experiment with\nhundreds of classification problems, comparing to state-of-the-art, automated\napproaches. Finally, we use this tool to develop predictive models of septic\nshock in critical care patients.\n  Results: We find that matrix factorization-based recommendation systems\noutperform meta-learning methods for automating machine learning. This result\nmirrors the results of earlier recommender systems research in other domains.\nThe proposed AI is competitive with state-of-the-art automated machine learning\nmethods in terms of choosing optimal algorithm configurations for datasets. In\nour application to prediction of septic shock, the AI-driven analysis produces\na competent machine learning model (AUROC 0.85 +/- 0.02) that performs on par\nwith state-of-the-art deep learning results for this task, with much less\ncomputational effort.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 15:53:53 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 21:26:56 GMT"}, {"version": "v3", "created": "Sat, 4 Apr 2020 18:53:25 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 15:46:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["La Cava", "William", ""], ["Williams", "Heather", ""], ["Fu", "Weixuan", ""], ["Vitale", "Steve", ""], ["Srivatsan", "Durga", ""], ["Moore", "Jason H.", ""]]}, {"id": "1905.09217", "submitter": "Zhuyun Dai", "authors": "Zhuyun Dai and Jamie Callan", "title": "Deeper Text Understanding for IR with Contextual Neural Language\n  Modeling", "comments": "In proceedings of SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331303", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks provide new possibilities to automatically learn complex\nlanguage patterns and query-document relations. Neural IR models have achieved\npromising results in learning query-document relevance patterns, but few\nexplorations have been done on understanding the text content of a query or a\ndocument. This paper studies leveraging a recently-proposed contextual neural\nlanguage model, BERT, to provide deeper text understanding for IR. Experimental\nresults demonstrate that the contextual text representations from BERT are more\neffective than traditional word embeddings. Compared to bag-of-words retrieval\nmodels, the contextual language model can better leverage language structures,\nbringing large improvements on queries written in natural languages. Combining\nthe text understanding ability with search knowledge leads to an enhanced\npre-trained BERT model that can benefit related search tasks where training\ndata are limited.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 16:11:47 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Dai", "Zhuyun", ""], ["Callan", "Jamie", ""]]}, {"id": "1905.09248", "submitter": "Qi Pi", "authors": "Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, Kun Gai", "title": "Practice on Long Sequential User Behavior Modeling for Click-Through\n  Rate Prediction", "comments": "9 pages. Accepted by KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330666", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction is critical for industrial applications\nsuch as recommender system and online advertising. Practically, it plays an\nimportant role for CTR modeling in these applications by mining user interest\nfrom rich historical behavior data. Driven by the development of deep learning,\ndeep CTR models with ingeniously designed architecture for user interest\nmodeling have been proposed, bringing remarkable improvement of model\nperformance over offline metric.However, great efforts are needed to deploy\nthese complex models to online serving system for realtime inference, facing\nmassive traffic request. Things turn to be more difficult when it comes to long\nsequential user behavior data, as the system latency and storage cost increase\napproximately linearly with the length of user behavior sequence. In this\npaper, we face directly the challenge of long sequential user behavior modeling\nand introduce our hands-on practice with the co-design of machine learning\nalgorithm and online serving system for CTR prediction task. Theoretically, the\nco-design solution of UIC and MIMN enables us to handle the user interest\nmodeling with unlimited length of sequential behavior data. Comparison between\nmodel performance and system efficiency proves the effectiveness of proposed\nsolution. To our knowledge, this is one of the first industrial solutions that\nare capable of handling long sequential user behavior data with length scaling\nup to thousands. It now has been deployed in the display advertising system in\nAlibaba.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:12:03 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 05:12:19 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 03:00:13 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Pi", "Qi", ""], ["Bian", "Weijie", ""], ["Zhou", "Guorui", ""], ["Zhu", "Xiaoqiang", ""], ["Gai", "Kun", ""]]}, {"id": "1905.09624", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Phelim Bradley, Florian Gauger, and Zamin Iqbal", "title": "COBS: a Compact Bit-Sliced Signature Index", "comments": "To appear in 26th International Symposium on String Processing and\n  Information Retrieval (SPIRE'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present COBS, a COmpact Bit-sliced Signature index, which is a cross-over\nbetween an inverted index and Bloom filters. Our target application is to index\n$k$-mers of DNA samples or $q$-grams from text documents and process\napproximate pattern matching queries on the corpus with a user-chosen coverage\nthreshold. Query results may contain a number of false positives which\ndecreases exponentially with the query length. We compare COBS to seven other\nindex software packages on 100000 microbial DNA samples. COBS' compact but\nsimple data structure outperforms the other indexes in construction time and\nquery performance with Mantis by Pandey et al. in second place. However, unlike\nMantis and other previous work, COBS does not need the complete index in RAM\nand is thus designed to scale to larger document sets.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:49:58 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 13:00:54 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Bingmann", "Timo", ""], ["Bradley", "Phelim", ""], ["Gauger", "Florian", ""], ["Iqbal", "Zamin", ""]]}, {"id": "1905.09761", "submitter": "Md Faisal Mahbub Chowdhury", "authors": "Md Faisal Mahbub Chowdhury, Robert Farrell", "title": "An Efficient Approach for Super and Nested Term Indexing and Retrieval", "comments": "6 pages including references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a new approach, called Terminological Bucket Indexing\n(TBI), for efficient indexing and retrieval of both nested and super terms\nusing a single method. We propose a hybrid data structure for facilitating\nfaster indexing building. An evaluation of our approach with respect to widely\nused existing approaches on several publicly available dataset is provided.\nCompared to Trie based approaches, TBI provides comparable performance on\nnested term retrieval and far superior performance on super term retrieval.\nCompared to traditional hash table, TBI needs 80\\% less time for indexing.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:33:30 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Chowdhury", "Md Faisal Mahbub", ""], ["Farrell", "Robert", ""]]}, {"id": "1905.09763", "submitter": "Leo Torres", "authors": "Leo Torres, Kevin S Chan, Tina Eliassi-Rad", "title": "GLEE: Geometric Laplacian Eigenmap Embedding", "comments": null, "journal-ref": null, "doi": "10.1093/comnet/cnaa007", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph embedding seeks to build a low-dimensional representation of a graph G.\nThis low-dimensional representation is then used for various downstream tasks.\nOne popular approach is Laplacian Eigenmaps, which constructs a graph embedding\nbased on the spectral properties of the Laplacian matrix of G. The intuition\nbehind it, and many other embedding techniques, is that the embedding of a\ngraph must respect node similarity: similar nodes must have embeddings that are\nclose to one another. Here, we dispose of this distance-minimization\nassumption. Instead, we use the Laplacian matrix to find an embedding with\ngeometric properties instead of spectral ones, by leveraging the so-called\nsimplex geometry of G. We introduce a new approach, Geometric Laplacian\nEigenmap Embedding (or GLEE for short), and demonstrate that it outperforms\nvarious other techniques (including Laplacian Eigenmaps) in the tasks of graph\nreconstruction and link prediction.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 16:35:23 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:49:28 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Torres", "Leo", ""], ["Chan", "Kevin S", ""], ["Eliassi-Rad", "Tina", ""]]}, {"id": "1905.09864", "submitter": "Varun Kumar", "authors": "Varun Kumar, Alison Smith-Renner, Leah Findlater, Kevin Seppi and\n  Jordan Boyd-Graber", "title": "Why Didn't You Listen to Me? Comparing User Control of Human-in-the-Loop\n  Topic Models", "comments": "In proceedings of ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the lack of comparative evaluation of Human-in-the-Loop Topic\nModeling (HLTM) systems, we implement and evaluate three contrasting HLTM\nmodeling approaches using simulation experiments. These approaches extend\npreviously proposed frameworks, including constraints and informed prior-based\nmethods. Users should have a sense of control in HLTM systems, so we propose a\ncontrol metric to measure whether refinement operations' results match users'\nexpectations. Informed prior-based methods provide better control than\nconstraints, but constraints yield higher quality topics.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 18:40:57 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:24:03 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kumar", "Varun", ""], ["Smith-Renner", "Alison", ""], ["Findlater", "Leah", ""], ["Seppi", "Kevin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1905.09874", "submitter": "Francois Belletti", "authors": "Francois Belletti, Karthik Lakshmanan, Walid Krichene, Nicolas\n  Mayoraz, Yi-Fan Chen, John Anderson, Taylor Robie, Tayo Oguntebi, Dan\n  Shirron, Amit Bleiwess", "title": "Scaling Up Collaborative Filtering Data Sets through Randomized Fractal\n  Expansions", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.08910", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender system research suffers from a disconnect between the size of\nacademic data sets and the scale of industrial production systems. In order to\nbridge that gap, we propose to generate large-scale user/item interaction data\nsets by expanding pre-existing public data sets. Our key contribution is a\ntechnique that expands user/item incidence matrices matrices to large numbers\nof rows (users), columns (items), and non-zero values (interactions). The\nproposed method adapts Kronecker Graph Theory to preserve key higher order\nstatistical properties such as the fat-tailed distribution of user engagements,\nitem popularity, and singular value spectra of user/item interaction matrices.\nPreserving such properties is key to building large realistic synthetic data\nsets which in turn can be employed reliably to benchmark recommender systems\nand the systems employed to train them. We further apply our stochastic\nexpansion algorithm to the binarized MovieLens 20M data set, which comprises\n20M interactions between 27K movies and 138K users. The resulting expanded data\nset has 1.2B ratings, 2.2M users, and 855K items, which can be scaled up or\ndown.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:56:38 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Belletti", "Francois", ""], ["Lakshmanan", "Karthik", ""], ["Krichene", "Walid", ""], ["Mayoraz", "Nicolas", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""], ["Robie", "Taylor", ""], ["Oguntebi", "Tayo", ""], ["Shirron", "Dan", ""], ["Bleiwess", "Amit", ""]]}, {"id": "1905.10077", "submitter": "Lixin Su", "authors": "Lixin Su, Jiafeng Guo, Yixing Fan, Yanyan Lan, and Xueqi Cheng", "title": "Controlling Risk of Web Question Answering", "comments": "42nd International ACM SIGIR Conference on Research and Development\n  in Information Retrieval", "journal-ref": null, "doi": "10.1145/3331184.3331261", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web question answering (QA) has become an indispensable component in modern\nsearch systems, which can significantly improve users' search experience by\nproviding a direct answer to users' information need. This could be achieved by\napplying machine reading comprehension (MRC) models over the retrieved passages\nto extract answers with respect to the search query. With the development of\ndeep learning techniques, state-of-the-art MRC performances have been achieved\nby recent deep methods. However, existing studies on MRC seldom address the\npredictive uncertainty issue, i.e., how likely the prediction of an MRC model\nis wrong, leading to uncontrollable risks in real-world Web QA applications. In\nthis work, we first conduct an in-depth investigation over the risk of Web QA.\nWe then introduce a novel risk control framework, which consists of a qualify\nmodel for uncertainty estimation using the probe idea, and a decision model for\nselectively output. For evaluation, we introduce risk-related metrics, rather\nthan the traditional EM and F1 in MRC, for the evaluation of risk-aware Web QA.\nThe empirical results over both the real-world Web QA dataset and the academic\nMRC benchmark collection demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 07:55:42 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 02:24:32 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 05:10:47 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Su", "Lixin", ""], ["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Lan", "Yanyan", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1905.10215", "submitter": "Marco Winckler", "authors": "Gabriela Bosetti, Sergio Firmenich (UNLP), Alejandro Fernandez\n  (LIFIA), Marco Winckler (UFRGS, Polytech'Lab, IRIT), Gustavo Rossi (UNLP)", "title": "From Search Engines to Search Services: An End-User Driven Approach", "comments": null, "journal-ref": "17th International Conference, ICWE 2017, 2017, Rome, Italy", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web is a vast and continuously changing source of information\nwhere searching is a frequent, and sometimes critical, user task. Searching is\nnot always the user's primary goal but an ancillary task that is performed to\nfind complementary information allowing to complete another task. In this\npaper, we explore primary and/or ancillary search tasks and propose an approach\nfor simplifying the user interaction during search tasks. Rather than fo-cusing\non dedicated search engines, our approach allows the user to abstract search\nengines already provided by Web applications into pervasive search services\nthat will be available for performing searches from any other Web site. We also\npropose to allow users to manage the way in which searching results are\ndisplayed and the interaction with them. In order to illustrate the feasibility\nof this approach, we have built a support tool based on a plug-in architecture\nthat allows users to integrate new search services (created by themselves by\nmeans of visual tools) and execute them in the context of both kinds of\nsearches. A case study illustrates the use of these tools. We also present the\nresults of two evaluations that demonstrate the feasibility of the approach and\nthe benefits in its use.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:10:10 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bosetti", "Gabriela", "", "UNLP"], ["Firmenich", "Sergio", "", "UNLP"], ["Fernandez", "Alejandro", "", "LIFIA"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab, IRIT"], ["Rossi", "Gustavo", "", "UNLP"]]}, {"id": "1905.10289", "submitter": "Yixing Fan", "authors": "Jiafeng Guo, Yixing Fan, Xiang Ji and Xueqi Cheng", "title": "MatchZoo: A Learning, Practicing, and Developing System for Neural Text\n  Matching", "comments": null, "journal-ref": "Proceedings of the 42nd International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '19), July 21--25,\n  2019, Paris, France", "doi": "10.1145/3331184.3331403", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text matching is the core problem in many natural language processing (NLP)\ntasks, such as information retrieval, question answering, and conversation.\nRecently, deep leaning technology has been widely adopted for text matching,\nmaking neural text matching a new and active research domain. With a large\nnumber of neural matching models emerging rapidly, it becomes more and more\ndifficult for researchers, especially those newcomers, to learn and understand\nthese new models. Moreover, it is usually difficult to try these models due to\nthe tedious data pre-processing, complicated parameter configuration, and\nmassive optimization tricks, not to mention the unavailability of public codes\nsometimes. Finally, for researchers who want to develop new models, it is also\nnot an easy task to implement a neural text matching model from scratch, and to\ncompare with a bunch of existing models. In this paper, therefore, we present a\nnovel system, namely MatchZoo, to facilitate the learning, practicing and\ndesigning of neural text matching models. The system consists of a powerful\nmatching library and a user-friendly and interactive studio, which can help\nresearchers: 1) to learn state-of-the-art neural text matching models\nsystematically, 2) to train, test and apply these models with simple\nconfigurable steps; and 3) to develop their own models with rich APIs and\nassistance.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:34:54 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 10:08:49 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Guo", "Jiafeng", ""], ["Fan", "Yixing", ""], ["Ji", "Xiang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1905.10309", "submitter": "Yanshan Wang", "authors": "Yanshan Wang, Yiqing Zhao, Terry M. Therneau, Elizabeth J. Atkinson,\n  Ahmad P. Tafti, Nan Zhang, Shreyasee Amin, Andrew H. Limper, Hongfang Liu", "title": "Unsupervised Machine Learning for the Discovery of Latent Disease\n  Clusters and Patient Subgroups Using Electronic Health Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become ubiquitous and a key technology on mining\nelectronic health records (EHRs) for facilitating clinical research and\npractice. Unsupervised machine learning, as opposed to supervised learning, has\nshown promise in identifying novel patterns and relations from EHRs without\nusing human created labels. In this paper, we investigate the application of\nunsupervised machine learning models in discovering latent disease clusters and\npatient subgroups based on EHRs. We utilized Latent Dirichlet Allocation (LDA),\na generative probabilistic model, and proposed a novel model named Poisson\nDirichlet Model (PDM), which extends the LDA approach using a Poisson\ndistribution to model patients' disease diagnoses and to alleviate age and sex\nfactors by considering both observed and expected observations. In the\nempirical experiments, we evaluated LDA and PDM on three patient cohorts with\nEHR data retrieved from the Rochester Epidemiology Project (REP), for the\ndiscovery of latent disease clusters and patient subgroups. We compared the\neffectiveness of LDA and PDM in identifying latent disease clusters through the\nvisualization of disease representations learned by two approaches. We also\ntested the performance of LDA and PDM in differentiating patient subgroups\nthrough survival analysis, as well as statistical analysis. The experimental\nresults show that the proposed PDM could effectively identify distinguished\ndisease clusters by alleviating the impact of age and sex, and that LDA could\nstratify patients into more differentiable subgroups than PDM in terms of\np-values. However, the subgroups discovered by PDM might imply the underlying\npatterns of diseases of greater interest in epidemiology research due to the\nalleviation of age and sex. Both unsupervised machine learning approaches could\nbe leveraged to discover patient subgroups using EHRs but with different foci.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:07:22 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wang", "Yanshan", ""], ["Zhao", "Yiqing", ""], ["Therneau", "Terry M.", ""], ["Atkinson", "Elizabeth J.", ""], ["Tafti", "Ahmad P.", ""], ["Zhang", "Nan", ""], ["Amin", "Shreyasee", ""], ["Limper", "Andrew H.", ""], ["Liu", "Hongfang", ""]]}, {"id": "1905.10482", "submitter": "Subhasis Dasgupta", "authors": "Junan Guo, Subhasis Dasgupta and Amarnath Gupta", "title": "Multi-Model Investigative Exploration of Social Media Data with\n  boutique: A Case Study in Public Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our experience with a data science problem in Public Health, where\nresearchers use social media (Twitter) to determine whether the public shows\nawareness of HIV prevention measures offered by Public Health campaigns. To\nhelp the researcher, we develop an investigative exploration system called\nboutique that allows a user to perform a multi-step visualization and\nexploration of data through a dashboard interface. Unique features of boutique\nincludes its ability to handle heterogeneous types of data provided by a\npolystore, and its ability to use computation as part of the investigative\nexploration process. In this paper, we present the design of the boutique\nmiddleware and walk through an investigation process for a real-life problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:34:48 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Guo", "Junan", ""], ["Dasgupta", "Subhasis", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1905.10536", "submitter": "Shuai Zhang", "authors": "Shuai Zhang and Yi Tay and Lina Yao and Bin Wu and Aixin Sun", "title": "DeepRec: An Open-source Toolkit for Deep Learning based Recommendation", "comments": "Accepted by IJCAI-2019 Demonstrations Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning based recommender systems have been extensively explored in\nrecent years. However, the large number of models proposed each year poses a\nbig challenge for both researchers and practitioners in reproducing the results\nfor further comparisons. Although a portion of papers provides source code,\nthey adopted different programming languages or different deep learning\npackages, which also raises the bar in grasping the ideas. To alleviate this\nproblem, we released the open source project: \\textbf{DeepRec}. In this\ntoolkit, we have implemented a number of deep learning based recommendation\nalgorithms using Python and the widely used deep learning package - Tensorflow.\nThree major recommendation scenarios: rating prediction, top-N recommendation\n(item ranking) and sequential recommendation, were considered. Meanwhile,\nDeepRec maintains good modularity and extensibility to easily incorporate new\nmodels into the framework. It is distributed under the terms of the GNU General\nPublic License. The source code is available at github:\n\\url{https://github.com/cheungdaven/DeepRec}\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:00:26 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Shuai", ""], ["Tay", "Yi", ""], ["Yao", "Lina", ""], ["Wu", "Bin", ""], ["Sun", "Aixin", ""]]}, {"id": "1905.10565", "submitter": "Pepa Atanasova", "authors": "Pepa Atanasova, Georgi Karadzhov, Yasen Kiprov, Preslav Nakov,\n  Fabrizio Sebastiani", "title": "Evaluating Variable-Length Multiple-Option Lists in Chatbots and Mobile\n  Search", "comments": "4 pages, in Proceeding of SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331308", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the proliferation of smart mobile devices has lead to the\ngradual integration of search functionality within mobile platforms. This has\ncreated an incentive to move away from the \"ten blue links'' metaphor, as\nmobile users are less likely to click on them, expecting to get the answer\ndirectly from the snippets. In turn, this has revived the interest in Question\nAnswering. Then, along came chatbots, conversational systems, and messaging\nplatforms, where the user needs could be better served with the system asking\nfollow-up questions in order to better understand the user's intent. While\ntypically a user would expect a single response at any utterance, a system\ncould also return multiple options for the user to select from, based on\ndifferent system understandings of the user's intent. However, this possibility\nshould not be overused, as this practice could confuse and/or annoy the user.\nHow to produce good variable-length lists, given the conflicting objectives of\nstaying short while maximizing the likelihood of having a correct answer\nincluded in the list, is an underexplored problem. It is also unclear how to\nevaluate a system that tries to do that. Here we aim to bridge this gap. In\nparticular, we define some necessary and some optional properties that an\nevaluation measure fit for this purpose should have. We further show that\nexisting evaluation measures from the IR tradition are not entirely suitable\nfor this setup, and we propose novel evaluation measures that address it\nsatisfactorily.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 10:22:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Atanasova", "Pepa", ""], ["Karadzhov", "Georgi", ""], ["Kiprov", "Yasen", ""], ["Nakov", "Preslav", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1905.10668", "submitter": "Ninghao Liu", "authors": "Ninghao Liu, Qiaoyu Tan, Yuening Li, Hongxia Yang, Jingren Zhou, Xia\n  Hu", "title": "Is a Single Vector Enough? Exploring Node Polysemy for Network Embedding", "comments": null, "journal-ref": null, "doi": "10.1145/3292500.3330967", "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks have been widely used as the data structure for abstracting\nreal-world systems as well as organizing the relations among entities. Network\nembedding models are powerful tools in mapping nodes in a network into\ncontinuous vector-space representations in order to facilitate subsequent tasks\nsuch as classification and link prediction. Existing network embedding models\ncomprehensively integrate all information of each node, such as links and\nattributes, towards a single embedding vector to represent the node's general\nrole in the network. However, a real-world entity could be multifaceted, where\nit connects to different neighborhoods due to different motives or\nself-characteristics that are not necessarily correlated. For example, in a\nmovie recommender system, a user may love comedies or horror movies\nsimultaneously, but it is not likely that these two types of movies are\nmutually close in the embedding space, nor the user embedding vector could be\nsufficiently close to them at the same time. In this paper, we propose a\npolysemous embedding approach for modeling multiple facets of nodes, as\nmotivated by the phenomenon of word polysemy in language modeling. Each facet\nof a node is mapped as an embedding vector, while we also maintain association\ndegree between each pair of node and facet. The proposed method is adaptive to\nvarious existing embedding models, without significantly complicating the\noptimization process. We also discuss how to engage embedding vectors of\ndifferent facets for inference tasks including classification and link\nprediction. Experiments on real-world datasets help comprehensively evaluate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 19:57:57 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Liu", "Ninghao", ""], ["Tan", "Qiaoyu", ""], ["Li", "Yuening", ""], ["Yang", "Hongxia", ""], ["Zhou", "Jingren", ""], ["Hu", "Xia", ""]]}, {"id": "1905.10688", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Madelon Hulsebos and Kevin Hu and Michiel Bakker and Emanuel Zgraggen\n  and Arvind Satyanarayan and Tim Kraska and \\c{C}a\\u{g}atay Demiralp and\n  C\\'esar Hidalgo", "title": "Sherlock: A Deep Learning Approach to Semantic Data Type Detection", "comments": "KDD'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctly detecting the semantic type of data columns is crucial for data\nscience tasks such as automated data cleaning, schema matching, and data\ndiscovery. Existing data preparation and analysis systems rely on dictionary\nlookups and regular expression matching to detect semantic types. However,\nthese matching-based approaches often are not robust to dirty data and only\ndetect a limited number of types. We introduce Sherlock, a multi-input deep\nneural network for detecting semantic types. We train Sherlock on $686,765$\ndata columns retrieved from the VizNet corpus by matching $78$ semantic types\nfrom DBpedia to column headers. We characterize each matched column with\n$1,588$ features describing the statistical properties, character\ndistributions, word embeddings, and paragraph vectors of column values.\nSherlock achieves a support-weighted F$_1$ score of $0.89$, exceeding that of\nmachine learning baselines, dictionary and regular expression benchmarks, and\nthe consensus of crowdsourced annotations.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 22:36:05 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hulsebos", "Madelon", ""], ["Hu", "Kevin", ""], ["Bakker", "Michiel", ""], ["Zgraggen", "Emanuel", ""], ["Satyanarayan", "Arvind", ""], ["Kraska", "Tim", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Hidalgo", "C\u00e9sar", ""]]}, {"id": "1905.10718", "submitter": "Dong Xu", "authors": "Dong Xu and Wu-Jun Li", "title": "Hashing based Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer selection is an important subtask of question answering (QA), where\ndeep models usually achieve better performance. Most deep models adopt\nquestion-answer interaction mechanisms, such as attention, to get vector\nrepresentations for answers. When these interaction based deep models are\ndeployed for online prediction, the representations of all answers need to be\nrecalculated for each question. This procedure is time-consuming for deep\nmodels with complex encoders like BERT which usually have better accuracy than\nsimple encoders. One possible solution is to store the matrix representation\n(encoder output) of each answer in memory to avoid recalculation. But this will\nbring large memory cost. In this paper, we propose a novel method, called\nhashing based answer selection (HAS), to tackle this problem. HAS adopts a\nhashing strategy to learn a binary matrix representation for each answer, which\ncan dramatically reduce the memory cost for storing the matrix representations\nof answers. Hence, HAS can adopt complex encoders like BERT in the model, but\nthe online prediction of HAS is still fast with a low memory cost. Experimental\nresults on three popular answer selection datasets show that HAS can outperform\nexisting models to achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 03:33:42 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Dong", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1905.10720", "submitter": "Dong Xu", "authors": "Dong Xu and Jianhui Ji and Haikuan Huang and Hongbo Deng and Wu-Jun Li", "title": "Gated Group Self-Attention for Answer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer selection (answer ranking) is one of the key steps in many kinds of\nquestion answering (QA) applications, where deep models have achieved\nstate-of-the-art performance. Among these deep models, recurrent neural network\n(RNN) based models are most popular, typically with better performance than\nconvolutional neural network (CNN) based models. Nevertheless, it is difficult\nfor RNN based models to capture the information about long-range dependency\namong words in the sentences of questions and answers. In this paper, we\npropose a new deep model, called gated group self-attention (GGSA), for answer\nselection. GGSA is inspired by global self-attention which is originally\nproposed for machine translation and has not been explored in answer selection.\nGGSA tackles the problem of global self-attention that local and global\ninformation cannot be well distinguished. Furthermore, an interaction mechanism\nbetween questions and answers is also proposed to enhance GGSA by a residual\nstructure. Experimental results on two popular QA datasets show that GGSA can\noutperform existing answer selection models to achieve state-of-the-art\nperformance. Furthermore, GGSA can also achieve higher accuracy than global\nself-attention for the answer selection task, with a lower computation cost.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 03:40:17 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Xu", "Dong", ""], ["Ji", "Jianhui", ""], ["Huang", "Haikuan", ""], ["Deng", "Hongbo", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1905.10760", "submitter": "Feng Yuan", "authors": "Feng Yuan, Lina Yao, and Boualem Benatallah", "title": "DARec: Deep Domain Adaptation for Cross-Domain Recommendation via\n  Transferring Rating Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain recommendation has long been one of the major topics in\nrecommender systems. Recently, various deep models have been proposed to\ntransfer the learned knowledge across domains, but most of them focus on\nextracting abstract transferable features from auxilliary contents, e.g.,\nimages and review texts, and the patterns in the rating matrix itself is rarely\ntouched. In this work, inspired by the concept of domain adaptation, we\nproposed a deep domain adaptation model (DARec) that is capable of extracting\nand transferring patterns from rating matrices {\\em only} without relying on\nany auxillary information. We empirically demonstrate on public datasets that\nour method achieves the best performance among several state-of-the-art\nalternative cross-domain recommendation models.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 08:21:50 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yuan", "Feng", ""], ["Yao", "Lina", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1905.10847", "submitter": "Yi Tay", "authors": "Yi Tay, Shuohang Wang, Luu Anh Tuan, Jie Fu, Minh C. Phan, Xingdi\n  Yuan, Jinfeng Rao, Siu Cheung Hui, Aston Zhang", "title": "Simple and Effective Curriculum Pointer-Generator Networks for Reading\n  Comprehension over Long Narratives", "comments": "Accepted to ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of reading comprehension over long narratives\nwhere documents easily span over thousands of tokens. We propose a curriculum\nlearning (CL) based Pointer-Generator framework for reading/sampling over large\ndocuments, enabling diverse training of the neural model based on the notion of\nalternating contextual difficulty. This can be interpreted as a form of domain\nrandomization and/or generative pretraining during training. To this end, the\nusage of the Pointer-Generator softens the requirement of having the answer\nwithin the context, enabling us to construct diverse training samples for\nlearning. Additionally, we propose a new Introspective Alignment Layer (IAL),\nwhich reasons over decomposed alignments using block-based self-attention. We\nevaluate our proposed method on the NarrativeQA reading comprehension\nbenchmark, achieving state-of-the-art performance, improving existing baselines\nby $51\\%$ relative improvement on BLEU-4 and $17\\%$ relative improvement on\nRouge-L. Extensive ablations confirm the effectiveness of our proposed IAL and\nCL components.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 17:56:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Tay", "Yi", ""], ["Wang", "Shuohang", ""], ["Tuan", "Luu Anh", ""], ["Fu", "Jie", ""], ["Phan", "Minh C.", ""], ["Yuan", "Xingdi", ""], ["Rao", "Jinfeng", ""], ["Hui", "Siu Cheung", ""], ["Zhang", "Aston", ""]]}, {"id": "1905.10881", "submitter": "Pan Li", "authors": "Pan Li, Eli Chien, Olgica Milenkovic", "title": "Optimizing Generalized PageRank Methods for Seed-Expansion Community\n  Detection", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landing probabilities (LP) of random walks (RW) over graphs encode rich\ninformation regarding graph topology. Generalized PageRanks (GPR), which\nrepresent weighted sums of LPs of RWs, utilize the discriminative power of LP\nfeatures to enable many graph-based learning studies. Previous work in the area\nhas mostly focused on evaluating suitable weights for GPRs, and only a few\nstudies so far have attempted to derive the optimal weights of GRPs for a given\napplication. We take a fundamental step forward in this direction by using\nrandom graph models to better our understanding of the behavior of GPRs. In\nthis context, we provide a rigorous non-asymptotic analysis for the convergence\nof LPs and GPRs to their mean-field values on edge-independent random graphs.\nAlthough our theoretical results apply to many problem settings, we focus on\nthe task of seed-expansion community detection over stochastic block models.\nThere, we find that the predictive power of LPs decreases significantly slower\nthan previously reported based on asymptotic findings. Given this result, we\npropose a new GPR, termed Inverse PR (IPR), with LP weights that increase for\nthe initial few steps of the walks. Extensive experiments on both synthetic and\nreal, large-scale networks illustrate the superiority of IPR compared to other\nGPRs for seeded community detection.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:11:40 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 20:12:11 GMT"}, {"version": "v3", "created": "Wed, 25 Dec 2019 00:35:59 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Li", "Pan", ""], ["Chien", "Eli", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1905.10893", "submitter": "Shuhan Wang", "authors": "Shuhan Wang, Hao Wu, Ji Hun Kim and Erik Andersen", "title": "Adaptive Learning Material Recommendation in Online Language Education", "comments": "The short version of this paper is published at AIED 2019", "journal-ref": "The 20th International Conference on Artificial Intelligence in\n  Education (AIED), 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending personalized learning materials for online language learning is\nchallenging because we typically lack data about the student's ability and the\nrelative difficulty of learning materials. This makes it hard to recommend\nappropriate content that matches the student's prior knowledge. In this paper,\nwe propose a refined hierarchical knowledge structure to model vocabulary\nknowledge, which enables us to automatically organize the authentic and\nup-to-date learning materials collected from the internet. Based on this\nknowledge structure, we then introduce a hybrid approach to recommend learning\nmaterials that adapts to a student's language level. We evaluate our work with\nan online Japanese learning tool and the results suggest adding adaptivity into\nmaterial recommendation significantly increases student engagement.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:59:49 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Shuhan", ""], ["Wu", "Hao", ""], ["Kim", "Ji Hun", ""], ["Andersen", "Erik", ""]]}, {"id": "1905.10951", "submitter": "Qing-Yuan Jiang", "authors": "Qing-Yuan Jiang, Ming-Wei Li and Wu-Jun Li", "title": "On the Evaluation Metric for Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its low storage cost and fast query speed, hashing has been widely\nused for large-scale approximate nearest neighbor (ANN) search. Bucket search,\nalso called hash lookup, can achieve fast query speed with a sub-linear time\ncost based on the inverted index table constructed from hash codes. Many\nmetrics have been adopted to evaluate hashing algorithms. However, all existing\nmetrics are improper to evaluate the hash codes for bucket search. On one hand,\nall existing metrics ignore the retrieval time cost which is an important\nfactor reflecting the performance of search. On the other hand, some of them,\nsuch as mean average precision (MAP), suffer from the uncertainty problem as\nthe ranked list is based on integer-valued Hamming distance, and are\ninsensitive to Hamming radius as these metrics only depend on relative Hamming\ndistance. Other metrics, such as precision at Hamming radius R, fail to\nevaluate global performance as these metrics only depend on one specific\nHamming radius. In this paper, we first point out the problems of existing\nmetrics which have been ignored by the hashing community, and then propose a\nnovel evaluation metric called radius aware mean average precision (RAMAP) to\nevaluate hash codes for bucket search. Furthermore, two coding strategies are\nalso proposed to qualitatively show the problems of existing metrics.\nExperiments demonstrate that our proposed RAMAP can provide more proper\nevaluation than existing metrics.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:12:37 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Jiang", "Qing-Yuan", ""], ["Li", "Ming-Wei", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1905.10955", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Zeren Sun, Fumin Shen, Li Liu, Limin Wang, Fan Zhu,\n  Lizhong Ding, Gangshan Wu, Ling Shao", "title": "Dynamically Visual Disambiguation of Keyword-based Image Search", "comments": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high cost of manual annotation, learning directly from the web has\nattracted broad attention. One issue that limits their performance is the\nproblem of visual polysemy. To address this issue, we present an adaptive\nmulti-model framework that resolves polysemy by visual disambiguation. Compared\nto existing methods, the primary advantage of our approach lies in that our\napproach can adapt to the dynamic changes in the search results. Our proposed\nframework consists of two major steps: we first discover and dynamically select\nthe text queries according to the image search results, then we employ the\nproposed saliency-guided deep multi-instance learning network to remove\noutliers and learn classification models for visual disambiguation. Extensive\nexperiments demonstrate the superiority of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:33:28 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Yao", "Yazhou", ""], ["Sun", "Zeren", ""], ["Shen", "Fumin", ""], ["Liu", "Li", ""], ["Wang", "Limin", ""], ["Zhu", "Fan", ""], ["Ding", "Lizhong", ""], ["Wu", "Gangshan", ""], ["Shao", "Ling", ""]]}, {"id": "1905.10960", "submitter": "Marie Katsurai", "authors": "Marie Katsurai, Shunsuke Ono", "title": "TrendNets: Mapping Emerging Research Trends From Dynamic Co-Word\n  Networks via Sparse Representation", "comments": "This is a pre-print of an article published in Scientometrics, 2019", "journal-ref": null, "doi": "10.1007/s11192-019-03241-6", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping the knowledge structure from word co-occurrences in a collection of\nacademic papers has been widely used to provide insight into the topic\nevolution in an arbitrary research field. In a traditional approach, the paper\ncollection is first divided into temporal subsets, and then a co-word network\nis independently depicted in a 2D map to characterize each period's trend. To\neffectively map emerging research trends from such a time-series of co-word\nnetworks, this paper presents TrendNets, a novel visualization methodology that\nhighlights the rapid changes in edge weights over time. Specifically, we\nformulated a new convex optimization framework that decomposes the matrix\nconstructed from dynamic co-word networks into a smooth part and a sparse part:\nthe former represents stationary research topics, while the latter corresponds\nto bursty research topics. Simulation results on synthetic data demonstrated\nthat our matrix decomposition approach achieved the best burst detection\nperformance over four baseline methods. In experiments conducted using papers\npublished in the past 16 years at three conferences in different fields, we\nshowed the effectiveness of TrendNets compared to the traditional co-word\nrepresentation. We have made our codes available on the Web to encourage\nscientific mapping in all research fields.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:53:19 GMT"}, {"version": "v2", "created": "Sat, 19 Oct 2019 03:31:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Katsurai", "Marie", ""], ["Ono", "Shunsuke", ""]]}, {"id": "1905.11096", "submitter": "Juli\\'an Urbano", "authors": "Juli\\'an Urbano, Harlley Lima, Alan Hanjalic", "title": "Statistical Significance Testing in Information Retrieval: An Empirical\n  Analysis of Type I, Type II and Type III Errors", "comments": "10 pages, 6 figures, SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331259", "report-no": null, "categories": "cs.IR cs.DL cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical significance testing is widely accepted as a means to assess how\nwell a difference in effectiveness reflects an actual difference between\nsystems, as opposed to random noise because of the selection of topics.\nAccording to recent surveys on SIGIR, CIKM, ECIR and TOIS papers, the t-test is\nthe most popular choice among IR researchers. However, previous work has\nsuggested computer intensive tests like the bootstrap or the permutation test,\nbased mainly on theoretical arguments. On empirical grounds, others have\nsuggested non-parametric alternatives such as the Wilcoxon test. Indeed, the\nquestion of which tests we should use has accompanied IR and related fields for\ndecades now. Previous theoretical studies on this matter were limited in that\nwe know that test assumptions are not met in IR experiments, and empirical\nstudies were limited in that we do not have the necessary control over the null\nhypotheses to compute actual Type I and Type II error rates under realistic\nconditions. Therefore, not only is it unclear which test to use, but also how\nmuch trust we should put in them. In contrast to past studies, in this paper we\nemploy a recent simulation methodology from TREC data to go around these\nlimitations. Our study comprises over 500 million p-values computed for a range\nof tests, systems, effectiveness measures, topic set sizes and effect sizes,\nand for both the 2-tail and 1-tail cases. Having such a large supply of IR\nevaluation data with full knowledge of the null hypotheses, we are finally in a\nposition to evaluate how well statistical significance tests really behave with\nIR data, and make sound recommendations for practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:02:29 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 22:18:34 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Urbano", "Juli\u00e1n", ""], ["Lima", "Harlley", ""], ["Hanjalic", "Alan", ""]]}, {"id": "1905.11133", "submitter": "Ge Fan", "authors": "Ge Fan, Wei Zeng, Shan Sun, Biao Geng, Weiyi Wang, Weibo Liu", "title": "A collaborative filtering model with heterogeneous neural networks for\n  recommender systems", "comments": "Some technical errors, such as the lack of computational complexity\n  analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural network is introduced in recommender systems to\nsolve the collaborative filtering problem, which has achieved immense success\non computer vision, speech recognition and natural language processing. On one\nhand, deep neural network can be used to model the auxiliary information in\nrecommender systems. On the other hand, it is also capable of modeling\nnonlinear relationships between users and items. One advantage of deep neural\nnetwork is that the performance of the algorithm can be easily enhanced by\naugmenting the depth of the neural network. However, two potential problems may\nemerge when the deep neural work is exploited to model relationships between\nusers and items. The fundamental problem is that the complexity of the\nalgorithm grows significantly with the increment in the depth of the neural\nnetwork. The second one is that a deeper neural network may undermine the\naccuracy of the algorithm. In order to alleviate these problems, we propose a\nhybrid neural network that combines heterogeneous neural networks with\ndifferent structures. The experimental results on real datasets reveal that our\nmethod is superior to the state-of-the-art methods in terms of the item\nranking.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:29:52 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 05:12:49 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 02:47:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fan", "Ge", ""], ["Zeng", "Wei", ""], ["Sun", "Shan", ""], ["Geng", "Biao", ""], ["Wang", "Weiyi", ""], ["Liu", "Weibo", ""]]}, {"id": "1905.11139", "submitter": "Devraj Mandal", "authors": "Devraj Mandal, Pramod Rao, Soma Biswas", "title": "Label Prediction Framework for Semi-Supervised Cross-Modal Retrieval", "comments": "12 pages, 3 tables, 2 figures, 1 algorithm flowchart", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cross-modal data matching refers to retrieval of data from one modality, when\ngiven a query from another modality. In general, supervised algorithms achieve\nbetter retrieval performance compared to their unsupervised counterpart, as\nthey can learn better representative features by leveraging the available label\ninformation. However, this comes at the cost of requiring huge amount of\nlabeled examples, which may not always be available. In this work, we propose a\nnovel framework in a semi-supervised setting, which can predict the labels of\nthe unlabeled data using complementary information from different modalities.\nThe proposed framework can be used as an add-on with any baseline crossmodal\nalgorithm to give significant performance improvement, even in case of limited\nlabeled data. Finally, we analyze the challenging scenario where the unlabeled\nexamples can even come from classes not in the training data and evaluate the\nperformance of our algorithm under such setting. Extensive evaluation using\nseveral baseline algorithms across three different datasets shows the\neffectiveness of our label prediction framework.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:39:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Mandal", "Devraj", ""], ["Rao", "Pramod", ""], ["Biswas", "Soma", ""]]}, {"id": "1905.11244", "submitter": "Andrew Collins Mr", "authors": "Andrew Collins, Joeran Beel", "title": "Document Embeddings vs. Keyphrases vs. Terms: An Online Evaluation in\n  Digital Library Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recommendation algorithms are available to digital library recommender\nsystem operators. The effectiveness of algorithms is largely unreported by way\nof online evaluation. We compare a standard term-based recommendation approach\nto two promising approaches for related-article recommendation in digital\nlibraries: document embeddings, and keyphrases. We evaluate the consistency of\ntheir performance across multiple scenarios. Through our\nrecommender-as-a-service Mr. DLib, we delivered 33.5M recommendations to users\nof Sowiport and Jabref over the course of 19 months, from March 2017 to October\n2018. The effectiveness of the algorithms differs significantly between\nSowiport and Jabref (Wilcoxon rank-sum test; p < 0.05). There is a ~400%\ndifference in effectiveness between the best and worst algorithm in both\nscenarios separately. The best performing algorithm in Sowiport (terms) is the\nworst performing in Jabref. The best performing algorithm in Jabref\n(keyphrases) is 70% worse in Sowiport, than Sowiport`s best algorithm\n(click-through rate; 0.1% terms, 0.03% keyphrases).\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:05:50 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Collins", "Andrew", ""], ["Beel", "Joeran", ""]]}, {"id": "1905.11518", "submitter": "Peng Yu", "authors": "Dora Jambor, Peng Yu", "title": "On a scalable problem transformation method for multi-label learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary relevance is a simple approach to solve multi-label learning problems\nwhere an independent binary classifier is built per each label. A common\nchallenge with this in real-world applications is that the label space can be\nvery large, making it difficult to use binary relevance to larger scale\nproblems. In this paper, we propose a scalable alternative to this, via\ntransforming the multi-label problem into a single binary classification. We\nexperiment with a few variations of our method and show that our method\nachieves higher precision than binary relevance and faster execution times on a\ntop-K recommender system task.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 21:40:37 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Jambor", "Dora", ""], ["Yu", "Peng", ""]]}, {"id": "1905.11596", "submitter": "Bei Chen", "authors": "Yihong Chen, Bei Chen, Xiangnan He, Chen Gao, Yong Li, Jian-Guang Lou,\n  Yue Wang", "title": "LambdaOpt: Learn to Regularize Recommender Models in Finer Levels", "comments": "Accepted by KDD 2019", "journal-ref": null, "doi": "10.1145/3292500.3330880", "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation models mainly deal with categorical variables, such as\nuser/item ID and attributes. Besides the high-cardinality issue, the\ninteractions among such categorical variables are usually long-tailed, with the\nhead made up of highly frequent values and a long tail of rare ones. This\nphenomenon results in the data sparsity issue, making it essential to\nregularize the models to ensure generalization. The common practice is to\nemploy grid search to manually tune regularization hyperparameters based on the\nvalidation data. However, it requires non-trivial efforts and large computation\nresources to search the whole candidate space; even so, it may not lead to the\noptimal choice, for which different parameters should have different\nregularization strengths. In this paper, we propose a hyperparameter\noptimization method, LambdaOpt, which automatically and adaptively enforces\nregularization during training. Specifically, it updates the regularization\ncoefficients based on the performance of validation data. With LambdaOpt, the\nnotorious tuning of regularization hyperparameters can be avoided; more\nimportantly, it allows fine-grained regularization (i.e. each parameter can\nhave an individualized regularization coefficient), leading to better\ngeneralized models. We show how to employ LambdaOpt on matrix factorization, a\nclassical model that is representative of a large family of recommender models.\nExtensive experiments on two public benchmarks demonstrate the superiority of\nour method in boosting the performance of top-K recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 03:54:38 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chen", "Yihong", ""], ["Chen", "Bei", ""], ["He", "Xiangnan", ""], ["Gao", "Chen", ""], ["Li", "Yong", ""], ["Lou", "Jian-Guang", ""], ["Wang", "Yue", ""]]}, {"id": "1905.11668", "submitter": "Artur Strzelecki", "authors": "Artur Strzelecki", "title": "Application of Developers' and Users' Dependent Factors in App Store\n  Optimization", "comments": null, "journal-ref": "International Journal of Interactive Mobile Technologies (iJIM)\n  2020", "doi": "10.3991/ijim.v14i13.14143", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an application of developers' and users' dependent\nfactors in the app store optimization. The application is based on two main\nfields: developers' dependent factors and users' dependent factors. Developers'\ndependent factors are identified as: developer name, app name, subtitle, genre,\nshort description, long description, content rating, system requirements, page\nurl, last update, what's new and price. Users' dependent factors are identified\nas: download volume, average rating, rating volume and reviews. The proposed\napplication in its final form is modelled after mining sample data from two\nleading app stores: Google Play and Apple App Store. Results from analyzing\ncollected data show that developer dependent elements can be better optimized.\nNames and descriptions of mobile apps are not fully utilized. In Google Play\nthere is one significant correlation between download volume and number of\nreviews, whereas in App Store there is no significant correlation between\nfactors.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 08:17:03 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:33:06 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Strzelecki", "Artur", ""]]}, {"id": "1905.11862", "submitter": "Peixian Chen", "authors": "Peixian Chen, Pingyang Dai, Qiong Wu, Yuyu Huang", "title": "Video-based Person Re-identification with Two-stream Convolutional\n  Network and Co-attentive Snippet Embedding", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the applications of person re-identification in visual surveillance\nand human-computer interaction are sharply increasing, which signifies the\ncritical role of such a problem. In this paper, we propose a two-stream\nconvolutional network (ConvNet) based on the competitive similarity aggregation\nscheme and co-attentive embedding strategy for video-based person\nre-identification. By dividing the long video sequence into multiple short\nvideo snippets, we manage to utilize every snippet's RGB frames, optical flow\nmaps and pose maps to facilitate residual networks, e.g., ResNet, for feature\nextraction in the two-stream ConvNet. The extracted features are embedded by\nthe co-attentive embedding method, which allows for the reduction of the\neffects of noisy frames. Finally, we fuse the outputs of both streams as the\nembedding of a snippet, and apply competitive snippet-similarity aggregation to\nmeasure the similarity between two sequences. Our experiments show that the\nproposed method significantly outperforms current state-of-the-art approaches\non multiple datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:47:33 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chen", "Peixian", ""], ["Dai", "Pingyang", ""], ["Wu", "Qiong", ""], ["Huang", "Yuyu", ""]]}, {"id": "1905.11900", "submitter": "Wenyi Xiao", "authors": "Wenyi Xiao, Huan Zhao, Haojie Pan, Yangqiu Song, Vincent W. Zheng,\n  Qiang Yang", "title": "Beyond Personalization: Social Content Recommendation for Creator\n  Equality and Consumer Satisfaction", "comments": "Accepted by SIGKDD 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An effective content recommendation in modern social media platforms should\nbenefit both creators to bring genuine benefits to them and consumers to help\nthem get really interesting content. In this paper, we propose a model called\nSocial Explorative Attention Network (SEAN) for content recommendation. SEAN\nuses a personalized content recommendation model to encourage personal\ninterests driven recommendation. Moreover, SEAN allows the personalization\nfactors to attend to users' higher-order friends on the social network to\nimprove the accuracy and diversity of recommendation results. Constructing two\ndatasets from a popular decentralized content distribution platform, Steemit,\nwe compare SEAN with state-of-the-art CF and content based recommendation\napproaches. Experimental results demonstrate the effectiveness of SEAN in terms\nof both Gini coefficients for recommendation equality and F1 scores for\nrecommendation performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:55:47 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 14:47:05 GMT"}, {"version": "v3", "created": "Sun, 16 Jun 2019 04:58:15 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Xiao", "Wenyi", ""], ["Zhao", "Huan", ""], ["Pan", "Haojie", ""], ["Song", "Yangqiu", ""], ["Zheng", "Vincent W.", ""], ["Yang", "Qiang", ""]]}, {"id": "1905.11959", "submitter": "Juliano Henrique Foleiss", "authors": "Juliano H. Foleiss and Tiago F. Tavares", "title": "Texture Selection for Automatic Music Genre Classification", "comments": "Submitted to Pattern Recognition (may, 2019)", "journal-ref": null, "doi": "10.1016/j.asoc.2020.106127", "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music Genre Classification is the problem of associating genre-related labels\nto digitized music tracks. It has applications in the organization of\ncommercial and personal music collections. Often, music tracks are described as\na set of timbre-inspired sound textures. In shallow-learning systems, the total\nnumber of sound textures per track is usually too high, and texture\ndownsampling is necessary to make training tractable. Although previous work\nhas solved this by linear downsampling, no extensive work has been done to\nevaluate how texture selection benefits genre classification in the context of\nthe bag of frames track descriptions. In this paper, we evaluate the impact of\nframe selection on automatic music genre classification in a bag of frames\nscenario. We also present a novel texture selector based on K-Means aimed to\nidentify diverse sound textures within each track. We evaluated texture\nselection in diverse datasets, four different feature sets, as well as its\nrelationship to a univariate feature selection strategy. The results show that\nframe selection leads to significant improvement over the single vector\nbaseline on datasets consisting of full-length tracks, regardless of the\nfeature set. Results also indicate that the K-Means texture selector achieves\nsignificant improvements over the baseline, using fewer textures per track than\nthe commonly used linear downsampling. The results also suggest that texture\nselection is complementary to the feature selection strategy evaluated. Our\nqualitative analysis indicates that texture variety within classes benefits\nmodel generalization. Our analysis shows that selecting specific audio excerpts\ncan improve classification performance, and it can be done automatically.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 17:30:31 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Foleiss", "Juliano H.", ""], ["Tavares", "Tiago F.", ""]]}, {"id": "1905.11984", "submitter": "Rohit Vaish", "authors": "Haoming Li, Sujoy Sikdar, Rohit Vaish, Junming Wang, Lirong Xia,\n  Chaonan Ye", "title": "Minimizing Time-to-Rank: A Learning and Recommendation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem faced by an online voting platform: A user is\nprovided with a list of alternatives, and is asked to rank them in order of\npreference using only drag-and-drop operations. The platform's goal is to\nrecommend an initial ranking that minimizes the time spent by the user in\narriving at her desired ranking. We develop the first optimization framework to\naddress this problem, and make theoretical as well as practical contributions.\nOn the practical side, our experiments on Amazon Mechanical Turk provide two\ninteresting insights about user behavior: First, that users' ranking strategies\nclosely resemble selection or insertion sort, and second, that the time taken\nfor a drag-and-drop operation depends linearly on the number of positions\nmoved. These insights directly motivate our theoretical model of the\noptimization problem. We show that computing an optimal recommendation is\nNP-hard, and provide exact and approximation algorithms for a variety of\nspecial cases of the problem. Experimental evaluation on MTurk shows that,\ncompared to a random recommendation strategy, the proposed approach reduces the\n(average) time-to-rank by up to 50%.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 23:50:06 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Li", "Haoming", ""], ["Sikdar", "Sujoy", ""], ["Vaish", "Rohit", ""], ["Wang", "Junming", ""], ["Xia", "Lirong", ""], ["Ye", "Chaonan", ""]]}, {"id": "1905.12220", "submitter": "Alexander Nwala", "authors": "Alexander C. Nwala, Michele C. Weigle, Michael L. Nelson", "title": "Using Micro-collections in Social Media to Generate Seeds for Web\n  Archive Collections", "comments": "This is an extended version of the ACM/IEEE Joint Conference on\n  Digital Libraries (JCDL 2019) full paper. Some figures have been enlarged,\n  and appendices of additional figures included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In a Web plagued by disappearing resources, Web archive collections provide a\nvaluable means of preserving Web resources important to the study of past\nevents ranging from elections to disease outbreaks. These archived collections\nstart with seed URIs (Uniform Resource Identifiers) hand-selected by curators.\nCurators produce high quality seeds by removing non-relevant URIs and adding\nURIs from credible and authoritative sources, but it is time consuming to\ncollect these seeds. Two main strategies adopted by curators for discovering\nseeds include scraping Web (e.g., Google) Search Engine Result Pages (SERPs)\nand social media (e.g., Twitter) SERPs. In this work, we studied three social\nmedia platforms in order to provide insight on the characteristics of seeds\ngenerated from different sources. First, we developed a simple vocabulary for\ndescribing social media posts across different platforms. Second, we introduced\na novel source for generating seeds from URIs in the threaded conversations of\nsocial media posts created by single or multiple users. Users on social media\nsites routinely create and share posts about news events consisting of\nhand-selected URIs of news stories, tweets, videos, etc. In this work, we call\nthese posts micro-collections, and we consider them as an important source for\nseeds because the effort taken to create micro-collections is an indication of\neditorial activity, and a demonstration of domain expertise. Third, we\ngenerated 23,112 seed collections with text and hashtag queries from 449,347\nsocial media posts from Reddit, Twitter, and Scoop.it. We collected in total\n120,444 URIs from the conventional scraped SERP posts and micro-collections. We\ncharacterized the resultant seed collections across multiple dimensions\nincluding the distribution of URIs, precision, ages, diversity of webpages,\netc...\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:19:58 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Nwala", "Alexander C.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1905.12480", "submitter": "Hongtao Liu", "authors": "Hongtao Liu, Fangzhao Wu, Wenjun Wang, Xianchen Wang, Pengfei Jiao,\n  Chuhan Wu, Xing Xie", "title": "NRPA: Neural Recommendation with Personalized Attention", "comments": "4 pages, 4 figures", "journal-ref": "sigir 2019", "doi": "10.1145/3331184.3331371", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing review-based recommendation methods usually use the same model to\nlearn the representations of all users/items from reviews posted by users\ntowards items. However, different users have different preference and different\nitems have different characteristics. Thus, the same word or similar reviews\nmay have different informativeness for different users and items. In this paper\nwe propose a neural recommendation approach with personalized attention to\nlearn personalized representations of users and items from reviews. We use a\nreview encoder to learn representations of reviews from words, and a user/item\nencoder to learn representations of users or items from reviews. We propose a\npersonalized attention model, and apply it to both review and user/item\nencoders to select different important words and reviews for different\nusers/items. Experiments on five datasets validate our approach can effectively\nimprove the performance of neural recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:16:17 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Hongtao", ""], ["Wu", "Fangzhao", ""], ["Wang", "Wenjun", ""], ["Wang", "Xianchen", ""], ["Jiao", "Pengfei", ""], ["Wu", "Chuhan", ""], ["Xie", "Xing", ""]]}, {"id": "1905.12595", "submitter": "Mihai P\\^irvu", "authors": "Mihai Cristian P\\^irvu and Alexandra Anghel", "title": "Predicting next shopping stage using Google Analytics data for\n  E-commerce applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce web applications are almost ubiquitous in our day to day life,\nhowever as useful as they are, most of them have little to no adaptation to\nuser needs, which in turn can cause both lower conversion rates as well as\nunsatisfied customers. We propose a machine learning system which learns the\nuser behaviour from multiple previous sessions and predicts useful metrics for\nthe current session. In turn, these metrics can be used by the applications to\ncustomize and better target the customer, which can mean anything from offering\nbetter offers of specific products, targeted notifications or placing smart\nads. The data used for the learning algorithm is extracted from Google\nAnalytics Enhanced E-commerce, which is enabled by most e-commerce websites and\nthus the system can be used by any such merchant. In order to learn the user\npatterns, only its behaviour features were used, which don't include names,\ngender or any other personal information that could identify the user. The\nlearning model that was used is a double recurrent neural network which learns\nboth intra-session and inter-session features. The model predicts for each\nsession a probability score for each of the defined target classes.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:14:24 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["P\u00eervu", "Mihai Cristian", ""], ["Anghel", "Alexandra", ""]]}, {"id": "1905.12767", "submitter": "Eugene Ie", "authors": "Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui\n  Wu, Heng-Tze Cheng, Morgane Lustman, Vince Gatto, Paul Covington, Jim\n  McFadden, Tushar Chandra, Craig Boutilier", "title": "Reinforcement Learning for Slate-based Recommender Systems: A Tractable\n  Decomposition and Practical Methodology", "comments": "Short version to appear IJCAI-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most practical recommender systems focus on estimating immediate user\nengagement without considering the long-term effects of recommendations on user\nbehavior. Reinforcement learning (RL) methods offer the potential to optimize\nrecommendations for long-term user engagement. However, since users are often\npresented with slates of multiple items - which may have interacting effects on\nuser choice - methods are required to deal with the combinatorics of the RL\naction space. In this work, we address the challenge of making slate-based\nrecommendations to optimize long-term value using RL. Our contributions are\nthree-fold. (i) We develop SLATEQ, a decomposition of value-based\ntemporal-difference and Q-learning that renders RL tractable with slates. Under\nmild assumptions on user choice behavior, we show that the long-term value\n(LTV) of a slate can be decomposed into a tractable function of its component\nitem-wise LTVs. (ii) We outline a methodology that leverages existing myopic\nlearning-based recommenders to quickly develop a recommender that handles LTV.\n(iii) We demonstrate our methods in simulation, and validate the scalability of\ndecomposed TD-learning using SLATEQ in live experiments on YouTube.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 22:55:28 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 07:27:00 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Ie", "Eugene", ""], ["Jain", "Vihan", ""], ["Wang", "Jing", ""], ["Narvekar", "Sanmit", ""], ["Agarwal", "Ritesh", ""], ["Wu", "Rui", ""], ["Cheng", "Heng-Tze", ""], ["Lustman", "Morgane", ""], ["Gatto", "Vince", ""], ["Covington", "Paul", ""], ["McFadden", "Jim", ""], ["Chandra", "Tushar", ""], ["Boutilier", "Craig", ""]]}, {"id": "1905.12786", "submitter": "Anjishnu Kumar", "authors": "Daniele Bonadiman, Anjishnu Kumar and Arpit Mittal", "title": "Large Scale Question Paraphrase Retrieval with Smoothed Deep Metric\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of a Question Paraphrase Retrieval (QPR) system is to retrieve\nequivalent questions that result in the same answer as the original question.\nSuch a system can be used to understand and answer rare and noisy\nreformulations of common questions by mapping them to a set of canonical forms.\nThis has large-scale applications for community Question Answering (cQA) and\nopen-domain spoken language question answering systems. In this paper we\ndescribe a new QPR system implemented as a Neural Information Retrieval (NIR)\nsystem consisting of a neural network sentence encoder and an approximate\nk-Nearest Neighbour index for efficient vector retrieval. We also describe our\nmechanism to generate an annotated dataset for question paraphrase retrieval\nexperiments automatically from question-answer logs via distant supervision. We\nshow that the standard loss function in NIR, triplet loss, does not perform\nwell with noisy labels. We propose smoothed deep metric loss (SDML) and with\nour experiments on two QPR datasets we show that it significantly outperforms\ntriplet loss in the noisy label setting.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 23:40:54 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Bonadiman", "Daniele", ""], ["Kumar", "Anjishnu", ""], ["Mittal", "Arpit", ""]]}, {"id": "1905.12804", "submitter": "Angelo Mendes", "authors": "Angelo C. Mendes da Silva and Mauricio A. Nunes and Raul Fonseca Neto", "title": "A Music Classification Model based on Metric Learning and Feature\n  Extraction from MP3 Audio Files", "comments": "In a review process, I found some errors and made some changes in\n  methodology that improved my results. Once I finish the experiments, I will\n  upload the new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of models for learning music similarity and feature\nextraction from audio media files is an increasingly important task for the\nentertainment industry. This work proposes a novel music classification model\nbased on metric learning and feature extraction from MP3 audio files. The\nmetric learning process considers the learning of a set of parameterized\ndistances employing a structured prediction approach from a set of MP3 audio\nfiles containing several music genres. The main objective of this work is to\nmake possible learning a personalized metric for each customer. To extract the\nacoustic information we use the Mel-Frequency Cepstral Coefficient (MFCC) and\nmake a dimensionality reduction with the use of Principal Components Analysis.\nWe attest the model validity performing a set of experiments and comparing the\ntraining and testing results with baseline algorithms, such as K-means and Soft\nMargin Linear Support Vector Machine (SVM). Experiments show promising results\nand encourage the future development of an online version of the learning\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 00:52:57 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 22:29:02 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["da Silva", "Angelo C. Mendes", ""], ["Nunes", "Mauricio A.", ""], ["Neto", "Raul Fonseca", ""]]}, {"id": "1905.12862", "submitter": "Min Hou", "authors": "Min Hou, Le Wu, Enhong Chen, Zhi Li, Vincent W. Zheng, Qi Liu", "title": "Explainable Fashion Recommendation: A Semantic Attribute Region Guided\n  Approach", "comments": "Accepted to IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fashion recommender systems, each product usually consists of multiple\nsemantic attributes (e.g., sleeves, collar, etc). When making cloth decisions,\npeople usually show preferences for different semantic attributes (e.g., the\nclothes with v-neck collar). Nevertheless, most previous fashion recommendation\nmodels comprehend the clothing images with a global content representation and\nlack detailed understanding of users' semantic preferences, which usually leads\nto inferior recommendation performance. To bridge this gap, we propose a novel\nSemantic Attribute Explainable Recommender System (SAERS). Specifically, we\nfirst introduce a fine-grained interpretable semantic space. We then develop a\nSemantic Extraction Network (SEN) and Fine-grained Preferences Attention (FPA)\nmodule to project users and items into this space, respectively. With SAERS, we\nare capable of not only providing cloth recommendations for users, but also\nexplaining the reason why we recommend the cloth through intuitive visual\nattribute semantic highlights in a personalized manner. Extensive experiments\nconducted on real-world datasets clearly demonstrate the effectiveness of our\napproach compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 05:52:47 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 12:41:50 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Hou", "Min", ""], ["Wu", "Le", ""], ["Chen", "Enhong", ""], ["Li", "Zhi", ""], ["Zheng", "Vincent W.", ""], ["Liu", "Qi", ""]]}, {"id": "1905.12967", "submitter": "Marcel Kurovski", "authors": "Marcel Kurovski, Florian Wilhelm", "title": "On the Effectiveness of Low-rank Approximations for Collaborative\n  Filtering compared to Neural Networks", "comments": "5 pages, 3 figures, 2 tables, submitted as Short Paper to the 13th\n  ACM Conference on Recommender Systems (RecSys'19), source code available at\n  https://github.com/FlorianWilhelm/lrann", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even in times of deep learning, low-rank approximations by factorizing a\nmatrix into user and item latent factors continue to be a method of choice for\ncollaborative filtering tasks due to their great performance. While deep\nlearning based approaches excel in hybrid recommender tasks where additional\nfeatures for items, users or even context are available, their flexibility\nseems to rather impair the performance compared to low-rank approximations for\npure collaborative filtering tasks where no additional features are used.\nRecent works propose hybrid models combining low-rank approximations and\ntraditional deep neural architectures with promising results but fail to\nexplain why neural networks alone are unsuitable for this task. In this work,\nwe revisit the model and intuition behind low-rank approximation to point out\nits suitability for collaborative filtering tasks. In several experiments we\ncompare the performance and behavior of models based on a deep neural network\nand low-rank approximation to examine the reasons for the low effectiveness of\ntraditional deep neural networks. We conclude that the universal approximation\ncapabilities of traditional deep neural networks severely impair the\ndetermination of suitable latent vectors, leading to a worse performance\ncompared to low-rank approximations.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 11:22:21 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Kurovski", "Marcel", ""], ["Wilhelm", "Florian", ""]]}, {"id": "1905.13030", "submitter": "Jian Liu", "authors": "Jian Liu, Pengpeng Zhao, Yanchi Liu, Victor S. Sheng, Fuzheng Zhuang,\n  Jiajie Xu, Xiaofang Zhou and Hui Xiong", "title": "Deep Cross Networks with Aesthetic Preference for Cross-domain\n  Recommendation", "comments": "arXiv admin note: text overlap with arXiv:1901.07199,\n  arXiv:1804.06769 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When purchasing appearance-first products, e.g., clothes, product appearance\naesthetics plays an important role in the decision process. Moreover, user's\naesthetic preference, which can be regarded as a personality trait and a basic\nrequirement, is domain independent and could be used as a bridge between\ndomains for knowledge transfer. However, existing work has rarely considered\nthe aesthetic information in product photos for cross-domain recommendation. To\nthis end, in this paper, we propose a new deep Aesthetic preference\nCross-Domain Network (ACDN), in which parameters characterizing personal\naesthetic preferences are shared across networks to transfer knowledge between\ndomains. Specifically, we first leverage an aesthetic network to extract\nrelevant features. Then, we integrate the aesthetic features into a\ncross-domain network to transfer users' domain independent aesthetic\npreferences. Moreover, network cross-connections are introduced to enable dual\nknowledge transfer across domains. Finally, the experimental results on\nreal-world data show that our proposed ACDN outperforms other benchmark methods\nin terms of recommendation accuracy. The results also show that users'\naesthetic preferences are effective in alleviating the data sparsity issue on\nthe cross-domain recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 07:54:25 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Liu", "Jian", ""], ["Zhao", "Pengpeng", ""], ["Liu", "Yanchi", ""], ["Sheng", "Victor S.", ""], ["Zhuang", "Fuzheng", ""], ["Xu", "Jiajie", ""], ["Zhou", "Xiaofang", ""], ["Xiong", "Hui", ""]]}, {"id": "1905.13125", "submitter": "Ari Biswas", "authors": "Ari Biswas, Thai T Pham, Michael Vogelsong, Benjamin Snyder, Houssam\n  Nassif", "title": "Seeker: Real-Time Interactive Search", "comments": "This paper will appear in KDD 2019", "journal-ref": "Knowledge Discovery in Databases Conference (KDD'19), Anchorage,\n  Alaska, pp. 2867-2875, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces Seeker, a system that allows users to interactively\nrefine search rankings in real time, through feedback in the form of likes and\ndislikes. When searching online, users may not know how to accurately describe\ntheir product of choice in words. An alternative approach is to search an\nembedding space, allowing the user to query using a representation of the item\n(like a tune for a song, or a picture for an object). However, this approach\nrequires the user to possess an example representation of their desired item.\nAdditionally, most current search systems do not allow the user to dynamically\nadapt the results with further feedback. On the other hand, users often have a\nmental picture of the desired item and are able to answer ordinal questions of\nthe form: \"Is this item similar to what you have in mind?\" With this\nassumption, our algorithm allows for users to provide sequential feedback on\nsearch results to adapt the search feed. We show that our proposed approach\nworks well both qualitatively and quantitatively. Unlike most previous\nrepresentation-based search systems, we can quantify the quality of our\nalgorithm by evaluating humans-in-the-loop experiments.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 23:52:28 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Biswas", "Ari", ""], ["Pham", "Thai T", ""], ["Vogelsong", "Michael", ""], ["Snyder", "Benjamin", ""], ["Nassif", "Houssam", ""]]}, {"id": "1905.13126", "submitter": "Piper Armstrong", "authors": "Jeffrey Lund, Piper Armstrong, Wilson Fearn, Stephen Cowley, Courtni\n  Byun, Jordan Boyd-Graber, and Kevin Seppi", "title": "Automatic Evaluation of Local Topic Quality", "comments": "8 pages 4 figures 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are typically evaluated with respect to the global topic\ndistributions that they generate, using metrics such as coherence, but without\nregard to local (token-level) topic assignments. Token-level assignments are\nimportant for downstream tasks such as classification. Even recent models,\nwhich aim to improve the quality of these token-level topic assignments, have\nbeen evaluated only with respect to global metrics. We propose a task designed\nto elicit human judgments of token-level topic assignments. We use a variety of\ntopic model types and parameters and discover that global metrics agree poorly\nwith human assignments.\n  Since human evaluation is expensive we propose a variety of automated metrics\nto evaluate topic models at a local level. Finally, we correlate our proposed\nmetrics with human judgments from the task on several datasets. We show that an\nevaluation based on the percent of topic switches correlates most strongly with\nhuman judgment of local topic quality. We suggest that this new metric, which\nwe call consistency, be adopted alongside global metrics such as topic\ncoherence when evaluating new topic models.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 00:44:47 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Lund", "Jeffrey", ""], ["Armstrong", "Piper", ""], ["Fearn", "Wilson", ""], ["Cowley", "Stephen", ""], ["Byun", "Courtni", ""], ["Boyd-Graber", "Jordan", ""], ["Seppi", "Kevin", ""]]}, {"id": "1905.13127", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Cecilia Mascolo and Zhongxiang Zhao", "title": "Topic-Enhanced Memory Networks for Personalised Point-of-Interest\n  Recommendation", "comments": "11 pages, 6 figures, The 25th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '19)", "journal-ref": null, "doi": "10.1145/3292500.3330781", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-Interest (POI) recommender systems play a vital role in people's\nlives by recommending unexplored POIs to users and have drawn extensive\nattention from both academia and industry. Despite their value, however, they\nstill suffer from the challenges of capturing complicated user preferences and\nfine-grained user-POI relationship for spatio-temporal sensitive POI\nrecommendation. Existing recommendation algorithms, including both shallow and\ndeep approaches, usually embed the visiting records of a user into a single\nlatent vector to model user preferences: this has limited power of\nrepresentation and interpretability. In this paper, we propose a novel\ntopic-enhanced memory network (TEMN), a deep architecture to integrate the\ntopic model and memory network capitalising on the strengths of both the global\nstructure of latent patterns and local neighbourhood-based features in a\nnonlinear fashion. We further incorporate a geographical module to exploit\nuser-specific spatial preference and POI-specific spatial influence to enhance\nrecommendations. The proposed unified hybrid model is widely applicable to\nvarious POI recommendation scenarios. Extensive experiments on real-world\nWeChat datasets demonstrate its effectiveness (improvement ratio of 3.25% and\n29.95% for context-aware and sequential recommendation, respectively). Also,\nqualitative analysis of the attention weights and topic modeling provides\ninsight into the model's recommendation process and results.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 18:00:05 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Zhou", "Xiao", ""], ["Mascolo", "Cecilia", ""], ["Zhao", "Zhongxiang", ""]]}, {"id": "1905.13128", "submitter": "Olivier Gouvert Mr.", "authors": "Olivier Gouvert, Thomas Oberlin, C\\'edric F\\'evotte", "title": "Recommendation from Raw Data with Adaptive Compound Poisson\n  Factorization", "comments": "Accepted for publication at UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data are often used in recommender systems: they are widespread (song\nplay counts, product purchases, clicks on web pages) and can reveal user\npreference without any explicit rating from the user. Such data are known to be\nsparse, over-dispersed and bursty, which makes their direct use in recommender\nsystems challenging, often leading to pre-processing steps such as\nbinarization. The aim of this paper is to build recommender systems from these\nraw data, by means of the recently proposed compound Poisson Factorization\n(cPF). The paper contributions are three-fold: we present a unified framework\nfor discrete data (dcPF), leading to an adaptive and scalable algorithm; we\nshow that our framework achieves a trade-off between Poisson Factorization (PF)\napplied to raw and binarized data; we study four specific instances that are\nrelevant to recommendation and exhibit new links with combinatorics.\nExperiments with three different datasets show that dcPF is able to effectively\nadjust to over-dispersion, leading to better recommendation scores when\ncompared with PF on either raw or binarized data.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 15:00:49 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 09:58:51 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Gouvert", "Olivier", ""], ["Oberlin", "Thomas", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "1905.13129", "submitter": "Jiani Zhang", "authors": "Jiani Zhang, Xingjian Shi, Shenglin Zhao, Irwin King", "title": "STAR-GCN: Stacked and Reconstructed Graph Convolutional Networks for\n  Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new STAcked and Reconstructed Graph Convolutional Networks\n(STAR-GCN) architecture to learn node representations for boosting the\nperformance in recommender systems, especially in the cold start scenario.\nSTAR-GCN employs a stack of GCN encoder-decoders combined with intermediate\nsupervision to improve the final prediction performance. Unlike the graph\nconvolutional matrix completion model with one-hot encoding node inputs, our\nSTAR-GCN learns low-dimensional user and item latent factors as the input to\nrestrain the model space complexity. Moreover, our STAR-GCN can produce node\nembeddings for new nodes by reconstructing masked input node embeddings, which\nessentially tackles the cold start problem. Furthermore, we discover a label\nleakage issue when training GCN-based models for link prediction tasks and\npropose a training strategy to avoid the issue. Empirical results on multiple\nrating prediction benchmarks demonstrate our model achieves state-of-the-art\nperformance in four out of five real-world datasets and significant\nimprovements in predicting ratings in the cold start scenario. The code\nimplementation is available in https://github.com/jennyzhang0215/STAR-GCN.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 12:21:33 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Zhang", "Jiani", ""], ["Shi", "Xingjian", ""], ["Zhao", "Shenglin", ""], ["King", "Irwin", ""]]}, {"id": "1905.13130", "submitter": "Raehyun Kim", "authors": "Seoungjun Yun, Raehyun Kim, Miyoung Ko, Jaewoo Kang", "title": "SAIN: Self-Attentive Integration Network for Recommendation", "comments": "SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331342", "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing importance of personalized recommendation, numerous\nrecommendation models have been proposed recently. Among them, Matrix\nFactorization (MF) based models are the most widely used in the recommendation\nfield due to their high performance. However, MF based models suffer from cold\nstart problems where user-item interactions are sparse. To deal with this\nproblem, content based recommendation models which use the auxiliary attributes\nof users and items have been proposed. Since these models use auxiliary\nattributes, they are effective in cold start settings. However, most of the\nproposed models are either unable to capture complex feature interactions or\nnot properly designed to combine user-item feedback information with content\ninformation. In this paper, we propose Self-Attentive Integration Network\n(SAIN) which is a model that effectively combines user-item feedback\ninformation and auxiliary information for recommendation task. In SAIN, a\nself-attention mechanism is used in the feature-level interaction layer to\neffectively consider interactions between multiple features, while the\ninformation integration layer adaptively combines content and feedback\ninformation. The experimental results on two public datasets show that our\nmodel outperforms the state-of-the-art models by 2.13%\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:27:36 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 07:58:23 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Yun", "Seoungjun", ""], ["Kim", "Raehyun", ""], ["Ko", "Miyoung", ""], ["Kang", "Jaewoo", ""]]}, {"id": "1905.13131", "submitter": "Mathias Kraus", "authors": "Mathias Kraus, Stefan Feuerriegel", "title": "Personalized Purchase Prediction of Market Baskets with\n  Wasserstein-Based Sequence Matching", "comments": "Accepted for oral presentation at 25th ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining (KDD 2019)", "journal-ref": null, "doi": "10.1145/3292500.3330791", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization in marketing aims at improving the shopping experience of\ncustomers by tailoring services to individuals. In order to achieve this,\nbusinesses must be able to make personalized predictions regarding the next\npurchase. That is, one must forecast the exact list of items that will comprise\nthe next purchase, i.e., the so-called market basket. Despite its relevance to\nfirm operations, this problem has received surprisingly little attention in\nprior research, largely due to its inherent complexity. In fact,\nstate-of-the-art approaches are limited to intuitive decision rules for pattern\nextraction. However, the simplicity of the pre-coded rules impedes performance,\nsince decision rules operate in an autoregressive fashion: the rules can only\nmake inferences from past purchases of a single customer without taking into\naccount the knowledge transfer that takes place between customers. In contrast,\nour research overcomes the limitations of pre-set rules by contributing a novel\npredictor of market baskets from sequential purchase histories: our predictions\nare based on similarity matching in order to identify similar purchase habits\namong the complete shopping histories of all customers. Our contributions are\nas follows: (1) We propose similarity matching based on subsequential dynamic\ntime warping (SDTW) as a novel predictor of market baskets. Thereby, we can\neffectively identify cross-customer patterns. (2) We leverage the Wasserstein\ndistance for measuring the similarity among embedded purchase histories. (3) We\ndevelop a fast approximation algorithm for computing a lower bound of the\nWasserstein distance in our setting. An extensive series of computational\nexperiments demonstrates the effectiveness of our approach. The accuracy of\nidentifying the exact market baskets based on state-of-the-art decision rules\nfrom the literature is outperformed by a factor of 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 10:08:18 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 16:05:25 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kraus", "Mathias", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "1905.13132", "submitter": "Kevin Joseph", "authors": "Kevin Joseph, Hui Jiang", "title": "Content based News Recommendation via Shortest Entity Distance over\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based news recommendation systems need to recommend news articles\nbased on the topics and content of articles without using user specific\ninformation. Many news articles describe the occurrence of specific events and\nnamed entities including people, places or objects. In this paper, we propose a\ngraph traversal algorithm as well as a novel weighting scheme for cold-start\ncontent based news recommendation utilizing these named entities. Seeking to\ncreate a higher degree of user-specific relevance, our algorithm computes the\nshortest distance between named entities, across news articles, over a large\nknowledge graph. Moreover, we have created a new human annotated data set for\nevaluating content based news recommendation systems. Experimental results show\nour method is suitable to tackle the hard cold-start problem and it produces\nstronger Pearson correlation to human similarity scores than other cold-start\nmethods. Our method is also complementary and a combination with the\nconventional cold-start recommendation methods may yield significant\nperformance gains. The dataset, CNRec, is available at:\nhttps://github.com/kevinj22/CNRec\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 20:06:06 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Joseph", "Kevin", ""], ["Jiang", "Hui", ""]]}, {"id": "1905.13133", "submitter": "Kai-Lang Yao", "authors": "Kai-Lang Yao and Wu-Jun Li", "title": "Collaborative Self-Attention for Recommender Systems", "comments": "There are large modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS), which have been an essential part in a wide range\nof applications, can be formulated as a matrix completion (MC) problem. To\nboost the performance of MC, matrix completion with side information, called\ninductive matrix completion (IMC), was further proposed. In real applications,\nthe factorized version of IMC is more favored due to its efficiency of\noptimization and implementation. Regarding the factorized version, traditional\nIMC method can be interpreted as learning an individual representation for each\nfeature, which is independent from each other. Moreover, representations for\nthe same features are shared across all users/items. However, the independent\ncharacteristic for features and shared characteristic for the same features\nacross all users/items may limit the expressiveness of the model. The\nlimitation also exists in variants of IMC, such as deep learning based IMC\nmodels. To break the limitation, we generalize recent advances of\nself-attention mechanism to IMC and propose a context-aware model called\ncollaborative self-attention (CSA), which can jointly learn context-aware\nrepresentations for features and perform inductive matrix completion process.\nExtensive experiments on three large-scale datasets from real RS applications\ndemonstrate effectiveness of CSA.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:05:26 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:44:32 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Yao", "Kai-Lang", ""], ["Li", "Wu-Jun", ""]]}, {"id": "1905.13134", "submitter": "Meike Zehlike", "authors": "Meike Zehlike, Tom S\\\"uhr, Carlos Castillo and Ivan Kitanovski", "title": "FairSearch: A Tool For Fairness in Ranked Search Results", "comments": "4 pages, demo paper", "journal-ref": "Companion Proceedings of the Web Conference 2020 (WWW '20\n  Companion), April 20--24, 2020, Taipei, Taiwan", "doi": "10.1145/3366424.3383534", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked search results and recommendations have become the main mechanism by\nwhich we find content, products, places, and people online. With hiring,\nselecting, purchasing, and dating being increasingly mediated by algorithms,\nrankings may determine career and business opportunities, educational\nplacement, access to benefits, and even social and reproductive success. It is\ntherefore of societal and ethical importance to ask whether search results can\ndemote, marginalize, or exclude individuals of unprivileged groups or promote\nproducts with undesired features. In this paper we present FairSearch, the\nfirst fair open source search API to provide fairness notions in ranked search\nresults. We implement two algorithms from the fair ranking literature, namely\nFA*IR (Zehlike et al., 2017) and DELTR (Zehlike and Castillo, 2018) and provide\nthem as stand-alone libraries in Python and Java. Additionally we implement\ninterfaces to Elasticsearch for both algorithms, that use the aforementioned\nJava libraries and are then provided as Elasticsearch plugins. Elasticsearch is\na well-known search engine API based on Apache Lucene. With our plugins we\nenable search engine developers who wish to ensure fair search results of\ndifferent styles to easily integrate DELTR and FA*IR into their existing\nElasticsearch environment.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:59:45 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 08:52:55 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Zehlike", "Meike", ""], ["S\u00fchr", "Tom", ""], ["Castillo", "Carlos", ""], ["Kitanovski", "Ivan", ""]]}, {"id": "1905.13136", "submitter": "Amber Nigam", "authors": "Amber Nigam, Aakash Roy, Arpan Saxena, and Hartaran Singh", "title": "Job Recommendation through Progression of Job Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Job recommendation has traditionally been treated as a filter-based match or\nas a recommendation based on the features of jobs and candidates as discrete\nentities. In this paper, we introduce a methodology where we leverage the\nprogression of job selection by candidates using machine learning.\nAdditionally, our recommendation is composed of several other\nsub-recommendations that contribute to at least one of a) making\nrecommendations serendipitous for the end user b) overcoming cold-start for\nboth candidates and jobs. One of the unique selling propositions of our\nmethodology is the way we have used skills as embedded features and derived\nlatent competencies from them, thereby attempting to expand the skills of\ncandidates and jobs to achieve more coverage in the skill domain. We have\ndeployed our model in a real-world job recommender system and have achieved the\nbest click-through rate through a blended approach of machine-learned\nrecommendations and other sub-recommendations. For recommending jobs through\nmachine learning that forms a significant part of our recommendation, we\nachieve the best results through Bi-LSTM with attention.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:36:48 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 18:24:31 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Nigam", "Amber", ""], ["Roy", "Aakash", ""], ["Saxena", "Arpan", ""], ["Singh", "Hartaran", ""]]}, {"id": "1905.13160", "submitter": "Wenqi Fan", "authors": "Wenqi Fan, Tyler Derr, Yao Ma, Jianping Wang, Jiliang Tang and Qing Li", "title": "Deep Adversarial Social Recommendation", "comments": "Accepted by International Joint Conference on Artificial Intelligence\n  (IJCAI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed rapid developments on social recommendation\ntechniques for improving the performance of recommender systems due to the\ngrowing influence of social networks to our daily life. The majority of\nexisting social recommendation methods unify user representation for the\nuser-item interactions (item domain) and user-user connections (social domain).\nHowever, it may restrain user representation learning in each respective\ndomain, since users behave and interact differently in the two domains, which\nmakes their representations to be heterogeneous. In addition, most of\ntraditional recommender systems can not efficiently optimize these objectives,\nsince they utilize negative sampling technique which is unable to provide\nenough informative guidance towards the training during the optimization\nprocess. In this paper, to address the aforementioned challenges, we propose a\nnovel deep adversarial social recommendation framework DASO. It adopts a\nbidirectional mapping method to transfer users' information between social\ndomain and item domain using adversarial learning. Comprehensive experiments on\ntwo real-world datasets show the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:32:19 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Fan", "Wenqi", ""], ["Derr", "Tyler", ""], ["Ma", "Yao", ""], ["Wang", "Jianping", ""], ["Tang", "Jiliang", ""], ["Li", "Qing", ""]]}, {"id": "1905.13339", "submitter": "Pranav Aggarwal", "authors": "Pranav Aggarwal, Zhe Lin, Baldo Faieta, Saeid Motiian", "title": "Multitask Text-to-Visual Embedding with Titles and Clickthrough Data", "comments": "4 pages. Language and Vision Workshop, in conjunction with CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Text-visual (or called semantic-visual) embedding is a central problem in\nvision-language research. It typically involves mapping of an image and a text\ndescription to a common feature space through a CNN image encoder and a RNN\nlanguage encoder. In this paper, we propose a new method for learning\ntext-visual embedding using both image titles and click-through data from an\nimage search engine. We also propose a new triplet loss function by modeling\npositive awareness of the embedding, and introduce a novel mini-batch-based\nhard negative sampling approach for better data efficiency in the learning\nprocess. Experimental results show that our proposed method outperforms\nexisting methods, and is also effective for real-world text-to-visual\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 22:33:15 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Aggarwal", "Pranav", ""], ["Lin", "Zhe", ""], ["Faieta", "Baldo", ""], ["Motiian", "Saeid", ""]]}, {"id": "1905.13350", "submitter": "Sabine Wehnert", "authors": "Sabine Wehnert and Sayed Anisul Hoque and Wolfram Fenske and Gunter\n  Saake", "title": "Threshold-Based Retrieval and Textual Entailment Detection on Legal Bar\n  Exam Questions", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Getting an overview over the legal domain has become challenging, especially\nin a broad, international context. Legal question answering systems have the\npotential to alleviate this task by automatically retrieving relevant legal\ntexts for a specific statement and checking whether the meaning of the\nstatement can be inferred from the found documents. We investigate a\ncombination of the BM25 scoring method of Elasticsearch with word embeddings\ntrained on English translations of the German and Japanese civil law. For this,\nwe define criteria which select a dynamic number of relevant documents\naccording to threshold scores. Exploiting two deep learning classifiers and\ntheir respective prediction bias with a threshold-based answer inclusion\ncriterion has shown to be beneficial for the textual entailment task, when\ncompared to the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 23:17:26 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Wehnert", "Sabine", ""], ["Hoque", "Sayed Anisul", ""], ["Fenske", "Wolfram", ""], ["Saake", "Gunter", ""]]}, {"id": "1905.13538", "submitter": "Guillaume Jaume", "authors": "Guillaume Jaume, Hazim Kemal Ekenel, Jean-Philippe Thiran", "title": "FUNSD: A Dataset for Form Understanding in Noisy Scanned Documents", "comments": "ICDAR'19 OST workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset for form understanding in noisy scanned documents\n(FUNSD) that aims at extracting and structuring the textual content of forms.\nThe dataset comprises 199 real, fully annotated, scanned forms. The documents\nare noisy and vary widely in appearance, making form understanding (FoUn) a\nchallenging task. The proposed dataset can be used for various tasks, including\ntext detection, optical character recognition, spatial layout analysis, and\nentity labeling/linking. To the best of our knowledge, this is the first\npublicly available dataset with comprehensive annotations to address FoUn task.\nWe also present a set of baselines and introduce metrics to evaluate\nperformance on the FUNSD dataset, which can be downloaded at\nhttps://guillaumejaume.github.io/FUNSD/.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:40:40 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:46:39 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Jaume", "Guillaume", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1905.13612", "submitter": "Dimitrios Rafailidis Dr", "authors": "Dimitrios Rafailidis", "title": "Leveraging Trust and Distrust in Recommender Systems via Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data scarcity of user preferences and the cold-start problem often appear\nin real-world applications and limit the recommendation accuracy of\ncollaborative filtering strategies. Leveraging the selections of social friends\nand foes can efficiently face both problems. In this study, we propose a\nstrategy that performs social deep pairwise learning. Firstly, we design a\nranking loss function incorporating multiple ranking criteria based on the\nchoice in users, and the choice in their friends and foes to improve the\naccuracy in the top-k recommendation task. We capture the nonlinear\ncorrelations between user preferences and the social information of trust and\ndistrust relationships via a deep learning strategy. In each backpropagation\nstep, we follow a social negative sampling strategy to meet the multiple\nranking criteria of our ranking loss function. We conduct comprehensive\nexperiments on a benchmark dataset from Epinions, among the largest publicly\navailable that has been reported in the relevant literature. The experimental\nresults demonstrate that the proposed model beats other state-of-the art\nmethods, attaining an 11.49% average improvement over the most competitive\nmodel. We show that our deep learning strategy plays an important role in\ncapturing the nonlinear correlations between user preferences and the social\ninformation of trust and distrust relationships, and demonstrate the importance\nof our social negative sampling strategy on the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:35:34 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Rafailidis", "Dimitrios", ""]]}]