[{"id": "1704.00016", "submitter": "Esra Akbas", "authors": "Esra Akbas", "title": "Opinion Mining on Non-English Short Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the type and the number of such venues increase, automated analysis of\nsentiment on textual resources has become an essential data mining task. In\nthis paper, we investigate the problem of mining opinions on the collection of\ninformal short texts. Both positive and negative sentiment strength of texts\nare detected. We focus on a non-English language that has few resources for\ntext mining. This approach would help enhance the sentiment analysis in\nlanguages where a list of opinionated words does not exist. We propose a new\nmethod projects the text into dense and low dimensional feature vectors\naccording to the sentiment strength of the words. We detect the mixture of\npositive and negative sentiments on a multi-variant scale. Empirical evaluation\nof the proposed framework on Turkish tweets shows that our approach gets good\nresults for opinion mining.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 18:05:44 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 01:51:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Akbas", "Esra", ""]]}, {"id": "1704.00156", "submitter": "Joeran Beel", "authors": "Joeran Beel, Siddharth Dinesh", "title": "Real-World Recommender Systems for Academia: The Pain and Gain in\n  Building, Operating, and Researching them [Long Version]", "comments": "This article is a long version of the article published in the\n  Proceedings of the 5th International Workshop on Bibliometric-enhanced\n  Information Retrieval (BIR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on recommender systems is a challenging task, as is building and\noperating such systems. Major challenges include non-reproducible research\nresults, dealing with noisy data, and answering many questions such as how many\nrecommendations to display, how often, and, of course, how to generate\nrecommendations most effectively. In the past six years, we built three\nresearch-article recommender systems for digital libraries and reference\nmanagers, and conducted research on these systems. In this paper, we share some\nexperiences we made during that time. Among others, we discuss the required\nskills to build recommender systems, and why the literature provides little\nhelp in identifying promising recommendation approaches. We explain the\nchallenge in creating a randomization engine to run A/B tests, and how low data\nquality impacts the calculation of bibliometrics. We further discuss why\nseveral of our experiments delivered disappointing results, and provide\nstatistics on how many researchers showed interest in our recommendation\ndataset.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 11:36:26 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Beel", "Joeran", ""], ["Dinesh", "Siddharth", ""]]}, {"id": "1704.00393", "submitter": "Felix Beierle", "authors": "Felix Beierle, Akiko Aizawa, Joeran Beel", "title": "Exploring Choice Overload in Related-Article Recommendations in Digital\n  Libraries", "comments": "Accepted for publication at the 5th International Workshop on\n  Bibliometric-enhanced Information Retrieval (BIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of choice overload - the difficulty of making a\ndecision when faced with many options - when displaying related-article\nrecommendations in digital libraries. So far, research regarding to how many\nitems should be displayed has mostly been done in the fields of media\nrecommendations and search engines. We analyze the number of recommendations in\ncurrent digital libraries. When browsing fullscreen with a laptop or desktop\nPC, all display a fixed number of recommendations. 72% display three, four, or\nfive recommendations, none display more than ten. We provide results from an\nempirical evaluation conducted with GESIS' digital library Sowiport, with\nrecommendations delivered by recommendations-as-a-service provider Mr. DLib. We\nuse click-through rate as a measure of recommendation effectiveness based on\n3.4 million delivered recommendations. Our results show lower click-through\nrates for higher numbers of recommendations and twice as many clicked\nrecommendations when displaying ten instead of one related-articles. Our\nresults indicate that users might quickly feel overloaded by choice.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 00:30:25 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Beierle", "Felix", ""], ["Aizawa", "Akiko", ""], ["Beel", "Joeran", ""]]}, {"id": "1704.00551", "submitter": "Shuai Zhang", "authors": "Shuai Zhang, Lina Yao and Xiwei Xu", "title": "AutoSVD++: An Efficient Hybrid Collaborative Filtering Model via\n  Contractive Auto-encoders", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3077136.3080689", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) has been successfully used to provide users with\npersonalized products and services. However, dealing with the increasing\nsparseness of user-item matrix still remains a challenge. To tackle such issue,\nhybrid CF such as combining with content based filtering and leveraging side\ninformation of users and items has been extensively studied to enhance\nperformance. However, most of these approaches depend on hand-crafted feature\nengineering, which are usually noise-prone and biased by different feature\nextraction and selection schemes. In this paper, we propose a new hybrid model\nby generalizing contractive auto-encoder paradigm into matrix factorization\nframework with good scalability and computational efficiency, which jointly\nmodel content information as representations of effectiveness and compactness,\nand leverage implicit user feedback to make accurate recommendations. Extensive\nexperiments conducted over three large scale real datasets indicate the\nproposed approach outperforms the compared methods for item recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 12:39:25 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 00:17:14 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 01:01:30 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Zhang", "Shuai", ""], ["Yao", "Lina", ""], ["Xu", "Xiwei", ""]]}, {"id": "1704.00656", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga, Ahmet Aker, Kalina Bontcheva, Maria Liakata, Rob\n  Procter", "title": "Detection and Resolution of Rumours in Social Media: A Survey", "comments": "ACM Computing Surveys", "journal-ref": "ACM Computing Surveys 51, 2, Article 32 (February 2018), 36 pages", "doi": "10.1145/3161603", "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the increasing use of social media platforms for information and news\ngathering, its unmoderated nature often leads to the emergence and spread of\nrumours, i.e. pieces of information that are unverified at the time of posting.\nAt the same time, the openness of social media platforms provides opportunities\nto study how users share and discuss rumours, and to explore how natural\nlanguage processing and data mining techniques may be used to find ways of\ndetermining their veracity. In this survey we introduce and discuss two types\nof rumours that circulate on social media; long-standing rumours that circulate\nfor long periods of time, and newly-emerging rumours spawned during fast-paced\nevents such as breaking news, where reports are released piecemeal and often\nwith an unverified status in their early stages. We provide an overview of\nresearch into social media rumours with the ultimate goal of developing a\nrumour classification system that consists of four components: rumour\ndetection, rumour tracking, rumour stance classification and rumour veracity\nclassification. We delve into the approaches presented in the scientific\nliterature for the development of each of these four components. We summarise\nthe efforts and achievements so far towards the development of rumour\nclassification systems and conclude with suggestions for avenues for future\nresearch in social media mining for detection and resolution of rumours.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:57:44 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 20:28:29 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 08:24:46 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zubiaga", "Arkaitz", ""], ["Aker", "Ahmet", ""], ["Bontcheva", "Kalina", ""], ["Liakata", "Maria", ""], ["Procter", "Rob", ""]]}, {"id": "1704.01213", "submitter": "Pantelis Pipergias Analytis", "authors": "Pantelis P. Analytis, Alexia Delfino, Juliane K\\\"ammer, Mehdi\n  Moussa\\\"id and Thorsten Joachims", "title": "Ranking with social cues: Integrating online review scores and\n  popularity information", "comments": "4 pages, 3 figures, ICWSM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online marketplaces, search engines, and databases employ aggregated social\ninformation to rank their content for users. Two ranking heuristics commonly\nimplemented to order the available options are the average review score and\nitem popularity-that is, the number of users who have experienced an item.\nThese rules, although easy to implement, only partly reflect actual user\npreferences, as people may assign values to both average scores and popularity\nand trade off between the two. How do people integrate these two pieces of\nsocial information when making choices? We present two experiments in which we\nasked participants to choose 200 times among options drawn directly from two\nwidely used online venues: Amazon and IMDb. The only information presented to\nparticipants was the average score and the number of reviews, which served as a\nproxy for popularity. We found that most people are willing to settle for items\nwith somewhat lower average scores if they are more popular. Yet, our study\nuncovered substantial diversity of preferences among participants, which\nindicates a sizable potential for personalizing ranking schemes that rely on\nsocial information.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 23:01:17 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 23:59:31 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Analytis", "Pantelis P.", ""], ["Delfino", "Alexia", ""], ["K\u00e4mmer", "Juliane", ""], ["Moussa\u00efd", "Mehdi", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1704.01599", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Wei Lu", "title": "Rhetorical relations for information retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, every part in most coherent text has some plausible reason for its\npresence, some function that it performs to the overall semantics of the text.\nRhetorical relations, e.g. contrast, cause, explanation, describe how the parts\nof a text are linked to each other. Knowledge about this socalled discourse\nstructure has been applied successfully to several natural language processing\ntasks. This work studies the use of rhetorical relations for Information\nRetrieval (IR): Is there a correlation between certain rhetorical relations and\nretrieval performance? Can knowledge about a document's rhetorical relations be\nuseful to IR? We present a language model modification that considers\nrhetorical relations when estimating the relevance of a document to a query.\nEmpirical evaluation of different versions of our model on TREC settings shows\nthat certain rhetorical relations can benefit retrieval effectiveness notably\n(> 10% in mean average precision over a state-of-the-art baseline).\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:21:39 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Lu", "Wei", ""]]}, {"id": "1704.01603", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Peter Ingwersen", "title": "Preliminary Experiments using Subjective Logic for the\n  Polyrepresentation of Information Needs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the principle of polyrepresentation, retrieval accuracy may\nimprove through the combination of multiple and diverse information object\nrepresentations about e.g. the context of the user, the information sought, or\nthe retrieval system. Recently, the principle of polyrepresentation was\nmathematically expressed using subjective logic, where the potential\nsuitability of each representation for improving retrieval performance was\nformalised through degrees of belief and uncertainty. No experimental evidence\nor practical application has so far validated this model. We extend the work of\nLioma et al. (2010), by providing a practical application and analysis of the\nmodel. We show how to map the abstract notions of belief and uncertainty to\nreal-life evidence drawn from a retrieval dataset. We also show how to estimate\ntwo different types of polyrepresentation assuming either (a) independence or\n(b) dependence between the information objects that are combined. We focus on\nthe polyrepresentation of different types of context relating to user\ninformation needs (i.e. work task, user background knowledge, ideal answer) and\nshow that the subjective logic model can predict their optimal combination\nprior and independently to the retrieval process.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:45:31 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Ingwersen", "Peter", ""]]}, {"id": "1704.01610", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Birger Larsen and Hinrich Sch\\\"utze and Peter\n  Ingwersen", "title": "A Subjective Logic Formalisation of the Principle of Polyrepresentation\n  for Information Needs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Information Retrieval refers to the branch of Information\nRetrieval that considers the retrieval process with respect to a wide range of\ncontexts, which may affect the user's information seeking experience. The\nidentification and representation of such contexts has been the object of the\nprinciple of Polyrepresentation, a theoretical framework for reasoning about\ndifferent representations arising from interactive information retrieval in a\ngiven context. Although the principle of Polyrepresentation has received\nattention from many researchers, not much empirical work has been done based on\nit. One reason may be that it has not yet been formalised mathematically. In\nthis paper we propose an up-to-date and exible mathematical formalisation of\nthe principle of Polyrepresentation for information needs. Specifically, we\napply Subjective Logic to model different representations of information needs\nas beliefs marked by degrees of uncertainty. We combine such beliefs using\ndifferent logical operators, and we discuss these combinations with respect to\ndifferent retrieval scenarios and situations. A formal model is introduced and\ndiscussed, with illustrative applications to the modelling of information\nneeds.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 18:55:37 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lioma", "Christina", ""], ["Larsen", "Birger", ""], ["Sch\u00fctze", "Hinrich", ""], ["Ingwersen", "Peter", ""]]}, {"id": "1704.01617", "submitter": "Christina Lioma Assoc. Prof", "authors": "Christina Lioma and Roi Blanco", "title": "Part of Speech Based Term Weighting for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic language processing tools typically assign to terms so-called\nweights corresponding to the contribution of terms to information content.\nTraditionally, term weights are computed from lexical statistics, e.g., term\nfrequencies. We propose a new type of term weight that is computed from part of\nspeech (POS) n-gram statistics. The proposed POS-based term weight represents\nhow informative a term is in general, based on the POS contexts in which it\ngenerally occurs in language. We suggest five different computations of\nPOS-based term weights by extending existing statistical approximations of term\ninformation measures. We apply these POS-based term weights to information\nretrieval, by integrating them into the model that matches documents to\nqueries. Experiments with two TREC collections and 300 queries, using TF-IDF &\nBM25 as baselines, show that integrating our POS-based term weights to\nretrieval always leads to gains (up to +33.7% from the baseline). Additional\nexperiments with a different retrieval model as baseline (Language Model with\nDirichlet priors smoothing) and our best performing POS-based term weight, show\nretrieval gains always and consistently across the whole smoothing range of the\nbaseline.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 19:16:58 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lioma", "Christina", ""], ["Blanco", "Roi", ""]]}, {"id": "1704.01754", "submitter": "Tuan N.A. Hoang", "authors": "Tuan Hoang, Thanh-Toan Do, Dang-Khoa Le Tan and Ngai-Man Cheung", "title": "Enhance Feature Discrimination for Unsupervised Hashing", "comments": "Accepted to ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to improve unsupervised hashing. Specifically,\nwe propose a very efficient embedding method: Gaussian Mixture Model embedding\n(Gemb). The proposed method, using Gaussian Mixture Model, embeds feature\nvector into a low-dimensional vector and, simultaneously, enhances the\ndiscriminative property of features before passing them into hashing. Our\nexperiment shows that the proposed method boosts the hashing performance of\nmany state-of-the-art, e.g. Binary Autoencoder (BA) [1], Iterative Quantization\n(ITQ) [2], in standard evaluation metrics for the three main benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 08:58:39 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 03:30:42 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Hoang", "Tuan", ""], ["Do", "Thanh-Toan", ""], ["Tan", "Dang-Khoa Le", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1704.01845", "submitter": "Christina Lioma Assoc. Prof", "authors": "Birger Larsen and Christina Lioma and Arjen de Vries", "title": "Report on TBAS 2012: Workshop on Task-Based and Aggregated Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ECIR half-day workshop on Task-Based and Aggregated Search (TBAS) was\nheld in Barcelona, Spain on 1 April 2012. The program included a keynote talk\nby Professor Jarvelin, six full paper presentations, two poster presentations,\nand an interactive discussion among the approximately 25 participants. This\nreport overviews the aims and contents of the workshop and outlines the major\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 13:56:31 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Larsen", "Birger", ""], ["Lioma", "Christina", ""], ["de Vries", "Arjen", ""]]}, {"id": "1704.01851", "submitter": "Christina Lioma Assoc. Prof", "authors": "Wei Lu and Qikai Cheng and Christina Lioma", "title": "Fixed versus Dynamic Co-Occurrence Windows in TextRank Term Weights for\n  Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TextRank is a variant of PageRank typically used in graphs that represent\ndocuments, and where vertices denote terms and edges denote relations between\nterms. Quite often the relation between terms is simple term co-occurrence\nwithin a fixed window of k terms. The output of TextRank when applied\niteratively is a score for each vertex, i.e. a term weight, that can be used\nfor information retrieval (IR) just like conventional term frequency based term\nweights. So far, when computing TextRank term weights over co- occurrence\ngraphs, the window of term co-occurrence is al- ways ?xed. This work departs\nfrom this, and considers dy- namically adjusted windows of term co-occurrence\nthat fol- low the document structure on a sentence- and paragraph- level. The\nresulting TextRank term weights are used in a ranking function that re-ranks\n1000 initially returned search results in order to improve the precision of the\nranking. Ex- periments with two IR collections show that adjusting the vicinity\nof term co-occurrence when computing TextRank term weights can lead to gains in\nearly precision.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:07:06 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Lu", "Wei", ""], ["Cheng", "Qikai", ""], ["Lioma", "Christina", ""]]}, {"id": "1704.01889", "submitter": "Farhan Khawar", "authors": "Farhan Khawar, Nevin L. Zhang", "title": "Conformative Filtering for Implicit Feedback Data", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-15712-8_11", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit feedback is the simplest form of user feedback that can be used for\nitem recommendation. It is easy to collect and is domain independent. However,\nthere is a lack of negative examples. Previous work tackles this problem by\nassuming that users are not interested or not as much interested in the\nunconsumed items. Those assumptions are often severely violated since\nnon-consumption can be due to factors like unawareness or lack of resources.\nTherefore, non-consumption by a user does not always mean disinterest or\nirrelevance. In this paper, we propose a novel method called Conformative\nFiltering (CoF) to address the issue. The motivating observation is that if\nthere is a large group of users who share the same taste and none of them have\nconsumed an item before, then it is likely that the item is not of interest to\nthe group. We perform multidimensional clustering on implicit feedback data\nusing hierarchical latent tree analysis (HLTA) to identify user `tastes' groups\nand make recommendations for a user based on her memberships in the groups and\non the past behavior of the groups. Experiments on two real-world datasets from\ndifferent domains show that CoF has superior performance compared to several\ncommon baselines.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 15:31:48 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 18:17:56 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1704.02090", "submitter": "Xian-Ling Mao", "authors": "Yi-Kun Tang, Xian-Ling Mao, Heyan Huang, Guihua Wen", "title": "Conceptualization Topic Modeling", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, topic modeling has been widely used to discover the abstract topics\nin text corpora. Most of the existing topic models are based on the assumption\nof three-layer hierarchical Bayesian structure, i.e. each document is modeled\nas a probability distribution over topics, and each topic is a probability\ndistribution over words. However, the assumption is not optimal. Intuitively,\nit's more reasonable to assume that each topic is a probability distribution\nover concepts, and then each concept is a probability distribution over words,\ni.e. adding a latent concept layer between topic layer and word layer in\ntraditional three-layer assumption. In this paper, we verify the proposed\nassumption by incorporating the new assumption in two representative topic\nmodels, and obtain two novel topic models. Extensive experiments were conducted\namong the proposed models and corresponding baselines, and the results show\nthat the proposed models significantly outperform the baselines in terms of\ncase study and perplexity, which means the new assumption is more reasonable\nthan traditional one.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 05:12:38 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Tang", "Yi-Kun", ""], ["Mao", "Xian-Ling", ""], ["Huang", "Heyan", ""], ["Wen", "Guihua", ""]]}, {"id": "1704.02216", "submitter": "Ashkan Esmaeili", "authors": "Ali Mottaghi, Kayhan Behdin, Ashkan Esmaeili, Mohammadreza Heydari,\n  and Farokh Marvasti", "title": "OBTAIN: Real-Time Beat Tracking in Audio Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we design a system in order to perform the real-time beat\ntracking for an audio signal. We use Onset Strength Signal (OSS) to detect the\nonsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal\n(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak\ndetection by extracting the periodic sequence of beats among all CBSS peaks. In\nsimulations, we can see that our proposed algorithm, Online Beat TrAckINg\n(OBTAIN), outperforms state-of-art results in terms of prediction accuracy\nwhile maintaining comparable and practical computational complexity. The\nreal-time performance is tractable visually as illustrated in the simulations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:15:15 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:36:55 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mottaghi", "Ali", ""], ["Behdin", "Kayhan", ""], ["Esmaeili", "Ashkan", ""], ["Heydari", "Mohammadreza", ""], ["Marvasti", "Farokh", ""]]}, {"id": "1704.02298", "submitter": "Rose Catherine", "authors": "Rose Catherine, William Cohen", "title": "TransNets: Learning to Transform for Recommendation", "comments": "Accepted for publication in the 11th ACM Conference on Recommender\n  Systems (RecSys 2017)", "journal-ref": null, "doi": "10.1145/3109859.3109878", "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning methods have been shown to improve the performance of\nrecommender systems over traditional methods, especially when review text is\navailable. For example, a recent model, DeepCoNN, uses neural nets to learn one\nlatent representation for the text of all reviews written by a target user, and\na second latent representation for the text of all reviews for a target item,\nand then combines these latent representations to obtain state-of-the-art\nperformance on recommendation tasks. We show that (unsurprisingly) much of the\npredictive value of review text comes from reviews of the target user for the\ntarget item. We then introduce a way in which this information can be used in\nrecommendation, even when the target user's review for the target item is not\navailable. Our model, called TransNets, extends the DeepCoNN model by\nintroducing an additional latent layer representing the target user-target item\npair. We then regularize this layer, at training time, to be similar to another\nlatent representation of the target user's review of the target item. We show\nthat TransNets and extensions of it improve substantially over the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 17:13:03 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 15:14:22 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Catherine", "Rose", ""], ["Cohen", "William", ""]]}, {"id": "1704.02552", "submitter": "Yubo Zhou", "authors": "Yubo Zhou, Ali Nadaf", "title": "Embedded Collaborative Filtering for \"Cold Start\" Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using only implicit data, many recommender systems fail in general to provide\na precise set of recommendations to users with limited interaction history.\nThis issue is regarded as the \"Cold Start\" problem and is typically resolved by\nswitching to content-based approaches where extra costly information is\nrequired. In this paper, we use a dimensionality reduction algorithm, Word2Vec\n(W2V), originally applied in Natural Language Processing problems under the\nframework of Collaborative Filtering (CF) to tackle the \"Cold Start\" problem\nusing only implicit data. This combined method is named Embedded Collaborative\nFiltering (ECF). An experiment is conducted to determine the performance of ECF\non two different implicit data sets. We show that the ECF approach outperforms\nother popular and state-of-the-art approaches in \"Cold Start\" scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 01:24:32 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Zhou", "Yubo", ""], ["Nadaf", "Ali", ""]]}, {"id": "1704.03289", "submitter": "Etienne Papegnies", "authors": "Etienne Papegnies (LIA), Vincent Labatut (LIA), Richard Dufour (LIA),\n  Georges Linares (LIA)", "title": "Impact Of Content Features For Automatic Online Abuse Detection", "comments": null, "journal-ref": "International Conference on Computational Linguistics and\n  Intelligent Text Processing (CICLing), Apr 2017, Budapest, Hungary", "doi": "10.1007/978-3-319-77116-8_30", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online communities have gained considerable importance in recent years due to\nthe increasing number of people connected to the Internet. Moderating user\ncontent in online communities is mainly performed manually, and reducing the\nworkload through automatic methods is of great financial interest for community\nmaintainers. Often, the industry uses basic approaches such as bad words\nfiltering and regular expression matching to assist the moderators. In this\narticle, we consider the task of automatically determining if a message is\nabusive. This task is complex since messages are written in a non-standardized\nway, including spelling errors, abbreviations, community-specific codes...\nFirst, we evaluate the system that we propose using standard features of online\nmessages. Then, we evaluate the impact of the addition of pre-processing\nstrategies, as well as original specific features developed for the community\nof an online in-browser strategy game. We finally propose to analyze the\nusefulness of this wide range of features using feature selection. This work\ncan lead to two possible applications: 1) automatically flag potentially\nabusive messages to draw the moderator's attention on a narrow subset of\nmessages ; and 2) fully automate the moderation process by deciding whether a\nmessage is abusive without any human intervention.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 13:59:33 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Papegnies", "Etienne", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Dufour", "Richard", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1704.03407", "submitter": "Hwiyeol Jo", "authors": "Hwiyeol Jo, Soo-Min Kim, Jeong Ryu", "title": "What we really want to find by Sentiment Analysis: The Relationship\n  between Computational Models and Psychological State", "comments": "Paper version of \"Psychological State in Text: A Limitation of\n  Sentiment Analysis.\". arXiv admin note: text overlap with arXiv:1607.03707", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the first step to model emotional state of a person, we build sentiment\nanalysis models with existing deep neural network algorithms and compare the\nmodels with psychological measurements to enlighten the relationship. In the\nexperiments, we first examined psychological state of 64 participants and asked\nthem to summarize the story of a book, Chronicle of a Death Foretold (Marquez,\n1981). Secondly, we trained models using crawled 365,802 movie review data;\nthen we evaluated participants' summaries using the pretrained model as a\nconcept of transfer learning. With the background that emotion affects on\nmemories, we investigated the relationship between the evaluation score of the\nsummaries from computational models and the examined psychological\nmeasurements. The result shows that although CNN performed the best among other\ndeep neural network algorithms (LSTM, GRU), its results are not related to the\npsychological state. Rather, GRU shows more explainable results depending on\nthe psychological state. The contribution of this paper can be summarized as\nfollows: (1) we enlighten the relationship between computational models and\npsychological measurements. (2) we suggest this framework as objective methods\nto evaluate the emotion; the real sentiment analysis of a person.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:42:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 08:53:42 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jo", "Hwiyeol", ""], ["Kim", "Soo-Min", ""], ["Ryu", "Jeong", ""]]}, {"id": "1704.03507", "submitter": "Jing Yang", "authors": "Jing Yang and Carsten Eickhoff", "title": "Unsupervised Learning of Parsimonious General-Purpose Embeddings for\n  User and Location Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social network applications depend on robust representations of\nspatio-temporal data. In this work, we present an embedding model based on\nfeed-forward neural networks which transforms social media check-ins into dense\nfeature vectors encoding geographic, temporal, and functional aspects for\nmodelling places, neighborhoods, and users. We employ the embedding model in a\nvariety of applications including location recommendation, urban functional\nzone study, and crime prediction. For location recommendation, we propose a\nSpatio-Temporal Embedding Similarity algorithm (STES) based on the embedding\nmodel.\n  In a range of experiments on real life data collected from Foursquare, we\ndemonstrate our model's effectiveness at characterizing places and people and\nits applicability in aforementioned problem domains. Finally, we select eight\nmajor cities around the globe and verify the robustness and generality of our\nmodel by porting pre-trained models from one city to another, thereby\nalleviating the need for costly local training.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 19:35:13 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 10:28:48 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Yang", "Jing", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "1704.03543", "submitter": "Peter Turney", "authors": "Peter D. Turney", "title": "Leveraging Term Banks for Answering Complex Questions: A Case for Sparse\n  Vectors", "comments": "Related datasets can be found at http://allenai.org/data.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While open-domain question answering (QA) systems have proven effective for\nanswering simple questions, they struggle with more complex questions. Our goal\nis to answer more complex questions reliably, without incurring a significant\ncost in knowledge resource construction to support the QA. One readily\navailable knowledge resource is a term bank, enumerating the key concepts in a\ndomain. We have developed an unsupervised learning approach that leverages a\nterm bank to guide a QA system, by representing the terminological knowledge\nwith thousands of specialized vector spaces. In experiments with complex\nscience questions, we show that this approach significantly outperforms several\nstate-of-the-art QA systems, demonstrating that significant leverage can be\ngained from continuous vector representations of domain terminology.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 21:21:39 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Turney", "Peter D.", ""]]}, {"id": "1704.03624", "submitter": "Sudheesh Singanamalla", "authors": "Sudheesh Singanamalla, Michael Peter Christen", "title": "Loklak - A Distributed Crawler and Data Harvester for Overcoming Rate\n  Limits", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social networks have become sources for vast quantities of data.\nHaving access to such big data can be very useful for various researchers and\ndata scientists. In this paper we describe Loklak, an open source distributed\npeer to peer crawler and scraper for supporting such research on platforms like\nTwitter, Weibo and other social networks. Social networks such as Twitter and\nWeibo pose various limitations to the user on the rate at which one could\nfreely collect such data for research. Our crawler enables researchers to\ncontinuously collect data while overcoming the barriers of authentication and\nrate limits imposed to provide a repository of open data as a service.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 05:36:28 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Singanamalla", "Sudheesh", ""], ["Christen", "Michael Peter", ""]]}, {"id": "1704.03755", "submitter": "Ronan Sicre", "authors": "Ronan Sicre, Yannis Avrithis, Ewa Kijak, Frederic Jurie", "title": "Unsupervised part learning for visual recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based image classification aims at representing categories by small sets\nof learned discriminative parts, upon which an image representation is built.\nConsidered as a promising avenue a decade ago, this direction has been\nneglected since the advent of deep neural networks. In this context, this paper\nbrings two contributions: first, it shows that despite the recent success of\nend-to-end holistic models, explicit part learning can boosts classification\nperformance. Second, this work proceeds one step further than recent part-based\nmodels (PBM), focusing on how to learn parts without using any labeled data.\nInstead of learning a set of parts per class, as generally done in the PBM\nliterature, the proposed approach both constructs a partition of a given set of\nimages into visually similar groups, and subsequently learn a set of\ndiscriminative parts per group in a fully unsupervised fashion. This strategy\nopens the door to the use of PBM in new applications for which the notion of\nimage categories is irrelevant, such as instance-based image retrieval, for\nexample. We experimentally show that our learned parts can help building\nefficient image representations, for classification as well as for indexing\ntasks, resulting in performance superior to holistic state-of-the art Deep\nConvolutional Neural Networks (DCNN) encoding.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 13:35:03 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Sicre", "Ronan", ""], ["Avrithis", "Yannis", ""], ["Kijak", "Ewa", ""], ["Jurie", "Frederic", ""]]}, {"id": "1704.03940", "submitter": "Kai Hui", "authors": "Kai Hui, Andrew Yates, Klaus Berberich, Gerard de Melo", "title": "PACRR: A Position-Aware Neural IR Model for Relevance Matching", "comments": "To appear in EMNLP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to adopt deep learning for information retrieval, models are needed\nthat can capture all relevant information required to assess the relevance of a\ndocument to a given user query. While previous works have successfully captured\nunigram term matches, how to fully employ position-dependent information such\nas proximity and term dependencies has been insufficiently explored. In this\nwork, we propose a novel neural IR model named PACRR aiming at better modeling\nposition-dependent interactions between a query and a document. Extensive\nexperiments on six years' TREC Web Track data confirm that the proposed model\nyields better results under multiple benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 21:56:59 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 16:12:36 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 23:02:06 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hui", "Kai", ""], ["Yates", "Andrew", ""], ["Berberich", "Klaus", ""], ["de Melo", "Gerard", ""]]}, {"id": "1704.03970", "submitter": "Joel Mackenzie", "authors": "Joel Mackenzie and J. Shane Culpepper and Roi Blanco and Matt Crane\n  and Charles L. A. Clarke and Jimmy Lin", "title": "Efficient and Effective Tail Latency Minimization in Multi-Stage\n  Retrieval Systems", "comments": "Update 1: Edited email address", "journal-ref": null, "doi": "10.1145/3159652.3159676", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable web search systems typically employ multi-stage retrieval\narchitectures, where an initial stage generates a set of candidate documents\nthat are then pruned and re-ranked. Since subsequent stages typically exploit a\nmultitude of features of varying costs using machine-learned models, reducing\nthe number of documents that are considered at each stage improves latency. In\nthis work, we propose and validate a unified framework that can be used to\npredict a wide range of performance-sensitive parameters which minimize\neffectiveness loss, while simultaneously minimizing query latency, across all\nstages of a multi-stage search architecture. Furthermore, our framework can be\neasily applied in large-scale IR systems, can be trained without explicitly\nrequiring relevance judgments, and can target a variety of different\nefficiency-effectiveness trade-offs, making it well suited to a wide range of\nsearch scenarios. Our results show that we can reliably predict a number of\ndifferent parameters on a per-query basis, while simultaneously detecting and\nminimizing the likelihood of tail-latency queries that exceed a pre-specified\nperformance budget. As a proof of concept, we use the prediction framework to\nhelp alleviate the problem of tail-latency queries in early stage retrieval. On\nthe standard ClueWeb09B collection and 31k queries, we show that our new hybrid\nsystem can reliably achieve a maximum query time of 200 ms with a 99.99%\nresponse time guarantee without a significant loss in overall effectiveness.\nThe solutions presented are practical, and can easily be used in large-scale\ndistributed search engine deployments with a small amount of additional\noverhead.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 02:09:37 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 23:34:13 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Mackenzie", "Joel", ""], ["Culpepper", "J. Shane", ""], ["Blanco", "Roi", ""], ["Crane", "Matt", ""], ["Clarke", "Charles L. A.", ""], ["Lin", "Jimmy", ""]]}, {"id": "1704.04572", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Nogueira and Kyunghyun Cho", "title": "Task-Oriented Query Reformulation with Reinforcement Learning", "comments": "EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines play an important role in our everyday lives by assisting us\nin finding the information we need. When we input a complex query, however,\nresults are often far from satisfactory. In this work, we introduce a query\nreformulation system based on a neural network that rewrites a query to\nmaximize the number of relevant documents returned. We train this neural\nnetwork with reinforcement learning. The actions correspond to selecting terms\nto build a reformulated query, and the reward is the document recall. We\nevaluate our approach on three datasets against strong baselines and show a\nrelative improvement of 5-20% in terms of recall. Furthermore, we present a\nsimple method to estimate a conservative upper-bound performance of a model in\na particular environment and verify that there is still large room for\nimprovements.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 03:42:51 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 15:49:20 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 12:29:58 GMT"}, {"version": "v4", "created": "Sun, 24 Sep 2017 13:25:58 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Nogueira", "Rodrigo", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "1704.04576", "submitter": "Chenliang Li", "authors": "Zhiqian Zhang, Chenliang Li, Zhiyong Wu, Aixin Sun, Dengpan Ye,\n  Xiangyang Luo", "title": "NEXT: A Neural Network Framework for Next POI Recommendation", "comments": null, "journal-ref": "Frontiers of Computer Science, 2019", "doi": "10.1007/s11704-018-8011-2", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of next POI recommendation has been studied extensively in recent\nyears. However, developing an unified recommendation framework to incorporate\nmultiple factors associated with both POIs and users remains challenging,\nbecause of the heterogeneity nature of these information. Further, effective\nmechanisms to handle cold-start and endow the system with interpretability are\nalso difficult topics. Inspired by the recent success of neural networks in\nmany areas, in this paper, we present a simple but effective neural network\nframework for next POI recommendation, named NEXT. NEXT is an unified framework\nto learn the hidden intent regarding user's next move, by incorporating\ndifferent factors in an unified manner. Specifically, in NEXT, we incorporate\nmeta-data information and two kinds of temporal contexts (i.e., time interval\nand visit time). To leverage sequential relations and geographical influence,\nwe propose to adopt DeepWalk, a network representation learning technique, to\nencode such knowledge. We evaluate the effectiveness of NEXT against\nstate-of-the-art alternatives and neural networks based solutions. Experimental\nresults over three publicly available datasets demonstrate that NEXT\nsignificantly outperforms baselines in real-time next POI recommendation.\nFurther experiments demonstrate the superiority of NEXT in handling cold-start.\nMore importantly, we show that NEXT provides meaningful explanation of the\ndimensions in hidden intent space.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 04:00:53 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Zhang", "Zhiqian", ""], ["Li", "Chenliang", ""], ["Wu", "Zhiyong", ""], ["Sun", "Aixin", ""], ["Ye", "Dengpan", ""], ["Luo", "Xiangyang", ""]]}, {"id": "1704.04684", "submitter": "Luis Argerich", "authors": "Luis Argerich, Natalia Golmar", "title": "Generic LSH Families for the Angular Distance Based on\n  Johnson-Lindenstrauss Projections and Feature Hashing LSH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the creation of generic LSH families for the angular\ndistance based on Johnson-Lindenstrauss projections. We show that feature\nhashing is a valid J-L projection and propose two new LSH families based on\nfeature hashing. These new LSH families are tested on both synthetic and real\ndatasets with very good results and a considerable performance improvement over\nother LSH families. While the theoretical analysis is done for the angular\ndistance, these families can also be used in practice for the euclidean\ndistance with excellent results [2]. Our tests using real datasets show that\nthe proposed LSH functions work well for the euclidean distance.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 19:32:51 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Argerich", "Luis", ""], ["Golmar", "Natalia", ""]]}, {"id": "1704.05091", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro, Eduarda Mendes Rodrigues, Carlos Soares, Eug\\'enio\n  Oliveira", "title": "FEUP at SemEval-2017 Task 5: Predicting Sentiment Polarity and Intensity\n  with Financial Word Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the approach developed at the Faculty of Engineering of\nUniversity of Porto, to participate in SemEval 2017, Task 5: Fine-grained\nSentiment Analysis on Financial Microblogs and News. The task consisted in\npredicting a real continuous variable from -1.0 to +1.0 representing the\npolarity and intensity of sentiment concerning companies/stocks mentioned in\nshort texts. We modeled the task as a regression analysis problem and combined\ntraditional techniques such as pre-processing short texts, bag-of-words\nrepresentations and lexical-based features with enhanced financial specific\nbag-of-embeddings. We used an external collection of tweets and news headlines\nmentioning companies/stocks from S\\&P 500 to create financial word embeddings\nwhich are able to capture domain-specific syntactic and semantic similarities.\nThe resulting approach obtained a cosine similarity score of 0.69 in sub-task\n5.1 - Microblogs and 0.68 in sub-task 5.2 - News Headlines.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 18:48:00 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Saleiro", "Pedro", ""], ["Rodrigues", "Eduarda Mendes", ""], ["Soares", "Carlos", ""], ["Oliveira", "Eug\u00e9nio", ""]]}, {"id": "1704.05393", "submitter": "Michela Fazzolari", "authors": "Michela Fazzolari, Marinella Petrocchi, Alessandro Tommasi, Cesare\n  Zavattari", "title": "Mining Worse and Better Opinions. Unsupervised and Agnostic Aggregation\n  of Online Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for aggregating online reviews,\naccording to the opinions they express. Our methodology is unsupervised - due\nto the fact that it does not rely on pre-labeled reviews - and it is agnostic -\nsince it does not make any assumption about the domain or the language of the\nreview content. We measure the adherence of a review content to the domain\nterminology extracted from a review set. First, we demonstrate the\ninformativeness of the adherence metric with respect to the score associated\nwith a review. Then, we exploit the metric values to group reviews, according\nto the opinions they express. Our experimental campaign has been carried out on\ntwo large datasets collected from Booking and Amazon, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 15:20:25 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Fazzolari", "Michela", ""], ["Petrocchi", "Marinella", ""], ["Tommasi", "Alessandro", ""], ["Zavattari", "Cesare", ""]]}, {"id": "1704.05550", "submitter": "Daniel Lee", "authors": "Rakesh Verma and Daniel Lee", "title": "Extractive Summarization: Limits, Compression, Generalized Model and\n  Heuristics", "comments": null, "journal-ref": "Computaci\\'on y Sistemas 21(4) (2017)", "doi": "10.13053/CyS-21-4-2885", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its promise to alleviate information overload, text summarization has\nattracted the attention of many researchers. However, it has remained a serious\nchallenge. Here, we first prove empirical limits on the recall (and F1-scores)\nof extractive summarizers on the DUC datasets under ROUGE evaluation for both\nthe single-document and multi-document summarization tasks. Next we define the\nconcept of compressibility of a document and present a new model of\nsummarization, which generalizes existing models in the literature and\nintegrates several dimensions of the summarization, viz., abstractive versus\nextractive, single versus multi-document, and syntactic versus semantic.\nFinally, we examine some new and existing single-document summarization\nalgorithms in a single framework and compare with state of the art summarizers\non DUC data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 22:21:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Verma", "Rakesh", ""], ["Lee", "Daniel", ""]]}, {"id": "1704.05617", "submitter": "Chun-Nan Hsu", "authors": "Sanjeev Shenoy, Tsung-Ting Kuo, Rodney Gabriel, Julian McAuley and\n  Chun-Nan Hsu", "title": "Deduplication in a massive clinical note dataset", "comments": "Extended from the Master project report of Sanjeev Shenoy, Department\n  of Computer Science and Engineering, University of California, San Diego.\n  June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplication, whether exact or partial, is a common issue in many datasets. In\nclinical notes data, duplication (and near duplication) can arise for many\nreasons, such as the pervasive use of templates, copy-pasting, or notes being\ngenerated by automated procedures. A key challenge in removing such near\nduplicates is the size of such datasets; our own dataset consists of more than\n10 million notes. To detect and correct such duplicates requires algorithms\nthat both accurate and highly scalable. We describe a solution based on\nMinhashing with Locality Sensitive Hashing. In this paper, we present the\ntheory behind this method and present a database-inspired approach to make the\nmethod scalable. We also present a clustering technique using disjoint sets to\nproduce dense clusters, which speeds up our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 05:33:21 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Shenoy", "Sanjeev", ""], ["Kuo", "Tsung-Ting", ""], ["Gabriel", "Rodney", ""], ["McAuley", "Julian", ""], ["Hsu", "Chun-Nan", ""]]}, {"id": "1704.05841", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "The Magic Barrier Revisited: Accessing Natural Limitations of\n  Recommender Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems nowadays have many applications and are of great economic\nbenefit. Hence, it is imperative for success-oriented companies to compare\ndifferent of such systems and select the better one for their purposes. To this\nend, various metrics of predictive accuracy are commonly used, such as the Root\nMean Square Error (RMSE), or precision and recall. All these metrics more or\nless measure how well a recommender system can predict human behaviour.\n  Unfortunately, human behaviour is always associated with some degree of\nuncertainty, making the evaluation difficult, since it is not clear whether a\ndeviation is system-induced or just originates from the natural variability of\nhuman decision making. At this point, some authors speculated that we may be\nreaching some Magic Barrier where this variability prevents us from getting\nmuch more accurate.\n  In this article, we will extend the existing theory of the Magic Barrier into\na new probabilistic but a yet pragmatic model. In particular, we will use\nmethods from metrology and physics to develop easy-to-handle quantities for\ncomputation to describe the Magic Barrier for different accuracy metrics and\nprovide suggestions for common application. This discussion is substantiated by\ncomprehensive experiments with real users and large-scale simulations on a\nhigh-performance cluster.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 16:57:25 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1704.06109", "submitter": "Yashar Deldjoo", "authors": "Yashar Deldjoo, Massimo Quadrana, Mehdi Elahi, Paolo Cremonesi", "title": "Using Mise-En-Sc\\`ene Visual Features based on MPEG-7 and Deep Learning\n  for Movie Recommendation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item features play an important role in movie recommender systems, where\nrecommendations can be generated by using explicit or implicit preferences of\nusers on traditional features (attributes) such as tag, genre, and cast.\nTypically, movie features are human-generated, either editorially (e.g., genre\nand cast) or by leveraging the wisdom of the crowd (e.g., tag), and as such,\nthey are prone to noise and are expensive to collect. Moreover, these features\nare often rare or absent for new items, making it difficult or even impossible\nto provide good quality recommendations.\n  In this paper, we show that user's preferences on movies can be better\ndescribed in terms of the mise-en-sc\\`ene features, i.e., the visual aspects of\na movie that characterize design, aesthetics and style (e.g., colors,\ntextures). We use both MPEG-7 visual descriptors and Deep Learning hidden\nlayers as example of mise-en-sc\\`ene features that can visually describe\nmovies. Interestingly, mise-en-sc\\`ene features can be computed automatically\nfrom video files or even from trailers, offering more flexibility in handling\nnew items, avoiding the need for costly and error-prone human-based tagging,\nand providing good scalability.\n  We have conducted a set of experiments on a large catalogue of 4K movies.\nResults show that recommendations based on mise-en-sc\\`ene features\nconsistently provide the best performance with respect to richer sets of more\ntraditional features, such as genre and tag.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 12:33:48 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Deldjoo", "Yashar", ""], ["Quadrana", "Massimo", ""], ["Elahi", "Mehdi", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1704.06619", "submitter": "Arman Cohan", "authors": "Arman Cohan and Nazli Goharian", "title": "Scientific Article Summarization Using Citation-Context and Article's\n  Discourse Structure", "comments": "EMNLP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a summarization approach for scientific articles which takes\nadvantage of citation-context and the document discourse model. While citations\nhave been previously used in generating scientific summaries, they lack the\nrelated context from the referenced article and therefore do not accurately\nreflect the article's content. Our method overcomes the problem of\ninconsistency between the citation summary and the article's content by\nproviding context for each citation. We also leverage the inherent scientific\narticle's discourse for producing better summaries. We show that our proposed\nmethod effectively improves over existing summarization approaches (greater\nthan 30% improvement over the best performing baseline) in terms of\n\\textsc{Rouge} scores on TAC2014 scientific summarization dataset. While the\ndataset we use for evaluation is in the biomedical domain, most of our\napproaches are general and therefore adaptable to other domains.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:17:58 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Cohan", "Arman", ""], ["Goharian", "Nazli", ""]]}, {"id": "1704.06687", "submitter": "Mathieu Cliche", "authors": "Mathieu Cliche, David Rosenberg, Dhruv Madeka and Connie Yee", "title": "Scatteract: Automated extraction of data from scatter plots", "comments": "Submitted to ECML PKDD 2017 proceedings, 16 pages", "journal-ref": "Machine Learning and Knowledge Discovery in Databases. ECML PKDD\n  2017. Lecture Notes in Computer Science, vol 10534. Springer, Cham", "doi": "10.1007/978-3-319-71249-9_9", "report-no": null, "categories": "cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charts are an excellent way to convey patterns and trends in data, but they\ndo not facilitate further modeling of the data or close inspection of\nindividual data points. We present a fully automated system for extracting the\nnumerical values of data points from images of scatter plots. We use deep\nlearning techniques to identify the key components of the chart, and optical\ncharacter recognition together with robust regression to map from pixels to the\ncoordinate system of the chart. We focus on scatter plots with linear scales,\nwhich already have several interesting challenges. Previous work has done fully\nautomatic extraction for other types of charts, but to our knowledge this is\nthe first approach that is fully automatic for scatter plots. Our method\nperforms well, achieving successful data extraction on 89% of the plots in our\ntest set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 19:25:32 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Cliche", "Mathieu", ""], ["Rosenberg", "David", ""], ["Madeka", "Dhruv", ""], ["Yee", "Connie", ""]]}, {"id": "1704.06726", "submitter": "Jimmy Lin", "authors": "Salman Mohammed, Nimesh Ghelani, and Jimmy Lin", "title": "Distant Supervision for Topic Classification of Tweets in Curated\n  Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the challenge of topic classification of tweets in the context of\nanalyzing a large collection of curated streams by news outlets and other\norganizations to deliver relevant content to users. Our approach is novel in\napplying distant supervision based on semi-automatically identifying curated\nstreams that are topically focused (for example, on politics, entertainment, or\nsports). These streams provide a source of labeled data to train topic\nclassifiers that can then be applied to categorize tweets from more\ntopically-diffuse streams. Experiments on both noisy labels and human\nground-truth judgments demonstrate that our approach yields good topic\nclassifiers essentially \"for free\", and that topic classifiers trained in this\nmanner are able to dynamically adjust for topic drift as news on Twitter\nevolves.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 00:23:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Mohammed", "Salman", ""], ["Ghelani", "Nimesh", ""], ["Lin", "Jimmy", ""]]}, {"id": "1704.06803", "submitter": "Federico Monti", "authors": "Federico Monti, Michael M. Bronstein, Xavier Bresson", "title": "Geometric Matrix Completion with Recurrent Multi-Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion models are among the most common formulations of\nrecommender systems. Recent works have showed a boost of performance of these\ntechniques when introducing the pairwise relationships between users/items in\nthe form of graphs, and imposing smoothness priors on these graphs. However,\nsuch techniques do not fully exploit the local stationarity structures of\nuser/item graphs, and the number of parameters to learn is linear w.r.t. the\nnumber of users and items. We propose a novel approach to overcome these\nlimitations by using geometric deep learning on graphs. Our matrix completion\narchitecture combines graph convolutional neural networks and recurrent neural\nnetworks to learn meaningful statistical graph-structured patterns and the\nnon-linear diffusion process that generates the known ratings. This neural\nnetwork system requires a constant number of parameters independent of the\nmatrix size. We apply our method on both synthetic and real datasets, showing\nthat it outperforms state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 14:02:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Monti", "Federico", ""], ["Bronstein", "Michael M.", ""], ["Bresson", "Xavier", ""]]}, {"id": "1704.06840", "submitter": "Damian Straszak", "authors": "L. Elisa Celis and Damian Straszak and Nisheeth K. Vishnoi", "title": "Ranking with Fairness Constraints", "comments": "appeared in ICALP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking algorithms are deployed widely to order a set of items in\napplications such as search engines, news feeds, and recommendation systems.\nRecent studies, however, have shown that, left unchecked, the output of ranking\nalgorithms can result in decreased diversity in the type of content presented,\npromote stereotypes, and polarize opinions. In order to address such issues, we\nstudy the following variant of the traditional ranking problem when, in\naddition, there are fairness or diversity constraints. Given a collection of\nitems along with 1) the value of placing an item in a particular position in\nthe ranking, 2) the collection of sensitive attributes (such as gender, race,\npolitical opinion) of each item and 3) a collection of constraints that, for\neach k, bound the number of items with each attribute that are allowed to\nappear in the top k positions of the ranking, the goal is to output a ranking\nthat maximizes the value with respect to the original rank quality metric while\nrespecting the constraints. This problem encapsulates various well-studied\nproblems related to bipartite and hypergraph matching as special cases and\nturns out to be hard to approximate even with simple constraints. Our main\ntechnical contributions are fast exact and approximation algorithms along with\ncomplementary hardness results that, together, come close to settling the\napproximability of this constrained ranking maximization problem. Unlike prior\nwork on the constrained matching problems, our algorithm runs in linear time,\neven when the number of constraints is large, its approximation ratio does not\ndepend on the number of constraints, and it produces solutions with small\nconstraint violations. Our results rely on insights about the constrained\nmatching problem when the objective satisfies properties that appear in common\nranking metrics such as Discounted Cumulative Gain, Spearman's rho or\nBradley-Terry.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 19:31:29 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 17:39:44 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 10:33:10 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 16:30:07 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Celis", "L. Elisa", ""], ["Straszak", "Damian", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1704.07171", "submitter": "Vincent Labatut", "authors": "G\\\"unce Keziban Orman, Vincent Labatut (LIA), Ahmet Teoman Naskali", "title": "Exploring the Evolution of Node Neighborhoods in Dynamic Networks", "comments": null, "journal-ref": "Physica A, Elsevier, 482, pp.375-391 (2017)", "doi": "10.1016/j.physa.2017.04.084", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Networks are a popular way of modeling and studying the behavior of\nevolving systems. However, their analysis constitutes a relatively recent\nsubfield of Network Science, and the number of available tools is consequently\nmuch smaller than for static networks. In this work, we propose a method\nspecifically designed to take advantage of the longitudinal nature of dynamic\nnetworks. It characterizes each individual node by studying the evolution of\nits direct neighborhood, based on the assumption that the way this neighborhood\nchanges reflects the role and position of the node in the whole network. For\nthis purpose, we define the concept of \\textit{neighborhood event}, which\ncorresponds to the various transformations such groups of nodes can undergo,\nand describe an algorithm for detecting such events. We demonstrate the\ninterest of our method on three real-world networks: DBLP, LastFM and Enron. We\napply frequent pattern mining to extract meaningful information from temporal\nsequences of neighborhood events. This results in the identification of\nbehavioral trends emerging in the whole network, as well as the individual\ncharacterization of specific nodes. We also perform a cluster analysis, which\nreveals that, in all three networks, one can distinguish two types of nodes\nexhibiting different behaviors: a very small group of active nodes, whose\nneighborhood undergo diverse and frequent events, and a very large group of\nstable nodes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 12:18:21 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 12:26:08 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Orman", "G\u00fcnce Keziban", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Naskali", "Ahmet Teoman", ""]]}, {"id": "1704.07355", "submitter": "Fabien Andr\\'e", "authors": "Fabien Andr\\'e (Technicolor) and Anne-Marie Kermarrec (Inria) and\n  Nicolas Le Scouarnec (Technicolor)", "title": "Accelerated Nearest Neighbor Search with Quick ADC", "comments": "8 pages, 5 figures, published in Proceedings of ICMR'17, Bucharest,\n  Romania, June 06-09, 2017", "journal-ref": null, "doi": "10.1145/3078971.3078992", "report-no": null, "categories": "cs.CV cs.DB cs.IR cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Nearest Neighbor (NN) search in high-dimensional spaces is a\nfoundation of many multimedia retrieval systems. Because it offers low\nresponses times, Product Quantization (PQ) is a popular solution. PQ compresses\nhigh-dimensional vectors into short codes using several sub-quantizers, which\nenables in-RAM storage of large databases. This allows fast answers to NN\nqueries, without accessing the SSD or HDD. The key feature of PQ is that it can\ncompute distances between short codes and high-dimensional vectors using\ncache-resident lookup tables. The efficiency of this technique, named\nAsymmetric Distance Computation (ADC), remains limited because it performs many\ncache accesses.\n  In this paper, we introduce Quick ADC, a novel technique that achieves a 3 to\n6 times speedup over ADC by exploiting Single Instruction Multiple Data (SIMD)\nunits available in current CPUs. Efficiently exploiting SIMD requires\nalgorithmic changes to the ADC procedure. Namely, Quick ADC relies on two key\nmodifications of ADC: (i) the use 4-bit sub-quantizers instead of the standard\n8-bit sub-quantizers and (ii) the quantization of floating-point distances.\nThis allows Quick ADC to exceed the performance of state-of-the-art systems,\ne.g., it achieves a Recall@100 of 0.94 in 3.4 ms on 1 billion SIFT descriptors\n(128-bit codes).\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 17:49:37 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Andr\u00e9", "Fabien", "", "Technicolor"], ["Kermarrec", "Anne-Marie", "", "Inria"], ["Scouarnec", "Nicolas Le", "", "Technicolor"]]}, {"id": "1704.07624", "submitter": "Amit Gupta", "authors": "Amit Gupta and R\\'emi Lebret and Hamza Harkous and Karl Aberer", "title": "280 Birds with One Stone: Inducing Multilingual Taxonomies from\n  Wikipedia using Character-level Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, yet effective, approach towards inducing multilingual\ntaxonomies from Wikipedia. Given an English taxonomy, our approach leverages\nthe interlanguage links of Wikipedia followed by character-level classifiers to\ninduce high-precision, high-coverage taxonomies in other languages. Through\nexperiments, we demonstrate that our approach significantly outperforms the\nstate-of-the-art, heuristics-heavy approaches for six languages. As a\nconsequence of our work, we release presumably the largest and the most\naccurate multilingual taxonomic resource spanning over 280 languages.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:45:43 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 09:23:40 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Gupta", "Amit", ""], ["Lebret", "R\u00e9mi", ""], ["Harkous", "Hamza", ""], ["Aberer", "Karl", ""]]}, {"id": "1704.07626", "submitter": "Amit Gupta", "authors": "Amit Gupta, R\\'emi Lebret, Hamza Harkous and Karl Aberer", "title": "Taxonomy Induction using Hypernym Subsequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, semi-supervised approach towards domain taxonomy\ninduction from an input vocabulary of seed terms. Unlike all previous\napproaches, which typically extract direct hypernym edges for terms, our\napproach utilizes a novel probabilistic framework to extract hypernym\nsubsequences. Taxonomy induction from extracted subsequences is cast as an\ninstance of the minimumcost flow problem on a carefully designed directed\ngraph. Through experiments, we demonstrate that our approach outperforms\nstateof- the-art taxonomy induction approaches across four languages.\nImportantly, we also show that our approach is robust to the presence of noise\nin the input vocabulary. To the best of our knowledge, no previous approaches\nhave been empirically proven to manifest noise-robustness in the input\nvocabulary.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 10:49:53 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 17:03:17 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 14:42:59 GMT"}, {"version": "v4", "created": "Thu, 14 Sep 2017 20:34:26 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Gupta", "Amit", ""], ["Lebret", "R\u00e9mi", ""], ["Harkous", "Hamza", ""], ["Aberer", "Karl", ""]]}, {"id": "1704.07751", "submitter": "Maxim Rabinovich", "authors": "Maxim Rabinovich, Dan Klein", "title": "Fine-Grained Entity Typing with High-Multiplicity Assignments", "comments": "ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As entity type systems become richer and more fine-grained, we expect the\nnumber of types assigned to a given entity to increase. However, most\nfine-grained typing work has focused on datasets that exhibit a low degree of\ntype multiplicity. In this paper, we consider the high-multiplicity regime\ninherent in data sources such as Wikipedia that have semi-open type systems. We\nintroduce a set-prediction approach to this problem and show that our model\noutperforms unstructured baselines on a new Wikipedia-based fine-grained typing\ncorpus.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:52:52 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""]]}, {"id": "1704.07757", "submitter": "Harshita Sahijwani", "authors": "Harshita Sahijwani, Sourish Dasgupta", "title": "User Profile Based Research Paper Recommendation", "comments": "Work in progress. arXiv admin note: text overlap with\n  arXiv:1611.04822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a recommender system for research papers based on topic-modeling.\nThe users feedback to the results is used to make the results more relevant the\nnext time they fire a query. The user's needs are understood by observing the\nchange in the themes that the user shows a preference for over time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 16:01:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Sahijwani", "Harshita", ""], ["Dasgupta", "Sourish", ""]]}, {"id": "1704.08027", "submitter": "Manuel Sebastian Mariani", "authors": "Hao Liao, Manuel Sebastian Mariani, Matus Medo, Yi-Cheng Zhang,\n  Ming-Yang Zhou", "title": "Ranking in evolving complex networks", "comments": "54 pages, 16 figures", "journal-ref": "Physics Reports 689, 1-54 (2017)", "doi": "10.1016/j.physrep.2017.05.001", "report-no": null, "categories": "physics.soc-ph cs.DL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks have emerged as a simple yet powerful framework to represent\nand analyze a wide range of complex systems. The problem of ranking the nodes\nand the edges in complex networks is critical for a broad range of real-world\nproblems because it affects how we access online information and products, how\nsuccess and talent are evaluated in human activities, and how scarce resources\nare allocated by companies and policymakers, among others. This calls for a\ndeep understanding of how existing ranking algorithms perform, and which are\ntheir possible biases that may impair their effectiveness. Well-established\nranking algorithms (such as the popular Google's PageRank) are static in nature\nand, as a consequence, they exhibit important shortcomings when applied to real\nnetworks that rapidly evolve in time. The recent advances in the understanding\nand modeling of evolving networks have enabled the development of a wide and\ndiverse range of ranking algorithms that take the temporal dimension into\naccount. The aim of this review is to survey the existing ranking algorithms,\nboth static and time-aware, and their applications to evolving networks. We\nemphasize both the impact of network evolution on well-established static\nalgorithms and the benefits from including the temporal dimension for tasks\nsuch as prediction of real network traffic, prediction of future links, and\nidentification of highly-significant nodes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 09:18:45 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Liao", "Hao", ""], ["Mariani", "Manuel Sebastian", ""], ["Medo", "Matus", ""], ["Zhang", "Yi-Cheng", ""], ["Zhou", "Ming-Yang", ""]]}, {"id": "1704.08803", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Hamed Zamani, Aliaksei Severyn, Jaap Kamps, W. Bruce\n  Croft", "title": "Neural Ranking Models with Weak Supervision", "comments": "In proceedings of The 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the impressive improvements achieved by unsupervised deep neural\nnetworks in computer vision and NLP tasks, such improvements have not yet been\nobserved in ranking for information retrieval. The reason may be the complexity\nof the ranking problem, as it is not obvious how to learn from queries and\ndocuments when no supervised signal is available. Hence, in this paper, we\npropose to train a neural ranking model using weak supervision, where labels\nare obtained automatically without human annotators or any external resources\n(e.g., click data). To this aim, we use the output of an unsupervised ranking\nmodel, such as BM25, as a weak supervision signal. We further train a set of\nsimple yet effective ranking models based on feed-forward neural networks. We\nstudy their effectiveness under various learning scenarios (point-wise and\npair-wise models) and using different input representations (i.e., from\nencoding query-document pairs into dense/sparse vectors to using word embedding\nrepresentation). We train our networks using tens of millions of training\ninstances and evaluate it on two standard collections: a homogeneous news\ncollection(Robust) and a heterogeneous large-scale web collection (ClueWeb).\nOur experiments indicate that employing proper objective functions and letting\nthe networks to learn the input representation based on weakly supervised data\nleads to impressive performance, with over 13% and 35% MAP improvements over\nthe BM25 model on the Robust and the ClueWeb collections. Our findings also\nsuggest that supervised neural ranking models can greatly benefit from\npre-training on large amounts of weakly labeled data that can be easily\nobtained from unsupervised IR models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 04:08:47 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 11:58:34 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Zamani", "Hamed", ""], ["Severyn", "Aliaksei", ""], ["Kamps", "Jaap", ""], ["Croft", "W. Bruce", ""]]}, {"id": "1704.08853", "submitter": "Tieyun Qian", "authors": "Bei Liu, Tieyun Qian, Bing Liu, Liang Hong, Zhenni You, Yuxiang Li", "title": "Learning Spatiotemporal-Aware Representation for POI Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide spread of location-based social networks brings about a huge volume\nof user check-in data, which facilitates the recommendation of points of\ninterest (POIs). Recent advances on distributed representation shed light on\nlearning low dimensional dense vectors to alleviate the data sparsity problem.\nCurrent studies on representation learning for POI recommendation embed both\nusers and POIs in a common latent space, and users' preference is inferred\nbased on the distance/similarity between a user and a POI. Such an approach is\nnot in accordance with the semantics of users and POIs as they are inherently\ndifferent objects. In this paper, we present a novel spatiotemporal aware (STA)\nrepresentation, which models the spatial and temporal information as \\emph{a\nrelationship connecting users and POIs}. Our model generalizes the recent\nadvances in knowledge graph embedding. The basic idea is that the embedding of\na $<$time, location$>$ pair corresponds to a translation from embeddings of\nusers to POIs. Since the POI embedding should be close to the user embedding\nplus the relationship vector, the recommendation can be performed by selecting\nthe top-\\emph{k} POIs similar to the translated POI, which are all of the same\ntype of objects. We conduct extensive experiments on two real-world datasets.\nThe results demonstrate that our STA model achieves the state-of-the-art\nperformance in terms of high recommendation accuracy, robustness to data\nsparsity and effectiveness in handling cold start problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 09:01:01 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Liu", "Bei", ""], ["Qian", "Tieyun", ""], ["Liu", "Bing", ""], ["Hong", "Liang", ""], ["You", "Zhenni", ""], ["Li", "Yuxiang", ""]]}, {"id": "1704.08991", "submitter": "Erwan Le Merrer", "authors": "Erwan Le Merrer and Gilles Tr\\'edan", "title": "The topological face of recommendation: models and application to bias\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation plays a key role in e-commerce and in the entertainment\nindustry. We propose to consider successive recommendations to users under the\nform of graphs of recommendations. We give models for this representation.\nMotivated by the growing interest for algorithmic transparency, we then propose\na first application for those graphs, that is the potential detection of\nintroduced recommendation bias by the service provider. This application relies\non the analysis of the topology of the extracted graph for a given user; we\npropose a notion of recommendation coherence with regards to the topological\nproximity of recommended items (under the measure of items' k-closest\nneighbors, reminding the \"small-world\" model by Watts & Stroggatz). We finally\nillustrate this approach on a model and on Youtube crawls, targeting the\nprediction of \"Recommended for you\" links (i.e., biased or not by Youtube).\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 16:15:11 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Merrer", "Erwan Le", ""], ["Tr\u00e9dan", "Gilles", ""]]}]