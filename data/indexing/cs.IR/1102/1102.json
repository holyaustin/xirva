[{"id": "1102.0540", "submitter": "Oleg Zaboronski V", "authors": "Oliver Hambrey, Thomas Parnell and Oleg Zaboronski", "title": "Information theory of massively parallel probe storage channels", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the concept of probe storage, we study the problem of\ninformation retrieval using a large array of N nano-mechanical probes, N ~\n4000. At the nanometer scale it is impossible to avoid errors in the\npositioning of the array, thus all signals retrieved by the probes of the array\nat a given sampling moment are affected by the same amount of random position\njitter. Therefore a massively parallel probe storage device is an example of a\nnoisy communication channel with long range correlations between channel\noutputs due to the global positioning errors. We find that these correlations\nhave a profound effect on the channel's properties. For example, it turns out\nthat the channel's information capacity does approach 1 bit per probe in the\nlimit of high signal-to-noise ratio, but the rate of the approach is only\npolynomial in the channel noise strength. Moreover, any error correction code\nwith block size N >> 1 such that codewords correspond to the instantaneous\noutputs of the all probes in the array exhibits an error floor independently of\nthe code rate. We illustrate this phenomenon explicitly using Reed-Solomon\ncodes the performance of which is easy to simulate numerically. We also discuss\ncapacity-achieving error correction codes for the global jitter channel and\ntheir complexity.\n", "versions": [{"version": "v1", "created": "Wed, 2 Feb 2011 20:02:15 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hambrey", "Oliver", ""], ["Parnell", "Thomas", ""], ["Zaboronski", "Oleg", ""]]}, {"id": "1102.0651", "submitter": "Adolfo Paolo Masucci apm", "authors": "A.P. Masucci, A. Kalampokis, V.M. Egu\\'iluz, E. Hern\\'andez-Garc\\'ia", "title": "Wikipedia information flow analysis reveals the scale-free architecture\n  of the Semantic Space", "comments": "12 pages, in press at PloS ONE", "journal-ref": "PLoS ONE 6(2): e17333 (2011)", "doi": "10.1371/journal.pone.0017333", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extract the topology of the semantic space in its\nencyclopedic acception, measuring the semantic flow between the different\nentries of the largest modern encyclopedia, Wikipedia, and thus creating a\ndirected complex network of semantic flows. Notably at the percolation\nthreshold the semantic space is characterised by scale-free behaviour at\ndifferent levels of complexity and this relates the semantic space to a wide\nrange of biological, social and linguistics phenomena. In particular we find\nthat the cluster size distribution, representing the size of different semantic\nareas, is scale-free. Moreover the topology of the resulting semantic space is\nscale-free in the connectivity distribution and displays small-world\nproperties. However its statistical properties do not allow a classical\ninterpretation via a generative model based on a simple multiplicative process.\nAfter giving a detailed description and interpretation of the topological\nproperties of the semantic space, we introduce a stochastic model of\ncontent-based network, based on a copy and mutation algorithm and on the Heaps'\nlaw, that is able to capture the main statistical properties of the analysed\nsemantic space, including the Zipf's law for the word frequency distribution.\n", "versions": [{"version": "v1", "created": "Wed, 2 Feb 2011 11:56:32 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Masucci", "A. P.", ""], ["Kalampokis", "A.", ""], ["Egu\u00edluz", "V. M.", ""], ["Hern\u00e1ndez-Garc\u00eda", "E.", ""]]}, {"id": "1102.0676", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Sajal Mukherjee, Soumya Ghosh, Saheli Kar,\n  Young-Chon Kim", "title": "Architecture of A Scalable Dynamic Parallel WebCrawler with High Speed\n  Downloadable Capability for a Web Search Engine", "comments": "6 pages, 6 figures", "journal-ref": "MSPT 2006 - 6th International Workshop on MSPT Proceedings,\n  Republic of Korea", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today World Wide Web (WWW) has become a huge ocean of information and it is\ngrowing in size everyday. Downloading even a fraction of this mammoth data is\nlike sailing through a huge ocean and it is a challenging task indeed. In order\nto download a large portion of data from WWW, it has become absolutely\nessential to make the crawling process parallel. In this paper we offer the\narchitecture of a dynamic parallel Web crawler, christened as \"WEB-SAILOR,\"\nwhich presents a scalable approach based on Client-Server model to speed up the\ndownload process on behalf of a Web Search Engine in a distributed Domain-set\nspecific environment. WEB-SAILOR removes the possibility of overlapping of\ndownloaded documents by multiple crawlers without even incurring the cost of\ncommunication overhead among several parallel \"client\" crawling processes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 13:20:51 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Mukherjee", "Sajal", ""], ["Ghosh", "Soumya", ""], ["Kar", "Saheli", ""], ["Kim", "Young-Chon", ""]]}, {"id": "1102.0694", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Pradipta Biswas, Young-Chon Kim", "title": "A Syntactic Classification based Web Page Ranking Algorithm", "comments": "10 pages, 4 figures, 2 tables MSPT 2006 - 6th International Workshop\n  on MSPT Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing search engines sometimes give unsatisfactory search result for\nlack of any categorization of search result. If there is some means to know the\npreference of user about the search result and rank pages according to that\npreference, the result will be more useful and accurate to the user. In the\npresent paper a web page ranking algorithm is being proposed based on syntactic\nclassification of web pages. Syntactic Classification does not bother about the\nmeaning of the content of a web page. The proposed approach mainly consists of\nthree steps: select some properties of web pages based on user's demand,\nmeasure them, and give different weightage to each property during ranking for\ndifferent types of pages. The existence of syntactic classification is\nsupported by running fuzzy c-means algorithm and neural network classification\non a set of web pages. The change in ranking for difference in type of pages\nbut for same query string is also being demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 14:19:10 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Biswas", "Pradipta", ""], ["Kim", "Young-Chon", ""]]}, {"id": "1102.0695", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Aritra Banik, Sreemoyee Mukherjee, Jhilik\n  Bhattacharya, Young-Chon Kim", "title": "A Domain Specific Ontology Based Semantic Web Search Engine", "comments": "9 pages, 11 figures, MSPT 2007 - 7th International Workshop on MSPT\n  Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its emergence in the 1990s the World Wide Web (WWW) has rapidly evolved\ninto a huge mine of global information and it is growing in size everyday. The\npresence of huge amount of resources on the Web thus poses a serious problem of\naccurate search. This is mainly because today's Web is a human-readable Web\nwhere information cannot be easily processed by machine. Highly sophisticated,\nefficient keyword based search engines that have evolved today have not been\nable to bridge this gap. So comes up the concept of the Semantic Web which is\nenvisioned by Tim Berners-Lee as the Web of machine interpretable information\nto make a machine processable form for expressing information. Based on the\nsemantic Web technologies we present in this paper the design methodology and\ndevelopment of a semantic Web search engine which provides exact search results\nfor a domain specific search. This search engine is developed for an\nagricultural Website which hosts agricultural information about the state of\nWest Bengal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 14:31:25 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Banik", "Aritra", ""], ["Mukherjee", "Sreemoyee", ""], ["Bhattacharya", "Jhilik", ""], ["Kim", "Young-Chon", ""]]}, {"id": "1102.0735", "submitter": "Mohammad Amin Omidvar", "authors": "Mohammad Amin Omidvar, Vahid Reza Mirabi and Najes Shokry", "title": "Analyzing the Impact of Visitors on Page Views with Google Analytics", "comments": "32 pages,16 table, 10 figure", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT) Vol.2,\n  No.1, January 2011", "doi": "10.5121/ijwest.2011.2102", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a flexible methodology to analyze the effectiveness of\ndifferent variables on various dependent variables which all are times series\nand especially shows how to use a time series regression on one of the most\nimportant and primary index (page views per visit) on Google analytic and in\nconjunction it shows how to use the most suitable data to gain a more accurate\nresult. Search engine visitors have a variety of impact on page views which\ncannot be described by single regression. On one hand referral visitors are\nwell-fitted on linear regression with low impact. On the other hand, direct\nvisitors made a huge impact on page views. The higher connection speed does not\nsimply imply higher impact on page views and the content of web page and the\nterritory of visitors can help connection speed to describe user behavior.\nReturning visitors have some similarities with direct visitors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Feb 2011 17:18:43 GMT"}], "update_date": "2011-02-04", "authors_parsed": [["Omidvar", "Mohammad Amin", ""], ["Mirabi", "Vahid Reza", ""], ["Shokry", "Najes", ""]]}, {"id": "1102.0930", "submitter": "Martin Klein", "authors": "Jeb Ware, Martin Klein, Michael L. Nelson", "title": "An Evaluation of Link Neighborhood Lexical Signatures to Rediscover\n  Missing Web Pages", "comments": "24 pages, 13 figures, 8 tables, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discovering the new URI of a missing web page, lexical signatures, which\nconsist of a small number of words chosen to represent the \"aboutness\" of a\npage, have been previously proposed. However, prior methods relied on computing\nthe lexical signature before the page was lost, or using cached or archived\nversions of the page to calculate a lexical signature. We demonstrate a system\nof constructing a lexical signature for a page from its link neighborhood, that\nis the \"backlinks\", or pages that link to the missing page. After testing\nvarious methods, we show that one can construct a lexical signature for a\nmissing web page using only ten backlink pages. Further, we show that only the\nfirst level of backlinks are useful in this effort. The text that the backlinks\nuse to point to the missing page is used as input for the creation of a\nfour-word lexical signature. That lexical signature is shown to successfully\nfind the target URI in over half of the test cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 14:50:45 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["Ware", "Jeb", ""], ["Klein", "Martin", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1102.1027", "submitter": "Alaa Abi Haidar", "authors": "Alaa Abi-Haidar and Luis M. Rocha", "title": "Collective Classification of Textual Documents by Guided\n  Self-Organization in T-Cell Cross-Regulation Dynamics", "comments": null, "journal-ref": "Evolutionary Intelligence. 2011. Volume 4, Number 2, 69-80", "doi": "10.1007/s12065-011-0052-5", "report-no": null, "categories": "cs.IR cs.AI cs.LG nlin.AO q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and study an agent-based model of T-Cell cross-regulation in the\nadaptive immune system, which we apply to binary classification. Our method\nexpands an existing analytical model of T-cell cross-regulation (Carneiro et\nal. in Immunol Rev 216(1):48-68, 2007) that was used to study the\nself-organizing dynamics of a single population of T-Cells in interaction with\nan idealized antigen presenting cell capable of presenting a single antigen.\nWith agent-based modeling we are able to study the self-organizing dynamics of\nmultiple populations of distinct T-cells which interact via antigen presenting\ncells that present hundreds of distinct antigens. Moreover, we show that such\nself-organizing dynamics can be guided to produce an effective binary\nclassification of antigens, which is competitive with existing machine learning\nmethods when applied to biomedical text classification. More specifically, here\nwe test our model on a dataset of publicly available full-text biomedical\narticles provided by the BioCreative challenge (Krallinger in The biocreative\nii. 5 challenge overview, p 19, 2009). We study the robustness of our model's\nparameter configurations, and show that it leads to encouraging results\ncomparable to state-of-the-art classifiers. Our results help us understand both\nT-cell cross-regulation as a general principle of guided self-organization, as\nwell as its applicability to document classification. Therefore, we show that\nour bio-inspired algorithm is a promising novel method for biomedical article\nclassification and for binary document classification in general.\n", "versions": [{"version": "v1", "created": "Fri, 4 Feb 2011 22:10:45 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Abi-Haidar", "Alaa", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1102.1111", "submitter": "Matt Mullins", "authors": "Matt Mullins, Perry Fizzano", "title": "Treelicious: a System for Semantically Navigating Tagged Web Pages", "comments": "6 pages, 3 figures", "journal-ref": "WI-IAT '10: Proceedings of the 2010 IEEE/WIC/ACM International\n  Conference on Web Intelligence and Intelligent Agent Technology", "doi": "10.1109/WI-IAT.2010.289", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative tagging has emerged as a popular and effective method for\norganizing and describing pages on the Web. We present Treelicious, a system\nthat allows hierarchical navigation of tagged web pages. Our system enriches\nthe navigational capabilities of standard tagging systems, which typically\nexploit only popularity and co-occurrence data. We describe a prototype that\nleverages the Wikipedia category structure to allow a user to semantically\nnavigate pages from the Delicious social bookmarking service. In our system a\nuser can perform an ordinary keyword search and browse relevant pages but is\nalso given the ability to broaden the search to more general topics and narrow\nit to more specific topics. We show that Treelicious indeed provides an\nintuitive framework that allows for improved and effective discovery of\nknowledge.\n", "versions": [{"version": "v1", "created": "Sat, 5 Feb 2011 23:54:04 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Mullins", "Matt", ""], ["Fizzano", "Perry", ""]]}, {"id": "1102.1345", "submitter": "Debajyoti Mukhopadhyay Prof.", "authors": "Debajyoti Mukhopadhyay, Sukanta Sinha", "title": "Introducing a New Mechanism for Construction of an Efficient Search\n  Model", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engine has become an inevitable tool for retrieving information from\nthe WWW. Web researchers introduce lots of algorithms to modify search engine\nbased on different features. Sometimes those algorithms are domain related,\nsometimes they are Web page ranking related, and sometimes they are efficiency\nrelated and so on. We are introducing such a type of algorithm which is\nmultiple domains as well as efficiency related. In this paper, we are providing\nmultilevel indexing on top of Index Based Acyclic Graph (IBAG) which support\nmultiple Ontologies as well as reduce search time. IBAG contains only domains\nrelated pages and are constructed from Relevant Page Graph (RPaG). We have also\nprovided a comparative study of time complexity for the various models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Feb 2011 16:04:50 GMT"}], "update_date": "2011-02-08", "authors_parsed": [["Mukhopadhyay", "Debajyoti", ""], ["Sinha", "Sukanta", ""]]}, {"id": "1102.1803", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed", "title": "Proposing LT based Search in PDM Systems for Better Information\n  Retrieval", "comments": "15 pages, 31 figures", "journal-ref": "International Journal of Computer Science & Emerging Technologies\n  (E-ISSN: 2044-6004), Volume 1, Issue 4, P86-100, December 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  PDM Systems contain and manage heavy amount of data but the search mechanism\nof most of the systems is not intelligent which can process user\"s natural\nlanguage based queries to extract desired information. Currently available\nsearch mechanisms in almost all of the PDM systems are not very efficient and\nbased on old ways of searching information by entering the relevant information\nto the respective fields of search forms to find out some specific information\nfrom attached repositories. Targeting this issue, a thorough research was\nconducted in fields of PDM Systems and Language Technology. Concerning the PDM\nSystem, conducted research provides the information about PDM and PDM Systems\nin detail. Concerning the field of Language Technology, helps in implementing a\nsearch mechanism for PDM Systems to search user\"s needed information by\nanalyzing user\"s natural language based requests. The accomplished goal of this\nresearch was to support the field of PDM with a new proposition of a conceptual\nmodel for the implementation of natural language based search. The proposed\nconceptual model is successfully designed and partially implementation in the\nform of a prototype. Describing the proposition in detail the main concept,\nimplementation designs and developed prototype of proposed approach is\ndiscussed in this paper. Implemented prototype is compared with respective\nfunctions of existing PDM systems .i.e., Windchill and CIM to evaluate its\neffectiveness against targeted challenges.\n", "versions": [{"version": "v1", "created": "Wed, 9 Feb 2011 08:16:50 GMT"}], "update_date": "2011-02-10", "authors_parsed": [["Ahmed", "Zeeshan", ""]]}, {"id": "1102.2684", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "Chernoff information of exponential families", "comments": null, "journal-ref": "IEEE Signal Processing Letters 20.3 (2013): 269-272", "doi": "10.1109/LSP.2013.2243726", "report-no": null, "categories": "cs.IT cs.CV cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chernoff information upper bounds the probability of error of the optimal\nBayesian decision rule for $2$-class classification problems. However, it turns\nout that in practice the Chernoff bound is hard to calculate or even\napproximate. In statistics, many usual distributions, such as Gaussians,\nPoissons or frequency histograms called multinomials, can be handled in the\nunified framework of exponential families. In this note, we prove that the\nChernoff information for members of the same exponential family can be either\nderived analytically in closed form, or efficiently approximated using a simple\ngeodesic bisection optimization technique based on an exact geometric\ncharacterization of the \"Chernoff point\" on the underlying statistical\nmanifold.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 06:35:51 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1102.2891", "submitter": "Michael J. Kurtz", "authors": "Michael J. Kurtz and Johan Bollen", "title": "Usage Bibliometrics", "comments": "Publisher's PDF (by permission). Publisher web site:\n  books.infotoday.com/asist/arist44.shtml", "journal-ref": "Annual Review of Information Science and Technology, vol 44, p.\n  3-64 (2010)", "doi": "10.1002/aris.2010.1440440108", "report-no": null, "categories": "cs.DL astro-ph.IM cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scholarly usage data provides unique opportunities to address the known\nshortcomings of citation analysis. However, the collection, processing and\nanalysis of usage data remains an area of active research. This article\nprovides a review of the state-of-the-art in usage-based informetric, i.e. the\nuse of usage data to study the scholarly process.\n", "versions": [{"version": "v1", "created": "Mon, 14 Feb 2011 20:58:28 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Kurtz", "Michael J.", ""], ["Bollen", "Johan", ""]]}, {"id": "1102.3306", "submitter": "Dennis Luxen", "authors": "Christian Jung, Daniel Karch, Sebastian Knopp, Dennis Luxen, and Peter\n  Sanders", "title": "Efficient Error-Correcting Geocoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of resolving a perhaps misspelled address of a location\ninto geographic coordinates of latitude and longitude. Our data structure\nsolves this problem within a few milliseconds even for misspelled and\nfragmentary queries. Compared to major geographic search engines such as Google\nor Bing we achieve results of significantly better quality.\n", "versions": [{"version": "v1", "created": "Wed, 16 Feb 2011 11:16:51 GMT"}], "update_date": "2011-02-17", "authors_parsed": [["Jung", "Christian", ""], ["Karch", "Daniel", ""], ["Knopp", "Sebastian", ""], ["Luxen", "Dennis", ""], ["Sanders", "Peter", ""]]}, {"id": "1102.3828", "submitter": "Herve Jegou", "authors": "Herv\\'e J\\'egou (INRIA - IRISA), Romain Tavenard (INRIA - IRISA),\n  Matthijs Douze (INRIA Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann, SED),\n  Laurent Amsaleg (INRIA - IRISA)", "title": "Searching in one billion vectors: re-rank with source coding", "comments": "International Conference on Acoustics, Speech and Signal Processing,\n  Prague : Czech Republic (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent indexing techniques inspired by source coding have been shown\nsuccessful to index billions of high-dimensional vectors in memory. In this\npaper, we propose an approach that re-ranks the neighbor hypotheses obtained by\nthese compressed-domain indexing methods. In contrast to the usual\npost-verification scheme, which performs exact distance calculation on the\nshort-list of hypotheses, the estimated distances are refined based on short\nquantization codes, to avoid reading the full vectors from disk. We have\nreleased a new public dataset of one billion 128-dimensional vectors and\nproposed an experimental setup to evaluate high dimensional indexing algorithms\non a realistic scale. Experiments show that our method accurately and\nefficiently re-ranks the neighbor hypotheses using little memory compared to\nthe full vectors representation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 13:15:37 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["J\u00e9gou", "Herv\u00e9", "", "INRIA - IRISA"], ["Tavenard", "Romain", "", "INRIA - IRISA"], ["Douze", "Matthijs", "", "INRIA Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann, SED"], ["Amsaleg", "Laurent", "", "INRIA - IRISA"]]}, {"id": "1102.3865", "submitter": "Thomas Mandl", "authors": "Thomas Mandl, Christa Womser-Hacker", "title": "Probability Based Clustering for Document and User Properties", "comments": "In: Ojala, Timo (ed.): Infotech Oulo International Workshop on\n  Information Retrieval (IR 2001). Oulo, Finnland. 19.- 21.9.2001. S. 100-107", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval systems can be improved by exploiting context\ninformation such as user and document features. This article presents a model\nbased on overlapping probabilistic or fuzzy clusters for such features. The\nmodel is applied within a fusion method which linearly combines several\nretrieval systems. The fusion is based on weights for the different retrieval\nsystems which are learned by exploiting relevance feedback information. This\ncalculation can be improved by maintaining a model for each document and user\ncluster. That way, the optimal retrieval system for each document or user type\ncan be identified and applied. The extension presented in this article allows\noverlapping, probabilistic clusters of features to further refine the process.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 16:03:52 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Mandl", "Thomas", ""], ["Womser-Hacker", "Christa", ""]]}, {"id": "1102.3866", "submitter": "Thomas Mandl", "authors": "Heiko Hellweg, J\\\"urgen Krause, Thomas Mandl, Jutta Marx, Matthias\n  N.O. M\\\"uller, Peter Mutschke, Robert Str\\\"otgen", "title": "Treatment of Semantic Heterogeneity in Information Retrieval", "comments": "Technical Report (Arbeitsbericht) GESIS - Leibniz Institute for the\n  Social Sciences", "journal-ref": null, "doi": null, "report-no": "IZ-Arbeitsbericht Nr. 23 2001", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step to handle semantic heterogeneity should be the attempt to\nenrich the semantic information about documents, i.e. to fill up the gaps in\nthe documents meta-data automatically. Section 2 describes a set of cascading\ndeductive and heuristic extraction rules, which were developed in the project\nCARMEN for the domain of Social Sciences. The mapping between different\nterminologies can be done by using intellectual, statistical and/or neural\nnetwork transfer modules. Intellectual transfers use cross-concordances between\ndifferent classification schemes or thesauri. Section 3 describes the creation,\nstorage and handling of such transfers.\n", "versions": [{"version": "v1", "created": "Fri, 18 Feb 2011 16:11:51 GMT"}], "update_date": "2011-02-21", "authors_parsed": [["Hellweg", "Heiko", ""], ["Krause", "J\u00fcrgen", ""], ["Mandl", "Thomas", ""], ["Marx", "Jutta", ""], ["M\u00fcller", "Matthias N. O.", ""], ["Mutschke", "Peter", ""], ["Str\u00f6tgen", "Robert", ""]]}, {"id": "1102.5458", "submitter": "Amruta Joshi", "authors": "Amruta Joshi, Junghoo Cho, Dragomir Radev, Ahmed Hassan", "title": "Improving Image Search based on User Created Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tag-based retrieval of multimedia content is a difficult problem, not only\nbecause of the shorter length of tags associated with images and videos, but\nalso due to mismatch in the terminologies used by searcher and content creator.\nTo alleviate this problem, we propose a simple concept-driven probabilistic\nmodel for improving text-based rich-media search. While our approach is similar\nto existing topic-based retrieval and cluster-based language modeling work,\nthere are two important differences: (1) our proposed model considers not only\nthe query-generation likelihood from cluster, but explicitly accounts for the\noverall \"popularity\" of the cluster or underlying concept, and (2) we explore\nthe possibility of inferring the likely concept relevant to a rich-media\ncontent through the user-created communities that the content belongs to.\n  We implement two methods of concept extraction: a traditional cluster based\napproach, and the proposed community based approach. We evaluate these two\ntechniques for how effectively they capture the intended meaning of a term from\nthe content creator and searcher, and their overall value in improving image\nsearch. Our results show that concept-driven search, though simple, clearly\noutperforms plain search. Among the two techniques for concept-driven search,\ncommunity-based approach is more successful, as the concepts generated from\nuser communities are found to be more intuitive and appealing.\n", "versions": [{"version": "v1", "created": "Sat, 26 Feb 2011 23:00:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Joshi", "Amruta", ""], ["Cho", "Junghoo", ""], ["Radev", "Dragomir", ""], ["Hassan", "Ahmed", ""]]}, {"id": "1102.5499", "submitter": "Linyuan Lu", "authors": "Linyuan Lu, Weiping Liu", "title": "Information filtering via preferential diffusion", "comments": "12 pages, 10 figures, 2 tables", "journal-ref": "Physical Review E 83, 066119 (2011)", "doi": "10.1103/PhysRevE.83.066119", "report-no": null, "categories": "physics.data-an cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have shown great potential to address information\noverload problem, namely to help users in finding interesting and relevant\nobjects within a huge information space. Some physical dynamics, including heat\nconduction process and mass or energy diffusion on networks, have recently\nfound applications in personalized recommendation. Most of the previous studies\nfocus overwhelmingly on recommendation accuracy as the only important factor,\nwhile overlook the significance of diversity and novelty which indeed provide\nthe vitality of the system. In this paper, we propose a recommendation\nalgorithm based on the preferential diffusion process on user-object bipartite\nnetwork. Numerical analyses on two benchmark datasets, MovieLens and Netflix,\nindicate that our method outperforms the state-of-the-art methods.\nSpecifically, it can not only provide more accurate recommendations, but also\ngenerate more diverse and novel recommendations by accurately recommending\nunpopular objects.\n", "versions": [{"version": "v1", "created": "Sun, 27 Feb 2011 13:12:53 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Lu", "Linyuan", ""], ["Liu", "Weiping", ""]]}, {"id": "1102.5728", "submitter": "Wahiba Ben abdessalem Karaa", "authors": "Wahiba Ben Abdessalem Karaa", "title": "Named Entity Recognition Using Web Document Corpus", "comments": "11 pages 4 figures, 2 tables", "journal-ref": "International Journal of Managing Information Technology (IJMIT)\n  Vol.3, No.1, February 2011", "doi": "10.5121/ijmit.2011.3104", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a named entity recognition approach in textual corpus.\nThis Named Entity (NE) can be a named: location, person, organization, date,\ntime, etc., characterized by instances. A NE is found in texts accompanied by\ncontexts: words that are left or right of the NE. The work mainly aims at\nidentifying contexts inducing the NE's nature. As such, The occurrence of the\nword \"President\" in a text, means that this word or context may be followed by\nthe name of a president as President \"Obama\". Likewise, a word preceded by the\nstring \"footballer\" induces that this is the name of a footballer. NE\nrecognition may be viewed as a classification method, where every word is\nassigned to a NE class, regarding the context. The aim of this study is then to\nidentify and classify the contexts that are most relevant to recognize a NE,\nthose which are frequently found with the NE. A learning approach using\ntraining corpus: web documents, constructed from learning examples is then\nsuggested. Frequency representations and modified tf-idf representations are\nused to calculate the context weights associated to context frequency, learning\nexample frequency, and document frequency in the corpus.\n", "versions": [{"version": "v1", "created": "Mon, 28 Feb 2011 18:33:09 GMT"}], "update_date": "2011-03-01", "authors_parsed": [["Karaa", "Wahiba Ben Abdessalem", ""]]}]