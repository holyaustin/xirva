[{"id": "2106.00062", "submitter": "Haonan Wang", "authors": "Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, Jingrui He", "title": "Controllable Gradient Item Retrieval", "comments": "Accepted by The International World Wide Web Conference (WWW), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify and study an important problem of gradient item\nretrieval. We define the problem as retrieving a sequence of items with a\ngradual change on a certain attribute, given a reference item and a\nmodification text. For example, after a customer saw a white dress, she/he\nwants to buy a similar one but more floral on it. The extent of \"more floral\"\nis subjective, thus prompting one floral dress is hard to satisfy the\ncustomer's needs. A better way is to present a sequence of products with\nincreasingly floral attributes based on the white dress, and allow the customer\nto select the most satisfactory one from the sequence. Existing item retrieval\nmethods mainly focus on whether the target items appear at the top of the\nretrieved sequence, but ignore the demand for retrieving a sequence of products\nwith gradual change on a certain attribute. To deal with this problem, we\npropose a weakly-supervised method that can learn a disentangled item\nrepresentation from user-item interaction data and ground the semantic meaning\nof attributes to dimensions of the item representation. Our method takes a\nreference item and a modification as a query. During inference, we start from\nthe reference item and \"walk\" along the direction of the modification in the\nitem representation space to retrieve a sequence of items in a gradient manner.\nWe demonstrate our proposed method can achieve disentanglement through weak\nsupervision. Besides, we empirically show that an item sequence retrieved by\nour method is gradually changed on an indicated attribute and, in the item\nretrieval task, our method outperforms existing approaches on three different\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:07:17 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Haonan", ""], ["Zhou", "Chang", ""], ["Yang", "Carl", ""], ["Yang", "Hongxia", ""], ["He", "Jingrui", ""]]}, {"id": "2106.00102", "submitter": "Michal Kompan", "authors": "Juraj Visnovsky, Ondrej Kassak, Michal Kompan, Maria Bielikova", "title": "The Cold-start Problem: Minimal Users' Activity Estimation", "comments": "1st Workshop on Recommender Systems for Television and online Video\n  (RecSysTV) in conjunction with 8th ACM Conference on Recommender Systems,\n  2014", "journal-ref": "1st Workshop on Recommender Systems for Television and online\n  Video (RecSysTV) in conjunction with 8th ACM Conference on Recommender\n  Systems, 2014", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cold-start problem, which arises upon the new users arrival, is one of the\nfundamental problems in today's recommender approaches. Moreover, in some\ndomains as TV or multime-dia-items take long time to experience by users, thus\nusers usually do not provide rich preference information. In this paper we\nanalyze the minimal amount of ratings needs to be done by a user over a set of\nitems, in order to solve or reduce the cold-start problem. In our analysis we\napplied clustering data mining technique in order to identify minimal amount of\nitem's ratings required from recommender system's users, in order to be\nassigned to a correct cluster. In this context, cluster quality is being\nmonitored and in case of reaching certain cluster quality threshold, the\nrec-ommender system could start to generate recommendations for given user, as\nin this point cold-start problem is considered as resolved. Our proposed\napproach is applicable to any domain in which user preferences are received\nbased on explicit items rating. Our experiments are performed within the movie\nand jokes recommendation domain using the MovieLens and Jester dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 21:09:23 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Visnovsky", "Juraj", ""], ["Kassak", "Ondrej", ""], ["Kompan", "Michal", ""], ["Bielikova", "Maria", ""]]}, {"id": "2106.00142", "submitter": "Ujun Jeong", "authors": "Ujun Jeong, Kaize Ding, Huan Liu", "title": "FBAdTracker: An Interactive Data Collection and Analysis Tool for\n  Facebook Advertisements", "comments": "3 pages, 1 figure, 2021 International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, demo track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The growing use of social media has led to drastic changes in our\ndecision-making. Especially, Facebook offers marketing API which promotes\nbusiness to target potential groups who are likely to consume their items.\nHowever, this service can be abused by malicious advertisers who attempt to\ndeceive people by disinformation such as propaganda and divisive opinion. To\ncounter this problem, we introduce a new application named FBAdTracker. The\npurpose of this application is to provide an integrated data collection and\nanalysis system for current research on fact-checking related to Facebook\nadvertisements. Our system is capable of monitoring up-to-date Facebook ads and\nanalyzing ads retrieved from Facebook Ads Library.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 23:20:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Jeong", "Ujun", ""], ["Ding", "Kaize", ""], ["Liu", "Huan", ""]]}, {"id": "2106.00302", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Anastasia Krithara, Grigorios Tsoumakas, Georgios\n  Paliouras", "title": "Harvesting the Public MeSH Note field", "comments": "3 pages, 1 figure, 1 table. Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we report an analysis of the Public MeSH Note field of the\nnew descriptors introduced in the MeSH thesaurus between 2006 and 2020. The aim\nof this analysis was to extract information about the previous status of these\nnew descriptors as Supplementary Concept Records. The Public MeSH Note field\ncontains information in semi-structured text, meant to be read by humans.\nTherefore, we adopted a semi-automated approach, based on regular expressions,\nto extract information from it. In the large majority of cases, we managed to\nminimize the required manual effort for extracting the previous state of a new\ndescriptor as a Supplementary Concept Record. The source code for this analysis\nis openly available on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:17:13 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Krithara", "Anastasia", ""], ["Tsoumakas", "Grigorios", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2106.00314", "submitter": "Wei Guo", "authors": "Wei Guo, Rong Su, Renhao Tan, Huifeng Guo, Yingxue Zhang, Zhirong Liu,\n  Ruiming Tang, Xiuqiang He", "title": "Dual Graph enhanced Embedding Neural Network for CTR Prediction", "comments": "KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  CTR prediction, which aims to estimate the probability that a user will click\nan item, plays a crucial role in online advertising and recommender system.\nFeature interaction modeling based and user interest mining based methods are\nthe two kinds of most popular techniques that have been extensively explored\nfor many years and have made great progress for CTR prediction. However, (1)\nfeature interaction based methods which rely heavily on the co-occurrence of\ndifferent features, may suffer from the feature sparsity problem (i.e., many\nfeatures appear few times); (2) user interest mining based methods which need\nrich user behaviors to obtain user's diverse interests, are easy to encounter\nthe behavior sparsity problem (i.e., many users have very short behavior\nsequences). To solve these problems, we propose a novel module named Dual Graph\nenhanced Embedding, which is compatible with various CTR prediction models to\nalleviate these two problems. We further propose a Dual Graph enhanced\nEmbedding Neural Network (DG-ENN) for CTR prediction. Dual Graph enhanced\nEmbedding exploits the strengths of graph representation with two carefully\ndesigned learning strategies (divide-and-conquer, curriculum-learning-inspired\norganized learning) to refine the embedding. We conduct comprehensive\nexperiments on three real-world industrial datasets. The experimental results\nshow that our proposed DG-ENN significantly outperforms state-of-the-art CTR\nprediction models. Moreover, when applying to state-of-the-art CTR prediction\nmodels, Dual graph enhanced embedding always obtains better performance.\nFurther case studies prove that our proposed dual graph enhanced embedding\ncould alleviate the feature sparsity and behavior sparsity problems. Our\nframework will be open-source based on MindSpore in the near future.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:43:31 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 09:02:40 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Guo", "Wei", ""], ["Su", "Rong", ""], ["Tan", "Renhao", ""], ["Guo", "Huifeng", ""], ["Zhang", "Yingxue", ""], ["Liu", "Zhirong", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""]]}, {"id": "2106.00411", "submitter": "D\\'avid Lupt\\'ak", "authors": "D\\'avid Lupt\\'ak, V\\'it Novotn\\'y, Michal \\v{S}tef\\'anik, and Petr\n  Sojka", "title": "WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code", "comments": "Accepted to be published in: Intelligent Computer Mathematics 14th\n  International Conference, CICM 2021, Timisoara, Romania, July 26--31, 2021,\n  Proceedings, Fairouz Kamareddine and Claudio Sacerdotti-Coen (eds.), Lecture\n  Notes in Artificial Intelligence, Springer, Cham, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Math informational retrieval (MIR) search engines are absent in the\nwide-spread production use, even though documents in the STEM fields contain\nmany mathematical formulae, which are sometimes more important than text for\nunderstanding. We have developed and open-sourced the WebMIaS MIR search engine\nthat has been successfully deployed in the European Digital Mathematics Library\n(EuDML). However, its deployment is difficult to automate due to the complexity\nof this task. Moreover, the solutions developed so far to tackle this challenge\nare imperfect in terms of speed, maintenance, and robustness. In this paper, we\nwill describe the virtualization of WebMIaS using Docker that solves all three\nproblems and allows anyone to deploy containerized WebMIaS in a single line of\ncode. The publicly available Docker image will also help the community push the\ndevelopment of math-aware search engines in the ARQMath workshop series.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:49:37 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 08:12:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lupt\u00e1k", "D\u00e1vid", ""], ["Novotn\u00fd", "V\u00edt", ""], ["\u0160tef\u00e1nik", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "2106.00573", "submitter": "Hanock Kwak", "authors": "Kyuyong Shin, Hanock Kwak, Kyung-Min Kim, Minkyu Kim, Young-Jin Park,\n  Jisu Jeong, Seungjae Jung", "title": "One4all User Representation for Recommender Systems in E-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General-purpose representation learning through large-scale pre-training has\nshown promising results in the various machine learning fields. For an\ne-commerce domain, the objective of general-purpose, i.e., one for all,\nrepresentations would be efficient applications for extensive downstream tasks\nsuch as user profiling, targeting, and recommendation tasks. In this paper, we\nsystematically compare the generalizability of two learning strategies, i.e.,\ntransfer learning through the proposed model, ShopperBERT, vs. learning from\nscratch. ShopperBERT learns nine pretext tasks with 79.2M parameters from 0.8B\nuser behaviors collected over two years to produce user embeddings. As a\nresult, the MLPs that employ our embedding method outperform more complex\nmodels trained from scratch for five out of six tasks. Specifically, the\npre-trained embeddings have superiority over the task-specific supervised\nfeatures and the strong baselines, which learn the auxiliary dataset for the\ncold-start problem. We also show the computational efficiency and embedding\nvisualization of the pre-trained features.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 03:17:05 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Shin", "Kyuyong", ""], ["Kwak", "Hanock", ""], ["Kim", "Kyung-Min", ""], ["Kim", "Minkyu", ""], ["Park", "Young-Jin", ""], ["Jeong", "Jisu", ""], ["Jung", "Seungjae", ""]]}, {"id": "2106.00590", "submitter": "Jialu Liu", "authors": "Jialu Liu, Tianqi Liu, Cong Yu", "title": "NewsEmbed: Modeling News through Pre-trained Document Representations", "comments": "Accepted in SIGKDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467392", "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effectively modeling text-rich fresh content such as news articles at\ndocument-level is a challenging problem. To ensure a content-based model\ngeneralize well to a broad range of applications, it is critical to have a\ntraining dataset that is large beyond the scale of human labels while achieving\ndesired quality. In this work, we address those two challenges by proposing a\nnovel approach to mine semantically-relevant fresh documents, and their topic\nlabels, with little human supervision. Meanwhile, we design a multitask model\ncalled NewsEmbed that alternatively trains a contrastive learning with a\nmulti-label classification to derive a universal document encoder. We show that\nthe proposed approach can provide billions of high quality organic training\nexamples and can be naturally extended to multilingual setting where texts in\ndifferent languages are encoded in the same semantic space. We experimentally\ndemonstrate NewsEmbed's competitive performance across multiple natural\nlanguage understanding tasks, both supervised and unsupervised.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:59:40 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 03:00:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Liu", "Jialu", ""], ["Liu", "Tianqi", ""], ["Yu", "Cong", ""]]}, {"id": "2106.00874", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Wei Emma Zhang and Quan Z. Sheng and Adnan Mahmood\n  and Yang Zhang", "title": "Conversational Question Answering: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) systems provide a way of querying the information\navailable in various formats including, but not limited to, unstructured and\nstructured data in natural languages. It constitutes a considerable part of\nconversational artificial intelligence (AI) which has led to the introduction\nof a special research topic on Conversational Question Answering (CQA), wherein\na system is required to understand the given context and then engages in\nmulti-turn QA to satisfy the user's information needs. Whilst the focus of most\nof the existing research work is subjected to single-turn QA, the field of\nmulti-turn QA has recently grasped attention and prominence owing to the\navailability of large-scale, multi-turn QA datasets and the development of\npre-trained language models. With a good amount of models and research papers\nadding to the literature every year recently, there is a dire need of arranging\nand presenting the related work in a unified manner to streamline future\nresearch. This survey, therefore, is an effort to present a comprehensive\nreview of the state-of-the-art research trends of CQA primarily based on\nreviewed papers from 2016-2021. Our findings show that there has been a trend\nshift from single-turn to multi-turn QA which empowers the field of\nConversational AI from different perspectives. This survey is intended to\nprovide an epitome for the research community with the hope of laying a strong\nfoundation for the field of CQA.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:06:34 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:02:38 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zaib", "Munazza", ""], ["Zhang", "Wei Emma", ""], ["Sheng", "Quan Z.", ""], ["Mahmood", "Adnan", ""], ["Zhang", "Yang", ""]]}, {"id": "2106.00882", "submitter": "Ikuya Yamada", "authors": "Ikuya Yamada, Akari Asai, Hannaneh Hajishirzi", "title": "Efficient Passage Retrieval with Hashing for Open-domain Question\n  Answering", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art open-domain question answering systems use a neural\nretrieval model to encode passages into continuous vectors and extract them\nfrom a knowledge source. However, such retrieval models often require large\nmemory to run because of the massive size of their passage index. In this\npaper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural\nretrieval model that integrates a learning-to-hash technique into the\nstate-of-the-art Dense Passage Retriever (DPR) to represent the passage index\nusing compact binary codes rather than continuous vectors. BPR is trained with\na multi-task objective over two tasks: efficient candidate generation based on\nbinary codes and accurate reranking based on continuous vectors. Compared with\nDPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss\nof accuracy on two standard open-domain question answering benchmarks: Natural\nQuestions and TriviaQA. Our code and trained models are available at\nhttps://github.com/studio-ousia/bpr.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 01:34:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yamada", "Ikuya", ""], ["Asai", "Akari", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2106.01033", "submitter": "Kunwoo Park", "authors": "Kunwoo Park, Zhufeng Pan, and Jungseock Joo", "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment\n  Extraction in News Text", "comments": "Published in Findings of ACL 2021 (Long paper). The manuscript is\n  slightly revised after the camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:02:14 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 07:32:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Park", "Kunwoo", ""], ["Pan", "Zhufeng", ""], ["Joo", "Jungseock", ""]]}, {"id": "2106.01149", "submitter": "Ho-Hsiang Wu", "authors": "Ho-Hsiang Wu, Magdalena Fuentes, and Juan P. Bello", "title": "Exploring modality-agnostic representations for music classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music information is often conveyed or recorded across multiple data\nmodalities including but not limited to audio, images, text and scores.\nHowever, music information retrieval research has almost exclusively focused on\nsingle modality recognition, requiring development of separate models for each\nmodality. Some multi-modal works require multiple coexisting modalities given\nto the model as inputs, constraining the use of these models to the few cases\nwhere data from all modalities are available. To the best of our knowledge, no\nexisting model has the ability to take inputs from varying modalities, e.g.\nimages or sounds, and classify them into unified music categories. We explore\nthe use of cross-modal retrieval as a pretext task to learn modality-agnostic\nrepresentations, which can then be used as inputs to classifiers that are\nindependent of modality. We select instrument classification as an example task\nfor our study as both visual and audio components provide relevant semantic\ninformation. We train music instrument classifiers that can take both images or\nsounds as input, and perform comparably to sound-only or image-only\nclassifiers. Furthermore, we explore the case when there is limited labeled\ndata for a given modality, and the impact in performance by using labeled data\nfrom other modalities. We are able to achieve almost 70% of best performing\nsystem in a zero-shot setting. We provide a detailed analysis of experimental\nresults to understand the potential and limitations of the approach, and\ndiscuss future steps towards modality-agnostic classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:39:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wu", "Ho-Hsiang", ""], ["Fuentes", "Magdalena", ""], ["Bello", "Juan P.", ""]]}, {"id": "2106.01232", "submitter": "Kiran Sharma Dr.", "authors": "Parul Khurana, Geetha Ganesan, Gulshan Kumar, Kiran Sharma", "title": "A weighted unified informetrics based on Scopus and WoS", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous indexing databases keep track of the number of publications,\ncitations, etc. in order to maintain the progress of science and individual.\nHowever, the choice of journals and articles varies among these indexing\ndatabases, hence the number of citations and h-index varies. There is no common\nplatform exists that can provide a single count for the number of publications,\ncitations, h-index, etc. To overcome this limitation, we have proposed a\nweighted unified informetrics, named \"conflate\". The proposed system takes into\naccount the input from multiple indexing databases and generates a single\noutput. Here, we have used the data from Scopus and WoS to generate a conflate\ndataset. Further, a comparative analysis of conflate has been performed with\nScopus and WoS at three levels: author, organization, and journal. Finally, a\nmapping is proposed between research publications and distributed ledger\ntechnology in order to provide a transparent and distributed view to its\nstakeholders.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:30:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Khurana", "Parul", ""], ["Ganesan", "Geetha", ""], ["Kumar", "Gulshan", ""], ["Sharma", "Kiran", ""]]}, {"id": "2106.01251", "submitter": "Vishal Vinod", "authors": "Vishal Vinod, Susmit Agrawal, Vipul Gaurav, Pallavi R, Savita\n  Choudhary", "title": "Multilingual Medical Question Answering and Information Retrieval for\n  Rural Health Intelligence Access", "comments": null, "journal-ref": "ICLR 2021 Workshop", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In rural regions of several developing countries, access to quality\nhealthcare, medical infrastructure, and professional diagnosis is largely\nunavailable. Many of these regions are gradually gaining access to internet\ninfrastructure, although not with a strong enough connection to allow for\nsustained communication with a medical practitioner. Several deaths resulting\nfrom this lack of medical access, absence of patient's previous health records,\nand the unavailability of information in indigenous languages can be easily\nprevented. In this paper, we describe an approach leveraging the phenomenal\nprogress in Machine Learning and NLP (Natural Language Processing) techniques\nto design a model that is low-resource, multilingual, and a preliminary\nfirst-point-of-contact medical assistant. Our contribution includes defining\nthe NLP pipeline required for named-entity-recognition, language-agnostic\nsentence embedding, natural language translation, information retrieval,\nquestion answering, and generative pre-training for final query processing. We\nobtain promising results for this pipeline and preliminary results for EHR\n(Electronic Health Record) analysis with text summarization for medical\npractitioners to peruse for their diagnosis. Through this NLP pipeline, we aim\nto provide preliminary medical information to the user and do not claim to\nsupplant diagnosis from qualified medical practitioners. Using the input from\nsubject matter experts, we have compiled a large corpus to pre-train and\nfine-tune our BioBERT based NLP model for the specific tasks. We expect recent\nadvances in NLP architectures, several of which are efficient and\nprivacy-preserving models, to further the impact of our solution and improve on\nindividual task performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:05:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vinod", "Vishal", ""], ["Agrawal", "Susmit", ""], ["Gaurav", "Vipul", ""], ["R", "Pallavi", ""], ["Choudhary", "Savita", ""]]}, {"id": "2106.01300", "submitter": "Tao Qi", "authors": "Tao Qi, Fangzhao Wu, Chuhan Wu, Yongfeng Huang", "title": "PP-Rec: News Recommendation with Personalized User Interest and\n  Time-aware News Popularity", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized news recommendation methods are widely used in online news\nservices. These methods usually recommend news based on the matching between\nnews content and user interest inferred from historical behaviors. However,\nthese methods usually have difficulties in making accurate recommendations to\ncold-start users, and tend to recommend similar news with those users have\nread. In general, popular news usually contain important information and can\nattract users with different interests. Besides, they are usually diverse in\ncontent and topic. Thus, in this paper we propose to incorporate news\npopularity information to alleviate the cold-start and diversity problems for\npersonalized news recommendation. In our method, the ranking score for\nrecommending a candidate news to a target user is the combination of a\npersonalized matching score and a news popularity score. The former is used to\ncapture the personalized user interest in news. The latter is used to measure\ntime-aware popularity of candidate news, which is predicted based on news\ncontent, recency, and real-time CTR using a unified framework. Besides, we\npropose a popularity-aware user encoder to eliminate the popularity bias in\nuser behaviors for accurate interest modeling. Experiments on two real-world\ndatasets show our method can effectively improve the accuracy and diversity for\nnews recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:15:57 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 08:51:09 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Qi", "Tao", ""], ["Wu", "Fangzhao", ""], ["Wu", "Chuhan", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.01490", "submitter": "Yuan Tian Miss", "authors": "Yuan Tian, Ke Zhou, Dan Pelleg", "title": "What and How long: Prediction of Mobile App Engagement", "comments": "38 pages, to appear in ACM Transactions on Information Systems 2021", "journal-ref": null, "doi": "10.1145/3464301", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User engagement is crucial to the long-term success of a mobile app. Several\nmetrics, such as dwell time, have been used for measuring user engagement.\nHowever, how to effectively predict user engagement in the context of mobile\napps is still an open research question. For example, do the mobile usage\ncontexts (e.g.,~time of day) in which users access mobile apps impact their\ndwell time? Answers to such questions could help mobile operating system and\npublishers to optimize advertising and service placement. In this paper, we\nfirst conduct an empirical study for assessing how user characteristics,\ntemporal features, and the short/long-term contexts contribute to gains in\npredicting users' app dwell time on the population level. The comprehensive\nanalysis is conducted on large app usage logs collected through a mobile\nadvertising company. The dataset covers more than 12K anonymous users and 1.3\nmillion log events. Based on the analysis, we further investigate a novel\nmobile app engagement prediction problem -- can we predict simultaneously what\napp the user will use next and how long he/she will stay on that app? We\npropose several strategies for this joint prediction problem and demonstrate\nthat our model can improve the performance significantly when compared with the\nstate-of-the-art baselines. Our work can help mobile system developers in\ndesigning a better and more engagement-aware mobile app user experience.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 22:10:09 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tian", "Yuan", ""], ["Zhou", "Ke", ""], ["Pelleg", "Dan", ""]]}, {"id": "2106.01674", "submitter": "Hao Liu", "authors": "Hao Liu, Qian Gao, Jiang Li, Xiaochao Liao, Hao Xiong, Guangxing Chen,\n  Wenlin Wang, Guobao Yang, Zhiwei Zha, Daxiang Dong, Dejing Dou, Haoyi Xiong", "title": "JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale\n  Online Inference at Baidu", "comments": "Accepted to SIGKDD 2021 applied data science track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modern internet industries, deep learning based recommender systems have\nbecame an indispensable building block for a wide spectrum of applications,\nsuch as search engine, news feed, and short video clips. However, it remains\nchallenging to carry the well-trained deep models for online real-time\ninference serving, with respect to the time-varying web-scale traffics from\nbillions of users, in a cost-effective manner. In this work, we present JIZHI -\na Model-as-a-Service system - that per second handles hundreds of millions of\nonline inference requests to huge deep models with more than trillions of\nsparse parameters, for over twenty real-time recommendation services at Baidu,\nInc. In JIZHI, the inference workflow of every recommendation request is\ntransformed to a Staged Event-Driven Pipeline (SEDP), where each node in the\npipeline refers to a staged computation or I/O intensive task processor. With\ntraffics of real-time inference requests arrived, each modularized processor\ncan be run in a fully asynchronized way and managed separately. Besides, JIZHI\nintroduces heterogeneous and hierarchical storage to further accelerate the\nonline inference process by reducing unnecessary computations and potential\ndata access latency induced by ultra-sparse model parameters. Moreover, an\nintelligent resource manager has been deployed to maximize the throughput of\nJIZHI over the shared infrastructure by searching the optimal resource\nallocation plan from historical logs and fine-tuning the load shedding policies\nover intermediate system feedback. Extensive experiments have been done to\ndemonstrate the advantages of JIZHI from the perspectives of end-to-end service\nlatency, system-wide throughput, and resource consumption. JIZHI has helped\nBaidu saved more than ten million US dollars in hardware and utility costs\nwhile handling 200% more traffics without sacrificing inference efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:23:24 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Hao", ""], ["Gao", "Qian", ""], ["Li", "Jiang", ""], ["Liao", "Xiaochao", ""], ["Xiong", "Hao", ""], ["Chen", "Guangxing", ""], ["Wang", "Wenlin", ""], ["Yang", "Guobao", ""], ["Zha", "Zhiwei", ""], ["Dong", "Daxiang", ""], ["Dou", "Dejing", ""], ["Xiong", "Haoyi", ""]]}, {"id": "2106.01706", "submitter": "Saeid Hosseini", "authors": "Sara Kamran, Raziyeh Zall, Mohammad Reza Kangavari, Saeid Hosseini,\n  Sana Rahmani, and Wen Hua", "title": "EmoDNN: Understanding emotions from short texts through a deep neural\n  network ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The latent knowledge in the emotions and the opinions of the individuals that\nare manifested via social networks are crucial to numerous applications\nincluding social management, dynamical processes, and public security.\nAffective computing, as an interdisciplinary research field, linking artificial\nintelligence to cognitive inference, is capable to exploit emotion-oriented\nknowledge from brief contents. The textual contents convey hidden information\nsuch as personality and cognition about corresponding authors that can\ndetermine both correlations and variations between users. Emotion recognition\nfrom brief contents should embrace the contrast between authors where the\ndifferences in personality and cognition can be traced within emotional\nexpressions. To tackle this challenge, we devise a framework that, on the one\nhand, infers latent individual aspects, from brief contents and, on the other\nhand, presents a novel ensemble classifier equipped with dynamic dropout\nconvnets to extract emotions from textual context. To categorize short text\ncontents, our proposed method conjointly leverages cognitive factors and\nexploits hidden information. We utilize the outcome vectors in a novel\nembedding model to foster emotion-pertinent features that are collectively\nassembled by lexicon inductions. Experimental results show that compared to\nother competitors, our proposed model can achieve a higher performance in\nrecognizing emotion from noisy contents.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:17:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Kamran", "Sara", ""], ["Zall", "Raziyeh", ""], ["Kangavari", "Mohammad Reza", ""], ["Hosseini", "Saeid", ""], ["Rahmani", "Sana", ""], ["Hua", "Wen", ""]]}, {"id": "2106.01941", "submitter": "Yi Su", "authors": "Yi Su, Magd Bayoumi, Thorsten Joachims", "title": "Optimizing Rankings for Recommendation in Matching Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the success of recommender systems in e-commerce, there is growing\ninterest in their use in matching markets (e.g., labor). While this holds\npotential for improving market fluidity and fairness, we show in this paper\nthat naively applying existing recommender systems to matching markets is\nsub-optimal. Considering the standard process where candidates apply and then\nget evaluated by employers, we present a new recommendation framework to model\nthis interaction mechanism and propose efficient algorithms for computing\npersonalized rankings in this setting. We show that the optimal rankings need\nto not only account for the potentially divergent preferences of candidates and\nemployers, but they also need to account for capacity constraints. This makes\nconventional ranking systems that merely rank by some local score (e.g.,\none-sided or reciprocal relevance) highly sub-optimal -- not only for an\nindividual user, but also for societal goals (e.g., low unemployment). To\naddress this shortcoming, we propose the first method for jointly optimizing\nthe rankings for all candidates in the market to explicitly maximize social\nwelfare. In addition to the theoretical derivation, we evaluate the method both\non simulated environments and on data from a real-world\nnetworking-recommendation system that we built and fielded at a large computer\nscience conference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 15:46:02 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Su", "Yi", ""], ["Bayoumi", "Magd", ""], ["Joachims", "Thorsten", ""]]}, {"id": "2106.02223", "submitter": "Rocky Chen", "authors": "Tong Chen, Hongzhi Yin, Yujia Zheng, Zi Huang, Yang Wang, Meng Wang", "title": "Learning Elastic Embeddings for Customizing On-Device Recommenders", "comments": "To appear in KDD'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's context, deploying data-driven services like recommendation on\nedge devices instead of cloud servers becomes increasingly attractive due to\nprivacy and network latency concerns. A common practice in building compact\non-device recommender systems is to compress their embeddings which are\nnormally the cause of excessive parameterization. However, despite the vast\nvariety of devices and their associated memory constraints, existing\nmemory-efficient recommender systems are only specialized for a fixed memory\nbudget in every design and training life cycle, where a new model has to be\nretrained to obtain the optimal performance while adapting to a smaller/larger\nmemory budget. In this paper, we present a novel lightweight recommendation\nparadigm that allows a well-trained recommender to be customized for arbitrary\ndevice-specific memory constraints without retraining. The core idea is to\ncompose elastic embeddings for each item, where an elastic embedding is the\nconcatenation of a set of embedding blocks that are carefully chosen by an\nautomated search function. Correspondingly, we propose an innovative approach,\nnamely recommendation with universally learned elastic embeddings (RULE). To\nensure the expressiveness of all candidate embedding blocks, RULE enforces a\ndiversity-driven regularization when learning different embedding blocks. Then,\na performance estimator-based evolutionary search function is designed,\nallowing for efficient specialization of elastic embeddings under any memory\nconstraint for on-device recommendation. Extensive experiments on real-world\ndatasets reveal the superior performance of RULE under tight memory budgets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 02:56:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Tong", ""], ["Yin", "Hongzhi", ""], ["Zheng", "Yujia", ""], ["Huang", "Zi", ""], ["Wang", "Yang", ""], ["Wang", "Meng", ""]]}, {"id": "2106.02250", "submitter": "Yihong Zhang", "authors": "Yihong Zhang, Masumi Shirakawa and Takahiro Hara", "title": "A General Method for Event Detection on Social Media", "comments": "Accepted for presentation in the 25th European Conference on Advances\n  in Databases and Information Systems (ADBIS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event detection on social media has attracted a number of researches, given\nthe recent availability of large volumes of social media discussions. Previous\nworks on social media event detection either assume a specific type of event,\nor assume certain behavior of observed variables. In this paper, we propose a\ngeneral method for event detection on social media that makes few assumptions.\nThe main assumption we make is that when an event occurs, affected semantic\naspects will behave differently from its usual behavior. We generalize the\nrepresentation of time units based on word embeddings of social media text, and\npropose an algorithm to detect events in time series in a general sense. In the\nexperimental evaluation, we use a novel setting to test if our method and\nbaseline methods can exhaustively catch all real-world news in the test period.\nThe evaluation results show that when the event is quite unusual with regard to\nthe base social media discussion, it can be captured more effectively with our\nmethod. Our method can be easily implemented and can be treated as a starting\npoint for more specific applications.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:29:20 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Yihong", ""], ["Shirakawa", "Masumi", ""], ["Hara", "Takahiro", ""]]}, {"id": "2106.02256", "submitter": "Yihong Zhang", "authors": "Yihong Zhang, Takuya Maekawa, and Takahiro Hara", "title": "Using Social Media Background to Improve Cold-start Recommendation Deep\n  Models", "comments": "Accepted for presentation in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recommender systems, a cold-start problem occurs when there is no past\ninteraction record associated with the user or item. Typical solutions to the\ncold-start problem make use of contextual information, such as user demographic\nattributes or product descriptions. A group of works have shown that social\nmedia background can help predicting temporal phenomenons such as product sales\nand stock price movements. In this work, our goal is to investigate whether\nsocial media background can be used as extra contextual information to improve\nrecommendation models. Based on an existing deep neural network model, we\nproposed a method to represent temporal social media background as embeddings\nand fuse them as an extra component in the model. We conduct experimental\nevaluations on a real-world e-commerce dataset and a Twitter dataset. The\nresults show that our method of fusing social media background with the\nexisting model does generally improve recommendation performance. In some cases\nthe recommendation accuracy measured by hit-rate@K doubles after fusing with\nsocial media background. Our findings can be beneficial for future recommender\nsystem designs that consider complex temporal information representing social\ninterests.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 04:46:29 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhang", "Yihong", ""], ["Maekawa", "Takuya", ""], ["Hara", "Takahiro", ""]]}, {"id": "2106.02293", "submitter": "Yanda Chen", "authors": "Yanda Chen, Chris Kedzie, Suraj Nair, Petra Galu\\v{s}\\v{c}\\'akov\\'a,\n  Rui Zhang, Douglas W. Oard, Kathleen McKeown", "title": "Cross-language Sentence Selection via Data Augmentation and Rationale\n  Training", "comments": "ACL 2021 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to cross-language sentence selection in a\nlow-resource setting. It uses data augmentation and negative sampling\ntechniques on noisy parallel sentence data to directly learn a cross-lingual\nembedding-based query relevance model. Results show that this approach performs\nas well as or better than multiple state-of-the-art machine translation +\nmonolingual retrieval systems trained on the same parallel data. Moreover, when\na rationale training secondary objective is applied to encourage the model to\nmatch word alignment hints from a phrase-based statistical machine translation\nmodel, consistent improvements are seen across three language pairs\n(English-Somali, English-Swahili and English-Tagalog) over a variety of\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:08:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chen", "Yanda", ""], ["Kedzie", "Chris", ""], ["Nair", "Suraj", ""], ["Galu\u0161\u010d\u00e1kov\u00e1", "Petra", ""], ["Zhang", "Rui", ""], ["Oard", "Douglas W.", ""], ["McKeown", "Kathleen", ""]]}, {"id": "2106.02361", "submitter": "Enrico Daga", "authors": "Enrico Daga, Luigi Asprino, Paul Mulholland, Aldo Gangemi", "title": "Facade-X: an opinionated approach to SPARQL anything", "comments": "Version submitted to the SEMANTICS 2021 EU conference (Accepted May\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Semantic Web research community understood since its beginning how\ncrucial it is to equip practitioners with methods to transform non-RDF\nresources into RDF. Proposals focus on either engineering content\ntransformations or accessing non-RDF resources with SPARQL. Existing solutions\nrequire users to learn specific mapping languages (e.g. RML), to know how to\nquery and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to\ncombine multiple languages (e.g. SPARQL Generate). In this paper, we explore an\nalternative solution and contribute a general-purpose meta-model for converting\nnon-RDF resources into RDF: Facade-X. Our approach can be implemented by\noverriding the SERVICE operator and does not require to extend the SPARQL\nsyntax. We compare our approach with the state of art methods RML and SPARQL\nGenerate and show how our solution has lower learning demands and cognitive\ncomplexity, and it is cheaper to implement and maintain, while having\ncomparable extensibility and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:19:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Daga", "Enrico", ""], ["Asprino", "Luigi", ""], ["Mulholland", "Paul", ""], ["Gangemi", "Aldo", ""]]}, {"id": "2106.02400", "submitter": "Manh-Duy Nguyen", "authors": "Manh-Duy Nguyen, Binh T. Nguyen, Cathal Gurrin", "title": "A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conventional approaches to image-text retrieval mainly focus on indexing\nvisual objects appearing in pictures but ignore the interactions between these\nobjects. Such objects occurrences and interactions are equivalently useful and\nimportant in this field as they are usually mentioned in the text. Scene graph\npresentation is a suitable method for the image-text matching challenge and\nobtained good results due to its ability to capture the inter-relationship\ninformation. Both images and text are represented in scene graph levels and\nformulate the retrieval challenge as a scene graph matching challenge. In this\npaper, we introduce the Local and Global Scene Graph Matching (LGSGM) model\nthat enhances the state-of-the-art method by integrating an extra graph\nconvolution network to capture the general information of a graph.\nSpecifically, for a pair of scene graphs of an image and its caption, two\nseparate models are used to learn the features of each graph's nodes and edges.\nThen a Siamese-structure graph convolution model is employed to embed graphs\ninto vector forms. We finally combine the graph-level and the vector-level to\ncalculate the similarity of this image-text pair. The empirical experiments\nshow that our enhancement with the combination of levels can improve the\nperformance of the baseline method by increasing the recall by more than 10% on\nthe Flickr30k dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 10:33:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Nguyen", "Manh-Duy", ""], ["Nguyen", "Binh T.", ""], ["Gurrin", "Cathal", ""]]}, {"id": "2106.02545", "submitter": "Roger Zhe Li", "authors": "Roger Zhe Li, Juli\\'an Urbano, Alan Hanjalic", "title": "New Insights into Metric Optimization for Ranking-based Recommendation", "comments": "10 pages, 5 figures, accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462973", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Direct optimization of IR metrics has often been adopted as an approach to\ndevise and develop ranking-based recommender systems. Most methods following\nthis approach aim at optimizing the same metric being used for evaluation,\nunder the assumption that this will lead to the best performance. A number of\nstudies of this practice bring this assumption, however, into question. In this\npaper, we dig deeper into this issue in order to learn more about the effects\nof the choice of the metric to optimize on the performance of a ranking-based\nrecommender system. We present an extensive experimental study conducted on\ndifferent datasets in both pairwise and listwise learning-to-rank scenarios, to\ncompare the relative merit of four popular IR metrics, namely RR, AP, nDCG and\nRBP, when used for optimization and assessment of recommender systems in\nvarious combinations. For the first three, we follow the practice of loss\nfunction formulation available in literature. For the fourth one, we propose\nnovel loss functions inspired by RBP for both the pairwise and listwise\nscenario. Our results confirm that the best performance is indeed not\nnecessarily achieved when optimizing the same metric being used for evaluation.\nIn fact, we find that RBP-inspired losses perform at least as well as other\nmetrics in a consistent way, and offer clear benefits in several cases.\nInteresting to see is that RBP-inspired losses, while improving the\nrecommendation performance for all uses, may lead to an individual performance\ngain that is correlated with the activity level of a user in interacting with\nitems. The more active the users, the more they benefit. Overall, our results\nchallenge the assumption behind the current research practice of optimizing and\nevaluating the same metric, and point to RBP-based optimization instead as a\npromising alternative when learning to rank in the recommendation context.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:18:36 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Li", "Roger Zhe", ""], ["Urbano", "Juli\u00e1n", ""], ["Hanjalic", "Alan", ""]]}, {"id": "2106.02715", "submitter": "Aleksandra Urman", "authors": "Aleksandra Urman, Mykola Makhortykh, Roberto Ulloa", "title": "Auditing Source Diversity Bias in Video Search Results Using Virtual\n  Agents", "comments": null, "journal-ref": "WWW '21: Companion Proceedings of the Web Conference 2021", "doi": "10.1145/3442442.3452306", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We audit the presence of domain-level source diversity bias in video search\nresults. Using a virtual agent-based approach, we compare outputs of four\nWestern and one non-Western search engines for English and Russian queries. Our\nfindings highlight that source diversity varies substantially depending on the\nlanguage with English queries returning more diverse outputs. We also find\ndisproportionately high presence of a single platform, YouTube, in top search\noutputs for all Western search engines except Google. At the same time, we\nobserve that Youtube's major competitors such as Vimeo or Dailymotion do not\nappear in the sampled Google's video search results. This finding suggests that\nGoogle might be downgrading the results from the main competitors of\nGoogle-owned Youtube and highlights the necessity for further studies focusing\non the presence of own-content bias in Google's search results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:52:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Urman", "Aleksandra", ""], ["Makhortykh", "Mykola", ""], ["Ulloa", "Roberto", ""]]}, {"id": "2106.02768", "submitter": "Pan Li", "authors": "Pan Li, Zhichao Jiang, Maofei Que, Yao Hu and Alexander Tuzhilin", "title": "Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate\n  Prediction", "comments": "Accepted to KDD21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross domain recommender system constitutes a powerful method to tackle the\ncold-start and sparsity problem by aggregating and transferring user\npreferences across multiple category domains. Therefore, it has great potential\nto improve click-through-rate prediction performance in online commerce\nplatforms having many domains of products. While several cross domain\nsequential recommendation models have been proposed to leverage information\nfrom a source domain to improve CTR predictions in a target domain, they did\nnot take into account bidirectional latent relations of user preferences across\nsource-target domain pairs. As such, they cannot provide enhanced cross-domain\nCTR predictions for both domains simultaneously. In this paper, we propose a\nnovel approach to cross-domain sequential recommendations based on the dual\nlearning mechanism that simultaneously transfers information between two\nrelated domains in an iterative manner until the learning process stabilizes.\nIn particular, the proposed Dual Attentive Sequential Learning (DASL) model\nconsists of two novel components Dual Embedding and Dual Attention, which\njointly establish the two-stage learning process: we first construct dual\nlatent embeddings that extract user preferences in both domains simultaneously,\nand subsequently provide cross-domain recommendations by matching the extracted\nlatent embeddings with candidate items through dual-attention learning\nmechanism. We conduct extensive offline experiments on three real-world\ndatasets to demonstrate the superiority of our proposed model, which\nsignificantly and consistently outperforms several state-of-the-art baselines\nacross all experimental settings. We also conduct an online A/B test at a major\nvideo streaming platform Alibaba-Youku, where our proposed model significantly\nimproves business performance over the latest production system in the company.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:21:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Pan", ""], ["Jiang", "Zhichao", ""], ["Que", "Maofei", ""], ["Hu", "Yao", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2106.02771", "submitter": "Pan Li", "authors": "Pan Li, Maofei Que, Zhichao Jiang, Yao Hu and Alexander Tuzhilin", "title": "PURS: Personalized Unexpected Recommender System for Improving User\n  Satisfaction", "comments": "Accepted to RecSys20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical recommender system methods typically face the filter bubble problem\nwhen users only receive recommendations of their familiar items, making them\nbored and dissatisfied. To address the filter bubble problem, unexpected\nrecommendations have been proposed to recommend items significantly deviating\nfrom user's prior expectations and thus surprising them by presenting \"fresh\"\nand previously unexplored items to the users. In this paper, we describe a\nnovel Personalized Unexpected Recommender System (PURS) model that incorporates\nunexpectedness into the recommendation process by providing multi-cluster\nmodeling of user interests in the latent space and personalized unexpectedness\nvia the self-attention mechanism and via selection of an appropriate unexpected\nactivation function. Extensive offline experiments on three real-world datasets\nillustrate that the proposed PURS model significantly outperforms the\nstate-of-the-art baseline approaches in terms of both accuracy and\nunexpectedness measures. In addition, we conduct an online A/B test at a major\nvideo platform Alibaba-Youku, where our model achieves over 3\\% increase in the\naverage video view per user metric. The proposed model is in the process of\nbeing deployed by the company.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 01:33:21 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Pan", ""], ["Que", "Maofei", ""], ["Jiang", "Zhichao", ""], ["Hu", "Yao", ""], ["Tuzhilin", "Alexander", ""]]}, {"id": "2106.02831", "submitter": "Amir Jalaly Bidgoly", "authors": "Fahimeh Soltaninejad, Amir Jalaly Bidgoly", "title": "A novel method for recommendation systems using invasive weed\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the popular approaches in recommendation systems is Collaborative\nFiltering (CF). The most significant step in CF is choosing the appropriate set\nof users. For this purpose, similarity measures are usually used for computing\nthe similarity between a specific user and the other users. This paper proposes\na new invasive weed optimization (IWO) based CF approach that uses users'\ncontext to identify important and effective users set. By using a newly defined\nsimilarity measure based on both rating values and a measure values called\nconfidence, the proposed approach calculates the similarity between users and\nthus identifies and filters the most similar users to a specific user. It then\nuses IWO to calculate the importance degree of users and finally, by using the\nidentified important users and their importance degrees it predicts unknown\nratings. To evaluate the proposed method, several experiments have been\nperformed on two known real world datasets and the results show that the\nproposed method improves the state of the art results up to 15% in terms of\nRoot Mean Square Error (RMSE) and Mean Absolute Error (MAE).\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 08:12:41 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Soltaninejad", "Fahimeh", ""], ["Bidgoly", "Amir Jalaly", ""]]}, {"id": "2106.02870", "submitter": "Wonbin Kweon", "authors": "Wonbin Kweon, SeongKu Kang, Hwanjo Yu", "title": "Bidirectional Distillation for Top-K Recommender System", "comments": "WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449878", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recommender systems (RS) have started to employ knowledge distillation, which\nis a model compression technique training a compact model (student) with the\nknowledge transferred from a cumbersome model (teacher). The state-of-the-art\nmethods rely on unidirectional distillation transferring the knowledge only\nfrom the teacher to the student, with an underlying assumption that the teacher\nis always superior to the student. However, we demonstrate that the student\nperforms better than the teacher on a significant proportion of the test set,\nespecially for RS. Based on this observation, we propose Bidirectional\nDistillation (BD) framework whereby both the teacher and the student\ncollaboratively improve with each other. Specifically, each model is trained\nwith the distillation loss that makes to follow the other's prediction along\nwith its original loss function. For effective bidirectional distillation, we\npropose rank discrepancy-aware sampling scheme to distill only the informative\nknowledge that can fully enhance each other. The proposed scheme is designed to\neffectively cope with a large performance gap between the teacher and the\nstudent. Trained in the bidirectional way, it turns out that both the teacher\nand the student are significantly improved compared to when being trained\nseparately. Our extensive experiments on real-world datasets show that our\nproposed framework consistently outperforms the state-of-the-art competitors.\nWe also provide analyses for an in-depth understanding of BD and ablation\nstudies to verify the effectiveness of each proposed component.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 11:03:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Kweon", "Wonbin", ""], ["Kang", "SeongKu", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2106.03042", "submitter": "Muhammad Hammad", "authors": "Muhammad Hammad, \\\"Onder Babur, Hamid Abdul Basit, Mark van den Brand", "title": "Clone-Seeker: Effective Code Clone Search Using Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Source code search plays an important role in software development, e.g. for\nexploratory development or opportunistic reuse of existing code from a code\nbase. Often, exploration of different implementations with the same\nfunctionality is needed for tasks like automated software transplantation,\nsoftware diversification, and software repair. Code clones, which are\nsyntactically or semantically similar code fragments, are perfect candidates\nfor such tasks. Searching for code clones involves a given search query to\nretrieve the relevant code fragments. We propose a novel approach called\nClone-Seeker that focuses on utilizing clone class features in retrieving code\nclones. For this purpose, we generate metadata for each code clone in the form\nof a natural language document. The metadata includes a pre-processed list of\nidentifiers from the code clones augmented with a list of keywords indicating\nthe semantics of the code clone. This keyword list can be extracted from a\nmanually annotated general description of the clone class, or automatically\ngenerated from the source code of the entire clone class. This approach helps\ndevelopers to perform code clone search based on a search query written either\nas source code terms, or as natural language. In our quantitative evaluation,\nwe show that (1) Clone-Seeker has a higher recall when searching for semantic\ncode clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; and (2)\nClone-Seeker can accurately search for relevant code clones by applying natural\nlanguage queries.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 06:14:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hammad", "Muhammad", ""], ["Babur", "\u00d6nder", ""], ["Basit", "Hamid Abdul", ""], ["Brand", "Mark van den", ""]]}, {"id": "2106.03060", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Liming Luke Chen, Nyothiri Aung, Wenyin Zhang,\n  Huansheng Ning", "title": "Big-Five, MPTI, Eysenck or HEXACO: The Ideal Personality Model for\n  Personality-aware Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality-aware recommendation systems have been proven to achieve high\naccuracy compared to conventional recommendation systems. In addition to that,\npersonality-aware recommendation systems could help alleviate cold start and\ndata sparsity problems. Most of the existing works use Big-Five personality\nmodel to represent the user's personality, this is due to the popularity of\nBig-Five model in the literature of psychology. However, from personality\ncomputing perspective, the choice of the most suitable personality model that\nsatisfy the requirements of the recommendation application and the recommended\ncontent type still needs further investigation. In this paper, we study and\ncompare four personality-aware recommendation systems based on different\npersonality models, namely Big-Five, Eysenck and HEXACO from the personality\ntraits theory, and Myers-Briggs Type Indicator (MPTI) from the personality\ntypes theory. Following that, we propose a hybrid personality model for\nrecommendation that takes advantage of the personality traits models, as well\nas the personality types models. Through extensive experiments on\nrecommendation dataset, we prove the efficiency of the proposed model,\nespecially in cold start settings.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 08:17:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Chen", "Liming Luke", ""], ["Aung", "Nyothiri", ""], ["Zhang", "Wenyin", ""], ["Ning", "Huansheng", ""]]}, {"id": "2106.03151", "submitter": "Qianren Mao", "authors": "Qianren Mao, Xi Li, Hao Peng, Bang Liu, Shu Guo, Jianxin Li, Lihong\n  Wang, Philip S. Yu", "title": "Attend and Select: A Segment Attention based Selection Mechanism for\n  Microblog Hashtag Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic microblog hashtag generation can help us better and faster\nunderstand or process the critical content of microblog posts.\n  Conventional sequence-to-sequence generation methods can produce phrase-level\nhashtags and have achieved remarkable performance on this task. However, they\nare incapable of filtering out secondary information and not good at capturing\nthe discontinuous semantics among crucial tokens.\n  A hashtag is formed by tokens or phrases that may originate from various\nfragmentary segments of the original text.\n  In this work, we propose an end-to-end Transformer-based generation model\nwhich consists of three phases: encoding, segments-selection, and decoding. The\nmodel transforms discontinuous semantic segments from the source text into a\nsequence of hashtags.\n  Specifically, we introduce a novel Segments Selection Mechanism (SSM) for\nTransformer to obtain segmental representations tailored to phrase-level\nhashtag generation.\n  Besides, we introduce two large-scale hashtag generation datasets, which are\nnewly collected from Chinese Weibo and English Twitter.\n  Extensive evaluations on the two datasets reveal our approach's superiority\nwith significant improvements to extraction and generation baselines. The code\nand datasets are available at \\url{https://github.com/OpenSUM/HashtagGen}.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:13:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mao", "Qianren", ""], ["Li", "Xi", ""], ["Peng", "Hao", ""], ["Liu", "Bang", ""], ["Guo", "Shu", ""], ["Li", "Jianxin", ""], ["Wang", "Lihong", ""], ["Yu", "Philip S.", ""]]}, {"id": "2106.03356", "submitter": "Fengtong Xiao", "authors": "Fengtong Xiao, Lin Li, Weinan Xu, Jingyu Zhao, Xiaofeng Yang, Jun\n  Lang, Hao Wang", "title": "DMBGN: Deep Multi-Behavior Graph Networks for Voucher Redemption Rate\n  Prediction", "comments": "9 pages, 5 figures, accepted full paper SIGKDD'21 applied data\n  science track", "journal-ref": null, "doi": "10.1145/3447548.3467191", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In E-commerce, vouchers are important marketing tools to enhance users'\nengagement and boost sales and revenue. The likelihood that a user redeems a\nvoucher is a key factor in voucher distribution decision. User-item\nClick-Through-Rate (CTR) models are often applied to predict the user-voucher\nredemption rate. However, the voucher scenario involves more complicated\nrelations among users, items and vouchers. The users' historical behavior in a\nvoucher collection activity reflects users' voucher usage patterns, which is\nnevertheless overlooked by the CTR-based solutions. In this paper, we propose a\nDeep Multi-behavior Graph Networks (DMBGN) to shed light on this field for the\nvoucher redemption rate prediction. The complex structural user-voucher-item\nrelationships are captured by a User-Behavior Voucher Graph (UVG). User\nbehavior happening both before and after voucher collection is taken into\nconsideration, and a high-level representation is extracted by Higher-order\nGraph Neural Networks. On top of a sequence of UVGs, an attention network is\nbuilt which can help to learn users' long-term voucher redemption preference.\nExtensive experiments on three large-scale production datasets demonstrate the\nproposed DMBGN model is effective, with 10% to 16% relative AUC improvement\nover Deep Neural Networks (DNN), and 2% to 4% AUC improvement over Deep\nInterest Network (DIN). Source code and a sample dataset are made publicly\navailable to facilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:16:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xiao", "Fengtong", ""], ["Li", "Lin", ""], ["Xu", "Weinan", ""], ["Zhao", "Jingyu", ""], ["Yang", "Xiaofeng", ""], ["Lang", "Jun", ""], ["Wang", "Hao", ""]]}, {"id": "2106.03373", "submitter": "Yiding Liu Dr.", "authors": "Yiding Liu, Guan Huang, Jiaxiang Liu, Weixue Lu, Suqi Cheng, Yukun Li,\n  Daiting Shi, Shuaiqiang Wang, Zhicong Cheng, Dawei Yin", "title": "Pre-trained Language Model for Web-scale Retrieval in Baidu Search", "comments": "Accepted by KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Retrieval is a crucial stage in web search that identifies a small set of\nquery-relevant candidates from a billion-scale corpus. Discovering more\nsemantically-related candidates in the retrieval stage is very promising to\nexpose more high-quality results to the end users. However, it still remains\nnon-trivial challenges of building and deploying effective retrieval models for\nsemantic matching in real search engine. In this paper, we describe the\nretrieval system that we developed and deployed in Baidu Search. The system\nexploits the recent state-of-the-art Chinese pretrained language model, namely\nEnhanced Representation through kNowledge IntEgration (ERNIE), which\nfacilitates the system with expressive semantic matching. In particular, we\ndeveloped an ERNIE-based retrieval model, which is equipped with 1) expressive\nTransformer-based semantic encoders, and 2) a comprehensive multi-stage\ntraining paradigm. More importantly, we present a practical system workflow for\ndeploying the model in web-scale retrieval. Eventually, the system is fully\ndeployed into production, where rigorous offline and online experiments were\nconducted. The results show that the system can perform high-quality candidate\nretrieval, especially for those tail queries with uncommon demands. Overall,\nthe new retrieval system facilitated by pretrained language model (i.e., ERNIE)\ncan largely improve the usability and applicability of our search engine.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:55:45 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 13:32:13 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 05:38:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Yiding", ""], ["Huang", "Guan", ""], ["Liu", "Jiaxiang", ""], ["Lu", "Weixue", ""], ["Cheng", "Suqi", ""], ["Li", "Yukun", ""], ["Shi", "Daiting", ""], ["Wang", "Shuaiqiang", ""], ["Cheng", "Zhicong", ""], ["Yin", "Dawei", ""]]}, {"id": "2106.03399", "submitter": "Shichao Pei", "authors": "Basmah Altaf, Shichao Pei, Xiangliang Zhang", "title": "Scientific Dataset Discovery via Topic-level Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data intensive research requires the support of appropriate datasets.\nHowever, it is often time-consuming to discover usable datasets matching a\nspecific research topic. We formulate the dataset discovery problem on an\nattributed heterogeneous graph, which is composed of paper-paper citation,\npaper-dataset citation, and also paper content. We propose to characterize both\npaper and dataset nodes by their commonly shared latent topics, rather than\nlearning user and item representations via canonical graph embedding models,\nbecause the usage of datasets and the themes of research projects can be\nunderstood on the common base of research topics. The relevant datasets to a\ngiven research project can then be inferred in the shared topic space. The\nexperimental results show that our model can generate reasonable profiles for\ndatasets, and recommend proper datasets for a query, which represents a\nresearch project linked with several papers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:01:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Altaf", "Basmah", ""], ["Pei", "Shichao", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2106.03415", "submitter": "Zhuoxuan Jiang", "authors": "Sanshi Yu and Zhuoxuan Jiang and Dong-Dong Chen and Shanshan Feng and\n  Dongsheng Li and Qi Liu and Jinfeng Yi", "title": "Leveraging Tripartite Interaction Information from Live Stream\n  E-Commerce for Improving Product Recommendation", "comments": "To appear in KDD'21 ADS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a new form of online shopping becomes more and more popular, which\ncombines live streaming with E-Commerce activity. The streamers introduce\nproducts and interact with their audiences, and hence greatly improve the\nperformance of selling products. Despite of the successful applications in\nindustries, the live stream E-commerce has not been well studied in the data\nscience community. To fill this gap, we investigate this brand-new scenario and\ncollect a real-world Live Stream E-Commerce (LSEC) dataset. Different from\nconventional E-commerce activities, the streamers play a pivotal role in the\nLSEC events. Hence, the key is to make full use of rich interaction information\namong streamers, users, and products. We first conduct data analysis on the\ntripartite interaction data and quantify the streamer's influence on users'\npurchase behavior. Based on the analysis results, we model the tripartite\ninformation as a heterogeneous graph, which can be decomposed to multiple\nbipartite graphs in order to better capture the influence. We propose a novel\nLive Stream E-Commerce Graph Neural Network framework (LSEC-GNN) to learn the\nnode representations of each bipartite graph, and further design a multi-task\nlearning approach to improve product recommendation. Extensive experiments on\ntwo real-world datasets with different scales show that our method can\nsignificantly outperform various baseline approaches.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:32:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yu", "Sanshi", ""], ["Jiang", "Zhuoxuan", ""], ["Chen", "Dong-Dong", ""], ["Feng", "Shanshan", ""], ["Li", "Dongsheng", ""], ["Liu", "Qi", ""], ["Yi", "Jinfeng", ""]]}, {"id": "2106.03532", "submitter": "Nour Karessli", "authors": "Andrea Nestler, Nour Karessli, Karl Hajjar, Rodrigo Weffer, Reza\n  Shirvany", "title": "SizeFlags: Reducing Size and Fit Related Returns in Fashion E-Commerce", "comments": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "journal-ref": null, "doi": "10.1145/3447548.3467160", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce is growing at an unprecedented rate and the fashion industry has\nrecently witnessed a noticeable shift in customers' order behaviour towards\nstronger online shopping. However, fashion articles ordered online do not\nalways find their way to a customer's wardrobe. In fact, a large share of them\nend up being returned. Finding clothes that fit online is very challenging and\naccounts for one of the main drivers of increased return rates in fashion\ne-commerce. Size and fit related returns severely impact 1. the customers\nexperience and their dissatisfaction with online shopping, 2. the environment\nthrough an increased carbon footprint, and 3. the profitability of online\nfashion platforms. Due to poor fit, customers often end up returning articles\nthat they like but do not fit them, which they have to re-order in a different\nsize. To tackle this issue we introduce SizeFlags, a probabilistic Bayesian\nmodel based on weakly annotated large-scale data from customers. Leveraging the\nadvantages of the Bayesian framework, we extend our model to successfully\nintegrate rich priors from human experts feedback and computer vision\nintelligence. Through extensive experimentation, large-scale A/B testing and\ncontinuous evaluation of the model in production, we demonstrate the strong\nimpact of the proposed approach in robustly reducing size-related returns in\nonline fashion over 14 countries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:43:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Nestler", "Andrea", ""], ["Karessli", "Nour", ""], ["Hajjar", "Karl", ""], ["Weffer", "Rodrigo", ""], ["Shirvany", "Reza", ""]]}, {"id": "2106.03569", "submitter": "Junliang Yu", "authors": "Junliang Yu, Hongzhi Yin, Min Gao, Xin Xia, Xiangliang Zhang, Nguyen\n  Quoc Viet Hung", "title": "Socially-Aware Self-Supervised Tri-Training for Recommendation", "comments": "9 pages, accepted by KDD'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning (SSL), which can automatically generate ground-truth\nsamples from raw data, holds vast potential to improve recommender systems.\nMost existing SSL-based methods perturb the raw data graph with uniform\nnode/edge dropout to generate new data views and then conduct the\nself-discrimination based contrastive learning over different views to learn\ngeneralizable representations. Under this scheme, only a bijective mapping is\nbuilt between nodes in two different views, which means that the\nself-supervision signals from other nodes are being neglected. Due to the\nwidely observed homophily in recommender systems, we argue that the supervisory\nsignals from other nodes are also highly likely to benefit the representation\nlearning for recommendation. To capture these signals, a general socially-aware\nSSL framework that integrates tri-training is proposed in this paper.\nTechnically, our framework first augments the user data views with the user\nsocial information. And then under the regime of tri-training for multi-view\nencoding, the framework builds three graph encoders (one for recommendation)\nupon the augmented views and iteratively improves each encoder with\nself-supervision signals from other users, generated by the other two encoders.\nSince the tri-training operates on the augmented views of the same data sources\nfor self-supervision signals, we name it self-supervised tri-training.\nExtensive experiments on multiple real-world datasets consistently validate the\neffectiveness of the self-supervised tri-training framework for improving\nrecommendation. The code is released at https://github.com/Coder-Yu/QRec.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 12:50:52 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 17:14:33 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 18:46:48 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Yu", "Junliang", ""], ["Yin", "Hongzhi", ""], ["Gao", "Min", ""], ["Xia", "Xin", ""], ["Zhang", "Xiangliang", ""], ["Hung", "Nguyen Quoc Viet", ""]]}, {"id": "2106.03819", "submitter": "Guillaume Salha-Galvan", "authors": "L\\'ea Briand and Guillaume Salha-Galvan and Walid Bendada and Mathieu\n  Morlon and Viet-Anh Tran", "title": "A Semi-Personalized System for User Cold Start Recommendation on Music\n  Streaming Apps", "comments": "27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining\n  (KDD 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music streaming services heavily rely on recommender systems to improve their\nusers' experience, by helping them navigate through a large musical catalog and\ndiscover new songs, albums or artists. However, recommending relevant and\npersonalized content to new users, with few to no interactions with the\ncatalog, is challenging. This is commonly referred to as the user cold start\nproblem. In this applied paper, we present the system recently deployed on the\nmusic streaming service Deezer to address this problem. The solution leverages\na semi-personalized recommendation strategy, based on a deep neural network\narchitecture and on a clustering of users from heterogeneous sources of\ninformation. We extensively show the practical impact of this system and its\neffectiveness at predicting the future musical preferences of cold start users\non Deezer, through both offline and online large-scale experiments. Besides, we\npublicly release our code as well as anonymized usage data from our\nexperiments. We hope that this release of industrial resources will benefit\nfuture research on user cold start recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:35:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Briand", "L\u00e9a", ""], ["Salha-Galvan", "Guillaume", ""], ["Bendada", "Walid", ""], ["Morlon", "Mathieu", ""], ["Tran", "Viet-Anh", ""]]}, {"id": "2106.03954", "submitter": "Gean Pereira", "authors": "Geand Trindade Pereira, Moises Rocha dos Santos, Andre Carlos Ponce de\n  Leon Ferreira de Carvalho", "title": "Evaluating Meta-Feature Selection for the Algorithm Recommendation\n  Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the popularity of Machine Learning (ML) solutions, algorithms and data\nhave been released faster than the capacity of processing them. In this\ncontext, the problem of Algorithm Recommendation (AR) is receiving a\nsignificant deal of attention recently. This problem has been addressed in the\nliterature as a learning task, often as a Meta-Learning problem where the aim\nis to recommend the best alternative for a specific dataset. For such, datasets\nencoded by meta-features are explored by ML algorithms that try to learn the\nmapping between meta-representations and the best technique to be used. One of\nthe challenges for the successful use of ML is to define which features are the\nmost valuable for a specific dataset since several meta-features can be used,\nwhich increases the meta-feature dimension. This paper presents an empirical\nanalysis of Feature Selection and Feature Extraction in the meta-level for the\nAR problem. The present study was focused on three criteria: predictive\nperformance, dimensionality reduction, and pipeline runtime. As we verified,\napplying Dimensionality Reduction (DR) methods did not improve predictive\nperformances in general. However, DR solutions reduced about 80% of the\nmeta-features, obtaining pretty much the same performance as the original setup\nbut with lower runtimes. The only exception was PCA, which presented about the\nsame runtime as the original meta-features. Experimental results also showed\nthat various datasets have many non-informative meta-features and that it is\npossible to obtain high predictive performance using around 20% of the original\nmeta-features. Therefore, due to their natural trend for high dimensionality,\nDR methods should be used for Meta-Feature Selection and Meta-Feature\nExtraction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:36:47 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 19:36:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Pereira", "Geand Trindade", ""], ["Santos", "Moises Rocha dos", ""], ["de Carvalho", "Andre Carlos Ponce de Leon Ferreira", ""]]}, {"id": "2106.03965", "submitter": "Somalee Datta", "authors": "Sanjay Malunjkar, Susan Weber, Somalee Datta", "title": "A highly scalable repository of waveform and vital signs data from\n  bedside monitoring devices", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The advent of cost effective cloud computing over the past decade and\never-growing accumulation of high-fidelity clinical data in a modern hospital\nsetting is leading to new opportunities for translational medicine. Machine\nlearning is driving the appetite of the research community for various types of\nsignal data such as patient vitals. Health care systems, however, are ill\nsuited for massive processing of large volumes of data. In addition, due to the\nsheer magnitude of the data being collected, it is not feasible to retain all\nof the data in health care systems in perpetuity. This gold mine of information\ngets purged periodically thereby losing invaluable future research\nopportunities. We have developed a highly scalable solution that: a) siphons\noff patient vital data on a nightly basis from on-premises bio-medical systems\nto a cloud storage location as a permanent archive, b) reconstructs the\ndatabase in the cloud, c) generates waveforms, alarms and numeric data in a\nresearch-ready format, and d) uploads the processed data to a storage location\nin the cloud ready for research.\n  The data is de-identified and catalogued such that it can be joined with\nElectronic Medical Records (EMR) and other ancillary data types such as\nelectroencephalogram (EEG), radiology, video monitoring etc. This technique\neliminates the research burden from health care systems. This highly scalable\nsolution is used to process high density patient monitoring data aggregated by\nthe Philips Patient Information Center iX (PIC iX) hospital surveillance system\nfor archival storage in the Philips Data Warehouse Connect enterprise-level\ndatabase. The solution is part of a broader platform that supports a secure\nhigh performance clinical data science platform.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:59:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Malunjkar", "Sanjay", ""], ["Weber", "Susan", ""], ["Datta", "Somalee", ""]]}, {"id": "2106.04128", "submitter": "Yifei Yuan", "authors": "Yifei Yuan and Wai Lam", "title": "Conversational Fashion Image Retrieval via Multiturn Natural Language\n  Feedback", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of conversational fashion image retrieval via multiturn\nnatural language feedback. Most previous studies are based on single-turn\nsettings. Existing models on multiturn conversational fashion image retrieval\nhave limitations, such as employing traditional models, and leading to\nineffective performance. We propose a novel framework that can effectively\nhandle conversational fashion image retrieval with multiturn natural language\nfeedback texts. One characteristic of the framework is that it searches for\ncandidate images based on exploitation of the encoded reference image and\nfeedback text information together with the conversation history. Furthermore,\nthe image fashion attribute information is leveraged via a mutual attention\nstrategy. Since there is no existing fashion dataset suitable for the multiturn\nsetting of our task, we derive a large-scale multiturn fashion dataset via\nadditional manual annotation efforts on an existing single-turn dataset. The\nexperiments show that our proposed model significantly outperforms existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:34:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yuan", "Yifei", ""], ["Lam", "Wai", ""]]}, {"id": "2106.04155", "submitter": "Han Liu", "authors": "Han Liu, Yangyang Guo, Jianhua Yin, Zan Gao, and Liqiang Nie", "title": "Review Polarity-wise Recommender", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilizing review information to enhance recommendation, the de facto\nreview-involved recommender systems, have received increasing interests over\nthe past few years. Thereinto, one advanced branch is to extract salient\naspects from textual reviews (i.e., the item attributes that users express) and\ncombine them with the matrix factorization technique. However, existing\napproaches all ignore the fact that semantically different reviews often\ninclude opposite aspect information. In particular, positive reviews usually\nexpress aspects that users prefer, while negative ones describe aspects that\nusers reject. As a result, it may mislead the recommender systems into making\nincorrect decisions pertaining to user preference modeling. Towards this end,\nin this paper, we propose a Review Polarity-wise Recommender model, dubbed as\nRPR, to discriminately treat reviews with different polarities. To be specific,\nin this model, positive and negative reviews are separately gathered and\nutilized to model the user-preferred and user-rejected aspects, respectively.\nBesides, in order to overcome the imbalance problem of semantically different\nreviews, we also develop an aspect-aware importance weighting approach to align\nthe aspect importance for these two kinds of reviews. Extensive experiments\nconducted on eight benchmark datasets have demonstrated the superiority of our\nmodel as compared to a series of state-of-the-art review-involved baselines.\nMoreover, our method can provide certain explanations to the real-world rating\nprediction scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:40:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Han", ""], ["Guo", "Yangyang", ""], ["Yin", "Jianhua", ""], ["Gao", "Zan", ""], ["Nie", "Liqiang", ""]]}, {"id": "2106.04209", "submitter": "Theis E. Jendal", "authors": "Anders H. Brams, Anders L. Jakobsen, Theis E. Jendal, Matteo\n  Lissandrini, Peter Dolog, Katja Hose", "title": "MindReader: Recommendation over Knowledge Graph Entities with Explicit\n  User Ratings", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412759", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge Graphs (KGs) have been integrated in several models of\nrecommendation to augment the informational value of an item by means of its\nrelated entities in the graph. Yet, existing datasets only provide explicit\nratings on items and no information is provided about user opinions of other\n(non-recommendable) entities. To overcome this limitation, we introduce a new\ndataset, called the MindReader, providing explicit user ratings both for items\nand for KG entities. In this first version, the MindReader dataset provides\nmore than 102 thousands explicit ratings collected from 1,174 real users on\nboth items and entities from a KG in the movie domain. This dataset has been\ncollected through an online interview application that we also release open\nsource. As a demonstration of the importance of this new dataset, we present a\ncomparative study of the effect of the inclusion of ratings on non-item KG\nentities in a variety of state-of-the-art recommendation models. In particular,\nwe show that most models, whether designed specifically for graph data or not,\nsee improvements in recommendation quality when trained on explicit non-item\nratings. Moreover, for some models, we show that non-item ratings can\neffectively replace item ratings without loss of recommendation quality. This\nfinding, thanks also to an observed greater familiarity of users towards common\nKG entities than towards long-tail items, motivates the use of KG entities for\nboth warm and cold-start recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:41:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Brams", "Anders H.", ""], ["Jakobsen", "Anders L.", ""], ["Jendal", "Theis E.", ""], ["Lissandrini", "Matteo", ""], ["Dolog", "Peter", ""], ["Hose", "Katja", ""]]}, {"id": "2106.04210", "submitter": "Vito Giordano", "authors": "Vito Giordano, Filippo Chiarello, Elena Cervelli", "title": "Defining definition: a Text mining Approach to Define Innovative\n  Technological Fields", "comments": null, "journal-ref": "R&D MANAGEMENT CONFERENCE 2019 - DATA SCIENCE FOR INNOVATION R&D\n  MANAGEMENT CONFERENCE 2019 - DATA SCIENCE FOR INNOVATION R&D Management\n  Conference 2021 - Data Science for Innovatopm", "doi": null, "report-no": null, "categories": "cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the first task of an innovative project is delineating the scope of\nthe project itself or of the product/service to be developed. A wrong scope\ndefinition can determine (in the worst case) project failure. A good scope\ndefinition become even more relevant in technological intensive innovation\nprojects, nowadays characterized by a highly dynamic multidisciplinary,\nturbulent and uncertain environment. In these cases, the boundaries of the\nproject are not easily detectable and it is difficult to decide what it is\nin-scope and out-of-scope. The present work proposes a tool for the scope\ndelineation process, that automatically define an innovative technological\nfield or a new technology. The tool is based on Text Mining algorithm that\nexploits Elsevier's Scopus abstracts in order to the extract relevant data to\ndefine a technological scope. The automatic definition tool is then applied on\nfour case studies: Artificial Intelligence and Data Science. The results show\nhow the tool can provide many crucial information in the definition process of\na technological field. In particular for the target technological field (or\ntechnology), it provides the definition and other elements related to the\ntarget.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:42:05 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Giordano", "Vito", ""], ["Chiarello", "Filippo", ""], ["Cervelli", "Elena", ""]]}, {"id": "2106.04376", "submitter": "Philipp Mayr", "authors": "Thomas Kr\\\"amer, Zeljko Carevic, Dwaipayan Roy, Claus-Peter Klas,\n  Philipp Mayr", "title": "ConSTR: A Contextual Search Term Recommender", "comments": "2 pages, 2 figures, accepted demo paper at JCDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo paper, we present ConSTR, a novel Contextual Search Term\nRecommender that utilises the user's interaction context for search term\nrecommendation and literature retrieval. ConSTR integrates a two-layered\nrecommendation interface: the first layer suggests terms with respect to a\nuser's current search term, and the second layer suggests terms based on the\nusers' previous search activities (interaction context). For the demonstration,\nConSTR is built on the arXiv, an academic repository consisting of 1.8 million\ndocuments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:09:59 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kr\u00e4mer", "Thomas", ""], ["Carevic", "Zeljko", ""], ["Roy", "Dwaipayan", ""], ["Klas", "Claus-Peter", ""], ["Mayr", "Philipp", ""]]}, {"id": "2106.04404", "submitter": "Tedo Vrbanec", "authors": "Tedo Vrbanec and Ana Mestrovic", "title": "The Struggle with Academic Plagiarism: Approaches based on Semantic\n  Similarity", "comments": "6 pages, 1 figure, 34 references", "journal-ref": "Proceedings of 40th Jubilee International Convention MIPRO 2017,\n  976-981", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Academic plagiarism is a serious problem nowadays. Due to the existence of\ninexhaustible sources of digital information, today it is easier to plagiarize\nmore than ever before. The good thing is that plagiarism detection techniques\nhave improved and are powerful enough to detect attempts of plagiarism in\neducation. We are now witnessing efficient plagiarism detection software in\naction, such as Turnitin, iThenticate or SafeAssign. In the introduction we\nexplore software that is used within the Croatian academic community for\nplagiarism detection in universities and/or in scientific journals. The\nquestion is: is this enough? Current software has proven to be successful,\nhowever the problem of identifying paraphrasing or obfuscation plagiarism\nremains unresolved. In this paper we present a report of how semantic\nsimilarity measures can be used in the plagiarism detection task.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 20:00:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Vrbanec", "Tedo", ""], ["Mestrovic", "Ana", ""]]}, {"id": "2106.04405", "submitter": "Vasileios Perifanis", "authors": "Vasileios Perifanis and Pavlos S. Efraimidis", "title": "Federated Neural Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a federated version of the state-of-the-art Neural\nCollaborative Filtering (NCF) approach for item recommendations. The system,\nnamed FedNCF, allows learning without requiring users to expose or transmit\ntheir raw data. Experimental validation shows that FedNCF achieves comparable\nrecommendation quality to the original NCF system. Although federated learning\n(FL) enables learning without raw data transmission, recent attacks showed that\nFL alone does not eliminate privacy concerns. To overcome this challenge, we\nintegrate a privacy-preserving enhancement with a secure aggregation scheme\nthat satisfies the security requirements against an honest-but-curious (HBC)\nentity, without affecting the quality of the original model. Finally, we\ndiscuss the peculiarities observed in the application of FL in a collaborative\nfiltering (CF) task as well as we evaluate the privacy-preserving mechanism in\nterms of computational cost.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 21:05:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Perifanis", "Vasileios", ""], ["Efraimidis", "Pavlos S.", ""]]}, {"id": "2106.04408", "submitter": "Tao Qi", "authors": "Tao Qi, Fangzhao Wu, Chuhan Wu, Peiru Yang, Yang Yu, Xing Xie and\n  Yongfeng Huang", "title": "HieRec: Hierarchical User Interest Modeling for Personalized News\n  Recommendation", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interest modeling is critical for personalized news recommendation.\nExisting news recommendation methods usually learn a single user embedding for\neach user from their previous behaviors to represent their overall interest.\nHowever, user interest is usually diverse and multi-grained, which is difficult\nto be accurately modeled by a single user embedding. In this paper, we propose\na news recommendation method with hierarchical user interest modeling, named\nHieRec. Instead of a single user embedding, in our method each user is\nrepresented in a hierarchical interest tree to better capture their diverse and\nmulti-grained interest in news. We use a three-level hierarchy to represent 1)\noverall user interest; 2) user interest in coarse-grained topics like sports;\nand 3) user interest in fine-grained topics like football. Moreover, we propose\na hierarchical user interest matching framework to match candidate news with\ndifferent levels of user interest for more accurate user interest targeting.\nExtensive experiments on two real-world datasets validate our method can\neffectively improve the performance of user modeling for personalized news\nrecommendation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:36:28 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Qi", "Tao", ""], ["Wu", "Fangzhao", ""], ["Wu", "Chuhan", ""], ["Yang", "Peiru", ""], ["Yu", "Yang", ""], ["Xie", "Xing", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.04415", "submitter": "Gaode Chen", "authors": "Gaode Chen, Xinghua Zhang, Yanyan Zhao, Cong Xue, Ji Xiang", "title": "Exploring Periodicity and Interactivity in Multi-Interest Framework for\n  Sequential Recommendation", "comments": "8 pages, 5 figures, to be published in IJCAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation systems alleviate the problem of information\noverload, and have attracted increasing attention in the literature. Most prior\nworks usually obtain an overall representation based on the user's behavior\nsequence, which can not sufficiently reflect the multiple interests of the\nuser. To this end, we propose a novel method called PIMI to mitigate this\nissue. PIMI can model the user's multi-interest representation effectively by\nconsidering both the periodicity and interactivity in the item sequence.\nSpecifically, we design a periodicity-aware module to utilize the time interval\ninformation between user's behaviors. Meanwhile, an ingenious graph is proposed\nto enhance the interactivity between items in user's behavior sequence, which\ncan capture both global and local item features. Finally, a multi-interest\nextraction module is applied to describe user's multiple interests based on the\nobtained item representation. Extensive experiments on two real-world datasets\nAmazon and Taobao show that PIMI outperforms state-of-the-art methods\nconsistently.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:30:24 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Gaode", ""], ["Zhang", "Xinghua", ""], ["Zhao", "Yanyan", ""], ["Xue", "Cong", ""], ["Xiang", "Ji", ""]]}, {"id": "2106.04494", "submitter": "Jiayan Gu Miss", "authors": "Jiayan Gu, Yan Wu, Ashiq Anjum, John Panneerselvam, Yao Lu, Bo Yuan", "title": "Optimization of Service Addition in Multilevel Index Model for Edge\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Edge Computing and Artificial Intelligence (AI)\ntechnologies, edge devices are witnessed to generate data at unprecedented\nvolume. The Edge Intelligence (EI) has led to the emergence of edge devices in\nvarious application domains. The EI can provide efficient services to\ndelay-sensitive applications, where the edge devices are deployed as edge nodes\nto host the majority of execution, which can effectively manage services and\nimprove service discovery efficiency. The multilevel index model is a\nwell-known model used for indexing service, such a model is being introduced\nand optimized in the edge environments to efficiently services discovery whilst\nmanaging large volumes of data. However, effectively updating the multilevel\nindex model by adding new services timely and precisely in the dynamic Edge\nComputing environments is still a challenge. Addressing this issue, this paper\nproposes a designated key selection method to improve the efficiency of adding\nservices in the multilevel index models. Our experimental results show that in\nthe partial index and the full index of multilevel index model, our method\nreduces the service addition time by around 84% and 76%, respectively when\ncompared with the original key selection method and by around 78% and 66%,\nrespectively when compared with the random selection method. Our proposed\nmethod significantly improves the service addition efficiency in the multilevel\nindex model, when compared with existing state-of-the-art key selection\nmethods, without compromising the service retrieval stability to any notable\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:27:55 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 10:27:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gu", "Jiayan", ""], ["Wu", "Yan", ""], ["Anjum", "Ashiq", ""], ["Panneerselvam", "John", ""], ["Lu", "Yao", ""], ["Yuan", "Bo", ""]]}, {"id": "2106.04515", "submitter": "Christopher Whitfield", "authors": "Christopher Whitfield, Yang Liu, Mohd Anwar", "title": "Surveillance of COVID-19 Pandemic using Social Media: A Reddit Study in\n  North Carolina", "comments": "12 pages, 6 figures, 7 tables, to be published in ACM-BCB 2021,\n  corrected misspelled author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease (COVID-19) pandemic has changed various aspects of\npeople's lives and behaviors. At this stage, there are no other ways to control\nthe natural progression of the disease than adopting mitigation strategies such\nas wearing masks, watching distance, and washing hands. Moreover, at this time\nof social distancing, social media plays a key role in connecting people and\nproviding a platform for expressing their feelings. In this study, we tap into\nsocial media to surveil the uptake of mitigation and detection strategies, and\ncapture issues and concerns about the pandemic. In particular, we explore the\nresearch question, \"how much can be learned regarding the public uptake of\nmitigation strategies and concerns about COVID-19 pandemic by using natural\nlanguage processing on Reddit posts?\" After extracting COVID-related posts from\nthe four largest subreddit communities of North Carolina over six months, we\nperformed NLP-based preprocessing to clean the noisy data. We employed a custom\nNamed-entity Recognition (NER) system and a Latent Dirichlet Allocation (LDA)\nmethod for topic modeling on a Reddit corpus. We observed that 'mask', 'flu',\nand 'testing' are the most prevalent named-entities for \"Personal Protective\nEquipment\", \"symptoms\", and \"testing\" categories, respectively. We also\nobserved that the most discussed topics are related to testing, masks, and\nemployment. The mitigation measures are the most prevalent theme of discussion\nacross all subreddits.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:55:25 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 01:04:35 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 03:48:19 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Whitfield", "Christopher", ""], ["Liu", "Yang", ""], ["Anwar", "Mohd", ""]]}, {"id": "2106.04612", "submitter": "Shauli Ravfogel", "authors": "Shauli Ravfogel, Hillel Taub-Tabib, Yoav Goldberg", "title": "Neural Extractive Search", "comments": "Accepted as a demo paper in ACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain experts often need to extract structured information from large\ncorpora. We advocate for a search paradigm called ``extractive search'', in\nwhich a search query is enriched with capture-slots, to allow for such rapid\nextraction. Such an extractive search system can be built around syntactic\nstructures, resulting in high-precision, low-recall results. We show how the\nrecall can be improved using neural retrieval and alignment. The goals of this\npaper are to concisely introduce the extractive-search paradigm; and to\ndemonstrate a prototype neural retrieval system for extractive search and its\nbenefits and potential. Our prototype is available at\n\\url{https://spike.neural-sim.apps.allenai.org/} and a video demonstration is\navailable at \\url{https://vimeo.com/559586687}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:03:31 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ravfogel", "Shauli", ""], ["Taub-Tabib", "Hillel", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2106.04873", "submitter": "Ruiming Tang", "authors": "Xiangli Yang, Qing Liu, Rong Su, Ruiming Tang, Zhirong Liu, Xiuqiang\n  He", "title": "AutoFT: Automatic Fine-Tune for Parameters Transfer Learning in\n  Click-Through Rate Prediction", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender systems are often asked to serve multiple recommendation\nscenarios or domains. Fine-tuning a pre-trained CTR model from source domains\nand adapting it to a target domain allows knowledge transferring. However,\noptimizing all the parameters of the pre-trained network may result in\nover-fitting if the target dataset is small and the number of parameters is\nlarge. This leads us to think of directly reusing parameters in the pre-trained\nmodel which represent more general features learned from multiple domains.\nHowever, the design of freezing or fine-tuning layers of parameters requires\nmuch manual effort since the decision highly depends on the pre-trained model\nand target instances. In this work, we propose an end-to-end transfer learning\nframework, called Automatic Fine-Tuning (AutoFT), for CTR prediction. AutoFT\nconsists of a field-wise transfer policy and a layer-wise transfer policy. The\nfield-wise transfer policy decides how the pre-trained embedding\nrepresentations are frozen or fine-tuned based on the given instance from the\ntarget domain. The layer-wise transfer policy decides how the high?order\nfeature representations are transferred layer by layer. Extensive experiments\non two public benchmark datasets and one private industrial dataset demonstrate\nthat AutoFT can significantly improve the performance of CTR prediction\ncompared with state-of-the-art transferring approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:54:24 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Yang", "Xiangli", ""], ["Liu", "Qing", ""], ["Su", "Rong", ""], ["Tang", "Ruiming", ""], ["Liu", "Zhirong", ""], ["He", "Xiuqiang", ""]]}, {"id": "2106.04993", "submitter": "Yinan Zhang", "authors": "Yinan Zhang, Boyang Li, Yong Liu, Hao Wang, Chunyan Miao", "title": "Initialization Matters: Regularizing Manifold-informed Initialization\n  for Neural Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper initialization is crucial to the optimization and the generalization\nof neural networks. However, most existing neural recommendation systems\ninitialize the user and item embeddings randomly. In this work, we propose a\nnew initialization scheme for user and item embeddings called Laplacian\nEigenmaps with Popularity-based Regularization for Isolated Data (LEPORID).\nLEPORID endows the embeddings with information regarding multi-scale\nneighborhood structures on the data manifold and performs adaptive\nregularization to compensate for high embedding variance on the tail of the\ndata distribution. Exploiting matrix sparsity, LEPORID embeddings can be\ncomputed efficiently. We evaluate LEPORID in a wide range of neural\nrecommendation models. In contrast to the recent surprising finding that the\nsimple K-nearest-neighbor (KNN) method often outperforms neural recommendation\nsystems, we show that existing neural systems initialized with LEPORID often\nperform on par or better than KNN. To maximize the effects of the\ninitialization, we propose the Dual-Loss Residual Recommendation (DLR2)\nnetwork, which, when initialized with LEPORID, substantially outperforms both\ntraditional and state-of-the-art neural recommender systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:26:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Yinan", ""], ["Li", "Boyang", ""], ["Liu", "Yong", ""], ["Wang", "Hao", ""], ["Miao", "Chunyan", ""]]}, {"id": "2106.05081", "submitter": "Ziyang Wang", "authors": "Ziyang Wang, Wei Wei, Gao Cong, Xiao-Li Li, Xian-Ling Mao, Minghui Qiu", "title": "Global Context Enhanced Graph Neural Networks for Session-based\n  Recommendation", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.10173", "journal-ref": "SIGIR 2020", "doi": "10.1145/3397271.3401142", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation (SBR) is a challenging task, which aims at\nrecommending items based on anonymous behavior sequences. Almost all the\nexisting solutions for SBR model user preference only based on the current\nsession without exploiting the other sessions, which may contain both relevant\nand irrelevant item-transitions to the current session. This paper proposes a\nnovel approach, called Global Context Enhanced Graph Neural Networks (GCE-GNN)\nto exploit item transitions over all sessions in a more subtle manner for\nbetter inferring the user preference of the current session. Specifically,\nGCE-GNN learns two levels of item embeddings from session graph and global\ngraph, respectively: (i) Session graph, which is to learn the session-level\nitem embedding by modeling pairwise item-transitions within the current\nsession; and (ii) Global graph, which is to learn the global-level item\nembedding by modeling pairwise item-transitions over all sessions. In GCE-GNN,\nwe propose a novel global-level item representation learning layer, which\nemploys a session-aware attention mechanism to recursively incorporate the\nneighbors' embeddings of each node on the global graph. We also design a\nsession-level item representation learning layer, which employs a GNN on the\nsession graph to learn session-level item embeddings within the current\nsession. Moreover, GCE-GNN aggregates the learnt item representations in the\ntwo levels with a soft attention mechanism. Experiments on three benchmark\ndatasets demonstrate that GCE-GNN outperforms the state-of-the-art methods\nconsistently.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:01:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wang", "Ziyang", ""], ["Wei", "Wei", ""], ["Cong", "Gao", ""], ["Li", "Xiao-Li", ""], ["Mao", "Xian-Ling", ""], ["Qiu", "Minghui", ""]]}, {"id": "2106.05144", "submitter": "Pau Riba", "authors": "Pau Riba, Adri\\`a Molina, Lluis Gomez, Oriol Ramos-Terrades and Josep\n  Llad\\'os", "title": "Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting", "comments": "Accepted at ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we explore and evaluate the use of ranking-based objective\nfunctions for learning simultaneously a word string and a word image encoder.\nWe consider retrieval frameworks in which the user expects a retrieval list\nranked according to a defined relevance score. In the context of a word\nspotting problem, the relevance score has been set according to the string edit\ndistance from the query string. We experimentally demonstrate the competitive\nperformance of the proposed model on query-by-string word spotting for both,\nhandwritten and real scene word images. We also provide the results for\nquery-by-example word spotting, although it is not the main focus of this work.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:39:05 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Riba", "Pau", ""], ["Molina", "Adri\u00e0", ""], ["Gomez", "Lluis", ""], ["Ramos-Terrades", "Oriol", ""], ["Llad\u00f3s", "Josep", ""]]}, {"id": "2106.05147", "submitter": "Suzan Verberne", "authors": "Ioannis Chios and Suzan Verberne", "title": "Helping results assessment by adding explainable elements to the deep\n  relevance matching model", "comments": "Published in the 3rd International Workshop on ExplainAble\n  Recommendation and Search (EARS 2020), July 30, 2020 Xi'an, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the explainability of web search engines. We propose\ntwo explainable elements on the search engine result page: a visualization of\nquery term weights and a visualization of passage relevance. The idea is that\nsearch engines that indicate to the user why results are retrieved are valued\nhigher by users and gain user trust. We deduce the query term weights from the\nterm gating network in the Deep Relevance Matching Model (DRMM) and visualize\nthem as a doughnut chart. In addition, we train a passage-level ranker with\nDRMM that selects the most relevant passage from each document and shows it as\nsnippet on the result page. Next to the snippet we show a document thumbnail\nwith this passage highlighted. We evaluate the proposed interface in an online\nuser study, asking users to judge the explainability and assessability of the\ninterface. We found that users judge our proposed interface significantly more\nexplainable and easier to assess than a regular search engine result page.\nHowever, they are not significantly better in selecting the relevant documents\nfrom the top-5. This indicates that the explainability of the search engine\nresult page leads to a better user experience. Thus, we conclude that the\nproposed explainable elements are promising as visualization for search engine\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:41:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chios", "Ioannis", ""], ["Verberne", "Suzan", ""]]}, {"id": "2106.05194", "submitter": "Yixuan He", "authors": "Yixuan He and Gesine Reinert and Mihai Cucuringu", "title": "DIGRAC: Digraph Clustering with Flow Imbalance", "comments": "33 pages (10 pages for main text)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Node clustering is a powerful tool in the analysis of networks. Here, we\nintroduce a graph neural network framework with a novel scalable Directed Mixed\nPath Aggregation(DIMPA) scheme to obtain node embeddings for directed networks\nin a self-supervised manner, including a novel probabilistic imbalance loss.\nThe method is end-to-end in combining embedding generation and clustering\nwithout an intermediate step. In contrast to standard approaches in the\nliterature, in this paper, directionality is not treated as a nuisance, but\nrather contains the main signal. In particular, we leverage the recently\nintroduced cut flow imbalance measure, which is tightly related to\ndirectionality; cut flow imbalance is optimized without resorting to spectral\nmethods or cluster labels. Experimental results on synthetic data, in the form\nof directed stochastic block models and real-world data at different scales,\ndemonstrate that our method attains state-of-the-art results on directed\nclustering, for a wide range of noise and sparsity levels, as well as graph\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:33:13 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["He", "Yixuan", ""], ["Reinert", "Gesine", ""], ["Cucuringu", "Mihai", ""]]}, {"id": "2106.05220", "submitter": "Anoosheh Heidarzadeh", "authors": "Anoosheh Heidarzadeh, Nahid Esmati, and Alex Sprintson", "title": "Single-Server Private Linear Transformation: The Joint Privacy Case", "comments": "12 pages, 1 figure. This work is a long version of arXiv:2102.01665", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the problem of Private Linear Transformation (PLT)\nwhich generalizes the problems of private information retrieval and private\nlinear computation. The PLT problem includes one or more remote server(s)\nstoring (identical copies of) $K$ messages and a user who wants to compute $L$\nindependent linear combinations of a $D$-subset of messages. The objective of\nthe user is to perform the computation by downloading minimum possible amount\nof information from the server(s), while protecting the identities of the $D$\nmessages required for the computation. In this work, we focus on the\nsingle-server setting of the PLT problem when the identities of the $D$\nmessages required for the computation must be protected jointly. We consider\ntwo different models, depending on whether the coefficient matrix of the\nrequired $L$ linear combinations generates a Maximum Distance Separable (MDS)\ncode. We prove that the capacity for both models is given by $L/(K-D+L)$, where\nthe capacity is defined as the supremum of all achievable download rates. Our\nconverse proofs are based on linear-algebraic and information-theoretic\narguments that establish connections between PLT schemes and linear codes. We\nalso present an achievability scheme for each of the models being considered.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:09:22 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:04:07 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Heidarzadeh", "Anoosheh", ""], ["Esmati", "Nahid", ""], ["Sprintson", "Alex", ""]]}, {"id": "2106.05222", "submitter": "Anoosheh Heidarzadeh", "authors": "Anoosheh Heidarzadeh, Nahid Esmati, and Alex Sprintson", "title": "Single-Server Private Linear Transformation: The Individual Privacy Case", "comments": "14 pages, 1 figure. This work is a long version of arXiv:2102.01662", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the single-server Private Linear Transformation (PLT)\nproblem with individual privacy guarantees. In this problem, there is a user\nthat wishes to obtain $L$ independent linear combinations of a $D$-subset of\nmessages belonging to a dataset of $K$ messages stored on a single server. The\ngoal is to minimize the download cost while keeping the identity of each\nmessage required for the computation individually private. The individual\nprivacy requirement ensures that the identity of each individual message\nrequired for the computation is kept private. This is in contrast to the\nstricter notion of joint privacy that protects the entire set of identities of\nall messages used for the computation, including the correlations between these\nidentities. The notion of individual privacy captures a broad set of practical\napplications. For example, such notion is relevant when the dataset contains\ninformation about individuals, each of them requires privacy guarantees for\ntheir data access patterns. We focus on the setting in which the required\nlinear transformation is associated with a maximum distance separable (MDS)\nmatrix. In particular, we require that the matrix of coefficients pertaining to\nthe required linear combinations is the generator matrix of an MDS code. We\nestablish lower and upper bounds on the capacity of PLT with individual\nprivacy, where the capacity is defined as the supremum of all achievable\ndownload rates. We show that our bounds are tight under certain conditions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:12:04 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 01:06:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Heidarzadeh", "Anoosheh", ""], ["Esmati", "Nahid", ""], ["Sprintson", "Alex", ""]]}, {"id": "2106.05260", "submitter": "Jane Adams", "authors": "Jane L. Adams, Todd F. Deluca, Christopher M. Danforth, Peter S.\n  Dodds, Yuhang Zheng, Konstantinos Anastasakis, Boyoon Choi, Allison Min,\n  Michael M. Bessey", "title": "Sirius: A Mutual Information Tool for Exploratory Visualization of Mixed\n  Data", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data scientists across disciplines are increasingly in need of exploratory\nanalysis tools for data sets with a high volume of features. We expand upon\ngraph mining approaches for exploratory analysis of high-dimensional data to\nintroduce Sirius, a visualization package for researchers to explore feature\nrelationships among mixed data types using mutual information and network\nbackbone sparsification. Visualizations of feature relationships aid data\nscientists in finding meaningful dependence among features, which can engender\nfurther analysis for feature selection, feature extraction, projection,\nidentification of proxy variables, or insight into temporal variation at the\nmacro scale. Graph mining approaches for feature analysis exist, such as\nassociation networks of binary features, or correlation networks of\nquantitative features, but mixed data types present a unique challenge for\ndeveloping comprehensive feature networks for exploratory analysis. Using an\ninformation theoretic approach, Sirius supports heterogeneous data sets\nconsisting of binary, continuous quantitative, and discrete categorical data\ntypes, and provides a user interface exploring feature pairs with high mutual\ninformation scores. We leverage a backbone sparsification approach from network\ntheory as a dimensionality reduction technique, which probabilistically trims\nedges according to the local network context. Sirius is an open source Python\npackage and Django web application for exploratory visualization, which can be\ndeployed in data analysis pipelines. The Sirius codebase and exemplary data\nsets can be found at: https://github.com/compstorylab/sirius\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:57:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Adams", "Jane L.", ""], ["Deluca", "Todd F.", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter S.", ""], ["Zheng", "Yuhang", ""], ["Anastasakis", "Konstantinos", ""], ["Choi", "Boyoon", ""], ["Min", "Allison", ""], ["Bessey", "Michael M.", ""]]}, {"id": "2106.05346", "submitter": "Devendra Singh Sachan", "authors": "Devendra Singh Sachan and Siva Reddy and William Hamilton and Chris\n  Dyer and Dani Yogatama", "title": "End-to-End Training of Multi-Document Reader and Retriever for\n  Open-Domain Question Answering", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end differentiable training method for\nretrieval-augmented open-domain question answering systems that combine\ninformation from multiple retrieved documents when generating answers. We model\nretrieval decisions as latent variables over sets of relevant documents. Since\nmarginalizing over sets of retrieved documents is computationally hard, we\napproximate this using an expectation-maximization algorithm. We iteratively\nestimate the value of our latent variable (the set of relevant documents for a\ngiven question) and then use this estimate to update the retriever and reader\nparameters. We hypothesize that such end-to-end training allows training\nsignals to flow to the reader and then to the retriever better than staged-wise\ntraining. This results in a retriever that is able to select more relevant\ndocuments for a question and a reader that is trained on more accurate\ndocuments to generate an answer. Experiments on three benchmark datasets\ndemonstrate that our proposed method outperforms all existing approaches of\ncomparable size by 2-3% absolute exact match points, achieving new\nstate-of-the-art results. Our results also demonstrate the feasibility of\nlearning to retrieve to improve answer generation without explicit supervision\nof retrieval decisions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:25:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Sachan", "Devendra Singh", ""], ["Reddy", "Siva", ""], ["Hamilton", "William", ""], ["Dyer", "Chris", ""], ["Yogatama", "Dani", ""]]}, {"id": "2106.05482", "submitter": "Jianqiang Huang", "authors": "Jianqiang Huang, Ke Hu, Qingtao Tang, Mingjian Chen, Yi Qi, Jia Cheng,\n  Jun Lei", "title": "Deep Position-wise Interaction Network for CTR Prediction", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3463117", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Click-through rate (CTR) prediction plays an important role in online\nadvertising and recommender systems. In practice, the training of CTR models\ndepends on click data which is intrinsically biased towards higher positions\nsince higher position has higher CTR by nature. Existing methods such as actual\nposition training with fixed position inference and inverse propensity weighted\ntraining with no position inference alleviate the bias problem to some extend.\nHowever, the different treatment of position information between training and\ninference will inevitably lead to inconsistency and sub-optimal online\nperformance. Meanwhile, the basic assumption of these methods, i.e., the click\nprobability is the product of examination probability and relevance\nprobability, is oversimplified and insufficient to model the rich interaction\nbetween position and other information. In this paper, we propose a Deep\nPosition-wise Interaction Network (DPIN) to efficiently combine all candidate\nitems and positions for estimating CTR at each position, achieving consistency\nbetween offline and online as well as modeling the deep non-linear interaction\namong position, user, context and item under the limit of serving performance.\nFollowing our new treatment to the position bias in CTR prediction, we propose\na new evaluation metrics named PAUC (position-wise AUC) that is suitable for\nmeasuring the ranking quality at a given position. Through extensive\nexperiments on a real world dataset, we show empirically that our method is\nboth effective and efficient in solving position bias problem. We have also\ndeployed our method in production and observed statistically significant\nimprovement over a highly optimized baseline in a rigorous A/B test.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 03:54:33 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 16:12:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Huang", "Jianqiang", ""], ["Hu", "Ke", ""], ["Tang", "Qingtao", ""], ["Chen", "Mingjian", ""], ["Qi", "Yi", ""], ["Cheng", "Jia", ""], ["Lei", "Jun", ""]]}, {"id": "2106.05630", "submitter": "Xu Tan", "authors": "Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, Tie-Yan Liu", "title": "MusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training", "comments": "Accepted by ACL 2021 Findings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.IR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic music understanding, which refers to the understanding of music from\nthe symbolic data (e.g., MIDI format, but not audio), covers many music\napplications such as genre classification, emotion classification, and music\npieces matching. While good music representations are beneficial for these\napplications, the lack of training data hinders representation learning.\nInspired by the success of pre-training models in natural language processing,\nin this paper, we develop MusicBERT, a large-scale pre-trained model for music\nunderstanding. To this end, we construct a large-scale symbolic music corpus\nthat contains more than 1 million music songs. Since symbolic music contains\nmore structural (e.g., bar, position) and diverse information (e.g., tempo,\ninstrument, and pitch), simply adopting the pre-training techniques from NLP to\nsymbolic music only brings marginal gains. Therefore, we design several\nmechanisms, including OctupleMIDI encoding and bar-level masking strategy, to\nenhance pre-training with symbolic music data. Experiments demonstrate the\nadvantages of MusicBERT on four music understanding tasks, including melody\ncompletion, accompaniment suggestion, genre classification, and style\nclassification. Ablation studies also verify the effectiveness of our designs\nof OctupleMIDI encoding and bar-level masking strategy in MusicBERT.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:13:05 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zeng", "Mingliang", ""], ["Tan", "Xu", ""], ["Wang", "Rui", ""], ["Ju", "Zeqian", ""], ["Qin", "Tao", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2106.05633", "submitter": "Arthur Brack", "authors": "Arthur Brack and Anett Hoppe and Ralph Ewerth", "title": "Citation Recommendation for Research Papers via Knowledge Graphs", "comments": "Accepted for publication in 25th International Conference on Theory\n  and Practice of Digital Libraries (TPDL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citation recommendation for research papers is a valuable task that can help\nresearchers improve the quality of their work by suggesting relevant related\nwork. Current approaches for this task rely primarily on the text of the papers\nand the citation network. In this paper, we propose to exploit an additional\nsource of information, namely research knowledge graphs (KG) that interlink\nresearch papers based on mentioned scientific concepts. Our experimental\nresults demonstrate that the combination of information from research KGs with\nexisting state-of-the-art approaches is beneficial. Experimental results are\npresented for the STM-KG (STM: Science, Technology, Medicine), which is an\nautomatically populated knowledge graph based on the scientific concepts\nextracted from papers of ten domains. The proposed approach outperforms the\nstate of the art with a mean average precision of 20.6% (+0.8) for the top-50\nretrieved results.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:16:51 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Brack", "Arthur", ""], ["Hoppe", "Anett", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.05729", "submitter": "Judith Hermanns", "authors": "Judith Hermanns, Anton Tsitsulin, Marina Munkhoeva, Alex Bronstein,\n  Davide Mottin, Panagiotis Karras", "title": "GRASP: Graph Alignment through Spectral Signatures", "comments": "Accepted to APWeb-WAIM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the best way to match the nodes of two graphs? This graph alignment\nproblem generalizes graph isomorphism and arises in applications from social\nnetwork analysis to bioinformatics. Some solutions assume that auxiliary\ninformation on known matches or node or edge attributes is available, or\nutilize arbitrary graph features. Such methods fare poorly in the pure form of\nthe problem, in which only graph structures are given. Other proposals\ntranslate the problem to one of aligning node embeddings, yet, by doing so,\nprovide only a single-scale view of the graph. In this paper, we transfer the\nshape-analysis concept of functional maps from the continuous to the discrete\ncase, and treat the graph alignment problem as a special case of the problem of\nfinding a mapping between functions on graphs. We present GRASP, a method that\nfirst establishes a correspondence between functions derived from Laplacian\nmatrix eigenvectors, which capture multiscale structural characteristics, and\nthen exploits this correspondence to align nodes. Our experimental study,\nfeaturing noise levels higher than anything used in previous studies, shows\nthat GRASP outperforms state-of-the-art methods for graph alignment across\nnoise levels and graph types.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:19:25 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 08:20:01 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hermanns", "Judith", ""], ["Tsitsulin", "Anton", ""], ["Munkhoeva", "Marina", ""], ["Bronstein", "Alex", ""], ["Mottin", "Davide", ""], ["Karras", "Panagiotis", ""]]}, {"id": "2106.05764", "submitter": "Norman Meuschke", "authors": "Norman Meuschke", "title": "Analyzing Non-Textual Content Elements to Detect Academic Plagiarism", "comments": "Ph.D. Thesis, University of Konstanz", "journal-ref": null, "doi": "10.5281/zenodo.4913345", "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying academic plagiarism is a pressing problem, among others, for\nresearch institutions, publishers, and funding organizations. Detection\napproaches proposed so far analyze lexical, syntactical, and semantic text\nsimilarity. These approaches find copied, moderately reworded, and literally\ntranslated text. However, reliably detecting disguised plagiarism, such as\nstrong paraphrases, sense-for-sense translations, and the reuse of non-textual\ncontent and ideas, is an open research problem.\n  The thesis addresses this problem by proposing plagiarism detection\napproaches that implement a different concept: analyzing non-textual content in\nacademic documents, specifically citations, images, and mathematical content.\n  To validate the effectiveness of the proposed detection approaches, the\nthesis presents five evaluations that use real cases of academic plagiarism and\nexploratory searches for unknown cases.\n  The evaluation results show that non-textual content elements contain a high\ndegree of semantic information, are language-independent, and largely immutable\nto the alterations that authors typically perform to conceal plagiarism.\nAnalyzing non-textual content complements text-based detection approaches and\nincreases the detection effectiveness, particularly for disguised forms of\nacademic plagiarism.\n  To demonstrate the benefit of combining non-textual and text-based detection\nmethods, the thesis describes the first plagiarism detection system that\nintegrates the analysis of citation-based, image-based, math-based, and\ntext-based document similarity. The system's user interface employs\nvisualizations that significantly reduce the effort and time users must invest\nin examining content similarity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:11:52 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Meuschke", "Norman", ""]]}, {"id": "2106.05768", "submitter": "Sophia Althammer", "authors": "Sophia Althammer, Mark Buckley, Sebastian Hofst\\\"atter, Allan Hanbury", "title": "Linguistically Informed Masking for Representation Learning in the\n  Patent Domain", "comments": "Published at SIGIR 2021 PatentSemTech workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain-specific contextualized language models have demonstrated substantial\neffectiveness gains for domain-specific downstream tasks, like similarity\nmatching, entity recognition or information retrieval. However successfully\napplying such models in highly specific language domains requires domain\nadaptation of the pre-trained models. In this paper we propose the empirically\nmotivated Linguistically Informed Masking (LIM) method to focus\ndomain-adaptative pre-training on the linguistic patterns of patents, which use\na highly technical sublanguage. We quantify the relevant differences between\npatent, scientific and general-purpose language and demonstrate for two\ndifferent language models (BERT and SciBERT) that domain adaptation with LIM\nleads to systematically improved representations by evaluating the performance\nof the domain-adapted representations of patent language on two independent\ndownstream tasks, the IPC classification and similarity matching. We\ndemonstrate the impact of balancing the learning from different information\nsources during domain adaptation for the patent domain. We make the source code\nas well as the domain-adaptive pre-trained patent language models publicly\navailable at https://github.com/sophiaalthammer/patent-lim.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:20:57 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Althammer", "Sophia", ""], ["Buckley", "Mark", ""], ["Hofst\u00e4tter", "Sebastian", ""], ["Hanbury", "Allan", ""]]}, {"id": "2106.06022", "submitter": "Martin Bauer", "authors": "Martin Bauer", "title": "IoT Virtualization with ML-based Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For IoT to reach its full potential, the sharing and reuse of information in\ndifferent applications and across verticals is of paramount importance.\nHowever, there are a plethora of IoT platforms using different representations,\nprotocols and interaction patterns. To address this issue, the Fed4IoT project\nhas developed an IoT virtualization platform that, on the one hand, integrates\ninformation from many different source platforms and, on the other hand, makes\nthe information required by the respective users available in the target\nplatform of choice. To enable this, information is translated into a common,\nneutral exchange format. The format of choice is NGSI-LD, which is being\nstandardized by the ETSI Industry Specification Group on Context Information\nManagement (ETSI ISG CIM). Thing Visors are the components that translate the\nsource information to NGSI-LD, which is then delivered to the target platform\nand translated into the target format. ThingVisors can be implemented by hand,\nbut this requires significant human effort, especially considering the\nheterogeneity of low level information produced by a multitude of sensors.\nThus, supporting the human developer and, ideally, fully automating the process\nof extracting and enriching data and translating it to NGSI-LD is a crucial\nstep. Machine learning is a promising approach for this, but it typically\nrequires large amounts of hand-labelled data for training, an effort that makes\nit unrealistic in many IoT scenarios. A programmatic labelling approach called\nknowledge infusion that encodes expert knowledge is used for matching a schema\nor ontology extracted from the data with a target schema or ontology, providing\nthe basis for annotating the data and facilitating the translation to NGSI-LD.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 19:56:48 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bauer", "Martin", ""]]}, {"id": "2106.06165", "submitter": "Ziwei Fan", "authors": "Ziwei Fan, Zhiwei Liu, Lei Zheng, Shen Wang, Philip S. Yu", "title": "Modeling Sequences as Distributions with Uncertainty for Sequential\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The sequential patterns within the user interactions are pivotal for\nrepresenting the user's preference and capturing latent relationships among\nitems. The recent advancements of sequence modeling by Transformers advocate\nthe community to devise more effective encoders for the sequential\nrecommendation. Most existing sequential methods assume users are\ndeterministic. However, item-item transitions might fluctuate significantly in\nseveral item aspects and exhibit randomness of user interests. This\n\\textit{stochastic characteristics} brings up a solid demand to include\nuncertainties in representing sequences and items. Additionally, modeling\nsequences and items with uncertainties expands users' and items' interaction\nspaces, thus further alleviating cold-start problems.\n  In this work, we propose a Distribution-based Transformer for Sequential\nRecommendation (DT4SR), which injects uncertainties into sequential modeling.\nWe use Elliptical Gaussian distributions to describe items and sequences with\nuncertainty. We describe the uncertainty in items and sequences as Elliptical\nGaussian distribution. And we adopt Wasserstein distance to measure the\nsimilarity between distributions. We devise two novel Trans-formers for\nmodeling mean and covariance, which guarantees the positive-definite property\nof distributions. The proposed method significantly outperforms the\nstate-of-the-art methods. The experiments on three benchmark datasets also\ndemonstrate its effectiveness in alleviating cold-start issues. The code is\navailable inhttps://github.com/DyGRec/DT4SR.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 04:35:21 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Fan", "Ziwei", ""], ["Liu", "Zhiwei", ""], ["Zheng", "Lei", ""], ["Wang", "Shen", ""], ["Yu", "Philip S.", ""]]}, {"id": "2106.06216", "submitter": "Luca Mazzola", "authors": "Andreas Waldis and Luca Mazzola", "title": "Nested and Balanced Entity Recognition using Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity Recognition (ER) within a text is a fundamental exercise in Natural\nLanguage Processing, enabling further depending tasks such as Knowledge\nExtraction, Text Summarisation, or Keyphrase Extraction. An entity consists of\nsingle words or of a consecutive sequence of terms, constituting the basic\nbuilding blocks for communication. Mainstream ER approaches are mainly limited\nto flat structures, concentrating on the outermost entities while ignoring the\ninner ones. This paper introduces a partly-layered network architecture that\ndeals with the complexity of overlapping and nested cases. The proposed\narchitecture consists of two parts: (1) a shared Sequence Layer and (2) a\nstacked component with multiple Tagging Layers. The adoption of such an\narchitecture has the advantage of preventing overfit to a specific word-length,\nthus maintaining performance for longer entities despite their lower frequency.\nTo verify the proposed architecture's effectiveness, we train and evaluate this\narchitecture to recognise two kinds of entities - Concepts (CR) and Named\nEntities (NER). Our approach achieves state-of-the-art NER performances, while\nit outperforms previous CR approaches. Considering these promising results, we\nsee the possibility to evolve the architecture for other cases such as the\nextraction of events or the detection of argumentative components.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 07:52:32 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Waldis", "Andreas", ""], ["Mazzola", "Luca", ""]]}, {"id": "2106.06244", "submitter": "Christian Otto", "authors": "Christian Otto, Ran Yu, Georg Pardi, Johannes von Hoyer, Markus\n  Rokicki, Anett Hoppe, Peter Holtz, Yvonne Kammerer, Stefan Dietze, Ralph\n  Ewerth", "title": "Predicting Knowledge Gain during Web Search based on Multimedia Resource\n  Consumption", "comments": "13 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In informal learning scenarios the popularity of multimedia content, such as\nvideo tutorials or lectures, has significantly increased. Yet, the users'\ninteractions, navigation behavior, and consequently learning outcome, have not\nbeen researched extensively. Related work in this field, also called search as\nlearning, has focused on behavioral or text resource features to predict\nlearning outcome and knowledge gain. In this paper, we investigate whether we\ncan exploit features representing multimedia resource consumption to predict of\nknowledge gain (KG) during Web search from in-session data, that is without\nprior knowledge about the learner. For this purpose, we suggest a set of\nmultimedia features related to image and video consumption. Our feature\nextraction is evaluated in a lab study with 113 participants where we collected\ndata for a given search as learning task on the formation of thunderstorms and\nlightning. We automatically analyze the monitored log data and utilize\nstate-of-the-art computer vision methods to extract features about the seen\nmultimedia resources. Experimental results demonstrate that multimedia features\ncan improve KG prediction. Finally, we provide an analysis on feature\nimportance (text and multimedia) for KG prediction.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 08:52:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Otto", "Christian", ""], ["Yu", "Ran", ""], ["Pardi", "Georg", ""], ["von Hoyer", "Johannes", ""], ["Rokicki", "Markus", ""], ["Hoppe", "Anett", ""], ["Holtz", "Peter", ""], ["Kammerer", "Yvonne", ""], ["Dietze", "Stefan", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.06258", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang", "title": "DebiasGAN: Eliminating Position Bias in News Recommendation with\n  Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News recommendation is important for improving news reading experience of\nusers. Users' news click behaviors are widely used for inferring user interests\nand predicting future clicks. However, click behaviors are heavily affected by\nthe biases brought by the positions of news displayed on the webpage. It is\nimportant to eliminate the effect of position biases on the recommendation\nmodel to accurately target user interests. In this paper, we propose a news\nrecommendation method named DebiasGAN that can effectively eliminate the effect\nof position biases via adversarial learning. We use a bias-aware click model to\ncapture the influence of position bias on click behaviors, and we use a\nbias-invariant click model with random candidate news positions to estimate the\nideally unbiased click scores. We apply adversarial learning techniques to the\nhidden representations learned by the two models to help the bias-invariant\nclick model capture the bias-independent interest of users on news.\nExperimental results on two real-world datasets show that DebiasGAN can\neffectively improve the accuracy of news recommendation by eliminating position\nbiases.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:20:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""]]}, {"id": "2106.06420", "submitter": "Davood Zabihzadeh", "authors": "Karrar Al-Kaabi, Reza Monsefi, Davood Zabihzadeh", "title": "A Framework to Enhance Generalization of Deep Metric Learning methods\n  using General Discriminative Feature Learning and Class Adversarial Neural\n  Networks", "comments": "Includes: 31 Pages, 5 Tables, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric learning algorithms aim to learn a distance function that brings the\nsemantically similar data items together and keeps dissimilar ones at a\ndistance. The traditional Mahalanobis distance learning is equivalent to find a\nlinear projection. In contrast, Deep Metric Learning (DML) methods are proposed\nthat automatically extract features from data and learn a non-linear\ntransformation from input space to a semantically embedding space. Recently,\nmany DML methods are proposed focused to enhance the discrimination power of\nthe learned metric by providing novel sampling strategies or loss functions.\nThis approach is very helpful when both the training and test examples are\ncoming from the same set of categories. However, it is less effective in many\napplications of DML such as image retrieval and person-reidentification. Here,\nthe DML should learn general semantic concepts from observed classes and employ\nthem to rank or identify objects from unseen categories. Neglecting the\ngeneralization ability of the learned representation and just emphasizing to\nlearn a more discriminative embedding on the observed classes may lead to the\noverfitting problem. To address this limitation, we propose a framework to\nenhance the generalization power of existing DML methods in a Zero-Shot\nLearning (ZSL) setting by general yet discriminative representation learning\nand employing a class adversarial neural network. To learn a more general\nrepresentation, we propose to employ feature maps of intermediate layers in a\ndeep neural network and enhance their discrimination power through an attention\nmechanism. Besides, a class adversarial network is utilized to enforce the deep\nmodel to seek class invariant features for the DML task. We evaluate our work\non widely used machine vision datasets in a ZSL setting.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:24:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Al-Kaabi", "Karrar", ""], ["Monsefi", "Reza", ""], ["Zabihzadeh", "Davood", ""]]}, {"id": "2106.06467", "submitter": "Bin Hao", "authors": "Bin Hao, Min Zhang, Weizhi Ma, Shaoyun Shi, Xinxing Yu, Houzhi Shan,\n  Yiqun Liu, Shaoping Ma", "title": "A Large-Scale Rich Context Query and Recommendation Dataset in Online\n  Knowledge-Sharing", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data plays a vital role in machine learning studies. In the research of\nrecommendation, both user behaviors and side information are helpful to model\nusers. So, large-scale real scenario datasets with abundant user behaviors will\ncontribute a lot. However, it is not easy to get such datasets as most of them\nare only hold and protected by companies. In this paper, a new large-scale\ndataset collected from a knowledge-sharing platform is presented, which is\ncomposed of around 100M interactions collected within 10 days, 798K users, 165K\nquestions, 554K answers, 240K authors, 70K topics, and more than 501K user\nquery keywords. There are also descriptions of users, answers, questions,\nauthors, and topics, which are anonymous. Note that each user's latest query\nkeywords have not been included in previous open datasets, which reveal users'\nexplicit information needs.\n  We characterize the dataset and demonstrate its potential applications for\nrecommendation study. Multiple experiments show the dataset can be used to\nevaluate algorithms in general top-N recommendation, sequential recommendation,\nand context-aware recommendation. This dataset can also be used to integrate\nsearch and recommendation and recommendation with negative feedback. Besides,\ntasks beyond recommendation, such as user gender prediction, most valuable\nanswerer identification, and high-quality answer recognition, can also use this\ndataset. To the best of our knowledge, this is the largest real-world\ninteraction dataset for personalized recommendation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:45:55 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hao", "Bin", ""], ["Zhang", "Min", ""], ["Ma", "Weizhi", ""], ["Shi", "Shaoyun", ""], ["Yu", "Xinxing", ""], ["Shan", "Houzhi", ""], ["Liu", "Yiqun", ""], ["Ma", "Shaoping", ""]]}, {"id": "2106.06471", "submitter": "Xingyi Yang", "authors": "Xingyi Yang, Muchao Ye, Quanzeng You, Fenglong Ma", "title": "Writing by Memorizing: Hierarchical Retrieval-based Medical Report\n  Generation", "comments": "Accepted by ACL 2021, Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical report generation is one of the most challenging tasks in medical\nimage analysis. Although existing approaches have achieved promising results,\nthey either require a predefined template database in order to retrieve\nsentences or ignore the hierarchical nature of medical report generation. To\naddress these issues, we propose MedWriter that incorporates a novel\nhierarchical retrieval mechanism to automatically extract both report and\nsentence-level templates for clinically accurate report generation. MedWriter\nfirst employs the Visual-Language Retrieval~(VLR) module to retrieve the most\nrelevant reports for the given images. To guarantee the logical coherence\nbetween sentences, the Language-Language Retrieval~(LLR) module is introduced\nto retrieve relevant sentences based on the previous generated description. At\nlast, a language decoder fuses image features and features from retrieved\nreports and sentences to generate meaningful medical reports. We verified the\neffectiveness of our model by automatic evaluation and human evaluation on two\ndatasets, i.e., Open-I and MIMIC-CXR.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 07:47:23 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Yang", "Xingyi", ""], ["Ye", "Muchao", ""], ["You", "Quanzeng", ""], ["Ma", "Fenglong", ""]]}, {"id": "2106.06713", "submitter": "Xiangyu Zhao", "authors": "Xiangyu Zhao, Haochen Liu, Wenqi Fan, Hui Liu, Jiliang Tang, Chong\n  Wang", "title": "AutoLoss: Automated Loss Function Search in Recommendations", "comments": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing an effective loss function plays a crucial role in training deep\nrecommender systems. Most existing works often leverage a predefined and fixed\nloss function that could lead to suboptimal recommendation quality and training\nefficiency. Some recent efforts rely on exhaustively or manually searched\nweights to fuse a group of candidate loss functions, which is exceptionally\ncostly in computation and time. They also neglect the various convergence\nbehaviors of different data examples. In this work, we propose an AutoLoss\nframework that can automatically and adaptively search for the appropriate loss\nfunction from a set of candidates. To be specific, we develop a novel\ncontroller network, which can dynamically adjust the loss probabilities in a\ndifferentiable manner. Unlike existing algorithms, the proposed controller can\nadaptively generate the loss probabilities for different data examples\naccording to their varied convergence behaviors. Such design improves the\nmodel's generalizability and transferability between deep recommender systems\nand datasets. We evaluate the proposed framework on two benchmark datasets. The\nresults show that AutoLoss outperforms representative baselines. Further\nexperiments have been conducted to deepen our understandings of AutoLoss,\nincluding its transferability, components and training efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:15:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhao", "Xiangyu", ""], ["Liu", "Haochen", ""], ["Fan", "Wenqi", ""], ["Liu", "Hui", ""], ["Tang", "Jiliang", ""], ["Wang", "Chong", ""]]}, {"id": "2106.06720", "submitter": "Junaid Baber", "authors": "Muhammad Nasir, Maheen Bakhtyar, Junaid Baber, Sadia Lakho, Bilal\n  Ahmed, Waheed Noor", "title": "BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan\n  using text mining", "comments": "Paper is accepted in SOFTA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infectious disease outbreak has a significant impact on morbidity, mortality\nand can cause economic instability of many countries. As global trade is\ngrowing, goods and individuals are expected to travel across the border, an\ninfected epidemic area carrier can pose a great danger to his hostile. If a\ndisease outbreak is recognized promptly, then commercial products and travelers\n(traders/visitors) will be effectively vaccinated, and therefore the disease\nstopped. Early detection of outbreaks plays an important role here, and beware\nof the rapid implementation of control measures by citizens, public health\norganizations, and government. Many indicators have valuable information, such\nas online news sources (RSS) and social media sources (Twitter, Facebook) that\ncan be used, but are unstructured and bulky, to extract information about\ndisease outbreaks. Few early warning outbreak systems exist with some\nlimitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few\nchannels are published the outbreak news in Urdu or English. The aim is to\nprocure information from Pakistan's English and Urdu news channels and then\ninvestigate process, integrate, and visualize the disease epidemic. Urdu\nontology is not existed before to match extracted diseases, so we also build\nthat ontology of disease.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:55:40 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nasir", "Muhammad", ""], ["Bakhtyar", "Maheen", ""], ["Baber", "Junaid", ""], ["Lakho", "Sadia", ""], ["Ahmed", "Bilal", ""], ["Noor", "Waheed", ""]]}, {"id": "2106.06722", "submitter": "Kun Zhou", "authors": "Hui Wang, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang and Ji-Rong Wen", "title": "Curriculum Pre-Training Heterogeneous Subgraph Transformer for Top-$N$\n  Recommendation", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the flexibility in modelling data heterogeneity, heterogeneous\ninformation network (HIN) has been adopted to characterize complex and\nheterogeneous auxiliary data in top-$N$ recommender systems, called\n\\emph{HIN-based recommendation}. HIN characterizes complex, heterogeneous data\nrelations, containing a variety of information that may not be related to the\nrecommendation task. Therefore, it is challenging to effectively leverage\nuseful information from HINs for improving the recommendation performance. To\naddress the above issue, we propose a Curriculum pre-training based\nHEterogeneous Subgraph Transformer (called \\emph{CHEST}) with new \\emph{data\ncharacterization}, \\emph{representation model} and \\emph{learning algorithm}.\n  Specifically, we consider extracting useful information from HIN to compose\nthe interaction-specific heterogeneous subgraph, containing both sufficient and\nrelevant context information for recommendation. Then we capture the rich\nsemantics (\\eg graph structure and path semantics) within the subgraph via a\nheterogeneous subgraph Transformer, where we encode the subgraph with\nmulti-slot sequence representations. Besides, we design a curriculum\npre-training strategy to provide an elementary-to-advanced learning process, by\nwhich we smoothly transfer basic semantics in HIN for modeling user-item\ninteraction relation.\n  Extensive experiments conducted on three real-world datasets demonstrate the\nsuperiority of our proposed method over a number of competitive baselines,\nespecially when only limited training data is available.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 09:08:19 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Hui", ""], ["Zhou", "Kun", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Jingyuan", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2106.06739", "submitter": "L Siddharth", "authors": "L Siddharth, Lucienne T.M. Blessing, Kristin L. Wood, Jianxi Luo", "title": "Engineering Knowledge Graph from Patent Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a large, scalable engineering knowledge graph, comprising sets of\n(entity, relationship, entity) triples that are real-world engineering facts\nfound in the patent database. We apply a set of rules based on the syntactic\nand lexical properties of claims in a patent document to extract facts. We\naggregate these facts within each patent document and integrate the aggregated\nsets of facts across the patent database to obtain the engineering knowledge\ngraph. Such a knowledge graph is expected to support inference, reasoning, and\nrecalling in various engineering tasks. The knowledge graph has a greater size\nand coverage in comparison with the previously used knowledge graphs and\nsemantic networks in the engineering literature.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:54:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Siddharth", "L", ""], ["Blessing", "Lucienne T. M.", ""], ["Wood", "Kristin L.", ""], ["Luo", "Jianxi", ""]]}, {"id": "2106.06900", "submitter": "Zefang Liu", "authors": "Zefang Liu, Shuran Wen, Yinzhu Quan", "title": "Deep Reinforcement Learning based Group Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group recommender systems are widely used in current web applications. In\nthis paper, we propose a novel group recommender system based on the deep\nreinforcement learning. We introduce the MovieLens data at first and generate\none random group dataset, MovieLens-Rand, from it. This randomly generated\ndataset is described and analyzed. We also present experimental settings and\ntwo state-of-art baselines, AGREE and GroupIM. The framework of our novel\nmodel, the Deep Reinforcement learning based Group Recommender system (DRGR),\nis proposed. Actor-critic networks are implemented with the deep deterministic\npolicy gradient algorithm. The DRGR model is applied on the MovieLens-Rand\ndataset with two baselines. Compared with baselines, we conclude that DRGR\nperforms better than GroupIM due to long interaction histories but worse than\nAGREE because of the self-attention mechanism. We express advantages and\nshortcomings of DRGR and also give future improvement directions at the end.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 02:45:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Zefang", ""], ["Wen", "Shuran", ""], ["Quan", "Yinzhu", ""]]}, {"id": "2106.06910", "submitter": "Sourav Das", "authors": "Arunava Kumar Chakraborty, Sourav Das and Anup Kumar Kolya", "title": "Sentiment Analysis of Covid-19 Tweets using Evolutionary\n  Classification-Based LSTM Model", "comments": "11 pages, 8 figures, 5 tables", "journal-ref": "In RAAI 2020. Advances in Intelligent Systems and Computing, vol\n  1355 (2021)", "doi": "10.1007/978-981-16-1543-6_7", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As the Covid-19 outbreaks rapidly all over the world day by day and also\naffects the lives of million, a number of countries declared complete lock-down\nto check its intensity. During this lockdown period, social media plat-forms\nhave played an important role to spread information about this pandemic across\nthe world, as people used to express their feelings through the social\nnetworks. Considering this catastrophic situation, we developed an experimental\napproach to analyze the reactions of people on Twitter taking into ac-count the\npopular words either directly or indirectly based on this pandemic. This paper\nrepresents the sentiment analysis on collected large number of tweets on\nCoronavirus or Covid-19. At first, we analyze the trend of public sentiment on\nthe topics related to Covid-19 epidemic using an evolutionary classification\nfollowed by the n-gram analysis. Then we calculated the sentiment ratings on\ncollected tweet based on their class. Finally, we trained the long-short term\nnetwork using two types of rated tweets to predict sentiment on Covid-19 data\nand obtained an overall accuracy of 84.46%.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 04:27:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chakraborty", "Arunava Kumar", ""], ["Das", "Sourav", ""], ["Kolya", "Anup Kumar", ""]]}, {"id": "2106.07316", "submitter": "Jurek Leonhardt", "authors": "Jurek Leonhardt, Fabian Beringer, Avishek Anand", "title": "Exploiting Sentence-Level Representations for Passage Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, pre-trained contextual models, such as BERT, have shown to perform\nwell in language related tasks. We revisit the design decisions that govern the\napplicability of these models for the passage re-ranking task in open-domain\nquestion answering. We find that common approaches in the literature rely on\nfine-tuning a pre-trained BERT model and using a single, global representation\nof the input, discarding useful fine-grained relevance signals in token- or\nsentence-level representations. We argue that these discarded tokens hold\nuseful information that can be leveraged. In this paper, we explicitly model\nthe sentence-level representations by using Dynamic Memory Networks (DMNs) and\nconduct empirical evaluation to show improvements in passage re-ranking over\nfine-tuned vanilla BERT models by memory-enhanced explicit sentence modelling\non a diverse set of open-domain QA datasets. We further show that freezing the\nBERT model and only training the DMN layer still comes close to the original\nperformance, while improving training efficiency drastically. This indicates\nthat the usual fine-tuning step mostly helps to aggregate the inherent\ninformation in a single output token, as opposed to adapting the whole model to\nthe new task, and only achieves rather small gains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:42:15 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Leonhardt", "Jurek", ""], ["Beringer", "Fabian", ""], ["Anand", "Avishek", ""]]}, {"id": "2106.07346", "submitter": "Zheng Fang", "authors": "Zheng Fang, Yulan He and Rob Procter", "title": "A Query-Driven Topic Model", "comments": "ACL2021 finding paper. For source code, see\n  https://github.com/Fitz-like-coding/QDTM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic modeling is an unsupervised method for revealing the hidden semantic\nstructure of a corpus. It has been increasingly widely adopted as a tool in the\nsocial sciences, including political science, digital humanities and\nsociological research in general. One desirable property of topic models is to\nallow users to find topics describing a specific aspect of the corpus. A\npossible solution is to incorporate domain-specific knowledge into topic\nmodeling, but this requires a specification from domain experts. We propose a\nnovel query-driven topic model that allows users to specify a simple query in\nwords or phrases and return query-related topics, thus avoiding tedious work\nfrom domain experts. Our proposed approach is particularly attractive when the\nuser-specified query has a low occurrence in a text corpus, making it difficult\nfor traditional topic models built on word cooccurrence patterns to identify\nrelevant topics. Experimental results demonstrate the effectiveness of our\nmodel in comparison with both classical topic models and neural topic models.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 22:49:42 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 11:56:12 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Fang", "Zheng", ""], ["He", "Yulan", ""], ["Procter", "Rob", ""]]}, {"id": "2106.07347", "submitter": "Hao Wang", "authors": "Hao Wang", "title": "Zipf Matrix Factorization : Matrix Factorization with Matthew Effect\n  Reduction", "comments": null, "journal-ref": "ICAIBD 2021", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommender system recommends interesting items to users based on users' past\ninformation history. Researchers have been paying attention to improvement of\nalgorithmic performance such as MAE and precision@K. Major techniques such as\nmatrix factorization and learning to rank are optimized based on such\nevaluation metrics. However, the intrinsic Matthew Effect problem poses great\nthreat to the fairness of the recommender system, and the unfairness problem\ncannot be resolved by optimization of traditional metrics. In this paper, we\npropose a novel algorithm that incorporates Matthew Effect reduction with the\nmatrix factorization framework. We demonstrate that our approach can boost the\nfairness of the algorithm and enhances performance evaluated by traditional\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 00:44:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Hao", ""]]}, {"id": "2106.07348", "submitter": "Sohom Ghosh", "authors": "Sohom Ghosh", "title": "Is it a click bait? Let's predict using Machine Learning", "comments": "M.Tech Thesis defended at BITS, Pilani", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this era of digitisation, news reader tend to read news online. This is\nbecause, online media instantly provides access to a wide variety of content.\nThus, people don't have to wait for tomorrow's newspaper to know what's\nhappening today. Along with these virtues, online news have some vices as well.\nOne such vice is presence of social media posts (tweets) relating to news\narticles whose sole purpose is to draw attention of the users rather than\ndirecting them to read the actual content. Such posts are referred to as\nclickbaits. The objective of this project is to develop a system which would be\ncapable of predicting how likely are the social media posts (tweets) relating\nto new articles tend to be clickbait.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:07:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ghosh", "Sohom", ""]]}, {"id": "2106.07352", "submitter": "Nicholas FitzGerald", "authors": "Nicholas FitzGerald, Jan A. Botha, Daniel Gillick, Daniel M. Bikel,\n  Tom Kwiatkowski, Andrew McCallum", "title": "MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation\n  Network", "comments": "Accepted to ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \"class prototypes\" as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor's entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:54:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["FitzGerald", "Nicholas", ""], ["Botha", "Jan A.", ""], ["Gillick", "Daniel", ""], ["Bikel", "Daniel M.", ""], ["Kwiatkowski", "Tom", ""], ["McCallum", "Andrew", ""]]}, {"id": "2106.07356", "submitter": "Zhenhui Xu", "authors": "Zhenhui Xu, Meng Zhao, Liqun Liu, Xiaopeng Zhang and Bifeng Zhang", "title": "Mixture of Virtual-Kernel Experts for Multi-Objective User Profile\n  Modeling", "comments": "10 pages, under review", "journal-ref": null, "doi": null, "report-no": "AMSDM00", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many industrial applications like online advertising and recommendation\nsystems, diverse and accurate user profiles can greatly help improve\npersonalization. For building user profiles, deep learning is widely used to\nmine expressive tags to describe users' preferences from their historical\nactions. For example, tags mined from users' click-action history can represent\nthe categories of ads that users are interested in, and they are likely to\ncontinue being clicked in the future. Traditional solutions usually introduce\nmultiple independent Two-Tower models to mine tags from different actions,\ne.g., click, conversion. However, the models cannot learn complementarily and\nsupport effective training for data-sparse actions. Besides, limited by the\nlack of information fusion between the two towers, the model learning is\ninsufficient to represent users' preferences on various topics well. This paper\nintroduces a novel multi-task model called Mixture of Virtual-Kernel Experts\n(MVKE) to learn multiple topic-related user preferences based on different\nactions unitedly. In MVKE, we propose a concept of Virtual-Kernel Expert, which\nfocuses on modeling one particular facet of the user's preference, and all of\nthem learn coordinately. Besides, the gate-based structure used in MVKE builds\nan information fusion bridge between two towers, improving the model's\ncapability much and maintaining high efficiency. We apply the model in Tencent\nAdvertising System, where both online and offline evaluations show that our\nmethod has a significant improvement compared with the existing ones and brings\nabout an obvious lift to actual advertising revenue.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:52:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Xu", "Zhenhui", ""], ["Zhao", "Meng", ""], ["Liu", "Liqun", ""], ["Zhang", "Xiaopeng", ""], ["Zhang", "Bifeng", ""]]}, {"id": "2106.07359", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers and Nada Beili and Timo Hartmann and Prantik Goswami and\n  Muhammad Arslan Zafar", "title": "MexPub: Deep Transfer Learning for Metadata Extraction from German\n  Publications", "comments": "A long version of an accepted paper @ JCDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.CV cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Extracting metadata from scientific papers can be considered a solved problem\nin NLP due to the high accuracy of state-of-the-art methods. However, this does\nnot apply to German scientific publications, which have a variety of styles and\nlayouts. In contrast to most of the English scientific publications that follow\nstandard and simple layouts, the order, content, position and size of metadata\nin German publications vary greatly among publications. This variety makes\ntraditional NLP methods fail to accurately extract metadata from these\npublications. In this paper, we present a method that extracts metadata from\nPDF documents with different layouts and styles by viewing the document as an\nimage. We used Mask R-CNN that is trained on COCO dataset and finetuned with\nPubLayNet dataset that consists of ~200K PDF snapshots with five basic classes\n(e.g. text, figure, etc). We refine-tuned the model on our proposed synthetic\ndataset consisting of ~30K article snapshots to extract nine patterns (i.e.\nauthor, title, etc). Our synthetic dataset is generated using contents in both\nlanguages German and English and a finite set of challenging templates obtained\nfrom German publications. Our method achieved an average accuracy of around\n$90\\%$ which validates its capability to accurately extract metadata from a\nvariety of PDF documents with challenging templates.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:43:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Beili", "Nada", ""], ["Hartmann", "Timo", ""], ["Goswami", "Prantik", ""], ["Zafar", "Muhammad Arslan", ""]]}, {"id": "2106.07363", "submitter": "Saeid Hosseini", "authors": "Sayna Esmailzadeh, Saeid Hosseini, Mohammad Reza Kangavari, Wen Hua", "title": "Cognitive-aware Short-text Understanding for Inferring Professions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging short-text contents to estimate the occupation of microblog\nauthors has significant gains in many applications. Yet challenges abound.\nFirstly brief textual contents come with excessive lexical noise that makes the\ninference problem challenging. Secondly, cognitive-semantics are not evident,\nand important linguistic features are latent in short-text contents. Thirdly,\nit is hard to measure the correlation between the cognitive short-text\nsemantics and the features pertaining various occupations. We argue that the\nmulti-aspect cognitive features are needed to correctly associate short-text\ncontents to a particular job and discover suitable people for the careers. To\nthis end, we devise a novel framework that on the one hand, can infer\nshort-text contents and exploit cognitive features, and on the other hand,\nfuses various adopted novel algorithms, such as curve fitting, support vector,\nand boosting modules to better predict the occupation of the authors. The final\nestimation module manufactures the $R^w$-tree via coherence weight to tune the\nbest outcome in the inferring process. We conduct comprehensive experiments on\nreal-life Twitter data. The experimental results show that compared to other\nrivals, our cognitive multi-aspect model can achieve a higher performance in\nthe career estimation procedure, where it is inevitable to neglect the\ncontextual semantics of users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:19:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Esmailzadeh", "Sayna", ""], ["Hosseini", "Saeid", ""], ["Kangavari", "Mohammad Reza", ""], ["Hua", "Wen", ""]]}, {"id": "2106.07374", "submitter": "Ick Hoon Jin", "authors": "Yeseul Jeon and Dongjun Chung and Jina Park and Ick Hoon Jin", "title": "Network-based Trajectory Topic Interaction Map for Text Mining of\n  COVID-19 Biomedical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the emergence of the worldwide pandemic of COVID-19, relevant research\nhas been published at a dazzling pace, which makes it hard to follow the\nresearch in this area without dedicated efforts. It is practically impossible\nto implement this task manually due to the high volume of the relevant\nliterature. Text mining has been considered to be a powerful approach to\naddress this challenge, especially the topic modeling, a well-known\nunsupervised method that aims to reveal latent topics from the literature.\nHowever, in spite of its potential utility, the results generated from this\napproach are often investigated manually. Hence, its application to the\nCOVID-19 literature is not straightforward and expert knowledge is needed to\nmake meaningful interpretations. In order to address these challenges, we\npropose a novel analytical framework for estimating topic interactions and\neffective visualization for topic interpretation. Here we assumed that topics\nconstituting a paper can be positioned on an interaction map, which belongs to\na high-dimensional Euclidean space. Based on this assumption, after summarizing\ntopics with their topic-word distributions using the biterm topic model, we\nmapped these latent topics on networks to visualize relationships among the\ntopics. Moreover, in the proposed approach, the change of relationships among\ntopics can be traced using a trajectory plot generated with different levels of\nword richness. These results together provide deeply mined and intuitive\nrepresentation of relationships among topics related to a specific research\narea. The application of this proposed framework to the PubMed literature shows\nthat our approach facilitates understanding of the topics constituting the\nCOVID-19 knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:01:17 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 02:39:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jeon", "Yeseul", ""], ["Chung", "Dongjun", ""], ["Park", "Jina", ""], ["Jin", "Ick Hoon", ""]]}, {"id": "2106.07380", "submitter": "Juno Kim", "authors": "Juno Kim", "title": "Predicting the Popularity of Reddit Posts with AI", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media creates crucial mass changes, as popular posts and opinions cast\na significant influence on users' decisions and thought processes. For example,\nthe recent Reddit uprising inspired by r/wallstreetbets which had remarkable\neconomic impact was started with a series of posts on the thread. The\nprediction of posts that may have a notable impact will allow for the\npreparation of possible following trends. This study aims to develop a machine\nlearning model capable of accurately predicting the popularity of a Reddit\npost. Specifically, the model is predicting the number of upvotes a post will\nreceive based on its textual content. I experimented with three different\nmodels: a baseline linear regression model, a random forest regression model,\nand a neural network. I collected Reddit post data from an online data set and\nanalyzed the model's performance when trained on a single subreddit and a\ncollection of subreddits. The results showed that the neural network model\nperformed the best when the loss of the models were compared. With the use of a\nmachine learning model to predict social trends through the reaction users have\nto post, a better picture of the near future can be envisioned.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:30:25 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:29:06 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kim", "Juno", ""]]}, {"id": "2106.07381", "submitter": "Li Dong", "authors": "Li Dong, Matthew C. Spencer, Amir Biagi", "title": "A Semi-supervised Multi-task Learning Approach to Classify Customer\n  Contact Intents", "comments": "To be published in ACL-IJCNLP 2021 workshop ECNLP", "journal-ref": "https://aclanthology.org/2021.ecnlp-1.7/", "doi": "10.18653/v1/2021.ecnlp-1.7", "report-no": "2021.ecnlp-1.7", "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of customer support, understanding customers' intents is a\ncrucial step. Machine learning plays a vital role in this type of intent\nclassification. In reality, it is typical to collect confirmation from customer\nsupport representatives (CSRs) regarding the intent prediction, though it can\nunnecessarily incur prohibitive cost to ask CSRs to assign existing or new\nintents to the mis-classified cases. Apart from the confirmed cases with and\nwithout intent labels, there can be a number of cases with no human curation.\nThis data composition (Positives + Unlabeled + multiclass Negatives) creates\nunique challenges for model development. In response to that, we propose a\nsemi-supervised multi-task learning paradigm. In this manuscript, we share our\nexperience in building text-based intent classification models for a customer\nsupport service on an E-commerce website. We improve the performance\nsignificantly by evolving the model from multiclass classification to\nsemi-supervised multi-task learning by leveraging the negative cases, domain-\nand task-adaptively pretrained ALBERT on customer contact texts, and a number\nof un-curated data with no labels. In the evaluation, the final model boosts\nthe average AUC ROC by almost 20 points compared to the baseline finetuned\nmulticlass classification ALBERT model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:13:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Dong", "Li", ""], ["Spencer", "Matthew C.", ""], ["Biagi", "Amir", ""]]}, {"id": "2106.07384", "submitter": "Mohammad Rahaman", "authors": "Mohammad Saiedur Rahaman, Wei Shao, Flora D. Salim, Ayad Turky, Andy\n  Song, Jeffrey Chan, Junliang Jiang, Doug Bradbrook", "title": "MoParkeR : Multi-objective Parking Recommendation", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3468791.3468810", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing parking recommendation solutions mainly focus on finding and\nsuggesting parking spaces based on the unoccupied options only. However, there\nare other factors associated with parking spaces that can influence someone's\nchoice of parking such as fare, parking rule, walking distance to destination,\ntravel time, likelihood to be unoccupied at a given time. More importantly,\nthese factors may change over time and conflict with each other which makes the\nrecommendations produced by current parking recommender systems ineffective. In\nthis paper, we propose a novel problem called multi-objective parking\nrecommendation. We present a solution by designing a multi-objective parking\nrecommendation engine called MoParkeR that considers various conflicting\nfactors together. Specifically, we utilise a non-dominated sorting technique to\ncalculate a set of Pareto-optimal solutions, consisting of recommended\ntrade-off parking spots. We conduct extensive experiments using two real-world\ndatasets to show the applicability of our multi-objective recommendation\nmethodology.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:57:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Rahaman", "Mohammad Saiedur", ""], ["Shao", "Wei", ""], ["Salim", "Flora D.", ""], ["Turky", "Ayad", ""], ["Song", "Andy", ""], ["Chan", "Jeffrey", ""], ["Jiang", "Junliang", ""], ["Bradbrook", "Doug", ""]]}, {"id": "2106.07385", "submitter": "Jennifer D'Souza", "authors": "Jennifer D'Souza, S\\\"oren Auer and Ted Pedersen", "title": "SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP\n  Contributions for a Research Knowledge Graph", "comments": "13 pages, 5 figures, 8 tables, In Proceedings of the Fifteenth\n  Workshop on Semantic Evaluation SemEval-2021 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n  Overall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:43:47 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:26:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["D'Souza", "Jennifer", ""], ["Auer", "S\u00f6ren", ""], ["Pedersen", "Ted", ""]]}, {"id": "2106.07453", "submitter": "Quanming Yao", "authors": "Chen Gao and Quanming Yao and Depeng Jin and Yong Li", "title": "Efficient Data-specific Model Search for Collaborative Filtering", "comments": "KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative filtering (CF), as a fundamental approach for recommender\nsystems, is usually built on the latent factor model with learnable parameters\nto predict users' preferences towards items. However, designing a proper CF\nmodel for a given data is not easy, since the properties of datasets are highly\ndiverse. In this paper, motivated by the recent advances in automated machine\nlearning (AutoML), we propose to design a data-specific CF model by AutoML\ntechniques. The key here is a new framework that unifies state-of-the-art\n(SOTA) CF methods and splits them into disjoint stages of input encoding,\nembedding function, interaction function, and prediction function. We further\ndevelop an easy-to-use, robust, and efficient search strategy, which utilizes\nrandom search and a performance predictor for efficient searching within the\nabove framework. In this way, we can combinatorially generalize data-specific\nCF models, which have not been visited in the literature, from SOTA ones.\nExtensive experiments on five real-world datasets demonstrate that our method\ncan consistently outperform SOTA ones for various CF tasks. Further experiments\nverify the rationality of the proposed framework and the efficiency of the\nsearch strategy. The searched CF models can also provide insights for exploring\nmore effective methods in the future\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:30:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gao", "Chen", ""], ["Yao", "Quanming", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2106.07720", "submitter": "Qiwei Han", "authors": "Joel Peito, Qiwei Han", "title": "Incorporating Domain Knowledge into Health Recommender Systems using\n  Hyperbolic Embeddings", "comments": "12 pages, 3 figures, accepted at the 2020 International Conference on\n  Complex Networks and Their Applications", "journal-ref": null, "doi": "10.1007/978-3-030-65351-4_11", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In contrast to many other domains, recommender systems in health services may\nbenefit particularly from the incorporation of health domain knowledge, as it\nhelps to provide meaningful and personalised recommendations catering to the\nindividual's health needs. With recent advances in representation learning\nenabling the hierarchical embedding of health knowledge into the hyperbolic\nPoincare space, this work proposes a content-based recommender system for\npatient-doctor matchmaking in primary care based on patients' health profiles,\nenriched by pre-trained Poincare embeddings of the ICD-9 codes through transfer\nlearning. The proposed model outperforms its conventional counterpart in terms\nof recommendation accuracy and has several important business implications for\nimproving the patient-doctor relationship.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:33:37 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Peito", "Joel", ""], ["Han", "Qiwei", ""]]}, {"id": "2106.07742", "submitter": "Alex Brandsen", "authors": "Alex Brandsen, Suzan Verberne, Karsten Lambers, Milco Wansleeben", "title": "Can BERT Dig It? -- Named Entity Recognition for Information Retrieval\n  in the Archaeology Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The amount of archaeological literature is growing rapidly. Until recently,\nthese data were only accessible through metadata search. We implemented a text\nretrieval engine for a large archaeological text collection ($\\sim 658$ Million\nwords). In archaeological IR, domain-specific entities such as locations, time\nperiods, and artefacts, play a central role. This motivated the development of\na named entity recognition (NER) model to annotate the full collection with\narchaeological named entities. In this paper, we present ArcheoBERTje, a BERT\nmodel pre-trained on Dutch archaeological texts. We compare the model's quality\nand output on a Named Entity Recognition task to a generic multilingual model\nand a generic Dutch model. We also investigate ensemble methods for combining\nmultiple BERT models, and combining the best BERT model with a domain thesaurus\nusing Conditional Random Fields (CRF). We find that ArcheoBERTje outperforms\nboth the multilingual and Dutch model significantly with a smaller standard\ndeviation between runs, reaching an average F1 score of 0.735. The model also\noutperforms ensemble methods combining the three models. Combining ArcheoBERTje\npredictions and explicit domain knowledge from the thesaurus did not increase\nthe F1 score. We quantitatively and qualitatively analyse the differences\nbetween the vocabulary and output of the BERT models on the full collection and\nprovide some valuable insights in the effect of fine-tuning for specific\ndomains. Our results indicate that for a highly specific text domain such as\narchaeology, further pre-training on domain-specific data increases the model's\nquality on NER by a much larger margin than shown for other domains in the\nliterature, and that domain-specific pre-training makes the addition of domain\nknowledge from a thesaurus unnecessary.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:26:19 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Brandsen", "Alex", ""], ["Verberne", "Suzan", ""], ["Lambers", "Karsten", ""], ["Wansleeben", "Milco", ""]]}, {"id": "2106.07813", "submitter": "Ashlee Milton", "authors": "Ashlee Milton, Garrett Allen, Maria Soledad Pera", "title": "To Infinity and Beyond! Accessibility is the Future for Kids' Search\n  Engines", "comments": "In the proceeding of IR for Children 2000-2020: Where Are We Now?\n  (https://www.fab4.science/ir4c/) -- Workshop co-located with the 44th\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research in the area of search engines for children remains in its infancy.\nSeminal works have studied how children use mainstream search engines, as well\nas how to design and evaluate custom search engines explicitly for children.\nThese works, however, tend to take a one-size-fits-all view, treating children\nas a unit. Nevertheless, even at the same age, children are known to possess\nand exhibit different capabilities. These differences affect how children\naccess and use search engines. To better serve children, in this vision paper,\nwe spotlight accessibility and discuss why current research on children and\nsearch engines does not, but should, focus on this significant matter.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:03:27 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Milton", "Ashlee", ""], ["Allen", "Garrett", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "2106.07864", "submitter": "Lei Chen", "authors": "Lei Chen, Fajie Yuan, Jiaxi Yang, Xiangnan He, Chengming Li and Min\n  Yang", "title": "User-specific Adaptive Fine-tuning for Cross-domain Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making accurate recommendations for cold-start users has been a longstanding\nand critical challenge for recommender systems (RS). Cross-domain\nrecommendations (CDR) offer a solution to tackle such a cold-start problem when\nthere is no sufficient data for the users who have rarely used the system. An\neffective approach in CDR is to leverage the knowledge (e.g., user\nrepresentations) learned from a related but different domain and transfer it to\nthe target domain. Fine-tuning works as an effective transfer learning\ntechnique for this objective, which adapts the parameters of a pre-trained\nmodel from the source domain to the target domain. However, current methods are\nmainly based on the global fine-tuning strategy: the decision of which layers\nof the pre-trained model to freeze or fine-tune is taken for all users in the\ntarget domain. In this paper, we argue that users in RS are personalized and\nshould have their own fine-tuning policies for better preference transfer\nlearning. As such, we propose a novel User-specific Adaptive Fine-tuning method\n(UAF), selecting which layers of the pre-trained network to fine-tune, on a\nper-user basis. Specifically, we devise a policy network with three alternative\nstrategies to automatically decide which layers to be fine-tuned and which\nlayers to have their parameters frozen for each user. Extensive experiments\nshow that the proposed UAF exhibits significantly better and more robust\nperformance for user cold-start recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:56:45 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 10:40:35 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chen", "Lei", ""], ["Yuan", "Fajie", ""], ["Yang", "Jiaxi", ""], ["He", "Xiangnan", ""], ["Li", "Chengming", ""], ["Yang", "Min", ""]]}, {"id": "2106.07931", "submitter": "Thomas Beelen", "authors": "T. Beelen, E. Velner, R. Ordelman, K.P. Truong, V. Evers, T. Huibers", "title": "Does your robot know? Enhancing children's information retrieval through\n  spoken conversation with responsible robots", "comments": "IR4Children'21 workshop at SIGIR 2021 - http://www.fab4.science/IR4C/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we identify challenges in children's current information\nretrieval process, and propose conversational robots as an opportunity to ease\nthis process in a responsible way. Tools children currently use in this\nprocess, such as search engines on a computer or voice agents, do not always\nmeet their specific needs. The conversational robot we propose maintains\ncontext, asks clarifying questions, and gives suggestions in order to better\nmeet children's needs. Since children are often too trusting of robots, we\npropose to have the robot measure, monitor and adapt to the trust the child has\nin the robot. This way, we hope to induce a critical attitude with the children\nduring their information retrieval process.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:32:43 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Beelen", "T.", ""], ["Velner", "E.", ""], ["Ordelman", "R.", ""], ["Truong", "K. P.", ""], ["Evers", "V.", ""], ["Huibers", "T.", ""]]}, {"id": "2106.08019", "submitter": "Michael V\\\"olske", "authors": "Michael V\\\"olske, Alexander Bondarenko, Maik Fr\\\"obe, Matthias Hagen,\n  Benno Stein, Jaspreet Singh, Avishek Anand", "title": "Towards Axiomatic Explanations for Neural Ranking Models", "comments": "10 pages, 2 figures. Published in the proceedings of ICTIR 2021", "journal-ref": null, "doi": "10.1145/3471158.3472256", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, neural networks have been successfully employed to improve upon\nstate-of-the-art performance in ad-hoc retrieval tasks via machine-learned\nranking functions. While neural retrieval models grow in complexity and impact,\nlittle is understood about their correspondence with well-studied IR\nprinciples. Recent work on interpretability in machine learning has provided\ntools and techniques to understand neural models in general, yet there has been\nlittle progress towards explaining ranking models.\n  We investigate whether one can explain the behavior of neural ranking models\nin terms of their congruence with well understood principles of document\nranking by using established theories from axiomatic IR. Axiomatic analysis of\ninformation retrieval models has formalized a set of constraints on ranking\ndecisions that reasonable retrieval models should fulfill. We operationalize\nthis axiomatic thinking to reproduce rankings based on combinations of\nelementary constraints. This allows us to investigate to what extent the\nranking decisions of neural rankers can be explained in terms of retrieval\naxioms, and which axioms apply in which situations. Our experimental study\nconsiders a comprehensive set of axioms over several representative neural\nrankers. While the existing axioms can already explain the particularly\nconfident ranking decisions rather well, future work should extend the axiom\nset to also cover the other still \"unexplainable\" neural IR rank decisions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:10:04 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 13:43:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["V\u00f6lske", "Michael", ""], ["Bondarenko", "Alexander", ""], ["Fr\u00f6be", "Maik", ""], ["Hagen", "Matthias", ""], ["Stein", "Benno", ""], ["Singh", "Jaspreet", ""], ["Anand", "Avishek", ""]]}, {"id": "2106.08042", "submitter": "Boris Tseytlin Mr", "authors": "Boris Tseytlin and Ilya Makarov", "title": "Hotel Recognition via Latent Image Embedding", "comments": "IWANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We approach the problem of hotel recognition with deep metric learning. We\noverview the existing approaches and propose a modification to Contrastive loss\ncalled Contrastive-Triplet loss. We construct a robust pipeline for\nbenchmarking metric learning models and perform experiments on Hotels-50K and\nCUB200 datasets. Contrastive-Triplet loss is shown to achieve better retrieval\non Hotels-50k. We open-source our code.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:52:07 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Tseytlin", "Boris", ""], ["Makarov", "Ilya", ""]]}, {"id": "2106.08072", "submitter": "Matthieu Latapy", "authors": "Jules Azad Emery and Matthieu Latapy", "title": "Full Bitcoin Blockchain Data Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the fact that it is publicly available, collecting and processing the\nfull bitcoin blockchain data is not trivial. Its mere size, history, and other\nfeatures indeed raise quite specific challenges, that we address in this paper.\nThe strengths of our approach are the following: it relies on very basic and\nstandard tools, which makes the procedure reliable and easily reproducible; it\nis a purely lossless procedure ensuring that we catch and preserve all existing\ndata; it provides additional indexing that makes it easy to further process the\nwhole data and select appropriate subsets of it. We present our procedure in\ndetails and illustrate its added value on large-scale use cases, like address\nclustering. We provide an implementation online, as well as the obtained\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:02:49 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Emery", "Jules Azad", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2106.08166", "submitter": "Dimitrios Alivanistos", "authors": "Dimitrios Alivanistos and Max Berrendorf and Michael Cochez and\n  Mikhail Galkin", "title": "Query Embedding on Hyper-relational Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hop logical reasoning is an established problem in the field of\nrepresentation learning on knowledge graphs (KGs). It subsumes both one-hop\nlink prediction as well as other more complex types of logical queries.\nExisting algorithms operate only on classical, triple-based graphs, whereas\nmodern KGs often employ a hyper-relational modeling paradigm. In this paradigm,\ntyped edges may have several key-value pairs known as qualifiers that provide\nfine-grained context for facts. In queries, this context modifies the meaning\nof relations, and usually reduces the answer set. Hyper-relational queries are\noften observed in real-world KG applications, and existing approaches for\napproximate query answering cannot make use of qualifier pairs. In this work,\nwe bridge this gap and extend the multi-hop reasoning problem to\nhyper-relational KGs allowing to tackle this new type of complex queries.\nBuilding upon recent advancements in Graph Neural Networks and query embedding\ntechniques, we study how to embed and answer hyper-relational conjunctive\nqueries. Besides that, we propose a method to answer such queries and\ndemonstrate in our experiments that qualifiers improve query answering on a\ndiverse set of query patterns.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 14:08:50 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 13:53:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Alivanistos", "Dimitrios", ""], ["Berrendorf", "Max", ""], ["Cochez", "Michael", ""], ["Galkin", "Mikhail", ""]]}, {"id": "2106.08252", "submitter": "Nima Ebadi", "authors": "Nima Ebadi and Peyman Najafirad", "title": "Interpretable Self-supervised Multi-task Learning for COVID-19\n  Information Retrieval and Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapidly evolving literature of COVID-19 related articles makes it\nchallenging for NLP models to be effectively trained for information retrieval\nand extraction with the corresponding labeled data that follows the current\ndistribution of the pandemic. On the other hand, due to the uncertainty of the\nsituation, human experts' supervision would always be required to double check\nthe decision making of these models highlighting the importance of\ninterpretability. In the light of these challenges, this study proposes an\ninterpretable self-supervised multi-task learning model to jointly and\neffectively tackle the tasks of information retrieval (IR) and extraction (IE)\nduring the current emergency health crisis situation. Our results show that our\nmodel effectively leverage the multi-task and self-supervised learning to\nimprove generalization, data efficiency and robustness to the ongoing dataset\nshift problem. Our model outperforms baselines in IE and IR tasks, respectively\nby micro-f score of 0.08 (LCA-F score of 0.05), and MAP of 0.05 on average. In\nIE the zero- and few-shot learning performances are on average 0.32 and 0.19\nmicro-f score higher than those of the baselines.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:01:44 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Ebadi", "Nima", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2106.08433", "submitter": "Georgios Sidiropoulos", "authors": "Georgios Sidiropoulos, Nikos Voskarides, Svitlana Vakulenko, Evangelos\n  Kanoulas", "title": "Analysing Dense Passage Retrieval for Multi-hop Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the performance of passage retrieval models in the presence of\ncomplex (multi-hop) questions to provide a better understanding of how\nretrieval systems behave when multiple hops of reasoning are needed. In simple\nopen-domain question answering (QA), dense passage retrieval has become one of\nthe standard approaches for retrieving the relevant passages to infer an\nanswer. Recently, dense passage retrieval also achieved state-of-the-art\nresults in multi-hop QA, where aggregating information from multiple documents\nand reasoning over them is required. However, so far, the dense retrieval\nmodels are not evaluated properly concerning the multi-hop nature of the\nproblem: models are typically evaluated by the end result of the retrieval\npipeline, which leaves unclear where their success lies. In this work, we\nprovide an in-depth evaluation of such models not only unveiling the reasons\nbehind their success but also their limitations. Moreover, we introduce a\nhybrid (lexical and dense) retrieval approach that is highly competitive with\nthe state-of-the-art dense retrieval model, while requiring substantially less\ncomputational resources. Furthermore, we also perform qualitative analysis to\nbetter understand the challenges behind passage retrieval for multi-hop QA.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:52:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Sidiropoulos", "Georgios", ""], ["Voskarides", "Nikos", ""], ["Vakulenko", "Svitlana", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2106.08527", "submitter": "Ruoyuan Gao", "authors": "Ruoyuan Gao, Yingqiang Ge, Chirag Shah", "title": "FAIR: Fairness-Aware Information Retrieval Evaluation", "comments": "submitted to The Journal of the Association for Information Science\n  and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emerging needs of creating fairness-aware solutions for search and\nrecommendation systems, a daunting challenge exists of evaluating such\nsolutions. While many of the traditional information retrieval (IR) metrics can\ncapture the relevance, diversity and novelty for the utility with respect to\nusers, they are not suitable for inferring whether the presented results are\nfair from the perspective of responsible information exposure. On the other\nhand, various fairness metrics have been proposed but they do not account for\nthe user utility or do not measure it adequately. To address this problem, we\npropose a new metric called Fairness-Aware IR (FAIR). By unifying standard IR\nmetrics and fairness measures into an integrated metric, this metric offers a\nnew perspective for evaluating fairness-aware ranking results. Based on this\nmetric, we developed an effective ranking algorithm that jointly optimized user\nutility and fairness. The experimental results showed that our FAIR metric\ncould highlight results with good user utility and fair information exposure.\nWe showed how FAIR related to existing metrics and demonstrated the\neffectiveness of our FAIR-based algorithm. We believe our work opens up a new\ndirection of pursuing a computationally feasible metric for evaluating and\nimplementing the fairness-aware IR systems.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 02:38:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gao", "Ruoyuan", ""], ["Ge", "Yingqiang", ""], ["Shah", "Chirag", ""]]}, {"id": "2106.08700", "submitter": "SeongKu Kang", "authors": "SeongKu Kang, Junyoung Hwang, Wonbin Kweon, Hwanjo Yu", "title": "Topology Distillation for Recommender System", "comments": "KDD 2021. 9 pages + appendix (2 pages). 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems (RS) have employed knowledge distillation which is a\nmodel compression technique training a compact student model with the knowledge\ntransferred from a pre-trained large teacher model. Recent work has shown that\ntransferring knowledge from the teacher's intermediate layer significantly\nimproves the recommendation quality of the student. However, they transfer the\nknowledge of individual representation point-wise and thus have a limitation in\nthat primary information of RS lies in the relations in the representation\nspace. This paper proposes a new topology distillation approach that guides the\nstudent by transferring the topological structure built upon the relations in\nthe teacher space. We first observe that simply making the student learn the\nwhole topological structure is not always effective and even degrades the\nstudent's performance. We demonstrate that because the capacity of the student\nis highly limited compared to that of the teacher, learning the whole\ntopological structure is daunting for the student. To address this issue, we\npropose a novel method named Hierarchical Topology Distillation (HTD) which\ndistills the topology hierarchically to cope with the large capacity gap. Our\nextensive experiments on real-world datasets show that the proposed method\nsignificantly outperforms the state-of-the-art competitors. We also provide\nin-depth analyses to ascertain the benefit of distilling the topology for RS.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:06:52 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kang", "SeongKu", ""], ["Hwang", "Junyoung", ""], ["Kweon", "Wonbin", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2106.08701", "submitter": "Olesya Mryglod", "authors": "O. Mryglod, S. Nazarovets and S. Kozmenko", "title": "Universal and specific features of Ukrainian economic research:\n  publication analysis based on Crossref data", "comments": "This is the version of the Article before peer-review has taken\n  place. The paper is submitted to \\textit{Scientometrics} journal, Manuscript\n  Number: SCIM-D-21-00282R1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our study is one of the first examples of multidimensional and longitudinal\ndisciplinary analysis at the national level based on Crossref data. We present\na large-scale quantitative analysis of Ukrainian economics. This study is not\nyet another example of research aimed at ranking of local journals, authors or\ninstitutions, but rather exploring general tendencies that can be compared to\nother countries or regions. We study different aspects of Ukrainian economics\noutput. In particular, the collaborative nature, geographic landscape and some\npeculiarities of citation statistics are investigated. We have found that\nUkrainian economics is characterized by a comparably small share of co-authored\npublications, however, it demonstrates the tendency towards more collaborative\noutput. Based on our analysis, we discuss specific and universal features of\nUkrainian economic research. The importance of supporting various initiatives\naimed at enriching open scholarly metadata is considered. A comprehensive and\nhigh-quality meta description of publications is probably the shortest path to\na better understanding of national trends, especially for non-English speaking\ncountries. The results of our analysis can be used to better understand\nUkrainian economic research and support research policy decisions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:07:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Mryglod", "O.", ""], ["Nazarovets", "S.", ""], ["Kozmenko", "S.", ""]]}, {"id": "2106.08770", "submitter": "Alexis Dusart", "authors": "Alexis Dusart, Karen Pinel-Sauvagnat, Gilles Hubert", "title": "TSSuBERT: Tweet Stream Summarization Using BERT", "comments": "UNDER SUBMISSION", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of deep neural networks and the emergence of pre-trained\nlanguage models such as BERT allow to increase performance on many NLP tasks.\nHowever, these models do not meet the same popularity for tweet summarization,\nwhich can probably be explained by the lack of existing collections for\ntraining and evaluation. Our contribution in this paper is twofold : (1) we\nintroduce a large dataset for Twitter event summarization, and (2) we propose a\nneural model to automatically summarize huge tweet streams. This extractive\nmodel combines in an original way pre-trained language models and vocabulary\nfrequency-based representations to predict tweet salience. An additional\nadvantage of the model is that it automatically adapts the size of the output\nsummary according to the input tweet stream. We conducted experiments using two\ndifferent Twitter collections, and promising results are observed in comparison\nwith state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:28:35 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Dusart", "Alexis", ""], ["Pinel-Sauvagnat", "Karen", ""], ["Hubert", "Gilles", ""]]}, {"id": "2106.08908", "submitter": "Dimitris Pappas", "authors": "Dimitris Pappas and Ion Androutsopoulos", "title": "A Neural Model for Joint Document and Snippet Ranking in Question\n  Answering for Large Document Collections", "comments": "12 pages, 3 figures, 4 tables, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Question answering (QA) systems for large document collections typically use\npipelines that (i) retrieve possibly relevant documents, (ii) re-rank them,\n(iii) rank paragraphs or other snippets of the top-ranked documents, and (iv)\nselect spans of the top-ranked snippets as exact answers. Pipelines are\nconceptually simple, but errors propagate from one component to the next,\nwithout later components being able to revise earlier decisions. We present an\narchitecture for joint document and snippet ranking, the two middle stages,\nwhich leverages the intuition that relevant documents have good snippets and\ngood snippets come from relevant documents. The architecture is general and can\nbe used with any neural text relevance ranker. We experiment with two main\ninstantiations of the architecture, based on POSIT-DRMM (PDRMM) and a\nBERT-based ranker. Experiments on biomedical data from BIOASQ show that our\njoint models vastly outperform the pipelines in snippet retrieval, the main\ngoal for QA, with fewer trainable parameters, also remaining competitive in\ndocument retrieval. Furthermore, our joint PDRMM-based model is competitive\nwith BERT-based models, despite using orders of magnitude fewer parameters.\nThese claims are also supported by human evaluation on two test batches of\nBIOASQ. To test our key findings on another dataset, we modified the Natural\nQuestions dataset so that it can also be used for document and snippet\nretrieval. Our joint PDRMM-based model again outperforms the corresponding\npipeline in snippet retrieval on the modified Natural Questions dataset, even\nthough it performs worse than the pipeline in document retrieval. We make our\ncode and the modified Natural Questions dataset publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:04:19 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pappas", "Dimitris", ""], ["Androutsopoulos", "Ion", ""]]}, {"id": "2106.08934", "submitter": "Chuhan Wu", "authors": "Chuhan Wu, Fangzhao Wu, Yongfeng Huang, Xing Xie", "title": "Personalized News Recommendation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized news recommendation is an important technique to help users find\ntheir interested news information and alleviate their information overload. It\nhas been extensively studied over decades and has achieved notable success in\nimproving users' news reading experience. However, there are still many\nunsolved problems and challenges that need to be further studied. To help\nresearchers master the advances in personalized news recommendation over the\npast years, in this paper we present a comprehensive overview of personalized\nnews recommendation. Instead of following the conventional taxonomy of news\nrecommendation methods, in this paper we propose a novel perspective to\nunderstand personalized news recommendation based on its core problems and the\nassociated techniques and challenges. We first review the techniques for\ntackling each core problem in a personalized news recommender system and the\nchallenges they face. Next, we introduce the public datasets and evaluation\nmethods for personalized news recommendation. We then discuss the key points on\nimproving the responsibility of personalized news recommender systems. Finally,\nwe raise several research directions that are worth investigating in the\nfuture. This paper can provide up-to-date and comprehensive views to help\nreaders understand the personalized news recommendation field. We hope this\npaper can facilitate research on personalized news recommendation and as well\nas related fields in natural language processing and data mining.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:43:56 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 07:29:42 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wu", "Chuhan", ""], ["Wu", "Fangzhao", ""], ["Huang", "Yongfeng", ""], ["Xie", "Xing", ""]]}, {"id": "2106.09227", "submitter": "Ching-Wei Chen", "authors": "Rosie Jones, Hamed Zamani, Markus Schedl, Ching-Wei Chen, Sravana\n  Reddy, Ann Clifton, Jussi Karlgren, Helia Hashemi, Aasish Pappu, Zahra\n  Nazari, Longqi Yang, Oguz Semerci, Hugues Bouchard, Ben Carterette", "title": "Current Challenges and Future Directions in Podcast Information Access", "comments": "SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Podcasts are spoken documents across a wide-range of genres and styles, with\ngrowing listenership across the world, and a rapidly lowering barrier to entry\nfor both listeners and creators. The great strides in search and recommendation\nin research and industry have yet to see impact in the podcast space, where\nrecommendations are still largely driven by word of mouth. In this perspective\npaper, we highlight the many differences between podcasts and other media, and\ndiscuss our perspective on challenges and future research directions in the\ndomain of podcast information access.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 03:33:10 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Jones", "Rosie", ""], ["Zamani", "Hamed", ""], ["Schedl", "Markus", ""], ["Chen", "Ching-Wei", ""], ["Reddy", "Sravana", ""], ["Clifton", "Ann", ""], ["Karlgren", "Jussi", ""], ["Hashemi", "Helia", ""], ["Pappu", "Aasish", ""], ["Nazari", "Zahra", ""], ["Yang", "Longqi", ""], ["Semerci", "Oguz", ""], ["Bouchard", "Hugues", ""], ["Carterette", "Ben", ""]]}, {"id": "2106.09297", "submitter": "Fuyu Lv", "authors": "Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng,\n  Xiao-Ming Wu, Qianli Ma", "title": "Embedding-based Product Retrieval in Taobao Search", "comments": "9 pages, accepted by KDD2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the product search service of e-commerce platforms has become a\nvital shopping channel in people's life. The retrieval phase of products\ndetermines the search system's quality and gradually attracts researchers'\nattention. Retrieving the most relevant products from a large-scale corpus\nwhile preserving personalized user characteristics remains an open question.\nRecent approaches in this domain have mainly focused on embedding-based\nretrieval (EBR) systems. However, after a long period of practice on Taobao, we\nfind that the performance of the EBR system is dramatically degraded due to\nits: (1) low relevance with a given query and (2) discrepancy between the\ntraining and inference phases. Therefore, we propose a novel and practical\nembedding-based product retrieval model, named Multi-Grained Deep Semantic\nProduct Retrieval (MGDSPR). Specifically, we first identify the inconsistency\nbetween the training and inference stages, and then use the softmax\ncross-entropy loss as the training objective, which achieves better performance\nand faster convergence. Two efficient methods are further proposed to improve\nretrieval relevance, including smoothing noisy training data and generating\nrelevance-improving hard negative samples without requiring extra knowledge and\ntraining procedures. We evaluate MGDSPR on Taobao Product Search with\nsignificant metrics gains observed in offline experiments and online A/B tests.\nMGDSPR has been successfully deployed to the existing multi-channel retrieval\nsystem in Taobao Search. We also introduce the online deployment scheme and\nshare practical lessons of our retrieval system to contribute to the community.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:01:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Sen", ""], ["Lv", "Fuyu", ""], ["Jin", "Taiwei", ""], ["Lin", "Guli", ""], ["Yang", "Keping", ""], ["Zeng", "Xiaoyi", ""], ["Wu", "Xiao-Ming", ""], ["Ma", "Qianli", ""]]}, {"id": "2106.09306", "submitter": "Dou Hu", "authors": "Dou Hu, Lingwei Wei, Wei Zhou, Xiaoyong Huai, Zhiqi Fang, Songlin Hu", "title": "PEN4Rec: Preference Evolution Networks for Session-based Recommendation", "comments": "12 pages, accepted by KSEM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendation aims to predict user the next action based on\nhistorical behaviors in an anonymous session. For better recommendations, it is\nvital to capture user preferences as well as their dynamics. Besides, user\npreferences evolve over time dynamically and each preference has its own\nevolving track. However, most previous works neglect the evolving trend of\npreferences and can be easily disturbed by the effect of preference drifting.\nIn this paper, we propose a novel Preference Evolution Networks for\nsession-based Recommendation (PEN4Rec) to model preference evolving process by\na two-stage retrieval from historical contexts. Specifically, the first-stage\nprocess integrates relevant behaviors according to recent items. Then, the\nsecond-stage process models the preference evolving trajectory over time\ndynamically and infer rich preferences. The process can strengthen the effect\nof relevant sequential behaviors during the preference evolution and weaken the\ndisturbance from preference drifting. Extensive experiments on three public\ndatasets demonstrate the effectiveness and superiority of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:18:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Hu", "Dou", ""], ["Wei", "Lingwei", ""], ["Zhou", "Wei", ""], ["Huai", "Xiaoyong", ""], ["Fang", "Zhiqi", ""], ["Hu", "Songlin", ""]]}, {"id": "2106.09375", "submitter": "Marius Pesavento", "authors": "Khaled Ardah and Martin Haardt and Tianyi Liu and Frederic Matter and\n  Marius Pesavento and Marc E. Pfetsch", "title": "Recovery under Side Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper addresses sparse signal reconstruction under various types of\nstructural side constraints with applications in multi-antenna systems. Side\nconstraints may result from prior information on the measurement system and the\nsparse signal structure. They may involve the structure of the sensing matrix,\nthe structure of the non-zero support values, the temporal structure of the\nsparse representationvector, and the nonlinear measurement structure. First, we\ndemonstrate how a priori information in form of structural side constraints\ninfluence recovery guarantees (null space properties) using L1-minimization.\nFurthermore, for constant modulus signals, signals with row-, block- and\nrank-sparsity, as well as non-circular signals, we illustrate how structural\nprior information can be used to devise efficient algorithms with improved\nrecovery performance and reduced computational complexity. Finally, we address\nthe measurement system design for linear and nonlinear measurements of sparse\nsignals. Moreover, we discuss the linear mixing matrix design based on\ncoherence minimization. Then we extend our focus to nonlinear measurement\nsystems where we design parallel optimization algorithms to efficiently compute\nstationary points in the sparse phase retrieval problem with and without\ndictionary learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:49:06 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ardah", "Khaled", ""], ["Haardt", "Martin", ""], ["Liu", "Tianyi", ""], ["Matter", "Frederic", ""], ["Pesavento", "Marius", ""], ["Pfetsch", "Marc E.", ""]]}, {"id": "2106.09395", "submitter": "Xiao Liu", "authors": "Xiao Liu, Haoyun Hong, Xinghao Wang, Zeyi Chen, Evgeny Kharlamov,\n  Yuxiao Dong, Jie Tang", "title": "A Self-supervised Method for Entity Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:22:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Xiao", ""], ["Hong", "Haoyun", ""], ["Wang", "Xinghao", ""], ["Chen", "Zeyi", ""], ["Kharlamov", "Evgeny", ""], ["Dong", "Yuxiao", ""], ["Tang", "Jie", ""]]}, {"id": "2106.09533", "submitter": "Graham Tierney", "authors": "Graham Tierney and Christopher Bail and Alexander Volfovsky", "title": "Author Clustering and Topic Estimation for Short Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of short text, such as social media posts, is extremely difficult\nbecause it relies on observing many document-level word co-occurrence pairs.\nBeyond topic distributions, a common downstream task of the modeling is\ngrouping the authors of these documents for subsequent analyses. Traditional\nmodels estimate the document groupings and identify user clusters with an\nindependent procedure. We propose a novel model that expands on the Latent\nDirichlet Allocation by modeling strong dependence among the words in the same\ndocument, with user-level topic distributions. We also simultaneously cluster\nusers, removing the need for post-hoc cluster estimation and improving topic\nestimation by shrinking noisy user-level topic distributions towards typical\nvalues. Our method performs as well as -- or better -- than traditional\napproaches to problems arising in short text, and we demonstrate its usefulness\non a dataset of tweets from United States Senators, recovering both meaningful\ntopics and clusters that reflect partisan ideology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:55:55 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Tierney", "Graham", ""], ["Bail", "Christopher", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "2106.09590", "submitter": "Lisa Wenige", "authors": "Lisa Wenige, Claus Stadler, Michael Martin, Richard Figura, Robert\n  Sauter, Christopher W. Frank", "title": "Open Data and the Status Quo -- A Fine-Grained Evaluation Framework for\n  Open Data Quality and an Analysis of Open Data portals in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a framework for assessing data and metadata quality\nwithin Open Data portals. Although a few benchmark frameworks already exist for\nthis purpose, they are not yet detailed enough in both breadth and depth to\nmake valid statements about the actual discoverability and accessibility of\npublicly available data collections. To address this research gap, we have\ndesigned a quality framework that is able to evaluate data quality in Open Data\nportals on dedicated and fine-grained dimensions, such as interoperability,\nfindability, uniqueness or completeness. Additionally, we propose quality\nmeasures that allow for valid assessments regarding cross-portal findability\nand uniqueness of dataset descriptions. We have validated our novel quality\nframework for the German Open Data landscape and found out that metadata often\nstill lacks meaningful descriptions and is not yet extensively connected to the\nSemantic Web.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:16:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wenige", "Lisa", ""], ["Stadler", "Claus", ""], ["Martin", "Michael", ""], ["Figura", "Richard", ""], ["Sauter", "Robert", ""], ["Frank", "Christopher W.", ""]]}, {"id": "2106.09665", "submitter": "Zhichao Xu", "authors": "Zhichao Xu, Hansi Zeng, Qingyao Ai", "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N\n  Recommendation", "comments": "in proceedings of ICTIR 2021", "journal-ref": null, "doi": "10.1145/3471158.3472258", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:15:45 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 05:49:55 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 17:21:57 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Xu", "Zhichao", ""], ["Zeng", "Hansi", ""], ["Ai", "Qingyao", ""]]}, {"id": "2106.09775", "submitter": "Md Mustafizur Rahman", "authors": "Md Mustafizur Rahman, Dinesh Balakrishnan, Dhiraj Murthy, Mucahid\n  Kutlu, Matthew Lease", "title": "An Information Retrieval Approach to Building Datasets for Hate Speech\n  Detection", "comments": "10 pages (Under review in CIKM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building a benchmark dataset for hate speech detection presents several\nchallenges. Firstly, because hate speech is relatively rare -- e.g., less than\n3\\% of Twitter posts are hateful \\citep{founta2018large} -- random sampling of\ntweets to annotate is inefficient in capturing hate speech. A common practice\nis to only annotate tweets containing known ``hate words'', but this risks\nyielding a biased benchmark that only partially captures the real-world\nphenomenon of interest. A second challenge is that definitions of hate speech\ntend to be highly variable and subjective. Annotators having diverse prior\nnotions of hate speech may not only disagree with one another but also struggle\nto conform to specified labeling guidelines. Our key insight is that the rarity\nand subjectivity of hate speech are akin to that of relevance in information\nretrieval (IR). This connection suggests that well-established methodologies\nfor creating IR test collections might also be usefully applied to create\nbetter benchmark datasets for hate speech detection. Firstly, to intelligently\nand efficiently select which tweets to annotate, we apply established IR\ntechniques of {\\em pooling} and {\\em active learning}. Secondly, to improve\nboth consistency and value of annotations, we apply {\\em task decomposition}\n\\cite{Zhang-sigir14} and {\\em annotator rationale} \\cite{mcdonnell16-hcomp}\ntechniques. Using the above techniques, we create and share a new benchmark\ndataset\\footnote{We will release the dataset upon publication.} for hate speech\ndetection with broader coverage than prior datasets. We also show a dramatic\ndrop in accuracy of existing detection models when tested on these broader\nforms of hate. Collected annotator rationales not only provide documented\nsupport for labeling decisions but also create exciting future work\nopportunities for dual-supervision and/or explanation generation in modeling.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 19:25:39 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 00:45:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rahman", "Md Mustafizur", ""], ["Balakrishnan", "Dinesh", ""], ["Murthy", "Dhiraj", ""], ["Kutlu", "Mucahid", ""], ["Lease", "Matthew", ""]]}, {"id": "2106.09866", "submitter": "Eugene Yang", "authors": "Eugene Yang and David D. Lewis and Ophir Frieder", "title": "On Minimizing Cost in Legal Document Review Workflows", "comments": "10 pages, 3 figures. Accepted at DocEng 21", "journal-ref": null, "doi": "10.1145/3469096.3469872", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:51:47 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yang", "Eugene", ""], ["Lewis", "David D.", ""], ["Frieder", "Ophir", ""]]}, {"id": "2106.09871", "submitter": "Eugene Yang", "authors": "Eugene Yang and David D. Lewis and Ophir Frieder", "title": "Heuristic Stopping Rules For Technology-Assisted Review", "comments": "10 pages, 2 figures. Accepted at DocEng 21", "journal-ref": null, "doi": "10.1145/3469096.3469873", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technology-assisted review (TAR) refers to human-in-the-loop active learning\nworkflows for finding relevant documents in large collections. These workflows\noften must meet a target for the proportion of relevant documents found (i.e.\nrecall) while also holding down costs. A variety of heuristic stopping rules\nhave been suggested for striking this tradeoff in particular settings, but none\nhave been tested against a range of recall targets and tasks. We propose two\nnew heuristic stopping rules, Quant and QuantCI based on model-based estimation\ntechniques from survey research. We compare them against a range of proposed\nheuristics and find they are accurate at hitting a range of recall targets\nwhile substantially reducing review costs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 02:14:20 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yang", "Eugene", ""], ["Lewis", "David D.", ""], ["Frieder", "Ophir", ""]]}, {"id": "2106.10069", "submitter": "Alejandro Bellogin", "authors": "Pablo S\\'anchez and Alejandro Bellog\\'in", "title": "Point-of-Interest Recommender Systems: A Survey from an Experimental\n  Perspective", "comments": "Submitted in Jul 2020 (revised in Jun 2021, still under review) to\n  ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point-of-Interest recommendation is an increasing research and developing\narea within the widely adopted technologies known as Recommender Systems. Among\nthem, those that exploit information coming from Location-Based Social Networks\n(LBSNs) are very popular nowadays and could work with different information\nsources, which pose several challenges and research questions to the community\nas a whole. We present a systematic review focused on the research done in the\nlast 10 years about this topic. We discuss and categorize the algorithms and\nevaluation methodologies used in these works and point out the opportunities\nand challenges that remain open in the field. More specifically, we report the\nleading recommendation techniques and information sources that have been\nexploited more often (such as the geographical signal and deep learning\napproaches) while we also alert about the lack of reproducibility in the field\nthat may hinder real performance improvements.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:31:42 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["S\u00e1nchez", "Pablo", ""], ["Bellog\u00edn", "Alejandro", ""]]}, {"id": "2106.10159", "submitter": "Cheng-Te Li", "authors": "Yi-Ling Hsu, Yu-Che Tsai, Cheng-Te Li", "title": "FinGAT: Financial Graph Attention Networks for Recommending Top-K\n  Profitable Stocks", "comments": "Accepted to IEEE TKDE 2021. The first two authors equally contribute\n  to this work. Code is available at\n  https://github.com/Roytsai27/Financial-GraphAttention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial technology (FinTech) has drawn much attention among investors and\ncompanies. While conventional stock analysis in FinTech targets at predicting\nstock prices, less effort is made for profitable stock recommendation. Besides,\nin existing approaches on modeling time series of stock prices, the\nrelationships among stocks and sectors (i.e., categories of stocks) are either\nneglected or pre-defined. Ignoring stock relationships will miss the\ninformation shared between stocks while using pre-defined relationships cannot\ndepict the latent interactions or influence of stock prices between stocks. In\nthis work, we aim at recommending the top-K profitable stocks in terms of\nreturn ratio using time series of stock prices and sector information. We\npropose a novel deep learning-based model, Financial Graph Attention Networks\n(FinGAT), to tackle the task under the setting that no pre-defined\nrelationships between stocks are given. The idea of FinGAT is three-fold.\nFirst, we devise a hierarchical learning component to learn short-term and\nlong-term sequential patterns from stock time series. Second, a fully-connected\ngraph between stocks and a fully-connected graph between sectors are\nconstructed, along with graph attention networks, to learn the latent\ninteractions among stocks and sectors. Third, a multi-task objective is devised\nto jointly recommend the profitable stocks and predict the stock movement.\nExperiments conducted on Taiwan Stock, S&P 500, and NASDAQ datasets exhibit\nremarkable recommendation performance of our FinGAT, comparing to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:51:14 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Hsu", "Yi-Ling", ""], ["Tsai", "Yu-Che", ""], ["Li", "Cheng-Te", ""]]}, {"id": "2106.10547", "submitter": "Chirag Mahapatra", "authors": "Chirag Mahapatra and Kedar Bellare", "title": "Leveraging Multiple Online Sources for Accurate Income Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Income verification is the problem of validating a person's stated income\ngiven basic identity information such as name, location, job title and\nemployer. It is widely used in the context of mortgage lending, rental\napplications and other financial risk models. However, the current processes\nsurrounding verification involve significant human effort and document\ngathering which can be both time-consuming and expensive. In this paper, we\npropose a novel model for verifying an individual's income given very limited\nidentity information typically available in loan applications. Our model is a\ncombination of a deep neural network and hand-engineered features. The hand\nengineered features are based upon matching the input information against\nincome records extracted automatically from various publicly available online\nsources (e.g. payscale.com, H-1B filings, government employee salaries). We\nconduct experiments on two data sets, one simulated from H-1B records and the\nother from a real-world data set of peer-to-peer (P2P) loan applications\nobtained from one of the world's largest P2P lending platform. Our results show\na significant reduction in error of 3-6% relative to several strong baselines.\nWe also perform ablation studies to demonstrate that a combined model is indeed\nnecessary to achieve state-of-the-art performance on this task.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 18:22:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mahapatra", "Chirag", ""], ["Bellare", "Kedar", ""]]}, {"id": "2106.10621", "submitter": "Dong Li", "authors": "Dong Li and Ruoming Jin and Jing Gao and Zhi Liu", "title": "On Sampling Top-K Recommendation Evaluation", "comments": null, "journal-ref": null, "doi": "10.1145/3394486.3403262", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Rendle has warned that the use of sampling-based top-$k$ metrics\nmight not suffice. This throws a number of recent studies on deep\nlearning-based recommendation algorithms, and classic non-deep-learning\nalgorithms using such a metric, into jeopardy. In this work, we thoroughly\ninvestigate the relationship between the sampling and global top-$K$ Hit-Ratio\n(HR, or Recall), originally proposed by Koren[2] and extensively used by\nothers. By formulating the problem of aligning sampling top-$k$ ($SHR@k$) and\nglobal top-$K$ ($HR@K$) Hit-Ratios through a mapping function $f$, so that\n$SHR@k\\approx HR@f(k)$, we demonstrate both theoretically and experimentally\nthat the sampling top-$k$ Hit-Ratio provides an accurate approximation of its\nglobal (exact) counterpart, and can consistently predict the correct winners\n(the same as indicate by their corresponding global Hit-Ratios).\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 04:48:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Dong", ""], ["Jin", "Ruoming", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""]]}, {"id": "2106.10679", "submitter": "Carmel Wenga", "authors": "Carmel Wenga, Majirus Fansi, S\\'ebastien Chabrier, Jean-Martial Mari,\n  Alban Gabillon", "title": "A Comprehensive Review on Non-Neural Networks Collaborative Filtering\n  Recommendation Systems", "comments": "29 pages, 7 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, recommender systems have attracted a lot of\ninterest due to the explosion in the amount of data in online applications. A\nparticular attention has been paid to collaborative filtering, which is the\nmost widely used in applications that involve information recommendations.\nCollaborative filtering (CF) uses the known preference of a group of users to\nmake predictions and recommendations about the unknown preferences of other\nusers (recommendations are made based on the past behavior of users). First\nintroduced in the 1990s, a wide variety of increasingly successful models have\nbeen proposed. Due to the success of machine learning techniques in many areas,\nthere has been a growing emphasis on the application of such algorithms in\nrecommendation systems. In this article, we present an overview of the CF\napproaches for recommender systems, their two main categories, and their\nevaluation metrics. We focus on the application of classical Machine Learning\nalgorithms to CF recommender systems by presenting their evolution from their\nfirst use-cases to advanced Machine Learning models. We attempt to provide a\ncomprehensive and comparative overview of CF systems (with python\nimplementations) that can serve as a guideline for research and practice in\nthis area.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 11:13:33 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 17:52:30 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wenga", "Carmel", ""], ["Fansi", "Majirus", ""], ["Chabrier", "S\u00e9bastien", ""], ["Mari", "Jean-Martial", ""], ["Gabillon", "Alban", ""]]}, {"id": "2106.10681", "submitter": "Jiapeng Wang", "authors": "Jiapeng Wang, Tianwei Wang, Guozhi Tang, Lianwen Jin, Weihong Ma, Kai\n  Ding, Yichao Huang", "title": "Tag, Copy or Predict: A Unified Weakly-Supervised Learning Framework for\n  Visual Information Extraction using Sequences", "comments": "IJCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual information extraction (VIE) has attracted increasing attention in\nrecent years. The existing methods usually first organized optical character\nrecognition (OCR) results into plain texts and then utilized token-level entity\nannotations as supervision to train a sequence tagging model. However, it\nexpends great annotation costs and may be exposed to label confusion, and the\nOCR errors will also significantly affect the final performance. In this paper,\nwe propose a unified weakly-supervised learning framework called TCPN (Tag,\nCopy or Predict Network), which introduces 1) an efficient encoder to\nsimultaneously model the semantic and layout information in 2D OCR results; 2)\na weakly-supervised training strategy that utilizes only key information\nsequences as supervision; and 3) a flexible and switchable decoder which\ncontains two inference modes: one (Copy or Predict Mode) is to output key\ninformation sequences of different categories by copying a token from the input\nor predicting one in each time step, and the other (Tag Mode) is to directly\ntag the input sequence in a single forward pass. Our method shows new\nstate-of-the-art performance on several public benchmarks, which fully proves\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 11:56:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Jiapeng", ""], ["Wang", "Tianwei", ""], ["Tang", "Guozhi", ""], ["Jin", "Lianwen", ""], ["Ma", "Weihong", ""], ["Ding", "Kai", ""], ["Huang", "Yichao", ""]]}, {"id": "2106.10776", "submitter": "Matthias Grabmair", "authors": "Zihan Huang, Charles Low, Mengqiu Teng, Hongyi Zhang, Daniel E. Ho,\n  Mark S. Krass, Matthias Grabmair", "title": "Context-Aware Legal Citation Recommendation using Deep Learning", "comments": "10 pages published in Proceedings of ICAIL 2021; link to data here:\n  https://reglab.stanford.edu/data/bva-case-citation-dataset ; code available\n  here: https://github.com/TUMLegalTech/bva-citation-prediction", "journal-ref": null, "doi": "10.1145/3462757.3466066", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lawyers and judges spend a large amount of time researching the proper legal\nauthority to cite while drafting decisions. In this paper, we develop a\ncitation recommendation tool that can help improve efficiency in the process of\nopinion drafting. We train four types of machine learning models, including a\ncitation-list based method (collaborative filtering) and three context-based\nmethods (text similarity, BiLSTM and RoBERTa classifiers). Our experiments show\nthat leveraging local textual context improves recommendation, and that deep\nneural models achieve decent performance. We show that non-deep text-based\nmethods benefit from access to structured case metadata, but deep models only\nbenefit from such access when predicting from context of insufficient length.\nWe also find that, even after extensive training, RoBERTa does not outperform a\nrecurrent neural model, despite its benefits of pretraining. Our behavior\nanalysis of the RoBERTa model further shows that predictive performance is\nstable across time and citation classes.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 23:23:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huang", "Zihan", ""], ["Low", "Charles", ""], ["Teng", "Mengqiu", ""], ["Zhang", "Hongyi", ""], ["Ho", "Daniel E.", ""], ["Krass", "Mark S.", ""], ["Grabmair", "Matthias", ""]]}, {"id": "2106.10879", "submitter": "Yifan Wang", "authors": "Yifan Wang, Suyao Tang, Yuntong Lei, Weiping Song, Sheng Wang, Ming\n  Zhang", "title": "DisenHAN: Disentangled Heterogeneous Graph Attention Network for\n  Recommendation", "comments": "Accepted at CIKM2020", "journal-ref": null, "doi": "10.1145/3340531.3411996", "report-no": null, "categories": "cs.IR cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous information network has been widely used to alleviate sparsity\nand cold start problems in recommender systems since it can model rich context\ninformation in user-item interactions. Graph neural network is able to encode\nthis rich context information through propagation on the graph. However,\nexisting heterogeneous graph neural networks neglect entanglement of the latent\nfactors stemming from different aspects. Moreover, meta paths in existing\napproaches are simplified as connecting paths or side information between node\npairs, overlooking the rich semantic information in the paths. In this paper,\nwe propose a novel disentangled heterogeneous graph attention network DisenHAN\nfor top-$N$ recommendation, which learns disentangled user/item representations\nfrom different aspects in a heterogeneous information network. In particular,\nwe use meta relations to decompose high-order connectivity between node pairs\nand propose a disentangled embedding propagation layer which can iteratively\nidentify the major aspect of meta relations. Our model aggregates corresponding\naspect features from each meta relation for the target user/item. With\ndifferent layers of embedding propagation, DisenHAN is able to explicitly\ncapture the collaborative filtering effect semantically. Extensive experiments\non three real-world datasets show that DisenHAN consistently outperforms\nstate-of-the-art approaches. We further demonstrate the effectiveness and\ninterpretability of the learned disentangled representations via insightful\ncase studies and visualization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:26:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Yifan", ""], ["Tang", "Suyao", ""], ["Lei", "Yuntong", ""], ["Song", "Weiping", ""], ["Wang", "Sheng", ""], ["Zhang", "Ming", ""]]}, {"id": "2106.10898", "submitter": "Shenghao Xu", "authors": "Shenghao Xu", "title": "BanditMF: Multi-Armed Bandit Based Matrix Factorization Recommender\n  System", "comments": "MSc dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandits (MAB) provide a principled online learning approach to\nattain the balance between exploration and exploitation. Due to the superior\nperformance and low feedback learning without the learning to act in multiple\nsituations, Multi-armed Bandits drawing widespread attention in applications\nranging such as recommender systems. Likewise, within the recommender system,\ncollaborative filtering (CF) is arguably the earliest and most influential\nmethod in the recommender system. Crucially, new users and an ever-changing\npool of recommended items are the challenges that recommender systems need to\naddress. For collaborative filtering, the classical method is training the\nmodel offline, then perform the online testing, but this approach can no longer\nhandle the dynamic changes in user preferences which is the so-called cold\nstart. So how to effectively recommend items to users in the absence of\neffective information? To address the aforementioned problems, a multi-armed\nbandit based collaborative filtering recommender system has been proposed,\nnamed BanditMF. BanditMF is designed to address two challenges in the\nmulti-armed bandits algorithm and collaborative filtering: (1) how to solve the\ncold start problem for collaborative filtering under the condition of scarcity\nof valid information, (2) how to solve the sub-optimal problem of bandit\nalgorithms in strong social relations domains caused by independently\nestimating unknown parameters associated with each user and ignoring\ncorrelations between users.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 07:35:39 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 07:30:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Xu", "Shenghao", ""]]}, {"id": "2106.10928", "submitter": "Nawshad Farruque", "authors": "Nawshad Farruque, Randy Goebel, Osmar Zaiane, Sudhakar Sivapalan", "title": "STEP-EZ: Syntax Tree guided semantic ExPlanation for Explainable\n  Zero-shot modeling of clinical depression symptoms from text", "comments": "Fixed an algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on exploring various approaches of Zero-Shot Learning (ZSL) and\ntheir explainability for a challenging yet important supervised learning task\nnotorious for training data scarcity, i.e. Depression Symptoms Detection (DSD)\nfrom text. We start with a comprehensive synthesis of different components of\nour ZSL modeling and analysis of our ground truth samples and Depression\nsymptom clues curation process with the help of a practicing clinician. We next\nanalyze the accuracy of various state-of-the-art ZSL models and their potential\nenhancements for our task. Further, we sketch a framework for the use of ZSL\nfor hierarchical text-based explanation mechanism, which we call, Syntax\nTree-Guided Semantic Explanation (STEP). Finally, we summarize experiments from\nwhich we conclude that we can use ZSL models and achieve reasonable accuracy\nand explainability, measured by a proposed Explainability Index (EI). This work\nis, to our knowledge, the first work to exhaustively explore the efficacy of\nZSL models for DSD task, both in terms of accuracy and explainability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:57:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 07:06:06 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Farruque", "Nawshad", ""], ["Goebel", "Randy", ""], ["Zaiane", "Osmar", ""], ["Sivapalan", "Sudhakar", ""]]}, {"id": "2106.10977", "submitter": "Emir Demirel", "authors": "Emir Demirel, Sven Ahlback, Simon Dixon", "title": "Computational Pronunciation Analysis in Sung Utterances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent automatic lyrics transcription (ALT) approaches focus on building\nstronger acoustic models or in-domain language models, while the pronunciation\naspect is seldom touched upon. This paper applies a novel computational\nanalysis on the pronunciation variances in sung utterances and further proposes\na new pronunciation model adapted for singing. The singing-adapted model is\ntested on multiple public datasets via word recognition experiments. It\nperforms better than the standard speech dictionary in all settings reporting\nthe best results on ALT in a capella recordings using n-gram language models.\nFor reproducibility, we share the sentence-level annotations used in testing,\nproviding a new benchmark evaluation set for ALT.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:53:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Demirel", "Emir", ""], ["Ahlback", "Sven", ""], ["Dixon", "Simon", ""]]}, {"id": "2106.11096", "submitter": "Yang Deng", "authors": "Yang Deng, Wenxuan Zhang, Wai Lam", "title": "Learning to Rank Question Answer Pairs with Bilateral Contrastive Data\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel and easy-to-apply data augmentation\nstrategy, namely Bilateral Generation (BiG), with a contrastive training\nobjective for improving the performance of ranking question answer pairs with\nexisting labeled data. In specific, we synthesize pseudo-positive QA pairs in\ncontrast to the original negative QA pairs with two pre-trained generation\nmodels, one for question generation, the other for answer generation, which are\nfine-tuned on the limited positive QA pairs from the original dataset. With the\naugmented dataset, we design a contrastive training objective for learning to\nrank question answer pairs. Experimental results on three benchmark datasets,\nnamely TREC-QA, WikiQA, and ANTIQUE, show that our method significantly\nimproves the performance of ranking models by making full use of existing\nlabeled data and can be easily applied to different ranking models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:29:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Deng", "Yang", ""], ["Zhang", "Wenxuan", ""], ["Lam", "Wai", ""]]}, {"id": "2106.11218", "submitter": "Martin Tegner", "authors": "Gustav Hertz, Sandhya Sachidanandan, Bal\\'azs T\\'oth, Emil S.\n  J{\\o}rgensen and Martin Tegn\\'er", "title": "Data Optimisation for a Deep Learning Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advocates privacy preserving requirements on collection of user\ndata for recommender systems. The purpose of our study is twofold. First, we\nask if restrictions on data collection will hurt test quality of RNN-based\nrecommendations. We study how validation performance depends on the available\namount of training data. We use a combination of top-K accuracy, catalog\ncoverage and novelty for this purpose, since good recommendations for the user\nis not necessarily captured by a traditional accuracy metric. Second, we ask if\nwe can improve the quality under minimal data by using secondary data sources.\nWe propose knowledge transfer for this purpose and construct a representation\nto measure similarities between purchase behaviour in data. This to make\nqualified judgements of which source domain will contribute the most. Our\nresults show that (i) there is a saturation in test performance when training\nsize is increased above a critical point. We also discuss the interplay between\ndifferent performance metrics, and properties of data. Moreover, we demonstrate\nthat (ii) our representation is meaningful for measuring purchase behaviour. In\nparticular, results show that we can leverage secondary data to improve\nvalidation performance if we select a relevant source domain according to our\nsimilarly measure.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:05:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hertz", "Gustav", ""], ["Sachidanandan", "Sandhya", ""], ["T\u00f3th", "Bal\u00e1zs", ""], ["J\u00f8rgensen", "Emil S.", ""], ["Tegn\u00e9r", "Martin", ""]]}, {"id": "2106.11251", "submitter": "Xiao Wang", "authors": "Xiao Wang, Craig Macdonald, Nicola Tonellotto, Iadh Ounis", "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval", "comments": "10 pages", "journal-ref": "Proceedings of ICTIR 2021", "doi": "10.1145/3471158.3472250", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users' initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval -- through the use\nof neural contextual language models such as BERT for analysing the documents'\nand queries' contents and computing their relevance scores -- has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT's [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) -- while ensuring\nthat these embeddings discriminate among passages (based on IDF) -- which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 16:49:03 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 14:02:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wang", "Xiao", ""], ["Macdonald", "Craig", ""], ["Tonellotto", "Nicola", ""], ["Ounis", "Iadh", ""]]}, {"id": "2106.11403", "submitter": "Yefeng Wang", "authors": "Yefeng Wang, Yunpeng Zhao, Jiang Bian, Rui Zhang", "title": "Deep Learning Models in Detection of Dietary Supplement Adverse Event\n  Signals from Twitter", "comments": "1 Figure, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The objective of this study is to develop a deep learning pipeline\nto detect signals on dietary supplement-related adverse events (DS AEs) from\nTwitter. Material and Methods: We obtained 247,807 tweets ranging from 2012 to\n2018 that mentioned both DS and AE. We annotated biomedical entities and\nrelations on 2,000 randomly selected tweets. For the concept extraction task,\nwe compared the performance of traditional word embeddings with SVM, CRF and\nLSTM-CRF classifiers to BERT models. For the relation extraction task, we\ncompared GloVe vectors with CNN classifiers to BERT models. We chose the best\nperforming models in each task to assemble an end-to-end deep learning pipeline\nto detect DS AE signals and compared the results to the known DS AEs from a DS\nknowledge base (i.e., iDISK). Results: In both tasks, the BERT-based models\noutperformed traditional word embeddings. The best performing concept\nextraction model is the BioBERT model that can identify supplement, symptom,\nand body organ entities with F1-scores of 0.8646, 0.8497, and 0.7104,\nrespectively. The best performing relation extraction model is the BERT model\nthat can identify purpose and AE relations with F1-scores of 0.8335 and 0.7538,\nrespectively. The end-to-end pipeline was able to extract DS indication and DS\nAEs with an F1-score of 0.7459 and 0,7414, respectively. Comparing to the\niDISK, we could find both known and novel DS-AEs. Conclusion: We have\ndemonstrated the feasibility of detecting DS AE signals from Twitter with a\nBioBERT-based deep learning pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:35:01 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Yefeng", ""], ["Zhao", "Yunpeng", ""], ["Bian", "Jiang", ""], ["Zhang", "Rui", ""]]}, {"id": "2106.11481", "submitter": "Sourav Garg", "authors": "Sourav Garg and Michael Milford", "title": "SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for\n  Day-Night Place Recognition", "comments": "Accepted to CVPR 2021 Workshop on 3D Vision and Robotics (3DVR).\n  https://sites.google.com/view/cvpr2021-3d-vision-robotics/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Place Recognition is a crucial capability for mobile robot localization and\nnavigation. Image-based or Visual Place Recognition (VPR) is a challenging\nproblem as scene appearance and camera viewpoint can change significantly when\nplaces are revisited. Recent VPR methods based on ``sequential\nrepresentations'' have shown promising results as compared to traditional\nsequence score aggregation or single image based techniques. In parallel to\nthese endeavors, 3D point clouds based place recognition is also being explored\nfollowing the advances in deep learning based point cloud processing. However,\na key question remains: is an explicit 3D structure based place representation\nalways superior to an implicit ``spatial'' representation based on sequence of\nRGB images which can inherently learn scene structure. In this extended\nabstract, we attempt to compare these two types of methods by considering a\nsimilar ``metric span'' to represent places. We compare a 3D point cloud based\nmethod (PointNetVLAD) with image sequence based methods (SeqNet and others) and\nshowcase that image sequence based techniques approach, and can even surpass,\nthe performance achieved by point cloud based methods for a given metric span.\nThese performance variations can be attributed to differences in data richness\nof input sensors as well as data accumulation strategies for a mobile robot.\nWhile a perfect apple-to-apple comparison may not be feasible for these two\ndifferent modalities, the presented comparison takes a step in the direction of\nanswering deeper questions regarding spatial representations, relevant to\nseveral applications like Autonomous Driving and Augmented/Virtual Reality.\nSource code available publicly https://github.com/oravus/seqNet.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:05:32 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Garg", "Sourav", ""], ["Milford", "Michael", ""]]}, {"id": "2106.11517", "submitter": "Rivindu Weerasekera", "authors": "Shamane Siriwardhana, Rivindu Weerasekera, Elliott Wen, Suranga\n  Nanayakkara", "title": "Fine-tune the Entire RAG Architecture (including DPR retriever) for\n  Question-Answering", "comments": "for associated code, see\n  https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag-end2end-retriever", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we illustrate how to fine-tune the entire Retrieval Augment\nGeneration (RAG) architecture in an end-to-end manner. We highlighted the main\nengineering challenges that needed to be addressed to achieve this objective.\nWe also compare how end-to-end RAG architecture outperforms the original RAG\narchitecture for the task of question answering. We have open-sourced our\nimplementation in the HuggingFace Transformers library.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:17:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Siriwardhana", "Shamane", ""], ["Weerasekera", "Rivindu", ""], ["Wen", "Elliott", ""], ["Nanayakkara", "Suranga", ""]]}, {"id": "2106.11534", "submitter": "Yinyu Jin Ms.", "authors": "Yinyu Jin, Sha Yuan, Zhou Shao, Wendy Hall and Jie Tang", "title": "Turing Award elites revisited: patterns of productivity, collaboration,\n  authorship and impact", "comments": null, "journal-ref": "Scientometrics 126, 2329-2348 (2021)", "doi": "10.1007/s11192-020-03860-4", "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turing Award is recognized as the most influential and prestigious award\nin the field of computer science(CS). With the rise of the science of science\n(SciSci), a large amount of bibliographic data has been analyzed in an attempt\nto understand the hidden mechanism of scientific evolution. These include the\nanalysis of the Nobel Prize, including physics, chemistry, medicine, etc. In\nthis article, we extract and analyze the data of 72 Turing Award laureates from\nthe complete bibliographic data, fill the gap in the lack of Turing Award\nanalysis, and discover the development characteristics of computer science as\nan independent discipline. First, we show most Turing Award laureates have\nlong-term and high-quality educational backgrounds, and more than 61% of them\nhave a degree in mathematics, which indicates that mathematics has played a\nsignificant role in the development of computer science. Secondly, the data\nshows that not all scholars have high productivity and high h-index; that is,\nthe number of publications and h-index is not the leading indicator for\nevaluating the Turing Award. Third, the average age of awardees has increased\nfrom 40 to around 70 in recent years. This may be because new breakthroughs\ntake longer, and some new technologies need time to prove their influence.\nBesides, we have also found that in the past ten years, international\ncollaboration has experienced explosive growth, showing a new paradigm in the\nform of collaboration. It is also worth noting that in recent years, the\nemergence of female winners has also been eye-catching. Finally, by analyzing\nthe personal publication records, we find that many people are more likely to\npublish high-impact articles during their high-yield periods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:20:30 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Jin", "Yinyu", ""], ["Yuan", "Sha", ""], ["Shao", "Zhou", ""], ["Hall", "Wendy", ""], ["Tang", "Jie", ""]]}, {"id": "2106.11846", "submitter": "Austin Wright", "authors": "Austin P Wright, Caleb Ziems, Haekyu Park, Jon Saad-Falcon, Duen Horng\n  Chau, Diyi Yang, Maria Tomprou", "title": "Quantifying the Impact of Human Capital, Job History, and Language\n  Factors on Job Seniority with a Large-scale Analysis of Resumes", "comments": "9 Pages, 5 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.IR q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As job markets worldwide have become more competitive and applicant selection\ncriteria have become more opaque, and different (and sometimes contradictory)\ninformation and advice is available for job seekers wishing to progress in\ntheir careers, it has never been more difficult to determine which factors in a\nr\\'esum\\'e most effectively help career progression. In this work we present a\nnovel, large scale dataset of over half a million r\\'esum\\'es with preliminary\nanalysis to begin to answer empirically which factors help or hurt people\nwishing to transition to more senior roles as they progress in their career. We\nfind that previous experience forms the most important factor, outweighing\nother aspects of human capital, and find which language factors in a r\\'esum\\'e\nhave significant effects. This lays the groundwork for future inquiry in career\ntrajectories using large scale data analysis and natural language processing\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:12:59 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wright", "Austin P", ""], ["Ziems", "Caleb", ""], ["Park", "Haekyu", ""], ["Saad-Falcon", "Jon", ""], ["Chau", "Duen Horng", ""], ["Yang", "Diyi", ""], ["Tomprou", "Maria", ""]]}, {"id": "2106.12085", "submitter": "Marina Delianidi", "authors": "Marina Delianidi, Michail Salampasis, Konstantinos Diamantaras,\n  Theodosios Siomos, Alkiviadis Katsalis, Iphigenia Karaveli", "title": "A Graph-based Method for Session-based Recommendations", "comments": "Preprint version of the paper, the original paper is published on ACM\n  DL. 6 pages, 1 figure, 1 table", "journal-ref": null, "doi": "10.1145/3437120.3437321", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a graph-based approach for the data management tasks and the\nefficient operation of a system for session-based next-item recommendations.\nThe proposed method can collect data continuously and incrementally from an\necommerce web site, thus seemingly prepare the necessary data infrastructure\nfor the recommendation algorithm to operate without any excessive training\nphase. Our work aims at developing a recommender method that represents a\nbalance between data processing and management efficiency requirements and the\neffectiveness of the recommendations produced. We use the Neo4j graph database\nto implement a prototype of such a system. Furthermore, we use an industry\ndataset corresponding to a typical e-commerce session-based scenario, and we\nreport on experiments using our graph-based approach and other state-of-the-art\nmachine learning and deep learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:33:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Delianidi", "Marina", ""], ["Salampasis", "Michail", ""], ["Diamantaras", "Konstantinos", ""], ["Siomos", "Theodosios", ""], ["Katsalis", "Alkiviadis", ""], ["Karaveli", "Iphigenia", ""]]}, {"id": "2106.12120", "submitter": "Muyang Ma", "authors": "Muyang Ma, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang, Jun\n  Ma, Maarten de Rijke", "title": "Improving Transformer-based Sequential Recommenders through Preference\n  Editing", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the key challenges in Sequential Recommendation (SR) is how to extract\nand represent user preferences. Traditional SR methods rely on the next item as\nthe supervision signal to guide preference extraction and representation. We\npropose a novel learning strategy, named preference editing. The idea is to\nforce the SR model to discriminate the common and unique preferences in\ndifferent sequences of interactions between users and the recommender system.\nBy doing so, the SR model is able to learn how to identify common and unique\nuser preferences, and thereby do better user preference extraction and\nrepresentation. We propose a transformer based SR model, named MrTransformer\n(Multi-preference Transformer), that concatenates some special tokens in front\nof the sequence to represent multiple user preferences and makes sure they\ncapture different aspects through a preference coverage mechanism. Then, we\ndevise a preference editing-based self-supervised learning mechanism for\ntraining MrTransformer which contains two main operations: preference\nseparation and preference recombination. The former separates the common and\nunique user preferences for a given pair of sequences. The latter swaps the\ncommon preferences to obtain recombined user preferences for each sequence.\nBased on the preference separation and preference recombination operations, we\ndefine two types of SSL loss that require that the recombined preferences are\nsimilar to the original ones, and the common preferences are close to each\nother.\n  We carry out extensive experiments on two benchmark datasets. MrTransformer\nwith preference editing significantly outperforms state-of-the-art SR methods\nin terms of Recall, MRR and NDCG. We find that long sequences whose user\npreferences are harder to extract and represent benefit most from preference\nediting.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 01:43:42 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ma", "Muyang", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["Liang", "Huasheng", ""], ["Ma", "Jun", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2106.12320", "submitter": "Zeyd Boukhers", "authors": "Zeyd Boukhers, Philipp Mayr, Silvio Peroni", "title": "BiblioDAP: The 1st Workshop on Bibliographic Data Analysis and\n  Processing", "comments": "This workshop will be held in conjunction with KDD' 2021", "journal-ref": null, "doi": "10.1145/3447548.346", "report-no": null, "categories": "cs.DL cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic processing of bibliographic data becomes very important in digital\nlibraries, data science and machine learning due to its importance in keeping\npace with the significant increase of published papers every year from one side\nand to the inherent challenges from the other side. This processing has several\naspects including but not limited to I) Automatic extraction of references from\nPDF documents, II) Building an accurate citation graph, III) Author name\ndisambiguation, etc. Bibliographic data is heterogeneous by nature and occurs\nin both structured (e.g. citation graph) and unstructured (e.g. publications)\nformats. Therefore, it requires data science and machine learning techniques to\nbe processed and analysed. Here we introduce BiblioDAP'21: The 1st Workshop on\nBibliographic Data Analysis and Processing.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:33:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Boukhers", "Zeyd", ""], ["Mayr", "Philipp", ""], ["Peroni", "Silvio", ""]]}, {"id": "2106.12340", "submitter": "Andreea Iana", "authors": "Andreea Iana, Heiko Paulheim", "title": "GraphConfRec: A Graph Neural Network-Based Conference Recommender System", "comments": "Accepted at the Joint Conference on Digital Libraries (JCDL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's academic publishing model, especially in Computer Science,\nconferences commonly constitute the main platforms for releasing the latest\npeer-reviewed advancements in their respective fields. However, choosing a\nsuitable academic venue for publishing one's research can represent a\nchallenging task considering the plethora of available conferences,\nparticularly for those at the start of their academic careers, or for those\nseeking to publish outside of their usual domain. In this paper, we propose\nGraphConfRec, a conference recommender system which combines SciGraph and graph\nneural networks, to infer suggestions based not only on title and abstract, but\nalso on co-authorship and citation relationships. GraphConfRec achieves a\nrecall@10 of up to 0.580 and a MAP of up to 0.336 with a graph attention\nnetwork-based recommendation model. A user study with 25 subjects supports the\npositive results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 12:10:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Iana", "Andreea", ""], ["Paulheim", "Heiko", ""]]}, {"id": "2106.12460", "submitter": "Jurek Leonhardt", "authors": "Jurek Leonhardt, Koustav Rudra, Avishek Anand", "title": "Learnt Sparsity for Effective and Interpretable Document Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models for the ad-hoc retrieval of documents and passages\nhave recently shown impressive improvements due to better language\nunderstanding using large pre-trained language models. However, these\nover-parameterized models are inherently non-interpretable and do not provide\nany information on the parts of the documents that were used to arrive at a\ncertain prediction.\n  In this paper we introduce the select and rank paradigm for document ranking,\nwhere interpretability is explicitly ensured when scoring longer documents.\nSpecifically, we first select sentences in a document based on the input query\nand then predict the query-document score based only on the selected sentences,\nacting as an explanation. We treat sentence selection as a latent variable\ntrained jointly with the ranker from the final output. We conduct extensive\nexperiments to demonstrate that our inherently interpretable select-and-rank\napproach is competitive in comparison to other state-of-the-art methods and\nsometimes even outperforms them. This is due to our novel end-to-end training\napproach based on weighted reservoir sampling that manages to train the\nselector despite the stochastic sentence selection. We also show that our\nsentence selection approach can be used to provide explanations for models that\noperate on only parts of the document, such as BERT.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:13:52 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Leonhardt", "Jurek", ""], ["Rudra", "Koustav", ""], ["Anand", "Avishek", ""]]}, {"id": "2106.12621", "submitter": "Hayden Helm", "authors": "Hayden S. Helm and Marah Abdin and Benjamin D. Pedigo and Shweti\n  Mahajan and Vince Lyzinski and Youngser Park and Amitabh Basu and\n  Piali~Choudhury and Christopher M. White and Weiwei Yang and Carey E. Priebe", "title": "Leveraging semantically similar queries for ranking via combining\n  representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:36:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Helm", "Hayden S.", ""], ["Abdin", "Marah", ""], ["Pedigo", "Benjamin D.", ""], ["Mahajan", "Shweti", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Basu", "Amitabh", ""], ["Piali~Choudhury", "", ""], ["White", "Christopher M.", ""], ["Yang", "Weiwei", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2106.12622", "submitter": "Wenshuo Guo", "authors": "Wenshuo Guo, Karl Krauth, Michael I. Jordan, Nikhil Garg", "title": "The Stereotyping Problem in Collaboratively Filtered Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems -- and especially matrix factorization-based\ncollaborative filtering algorithms -- play a crucial role in mediating our\naccess to online information. We show that such algorithms induce a particular\nkind of stereotyping: if preferences for a \\textit{set} of items are\nanti-correlated in the general user population, then those items may not be\nrecommended together to a user, regardless of that user's preferences and\nratings history. First, we introduce a notion of \\textit{joint accessibility},\nwhich measures the extent to which a set of items can jointly be accessed by\nusers. We then study joint accessibility under the standard factorization-based\ncollaborative filtering framework, and provide theoretical necessary and\nsufficient conditions when joint accessibility is violated. Moreover, we show\nthat these conditions can easily be violated when the users are represented by\na single feature vector. To improve joint accessibility, we further propose an\nalternative modelling fix, which is designed to capture the diverse multiple\ninterests of each user using a multi-vector representation. We conduct\nextensive experiments on real and simulated datasets, demonstrating the\nstereotyping problem with standard single-vector matrix factorization models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:37:47 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Guo", "Wenshuo", ""], ["Krauth", "Karl", ""], ["Jordan", "Michael I.", ""], ["Garg", "Nikhil", ""]]}, {"id": "2106.12657", "submitter": "Wei-Cheng Chang", "authors": "Wei-Cheng Chang, Daniel Jiang, Hsiang-Fu Yu, Choon-Hui Teo, Jiong\n  Zhang, Kai Zhong, Kedarnath Kolluri, Qie Hu, Nikhil Shandilya, Vyacheslav\n  Ievgrafov, Japinder Singh, Inderjit S. Dhillon", "title": "Extreme Multi-label Learning for Semantic Matching in Product Search", "comments": "Accepted in KDD 2021 Applied Data Science Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of semantic matching in product search: given a\ncustomer query, retrieve all semantically related products from a huge catalog\nof size 100 million, or more. Because of large catalog spaces and real-time\nlatency constraints, semantic matching algorithms not only desire high recall\nbut also need to have low latency. Conventional lexical matching approaches\n(e.g., Okapi-BM25) exploit inverted indices to achieve fast inference time, but\nfail to capture behavioral signals between queries and products. In contrast,\nembedding-based models learn semantic representations from customer behavior\ndata, but the performance is often limited by shallow neural encoders due to\nlatency constraints. Semantic product search can be viewed as an eXtreme\nMulti-label Classification (XMC) problem, where customer queries are input\ninstances and products are output labels. In this paper, we aim to improve\nsemantic product search by using tree-based XMC models where inference time\ncomplexity is logarithmic in the number of products. We consider hierarchical\nlinear models with n-gram features for fast real-time inference.\nQuantitatively, our method maintains a low latency of 1.25 milliseconds per\nquery and achieves a 65% improvement of Recall@100 (60.9% v.s. 36.8%) over a\ncompeting embedding-based DSSM model. Our model is robust to weight pruning\nwith varying thresholds, which can flexibly meet different system requirements\nfor online deployments. Qualitatively, our method can retrieve products that\nare complementary to existing product search system and add diversity to the\nmatch set.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 21:16:52 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chang", "Wei-Cheng", ""], ["Jiang", "Daniel", ""], ["Yu", "Hsiang-Fu", ""], ["Teo", "Choon-Hui", ""], ["Zhang", "Jiong", ""], ["Zhong", "Kai", ""], ["Kolluri", "Kedarnath", ""], ["Hu", "Qie", ""], ["Shandilya", "Nikhil", ""], ["Ievgrafov", "Vyacheslav", ""], ["Singh", "Japinder", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "2106.12741", "submitter": "Dalton Schutte", "authors": "Dalton Schutte, Jake Vasilakes, Anu Bompelli, Yuqi Zhou, Marcelo\n  Fiszman, Hua Xu, Halil Kilicoglu, Jeffrey R. Bishop, Terrence Adam, Rui Zhang", "title": "Discovering novel drug-supplement interactions using a dietary\n  supplements knowledge graph generated from the biomedical literature", "comments": "14 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OBJECTIVE: Leverage existing biomedical NLP tools and DS domain terminology\nto produce a novel and comprehensive knowledge graph containing dietary\nsupplement (DS) information for discovering interactions between DS and drugs,\nor Drug-Supplement Interactions (DSI). MATERIALS AND METHODS: We created\nSemRepDS (an extension of SemRep), capable of extracting semantic relations\nfrom abstracts by leveraging a DS-specific terminology (iDISK) containing\n28,884 DS terms not found in the UMLS. PubMed abstracts were processed using\nSemRepDS to generate semantic relations, which were then filtered using a\nPubMedBERT-based model to remove incorrect relations before generating our\nknowledge graph (SuppKG). Two pathways are used to identify potential DS-Drug\ninteractions which are then evaluated by medical professionals for mechanistic\nplausibility. RESULTS: Comparison analysis found that SemRepDS returned 206.9%\nmore DS relations and 158.5% more DS entities than SemRep. The fine-tuned BERT\nmodel obtained an F1 score of 0.8605 and removed 43.86% of the relations,\nimproving the precision of the relations by 26.4% compared to pre-filtering.\nSuppKG consists of 2,928 DS-specific nodes. Manual review of findings\nidentified 44 (88%) proposed DS-Gene-Drug and 32 (64%) proposed\nDS-Gene1-Function-Gene2-Drug pathways to be mechanistically plausible.\nDISCUSSION: The additional relations extracted using SemRepDS generated SuppKG\nthat was used to find plausible DSI not found in the current literature. By the\nnature of the SuppKG, these interactions are unlikely to have been found using\nSemRep without the expanded DS terminology. CONCLUSION: We successfully extend\nSemRep to include DS information and produce SuppKG which can be used to find\npotential DS-Drug interactions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 02:57:24 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Schutte", "Dalton", ""], ["Vasilakes", "Jake", ""], ["Bompelli", "Anu", ""], ["Zhou", "Yuqi", ""], ["Fiszman", "Marcelo", ""], ["Xu", "Hua", ""], ["Kilicoglu", "Halil", ""], ["Bishop", "Jeffrey R.", ""], ["Adam", "Terrence", ""], ["Zhang", "Rui", ""]]}, {"id": "2106.12744", "submitter": "Zhiyuan Chen Dr", "authors": "Jia Wei Chong, Zhiyuan Chen and Mei Shin Oh", "title": "An Automated Knowledge Mining and Document Classification System with\n  Multi-model Transfer Learning", "comments": "This paper has been submitted to journal of System and Management\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Service manual documents are crucial to the engineering company as they\nprovide guidelines and knowledge to service engineers. However, it has become\ninconvenient and inefficient for service engineers to retrieve specific\nknowledge from documents due to the complexity of resources. In this research,\nwe propose an automated knowledge mining and document classification system\nwith novel multi-model transfer learning approaches. Particularly, the\nclassification performance of the system has been improved with three effective\ntechniques: fine-tuning, pruning, and multi-model method. The fine-tuning\ntechnique optimizes a pre-trained BERT model by adding a feed-forward neural\nnetwork layer and the pruning technique is used to retrain the BERT model with\nnew data. The multi-model method initializes and trains multiple BERT models to\novercome the randomness of data ordering during the fine-tuning process. In the\nfirst iteration of the training process, multiple BERT models are being trained\nsimultaneously. The best model is then selected for the next phase of the\ntraining process with another two iterations and the training processes for\nother BERT models will be terminated. The performance of the proposed system\nhas been evaluated by comparing with two robust baseline methods, BERT and\nBERT-CNN. Experimental results on a widely used Corpus of Linguistic\nAcceptability (CoLA) dataset have shown that the proposed techniques perform\nbetter than these baseline methods in terms of accuracy and MCC score.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:03:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Chong", "Jia Wei", ""], ["Chen", "Zhiyuan", ""], ["Oh", "Mei Shin", ""]]}, {"id": "2106.12765", "submitter": "Yang Lu", "authors": "Yang Lu, Qifan Chen and Simon Poon", "title": "A Novel Approach to Discover Switch Behaviours in Process Mining", "comments": "ICPM Workshop 2020", "journal-ref": "Process Mining Workshops. ICPM 2020. Lecture Notes in Business\n  Information Processing, vol 406. Springer, Cham", "doi": "10.1007/978-3-030-72693-5_5", "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a relatively new subject which builds a bridge between\nprocess modelling and data mining. An exclusive choice in a process model\nusually splits the process into different branches. However, in some processes,\nit is possible to switch from one branch to another. The inductive miner\nguarantees to return sound process models, but fails to return a precise model\nwhen there are switch behaviours between different exclusive choice branches\ndue to the limitation of process trees. In this paper, we present a novel\nextension to the process tree model to support switch behaviours between\ndifferent branches of the exclusive choice operator and propose a novel\nextension to the inductive miner to discover sound process models with switch\nbehaviours. The proposed discovery technique utilizes the theory of a previous\nstudy to detect possible switch behaviours. We apply both artificial and\npublicly-available datasets to evaluate our approach. Our results show that our\napproach can improve the precision of discovered models by 36% while\nmaintaining high fitness values compared to the original inductive miner.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:25:28 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Yang", ""], ["Chen", "Qifan", ""], ["Poon", "Simon", ""]]}, {"id": "2106.12857", "submitter": "Luigi Asprino", "authors": "Luigi Asprino, Christian Colonna, Misael Mongiov\\`i, Margherita\n  Porena, Valentina Presutti", "title": "Pattern-based Visualization of Knowledge Graphs", "comments": "16 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to knowledge graph visualization based on\nontology design patterns. This approach relies on OPLa (Ontology Pattern\nLanguage) annotations and on a catalogue of visual frames, which are associated\nwith foundational ontology design patterns. We demonstrate that this approach\nsignificantly reduces the cognitive load required to users for visualizing and\ninterpreting a knowledge graph and guides the user in exploring it through\nmeaningful thematic paths provided by ontology patterns.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:43:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Asprino", "Luigi", ""], ["Colonna", "Christian", ""], ["Mongiov\u00ec", "Misael", ""], ["Porena", "Margherita", ""], ["Presutti", "Valentina", ""]]}, {"id": "2106.12875", "submitter": "Angelo Salatino Dr", "authors": "Angelo Salatino and Andrea Mannocci and Francesco Osborne", "title": "Detection, Analysis, and Prediction of Research Topics with Scientific\n  Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysing research trends and predicting their impact on academia and\nindustry is crucial to gain a deeper understanding of the advances in a\nresearch field and to inform critical decisions about research funding and\ntechnology adoption. In the last years, we saw the emergence of several\npublicly-available and large-scale Scientific Knowledge Graphs fostering the\ndevelopment of many data-driven approaches for performing quantitative analyses\nof research trends. This chapter presents an innovative framework for\ndetecting, analysing, and forecasting research topics based on a large-scale\nknowledge graph characterising research articles according to the research\ntopics from the Computer Science Ontology. We discuss the advantages of a\nsolution based on a formal representation of topics and describe how it was\napplied to produce bibliometric studies and innovative tools for analysing and\npredicting research dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:17:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Salatino", "Angelo", ""], ["Mannocci", "Andrea", ""], ["Osborne", "Francesco", ""]]}, {"id": "2106.12970", "submitter": "Badal Soni", "authors": "Badal Soni, Debangan Thakuria, Nilutpal Nath, Navarun Das and\n  Bhaskarananda Boro", "title": "RikoNet: A Novel Anime Recommendation Engine", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anime is quite well-received today, especially among the younger generations.\nWith many genres of available shows, more and more people are increasingly\ngetting attracted to this niche section of the entertainment industry. As anime\nhas recently garnered mainstream attention, we have insufficient information\nregarding users' penchant and watching habits. Therefore, it is an uphill task\nto build a recommendation engine for this relatively obscure entertainment\nmedium. In this attempt, we have built a novel hybrid recommendation system\nthat could act both as a recommendation system and as a means of exploring new\nanime genres and titles. We have analyzed the general trends in this field and\nthe users' watching habits for coming up with our efficacious solution. Our\nsolution employs deep autoencoders for the tasks of predicting ratings and\ngenerating embeddings. Following this, we formed clusters using the embeddings\nof the anime titles. These clusters form the search space for anime with\nsimilarities and are used to find anime similar to the ones liked and disliked\nby the user. This method, combined with the predicted ratings, forms the novel\nhybrid filter. In this article, we have demonstrated this idea and compared the\nperformance of our implemented model with the existing state-of-the-art\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:39:48 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Soni", "Badal", ""], ["Thakuria", "Debangan", ""], ["Nath", "Nilutpal", ""], ["Das", "Navarun", ""], ["Boro", "Bhaskarananda", ""]]}, {"id": "2106.13266", "submitter": "Giorgos Kordopatis-Zilos", "authors": "Giorgos Kordopatis-Zilos, Christos Tzelepis, Symeon Papadopoulos,\n  Ioannis Kompatsiaris, Ioannis Patras", "title": "DnS: Distill-and-Select for Efficient and Accurate Video Indexing and\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of high performance and computationally\nefficient content-based video retrieval in large-scale datasets. Current\nmethods typically propose either: (i) fine-grained approaches employing\nspatio-temporal representations and similarity calculations, achieving high\nperformance at a high computational cost or (ii) coarse-grained approaches\nrepresenting/indexing videos as global vectors, where the spatio-temporal\nstructure is lost, providing low performance but also having low computational\ncost. In this work, we propose a Knowledge Distillation framework, which we\ncall Distill-and-Select (DnS), that starting from a well-performing\nfine-grained Teacher Network learns: a) Student Networks at different retrieval\nperformance and computational efficiency trade-offs and b) a Selection Network\nthat at test time rapidly directs samples to the appropriate student to\nmaintain both high retrieval performance and high computational efficiency. We\ntrain several students with different architectures and arrive at different\ntrade-offs of performance and efficiency, i.e., speed and storage requirements,\nincluding fine-grained students that store index videos using binary\nrepresentations. Importantly, the proposed scheme allows Knowledge Distillation\nin large, unlabelled datasets -- this leads to good students. We evaluate DnS\non five public datasets on three different video retrieval tasks and\ndemonstrate a) that our students achieve state-of-the-art performance in\nseveral cases and b) that our DnS framework provides an excellent trade-off\nbetween retrieval performance, computational speed, and storage space. In\nspecific configurations, our method achieves similar mAP with the teacher but\nis 20 times faster and requires 240 times less storage space. Our collected\ndataset and implementation are publicly available:\nhttps://github.com/mever-team/distill-and-select.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:34:24 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kordopatis-Zilos", "Giorgos", ""], ["Tzelepis", "Christos", ""], ["Papadopoulos", "Symeon", ""], ["Kompatsiaris", "Ioannis", ""], ["Patras", "Ioannis", ""]]}, {"id": "2106.13375", "submitter": "Tristan Naumann", "authors": "Yu Wang, Jinchao Li, Tristan Naumann, Chenyan Xiong, Hao Cheng, Robert\n  Tinn, Cliff Wong, Naoto Usuyama, Richard Rogahn, Zhihong Shen, Yang Qin, Eric\n  Horvitz, Paul N. Bennett, Jianfeng Gao, Hoifung Poon", "title": "Domain-Specific Pretraining for Vertical Search: Case Study on\n  Biomedical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 01:02:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Yu", ""], ["Li", "Jinchao", ""], ["Naumann", "Tristan", ""], ["Xiong", "Chenyan", ""], ["Cheng", "Hao", ""], ["Tinn", "Robert", ""], ["Wong", "Cliff", ""], ["Usuyama", "Naoto", ""], ["Rogahn", "Richard", ""], ["Shen", "Zhihong", ""], ["Qin", "Yang", ""], ["Horvitz", "Eric", ""], ["Bennett", "Paul N.", ""], ["Gao", "Jianfeng", ""], ["Poon", "Hoifung", ""]]}, {"id": "2106.13386", "submitter": "Weiwen Liu", "authors": "Weiwen Liu, Feng Liu, Ruiming Tang, Ben Liao, Guangyong Chen, Pheng\n  Ann Heng", "title": "Balancing Accuracy and Fairness for Interactive Recommendation with\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fairness in recommendation has attracted increasing attention due to bias and\ndiscrimination possibly caused by traditional recommenders. In Interactive\nRecommender Systems (IRS), user preferences and the system's fairness status\nare constantly changing over time. Existing fairness-aware recommenders mainly\nconsider fairness in static settings. Directly applying existing methods to IRS\nwill result in poor recommendation. To resolve this problem, we propose a\nreinforcement learning based framework, FairRec, to dynamically maintain a\nlong-term balance between accuracy and fairness in IRS. User preferences and\nthe system's fairness status are jointly compressed into the state\nrepresentation to generate recommendations. FairRec aims at maximizing our\ndesigned cumulative reward that combines accuracy and fairness. Extensive\nexperiments validate that FairRec can improve fairness, while preserving good\nrecommendation quality.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:02:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Liu", "Weiwen", ""], ["Liu", "Feng", ""], ["Tang", "Ruiming", ""], ["Liao", "Ben", ""], ["Chen", "Guangyong", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2106.13500", "submitter": "Haoyu Dong", "authors": "Haoyu Dong, Shijie Liu, Shi Han, Zhouyu Fu, Dongmei Zhang", "title": "TableSense: Spreadsheet Table Detection with Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1609/aaai.v33i01.330169", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheet table detection is the task of detecting all tables on a given\nsheet and locating their respective ranges. Automatic table detection is a key\nenabling technique and an initial step in spreadsheet data intelligence.\nHowever, the detection task is challenged by the diversity of table structures\nand table layouts on the spreadsheet. Considering the analogy between a cell\nmatrix as spreadsheet and a pixel matrix as image, and encouraged by the\nsuccessful application of Convolutional Neural Networks (CNN) in computer\nvision, we have developed TableSense, a novel end-to-end framework for\nspreadsheet table detection. First, we devise an effective cell featurization\nscheme to better leverage the rich information in each cell; second, we develop\nan enhanced convolutional neural network model for table detection to meet the\ndomain-specific requirement on precise table boundary detection; third, we\npropose an effective uncertainty metric to guide an active learning based smart\nsampling algorithm, which enables the efficient build-up of a training dataset\nwith 22,176 tables on 10,220 sheets with broad coverage of diverse table\nstructures and layouts. Our evaluation shows that TableSense is highly\neffective with 91.3\\% recall and 86.5\\% precision in EoB-2 metric, a\nsignificant improvement over both the current detection algorithm that are used\nin commodity spreadsheet tools and state-of-the-art convolutional neural\nnetworks in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:41:01 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dong", "Haoyu", ""], ["Liu", "Shijie", ""], ["Han", "Shi", ""], ["Fu", "Zhouyu", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2106.13528", "submitter": "Tony Russell-Rose", "authors": "Tony Russell-Rose, Philip Gooch, Udo Kruschwitz", "title": "Interactive query expansion for professional search applications", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge workers (such as healthcare information professionals, patent\nagents and recruitment professionals) undertake work tasks where search forms a\ncore part of their duties. In these instances, the search task is often complex\nand time-consuming and requires specialist expert knowledge to formulate\naccurate search strategies. Interactive features such as query expansion can\nplay a key role in supporting these tasks. However, generating query\nsuggestions within a professional search context requires that consideration be\ngiven to the specialist, structured nature of the search strategies they\nemploy. In this paper, we investigate a variety of query expansion methods\napplied to a collection of Boolean search strategies used in a variety of\nreal-world professional search tasks. The results demonstrate the utility of\ncontext-free distributional language models and the value of using linguistic\ncues such as ngram order to optimise the balance between precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:40:46 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Russell-Rose", "Tony", ""], ["Gooch", "Philip", ""], ["Kruschwitz", "Udo", ""]]}, {"id": "2106.13618", "submitter": "Oleg Lesota", "authors": "Oleg Lesota, Navid Rekabsaz, Daniel Cohen, Klaus Antonius\n  Grasserbauer, Carsten Eickhoff and Markus Schedl", "title": "A Modern Perspective on Query Likelihood with Deep Generative Retrieval\n  Models", "comments": "ICTIR'21", "journal-ref": null, "doi": "10.1145/3471158.3472229", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural ranking models follow the text matching paradigm, where\ndocument-to-query relevance is estimated through predicting the matching score.\nDrawing from the rich literature of classical generative retrieval models, we\nintroduce and formalize the paradigm of deep generative retrieval models\ndefined via the cumulative probabilities of generating query terms. This\nparadigm offers a grounded probabilistic view on relevance estimation while\nstill enabling the use of modern neural architectures. In contrast to the\nmatching paradigm, the probabilistic nature of generative rankers readily\noffers a fine-grained measure of uncertainty. We adopt several current neural\ngenerative models in our framework and introduce a novel generative ranker\n(T-PGN), which combines the encoding capacity of Transformers with the Pointer\nGenerator Network model. We conduct an extensive set of evaluation experiments\non passage retrieval, leveraging the MS MARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking collections. Our results show the\nsignificantly higher performance of the T-PGN model when compared with other\ngenerative models. Lastly, we demonstrate that exploiting the uncertainty\ninformation of deep generative rankers opens new perspectives to\nquery/collection understanding, and significantly improves the cut-off\nprediction task.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:15:38 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Lesota", "Oleg", ""], ["Rekabsaz", "Navid", ""], ["Cohen", "Daniel", ""], ["Grasserbauer", "Klaus Antonius", ""], ["Eickhoff", "Carsten", ""], ["Schedl", "Markus", ""]]}, {"id": "2106.13711", "submitter": "Yaqing Wang", "authors": "Yaqing Wang, Fenglong Ma, Haoyu Wang, Kishlay Jha and Jing Gao", "title": "Multimodal Emergent Fake News Detection via Meta Neural Process Networks", "comments": "accepted by KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467153", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news travels at unprecedented speeds, reaches global audiences and puts\nusers and communities at great risk via social media platforms. Deep learning\nbased models show good performance when trained on large amounts of labeled\ndata on events of interest, whereas the performance of models tends to degrade\non other events due to domain shift. Therefore, significant challenges are\nposed for existing detection approaches to detect fake news on emergent events,\nwhere large-scale labeled datasets are difficult to obtain. Moreover, adding\nthe knowledge from newly emergent events requires to build a new model from\nscratch or continue to fine-tune the model, which can be challenging,\nexpensive, and unrealistic for real-world settings. In order to address those\nchallenges, we propose an end-to-end fake news detection framework named\nMetaFEND, which is able to learn quickly to detect fake news on emergent events\nwith a few verified posts. Specifically, the proposed model integrates\nmeta-learning and neural process methods together to enjoy the benefits of\nthese approaches. In particular, a label embedding module and a hard attention\nmechanism are proposed to enhance the effectiveness by handling categorical\ninformation and trimming irrelevant posts. Extensive experiments are conducted\non multimedia datasets collected from Twitter and Weibo. The experimental\nresults show our proposed MetaFEND model can detect fake news on never-seen\nevents effectively and outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:21:29 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Yaqing", ""], ["Ma", "Fenglong", ""], ["Wang", "Haoyu", ""], ["Jha", "Kishlay", ""], ["Gao", "Jing", ""]]}, {"id": "2106.13732", "submitter": "Jinjin Guo", "authors": "Jinjin Guo, Longbing Cao and Zhiguo Gong", "title": "Recurrent Coupled Topic Modeling over Sequential Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundant sequential documents such as online archival, social media and\nnews feeds are streamingly updated, where each chunk of documents is\nincorporated with smoothly evolving yet dependent topics. Such digital texts\nhave attracted extensive research on dynamic topic modeling to infer hidden\nevolving topics and their temporal dependencies. However, most of the existing\napproaches focus on single-topic-thread evolution and ignore the fact that a\ncurrent topic may be coupled with multiple relevant prior topics. In addition,\nthese approaches also incur the intractable inference problem when inferring\nlatent parameters, resulting in a high computational cost and performance\ndegradation. In this work, we assume that a current topic evolves from all\nprior topics with corresponding coupling weights, forming the\nmulti-topic-thread evolution. Our method models the dependencies between\nevolving topics and thoroughly encodes their complex multi-couplings across\ntime steps. To conquer the intractable inference challenge, a new solution with\na set of novel data augmentation techniques is proposed, which successfully\ndiscomposes the multi-couplings between evolving topics. A fully conjugate\nmodel is thus obtained to guarantee the effectiveness and efficiency of the\ninference technique. A novel Gibbs sampler with a backward-forward filter\nalgorithm efficiently learns latent timeevolving parameters in a closed-form.\nIn addition, the latent Indian Buffet Process (IBP) compound distribution is\nexploited to automatically infer the overall topic number and customize the\nsparse topic proportions for each sequential document without bias. The\nproposed method is evaluated on both synthetic and real-world datasets against\nthe competitive baselines, demonstrating its superiority over the baselines in\nterms of the low per-word perplexity, high coherent topics, and better document\ntime prediction.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:58:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Guo", "Jinjin", ""], ["Cao", "Longbing", ""], ["Gong", "Zhiguo", ""]]}, {"id": "2106.13767", "submitter": "Hrishikesh Kulkarni", "authors": "Hrishikesh Kulkarni and Bradly Alicea", "title": "Sentiment Progression based Searching and Indexing of Literary Textual\n  Artefacts", "comments": "12 pages, 2 figures, accepted at NLDB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary artefacts are generally indexed and searched based on titles, meta\ndata and keywords over the years. This searching and indexing works well when\nuser/reader already knows about that particular creative textual artefact or\ndocument. This indexing and search hardly takes into account interest and\nemotional makeup of readers and its mapping to books. When a person is looking\nfor a literary textual artefact, he/she might be looking for not only\ninformation but also to seek the joy of reading. In case of literary artefacts,\nprogression of emotions across the key events could prove to be the key for\nindexing and searching. In this paper, we establish clusters among literary\nartefacts based on computational relationships among sentiment progressions\nusing intelligent text analysis. We have created a database of 1076 English\ntitles + 20 Marathi titles and also used database\nhttp://www.cs.cmu.edu/~dbamman/booksummaries.html with 16559 titles and their\nsummaries. We have proposed Sentiment Progression based Indexing for searching\nand recommending books. This can be used to create personalized clusters of\nbook titles of interest to readers. The analysis clearly suggests better\nsearching and indexing when we are targeting book lovers looking for a\nparticular type of book or creative artefact. This indexing and searching can\nfind many real-life applications for recommending books.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:49:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kulkarni", "Hrishikesh", ""], ["Alicea", "Bradly", ""]]}, {"id": "2106.14031", "submitter": "Xu Yuan", "authors": "Xu Yuan, Hongshen Chen, Yonghao Song, Xiaofang Zhao, Zhuoye Ding, Zhen\n  He, Bo Long", "title": "Improving Sequential Recommendation Consistency with Self-Supervised\n  Imitation", "comments": "accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, SSI, to improve sequential\nrecommendation consistency with Self-Supervised Imitation. Precisely, we\nextract the consistency knowledge by utilizing three self-supervised\npre-training tasks, where temporal consistency and persona consistency capture\nuser-interaction dynamics in terms of the chronological order and persona\nsensitivities, respectively. Furthermore, to provide the model with a global\nperspective, global session consistency is introduced by maximizing the mutual\ninformation among global and local interaction sequences. Finally, to\ncomprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 14:15:29 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 11:09:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yuan", "Xu", ""], ["Chen", "Hongshen", ""], ["Song", "Yonghao", ""], ["Zhao", "Xiaofang", ""], ["Ding", "Zhuoye", ""], ["He", "Zhen", ""], ["Long", "Bo", ""]]}, {"id": "2106.14072", "submitter": "Mykola Makhortykh", "authors": "Mykola Makhortykh, Aleksandra Urman, Roberto Ulloa", "title": "Detecting race and gender bias in visual representation of AI on web\n  search engines", "comments": "16 pages, 3 figures", "journal-ref": "In Advances in Bias and Fairness in Information Retrieval (pp.\n  36-50). Springer (2021)", "doi": "10.1007/978-3-030-78818-6_5", "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:04:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Makhortykh", "Mykola", ""], ["Urman", "Aleksandra", ""], ["Ulloa", "Roberto", ""]]}, {"id": "2106.14174", "submitter": "Saeid Hosseini", "authors": "Sana Rahmani, Saeid Hosseini, Raziyeh Zall, Mohammad Reza Kangavari,\n  Sara Kamran, Wen Hua", "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on\n  user latent aspects", "comments": "Under Review on IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 09:10:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rahmani", "Sana", ""], ["Hosseini", "Saeid", ""], ["Zall", "Raziyeh", ""], ["Kangavari", "Mohammad Reza", ""], ["Kamran", "Sara", ""], ["Hua", "Wen", ""]]}, {"id": "2106.14213", "submitter": "Srikanth Chandar", "authors": "Muvazima Mansoor, Srikanth Chandar, Ramamoorthy Srinath", "title": "AI based Presentation Creator With Customized Audio Content Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 12:17:11 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mansoor", "Muvazima", ""], ["Chandar", "Srikanth", ""], ["Srinath", "Ramamoorthy", ""]]}, {"id": "2106.14226", "submitter": "Jianxin Chang", "authors": "Jianxin Chang, Chen Gao, Yu Zheng, Yiqun Hui, Yanan Niu, Yang Song,\n  Depeng Jin, Yong Li", "title": "Sequential Recommendation with Graph Neural Networks", "comments": "Accepted by SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation aims to leverage users' historical behaviors to\npredict their next interaction. Existing works have not yet addressed two main\nchallenges in sequential recommendation. First, user behaviors in their rich\nhistorical sequences are often implicit and noisy preference signals, they\ncannot sufficiently reflect users' actual preferences. In addition, users'\ndynamic preferences often change rapidly over time, and hence it is difficult\nto capture user patterns in their historical sequences. In this work, we\npropose a graph neural network model called SURGE (short for SeqUential\nRecommendation with Graph neural nEtworks) to address these two issues.\nSpecifically, SURGE integrates different types of preferences in long-term user\nbehaviors into clusters in the graph by re-constructing loose item sequences\ninto tight item-item interest graphs based on metric learning. This helps\nexplicitly distinguish users' core interests, by forming dense clusters in the\ninterest graph. Then, we perform cluster-aware and query-aware graph\nconvolutional propagation and graph pooling on the constructed graph. It\ndynamically fuses and extracts users' current activated core interests from\nnoisy user behavior sequences. We conduct extensive experiments on both public\nand proprietary industrial datasets. Experimental results demonstrate\nsignificant performance gains of our proposed method compared to\nstate-of-the-art methods. Further studies on sequence length confirm that our\nmethod can model long behavioral sequences effectively and efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 12:57:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chang", "Jianxin", ""], ["Gao", "Chen", ""], ["Zheng", "Yu", ""], ["Hui", "Yiqun", ""], ["Niu", "Yanan", ""], ["Song", "Yang", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2106.14269", "submitter": "Shuo Jiang", "authors": "Shuo Jiang, Jianxi Luo, Jie Hu, Christopher L. Magee", "title": "Deep Learning for Technical Document Classification", "comments": "34 pages, 7 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:12:47 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jiang", "Shuo", ""], ["Luo", "Jianxi", ""], ["Hu", "Jie", ""], ["Magee", "Christopher L.", ""]]}, {"id": "2106.14388", "submitter": "Liang Yile", "authors": "Tieyun Qian, Yile Liang, Qing Li, Xuan Ma, Ke Sun, Zhiyong Peng", "title": "Intent Disentanglement and Feature Self-supervision for Novel\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One key property in recommender systems is the long-tail distribution in\nuser-item interactions where most items only have few user feedback. Improving\nthe recommendation of tail items can promote novelty and bring positive effects\nto both users and providers, and thus is a desirable property of recommender\nsystems. Current novel recommendation studies over-emphasize the importance of\ntail items without differentiating the degree of users' intent on popularity\nand often incur a sharp decline of accuracy. Moreover, none of existing methods\nhas ever taken the extreme case of tail items, i.e., cold-start items without\nany interaction, into consideration.\n  In this work, we first disclose the mechanism that drives a user's\ninteraction towards popular or niche items by disentangling her intent into\nconformity influence (popularity) and personal interests (preference). We then\npresent a unified end-to-end framework to simultaneously optimize accuracy and\nnovelty targets based on the disentangled intent of popularity and that of\npreference. We further develop a new paradigm for novel recommendation of\ncold-start items which exploits the self-supervised learning technique to model\nthe correlation between collaborative features and content features. We conduct\nextensive experimental results on three real-world datasets. The results\ndemonstrate that our proposed model yields significant improvements over the\nstate-of-the-art baselines in terms of accuracy, novelty, coverage, and\ntrade-off.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 04:07:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Qian", "Tieyun", ""], ["Liang", "Yile", ""], ["Li", "Qing", ""], ["Ma", "Xuan", ""], ["Sun", "Ke", ""], ["Peng", "Zhiyong", ""]]}, {"id": "2106.14463", "submitter": "Saahil Jain", "authors": "Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du\n  Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P. Lungren,\n  Andrew Y. Ng, Curtis P. Langlotz, Pranav Rajpurkar", "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology\n  Reports", "comments": null, "journal-ref": null, "doi": "10.13026/hm87-5p47", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:24:23 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Jain", "Saahil", ""], ["Agrawal", "Ashwin", ""], ["Saporta", "Adriel", ""], ["Truong", "Steven QH", ""], ["Duong", "Du Nguyen", ""], ["Bui", "Tan", ""], ["Chambon", "Pierre", ""], ["Zhang", "Yuhao", ""], ["Lungren", "Matthew P.", ""], ["Ng", "Andrew Y.", ""], ["Langlotz", "Curtis P.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2106.14616", "submitter": "Antonio Jose Jimeno Yepes", "authors": "Antonio Jimeno Yepes, Xu Zhong, Douglas Burdick", "title": "ICDAR 2021 Competition on Scientific Literature Parsing", "comments": null, "journal-ref": "ICDAR, 2021", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific literature contain important information related to cutting-edge\ninnovations in diverse domains. Advances in natural language processing have\nbeen driving the fast development in automated information extraction from\nscientific literature. However, scientific literature is often available in\nunstructured PDF format. While PDF is great for preserving basic visual\nelements, such as characters, lines, shapes, etc., on a canvas for presentation\nto humans, automatic processing of the PDF format by machines presents many\nchallenges. With over 2.5 trillion PDF documents in existence, these issues are\nprevalent in many other important application domains as well.\n  Our ICDAR 2021 Scientific Literature Parsing Competition (ICDAR2021-SLP) aims\nto drive the advances specifically in document understanding. ICDAR2021-SLP\nleverages the PubLayNet and PubTabNet datasets, which provide hundreds of\nthousands of training and evaluation examples. In Task A, Document Layout\nRecognition, submissions with the highest performance combine object detection\nand specialised solutions for the different categories. In Task B, Table\nRecognition, top submissions rely on methods to identify table components and\npost-processing methods to generate the table structure and content. Results\nfrom both tasks show an impressive performance and opens the possibility for\nhigh performance practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:39:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yepes", "Antonio Jimeno", ""], ["Zhong", "Xu", ""], ["Burdick", "Douglas", ""]]}, {"id": "2106.14618", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Anastasia Krithara, Konstantinos Bougiatiotis,\n  Martin Krallinger, Carlos Rodriguez-Penagos, Marta Villegas, Georgios\n  Paliouras", "title": "Overview of BioASQ 2020: The eighth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering", "comments": "21 pages, 10 tables, 3 figures", "journal-ref": "Arampatzis A. et al. (eds) Experimental IR Meets Multilinguality,\n  Multimodality, and Interaction. CLEF 2020. Lecture Notes in Computer Science,\n  vol 12260. Springer, Cham", "doi": "10.1007/978-3-030-58219-7_16", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an overview of the eighth edition of the BioASQ\nchallenge, which ran as a lab in the Conference and Labs of the Evaluation\nForum (CLEF) 2020. BioASQ is a series of challenges aiming at the promotion of\nsystems and methodologies for large-scale biomedical semantic indexing and\nquestion answering. To this end, shared tasks are organized yearly since 2012,\nwhere different teams develop systems that compete on the same demanding\nbenchmark datasets that represent the real information needs of experts in the\nbiomedical domain. This year, the challenge has been extended with the\nintroduction of a new task on medical semantic indexing in Spanish. In total,\n34 teams with more than 100 systems participated in the three tasks of the\nchallenge. As in previous years, the results of the evaluation reveal that the\ntop-performing systems managed to outperform the strong baselines, which\nsuggests that state-of-the-art systems keep pushing the frontier of research\nthrough continuous improvements.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:24:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Krithara", "Anastasia", ""], ["Bougiatiotis", "Konstantinos", ""], ["Krallinger", "Martin", ""], ["Rodriguez-Penagos", "Carlos", ""], ["Villegas", "Marta", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2106.14652", "submitter": "Peiyuan Zhu", "authors": "Peiyuan Zhu, Xiaofeng Wang, Zisen Sang, Aiquan Yuan, Guodong Cao", "title": "Context-aware Heterogeneous Graph Attention Network for User Behavior\n  Prediction in Local Consumer Service Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As a new type of e-commerce platform developed in recent years, local\nconsumer service platform provides users with software to consume service to\nthe nearby store or to the home, such as Groupon and Koubei. Different from\nother common e-commerce platforms, the behavior of users on the local consumer\nservice platform is closely related to their real-time local context\ninformation. Therefore, building a context-aware user behavior prediction\nsystem is able to provide both merchants and users better service in local\nconsumer service platforms. However, most of the previous work just treats the\ncontextual information as an ordinary feature into the prediction model to\nobtain the prediction list under a specific context, which ignores the fact\nthat the interest of a user in different contexts is often significantly\ndifferent. Hence, in this paper, we propose a context-aware heterogeneous graph\nattention network (CHGAT) to dynamically generate the representation of the\nuser and to estimate the probability for future behavior. Specifically, we\nfirst construct the meta-path based heterogeneous graphs with the historical\nbehaviors from multiple sources and comprehend heterogeneous vertices in the\ngraph with a novel unified knowledge representing approach. Next, a multi-level\nattention mechanism is introduced for context-aware aggregation with graph\nvertices, which contains the vertex-level attention network and the path-level\nattention network. Both of them aim to capture the semantic correlation between\ninformation contained in the graph and the outside real-time contextual\ninformation in the search system. Then the model proposed in this paper\naggregates specific graphs with their corresponding context features and\nobtains the representation of user interest under a specific context and input\nit into the prediction network to finally obtain the predicted probability of\nuser behavior.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 03:08:21 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:54:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhu", "Peiyuan", ""], ["Wang", "Xiaofeng", ""], ["Sang", "Zisen", ""], ["Yuan", "Aiquan", ""], ["Cao", "Guodong", ""]]}, {"id": "2106.14657", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "A. Fronzetti Colladon, F. Grippa, L. Segneri", "title": "A new system for evaluating brand importance: A use case from the\n  fashion industry", "comments": null, "journal-ref": "13th ACM Web Science Conference 2021 (WebSci '21 Companion) (pp.\n  132-136). ACM, New York, NY, USA (2021)", "doi": "10.1145/3462741.3466678", "report-no": null, "categories": "cs.IR cs.CL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today brand managers and marketing specialists can leverage huge amount of\ndata to reveal patterns and trends in consumer perceptions, monitoring positive\nor negative associations of brands with respect to desired topics. In this\nstudy, we apply the Semantic Brand Score (SBS) indicator to assess brand\nimportance in the fashion industry. To this purpose, we measure and visualize\ntext data using the SBS Business Intelligence App (SBS BI), which relies on\nmethods and tools of text mining and social network analysis. We collected and\nanalyzed about 206,000 tweets that mentioned the fashion brands Fendi, Gucci\nand Prada, during the period from March 5 to March 12, 2021. From the analysis\nof the three SBS dimensions - prevalence, diversity and connectivity - we found\nthat Gucci dominated the discourse, with high values of SBS. We use this case\nstudy as an example to present a new system for evaluating brand importance and\nimage, through the analysis of (big) textual data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:04:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Segneri", "L.", ""]]}, {"id": "2106.14719", "submitter": "Matteo Allaix", "authors": "Matteo Allaix, Seunghoan Song, Lukas Holzbaur, Tefjol Pllaha, Masahito\n  Hayashi, Camilla Hollanti", "title": "On the Capacity of Quantum Private Information Retrieval from MDS-Coded\n  and Colluding Servers", "comments": "This work extends arXiv:2102.02511", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quantum private information retrieval (QPIR), a user retrieves a classical\nfile from multiple servers by downloading quantum systems without revealing the\nidentity of the file. The QPIR capacity is the maximal achievable ratio of the\nretrieved file size to the total download size. In this paper, the capacity of\nQPIR from MDS-coded and colluding servers is studied. Two classes of QPIR,\ncalled stabilizer QPIR and dimension squared QPIR induced from classical\nstrongly linear PIR are defined, and the related QPIR capacities are derived.\nFor the non-colluding case, the general QPIR capacity is derived when the\nnumber of files goes to infinity. The capacities of symmetric and non-symmetric\nQPIR with coded and colluding servers are proved to coincide, being double to\ntheir classical counterparts. A general statement on the converse bound for\nQPIR with coded and colluding servers is derived showing that the capacities of\nstabilizer QPIR and dimension squared QPIR induced from any class of PIR are\nupper bounded by twice the classical capacity of the respective PIR class. The\nproposed capacity-achieving scheme combines the star-product scheme by\nFreij-Hollanti et al. and the stabilizer QPIR scheme by Song et al. by\nemploying (weakly) self-dual Reed--Solomon codes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:48:22 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:38:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Allaix", "Matteo", ""], ["Song", "Seunghoan", ""], ["Holzbaur", "Lukas", ""], ["Pllaha", "Tefjol", ""], ["Hayashi", "Masahito", ""], ["Hollanti", "Camilla", ""]]}, {"id": "2106.14726", "submitter": "Florian Boudin", "authors": "Florian Boudin, Ygor Gallina, Akiko Aizawa", "title": "Keyphrase Generation for Scientific Document Retrieval", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": "10.18653/v1/2020.acl-main.105", "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequence-to-sequence models have lead to significant progress in keyphrase\ngeneration, but it remains unknown whether they are reliable enough to be\nbeneficial for document retrieval. This study provides empirical evidence that\nsuch models can significantly improve retrieval performance, and introduces a\nnew extrinsic evaluation framework that allows for a better understanding of\nthe limitations of keyphrase generation models. Using this framework, we point\nout and discuss the difficulties encountered with supplementing documents with\n-- not present in text -- keyphrases, and generalizing models across domains.\nOur code is available at https://github.com/boudinfl/ir-using-kg\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:55:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Boudin", "Florian", ""], ["Gallina", "Ygor", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2106.14731", "submitter": "Florian Boudin", "authors": "Florian Boudin, B\\'eatrice Daille, Evelyne Jacquey, Jian-Yun Nie", "title": "The DELICES project: Indexing scientific literature through semantic\n  expansion", "comments": "Accepted at CIRCLE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific digital libraries play a critical role in the development and\ndissemination of scientific literature. Despite dedicated search engines,\nretrieving relevant publications from the ever-growing body of scientific\nliterature remains challenging and time-consuming. Indexing scientific articles\nis indeed a difficult matter, and current models solely rely on a small portion\nof the articles (title and abstract) and on author-assigned keyphrases when\navailable. This results in a frustratingly limited access to scientific\nknowledge. The goal of the DELICES project is to address this pitfall by\nexploiting semantic relations between scientific articles to both improve and\nenrich indexing. To this end, we will rely on the latest advances in semantic\nrepresentations to both increase the relevance of keyphrases extracted from the\ndocuments, and extend indexing to new terms borrowed from semantically similar\ndocuments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:02:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Boudin", "Florian", ""], ["Daille", "B\u00e9atrice", ""], ["Jacquey", "Evelyne", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2106.14807", "submitter": "Jimmy Lin", "authors": "Jimmy Lin and Xueguang Ma", "title": "A Few Brief Notes on DeepImpact, COIL, and a Conceptual Framework for\n  Information Retrieval Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in representational learning for information retrieval\ncan be organized in a conceptual framework that establishes two pairs of\ncontrasts: sparse vs. dense representations and unsupervised vs. learned\nrepresentations. Sparse learned representations can further be decomposed into\nexpansion and term weighting components. This framework allows us to understand\nthe relationship between recently proposed techniques such as DPR, ANCE,\nDeepCT, DeepImpact, and COIL, and furthermore, gaps revealed by our analysis\npoint to \"low hanging fruit\" in terms of techniques that have yet to be\nexplored. We present a novel technique dubbed \"uniCOIL\", a simple extension of\nCOIL that achieves to our knowledge the current state-of-the-art in sparse\nretrieval on the popular MS MARCO passage ranking dataset. Our implementation\nusing the Anserini IR toolkit is built on the Lucene search library and thus\nfully compatible with standard inverted indexes.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:30:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lin", "Jimmy", ""], ["Ma", "Xueguang", ""]]}, {"id": "2106.14885", "submitter": "Anastasios Nentidis", "authors": "Anastasios Nentidis, Georgios Katsimpras, Eirini Vandorou, Anastasia\n  Krithara, Luis Gasco, Martin Krallinger, Georgios Paliouras", "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering", "comments": "25 pages, 15 tables, 3 figures. arXiv admin note: text overlap with\n  arXiv:2106.14618", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:03:11 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nentidis", "Anastasios", ""], ["Katsimpras", "Georgios", ""], ["Vandorou", "Eirini", ""], ["Krithara", "Anastasia", ""], ["Gasco", "Luis", ""], ["Krallinger", "Martin", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2106.14979", "submitter": "Niki Kilbertus", "authors": "Jiri Hron, Karl Krauth, Michael I. Jordan, Niki Kilbertus", "title": "On component interactions in two-stage recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their scalability, two-stage recommenders are used by many of\ntoday's largest online platforms, including YouTube, LinkedIn, and Pinterest.\nThese systems produce recommendations in two steps: (i) multiple nominators --\ntuned for low prediction latency -- preselect a small subset of candidates from\nthe whole item pool; (ii)~a slower but more accurate ranker further narrows\ndown the nominated items, and serves to the user. Despite their popularity, the\nliterature on two-stage recommenders is relatively scarce, and the algorithms\nare often treated as the sum of their parts. Such treatment presupposes that\nthe two-stage performance is explained by the behavior of individual components\nif they were deployed independently. This is not the case: using synthetic and\nreal-world data, we demonstrate that interactions between the ranker and the\nnominators substantially affect the overall performance. Motivated by these\nfindings, we derive a generalization lower bound which shows that careful\nchoice of each nominator's training set is sometimes the only difference\nbetween a poor and an optimal two-stage recommender. Since searching for a good\nchoice manually is difficult, we learn one instead. In particular, using a\nMixture-of-Experts approach, we train the nominators (experts) to specialize on\ndifferent subsets of the item pool. This significantly improves performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 20:53:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hron", "Jiri", ""], ["Krauth", "Karl", ""], ["Jordan", "Michael I.", ""], ["Kilbertus", "Niki", ""]]}, {"id": "2106.15230", "submitter": "Caglar Demir", "authors": "Caglar Demir, Diego Moussallem, Stefan Heindorf, Axel-Cyrille Ngonga\n  Ngomo", "title": "Convolutional Hypercomplex Embeddings for Link Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Knowledge graph embedding research has mainly focused on the two smallest\nnormed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest\nthat trilinear products of quaternion-valued embeddings can be a more effective\nmeans to tackle link prediction. In addition, models based on convolutions on\nreal-valued embeddings often yield state-of-the-art results for link\nprediction. In this paper, we investigate a composition of convolution\noperations with hypercomplex multiplications. We propose the four approaches\nQMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and\nOMult can be considered as quaternion and octonion extensions of previous\nstate-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO\nbuild upon QMult and OMult by including convolution operations in a way\ninspired by the residual learning framework. We evaluated our approaches on\nseven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.\nExperimental results suggest that the benefits of learning hypercomplex-valued\nvector representations become more apparent as the size and complexity of the\nknowledge graph grows. ConvO outperforms state-of-the-art approaches on\nFB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO\noutperform state-of-the-approaches on YAGO3-10 in all metrics. Results also\nsuggest that link prediction performances can be further improved via\nprediction averaging. To foster reproducible research, we provide an\nopen-source implementation of approaches, including training and evaluation\nscripts as well as pretrained models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 10:26:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Demir", "Caglar", ""], ["Moussallem", "Diego", ""], ["Heindorf", "Stefan", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "2106.15313", "submitter": "Shivam Patel", "authors": "Kalliath Abdul Rasheed Issam, Shivam Patel, Subalalitha C. N", "title": "Topic Modeling Based Extractive Text Summarization", "comments": "10 pages, 13 figures, 3 tables", "journal-ref": "International Journal of Innovative Technology and Exploring\n  Engineering, Volume-9 Issue-6, April 2020, Page No. 1710-1719", "doi": "10.35940/ijitee.F4611.049620", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:28:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Issam", "Kalliath Abdul Rasheed", ""], ["Patel", "Shivam", ""], ["N", "Subalalitha C.", ""]]}, {"id": "2106.15497", "submitter": "Chaochen Shi", "authors": "Chaochen Shi, Yong Xiang, Robin Ram Mohan Doss, Jiangshan Yu, Keshav\n  Sood, Longxiang Gao", "title": "A Bytecode-based Approach for Smart Contract Classification", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 03:00:29 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Shi", "Chaochen", ""], ["Xiang", "Yong", ""], ["Doss", "Robin Ram Mohan", ""], ["Yu", "Jiangshan", ""], ["Sood", "Keshav", ""], ["Gao", "Longxiang", ""]]}, {"id": "2106.15498", "submitter": "Gerhard Hagerer", "authors": "Gerhard Hagerer and Wenbin Le and Hannah Danner and Georg Groh", "title": "Classification of Consumer Belief Statements From Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:25:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hagerer", "Gerhard", ""], ["Le", "Wenbin", ""], ["Danner", "Hannah", ""], ["Groh", "Georg", ""]]}, {"id": "2106.15541", "submitter": "Giacomo Vaccario Dr.", "authors": "Giacomo Vaccario and Luca Verginer", "title": "When standard network measures fail to rank journals: A theoretical and\n  empirical analysis", "comments": "17 pages, 6 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Journal rankings are widely used and are often based on citation data in\ncombination with a network perspective. We argue that some of these\nnetwork-based rankings can produce misleading results. From a theoretical point\nof view, we show that the standard network modelling approach of citation data\nat the journal level (i.e., the projection of paper citations onto journals)\nintroduces fictitious relations among journals. To overcome this problem, we\npropose a citation path perspective, and empirically show that rankings based\non the network and the citation path perspective are very different. Based on\nour theoretical and empirical analysis, we highlight the limitations of\nstandard network metrics, and propose a method to overcome these limitations\nand compute journal rankings.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:20:33 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Vaccario", "Giacomo", ""], ["Verginer", "Luca", ""]]}, {"id": "2106.15779", "submitter": "Ning Yang", "authors": "Qiaomin Yi, Ning Yang, Philip S. Yu", "title": "Dual Adversarial Variational Embedding for Robust Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust recommendation aims at capturing true preference of users from noisy\ndata, for which there are two lines of methods have been proposed. One is based\non noise injection, and the other is to adopt the generative model Variational\nAuto-encoder (VAE). However, the existing works still face two challenges.\nFirst, the noise injection based methods often draw the noise from a fixed\nnoise distribution given in advance, while in real world, the noise\ndistributions of different users and items may differ from each other due to\npersonal behaviors and item usage patterns. Second, the VAE based models are\nnot expressive enough to capture the true preference since VAE often yields an\nembedding space of a single modal, while in real world, user-item interactions\nusually exhibit multi-modality on user preference distribution. In this paper,\nwe propose a novel model called Dual Adversarial Variational Embedding (DAVE)\nfor robust recommendation, which can provide personalized noise reduction for\ndifferent users and items, and capture the multi-modality of the embedding\nspace, by combining the advantages of VAE and adversarial training between the\nintroduced auxiliary discriminators and the variational inference networks. The\nextensive experiments conducted on real datasets verify the effectiveness of\nDAVE on robust recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 02:18:03 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Yi", "Qiaomin", ""], ["Yang", "Ning", ""], ["Yu", "Philip S.", ""]]}, {"id": "2106.15814", "submitter": "Yang Li", "authors": "Yang Li, Tong Chen, Hongzhi Yin, Zi Huang", "title": "Discovering Collaborative Signals for Next POI Recommendation with\n  Iterative Seq2Graph Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being an indispensable component in location-based social networks, next\npoint-of-interest (POI) recommendation recommends users unexplored POIs based\non their recent visiting histories. However, existing work mainly models\ncheck-in data as isolated POI sequences, neglecting the crucial collaborative\nsignals from cross-sequence check-in information. Furthermore, the sparse\nPOI-POI transitions restrict the ability of a model to learn effective\nsequential patterns for recommendation. In this paper, we propose\nSequence-to-Graph (Seq2Graph) augmentation for each POI sequence, allowing\ncollaborative signals to be propagated from correlated POIs belonging to other\nsequences. We then devise a novel Sequence-to-Graph POI Recommender (SGRec),\nwhich jointly learns POI embeddings and infers a user's temporal preferences\nfrom the graph-augmented POI sequence. To overcome the sparsity of POI-level\ninteractions, we further infuse category-awareness into SGRec with a multi-task\nlearning scheme that captures the denser category-wise transitions. As such,\nSGRec makes full use of the collaborative signals for learning expressive POI\nrepresentations, and also comprehensively uncovers multi-level sequential\npatterns for user preference modelling. Extensive experiments on two real-world\ndatasets demonstrate the superiority of SGRec against state-of-the-art methods\nin next POI recommendation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 05:12:13 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Yang", ""], ["Chen", "Tong", ""], ["Yin", "Hongzhi", ""], ["Huang", "Zi", ""]]}, {"id": "2106.15876", "submitter": "Paheli Bhattacharya", "authors": "Paheli Bhattacharya and Soham Poddar and Koustav Rudra and Kripabandhu\n  Ghosh and Saptarshi Ghosh", "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal\n  Case Documents", "comments": "Accepted at the 18th International Conference on Artificial\n  Intelligence and Law (ICAIL) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:06:15 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhattacharya", "Paheli", ""], ["Poddar", "Soham", ""], ["Rudra", "Koustav", ""], ["Ghosh", "Kripabandhu", ""], ["Ghosh", "Saptarshi", ""]]}, {"id": "2106.15903", "submitter": "Zhongkun Liu", "authors": "Zhongkun Liu, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Maarten de\n  Rijke, Ming Zhou", "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein\n  Distance", "comments": "13 pages, 4 figures, Published in ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:44:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Liu", "Zhongkun", ""], ["Ren", "Pengjie", ""], ["Chen", "Zhumin", ""], ["Ren", "Zhaochun", ""], ["de Rijke", "Maarten", ""], ["Zhou", "Ming", ""]]}, {"id": "2106.15984", "submitter": "Yang Li", "authors": "Yang Li and Yadan Luo and Zheng Zhang and Shazia W. Sadiq and Peng Cui", "title": "Context-Aware Attention-Based Data Augmentation for POI Recommendation", "comments": null, "journal-ref": "35th IEEE International Conference on Data Engineering Workshops,\n  ICDE Workshops 2019, Macao, China, April 8-12, 2019", "doi": "10.1109/ICDEW.2019.00-14", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of location-based social networks (LBSNs),\nPoint-Of-Interest (POI) recommendation has been broadly studied in this decade.\nRecently, the next POI recommendation, a natural extension of POI\nrecommendation, has attracted much attention. It aims at suggesting the next\nPOI to a user in spatial and temporal context, which is a practical yet\nchallenging task in various applications. Existing approaches mainly model the\nspatial and temporal information, and memorize historical patterns through\nuser's trajectories for recommendation. However, they suffer from the negative\nimpact of missing and irregular check-in data, which significantly influences\nthe model performance. In this paper, we propose an attention-based\nsequence-to-sequence generative model, namely POI-Augmentation Seq2Seq\n(PA-Seq2Seq), to address the sparsity of training set by making check-in\nrecords to be evenly-spaced. Specifically, the encoder summarises each check-in\nsequence and the decoder predicts the possible missing check-ins based on the\nencoded information. In order to learn time-aware correlation among user\nhistory, we employ local attention mechanism to help the decoder focus on a\nspecific range of context information when predicting a certain missing\ncheck-in point. Extensive experiments have been conducted on two real-world\ncheck-in datasets, Gowalla and Brightkite, for performance and effectiveness\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:17:50 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Yang", ""], ["Luo", "Yadan", ""], ["Zhang", "Zheng", ""], ["Sadiq", "Shazia W.", ""], ["Cui", "Peng", ""]]}, {"id": "2106.16053", "submitter": "Nikos Voskarides", "authors": "Nikos Voskarides, Edgar Meij, Sabrina Sauer, Maarten de Rijke", "title": "News Article Retrieval in Context for Event-centric Narrative Creation", "comments": "ICTIR 2021", "journal-ref": null, "doi": "10.1145/3471158.3472247", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:27:54 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Voskarides", "Nikos", ""], ["Meij", "Edgar", ""], ["Sauer", "Sabrina", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2106.16102", "submitter": "Victor Zitian Chen", "authors": "Victor Zitian Chen, Felipe Montano-Campos, Wlodek Zadrozny, and Evan\n  Canfield", "title": "Machine Reading of Hypotheses for Organizational Research Reviews and\n  Pre-trained Models via R Shiny App for Non-Programmers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:47:15 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 01:50:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Chen", "Victor Zitian", ""], ["Montano-Campos", "Felipe", ""], ["Zadrozny", "Wlodek", ""], ["Canfield", "Evan", ""]]}, {"id": "2106.16153", "submitter": "Jiaan Wang", "authors": "Jiaan Wang, Zhixu Li, Binbin Gu, Tingyi Zhang, Qingsheng Liu and\n  Zhigang Chen", "title": "Multi-Modal Chorus Recognition for Improving Song Search", "comments": "Accepted at the 30th International Conference on Artificial Neural\n  Networks (ICANN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a novel task, Chorus Recognition, which could potentially benefit\ndownstream tasks such as song search and music summarization. Different from\nthe existing tasks such as music summarization or lyrics summarization relying\non single-modal information, this paper models chorus recognition as a\nmulti-modal one by utilizing both the lyrics and the tune information of songs.\nWe propose a multi-modal Chorus Recognition model that considers diverse\nfeatures. Besides, we also create and publish the first Chorus Recognition\ndataset containing 627 songs for public use. Our empirical study performed on\nthe dataset demonstrates that our approach outperforms several baselines in\nchorus recognition. In addition, our approach also helps to improve the\naccuracy of its downstream task - song search by more than 10.6%.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:51:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wang", "Jiaan", ""], ["Li", "Zhixu", ""], ["Gu", "Binbin", ""], ["Zhang", "Tingyi", ""], ["Liu", "Qingsheng", ""], ["Chen", "Zhigang", ""]]}]