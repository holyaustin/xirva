[{"id": "1203.0332", "submitter": "Frederico Durao", "authors": "Frederico Durao, Peter Dolog", "title": "A Personalized Tag-Based Recommendation in Social Web Systems", "comments": null, "journal-ref": "Original language English, Journal CEUR Workshop Proceedings,\n  Publication date 2009, Volume 485, Pages 40-49, ISSN 1613-0073", "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Tagging activity has been recently identified as a potential source of\nknowledge about personal interests, preferences, goals, and other attributes\nknown from user models. Tags themselves can be therefore used for finding\npersonalized recommendations of items. In this paper, we present a tag-based\nrecommender system which suggests similar Web pages based on the similarity of\ntheir tags from a Web 2.0 tagging application. The proposed approach extends\nthe basic similarity calculus with external factors such as tag popularity, tag\nrepresentativeness and the affinity between user and tag. In order to study and\nevaluate the recommender system, we have conducted an experiment involving 38\npeople from 12 countries using data from Del.icio.us, a social bookmarking web\nsystem on which users can share their personal bookmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 22:11:22 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Durao", "Frederico", ""], ["Dolog", "Peter", ""]]}, {"id": "1203.0488", "submitter": "Shu Kong", "authors": "Shu Kong and Donghui Wang", "title": "Multi-Level Feature Descriptor for Robust Texture Classification via\n  Locality-Constrained Collaborative Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a simple but highly efficient ensemble for robust\ntexture classification, which can effectively deal with translation, scale and\nchanges of significant viewpoint problems. The proposed method first inherits\nthe spirit of spatial pyramid matching model (SPM), which is popular for\nencoding spatial distribution of local features, but in a flexible way,\npartitioning the original image into different levels and incorporating\ndifferent overlapping patterns of each level. This flexible setup helps capture\nthe informative features and produces sufficient local feature codes by some\nwell-chosen aggregation statistics or pooling operations within each\npartitioned region, even when only a few sample images are available for\ntraining. Then each texture image is represented by several orderless feature\ncodes and thereby all the training data form a reliable feature pond. Finally,\nto take full advantage of this feature pond, we develop a collaborative\nrepresentation-based strategy with locality constraint (LC-CRC) for the final\nclassification, and experimental results on three well-known public texture\ndatasets demonstrate the proposed approach is very competitive and even\noutperforms several state-of-the-art methods. Particularly, when only a few\nsamples of each category are available for training, our approach still\nachieves very high classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 15:15:50 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Kong", "Shu", ""], ["Wang", "Donghui", ""]]}, {"id": "1203.0518", "submitter": "Juli\\'an Urbano", "authors": "Juli\\'an Urbano, Diego Mart\\'in, M\\'onica Marrero, Jorge Morato", "title": "Overview of EIREX 2011: Crowdsourcing", "comments": "9 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The second Information Retrieval Education through EXperimentation track\n(EIREX 2011) was run at the University Carlos III of Madrid, during the 2011\nspring semester. EIREX 2011 is the second in a series of experiments designed\nto foster new Information Retrieval (IR) education methodologies and resources,\nwith the specific goal of teaching undergraduate IR courses from an\nexperimental perspective. For an introduction to the motivation behind the\nEIREX experiments, see the first sections of [Urbano et al., 2011a]. For\ninformation on other editions of EIREX and related data, see the website at\nhttp://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)\nto help students get a view of the Information Retrieval process as they would\nfind it in a real-world scenario, either industrial or academic; b) to make\nstudents realize the importance of laboratory experiments in Computer Science\nand have them initiated in their execution and analysis; c) to create a public\nrepository of resources to teach Information Retrieval courses; d) to seek the\ncollaboration and active participation of other Universities in this endeavor.\nThis overview paper summarizes the results of the EIREX 2011 track, focusing on\nthe creation of the test collection and the analysis to assess its reliability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 16:47:48 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Urbano", "Juli\u00e1n", ""], ["Mart\u00edn", "Diego", ""], ["Marrero", "M\u00f3nica", ""], ["Morato", "Jorge", ""]]}, {"id": "1203.0747", "submitter": "Marco Quartulli", "authors": "Marco Quartulli, Igor G. Olaizola", "title": "A review of EO image information mining", "comments": null, "journal-ref": "Quartulli, Marco, and Igor G Olaizola. \"A review of EO image\n  information mining.\" ISPRS Journal of Photogrammetry and Remote Sensing 75:\n  p11-28. 2013", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the state of the art of content-based retrieval in Earth\nobservation image archives focusing on complete systems showing promise for\noperational implementation. The different paradigms at the basis of the main\nsystem families are introduced. The approaches taken are analyzed, focusing in\nparticular on the phases after primitive feature extraction. The solutions\nenvisaged for the issues related to feature simplification and synthesis,\nindexing, semantic labeling are reviewed. The methodologies for query\nspecification and execution are analyzed.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 16:33:41 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 10:19:26 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Quartulli", "Marco", ""], ["Olaizola", "Igor G.", ""]]}, {"id": "1203.1005", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar and Rene Vidal", "title": "Sparse Subspace Clustering: Algorithm, Theory, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, we are dealing with collections of\nhigh-dimensional data, such as images, videos, text and web documents, DNA\nmicroarray data, and more. Often, high-dimensional data lie close to\nlow-dimensional structures corresponding to several classes or categories the\ndata belongs to. In this paper, we propose and study an algorithm, called\nSparse Subspace Clustering (SSC), to cluster data points that lie in a union of\nlow-dimensional subspaces. The key idea is that, among infinitely many possible\nrepresentations of a data point in terms of other points, a sparse\nrepresentation corresponds to selecting a few points from the same subspace.\nThis motivates solving a sparse optimization program whose solution is used in\na spectral clustering framework to infer the clustering of data into subspaces.\nSince solving the sparse optimization program is in general NP-hard, we\nconsider a convex relaxation and show that, under appropriate conditions on the\narrangement of subspaces and the distribution of data, the proposed\nminimization program succeeds in recovering the desired sparse representations.\nThe proposed algorithm can be solved efficiently and can handle data points\nnear the intersections of subspaces. Another key advantage of the proposed\nalgorithm with respect to the state of the art is that it can deal with data\nnuisances, such as noise, sparse outlying entries, and missing entries,\ndirectly by incorporating the model of the data into the sparse optimization\nprogram. We demonstrate the effectiveness of the proposed algorithm through\nexperiments on synthetic data as well as the two real-world problems of motion\nsegmentation and face clustering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 18:58:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 17:29:45 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2013 03:22:00 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Vidal", "Rene", ""]]}, {"id": "1203.1457", "submitter": "Olivier Fercoq", "authors": "Olivier Fercoq", "title": "PageRank optimization applied to spam detection", "comments": "8 pages, 6 figures", "journal-ref": "Proc. of the 6th International Conference on Network Games,\n  Control and Optimization (NetGCooP), Avignon, 2012, pp. 127-134", "doi": null, "report-no": null, "categories": "math.OC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new link spam detection and PageRank demotion algorithm called\nMaxRank. Like TrustRank and AntiTrustRank, it starts with a seed of hand-picked\ntrusted and spam pages. We define the MaxRank of a page as the frequency of\nvisit of this page by a random surfer minimizing an average cost per time unit.\nOn a given page, the random surfer selects a set of hyperlinks and clicks with\nuniform probability on any of these hyperlinks. The cost function penalizes\nspam pages and hyperlink removals. The goal is to determine a hyperlink\ndeletion policy that minimizes this score. The MaxRank is interpreted as a\nmodified PageRank vector, used to sort web pages instead of the usual PageRank\nvector. The bias vector of this ergodic control problem, which is unique up to\nan additive constant, is a measure of the \"spamicity\" of each page, used to\ndetect spam pages. We give a scalable algorithm for MaxRank computation that\nallowed us to perform experimental results on the WEBSPAM-UK2007 dataset. We\nshow that our algorithm outperforms both TrustRank and AntiTrustRank for spam\nand nonspam page detection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 12:54:44 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Fercoq", "Olivier", ""]]}, {"id": "1203.1793", "submitter": "Riadh Bouslimi", "authors": "Riadh Bouslimi, Jalel Akaichi", "title": "Using Hausdorff Distance for New Medical Image Annotation", "comments": "7 pages, 3 figures, 2 tables; International Journal of Database\n  Management Systems (IJDMS) Vol.4, No.1, February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images annotation is most of the time a repetitive hard task.\nCollecting old similar annotations and assigning them to new medical images may\nnot only enhance the annotation process, but also reduce ambiguity caused by\nrepetitive annotations. The goal of this work is to propose an approach based\non Hausdorff distance able to compute similarity between a new medical image\nand old stored images. User has to choose then one of the similar images and\nannotations related to the selected one are assigned to the new one.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 13:28:47 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Bouslimi", "Riadh", ""], ["Akaichi", "Jalel", ""]]}, {"id": "1203.2000", "submitter": "Soni madhulatha Tagaram", "authors": "T Soni Madhulatha", "title": "Overview of streaming-data algorithms", "comments": "10 pages", "journal-ref": "Advanced Computing: An International Journal ( ACIJ ), November\n  2011, Volume 2, Number 6 Advanced Computing: An International Journal ( ACIJ\n  ) ISSN : 2229 - 6727 [Online] ; 2229 - 726X [Print]", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Due to recent advances in data collection techniques, massive amounts of data\nare being collected at an extremely fast pace. Also, these data are potentially\nunbounded. Boundless streams of data collected from sensors, equipments, and\nother data sources are referred to as data streams. Various data mining tasks\ncan be performed on data streams in search of interesting patterns. This paper\nstudies a particular data mining task, clustering, which can be used as the\nfirst step in many knowledge discovery processes. By grouping data streams into\nhomogeneous clusters, data miners can learn about data characteristics which\ncan then be developed into classification models for new data or predictive\nmodels for unknown events. Recent research addresses the problem of data-stream\nmining to deal with applications that require processing huge amounts of data\nsuch as sensor data analysis and financial applications. For such analysis,\nsingle-pass algorithms that consume a small amount of memory are critical.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 06:59:40 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Madhulatha", "T Soni", ""]]}, {"id": "1203.2021", "submitter": "Sylvain Lespinats", "authors": "Sylvain Lespinats, Anke Meyer-Baese, Michael Aupetit", "title": "A new supervised non-linear mapping", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised mapping methods project multi-dimensional labeled data onto a\n2-dimensional space attempting to preserve both data similarities and topology\nof classes. Supervised mappings are expected to help the user to understand the\nunderlying original class structure and to classify new data visually. Several\nmethods have been designed to achieve supervised mapping, but many of them\nmodify original distances prior to the mapping so that original data\nsimilarities are corrupted and even overlapping classes tend to be separated\nonto the map ignoring their original topology. We propose ClassiMap, an\nalternative method for supervised mapping. Mappings come with distortions which\ncan be split between tears (close points mapped far apart) and false\nneighborhoods (points far apart mapped as neighbors). Some mapping methods\nfavor the former while others favor the latter. ClassiMap switches between such\nmapping methods so that tears tend to appear between classes and false\nneighborhood within classes, better preserving classes' topology. We also\npropose two new objective criteria instead of the usual subjective visual\ninspection to perform fair comparisons of supervised mapping methods. ClassiMap\nappears to be the best supervised mapping method according to these criteria in\nour experiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 09:15:43 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Lespinats", "Sylvain", ""], ["Meyer-Baese", "Anke", ""], ["Aupetit", "Michael", ""]]}, {"id": "1203.2293", "submitter": "Jose Fontanari", "authors": "Sergey Petrov, Jose F. Fontanari and Leonid I. Perlovsky", "title": "Categories of Emotion names in Web retrieved texts", "comments": null, "journal-ref": "International Journal of Psychology and Behavioral Sciences 2\n  (2012) 173-184", "doi": "10.5923/j.ijpbs.20120205.08", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The categorization of emotion names, i.e., the grouping of emotion words that\nhave similar emotional connotations together, is a key tool of Social\nPsychology used to explore people's knowledge about emotions. Without\nexception, the studies following that research line were based on the gauging\nof the perceived similarity between emotion names by the participants of the\nexperiments. Here we propose and examine a new approach to study the categories\nof emotion names - the similarities between target emotion names are obtained\nby comparing the contexts in which they appear in texts retrieved from the\nWorld Wide Web. This comparison does not account for any explicit semantic\ninformation; it simply counts the number of common words or lexical items used\nin the contexts. This procedure allows us to write the entries of the\nsimilarity matrix as dot products in a linear vector space of contexts. The\nproperties of this matrix were then explored using Multidimensional Scaling\nAnalysis and Hierarchical Clustering. Our main findings, namely, the underlying\ndimension of the emotion space and the categories of emotion names, were\nconsistent with those based on people's judgments of emotion names\nsimilarities.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 00:04:57 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Petrov", "Sergey", ""], ["Fontanari", "Jose F.", ""], ["Perlovsky", "Leonid I.", ""]]}, {"id": "1203.2569", "submitter": "Massimo Melucci", "authors": "Massimo Melucci", "title": "When Index Term Probability Violates the Classical Probability Axioms\n  Quantum Probability can be a Necessary Theory for Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models require the notion of event space for defining a\nprobability measure. An event space has a probability measure which ensues the\nKolmogorov axioms. However, the probabilities observed from distinct sources,\nsuch as that of relevance of documents, may not admit a single event space thus\ncausing some issues. In this article, some results are introduced for ensuring\nwhether the observed prob- abilities of relevance of documents admit a single\nevent space. More- over, an alternative framework of probability is introduced,\nthus chal- lenging the use of classical probability for ranking documents. Some\nreflections on the convenience of extending the classical probabilis- tic\nretrieval toward a more general framework which encompasses the issues are\nmade.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 17:57:40 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Melucci", "Massimo", ""]]}, {"id": "1203.3463", "submitter": "Amr Ahmed", "authors": "Amr Ahmed, Eric P. Xing", "title": "Timeline: A Dynamic Hierarchical Dirichlet Process Model for Recovering\n  Birth/Death and Evolution of Topics in Text Stream", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-20-29", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have proven to be a useful tool for discovering latent\nstructures in document collections. However, most document collections often\ncome as temporal streams and thus several aspects of the latent structure such\nas the number of topics, the topics' distribution and popularity are\ntime-evolving. Several models exist that model the evolution of some but not\nall of the above aspects. In this paper we introduce infinite dynamic topic\nmodels, iDTM, that can accommodate the evolution of all the aforementioned\naspects. Our model assumes that documents are organized into epochs, where the\ndocuments within each epoch are exchangeable but the order between the\ndocuments is maintained across epochs. iDTM allows for unbounded number of\ntopics: topics can die or be born at any epoch, and the representation of each\ntopic can evolve according to a Markovian dynamics. We use iDTM to analyze the\nbirth and evolution of topics in the NIPS community and evaluated the efficacy\nof our model on both simulated and real datasets with favorable outcome.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ahmed", "Amr", ""], ["Xing", "Eric P.", ""]]}, {"id": "1203.3535", "submitter": "Yu Zhang", "authors": "Yu Zhang, Bin Cao, Dit-Yan Yeung", "title": "Multi-Domain Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-725-732", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is an effective recommendation approach in which the\npreference of a user on an item is predicted based on the preferences of other\nusers with similar interests. A big challenge in using collaborative filtering\nmethods is the data sparsity problem which often arises because each user\ntypically only rates very few items and hence the rating matrix is extremely\nsparse. In this paper, we address this problem by considering multiple\ncollaborative filtering tasks in different domains simultaneously and\nexploiting the relationships between domains. We refer to it as a multi-domain\ncollaborative filtering (MCF) problem. To solve the MCF problem, we propose a\nprobabilistic framework which uses probabilistic matrix factorization to model\nthe rating problem in each domain and allows the knowledge to be adaptively\ntransferred across different domains by automatically learning the correlation\nbetween domains. We also introduce the link function for different domains to\ncorrect their biases. Experiments conducted on several real-world applications\ndemonstrate the effectiveness of our methods when compared with some\nrepresentative methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Yu", ""], ["Cao", "Bin", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1203.3586", "submitter": "Mohsen Pourvali", "authors": "Mohsen Pourvali and Mohammad Saniee Abadeh", "title": "Automated Text Summarization Base on Lexicales Chain and graph Using of\n  WordNet and Wikipedia Knowledge Base", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 3, January 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The technology of automatic document summarization is maturing and may\nprovide a solution to the information overload problem. Nowadays, document\nsummarization plays an important role in information retrieval. With a large\nvolume of documents, presenting the user with a summary of each document\ngreatly facilitates the task of finding the desired documents. Document\nsummarization is a process of automatically creating a compressed version of a\ngiven document that provides useful information to users, and multi-document\nsummarization is to produce a summary delivering the majority of information\ncontent from a set of documents about an explicit or implicit main topic. The\nlexical cohesion structure of the text can be exploited to determine the\nimportance of a sentence/phrase. Lexical chains are useful tools to analyze the\nlexical cohesion structure in a text .In this paper we consider the effect of\nthe use of lexical cohesion features in Summarization, And presenting a\nalgorithm base on the knowledge base. Ours algorithm at first find the correct\nsense of any word, Then constructs the lexical chains, remove Lexical chains\nthat less score than other, detects topics roughly from lexical chains,\nsegments the text with respect to the topics and selects the most important\nsentences. The experimental results on an open benchmark datasets from DUC01\nand DUC02 show that our proposed approach can improve the performance compared\nto sate-of-the-art summarization approaches.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 22:56:29 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Pourvali", "Mohsen", ""], ["Abadeh", "Mohammad Saniee", ""]]}, {"id": "1203.3589", "submitter": "Eya Ben Ahmed", "authors": "Eya Ben Ahmed, Ahlem Nabli, Fa\\\"iez Gargouri", "title": "Building MultiView Analyst Profile From Multidimensional Query Logs:\n  From Consensual to Conflicting Preferences", "comments": "8 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 1, No 2, January 2012 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In order to provide suitable results to the analyst needs, user preferences\nsummarization is widely used in several domains. In this paper, we introduce a\nnew approach for user profile construction from OLAP query logs. The key idea\nis to learn the user's preferences by drawing the evidence from OLAP logs. In\nfact, the analyst preferences are clustered into three main pools : (i)\nconsensual or non conflicting preferences referring to same preferences for all\nanalysts; (ii) semi-conflicting preferences corresponding to similar\npreferences for some analysts; (iii) conflicting preferences related to\ndisjoint preferences for all analysts. To build generic and global model\naccurately describing the analyst, we enrich the obtained characteristics\nthrough including several views, namely the personal view, the professional\nview and the behavioral view. After that, the multiview profile extracted from\nmultidimensional database can be annotated.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 23:22:47 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ahmed", "Eya Ben", ""], ["Nabli", "Ahlem", ""], ["Gargouri", "Fa\u00efez", ""]]}, {"id": "1203.3764", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Antarip Biswas, Sumit Das, Saurav Nag and Rajiv Pratap", "title": "The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient\n  Experiences to Big Data to Insights", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": "ABZ-TR-2012-1", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a technology to connect patients to information in the\nexperiences of other patients by using the power of structured big data. The\napproach, implemented in the Abzooba Smart Health Informatics Platform\n(SHIP),is to distill concepts of facts and expressions from conversations and\ndiscussions in health social media forums, and use those distilled concepts in\nconnecting patients to experiences and insights that are highly relevant to\nthem in particular. We envision our work, in progress, to provide new and\neffective tools to exploit the richness of content in social media in health\nfor outcomes research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 16:59:03 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ashish", "Naveen", ""], ["Biswas", "Antarip", ""], ["Das", "Sumit", ""], ["Nag", "Saurav", ""], ["Pratap", "Rajiv", ""]]}, {"id": "1203.4487", "submitter": "Frank Meyer", "authors": "Frank Meyer", "title": "Recommender systems in industrial contexts", "comments": "version 3.30, May 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis consists of four parts: - An analysis of the core functions and\nthe prerequisites for recommender systems in an industrial context: we identify\nfour core functions for recommendation systems: Help do Decide, Help to\nCompare, Help to Explore, Help to Discover. The implementation of these\nfunctions has implications for the choices at the heart of algorithmic\nrecommender systems. - A state of the art, which deals with the main techniques\nused in automated recommendation system: the two most commonly used algorithmic\nmethods, the K-Nearest-Neighbor methods (KNN) and the fast factorization\nmethods are detailed. The state of the art presents also purely content-based\nmethods, hybridization techniques, and the classical performance metrics used\nto evaluate the recommender systems. This state of the art then gives an\noverview of several systems, both from academia and industry (Amazon, Google\n...). - An analysis of the performances and implications of a recommendation\nsystem developed during this thesis: this system, Reperio, is a hybrid\nrecommender engine using KNN methods. We study the performance of the KNN\nmethods, including the impact of similarity functions used. Then we study the\nperformance of the KNN method in critical uses cases in cold start situation. -\nA methodology for analyzing the performance of recommender systems in\nindustrial context: this methodology assesses the added value of algorithmic\nstrategies and recommendation systems according to its core functions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 16:14:42 GMT"}, {"version": "v2", "created": "Mon, 14 May 2012 10:00:22 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Meyer", "Frank", ""]]}, {"id": "1203.4494", "submitter": "Piero Giacomelli", "authors": "Piero Giacomelli, Giulia Munaro and Roberto Rosso", "title": "Can an Ad-hoc ontology Beat a Medical Search Engine? The Chronious\n  Search Engine case", "comments": "presented at the The Fourth International Conference on eHealth,\n  Telemedicine, and Social Medicine (eTELEMED2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chronious is an Open, Ubiquitous and Adaptive Chronic Disease Management\nPlatform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease\n(CKD) and Renal Insufficiency. It consists of several modules: an ontology\nbased literature search engine, a rule based decision support system, remote\nsensors interacting with lifestyle interfaces (PDA, monitor touch-screen) and a\nmachine learning module. All these modules interact each other to allow the\nmonitoring of two types of chronic diseases and to help clinician in taking\ndecision for care purpose. This paper illustrates how the ontology search\nengine was created and fed and how some comparative test indicated that the\nontology based approach give better results, on some estimation parameters,\nthan the main reference web search engine.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 16:28:54 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Giacomelli", "Piero", ""], ["Munaro", "Giulia", ""], ["Rosso", "Roberto", ""]]}, {"id": "1203.5084", "submitter": "Leon Derczynski", "authors": "Leon Derczynski, Jun Wang, Robert Gaizauskas and Mark A. Greenwood", "title": "A Data Driven Approach to Query Expansion in Question Answering", "comments": null, "journal-ref": "Proc. IR4QA Workshop (2008) 34-41", "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Automated answering of natural language questions is an interesting and\nuseful problem to solve. Question answering (QA) systems often perform\ninformation retrieval at an initial stage. Information retrieval (IR)\nperformance, provided by engines such as Lucene, places a bound on overall\nsystem performance. For example, no answer bearing documents are retrieved at\nlow ranks for almost 40% of questions.\n  In this paper, answer texts from previous QA evaluations held as part of the\nText REtrieval Conferences (TREC) are paired with queries and analysed in an\nattempt to identify performance-enhancing words. These words are then used to\nevaluate the performance of a query expansion method.\n  Data driven extension words were found to help in over 70% of difficult\nquestions. These words can be used to improve and evaluate query expansion\nmethods. Simple blind relevance feedback (RF) was correctly predicted as\nunlikely to help overall performance, and an possible explanation is provided\nfor its low value in IR for QA.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 19:19:02 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Derczynski", "Leon", ""], ["Wang", "Jun", ""], ["Gaizauskas", "Robert", ""], ["Greenwood", "Mark A.", ""]]}, {"id": "1203.5188", "submitter": "Martin Monperrus", "authors": "Stefan Hen{\\ss}, Martin Monperrus (INRIA Lille - Nord Europe), Mira\n  Mezini", "title": "Semi-Automatically Extracting FAQs to Improve Accessibility of Software\n  Development Knowledge", "comments": "ICSE - 34th International Conference on Software Engineering (2012)", "journal-ref": "ICSE - 34th International Conference on Software Engineering, 2012", "doi": "10.1109/ICSE.2012.6227139", "report-no": null, "categories": "cs.SE cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequently asked questions (FAQs) are a popular way to document software\ndevelopment knowledge. As creating such documents is expensive, this paper\npresents an approach for automatically extracting FAQs from sources of software\ndevelopment discussion, such as mailing lists and Internet forums, by combining\ntechniques of text mining and natural language processing. We apply the\napproach to popular mailing lists and carry out a survey among software\ndevelopers to show that it is able to extract high-quality FAQs that may be\nfurther improved by experts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 07:13:06 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Hen\u00df", "Stefan", "", "INRIA Lille - Nord Europe"], ["Monperrus", "Martin", "", "INRIA Lille - Nord Europe"], ["Mezini", "Mira", ""]]}, {"id": "1203.5324", "submitter": "Paula Cristina Vaz", "authors": "Paula Cristina Vaz, David Martins de Matos, Bruno Martins, Pavel\n  Calado", "title": "Improving an Hybrid Literary Book Recommendation System through Author\n  Ranking", "comments": "Submitted to JCDL 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literary reading is an important activity for individuals and choosing to\nread a book can be a long time commitment, making book choice an important task\nfor book lovers and public library users. In this paper we present an hybrid\nrecommendation system to help readers decide which book to read next. We study\nbook and author recommendation in an hybrid recommendation setting and test our\napproach in the LitRec data set. Our hybrid book recommendation approach\npurposed combines two item-based collaborative filtering algorithms to predict\nbooks and authors that the user will like. Author predictions are expanded in\nto a book list that is subsequently aggregated with the former list generated\nthrough the initial collaborative recommender. Finally, the resulting book list\nis used to yield the top-n book recommendations. By means of various\nexperiments, we demonstrate that author recommendation can improve overall book\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 19:28:25 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Vaz", "Paula Cristina", ""], ["de Matos", "David Martins", ""], ["Martins", "Bruno", ""], ["Calado", "Pavel", ""]]}, {"id": "1203.5415", "submitter": "Xiaofeng Liao", "authors": "Yongji Wang, Xiaofeng Liao, Hu Wu, Jingzheng Wu", "title": "Incremental Collaborative Filtering Considering Temporal Effects", "comments": "18 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems require their recommendation algorithms to be accurate,\nscalable and should handle very sparse training data which keep changing over\ntime. Inspired by ant colony optimization, we propose a novel collaborative\nfiltering scheme: Ant Collaborative Filtering that enjoys those favorable\ncharacteristics above mentioned. With the mechanism of pheromone transmission\nbetween users and items, our method can pinpoint most relative users and items\neven in face of the sparsity problem. By virtue of the evaporation of existing\npheromone, we capture the evolution of user preference over time. Meanwhile,\nthe computation complexity is comparatively small and the incremental update\ncan be done online. We design three experiments on three typical recommender\nsystems, namely movie recommendation, book recommendation and music\nrecommendation, which cover both explicit and implicit rating data. The results\nshow that the proposed algorithm is well suited for real-world recommendation\nscenarios which have a high throughput and are time sensitive.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 14:14:00 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Wang", "Yongji", ""], ["Liao", "Xiaofeng", ""], ["Wu", "Hu", ""], ["Wu", "Jingzheng", ""]]}, {"id": "1203.6093", "submitter": "Santo Fortunato Prof.", "authors": "Andrea Lancichinetti, Santo Fortunato", "title": "Consensus clustering in complex networks", "comments": "11 pages, 12 figures. Published in Scientific Reports", "journal-ref": "Scientific Reports 2, 336 (2012)", "doi": "10.1038/srep00336", "report-no": null, "categories": "physics.soc-ph cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The community structure of complex networks reveals both their organization\nand hidden relationships among their constituents. Most community detection\nmethods currently available are not deterministic, and their results typically\ndepend on the specific random seeds, initial conditions and tie-break rules\nadopted for their execution. Consensus clustering is used in data analysis to\ngenerate stable results out of a set of partitions delivered by stochastic\nmethods. Here we show that consensus clustering can be combined with any\nexisting method in a self-consistent way, enhancing considerably both the\nstability and the accuracy of the resulting partitions. This framework is also\nparticularly suitable to monitor the evolution of community structure in\ntemporal networks. An application of consensus clustering to a large citation\nnetwork of physics papers demonstrates its capability to keep track of the\nbirth, death and diversification of topics.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 21:38:16 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Lancichinetti", "Andrea", ""], ["Fortunato", "Santo", ""]]}, {"id": "1203.6098", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi and David F. Gleich", "title": "Dynamic PageRank using Evolving Teleportation", "comments": "WAW 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR math.DS physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of nodes in a network constantly fluctuates based on changes\nin the network structure as well as changes in external interest. We propose an\nevolving teleportation adaptation of the PageRank method to capture how changes\nin external interest influence the importance of a node. This framework\nseamlessly generalizes PageRank because the importance of a node will converge\nto the PageRank values if the external influence stops changing. We demonstrate\nthe effectiveness of the evolving teleportation on the Wikipedia graph and the\nTwitter social network. The external interest is given by the number of hourly\nvisitors to each page and the number of monthly tweets for each user.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 22:44:56 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Gleich", "David F.", ""]]}, {"id": "1203.6339", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Intelligent Interface Architectures for Folksonomy Driven Structure\n  Network", "comments": "*** This paper has been accepted to the 5th International Workshop on\n  Intelligent Interfaces for Human-Computer Interaction (IIHCI 2012) - Palermo\n  Italy, 4-6 July 2012 *** 7 pages, 7 figures; for details see:\n  http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The folksonomy is the result of free personal information or assignment of\ntags to an object (determined by the URI) in order to find them. The practice\nof tagging is done in a collective environment. Folksonomies are self\nconstructed, based on co-occurrence of definitions, rather than a hierarchical\nstructure of the data. The downside of this was that a few sites and\napplications are able to successfully exploit the sharing of bookmarks. The\nneed for tools that are able to resolve the ambiguity of the definitions is\nbecoming urgent as the need of simple instruments for their visualization,\nediting and exploitation in web applications still hinders their diffusion and\nwide adoption. An intelligent interactive interface design for folksonomies\nshould consider the contextual design and inquiry based on a concurrent\ninteraction for a perceptual user interfaces. To represent folksonomies a new\nconcept structure called \"Folksodriven\" is used in this paper. While it is\npresented the Folksodriven Structure Network (FSN) to resolve the ambiguity of\ndefinitions of folksonomy tags suggestions for the user. On this base a\nHuman-Computer Interactive (HCI) systems is developed for the visualization,\nnavigation, updating and maintenance of folksonomies Knowledge Bases - the FSN\n- through the web. System functionalities as well as its internal architecture\nwill be introduced.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 18:59:40 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1203.6397", "submitter": "Yuli Ye", "authors": "Allan Borodin, Aadhar Jain, Hyun Chul Lee and Yuli Ye", "title": "Max-Sum Diversification, Monotone Submodular Functions and Dynamic\n  Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Result diversification is an important aspect in web-based search, document\nsummarization, facility location, portfolio management and other applications.\nGiven a set of ranked results for a set of objects (e.g. web documents,\nfacilities, etc.) with a distance between any pair, the goal is to select a\nsubset $S$ satisfying the following three criteria: (a) the subset $S$\nsatisfies some constraint (e.g. bounded cardinality); (b) the subset contains\nresults of high \"quality\"; and (c) the subset contains results that are\n\"diverse\" relative to the distance measure. The goal of result diversification\nis to produce a diversified subset while maintaining high quality as much as\npossible. We study a broad class of problems where the distances are a metric,\nwhere the constraint is given by independence in a matroid, where quality is\ndetermined by a monotone submodular function, and diversity is defined as the\nsum of distances between objects in $S$. Our problem is a generalization of the\n{\\em max sum diversification} problem studied in \\cite{GoSh09} which in turn is\na generaliztion of the {\\em max sum $p$-dispersion problem} studied extensively\nin location theory. It is NP-hard even with the triangle inequality. We propose\ntwo simple and natural algorithms: a greedy algorithm for a cardinality\nconstraint and a local search algorithm for an arbitary matroid constraint. We\nprove that both algorithms achieve constant approximation ratios.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 23:56:46 GMT"}, {"version": "v2", "created": "Wed, 20 Aug 2014 04:24:58 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 03:48:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Borodin", "Allan", ""], ["Jain", "Aadhar", ""], ["Lee", "Hyun Chul", ""], ["Ye", "Yuli", ""]]}, {"id": "1203.6845", "submitter": "Juli\\'an Urbano", "authors": "M\\'onica Marrero, Sonia S\\'anchez-Cuadrado, Juli\\'an Urbano, Jorge\n  Morato, Jos\\'e-Antonio Moreiro", "title": "Information Retrieval Systems Adapted to the Biomedical Domain", "comments": "6 pages, 4 tables", "journal-ref": "El Profesional de la Informaci\\'on (2010), 19-3", "doi": "10.3145/epi.2010.may.04", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The terminology used in Biomedicine shows lexical peculiarities that have\nrequired the elaboration of terminological resources and information retrieval\nsystems with specific functionalities. The main characteristics are the high\nrates of synonymy and homonymy, due to phenomena such as the proliferation of\npolysemic acronyms and their interaction with common language. Information\nretrieval systems in the biomedical domain use techniques oriented to the\ntreatment of these lexical peculiarities. In this paper we review some of the\ntechniques used in this domain, such as the application of Natural Language\nProcessing (BioNLP), the incorporation of lexical-semantic resources, and the\napplication of Named Entity Recognition (BioNER). Finally, we present the\nevaluation methods adopted to assess the suitability of these techniques for\nretrieving biomedical resources.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 16:16:43 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Marrero", "M\u00f3nica", ""], ["S\u00e1nchez-Cuadrado", "Sonia", ""], ["Urbano", "Juli\u00e1n", ""], ["Morato", "Jorge", ""], ["Moreiro", "Jos\u00e9-Antonio", ""]]}]