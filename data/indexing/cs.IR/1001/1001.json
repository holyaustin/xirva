[{"id": "1001.0440", "submitter": "Rajkumar Kannan", "authors": "Rajkumar Kannan, Frederic Andres, Balakrishnan Ramadoss", "title": "Tutoring System for Dance Learning", "comments": "IEEE International Advance Computing Conference 2009, Patiala, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in hardware sophistication related to graphics display, audio\nand video devices made available a large number of multimedia and hypermedia\napplications. These multimedia applications need to store and retrieve the\ndifferent forms of media like text, hypertext, graphics, still images,\nanimations, audio and video. Dance is one of the important cultural forms of a\nnation and dance video is one such multimedia types. Archiving and retrieving\nthe required semantics from these dance media collections is a crucial and\ndemanding multimedia application. This paper summarizes the difference dance\nvideo archival techniques and systems. Keywords: Multimedia, Culture Media,\nMetadata archival and retrieval systems, MPEG-7, XML.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2010 05:24:31 GMT"}], "update_date": "2010-01-05", "authors_parsed": [["Kannan", "Rajkumar", ""], ["Andres", "Frederic", ""], ["Ramadoss", "Balakrishnan", ""]]}, {"id": "1001.0700", "submitter": "Amit Belani", "authors": "Amit Belani", "title": "Vandalism Detection in Wikipedia: a Bag-of-Words Classifier Approach", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bag-of-words based probabilistic classifier is trained using regularized\nlogistic regression to detect vandalism in the English Wikipedia. Isotonic\nregression is used to calibrate the class membership probabilities. Learning\ncurve, reliability, ROC, and cost analysis are performed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2010 13:06:21 GMT"}], "update_date": "2010-01-06", "authors_parsed": [["Belani", "Amit", ""]]}, {"id": "1001.0827", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Shlomo Geva", "title": "Document Clustering with K-tree", "comments": "12 pages, INEX 2008", "journal-ref": null, "doi": "10.1007/978-3-642-03761-0_43", "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the approach taken to the XML Mining track at INEX 2008\nby a group at the Queensland University of Technology. We introduce the K-tree\nclustering algorithm in an Information Retrieval context by adapting it for\ndocument clustering. Many large scale problems exist in document clustering.\nK-tree scales well with large inputs due to its low complexity. It offers\npromising results both in terms of efficiency and quality. Document\nclassification was completed using Support Vector Machines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 07:51:23 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["De Vries", "Christopher M.", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.0830", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Shlomo Geva", "title": "K-tree: Large Scale Document Clustering", "comments": "2 pages, SIGIR 2009", "journal-ref": null, "doi": "10.1145/1571941.1572094", "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce K-tree in an information retrieval context. It is an efficient\napproximation of the k-means clustering algorithm. Unlike k-means it forms a\nhierarchy of clusters. It has been extended to address issues with sparse\nrepresentations. We compare performance and quality to CLUTO using document\ncollections. The K-tree has a low time complexity that is suitable for large\ndocument collections. This tree structure allows for efficient disk based\nimplementations where space requirements exceed that of main memory.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 07:43:31 GMT"}], "update_date": "2010-01-07", "authors_parsed": [["De Vries", "Christopher M.", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.0833", "submitter": "Chris De Vries", "authors": "Christopher M. De Vries and Lance De Vine and Shlomo Geva", "title": "Random Indexing K-tree", "comments": "8 pages, ADCS 2009; Hyperref and cleveref LaTeX packages conflicted.\n  Removed cleveref", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Indexing (RI) K-tree is the combination of two algorithms for\nclustering. Many large scale problems exist in document clustering. RI K-tree\nscales well with large inputs due to its low complexity. It also exhibits\nfeatures that are useful for managing a changing collection. Furthermore, it\nsolves previous issues with sparse document vectors when using K-tree. The\nalgorithms and data structures are defined, explained and motivated. Specific\nmodifications to K-tree are made for use with RI. Experiments have been\nexecuted to measure quality. The results indicate that RI K-tree improves\ndocument cluster quality over the original K-tree algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2010 08:03:20 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2010 02:46:22 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["De Vries", "Christopher M.", ""], ["De Vine", "Lance", ""], ["Geva", "Shlomo", ""]]}, {"id": "1001.1143", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff", "title": "Redundancy in Systems which Entertain a Model of Themselves: Interaction\n  Information and the Self-organization of Anticipation", "comments": null, "journal-ref": "Entropy 12(1) (2010) 63-79", "doi": "10.3390/e12010063", "report-no": null, "categories": "cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual information among three or more dimensions (mu-star = - Q) has been\nconsidered as interaction information. However, Krippendorff (2009a, 2009b) has\nshown that this measure cannot be interpreted as a unique property of the\ninteractions and has proposed an alternative measure of interaction information\nbased on iterative approximation of maximum entropies. Q can then be considered\nas a measure of the difference between interaction information and redundancy\ngenerated in a model entertained by an observer. I argue that this provides us\nwith a measure of the imprint of a second-order observing system -- a model\nentertained by the system itself -- on the underlying information processing.\nThe second-order system communicates meaning hyper-incursively; an observation\ninstantiates this meaning-processing within the information processing. The net\nresults may add to or reduce the prevailing uncertainty. The model is tested\nempirically for the case where textual organization can be expected to contain\nintellectual organization in terms of distributions of title words, author\nnames, and cited references.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2010 19:45:41 GMT"}], "update_date": "2010-01-08", "authors_parsed": [["Leydesdorff", "Loet", ""]]}, {"id": "1001.1320", "submitter": "Loet Leydesdorff", "authors": "Gaston Heimeriks, Loet Leydesdorff, Peter Van den Besselaar", "title": "Distributed scientific communication in the European information\n  society: Some cases of \"Mode 2\" fields of research", "comments": null, "journal-ref": "Paper presented at Science & Technology Indicators Conference,\n  Leiden, May 2000", "doi": null, "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can self-organization of scientific communication be specified by using\nliterature-based indicators? In this study, we explore this question by\napplying entropy measures to typical \"Mode-2\" fields of knowledge production.\nWe hypothesized these scientific systems to be developing from a\nself-organization of the interaction between cognitive and institutional\nlevels: European subsidized research programs aim at creating an institutional\nnetwork, while a cognitive reorganization is continuously ongoing at the\nscientific field level. The results indicate that the European system develops\ntowards a stable level of distribution of cited references and title-words\namong the European member states. We suggested that this distribution could be\na property of the emerging European system. In order to measure to degree of\nspecialization with respect to the respective distributions of countries, cited\nreferences and title words, the mutual information among the three frequency\ndistributions was calculated. The so-called transmission values informed us\nthat the European system shows increasing levels of differentiation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2010 17:01:06 GMT"}], "update_date": "2010-01-11", "authors_parsed": [["Heimeriks", "Gaston", ""], ["Leydesdorff", "Loet", ""], ["Besselaar", "Peter Van den", ""]]}, {"id": "1001.1685", "submitter": "Jacek Gwizdka", "authors": "Jacek Gwizdka", "title": "Assessing Cognitive Load on Web Search Tasks", "comments": "Published in the special issue Hot Topic: Cognition and the Web", "journal-ref": "Ergonomics Open Journal 2, 2009, 114-123", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing cognitive load on web search is useful for characterizing search\nsystem features and search tasks with respect to their demands on the\nsearcher's mental effort. It is also helpful for examining how individual\ndifferences among searchers (e.g. cognitive abilities) affect the search\nprocess. We examined cognitive load from the perspective of primary and\nsecondary task performance. A controlled web search study was conducted with 48\nparticipants. The primary task performance components were found to be\nsignificantly related to both the objective and the subjective task difficulty.\nHowever, the relationship between objective and subjective task difficulty and\nthe secondary task performance measures was weaker than expected. The results\nindicate that the dual-task approach needs to be used with caution.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2010 16:15:29 GMT"}], "update_date": "2010-01-12", "authors_parsed": [["Gwizdka", "Jacek", ""]]}, {"id": "1001.1988", "submitter": "Rdv Ijcsis", "authors": "P. Rajendran, M. Madheswaran", "title": "An Improved Image Mining Technique For Brain Tumour Classification Using\n  Efficient Classifier", "comments": "10 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS December 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 3, pp. 107-116, December 2009, USA", "doi": null, "report-no": "Volume 6, No. 3, ISSN 1947 5500", "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An improved image mining technique for brain tumor classification using\npruned association rule with MARI algorithm is presented in this paper. The\nmethod proposed makes use of association rule mining technique to classify the\nCT scan brain images into three categories namely normal, benign and malign. It\ncombines the low level features extracted from images and high level knowledge\nfrom specialists. The developed algorithm can assist the physicians for\nefficient classification with multiple keywords per image to improve the\naccuracy. The experimental result on prediagnosed database of brain images\nshowed 96 percent and 93 percent sensitivity and accuracy respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2010 19:30:10 GMT"}], "update_date": "2010-01-13", "authors_parsed": [["Rajendran", "P.", ""], ["Madheswaran", "M.", ""]]}, {"id": "1001.2186", "submitter": "Tao Zhou", "authors": "Luo-Luo Jiang, Matus Medo, Joseph R. Wakeling, Yi-Cheng Zhang, Tao\n  Zhou", "title": "Building reputation systems for better ranking", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to rank web pages, scientists and online resources has recently attracted\nincreasing attention from both physicists and computer scientists. In this\npaper, we study the ranking problem of rating systems where users vote objects\nby discrete ratings. We propose an algorithm that can simultaneously evaluate\nthe user reputation and object quality in an iterative refinement way.\nAccording to both the artificially generated data and the real data from\nMovieLens and Amazon, our algorithm can considerably enhance the ranking\naccuracy. This work highlights the significance of reputation systems in the\nInternet era and points out a way to evaluate and compare the performances of\ndifferent reputation systems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2010 14:48:05 GMT"}], "update_date": "2010-01-14", "authors_parsed": [["Jiang", "Luo-Luo", ""], ["Medo", "Matus", ""], ["Wakeling", "Joseph R.", ""], ["Zhang", "Yi-Cheng", ""], ["Zhou", "Tao", ""]]}, {"id": "1001.2270", "submitter": "Rdv Ijcsis", "authors": "Rajesh Kumar Boora, Ruchi Shukla, A. K. Misra", "title": "An Improved Approach to High Level Privacy Preserving Itemset Mining", "comments": "8 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS December 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 3, pp. 216-223, December 2009, USA", "doi": null, "report-no": "Volume 6, No. 3, ISSN 1947 5500", "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preserving association rule mining has triggered the development of\nmany privacy preserving data mining techniques. A large fraction of them use\nrandomized data distortion techniques to mask the data for preserving. This\npaper proposes a new transaction randomization method which is a combination of\nthe fake transaction randomization method and a new per transaction\nrandomization method. This method distorts the items within each transaction\nand ensures a higher level of data privacy in comparison to the previous\napproaches. The pertransaction randomization method involves a randomization\nfunction to replace the item by a random number guarantying privacy within the\ntransaction also. A tool has also been developed to implement the proposed\napproach to mine frequent itemsets and association rules from the data\nguaranteeing the antimonotonic property.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2010 19:08:49 GMT"}], "update_date": "2010-01-14", "authors_parsed": [["Boora", "Rajesh Kumar", ""], ["Shukla", "Ruchi", ""], ["Misra", "A. K.", ""]]}, {"id": "1001.3277", "submitter": "Rdv Ijcsis", "authors": "Sugam Sharma, Tzusheng Pei, and Hari Cohly", "title": "On Utilization and Importance of Perl Status Reporter (SRr) in Text\n  Mining", "comments": "7 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS December 2009, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/; Volume 6, No. 3, ISSN 1947 5500", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 6, No. 3, pp. 253-259, December 2009, USA", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bioinformatics, text mining and text data mining sometimes interchangeably\nused is a process to derive high-quality information from text. Perl Status\nReporter (SRr) is a data fetching tool from a flat text file and in this\nresearch paper we illustrate the use of SRr in text or data mining. SRr needs a\nflat text input file where the mining process to be performed. SRr reads input\nfile and derives the high quality information from it. Typically text mining\ntasks are text categorization, text clustering, concept and entity extraction,\nand document summarization. SRr can be utilized for any of these tasks with\nlittle or none customizing efforts. In our implementation we perform text\ncategorization mining operation on input file. The input file has two\nparameters of interest (firstKey and secondKey). The composition of these two\nparameters describes the uniqueness of entries in that file in the similar\nmanner as done by composite key in database. SRr reads the input file line by\nline and extracts the parameters of interest and form a composite key by\njoining them together. It subsequently generates an output file consisting of\nthe name as firstKey secondKey. SRr reads the input file and tracks the\ncomposite key. It further stores all that data lines, having the same composite\nkey, in output file generated by SRr based on that composite key.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2010 12:16:55 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2010 21:21:22 GMT"}], "update_date": "2010-01-21", "authors_parsed": [["Sharma", "Sugam", ""], ["Pei", "Tzusheng", ""], ["Cohly", "Hari", ""]]}, {"id": "1001.3745", "submitter": "Mat\\'u\\v{s} Medo", "authors": "Matus Medo and Joseph Rushton Wakeling", "title": "The effect of discrete vs. continuous-valued ratings on reputation and\n  ranking systems", "comments": "6 pages, 2 figures", "journal-ref": "EPL 91, 48004, 2010", "doi": "10.1209/0295-5075/91/48004", "report-no": null, "categories": "cs.IR cs.AI cs.DB physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When users rate objects, a sophisticated algorithm that takes into account\nability or reputation may produce a fairer or more accurate aggregation of\nratings than the straightforward arithmetic average. Recently a number of\nauthors have proposed different co-determination algorithms where estimates of\nuser and object reputation are refined iteratively together, permitting\naccurate measures of both to be derived directly from the rating data. However,\nsimulations demonstrating these methods' efficacy assumed a continuum of rating\nvalues, consistent with typical physical modelling practice, whereas in most\nactual rating systems only a limited range of discrete values (such as a 5-star\nsystem) is employed. We perform a comparative test of several co-determination\nalgorithms with different scales of discrete ratings and show that this\nseemingly minor modification in fact has a significant impact on algorithms'\nperformance. Paradoxically, where rating resolution is low, increased noise in\nusers' ratings may even improve the overall performance of the system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2010 14:03:09 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2010 21:41:45 GMT"}, {"version": "v3", "created": "Thu, 12 Aug 2010 16:19:49 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Medo", "Matus", ""], ["Wakeling", "Joseph Rushton", ""]]}, {"id": "1001.4136", "submitter": "T.R. Gopalakrishnan Nair", "authors": "K. Lakshmi Madhuri, T.R. Gopalakrishnan Nair", "title": "Authentication and Authorization in Server Systems for Bio-Informatics", "comments": null, "journal-ref": "International Conference TeamTech, pp 81, 2009", "doi": null, "report-no": null, "categories": "cs.CR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authentication and authorization are two tightly coupled and interrelated\nconcepts which are used to keep transactions secure and help in protecting\nconfidential information. This paper proposes to evaluate the current\ntechniques used for authentication and authorization also compares them with\nthe best practices and universally accepted authentication and authorization\nmethods. Authentication verifies user identity and provides reusable\ncredentials while authorization services stores information about user access\nlevels. These mechanisms by which a system checks what level of access a\nparticular authenticated user should have to view secure resources is\ncontrolled by the system\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2010 07:49:02 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Madhuri", "K. Lakshmi", ""], ["Nair", "T. R. Gopalakrishnan", ""]]}, {"id": "1001.4368", "submitter": "Loet Leydesdorff", "authors": "Iina Hellsten, James Dawson, Loet Leydesdorff", "title": "Implicit media frames: Automated analysis of public debate on artificial\n  sweeteners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The framing of issues in the mass media plays a crucial role in the public\nunderstanding of science and technology. This article contributes to research\nconcerned with diachronic analysis of media frames by making an analytical\ndistinction between implicit and explicit media frames, and by introducing an\nautomated method for analysing diachronic changes of implicit frames. In\nparticular, we apply a semantic maps method to a case study on the newspaper\ndebate about artificial sweeteners, published in The New York Times (NYT)\nbetween 1980 and 2006. Our results show that the analysis of semantic changes\nenables us to filter out the dynamics of implicit frames, and to detect\nemerging metaphors in public debates. Theoretically, we discuss the relation\nbetween implicit frames in public debates and codification of information in\nscientific discourses, and suggest further avenues for research interested in\nthe automated analysis of frame changes and trends in public debates.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2010 19:44:03 GMT"}], "update_date": "2010-01-26", "authors_parsed": [["Hellsten", "Iina", ""], ["Dawson", "James", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1001.4597", "submitter": "Jiang Chen", "authors": "Jiang Chen, Wei Chu, Zhenzhen Kou, Zhaohui Zheng", "title": "Learning to Blend by Relevance", "comments": "This paper has been withdrawn by the author due to conflict with a\n  patent filed by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergence of various vertical search engines highlights the fact that a\nsingle ranking technology cannot deal with the complexity and scale of search\nproblems. For example, technology behind video and image search is very\ndifferent from general web search. Their ranking functions share few features.\nQuestion answering websites (e.g., Yahoo! Answer) can make use of text matching\nand click features developed for general web, but they have unique page\nstructures and rich user feedback, e.g., thumbs up and thumbs down ratings in\nYahoo! answer, which greatly benefit their own ranking. Even for those features\nshared by answer and general web, the correlation between features and\nrelevance could be very different. Therefore, dedicated functions are needed in\norder to better rank documents within individual domains. These dedicated\nfunctions are defined on distinct feature spaces. However, having one search\nbox for each domain, is neither efficient nor scalable. Rather than typing the\nsame query two times into both Yahoo! Search and Yahoo! Answer and retrieving\ntwo ranking lists, we would prefer putting it only once but receiving a\ncomprehensive list of documents from both domains on the subject. This\nsituation calls for new technology that blends documents from different sources\ninto a single ranking list. Despite the content richness of the blended list,\nit has to be sorted by relevance none the less. We call such technology\nblending, which is the main subject of this paper.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2010 06:19:06 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2010 21:49:01 GMT"}, {"version": "v3", "created": "Thu, 23 Sep 2010 17:42:50 GMT"}], "update_date": "2010-09-24", "authors_parsed": [["Chen", "Jiang", ""], ["Chu", "Wei", ""], ["Kou", "Zhenzhen", ""], ["Zheng", "Zhaohui", ""]]}, {"id": "1001.5016", "submitter": "Loet Leydesdorff", "authors": "Loet Leydesdorff, Olle Persson", "title": "Mapping the Geography of Science: Distribution Patterns and Networks of\n  Relations among Cities and Institutes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Google Earth, Google Maps and/or network visualization programs such as\nPajek, one can overlay the network of relations among addresses in scientific\npublications on the geographic map. We discuss the pros en cons of the various\noptions, and provide software (freeware) for bridging existing gaps between the\nScience Citation Indices and Scopus, on the one side, and these various\nvisualization tools, on the other. At the level of city names, the global map\ncan be drawn reliably on the basis of the available address information. At the\nlevel of the names of organizations and institutes, there are problems of\nunification both in the ISI-databases and Scopus. Pajek enables us to combine\nthe visualization with statistical analysis, whereas the Google Maps and its\nderivates provide superior tools at the Internet.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2010 20:28:03 GMT"}], "update_date": "2010-01-28", "authors_parsed": [["Leydesdorff", "Loet", ""], ["Persson", "Olle", ""]]}]