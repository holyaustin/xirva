[{"id": "0910.1869", "submitter": "N Vunka Jungum", "authors": "Ravita Chahar, Komal Hooda and Annu Dhankhar", "title": "Management Of Volatile Information In Incremental Web Crawler", "comments": "Paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper has been withdrawn.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2009 21:42:58 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2009 07:09:51 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Chahar", "Ravita", ""], ["Hooda", "Komal", ""], ["Dhankhar", "Annu", ""]]}, {"id": "0910.1938", "submitter": "Patricio Galeas", "authors": "Patricio Galeas, Ralph Kretschmer and Bernd Freisleben", "title": "Information Retrieval via Truncated Hilbert-Space Expansions", "comments": "12 pages, submitted to proceedings of ECIR-2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to the frequency of terms in a document collection, the\ndistribution of terms plays an important role in determining the relevance of\ndocuments. In this paper, a new approach for representing term positions in\ndocuments is presented. The approach allows an efficient evaluation of\nterm-positional information at query evaluation time. Three applications are\ninvestigated: a function-based ranking optimization representing a user-defined\ndocument region, a query expansion technique based on overlapping the term\ndistributions in the top-ranked documents, and cluster analysis of terms in\ndocuments. Experimental results demonstrate the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2009 19:26:13 GMT"}], "update_date": "2009-10-13", "authors_parsed": [["Galeas", "Patricio", ""], ["Kretschmer", "Ralph", ""], ["Freisleben", "Bernd", ""]]}, {"id": "0910.2405", "submitter": "Maya Ramanath", "authors": "Maya Ramanath, Kondreddi Sarath Kumar, Georgiana Ifrim", "title": "Generating Concise and Readable Summaries of XML Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": "MPI-I-2009-5-002", "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML has become the de-facto standard for data representation and exchange,\nresulting in large scale repositories and warehouses of XML data. In order for\nusers to understand and explore these large collections, a summarized, bird's\neye view of the available data is a necessity. In this paper, we are interested\nin semantic XML document summaries which present the \"important\" information\navailable in an XML document to the user. In the best case, such a summary is a\nconcise replacement for the original document itself. At the other extreme, it\nshould at least help the user make an informed choice as to the relevance of\nthe document to his needs. In this paper, we address the two main issues which\narise in producing such meaningful and concise summaries: i) which tags or text\nunits are important and should be included in the summary, ii) how to generate\nsummaries of different sizes.%for different memory budgets. We conduct user\nstudies with different real-life datasets and show that our methods are useful\nand effective in practice.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2009 14:19:01 GMT"}], "update_date": "2009-10-14", "authors_parsed": [["Ramanath", "Maya", ""], ["Kumar", "Kondreddi Sarath", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "0910.3349", "submitter": "Ping Li", "authors": "Ping Li, Arnd Christian Konig", "title": "b-Bit Minwise Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the theoretical framework of b-bit minwise hashing.\nThe original minwise hashing method has become a standard technique for\nestimating set similarity (e.g., resemblance) with applications in information\nretrieval, data management, social networks and computational advertising.\n  By only storing the lowest $b$ bits of each (minwise) hashed value (e.g., b=1\nor 2), one can gain substantial advantages in terms of computational efficiency\nand storage space. We prove the basic theoretical results and provide an\nunbiased estimator of the resemblance for any b. We demonstrate that, even in\nthe least favorable scenario, using b=1 may reduce the storage space at least\nby a factor of 21.3 (or 10.7) compared to using b=64 (or b=32), if one is\ninterested in resemblance > 0.5.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2009 03:39:56 GMT"}], "update_date": "2009-10-20", "authors_parsed": [["Li", "Ping", ""], ["Konig", "Arnd Christian", ""]]}, {"id": "0910.3490", "submitter": "Mat\\'u\\v{s} Medo", "authors": "Matus Medo, Yi-Cheng Zhang, Tao Zhou", "title": "Adaptive model for recommendation of news", "comments": "6 pages, 6 figures", "journal-ref": "EPL 88, 38005, 2009", "doi": "10.1209/0295-5075/88/38005", "report-no": null, "categories": "cs.IR cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most news recommender systems try to identify users' interests and news'\nattributes and use them to obtain recommendations. Here we propose an adaptive\nmodel which combines similarities in users' rating patterns with epidemic-like\nspreading of news on an evolving network. We study the model by computer\nagent-based simulations, measure its performance and discuss its robustness\nagainst bias and malicious behavior. Subject to the approval fraction of news\nrecommended, the proposed model outperforms the widely adopted recommendation\nof news according to their absolute or relative popularity. This model provides\na general social mechanism for recommender systems and may find its\napplications also in other types of recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2009 09:43:12 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2009 07:52:46 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Medo", "Matus", ""], ["Zhang", "Yi-Cheng", ""], ["Zhou", "Tao", ""]]}, {"id": "0910.4769", "submitter": "Sahbi Sidhom", "authors": "Azza Harbaoui (ENSI-Riadi-GDL), Malek Ghenima (ENSI-Riadi-GDL), Sahbi\n  Sidhom (LORIA, Loria)", "title": "Enrichissement des contenus par la r\\'eindexation des usagers : un\n  \\'etat de l'art sur la probl\\'ematique", "comments": null, "journal-ref": "conf\\'erence internationale sur les Syst\\`emes d'information et\n  Intelligence \\'economique, Hammamet : Tunisie (2009)", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information retrieval (IR) is a user approach to obtain relevant information\nwhich meets needs with the help of a IR system (IRS). However, the IRS shows\ncertain differences between user relevance and system relevance. These gaps are\nessentially related to the imperfection of the indexing process (as approach\nrelated to the IR), to problems related to the misunderstanding of the natural\nlanguage and the non correspondence between the real needs of the user and the\nresults of his query. As idea is to think about an ?intellectual? indexing that\ntakes into account the point of view of the user. By consulting the document,\nuser can build information as added-value on the existing content: new\ninformation which grows contents and allows the semantic visibility or\nfacilitates the reading by the annotations, by links to other content, by new\ndescriptors, specific new abstracts of users: it is the reindexing of the\ncontents by the contribution or the vote of the uses\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2009 21:22:04 GMT"}], "update_date": "2009-10-27", "authors_parsed": [["Harbaoui", "Azza", "", "ENSI-Riadi-GDL"], ["Ghenima", "Malek", "", "ENSI-Riadi-GDL"], ["Sidhom", "Sahbi", "", "LORIA, Loria"]]}, {"id": "0910.5386", "submitter": "Arijit  Laha Ph.D.", "authors": "Arijit Laha", "title": "A theoretical foundation for building Knowledge-work Support Systems", "comments": "40 pages, Created June 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach aimed at building a new class of\ninformation system platforms which we call the \"Knowledge-work Support Systems\"\nor KwSS. KwSS can play a significant role in enhancing the IS support for\nknowledge management processes, including those customarily identified as less\namenable to IS support. In our approach we try to enhance basic functionalities\nprovided by the computer-based information systems, namely, that of improving\nthe efficiency of the knowledge workers in accessing, processing and creating\nuseful information. The improvement, along with proper focus on cultural,\nsocial and other aspects of the knowledge management processes, can enhance the\nworkers' efficiency significantly in performing high quality knowledge works.\nIn order to build the proposed approach, we develop several new concepts. The\napproach analyzes the information availability and usage from the knowledge\nworkers and their works' perspectives and consequently brings forth more\ntransparency in various aspects of information life-cycle with respect to\nknowledge management. KsSSes are technology platforms, which can be implemented\nindependently as well as in conjunction with other knowledge management and\ndata management technology platforms, to provide significant boost in the\nknowledge capabilities of organizations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2009 14:31:53 GMT"}], "update_date": "2009-10-29", "authors_parsed": [["Laha", "Arijit", ""]]}, {"id": "0910.5932", "submitter": "Prateek Jain", "authors": "Prateek Jain, Brian Kulis, Jason V. Davis, Inderjit S. Dhillon", "title": "Metric and Kernel Learning using a Linear Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric and kernel learning are important in several machine learning\napplications. However, most existing metric learning algorithms are limited to\nlearning metrics over low-dimensional data, while existing kernel learning\nalgorithms are often limited to the transductive setting and do not generalize\nto new data points. In this paper, we study metric learning as a problem of\nlearning a linear transformation of the input data. We show that for\nhigh-dimensional data, a particular framework for learning a linear\ntransformation of the data based on the LogDet divergence can be efficiently\nkernelized to learn a metric (or equivalently, a kernel function) over an\narbitrarily high dimensional space. We further demonstrate that a wide class of\nconvex loss functions for learning linear transformations can similarly be\nkernelized, thereby considerably expanding the potential applications of metric\nlearning. We demonstrate our learning approach by applying it to large-scale\nreal world problems in computer vision and text mining.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2009 18:19:03 GMT"}], "update_date": "2009-11-02", "authors_parsed": [["Jain", "Prateek", ""], ["Kulis", "Brian", ""], ["Davis", "Jason V.", ""], ["Dhillon", "Inderjit S.", ""]]}]