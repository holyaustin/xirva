[{"id": "1611.00027", "submitter": "Yasser El-Sonbaty", "authors": "Mahmoud El-Defrawy, Yasser El-Sonbaty and Nahla A. Belal", "title": "CBAS: context based arabic stemmer", "comments": null, "journal-ref": "International Journal on Natural Language Computing (IJNLC) Vol.\n  4, No.3, June 2015", "doi": "10.5121/ijnlc.2015.4301", "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arabic morphology encapsulates many valuable features such as word root.\nArabic roots are being utilized for many tasks; the process of extracting a\nword root is referred to as stemming. Stemming is an essential part of most\nNatural Language Processing tasks, especially for derivative languages such as\nArabic. However, stemming is faced with the problem of ambiguity, where two or\nmore roots could be extracted from the same word. On the other hand,\ndistributional semantics is a powerful co-occurrence model. It captures the\nmeaning of a word based on its context. In this paper, a distributional\nsemantics model utilizing Smoothed Pointwise Mutual Information (SPMI) is\nconstructed to investigate its effectiveness on the stemming analysis task. It\nshowed an accuracy of 81.5%, with a at least 9.4% improvement over other\nstemmers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 10:10:51 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["El-Defrawy", "Mahmoud", ""], ["El-Sonbaty", "Yasser", ""], ["Belal", "Nahla A.", ""]]}, {"id": "1611.00138", "submitter": "Sebastian Raschka SR", "authors": "Sebastian Raschka", "title": "MusicMood: Predicting the mood of music from song lyrics using machine\n  learning", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment prediction of contemporary music can have a wide-range of\napplications in modern society, for instance, selecting music for public\ninstitutions such as hospitals or restaurants to potentially improve the\nemotional well-being of personnel, patients, and customers, respectively. In\nthis project, music recommendation system built upon on a naive Bayes\nclassifier, trained to predict the sentiment of songs based on song lyrics\nalone. The experimental results show that music corresponding to a happy mood\ncan be detected with high precision based on text features obtained from song\nlyrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 06:05:49 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Raschka", "Sebastian", ""]]}, {"id": "1611.00144", "submitter": "Yanru Qu", "authors": "Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang", "title": "Product-based Neural Networks for User Response Prediction", "comments": "6 pages, 5 figures, ICDM2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting user responses, such as clicks and conversions, is of great\nimportance and has found its usage in many Web applications including\nrecommender systems, web search and online advertising. The data in those\napplications is mostly categorical and contains multiple fields; a typical\nrepresentation is to transform it into a high-dimensional sparse binary feature\nrepresentation via one-hot encoding. Facing with the extreme sparsity,\ntraditional models may limit their capacity of mining shallow patterns from the\ndata, i.e. low-order feature combinations. Deep models like deep neural\nnetworks, on the other hand, cannot be directly applied for the\nhigh-dimensional input because of the huge feature space. In this paper, we\npropose a Product-based Neural Networks (PNN) with an embedding layer to learn\na distributed representation of the categorical data, a product layer to\ncapture interactive patterns between inter-field categories, and further fully\nconnected layers to explore high-order feature interactions. Our experimental\nresults on two large-scale real-world ad click datasets demonstrate that PNNs\nconsistently outperform the state-of-the-art models on various metrics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 07:10:22 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Qu", "Yanru", ""], ["Cai", "Han", ""], ["Ren", "Kan", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Wen", "Ying", ""], ["Wang", "Jun", ""]]}, {"id": "1611.00384", "submitter": "Oren Barkan", "authors": "Oren Barkan, Noam Koenigstein, Eylon Yogev and Ori Katz", "title": "CB2CF: A Neural Multiview Content-to-Collaborative Filtering Model for\n  Completely Cold Item Recommendations", "comments": "In Proceedings of Recsys'19. ACM, Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Recommender Systems research, algorithms are often characterized as either\nCollaborative Filtering (CF) or Content Based (CB). CF algorithms are trained\nusing a dataset of user preferences while CB algorithms are typically based on\nitem profiles. These approaches harness different data sources and therefore\nthe resulting recommended items are generally very different. This paper\npresents the CB2CF, a deep neural multiview model that serves as a bridge from\nitems content into their CF representations. CB2CF is a real-world algorithm\ndesigned for Microsoft Store services that handle around a billion users\nworldwide. CB2CF is demonstrated on movies and apps recommendations, where it\nis shown to outperform an alternative CB model on completely cold items.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 20:48:34 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 13:59:21 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Barkan", "Oren", ""], ["Koenigstein", "Noam", ""], ["Yogev", "Eylon", ""], ["Katz", "Ori", ""]]}, {"id": "1611.00440", "submitter": "Yustinus Soelistio Eko", "authors": "Elvyna Tunggawan, Yustinus Eko Soelistio", "title": "And the Winner is ...: Bayesian Twitter-based Prediction on 2016 U.S.\n  Presidential Election", "comments": "This is the non-final version of the paper. The final version is\n  published in the IC3INA 2016 Conference (3-5 Oct. 2016,\n  http://situs.opi.lipi.go.id/ic3ina2016/). All citation should be directed to\n  the final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a Naive-Bayesian predictive model for 2016 U.S.\nPresidential Election based on Twitter data. We use 33,708 tweets gathered\nsince December 16, 2015 until February 29, 2016. We introduce a simpler data\npreprocessing method to label the data and train the model. The model achieves\n95.8% accuracy on 10-fold cross validation and predicts Ted Cruz and Bernie\nSanders as Republican and Democratic nominee respectively. It achieves a\ncomparable result to those in its competitor methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 01:45:28 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Tunggawan", "Elvyna", ""], ["Soelistio", "Yustinus Eko", ""]]}, {"id": "1611.00558", "submitter": "Jo\\~ao Vinagre", "authors": "Jo\\~ao Vinagre, Al\\'ipio M\\'ario Jorge, Jo\\~ao Gama", "title": "Improving incremental recommenders with online bagging", "comments": "Submitted to EPIA 2017", "journal-ref": "In: Oliveira E., Gama J., Vale Z., Lopes Cardoso H. (eds) Progress\n  in Artificial Intelligence. EPIA 2017. Lecture Notes in Computer Science, vol\n  10423. Springer, Cham", "doi": "10.1007/978-3-319-65340-2_49", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online recommender systems often deal with continuous, potentially fast and\nunbounded flows of data. Ensemble methods for recommender systems have been\nused in the past in batch algorithms, however they have never been studied with\nincremental algorithms that learn from data streams. We evaluate online bagging\nwith an incremental matrix factorization algorithm for top-N recommendation\nwith positive-only -- binary -- ratings. Our results show that online bagging\nis able to improve accuracy up to 35% over the baseline, with small\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:52:49 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 18:22:37 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Vinagre", "Jo\u00e3o", ""], ["Jorge", "Al\u00edpio M\u00e1rio", ""], ["Gama", "Jo\u00e3o", ""]]}, {"id": "1611.00598", "submitter": "Junseok Park", "authors": "Junseok Park, Gwangmin Kim, Dongjin Jang, Sungji Choo, Sunghwa Bae,\n  Doheon Lee", "title": "A bioinformatics system for searching Co-Occurrence based on\n  Co-Operational Formation with Advanced Method (COCOFAM)", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature analysis is a key step in obtaining background information in\nbiomedical research. However, it is difficult for researchers to obtain\nknowledge of their interests in an efficient manner because of the massive\namount of the published biomedical literature. Therefore, efficient and\nsystematic search strategies are required, which allow ready access to the\nsubstantial amount of literature. In this paper, we propose a novel search\nsystem, named Co-Occurrence based on Co-Operational Formation with Advanced\nMethod(COCOFAM) which is suitable for the large-scale literature analysis.\nCOCOFAM is based on integrating both Spark for local clusters and a global job\nscheduler to gather crowdsourced co-occurrence data on global clusters. It will\nallow users to obtain information of their interests from the substantial\namount of literature.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 13:34:42 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Park", "Junseok", ""], ["Kim", "Gwangmin", ""], ["Jang", "Dongjin", ""], ["Choo", "Sungji", ""], ["Bae", "Sunghwa", ""], ["Lee", "Doheon", ""]]}, {"id": "1611.00812", "submitter": "Jiemin Chen", "authors": "Jianguo Li, Yong Tang and Jiemin Chen", "title": "Leveraging tagging and rating for recommendation: RMF meets weighted\n  diffusion on tripartite graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.04.121", "report-no": null, "categories": "cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RSs) have been a widely exploited approach to solving\nthe information overload problem. However, the performance is still limited due\nto the extreme sparsity of the rating data. With the popularity of Web 2.0, the\nsocial tagging system provides more external information to improve\nrecommendation accuracy. Although some existing approaches combine the matrix\nfactorization models with co-occurrence properties and context of tags, they\nneglect the issue of tag sparsity without the commonly associated tags problem\nthat would also result in inaccurate recommendations. Consequently, in this\npaper, we propose a novel hybrid collaborative filtering model named\nWUDiff_RMF, which improves Regularized Matrix Factorization (RMF) model by\nintegrating Weighted User-Diffusion-based CF algorithm(WUDiff) that obtains the\ninformation of similar users from the weighted tripartite user-item-tag graph.\nThis model aims to capture the degree correlation of the user-item-tag\ntripartite network to enhance the performance of recommendation. Experiments\nconducted on four real-world datasets demonstrate that our approach\nsignificantly performs better than already widely used methods in the accuracy\nof recommendation. Moreover, results show that WUDiff_RMF can alleviate the\ndata sparsity, especially in the circumstance that users have made few ratings\nand few tags.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 02:59:04 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Li", "Jianguo", ""], ["Tang", "Yong", ""], ["Chen", "Jiemin", ""]]}, {"id": "1611.00872", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazi Nia", "title": "A Decision Support System for Inbound Marketers: An Empirical Use of\n  Latent Dirichlet Allocation Topic Model to Guide Infographic Designers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographic is a type of information presentation that inbound marketers use.\nI suggest a method that can allow the infographic designers to benchmark their\ndesign against the previous viral infographics to measure whether a given\ndesign decision can help or hurt the probability of the design becoming viral.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 03:53:25 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Nia", "Meisam Hejazi", ""]]}, {"id": "1611.01228", "submitter": "Deepika Punj", "authors": "Deepika and Ashutosh Dixit", "title": "URL ordering policies for distributed crawlers: a review", "comments": "6 Pages, 5 figures, 1 Table, International Conference on Recent\n  Trends in Computer and Information Technology Research September 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in size of web, the information is also spreading at large\nscale. Search Engines are the medium to access this information. Crawler is the\nmodule of search engine which is responsible for download the web pages. In\norder to download the fresh information and get the database rich, crawler\nshould crawl the web in some order. This is called as ordering of URLs. URL\nordering should be done in efficient and effective manner in order to crawl the\nweb in proficient manner. In this paper, a survey is done on some existing\nmethods of URL ordering and at the end of this paper comparison is also carried\nout among them.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 10:11:56 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Deepika", "", ""], ["Dixit", "Ashutosh", ""]]}, {"id": "1611.01259", "submitter": "Nika Haghtalab", "authors": "Avrim Blum, Nika Haghtalab", "title": "Generalized Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been significant activity in developing algorithms with\nprovable guarantees for topic modeling. In standard topic models, a topic (such\nas sports, business, or politics) is viewed as a probability distribution $\\vec\na_i$ over words, and a document is generated by first selecting a mixture $\\vec\nw$ over topics, and then generating words i.i.d. from the associated mixture\n$A{\\vec w}$. Given a large collection of such documents, the goal is to recover\nthe topic vectors and then to correctly classify new documents according to\ntheir topic mixture.\n  In this work we consider a broad generalization of this framework in which\nwords are no longer assumed to be drawn i.i.d. and instead a topic is a complex\ndistribution over sequences of paragraphs. Since one could not hope to even\nrepresent such a distribution in general (even if paragraphs are given using\nsome natural feature representation), we aim instead to directly learn a\ndocument classifier. That is, we aim to learn a predictor that given a new\ndocument, accurately predicts its topic mixture, without learning the\ndistributions explicitly. We present several natural conditions under which one\ncan do this efficiently and discuss issues such as noise tolerance and sample\ncomplexity in this model. More generally, our model can be viewed as a\ngeneralization of the multi-view or co-training setting in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 03:45:03 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Blum", "Avrim", ""], ["Haghtalab", "Nika", ""]]}, {"id": "1611.01400", "submitter": "Jesse Lingeman", "authors": "Jesse M Lingeman, Hong Yu", "title": "Learning to Rank Scientific Documents from the Crowd", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.DL cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding related published articles is an important task in any science, but\nwith the explosion of new work in the biomedical domain it has become\nespecially challenging. Most existing methodologies use text similarity metrics\nto identify whether two articles are related or not. However biomedical\nknowledge discovery is hypothesis-driven. The most related articles may not be\nones with the highest text similarities. In this study, we first develop an\ninnovative crowd-sourcing approach to build an expert-annotated\ndocument-ranking corpus. Using this corpus as the gold standard, we then\nevaluate the approaches of using text similarity to rank the relatedness of\narticles. Finally, we develop and evaluate a new supervised model to\nautomatically rank related scientific articles. Our results show that authors'\nranking differ significantly from rankings by text-similarity-based models. By\ntraining a learning-to-rank model on a subset of the annotated corpus, we found\nthe best supervised learning-to-rank model (SVM-Rank) significantly surpassed\nstate-of-the-art baseline systems.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 14:43:44 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Lingeman", "Jesse M", ""], ["Yu", "Hong", ""]]}, {"id": "1611.01546", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Sanjukta Bhowmick and Sharma Chakravarthy", "title": "Scalable Holistic Analysis of Multi-Source, Data-Intensive Problems\n  Using Multilayered Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DM cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Holistic analysis of many real-world problems are based on data collected\nfrom multiple sources contributing to some aspect of that problem. The word\nfusion has also been used in the literature for such problems involving\ndisparate data types. Holistically understanding traffic patterns, causes of\naccidents, bombings, terrorist planning and many natural phenomenon such as\nstorms, earthquakes fall into this category. Some may have real-time\nrequirements and some may need to be analyzed after the fact (post-mortem or\nforensic analysis.) What is common for all these problems is that the amount\nand types of data associated with the event. Data may also be incomplete and\ntrustworthiness of sources may also vary. Currently, manual and ad-hoc\napproaches are used in aggregating data in different ways for analyzing and\nunderstanding these problems.\n  In this paper, we approach this problem in a novel way using multilayered\nnetworks. We identify features of a central event and propose a network layer\nfor each feature. This approach allows us to study the effect of each feature\nindependently and its impact on the event. We also establish that the proposed\napproach allows us to compose these features in arbitrary ways (without loss of\ninformation) to analyze their combined effect. Additionally, formulation of\nrelationships (e.g., distance measure for a single feature instead of several\nat the same time) is simpler. Further, computations can be done once on each\nlayer in this approach and reused for mixing and matching the features for\naggregate impacts and \"what if\" scenarios to understand the problem\nholistically. This has been demonstrated by recreating the communities for the\nAND-Composed network by using the communities of the individual layers.\n  We believe that techniques proposed here make an important contribution to\nthe nascent yet fast growing area of data fusion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:32:18 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Santra", "Abhishek", ""], ["Bhowmick", "Sanjukta", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "1611.01802", "submitter": "Ricardo Usbeck", "authors": "Ricardo Usbeck, Jonathan Huthmann, Nico Duldhardt, Axel-Cyrille Ngonga\n  Ngomo", "title": "Self-Wiring Question Answering Systems", "comments": "6 pages, 1 figure, pre-print in lncs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) has been the subject of a resurgence over the past\nyears. The said resurgence has led to a multitude of question answering (QA)\nsystems being developed both by companies and research facilities. While a few\ncomponents of QA systems get reused across implementations, most systems do not\nleverage the full potential of component reuse. Hence, the development of QA\nsystems is currently still a tedious and time-consuming process. We address the\nchallenge of accelerating the creation of novel or tailored QA systems by\npresenting a concept for a self-wiring approach to composing QA systems. Our\napproach will allow the reuse of existing, web-based QA systems or modules\nwhile developing new QA platforms. To this end, it will rely on QA modules\nbeing described using the Web Ontology Language. Based on these descriptions,\nour approach will be able to automatically compose QA systems using a\ndata-driven approach automatically.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 16:08:21 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 17:27:39 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Usbeck", "Ricardo", ""], ["Huthmann", "Jonathan", ""], ["Duldhardt", "Nico", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1611.01974", "submitter": "Balint Daroczy", "authors": "B\\'alint Dar\\'oczy and Frederick Ayala-G\\'omez and Andr\\'as Bencz\\'ur", "title": "Item-to-item recommendation based on Contextual Fisher Information", "comments": "9 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web recommendation services bear great importance in e-commerce, as they aid\nthe user in navigating through the items that are most relevant to her needs.\nIn a typical Web site, long history of previous activities or purchases by the\nuser is rarely available. Hence in most cases, recommenders propose items that\nare similar to the most recent ones viewed in the current user session. The\ncorresponding task is called session based item-to-item recommendation. For\nfrequent items, it is easy to present item-to-item recommendations by \"people\nwho viewed this, also viewed\" lists. However, most of the items belong to the\nlong tail, where previous actions are sparsely available. Another difficulty is\nthe so-called cold start problem, when the item has recently appeared and had\nno time yet to accumulate sufficient number of transactions. In order to\nrecommend a next item in a session in sparse or cold start situations, we also\nhave to incorporate item similarity models. In this paper we describe a\nprobabilistic similarity model based on Random Fields to approximate\nitem-to-item transition probabilities. We give a generative model for the item\ninteractions based on arbitrary distance measures over the items including\nexplicit, implicit ratings and external metadata. The model may change in time\nto fit better recent events and recommend the next item based on the updated\nFisher Information. Our new model outperforms both simple similarity baseline\nmethods and recent item-to-item recommenders, under several different\nperformance metrics and publicly available data sets. We reach significant\ngains in particular for recommending a new item following a rare item.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:28:06 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 09:31:56 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Dar\u00f3czy", "B\u00e1lint", ""], ["Ayala-G\u00f3mez", "Frederick", ""], ["Bencz\u00far", "Andr\u00e1s", ""]]}, {"id": "1611.02119", "submitter": "Denis Parra", "authors": "Ivania Donoso and Denis Parra", "title": "EpistAid: Interactive Interface for Document Filtering in Evidence-based\n  Health Care", "comments": "5 pages, 4 figures, pre-print submitted to ACM IUI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based health care (EBHC) is an important practice of medicine which\nattempts to provide systematic scientific evidence to answer clinical\nquestions. In this context, Epistemonikos (www.epistemonikos.org) is one of the\nfirst and most important online systems in the field, providing an interface\nthat supports users on searching and filtering scientific articles for\npracticing EBHC. The system nowadays requires a large amount of expert human\neffort, where close to 500 physicians manually curate articles to be utilized\nin the platform. In order to scale up the large and continuous amount of data\nto keep the system updated, we introduce EpistAid, an interactive intelligent\ninterface which supports clinicians in the process of curating documents for\nEpistemonikos within lists of papers called evidence matrices. We introduce the\ncharacteristics, design and algorithms of our solution, as well as a prototype\nimplementation and a case study to show how our solution addresses the\ninformation overload problem in this area.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:38:07 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Donoso", "Ivania", ""], ["Parra", "Denis", ""]]}, {"id": "1611.02257", "submitter": "Hua Sun", "authors": "Hua Sun and Syed A. Jafar", "title": "Multiround Private Information Retrieval: Capacity and Storage Overhead", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity has recently been characterized for the private information\nretrieval (PIR) problem as well as several of its variants. In every case it is\nassumed that all the queries are generated by the user simultaneously. Here we\nconsider multiround PIR, where the queries in each round are allowed to depend\non the answers received in previous rounds. We show that the capacity of\nmultiround PIR is the same as the capacity of single-round PIR (the result is\ngeneralized to also include $T$-privacy constraints). Combined with previous\nresults, this shows that there is no capacity advantage from multiround over\nsingle-round schemes, non-linear over linear schemes or from $\\epsilon$-error\nover zero-error schemes. However, we show through an example that there is an\nadvantage in terms of storage overhead. We provide an example of a multiround,\nnon-linear, $\\epsilon$-error PIR scheme that requires a strictly smaller\nstorage overhead than the best possible with single-round, linear, zero-error\nPIR schemes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:36:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Sun", "Hua", ""], ["Jafar", "Syed A.", ""]]}, {"id": "1611.02337", "submitter": "Jose Texier PhD", "authors": "Daniel Robins, Fernando Emmanuel Frati, Jonatan Alvarez, Jose Texier", "title": "Balotage in Argentina 2015, a sentiment analysis of tweets", "comments": "in Spanish. Jornadas de Cloud Computing, La Plata - Argentina. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Twitter social network contains a large amount of information generated by\nits users. That information is composed of opinions and comments that may\nreflect trends in social behavior. There is talk of trend when it is possible\nto identify opinions and comments geared towards the same shared by a lot of\npeople direction. To determine if two or more written opinions share the same\naddress, techniques Natural Language Processing (NLP) are used. This paper\nproposes a methodology for predicting reflected in Twitter from the use of\nsentiment analysis functions NLP based on social behaviors. The case study was\nselected the 2015 Presidential in Argentina, and a software architecture Big\nData composed Vertica data base with the component called Pulse was used.\nThrough the analysis it was possible to detect trends in voting intentions with\nregard to the presidential candidates, achieving greater accuracy in predicting\nthat achieved with traditional systems surveys.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 23:05:40 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Robins", "Daniel", ""], ["Frati", "Fernando Emmanuel", ""], ["Alvarez", "Jonatan", ""], ["Texier", "Jose", ""]]}, {"id": "1611.02815", "submitter": "Emad Al-Shalabi Fawzi", "authors": "Emad Fawzi Al-Shalabi", "title": "An Automated System for Essay Scoring of Online Exams in Arabic based on\n  Stemming Techniques and Levenshtein Edit Operations", "comments": "5 pages, 2 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 13,\n  Issue 5, September 2016", "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, an automated system is proposed for essay scoring in Arabic\nlanguage for online exams based on stemming techniques and Levenshtein edit\noperations. An online exam has been developed on the proposed mechanisms,\nexploiting the capabilities of light and heavy stemming. The implemented online\ngrading system has shown to be an efficient tool for automated scoring of essay\nquestions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 15:25:02 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Al-Shalabi", "Emad Fawzi", ""]]}, {"id": "1611.03305", "submitter": "Kezban Dilek Onal", "authors": "Kezban Dilek Onal, Ismail Sengor Altingovde, Pinar Karagoz, Maarten de\n  Rijke", "title": "Getting Started with Neural Models for Semantic Matching in Web Search", "comments": "under review for the Information Retrieval Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vocabulary mismatch problem is a long-standing problem in information\nretrieval. Semantic matching holds the promise of solving the problem. Recent\nadvances in language technology have given rise to unsupervised neural models\nfor learning representations of words as well as bigger textual units. Such\nrepresentations enable powerful semantic matching methods. This survey is meant\nas an introduction to the use of neural models for semantic matching. To remain\nfocused we limit ourselves to web search. We detail the required background and\nterminology, a taxonomy grouping the rapidly growing body of work in the area,\nand then survey work on neural models for semantic matching in the context of\nthree tasks: query suggestion, ad retrieval, and document retrieval. We include\na section on resources and best practices that we believe will help readers who\nare new to the area. We conclude with an assessment of the state-of-the-art and\nsuggestions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 14:28:40 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Onal", "Kezban Dilek", ""], ["Altingovde", "Ismail Sengor", ""], ["Karagoz", "Pinar", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1611.03350", "submitter": "Giacomo Berardi", "authors": "Giacomo Berardi, Diego Ceccarelli, Andrea Esuli and Diego Marcheggiani", "title": "On the Impact of Entity Linking in Microblog Real-Time Filtering", "comments": "6 pages, 1 figure, 1 table. SAC 2015, Salamanca, Spain - April 13 -\n  17, 2015", "journal-ref": "Proceedings of the 30th Annual ACM Symposium on Applied Computing\n  (SAC 2015). pp 1066-1071. Salamanca, ES, 2015", "doi": "10.1145/2695664.2695761", "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microblogging is a model of content sharing in which the temporal locality of\nposts with respect to important events, either of foreseeable or unforeseeable\nnature, makes applica- tions of real-time filtering of great practical\ninterest. We propose the use of Entity Linking (EL) in order to improve the\nretrieval effectiveness, by enriching the representation of microblog posts and\nfiltering queries. EL is the process of recognizing in an unstructured text the\nmention of relevant entities described in a knowledge base. EL of short pieces\nof text is a difficult task, but it is also a scenario in which the information\nEL adds to the text can have a substantial impact on the retrieval process. We\nimplement a start-of-the-art filtering method, based on the best systems from\nthe TREC Microblog track realtime adhoc retrieval and filtering tasks , and\nextend it with a Wikipedia-based EL method. Results show that the use of EL\nsignificantly improves over non-EL based versions of the filtering methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:40:14 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Berardi", "Giacomo", ""], ["Ceccarelli", "Diego", ""], ["Esuli", "Andrea", ""], ["Marcheggiani", "Diego", ""]]}, {"id": "1611.03426", "submitter": "Ernesto Diaz-Aviles", "authors": "Avar\\'e Stewart, Sara Romano, Nattiya Kanhabua, Sergio Di Martino,\n  Wolf Siberski, Antonino Mazzeo, Wolfgang Nejdl, and Ernesto Diaz-Aviles", "title": "Why is it Difficult to Detect Sudden and Unexpected Epidemic Outbreaks\n  in Twitter?", "comments": "ACM CCS Concepts: Applied computing - Health informatics; Information\n  systems - Web mining; Document filtering; Novelty in information retrieval;\n  Recommender systems; Human-centered computing - Social media", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.SI stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Social media services such as Twitter are a valuable source of information\nfor decision support systems. Many studies have shown that this also holds for\nthe medical domain, where Twitter is considered a viable tool for public health\nofficials to sift through relevant information for the early detection,\nmanagement, and control of epidemic outbreaks. This is possible due to the\ninherent capability of social media services to transmit information faster\nthan traditional channels. However, the majority of current studies have\nlimited their scope to the detection of common and seasonal health recurring\nevents (e.g., Influenza-like Illness), partially due to the noisy nature of\nTwitter data, which makes outbreak detection and management very challenging.\n  Within the European project M-Eco, we developed a Twitter-based Epidemic\nIntelligence (EI) system, which is designed to also handle a more general class\nof unexpected and aperiodic outbreaks. In particular, we faced three main\nresearch challenges in this endeavor:\n  1) dynamic classification to manage terminology evolution of Twitter\nmessages, 2) alert generation to produce reliable outbreak alerts analyzing the\n(noisy) tweet time series, and 3) ranking and recommendation to support domain\nexperts for better assessment of the generated alerts.\n  In this paper, we empirically evaluate our proposed approach to these\nchallenges using real-world outbreak datasets and a large collection of tweets.\nWe validate our solution with domain experts, describe our experiences, and\ngive a more realistic view on the benefits and issues of analyzing social media\nfor public health.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:53:33 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Stewart", "Avar\u00e9", ""], ["Romano", "Sara", ""], ["Kanhabua", "Nattiya", ""], ["Di Martino", "Sergio", ""], ["Siberski", "Wolf", ""], ["Mazzeo", "Antonino", ""], ["Nejdl", "Wolfgang", ""], ["Diaz-Aviles", "Ernesto", ""]]}, {"id": "1611.03558", "submitter": "ShiLiang Zhang", "authors": "Dan Liu and Wei Lin and Shiliang Zhang and Si Wei and Hui Jiang", "title": "Neural Networks Models for Entity Discovery and Linking", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the USTC_NELSLIP systems submitted to the Trilingual\nEntity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population\n(KBP) contests. We have built two systems for entity discovery and mention\ndetection (MD): one uses the conditional RNNLM and the other one uses the\nattention-based encoder-decoder framework. The entity linking (EL) system\nconsists of two modules: a rule based candidate generation and a neural\nnetworks probability ranking model. Moreover, some simple string matching rules\nare used for NIL clustering. At the end, our best system has achieved an F1\nscore of 0.624 in the end-to-end typed mention ceaf plus metric.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 01:21:20 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Liu", "Dan", ""], ["Lin", "Wei", ""], ["Zhang", "Shiliang", ""], ["Wei", "Si", ""], ["Jiang", "Hui", ""]]}, {"id": "1611.03751", "submitter": "Pengfei Xu", "authors": "Pengfei Xu, Jiaheng Lu", "title": "Top-k String Auto-Completion with Synonyms", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-completion is one of the most prominent features of modern information\nsystems. The existing solutions of auto-completion provide the suggestions\nbased on the beginning of the currently input character sequence (i.e. prefix).\nHowever, in many real applications, one entity often has synonyms or\nabbreviations. For example, \"DBMS\" is an abbreviation of \"Database Management\nSystems\". In this paper, we study a novel type of auto-completion by using\nsynonyms and abbreviations. We propose three trie-based algorithms to solve the\ntop-k auto-completion with synonyms; each one with different space and time\ncomplexity trade-offs. Experiments on large-scale datasets show that it is\npossible to support effective and efficient synonym-based retrieval of\ncompletions of a million strings with thousands of synonyms rules at about a\nmicrosecond per-completion, while taking small space overhead (i.e. 160-200\nbytes per string). The source code of our experiments can be download at:\nhttp://udbms.cs.helsinki.fi/?projects/autocompletion/download .\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:40:06 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 20:12:56 GMT"}, {"version": "v3", "created": "Tue, 22 Nov 2016 22:29:33 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Xu", "Pengfei", ""], ["Lu", "Jiaheng", ""]]}, {"id": "1611.04033", "submitter": "Ibrahim Abu El-Khair", "authors": "Ibrahim Abu El-khair", "title": "1.5 billion words Arabic Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study is an attempt to build a contemporary linguistic corpus for Arabic\nlanguage. The corpus produced, is a text corpus includes more than five million\nnewspaper articles. It contains over a billion and a half words in total, out\nof which, there is about three million unique words. The data were collected\nfrom newspaper articles in ten major news sources from eight Arabic countries,\nover a period of fourteen years. The corpus was encoded with two types of\nencoding, namely: UTF-8, and Windows CP-1256. Also it was marked with two\nmark-up languages, namely: SGML, and XML.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 18:41:58 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["El-khair", "Ibrahim Abu", ""]]}, {"id": "1611.04369", "submitter": "Yujie Qian", "authors": "Yujie Qian, Yinpeng Dong, Ye Ma, Hailong Jin, and Juanzi Li", "title": "Feature Engineering and Ensemble Modeling for Paper Acceptance Rank\n  Prediction", "comments": "2nd place winner report of KDD Cup 2016. More details can be found at\n  https://kddcup2016.azurewebsites.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring research impact and ranking academic achievement are important and\nchallenging problems. Having an objective picture of research institution is\nparticularly valuable for students, parents and funding agencies, and also\nattracts attention from government and industry. KDD Cup 2016 proposes the\npaper acceptance rank prediction task, in which the participants are asked to\nrank the importance of institutions based on predicting how many of their\npapers will be accepted at the 8 top conferences in computer science. In our\nwork, we adopt a three-step feature engineering method, including basic\nfeatures definition, finding similar conferences to enhance the feature set,\nand dimension reduction using PCA. We propose three ranking models and the\nensemble methods for combining such models. Our experiment verifies the\neffectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of\nthe 2nd place.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:59:07 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Qian", "Yujie", ""], ["Dong", "Yinpeng", ""], ["Ma", "Ye", ""], ["Jin", "Hailong", ""], ["Li", "Juanzi", ""]]}, {"id": "1611.04666", "submitter": "Steffen Rendle", "authors": "Immanuel Bayer, Xiangnan He, Bhargav Kanagal, Steffen Rendle", "title": "A Generic Coordinate Descent Framework for Learning from Implicit\n  Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, interest in recommender research has shifted from explicit\nfeedback towards implicit feedback data. A diversity of complex models has been\nproposed for a wide variety of applications. Despite this, learning from\nimplicit feedback is still computationally challenging. So far, most work\nrelies on stochastic gradient descent (SGD) solvers which are easy to derive,\nbut in practice challenging to apply, especially for tasks with many items. For\nthe simple matrix factorization model, an efficient coordinate descent (CD)\nsolver has been previously proposed. However, efficient CD approaches have not\nbeen derived for more complex models.\n  In this paper, we provide a new framework for deriving efficient CD\nalgorithms for complex recommender models. We identify and introduce the\nproperty of k-separable models. We show that k-separability is a sufficient\nproperty to allow efficient optimization of implicit recommender problems with\nCD. We illustrate this framework on a variety of state-of-the-art models\nincluding factorization machines and Tucker decomposition. To summarize, our\nwork provides the theory and building blocks to derive efficient implicit CD\nalgorithms for complex recommender models.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 01:32:33 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Bayer", "Immanuel", ""], ["He", "Xiangnan", ""], ["Kanagal", "Bhargav", ""], ["Rendle", "Steffen", ""]]}, {"id": "1611.05010", "submitter": "Kejun Huang", "authors": "Kejun Huang, Xiao Fu, Nicholas D. Sidiropoulos", "title": "Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In topic modeling, many algorithms that guarantee identifiability of the\ntopics have been developed under the premise that there exist anchor words --\ni.e., words that only appear (with positive probability) in one topic.\nFollow-up work has resorted to three or higher-order statistics of the data\ncorpus to relax the anchor word assumption. Reliable estimates of higher-order\nstatistics are hard to obtain, however, and the identification of topics under\nthose models hinges on uncorrelatedness of the topics, which can be\nunrealistic. This paper revisits topic modeling based on second-order moments,\nand proposes an anchor-free topic mining framework. The proposed approach\nguarantees the identification of the topics under a much milder condition\ncompared to the anchor-word assumption, thereby exhibiting much better\nrobustness in practice. The associated algorithm only involves one\neigen-decomposition and a few small linear programs. This makes it easy to\nimplement and scale up to very large problem instances. Experiments using the\nTDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free\napproach exhibits very favorable performance (measured using coherence,\nsimilarity count, and clustering accuracy metrics) compared to the prior art.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 20:06:40 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Huang", "Kejun", ""], ["Fu", "Xiao", ""], ["Sidiropoulos", "Nicholas D.", ""]]}, {"id": "1611.05222", "submitter": "Drahomira Herrmannova", "authors": "Drahomira Herrmannova and Petr Knoth", "title": "Simple Yet Effective Methods for Large-Scale Scholarly Publication\n  Ranking", "comments": "WSDM Cup 2016 - Entity Ranking Challenge. The 9th ACM International\n  Conference on Web Search and Data Mining, San Francisco, CA, USA. February\n  22-25, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing amount of published research, automatic evaluation of\nscholarly publications is becoming an important task. In this paper we address\nthis problem and present a simple and transparent approach for evaluating the\nimportance of scholarly publications. Our method has been ranked among the top\nperformers in the WSDM Cup 2016 Challenge. The first part of this paper\ndescribes our method. In the second part we present potential improvements to\nthe method and analyse the evaluation setup which was provided during the\nchallenge. Finally, we discuss future challenges in automatic evaluation of\npapers including the use of full-texts based evaluation methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 11:12:55 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Herrmannova", "Drahomira", ""], ["Knoth", "Petr", ""]]}, {"id": "1611.05368", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Neural Style Representations and the Large-Scale Classification of\n  Artistic Style", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Proceedings of the Future Technologies Conference, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1611.05480", "submitter": "Jianbo Yuan", "authors": "Jianbo Yuan, Walid Shalaby, Mohammed Korayem, David Lin, Khalifeh\n  AlJadda, and Jiebo Luo", "title": "Solving Cold-Start Problem in Large-scale Recommendation Engines: A Deep\n  Learning Approach", "comments": "in Big Data, IEEE International Conference on, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) is widely used in large-scale recommendation\nengines because of its efficiency, accuracy and scalability. However, in\npractice, the fact that recommendation engines based on CF require interactions\nbetween users and items before making recommendations, make it inappropriate\nfor new items which haven't been exposed to the end users to interact with.\nThis is known as the cold-start problem. In this paper we introduce a novel\napproach which employs deep learning to tackle this problem in any CF based\nrecommendation engine. One of the most important features of the proposed\ntechnique is the fact that it can be applied on top of any existing CF based\nrecommendation engine without changing the CF core. We successfully applied\nthis technique to overcome the item cold-start problem in Careerbuilder's CF\nbased recommendation engine. Our experiments show that the proposed technique\nis very efficient to resolve the cold-start problem while maintaining high\naccuracy of the CF recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:03:04 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Yuan", "Jianbo", ""], ["Shalaby", "Walid", ""], ["Korayem", "Mohammed", ""], ["Lin", "David", ""], ["AlJadda", "Khalifeh", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.05973", "submitter": "Marcos Martinez-Romero PhD", "authors": "Marcos Martinez-Romero, Clement Jonquet, Martin J. O'Connor, John\n  Graybeal, Alejandro Pazos, Mark A. Musen", "title": "NCBO Ontology Recommender 2.0: An Enhanced Approach for Biomedical\n  Ontology Recommendation", "comments": "29 pages, 8 figures, 11 tables", "journal-ref": "Journal of Biomedical Semantics 8 (2017) 1-22", "doi": "10.1186/s13326-017-0128-y", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical researchers use ontologies to annotate their data with ontology\nterms, enabling better data integration and interoperability. However, the\nnumber, variety and complexity of current biomedical ontologies make it\ncumbersome for researchers to determine which ones to reuse for their specific\nneeds. To overcome this problem, in 2010 the National Center for Biomedical\nOntology (NCBO) released the Ontology Recommender, which is a service that\nreceives a biomedical text corpus or a list of keywords and suggests ontologies\nappropriate for referencing the indicated terms. We developed a new version of\nthe NCBO Ontology Recommender. Called Ontology Recommender 2.0, it uses a new\nrecommendation approach that evaluates the relevance of an ontology to\nbiomedical text data according to four criteria: (1) the extent to which the\nontology covers the input data; (2) the acceptance of the ontology in the\nbiomedical community; (3) the level of detail of the ontology classes that\ncover the input data; and (4) the specialization of the ontology to the domain\nof the input data. Our evaluation shows that the enhanced recommender provides\nhigher quality suggestions than the original approach, providing better\ncoverage of the input data, more detailed information about their concepts,\nincreased specialization for the domain of the input data, and greater\nacceptance and use in the community. In addition, it provides users with more\nexplanatory information, along with suggestions of not only individual\nontologies but also groups of ontologies. It also can be customized to fit the\nneeds of different scenarios. Ontology Recommender 2.0 combines the strengths\nof its predecessor with a range of adjustments and new features that improve\nits reliability and usefulness. Ontology Recommender 2.0 recommends over 500\nbiomedical ontologies from the NCBO BioPortal platform, where it is openly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 04:58:54 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 21:32:40 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Martinez-Romero", "Marcos", ""], ["Jonquet", "Clement", ""], ["O'Connor", "Martin J.", ""], ["Graybeal", "John", ""], ["Pazos", "Alejandro", ""], ["Musen", "Mark A.", ""]]}, {"id": "1611.06322", "submitter": "Dominik Wurzer Dominik Wurzer", "authors": "Yumeng Qin, Dominik Wurzer, Victor Lavrenko, Cunchen Tang", "title": "Spotting Rumors via Novelty Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rumour detection is hard because the most accurate systems operate\nretrospectively, only recognizing rumours once they have collected repeated\nsignals. By then the rumours might have already spread and caused harm. We\nintroduce a new category of features based on novelty, tailored to detect\nrumours early on. To compensate for the absence of repeated signals, we make\nuse of news wire as an additional data source. Unconfirmed (novel) information\nwith respect to the news articles is considered as an indication of rumours.\nAdditionally we introduce pseudo feedback, which assumes that documents that\nare similar to previous rumours, are more likely to also be a rumour.\nComparison with other real-time approaches shows that novelty based features in\nconjunction with pseudo feedback perform significantly better, when detecting\nrumours instantly after their publication.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 07:23:10 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Qin", "Yumeng", ""], ["Wurzer", "Dominik", ""], ["Lavrenko", "Victor", ""], ["Tang", "Cunchen", ""]]}, {"id": "1611.06620", "submitter": "Richard Oentaryo", "authors": "Jovian Lin, Richard J. Oentaryo, Ee-Peng Lim, Casey Vu, Adrian Vu,\n  Agus T. Kwee, Philips K. Prasetyo", "title": "A Business Zone Recommender System Based on Facebook and Urban Planning\n  Data", "comments": null, "journal-ref": "Proceedings of the European Conference on Information Retrieval,\n  2016, pp. 641-647", "doi": "10.1007/978-3-319-30671-1_47", "report-no": null, "categories": "cs.SI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ZoneRec---a zone recommendation system for physical businesses in\nan urban city, which uses both public business data from Facebook and urban\nplanning data. The system consists of machine learning algorithms that take in\na business' metadata and outputs a list of recommended zones to establish the\nbusiness in. We evaluate our system using data of food businesses in Singapore\nand assess the contribution of different feature groups to the recommendation\nquality.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 00:47:44 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Lin", "Jovian", ""], ["Oentaryo", "Richard J.", ""], ["Lim", "Ee-Peng", ""], ["Vu", "Casey", ""], ["Vu", "Adrian", ""], ["Kwee", "Agus T.", ""], ["Prasetyo", "Philips K.", ""]]}, {"id": "1611.06668", "submitter": "Qiang Cui", "authors": "Qiang Cui, Shu Wu, Qiang Liu, Wen Zhong, Liang Wang", "title": "MV-RNN: A Multi-View Recurrent Neural Network for Sequential\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential recommendation is a fundamental task for network applications, and\nit usually suffers from the item cold start problem due to the insufficiency of\nuser feedbacks. There are currently three kinds of popular approaches which are\nrespectively based on matrix factorization (MF) of collaborative filtering,\nMarkov chain (MC), and recurrent neural network (RNN). Although widely used,\nthey have some limitations. MF based methods could not capture dynamic user's\ninterest. The strong Markov assumption greatly limits the performance of MC\nbased methods. RNN based methods are still in the early stage of incorporating\nadditional information. Based on these basic models, many methods with\nadditional information only validate incorporating one modality in a separate\nway. In this work, to make the sequential recommendation and deal with the item\ncold start problem, we propose a Multi-View Recurrent Neural Network (MV-RNN})\nmodel. Given the latent feature, MV-RNN can alleviate the item cold start\nproblem by incorporating visual and textual information. First, At the input of\nMV-RNN, three different combinations of multi-view features are studied, like\nconcatenation, fusion by addition and fusion by reconstructing the original\nmulti-modal data. MV-RNN applies the recurrent structure to dynamically capture\nthe user's interest. Second, we design a separate structure and a united\nstructure on the hidden state of MV-RNN to explore a more effective way to\nhandle multi-view features. Experiments on two real-world datasets show that\nMV-RNN can effectively generate the personalized ranking list, tackle the\nmissing modalities problem and significantly alleviate the item cold start\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 07:00:51 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 09:24:05 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Cui", "Qiang", ""], ["Wu", "Shu", ""], ["Liu", "Qiang", ""], ["Zhong", "Wen", ""], ["Wang", "Liang", ""]]}, {"id": "1611.06671", "submitter": "Mark Magumba", "authors": "Mark Abraham Magumba, Peter Nabende", "title": "Ontology Driven Disease Incidence Detection on Twitter", "comments": "19 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the issue of generic automated disease incidence\nmonitoring on twitter. We employ an ontology of disease related concepts and\nuse it to obtain a conceptual representation of tweets. Unlike previous key\nword based systems and topic modeling approaches, our ontological approach\nallows us to apply more stringent criteria for determining which messages are\nrelevant such as spatial and temporal characteristics whilst giving a stronger\nguarantee that the resulting models will perform well on new data that may be\nlexically divergent. We achieve this by training learners on concepts rather\nthan individual words. For training we use a dataset containing mentions of\ninfluenza and Listeria and use the learned models to classify datasets\ncontaining mentions of an arbitrary selection of other diseases. We show that\nour ontological approach achieves good performance on this task using a variety\nof Natural Language Processing Techniques. We also show that word vectors can\nbe learned directly from our concepts to achieve even better results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 07:32:56 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Magumba", "Mark Abraham", ""], ["Nabende", "Peter", ""]]}, {"id": "1611.06792", "submitter": "Ye Zhang", "authors": "Ye Zhang, Md Mustafizur Rahman, Alex Braylan, Brandon Dang, Heng-Lu\n  Chang, Henna Kim, Quinten McNamara, Aaron Angert, Edward Banner, Vivek\n  Khetan, Tyler McDonnell, An Thanh Nguyen, Dan Xu, Byron C. Wallace, Matthew\n  Lease", "title": "Neural Information Retrieval: A Literature Review", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent \"third wave\" of Neural Network (NN) approaches now delivers\nstate-of-the-art performance in many machine learning tasks, spanning speech\nrecognition, computer vision, and natural language processing. Because these\nmodern NNs often comprise multiple interconnected layers, this new NN research\nis often referred to as deep learning. Stemming from this tide of NN work, a\nnumber of researchers have recently begun to investigate NN approaches to\nInformation Retrieval (IR). While deep NNs have yet to achieve the same level\nof success in IR as seen in other areas, the recent surge of interest and work\nin NNs for IR suggest that this state of affairs may be quickly changing. In\nthis work, we survey the current landscape of Neural IR research, paying\nspecial attention to the use of learned representations of queries and\ndocuments (i.e., neural embeddings). We highlight the successes of neural IR\nthus far, catalog obstacles to its wider adoption, and suggest potentially\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 03:44:34 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 07:26:07 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:46:43 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Zhang", "Ye", ""], ["Rahman", "Md Mustafizur", ""], ["Braylan", "Alex", ""], ["Dang", "Brandon", ""], ["Chang", "Heng-Lu", ""], ["Kim", "Henna", ""], ["McNamara", "Quinten", ""], ["Angert", "Aaron", ""], ["Banner", "Edward", ""], ["Khetan", "Vivek", ""], ["McDonnell", "Tyler", ""], ["Nguyen", "An Thanh", ""], ["Xu", "Dan", ""], ["Wallace", "Byron C.", ""], ["Lease", "Matthew", ""]]}, {"id": "1611.06961", "submitter": "Khushnood Abbas", "authors": "Khushnood Abbas", "title": "Rising Novelties on Evolving Networks: Recent Behavior Dominant and\n  Non-Dominant Model", "comments": "19 pages, 5 figures", "journal-ref": "Physica A: Statistical Mechanics and its Applications 484C (2017)\n  pp. 506-515", "doi": "10.1016/j.physa.2017.04.156", "report-no": null, "categories": "cs.SI cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty attracts attention like popularity. Hence predicting novelty is as\nimportant as popularity. Novelty is the side effect of competition and aging in\nevolving systems. Recent behavior or recent link gain in networks plays an\nimportant role in emergence or trend. We exploited this wisdom and came up with\ntwo models considering different scenarios and systems. Where recent behavior\ndominates over total behavior (total link gain) in the first one, and recent\nbehavior is as important as total behavior for future link gain in second one.\nIt suppose that random walker walks on a network and can jump to any node, the\nprobablity of jumping or making connection to other node is based on which node\nis recently more active or receiving more links. In our assumption random\nwalker can also jump to node which is already popular but recently not popular.\nWe are able to predict rising novelties or popular nodes which is generally\nsuppressed under preferential attachment effect. To show performance of our\nmodel we have conducted experiments on four real data sets namely, MovieLens,\nNetflix, Facebook and Arxiv High Energy Physics paper citation. For testing our\nmodel we used four information retrieval indices namely Precision, Novelty,\nArea Under Receiving Operating Characteristic(AUC) and Kendal's rank\ncorrelation coefficient. We have used four benchmark models for validating our\nproposed models. Although our model doesn't perform better in all the cases\nbut, it has theoretical significance in working better for recent behavior\ndominant systems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:20:32 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Abbas", "Khushnood", ""]]}, {"id": "1611.07135", "submitter": "Jason Portenoy", "authors": "Jason Portenoy, Jessica Hullman, Jevin D. West", "title": "Leveraging Citation Networks to Visualize Scholarly Influence Over Time", "comments": null, "journal-ref": null, "doi": "10.3389/frma.2017.00008", "report-no": null, "categories": "cs.HC cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the influence of a scholar's work is an important task for funding\norganizations, academic departments, and researchers. Common methods, such as\nmeasures of citation counts, can ignore much of the nuance and\nmultidimensionality of scholarly influence. We present an approach for\ngenerating dynamic visualizations of scholars' careers. This approach uses an\nanimated node-link diagram showing the citation network accumulated around the\nresearcher over the course of the career in concert with key indicators,\nhighlighting influence both within and across fields. We developed our design\nin collaboration with one funding organization---the Pew Biomedical Scholars\nprogram---but the methods are generalizable to visualizations of scholarly\ninfluence. We applied the design method to the Microsoft Academic Graph, which\nincludes more than 120 million publications. We validate our abstractions\nthroughout the process through collaboration with the Pew Biomedical Scholars\nprogram officers and summative evaluations with their scholars.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:17:23 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 23:25:00 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Portenoy", "Jason", ""], ["Hullman", "Jessica", ""], ["West", "Jevin D.", ""]]}, {"id": "1611.07139", "submitter": "Reza Rawassizadeh", "authors": "Reza Rawassizadeh, Chelsea Dobbins, Manouchehr Nourizadeh, Zahra\n  Ghamchili, Michael Pazzani", "title": "A Natural Language Query Interface for Searching Personal Information on\n  Smartwatches", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, personal assistant systems, run on smartphones and use natural\nlanguage interfaces. However, these systems rely mostly on the web for finding\ninformation. Mobile and wearable devices can collect an enormous amount of\ncontextual personal data such as sleep and physical activities. These\ninformation objects and their applications are known as quantified-self, mobile\nhealth or personal informatics, and they can be used to provide a deeper\ninsight into our behavior. To our knowledge, existing personal assistant\nsystems do not support all types of quantified-self queries. In response to\nthis, we have undertaken a user study to analyze a set of \"textual\nquestions/queries\" that users have used to search their quantified-self or\nmobile health data. Through analyzing these questions, we have constructed a\nlight-weight natural language based query interface, including a text parser\nalgorithm and a user interface, to process the users' queries that have been\nused for searching quantified-self information. This query interface has been\ndesigned to operate on small devices, i.e. smartwatches, as well as augmenting\nthe personal assistant systems by allowing them to process end users' natural\nlanguage queries about their quantified-self data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:42:44 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Rawassizadeh", "Reza", ""], ["Dobbins", "Chelsea", ""], ["Nourizadeh", "Manouchehr", ""], ["Ghamchili", "Zahra", ""], ["Pazzani", "Michael", ""]]}, {"id": "1611.07233", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Pascal Hager, Luca Benini", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression\n  Artifact Suppression", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965927", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression algorithms are pervasively used to reduce the size of\nimages transmitted over the web and recorded on data storage media. However, we\npay for their high compression rate with visual artifacts degrading the user\nexperience. Deep convolutional neural networks have become a widespread tool to\naddress high-level computer vision tasks very successfully. Recently, they have\nfound their way into the areas of low-level computer vision and image\nprocessing to solve regression problems mostly with relatively shallow\nnetworks.\n  We present a novel 12-layer deep convolutional network for image compression\nartifact suppression with hierarchical skip connections and a multi-scale loss\nfunction. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an\nimprovement of up to 0.36 dB over the best previous ConvNet result. We show\nthat a network trained for a specific quality factor (QF) is resilient to the\nQF used to compress the input image - a single network trained for QF 60\nprovides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:11:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Hager", "Pascal", ""], ["Benini", "Luca", ""]]}, {"id": "1611.08096", "submitter": "Zheqian Chen", "authors": "Zheqian Chen and Ben Gao and Huimin Zhang and Zhou Zhao and Deng Cai", "title": "User Personalized Satisfaction Prediction via Multiple Instance Deep\n  Learning", "comments": "draft for www", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community based question answering services have arisen as a popular\nknowledge sharing pattern for netizens. With abundant interactions among users,\nindividuals are capable of obtaining satisfactory information. However, it is\nnot effective for users to attain answers within minutes. Users have to check\nthe progress over time until the satisfying answers submitted. We address this\nproblem as a user personalized satisfaction prediction task. Existing methods\nusually exploit manual feature selection. It is not desirable as it requires\ncareful design and is labor intensive. In this paper, we settle this issue by\ndeveloping a new multiple instance deep learning framework. Specifically, in\nour settings, each question follows a weakly supervised learning multiple\ninstance learning assumption, where its obtained answers can be regarded as\ninstance sets and we define the question resolved with at least one\nsatisfactory answer. We thus design an efficient framework exploiting multiple\ninstance learning property with deep learning to model the question answer\npairs. Extensive experiments on large scale datasets from Stack Exchange\ndemonstrate the feasibility of our proposed framework in predicting askers\npersonalized satisfaction. Our framework can be extended to numerous\napplications such as UI satisfaction Prediction, multi armed bandit problem,\nexpert finding and so on.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 08:43:03 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chen", "Zheqian", ""], ["Gao", "Ben", ""], ["Zhang", "Huimin", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""]]}, {"id": "1611.08135", "submitter": "Zheqian Chen", "authors": "Zheqian Chen and Chi Zhang and Zhou Zhao and Deng Cai", "title": "Question Retrieval for Community-based Question Answering via\n  Heterogeneous Network Integration Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community based question answering platforms have attracted substantial users\nto share knowledge and learn from each other. As the rapid enlargement of CQA\nplatforms, quantities of overlapped questions emerge, which makes users\nconfounded to select a proper reference. It is urgent for us to take effective\nautomated algorithms to reuse historical questions with corresponding answers.\nIn this paper we focus on the problem with question retrieval, which aims to\nmatch historical questions that are relevant or semantically equivalent to\nresolve one s query directly. The challenges in this task are the lexical gaps\nbetween questions for the word ambiguity and word mismatch problem.\nFurthermore, limited words in queried sentences cause sparsity of word\nfeatures. To alleviate these challenges, we propose a novel framework named\nHNIL which encodes not only the question contents but also the askers social\ninteractions to enhance the question embedding performance. More specifically,\nwe apply random walk based learning method with recurrent neural network to\nmatch the similarities between askers question and historical questions\nproposed by other users. Extensive experiments on a large scale dataset from a\nreal world CQA site show that employing the heterogeneous social network\ninformation outperforms the other state of the art solutions in this task.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 11:01:32 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Chen", "Zheqian", ""], ["Zhang", "Chi", ""], ["Zhao", "Zhou", ""], ["Cai", "Deng", ""]]}, {"id": "1611.08848", "submitter": "Elad Yom-Tov", "authors": "Elad Yom-Tov", "title": "Predicting drug recalls from Internet search engine queries", "comments": null, "journal-ref": null, "doi": "10.1109/JTEHM.2017.2732945", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batches of pharmaceutical are sometimes recalled from the market when a\nsafety issue or a defect is detected in specific production runs of a drug.\nSuch problems are usually detected when patients or healthcare providers report\nabnormalities to medical authorities. Here we test the hypothesis that\ndefective production lots can be detected earlier by monitoring queries to\nInternet search engines.\n  We extracted queries from the USA to the Bing search engine which mentioned\none of 5,195 pharmaceutical drugs during 2015 and all recall notifications\nissued by the Food and Drug Administration (FDA) during that year. By using\nattributes that quantify the change in query volume at the state level, we\nattempted to predict if a recall of a specific drug will be ordered by FDA in a\ntime horizon ranging from one to 40 days in future.\n  Our results show that future drug recalls can indeed be identified with an\nAUC of 0.791 and a lift at 5% of approximately 6 when predicting a recall will\noccur one day ahead. This performance degrades as prediction is made for longer\nperiods ahead. The most indicative attributes for prediction are sudden spikes\nin query volume about a specific medicine in each state. Recalls of\nprescription drugs and those estimated to be of medium-risk are more likely to\nbe identified using search query data.\n  These findings suggest that aggregated Internet search engine data can be\nused to facilitate in early warning of faulty batches of medicines.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 14:12:38 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yom-Tov", "Elad", ""]]}, {"id": "1611.09028", "submitter": "Albin Zehe", "authors": "Fotis Jannidis, Isabella Reger, Albin Zehe, Martin Becker, Lena\n  Hettinger, Andreas Hotho", "title": "Analyzing Features for the Detection of Happy Endings in German Novels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With regard to a computational representation of literary plot, this paper\nlooks at the use of sentiment analysis for happy ending detection in German\nnovels. Its focus lies on the investigation of previously proposed sentiment\nfeatures in order to gain insight about the relevance of specific features on\nthe one hand and the implications of their performance on the other hand.\nTherefore, we study various partitionings of novels, considering the highly\nvariable concept of \"ending\". We also show that our approach, even though still\nrather simple, can potentially lead to substantial findings relevant to\nliterary studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 08:56:04 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Jannidis", "Fotis", ""], ["Reger", "Isabella", ""], ["Zehe", "Albin", ""], ["Becker", "Martin", ""], ["Hettinger", "Lena", ""], ["Hotho", "Andreas", ""]]}, {"id": "1611.09083", "submitter": "Alexey Drutsa", "authors": "Alexey Drutsa (Yandex, Moscow, Russia), Gleb Gusev (Yandex, Moscow,\n  Russia), Pavel Serdyukov (Yandex, Moscow, Russia)", "title": "Prediction of Video Popularity in the Absence of Reliable Data from\n  Video Hosting Services: Utility of Traces Left by Users on the Web", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of user-generated content, we observe the constant rise of\nthe number of companies, such as search engines, content aggregators, etc.,\nthat operate with tremendous amounts of web content not being the services\nhosting it. Thus, aiming to locate the most important content and promote it to\nthe users, they face the need of estimating the current and predicting the\nfuture content popularity.\n  In this paper, we approach the problem of video popularity prediction not\nfrom the side of a video hosting service, as done in all previous studies, but\nfrom the side of an operating company, which provides a popular video search\nservice that aggregates content from different video hosting websites. We\ninvestigate video popularity prediction based on features from three primary\nsources available for a typical operating company: first, the content hosting\nprovider may deliver its data via its API, second, the operating company makes\nuse of its own search and browsing logs, third, the company crawls information\nabout embeds of a video and links to a video page from publicly available\nresources on the Web. We show that video popularity prediction based on the\nembed and link data coupled with the internal search and browsing data\nsignificantly improves video popularity prediction based only on the data\nprovided by the video hosting and can even adequately replace the API data in\nthe cases when it is partly or completely unavailable.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:43:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Drutsa", "Alexey", "", "Yandex, Moscow, Russia"], ["Gusev", "Gleb", "", "Yandex, Moscow,\n  Russia"], ["Serdyukov", "Pavel", "", "Yandex, Moscow, Russia"]]}, {"id": "1611.09084", "submitter": "Dario Garcia-Gasulla", "authors": "Dario Garcia-Gasulla, Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises\n  Cort\\'es, Toyotaro Suzumura", "title": "Hierarchical Hyperlink Prediction for the WWW", "comments": "Submitted to Transactions on Internet Technology journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hyperlink prediction task, that of proposing new links between webpages,\ncan be used to improve search engines, expand the visibility of web pages, and\nincrease the connectivity and navigability of the web. Hyperlink prediction is\ntypically performed on webgraphs composed by thousands or millions of vertices,\nwhere on average each webpage contains less than fifty links. Algorithms\nprocessing graphs so large and sparse require to be both scalable and precise,\na challenging combination. Similarity-based algorithms are among the most\nscalable solutions within the link prediction field, due to their parallel\nnature and computational simplicity. These algorithms independently explore the\nnearby topological features of every missing link from the graph in order to\ndetermine its likelihood. Unfortunately, the precision of similarity-based\nalgorithms is limited, which has prevented their broad application so far. In\nthis work we explore the performance of similarity-based algorithms for the\nparticular problem of hyperlink prediction on large webgraphs, and propose a\nnovel method which assumes the existence of hierarchical properties. We\nevaluate this new approach on several webgraphs and compare its performance\nwith that of the current best similarity-based algorithms. Its remarkable\nperformance leads us to argue on the applicability of the proposal, identifying\nseveral use cases of hyperlink prediction. We also describes the approach we\ntook for the computation of large-scale graphs from the perspective of\nhigh-performance computing, providing details on the implementation and\nparallelization of code.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:47:52 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""], ["Suzumura", "Toyotaro", ""]]}, {"id": "1611.09194", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION)", "title": "Times series averaging and denoising from a probabilistic perspective on\n  time-elastic kernels", "comments": "arXiv admin note: text overlap with arXiv:1505.06897. International\n  Journal of Applied Mathematics and Computer Science, June 2019", "journal-ref": null, "doi": "10.2478/amcs-2019-0028", "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the light of regularized dynamic time warping kernels, this paper\nre-considers the concept of time elastic centroid for a setof time series. We\nderive a new algorithm based on a probabilistic interpretation of kernel\nalignment matrices. This algorithm expressesthe averaging process in terms of a\nstochastic alignment automata. It uses an iterative agglomerative heuristic\nmethod for averagingthe aligned samples, while also averaging the times of\noccurrence of the aligned samples. By comparing classification accuracies for45\nheterogeneous time series datasets obtained by first nearest centroid/medoid\nclassifiers we show that: i) centroid-basedapproaches significantly outperform\nmedoid-based approaches, ii) for the considered datasets, our algorithm that\ncombines averagingin the sample space and along the time axes, emerges as the\nmost significantly robust model for time-elastic averaging with apromising\nnoise reduction capability. We also demonstrate its benefit in an isolated\ngesture recognition experiment and its ability tosignificantly reduce the size\nof training instance sets. Finally we highlight its denoising capability using\ndemonstrative synthetic data:we show that it is possible to retrieve, from few\nnoisy instances, a signal whose components are scattered in a wide spectral\nband.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:34:30 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 12:41:07 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 11:50:08 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 08:40:10 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"]]}, {"id": "1611.09235", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Chuwei Luo, Wenjie Li, Sujian Li", "title": "Joint Copying and Restricted Generation for Paraphrase", "comments": "7 pages, 1 figure, AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many natural language generation tasks, such as abstractive summarization and\ntext simplification, are paraphrase-orientated. In these tasks, copying and\nrewriting are two main writing modes. Most previous sequence-to-sequence\n(Seq2Seq) models use a single decoder and neglect this fact. In this paper, we\ndevelop a novel Seq2Seq model to fuse a copying decoder and a restricted\ngenerative decoder. The copying decoder finds the position to be copied based\non a typical attention model. The generative decoder produces words limited in\nthe source-specific vocabulary. To combine the two decoders and determine the\nfinal output, we develop a predictor to predict the mode of copying or\nrewriting. This predictor can be guided by the actual writing mode in the\ntraining data. We conduct extensive experiments on two different paraphrase\ndatasets. The result shows that our model outperforms the state-of-the-art\napproaches in terms of both informativeness and language quality.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:49:37 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cao", "Ziqiang", ""], ["Luo", "Chuwei", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""]]}, {"id": "1611.09238", "submitter": "Ziqiang Cao", "authors": "Ziqiang Cao, Wenjie Li, Sujian Li, Furu Wei", "title": "Improving Multi-Document Summarization via Text Classification", "comments": "7 pages, 3 figures, AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Developed so far, multi-document summarization has reached its bottleneck due\nto the lack of sufficient training data and diverse categories of documents.\nText classification just makes up for these deficiencies. In this paper, we\npropose a novel summarization system called TCSum, which leverages plentiful\ntext classification data to improve the performance of multi-document\nsummarization. TCSum projects documents onto distributed representations which\nact as a bridge between text classification and summarization. It also utilizes\nthe classification results to produce summaries of different styles. Extensive\nexperiments on DUC generic multi-document summarization datasets show that,\nTCSum can achieve the state-of-the-art performance without using any\nhand-crafted features and has the capability to catch the variations of summary\nstyles with respect to different text categories.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:53:06 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cao", "Ziqiang", ""], ["Li", "Wenjie", ""], ["Li", "Sujian", ""], ["Wei", "Furu", ""]]}, {"id": "1611.09268", "submitter": "Bhaskar Mitra", "authors": "Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao,\n  Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen,\n  Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary and Tong Wang", "title": "MS MARCO: A Human Generated MAchine Reading COmprehension Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a large scale MAchine Reading COmprehension dataset, which we\nname MS MARCO. The dataset comprises of 1,010,916 anonymized\nquestions---sampled from Bing's search query logs---each with a human generated\nanswer and 182,669 completely human rewritten generated answers. In addition,\nthe dataset contains 8,841,823 passages---extracted from 3,563,535 web\ndocuments retrieved by Bing---that provide the information necessary for\ncurating the natural language answers. A question in the MS MARCO dataset may\nhave multiple answers or no answers at all. Using this dataset, we propose\nthree different tasks with varying levels of difficulty: (i) predict if a\nquestion is answerable given a set of context passages, and extract and\nsynthesize the answer as a human would (ii) generate a well-formed answer (if\npossible) based on the context passages that can be understood with the\nquestion and passage context, and finally (iii) rank a set of retrieved\npassages given a question. The size of the dataset and the fact that the\nquestions are derived from real user search queries distinguishes MS MARCO from\nother well-known publicly available datasets for machine reading comprehension\nand question-answering. We believe that the scale and the real-world nature of\nthis dataset makes it attractive for benchmarking machine reading comprehension\nand question-answering models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 18:14:11 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 02:39:53 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2018 14:46:47 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Bajaj", "Payal", ""], ["Campos", "Daniel", ""], ["Craswell", "Nick", ""], ["Deng", "Li", ""], ["Gao", "Jianfeng", ""], ["Liu", "Xiaodong", ""], ["Majumder", "Rangan", ""], ["McNamara", "Andrew", ""], ["Mitra", "Bhaskar", ""], ["Nguyen", "Tri", ""], ["Rosenberg", "Mir", ""], ["Song", "Xia", ""], ["Stoica", "Alina", ""], ["Tiwary", "Saurabh", ""], ["Wang", "Tong", ""]]}, {"id": "1611.09316", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi and Lin Yang and Tim Weninger", "title": "Forward Backward Similarity Search in Knowledge Networks", "comments": "Accepted for publication in Knowledge-Based Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Similarity search is a fundamental problem in social and knowledge networks\nlike GitHub, DBLP, Wikipedia, etc. Existing network similarity measures are\nlimited because they only consider similarity from the perspective of the query\nnode. However, due to the complicated topology of real-world networks, ignoring\nthe preferences of target nodes often results in odd or unintuitive\nperformance. In this work, we propose a dual perspective similarity metric\ncalled Forward Backward Similarity (FBS) that efficiently computes topological\nsimilarity from the perspective of both the query node and the perspective of\ncandidate nodes. The effectiveness of our method is evaluated by traditional\nquantitative ranking metrics and large-scale human judgement on four large real\nworld networks. The proposed method matches human preference and outperforms\nother similarity search algorithms on community overlap and link prediction.\nFinally, we demonstrate top-5 rankings for five famous researchers on an\nacademic collaboration network to illustrate how our approach captures\nsemantics more intuitively than other approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:07:22 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Shi", "Baoxu", ""], ["Yang", "Lin", ""], ["Weninger", "Tim", ""]]}, {"id": "1611.09392", "submitter": "Ang Li", "authors": "Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, Larry S.\n  Davis", "title": "Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial relationships between objects provide important information for\ntext-based image retrieval. As users are more likely to describe a scene from a\nreal world perspective, using 3D spatial relationships rather than 2D\nrelationships that assume a particular viewing direction, one of the main\nchallenges is to infer the 3D structure that bridges images with users' text\ndescriptions. However, direct inference of 3D structure from images requires\nlearning from large scale annotated data. Since interactions between objects\ncan be reduced to a limited set of atomic spatial relations in 3D, we study the\npossibility of inferring 3D structure from a text description rather than an\nimage, applying physical relation models to synthesize holistic 3D abstract\nobject layouts satisfying the spatial constraints present in a textual\ndescription. We present a generic framework for retrieving images from a\ntextual description of a scene by matching images with these generated abstract\nobject layouts. Images are ranked by matching object detection outputs\n(bounding boxes) to 2D layout candidates (also represented by bounding boxes)\nwhich are obtained by projecting the 3D scenes with sampled camera directions.\nWe validate our approach using public indoor scene datasets and show that our\nmethod outperforms baselines built upon object occurrence histograms and\nlearned 2D pairwise relations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:29:07 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 20:37:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Li", "Ang", ""], ["Sun", "Jin", ""], ["Ng", "Joe Yue-Hei", ""], ["Yu", "Ruichi", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1611.09496", "submitter": "Ruiming Tang", "authors": "Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He", "title": "A Graph-based Push Service Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that learning customers' preference and making\nrecommendations to them from today's information-exploded environment is\ncritical and non-trivial in an on-line system. There are two different modes of\nrecommendation systems, namely pull-mode and push-mode. The majority of the\nrecommendation systems are pull-mode, which recommend items to users only when\nand after users enter Application Market. While push-mode works more actively\nto enhance or re-build connection between Application Market and users. As one\nof the most successful phone manufactures,both the number of users and apps\nincrease dramatically in Huawei Application Store (also named Hispace Store),\nwhich has approximately 0.3 billion registered users and 1.2 million apps until\n2016 and whose number of users is growing with high-speed. For the needs of\nreal scenario, we establish a Push Service Platform (shortly, PSP) to discover\nthe target user group automatically from web-scale user operation log data with\nan additional small set of labelled apps (usually around 10 apps),in Hispace\nStore. As presented in this work,PSP includes distributed storage layer,\napplication layer and evaluation layer. In the application layer, we design a\npractical graph-based algorithm (named A-PARW) for user group discovery, which\nis an approximate version of partially absorbing random walk. Based on I mode\nof A-PARW, the effectiveness of our system is significantly improved, compared\nto the predecessor to presented system, which uses Personalized Pagerank in its\napplication layer.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 05:36:41 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Guo", "Huifeng", ""], ["Tang", "Ruiming", ""], ["Ye", "Yunming", ""], ["Li", "Zhenguo", ""], ["He", "Xiuqiang", ""]]}, {"id": "1611.09573", "submitter": "Anoop V S", "authors": "V. S. Anoop, S. Asharaf and P. Deepak", "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling", "comments": null, "journal-ref": "International Journal of Information Processing (IJIP), Volume 10,\n  Issue 3, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of semantic web, various tools and techniques have been\nintroduced for presenting and organizing knowledge. Concept hierarchies are one\nsuch technique which gained significant attention due to its usefulness in\ncreating domain ontologies that are considered as an integral part of semantic\nweb. Automated concept hierarchy learning algorithms focus on extracting\nrelevant concepts from unstructured text corpus and connect them together by\nidentifying some potential relations exist between them. In this paper, we\npropose a novel approach for identifying relevant concepts from plain text and\nthen learns hierarchy of concepts by exploiting subsumption relation between\nthem. To start with, we model topics using a probabilistic topic model and then\nmake use of some lightweight linguistic process to extract semantically rich\nconcepts. Then we connect concepts by identifying an \"is-a\" relationship\nbetween pair of concepts. The proposed method is completely unsupervised and\nthere is no need for a domain specific training corpus for concept extraction\nand learning. Experiments on large and real-world text corpora such as BBC News\ndataset and Reuters News corpus shows that the proposed method outperforms some\nof the existing methods for concept extraction and efficient concept hierarchy\nlearning is possible if the overall task is guided by a probabilistic topic\nmodeling algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:28:59 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Anoop", "V. S.", ""], ["Asharaf", "S.", ""], ["Deepak", "P.", ""]]}, {"id": "1611.09921", "submitter": "Jian Tang", "authors": "Jian Tang, Cheng Li, Ming Zhang, and Qiaozhu Mei", "title": "Less is More: Learning Prominent and Diverse Topics for Data\n  Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical topic models efficiently facilitate the exploration of\nlarge-scale data sets. Many models have been developed and broadly used to\nsummarize the semantic structure in news, science, social media, and digital\nhumanities. However, a common and practical objective in data exploration tasks\nis not to enumerate all existing topics, but to quickly extract representative\nones that broadly cover the content of the corpus, i.e., a few topics that\nserve as a good summary of the data. Most existing topic models fit exactly the\nsame number of topics as a user specifies, which have imposed an unnecessary\nburden to the users who have limited prior knowledge. We instead propose new\nmodels that are able to learn fewer but more representative topics for the\npurpose of data summarization. We propose a reinforced random walk that allows\nprominent topics to absorb tokens from similar and smaller topics, thus\nenhances the diversity among the top topics extracted. With this reinforced\nrandom walk as a general process embedded in classical topic models, we obtain\n\\textit{diverse topic models} that are able to extract the most prominent and\ndiverse topics from data. The inference procedures of these diverse topic\nmodels remain as simple and efficient as the classical models. Experimental\nresults demonstrate that the diverse topic models not only discover topics that\nbetter summarize the data, but also require minimal prior knowledge of the\nusers.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 22:24:30 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 02:45:34 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Tang", "Jian", ""], ["Li", "Cheng", ""], ["Zhang", "Ming", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1611.10248", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION)", "title": "Assessing pattern recognition or labeling in streams of temporal data", "comments": null, "journal-ref": "2nd ECML/PKDD Workshop on Advanced Analytics and Learning on\n  Temporal Data, Sep 2016, Riva del Garda, Italy. 2016", "doi": null, "report-no": null, "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the data deluge context, pattern recognition or labeling in streams is\nbecoming quite an essential and pressing task as data flows inside always\nbigger streams. The assessment of such tasks is not so easy when dealing with\ntemporal data, namely patterns that have a duration (a beginning and an end\ntime-stamp). This paper details an approach based on an editing distance to\nfirst align a sequence of labeled temporal segments with a ground truth\nsequence, and then, by back-tracing an optimal alignment path, to provide a\nconfusion matrix at the label level. From this confusion matrix, standard\nevaluation measures can easily be derived as well as other measures such as the\n\"latency\" that can be quite important in (early) pattern detection\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 16:08:40 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"]]}, {"id": "1611.10277", "submitter": "Ryan Gallagher", "authors": "Ryan J. Gallagher, Kyle Reing, David Kale, Greg Ver Steeg", "title": "Anchored Correlation Explanation: Topic Modeling with Minimal Domain\n  Knowledge", "comments": "21 pages, 7 figures. 2018/09/03: Updated citation for HA/DR dataset", "journal-ref": "Transactions of the Association for Computational Linguistics\n  (TACL), Vol. 5, 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While generative models such as Latent Dirichlet Allocation (LDA) have proven\nfruitful in topic modeling, they often require detailed assumptions and careful\nspecification of hyperparameters. Such model complexity issues only compound\nwhen trying to generalize generative models to incorporate human input. We\nintroduce Correlation Explanation (CorEx), an alternative approach to topic\nmodeling that does not assume an underlying generative model, and instead\nlearns maximally informative topics through an information-theoretic framework.\nThis framework naturally generalizes to hierarchical and semi-supervised\nextensions with no additional modeling assumptions. In particular, word-level\ndomain knowledge can be flexibly incorporated within CorEx through anchor\nwords, allowing topic separability and representation to be promoted with\nminimal human intervention. Across a variety of datasets, metrics, and\nexperiments, we demonstrate that CorEx produces topics that are comparable in\nquality to those produced by unsupervised and semi-supervised variants of LDA.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:32:17 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 17:41:04 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 03:53:19 GMT"}, {"version": "v4", "created": "Mon, 3 Sep 2018 15:23:40 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Gallagher", "Ryan J.", ""], ["Reing", "Kyle", ""], ["Kale", "David", ""], ["Steeg", "Greg Ver", ""]]}]