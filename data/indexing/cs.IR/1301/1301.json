[{"id": "1301.0503", "submitter": "Charles Sutton", "authors": "Quim Castella and Charles Sutton", "title": "Word Storms: Multiples of Word Clouds for Visual Comparison of Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word clouds are a popular tool for visualizing documents, but they are not a\ngood tool for comparing documents, because identical words are not presented\nconsistently across different clouds. We introduce the concept of word storms,\na visualization tool for analysing corpora of documents. A word storm is a\ngroup of word clouds, in which each cloud represents a single document,\njuxtaposed to allow the viewer to compare and contrast the documents. We\npresent a novel algorithm that creates a coordinated word storm, in which words\nthat appear in multiple documents are placed in the same location, using the\nsame color and orientation, in all of the corresponding clouds. In this way,\nsimilar documents are represented by similar-looking word clouds, making them\neasier to compare and contrast visually. We evaluate the algorithm in two ways:\nfirst, an automatic evaluation based on document classification; and second, a\nuser study. The results confirm that unlike standard word clouds, a coordinated\nword storm better allows for visual comparison of documents.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 17:02:56 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Castella", "Quim", ""], ["Sutton", "Charles", ""]]}, {"id": "1301.0556", "submitter": "David Blei", "authors": "David Blei, J Andrew Bagnell, Andrew McCallum", "title": "Learning with Scope, with Application to Information Extraction and\n  Classification", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-53-60", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probabilistic approaches to classification and information extraction, one\ntypically builds a statistical model of words under the assumption that future\ndata will exhibit the same regularities as the training data. In many data\nsets, however, there are scope-limited features whose predictive power is only\napplicable to a certain subset of the data. For example, in information\nextraction from web pages, word formatting may be indicative of extraction\ncategory in different ways on different web pages. The difficulty with using\nsuch features is capturing and exploiting the new regularities encountered in\npreviously unseen data. In this paper, we propose a hierarchical probabilistic\nmodel that uses both local/scope-limited features, such as word formatting, and\nglobal features, such as word content. The local regularities are modeled as an\nunobserved random parameter which is drawn once for each local data set. This\nrandom parameter is estimated during the inference process and then used to\nperform classification with both the local and global features--- a procedure\nwhich is akin to automatically retuning the classifier to the local\nregularities on each newly encountered web page. Exact inference is intractable\nand we present approximations via point estimates and variational methods.\nEmpirical results on large collections of web data demonstrate that this method\nsignificantly improves performance from traditional models of global features\nalone.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:55:25 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Blei", "David", ""], ["Bagnell", "J Andrew", ""], ["McCallum", "Andrew", ""]]}, {"id": "1301.0575", "submitter": "Carl Kadie", "authors": "Carl Kadie, Christopher Meek, David Heckerman", "title": "CFW: A Collaborative Filtering System Using Posteriors Over Weights Of\n  Evidence", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-242-250", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CFW, a computationally efficient algorithm for collaborative\nfiltering that uses posteriors over weights of evidence. In experiments on real\ndata, we show that this method predicts as well or better than other methods in\nsituations where the size of the user query is small. The new approach works\nparticularly well when the user s query CONTAINS low frequency(unpopular)\nitems.The approach complements that OF dependency networks which perform well\nWHEN the size OF the query IS large.Also IN this paper, we argue that the USE\nOF posteriors OVER weights OF evidence IS a natural way TO recommend similar\nitems collaborative - filtering task.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:56:42 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:04:17 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Kadie", "Carl", ""], ["Meek", "Christopher", ""], ["Heckerman", "David", ""]]}, {"id": "1301.0588", "submitter": "Thomas P. Minka", "authors": "Thomas P. Minka, John Lafferty", "title": "Expectation-Propogation for the Generative Aspect Model", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-352-359", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generative aspect model is an extension of the multinomial model for text\nthat allows word probabilities to vary stochastically across documents.\nPrevious results with aspect models have been promising, but hindered by the\ncomputational difficulty of carrying out inference and learning. This paper\ndemonstrates that the simple variational methods of Blei et al (2001) can lead\nto inaccurate inferences and biased learning for the generative aspect model.\nWe develop an alternative approach that leads to higher accuracy at comparable\ncost. An extension of Expectation-Propagation is used for inference and then\nembedded in an EM algorithm for learning. Experimental results are presented\nfor both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:57:35 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Minka", "Thomas P.", ""], ["Lafferty", "John", ""]]}, {"id": "1301.0600", "submitter": "Guy Shani", "authors": "Guy Shani, Ronen I. Brafman, David Heckerman", "title": "An MDP-based Recommender System", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-453-460", "categories": "cs.LG cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical Recommender systems adopt a static view of the recommendation process\nand treat it as a prediction problem. We argue that it is more appropriate to\nview the problem of generating recommendations as a sequential decision problem\nand, consequently, that Markov decision processes (MDP) provide a more\nappropriate model for Recommender systems. MDPs introduce two benefits: they\ntake into account the long-term effects of each recommendation, and they take\ninto account the expected value of each recommendation. To succeed in practice,\nan MDP-based Recommender system must employ a strong initial model; and the\nbulk of this paper is concerned with the generation of such a model. In\nparticular, we suggest the use of an n-gram predictive model for generating the\ninitial MDP. Our n-gram model induces a Markov-chain model of user behavior\nwhose predictive accuracy is greater than that of existing predictive models.\nWe describe our predictive model in detail and evaluate its performance on real\ndata. In addition, we show how the model can be used in an MDP-based\nRecommender system.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:58:21 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:00:34 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Shani", "Guy", ""], ["Brafman", "Ronen I.", ""], ["Heckerman", "David", ""]]}, {"id": "1301.0701", "submitter": "Rajendra Prasath Dr", "authors": "R.Rajendra Prasath and Pinar \\\"Ozt\\\"urk", "title": "Similarity Assessment through blocking and affordance assignment in\n  Textual CBR", "comments": "10 pages, 3 figures, WebCBR 2010, Alessandria, Italy", "journal-ref": "in: Proc. of the Reasoning from Experiences on the Web (WebCBR\n  2010), pp. 151-160, July 2010", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been conceived that children learn new objects through their\naffordances, that is, the actions that can be taken on them. We suggest that\nweb pages also have affordances defined in terms of the users' information need\nthey meet. An assumption of the proposed approach is that different parts of a\ntext may not be equally important / relevant to a given query. Judgment on the\nrelevance of a web document requires, therefore, a thorough look into its\nparts, rather than treating it as a monolithic content. We propose a method to\nextract and assign affordances to texts and then use these affordances to\nretrieve the corresponding web pages. The overall approach presented in the\npaper relies on case-based representations that bridge the queries to the\naffordances of web documents. We tested our method on the tourism domain and\nthe results are promising.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 10:36:34 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Prasath", "R. Rajendra", ""], ["\u00d6zt\u00fcrk", "Pinar", ""]]}, {"id": "1301.1626", "submitter": "Vivek Kandiah", "authors": "Vivek Kandiah and Dima L. Shepelyansky", "title": "Google matrix analysis of DNA sequences", "comments": "latex, 11 figs", "journal-ref": "PLoS ONE 8(5): e61519, 2013", "doi": "10.1371/journal.pone.0061519", "report-no": null, "categories": "q-bio.GN cs.IR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For DNA sequences of various species we construct the Google matrix G of\nMarkov transitions between nearby words composed of several letters. The\nstatistical distribution of matrix elements of this matrix is shown to be\ndescribed by a power law with the exponent being close to those of outgoing\nlinks in such scale-free networks as the World Wide Web (WWW). At the same time\nthe sum of ingoing matrix elements is characterized by the exponent being\nsignificantly larger than those typical for WWW networks. This results in a\nslow algebraic decay of the PageRank probability determined by the distribution\nof ingoing elements. The spectrum of G is characterized by a large gap leading\nto a rapid relaxation process on the DNA sequence networks. We introduce the\nPageRank proximity correlator between different species which determines their\nstatistical similarity from the view point of Markov chains. The properties of\nother eigenstates of the Google matrix are also discussed. Our results\nestablish scale-free features of DNA sequence networks showing their\nsimilarities and distinctions with the WWW and linguistic networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 18:22:43 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Kandiah", "Vivek", ""], ["Shepelyansky", "Dima L.", ""]]}, {"id": "1301.1894", "submitter": "Trisiladevi C Nagavi", "authors": "Trisiladevi C. Nagavi and Nagappa U. Bhajantri", "title": "An Extensive Analysis of Query by Singing/Humming System Through Query\n  Proportion", "comments": "14 pages,11 figures; The International Journal of Multimedia & Its\n  Applications (IJMA) Vol.4, No.6, December 2012. arXiv admin note: text\n  overlap with arXiv:1003.4083 by other authors", "journal-ref": null, "doi": "10.5121/ijma.2012.4606", "report-no": null, "categories": "cs.MM cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system\nwith small audio excerpt as query. The rising availability of digital music\nstipulates effective music retrieval methods. Further, MIR systems support\ncontent based searching for music and requires no musical acquaintance. Current\nwork on QBSH focuses mainly on melody features such as pitch, rhythm, note\netc., size of databases, response time, score matching and search algorithms.\nEven though a variety of QBSH techniques are proposed, there is a dearth of\nwork to analyze QBSH through query excerption. Here, we present an analysis\nthat works on QBSH through query excerpt. To substantiate a series of\nexperiments are conducted with the help of Mel-Frequency Cepstral Coefficients\n(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral\nCoefficients (LPCC) to portray the robustness of the knowledge representation.\nProposed experiments attempt to reveal that retrieval performance as well as\nprecision diminishes in the snail phase with the growing database size.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2013 15:36:40 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Nagavi", "Trisiladevi C.", ""], ["Bhajantri", "Nagappa U.", ""]]}, {"id": "1301.2172", "submitter": "Tarek Zlitni", "authors": "Bassem Bouaziz, Walid Mahdi, Tarek Zlitni and Abdelmajid ben Hamadou", "title": "Content-Based Video Browsing by Text Region Localization and\n  Classification", "comments": "11 pages, 12 figures, International Journal of Video & Image\n  Processing and Network Security IJVIPNS-IJENS Vol:10 No: 01", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of digital video data is increasing over the world. It highlights\nthe need for efficient algorithms that can index, retrieve and browse this data\nby content. This can be achieved by identifying semantic description captured\nautomatically from video structure. Among these descriptions, text within video\nis considered as rich features that enable a good way for video indexing and\nbrowsing. Unlike most video text detection and extraction methods that treat\nvideo sequences as collections of still images, we propose in this paper\nspatiotemporal. video-text localization and identification approach which\nproceeds in two main steps: text region localization and text region\nclassification. In the first step we detect the significant appearance of the\nnew objects in a frame by a split and merge processes applied on binarized edge\nframe pair differences. Detected objects are, a priori, considered as text.\nThey are then filtered according to both local contrast variation and texture\ncriteria in order to get the effective ones. The resulted text regions are\nclassified based on a visual grammar descriptor containing a set of semantic\ntext class regions characterized by visual features. A visual table of content\nis then generated based on extracted text regions occurring within video\nsequence enriched by a semantic identification. The experimentation performed\non a variety of video sequences shows the efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:40 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Bouaziz", "Bassem", ""], ["Mahdi", "Walid", ""], ["Zlitni", "Tarek", ""], ["Hamadou", "Abdelmajid ben", ""]]}, {"id": "1301.2173", "submitter": "Tarek Zlitni", "authors": "Baseem Bouaziz, Tarek Zlitni and Walid Mahdi", "title": "AViTExt: Automatic Video Text Extraction, A new Approach for video\n  content indexing Application", "comments": "5 pages, 5 figures, 3rd International Conference on Information and\n  Communication Technologies: From Theory to Applications(ICTTA 2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spatial temporal video-text detection technique\nwhich proceed in two principal steps:potential text region detection and a\nfiltering process. In the first step we divide dynamically each pair of\nconsecutive video frames into sub block in order to detect change. A\nsignificant difference between homologous blocks implies the appearance of an\nimportant object which may be a text region. The temporal redundancy is then\nused to filter these regions and forms an effective text region. The\nexperimentation driven on a variety of video sequences shows the effectiveness\nof our approach by obtaining a 89,39% as precision rate and 90,19 as recall.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:29:11 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Bouaziz", "Baseem", ""], ["Zlitni", "Tarek", ""], ["Mahdi", "Walid", ""]]}, {"id": "1301.2200", "submitter": "Tarek Zlitni", "authors": "Tarek Zlitni and Walid Mahdi", "title": "A Visual Grammar Approach for TV Program Identification", "comments": "8 pages, 6 figures, (IJCNS) International Journal of Computer and\n  Network Security, Vol. 2, No. 9, September 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic identification of TV programs within TV streams is an important\ntask for archive exploitation. This paper proposes a new spatial-temporal\napproach to identify programs in TV streams in two main steps: First, a\nreference catalogue for video grammars of visual jingles is constructed. We\nexploit visual grammars characterizing instances of the same program type in\norder to identify the various program types in the TV stream. The role of video\ngrammar is to represent the visual invariants for each visual jingle using a\nset of descriptors appropriate for each TV program. Secondly, programs in TV\nstreams are identified by examining the similarity of the video signal to the\nvisual grammars in the catalogue. The main idea of identification process\nconsists in comparing the visual similarity of the video signal signature in TV\nstream to the catalogue elements. After presenting the proposed approach, the\npaper overviews the encouraging experimental results on several streams\nextracted from different channels and composed of several programs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 17:56:02 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Zlitni", "Tarek", ""], ["Mahdi", "Walid", ""]]}, {"id": "1301.2303", "submitter": "Alexandrin Popescul", "authors": "Alexandrin Popescul, Lyle H. Ungar, David M Pennock, Steve Lawrence", "title": "Probabilistic Models for Unified Collaborative and Content-Based\n  Recommendation in Sparse-Data Environments", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-437-444", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems leverage product and community information to target\nproducts to consumers. Researchers have developed collaborative recommenders,\ncontent-based recommenders, and (largely ad-hoc) hybrid systems. We propose a\nunified probabilistic framework for merging collaborative and content-based\nrecommendations. We extend Hofmann's [1999] aspect model to incorporate\nthree-way co-occurrence data among users, items, and item content. The relative\ninfluence of collaboration data versus content data is not imposed as an\nexogenous parameter, but rather emerges naturally from the given data sources.\nGlobal probabilistic models coupled with standard Expectation Maximization (EM)\nlearning algorithms tend to drastically overfit in sparse-data situations, as\nis typical in recommendation applications. We show that secondary content\ninformation can often be used to overcome sparsity. Experiments on data from\nthe ResearchIndex library of Computer Science publications show that\nappropriate mixture models incorporating secondary data produce significantly\nbetter quality recommenders than k-nearest neighbors (k-NN). Global\nprobabilistic models also allow more general inferences than local methods like\nk-NN.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:25:59 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Popescul", "Alexandrin", ""], ["Ungar", "Lyle H.", ""], ["Pennock", "David M", ""], ["Lawrence", "Steve", ""]]}, {"id": "1301.2309", "submitter": "Rita Sharma", "authors": "Rita Sharma, David L Poole", "title": "Symmetric Collaborative Filtering Using the Noisy Sensor Model", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-488-495", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is the process of making recommendations regarding\nthe potential preference of a user, for example shopping on the Internet, based\non the preference ratings of the user and a number of other users for various\nitems. This paper considers collaborative filtering based on\nexplicitmulti-valued ratings. To evaluate the algorithms, weconsider only {em\npure} collaborative filtering, using ratings exclusively, and no other\ninformation about the people or items.Our approach is to predict a user's\npreferences regarding a particularitem by using other people who rated that\nitem and other items ratedby the user as noisy sensors. The noisy sensor model\nuses Bayes' theorem to compute the probability distribution for the\nuser'srating of a new item. We give two variant models: in one, we learn a{em\nclassical normal linear regression} model of how users rate items; in\nanother,we assume different users rate items the same, but the accuracy of\nthesensors needs to be learned. We compare these variant models\nwithstate-of-the-art techniques and show how they are significantly\nbetter,whether a user has rated only two items or many. We reportempirical\nresults using the EachMovie database\nfootnote{http://research.compaq.com/SRC/eachmovie/} of movie ratings. Wealso\nshow that by considering items similarity along with theusers similarity, the\naccuracy of the prediction increases.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:26:26 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Sharma", "Rita", ""], ["Poole", "David L", ""]]}, {"id": "1301.2320", "submitter": "Andrew Zimdars", "authors": "Andrew Zimdars, David Maxwell Chickering, Christopher Meek", "title": "Using Temporal Data for Making Recommendations", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-580-588", "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat collaborative filtering as a univariate time series estimation\nproblem: given a user's previous votes, predict the next vote. We describe two\nfamilies of methods for transforming data to encode time order in ways amenable\nto off-the-shelf classification and density estimation tools, and examine the\nresults of using these approaches on several real-world data sets. The\nimprovements in predictive accuracy we realize recommend the use of other\npredictive algorithms that exploit the temporal order of data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:27:15 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Zimdars", "Andrew", ""], ["Chickering", "David Maxwell", ""], ["Meek", "Christopher", ""]]}, {"id": "1301.2542", "submitter": "Amera Eletrebe", "authors": "Mohamed Eisa, Amira Eletrebi, Ebrahim Elhenawy", "title": "Enhancing the retrieval performance by combing the texture and edge\n  features", "comments": "7 pages,8 figures, one table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, anew algorithm which is based on geometrical moments and local\nbinary patterns (LBP) for content based image retrieval (CBIR) is proposed. In\ngeometrical moments, each vector is compared with the all other vectors for\nedge map generation. The same concept is utilized at LBP calculation which is\ngenerating nine LBP patterns from a given 3x3 pattern. Finally, nine LBP\nhistograms are calculated which are used as a feature vector for image\nretrieval. Moments are important features used in recognition of different\ntypes of images. Two experiments have been carried out for proving the worth of\nour algorithm. The results after being investigated shows a significant\nimprovement in terms of their evaluation measures as compared to LBP and other\nexisting transform domain techniques.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 15:13:47 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Eisa", "Mohamed", ""], ["Eletrebi", "Amira", ""], ["Elhenawy", "Ebrahim", ""]]}, {"id": "1301.2628", "submitter": "Xu-Cheng Yin", "authors": "Xu-Cheng Yin, Xuwang Yin, Kaizhu Huang, Hong-Wei Hao", "title": "Robust Text Detection in Natural Scene Images", "comments": "A Draft Version (Submitted to IEEE TPAMI)", "journal-ref": "IEEE Trans. Pattern Analysis and Machine Intelligence, vol. 36,\n  no. 5, pp. 970-983, 2014", "doi": "10.1109/TPAMI.2013.182", "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text detection in natural scene images is an important prerequisite for many\ncontent-based image analysis tasks. In this paper, we propose an accurate and\nrobust method for detecting texts in natural scene images. A fast and effective\npruning algorithm is designed to extract Maximally Stable Extremal Regions\n(MSERs) as character candidates using the strategy of minimizing regularized\nvariations. Character candidates are grouped into text candidates by the\ningle-link clustering algorithm, where distance weights and threshold of the\nclustering algorithm are learned automatically by a novel self-training\ndistance metric learning algorithm. The posterior probabilities of text\ncandidates corresponding to non-text are estimated with an character\nclassifier; text candidates with high probabilities are then eliminated and\nfinally texts are identified with a text classifier. The proposed system is\nevaluated on the ICDAR 2011 Robust Reading Competition dataset; the f measure\nis over 76% and is significantly better than the state-of-the-art performance\nof 71%. Experimental results on a publicly available multilingual dataset also\nshow that our proposed method can outperform the other competitive method with\nthe f measure increase of over 9 percent. Finally, we have setup an online demo\nof our proposed scene text detection system at\nhttp://kems.ustb.edu.cn/learning/yin/dtext.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 23:08:15 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2013 19:57:46 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2013 16:27:49 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Yin", "Xu-Cheng", ""], ["Yin", "Xuwang", ""], ["Huang", "Kaizhu", ""], ["Hao", "Hong-Wei", ""]]}, {"id": "1301.2785", "submitter": "Rafi Muhammad", "authors": "Muhammad Rafi, Mohammad Shahid Shaikh", "title": "A comparison of SVM and RVM for Document Classification", "comments": "ICoCSIM 2012, Medan Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document classification is a task of assigning a new unclassified document to\none of the predefined set of classes. The content based document classification\nuses the content of the document with some weighting criteria to assign it to\none of the predefined classes. It is a major task in library science,\nelectronic document management systems and information sciences. This paper\ninvestigates document classification by using two different classification\ntechniques (1) Support Vector Machine (SVM) and (2) Relevance Vector Machine\n(RVM). SVM is a supervised machine learning technique that can be used for\nclassification task. In its basic form, SVM represents the instances of the\ndata into space and tries to separate the distinct classes by a maximum\npossible wide gap (hyper plane) that separates the classes. On the other hand\nRVM uses probabilistic measure to define this separation space. RVM uses\nBayesian inference to obtain succinct solution, thus RVM uses significantly\nfewer basis functions. Experimental studies on three standard text\nclassification datasets reveal that although RVM takes more training time, its\nclassification is much better as compared to SVM.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2013 15:58:09 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Rafi", "Muhammad", ""], ["Shaikh", "Mohammad Shahid", ""]]}, {"id": "1301.3195", "submitter": "Zhen Hu", "authors": "Zhen Hu and Kun Fu and Changshui Zhang", "title": "Audio Classical Composer Identification by Deep Neural Network", "comments": "I will update it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Classical Composer Identification (ACC) is an important problem in\nMusic Information Retrieval (MIR) which aims at identifying the composer for\naudio classical music clips. The famous annual competition, Music Information\nRetrieval Evaluation eXchange (MIREX), also takes it as one of the four\ntraining&testing tasks. We built a hybrid model based on Deep Belief Network\n(DBN) and Stacked Denoising Autoencoder (SDA) to identify the composer from\naudio signal. As a matter of copyright, sponsors of MIREX cannot publish their\ndata set. We built a comparable data set to test our model. We got an accuracy\nof 76.26% in our data set which is better than some pure models and shallow\nmodels. We think our method is promising even though we test it in a different\ndata set, since our data set is comparable to that in MIREX by size. We also\nfound that samples from different classes become farther away from each other\nwhen transformed by more layers in our model.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 01:25:24 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 12:13:31 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2013 02:02:58 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2013 08:31:34 GMT"}, {"version": "v5", "created": "Thu, 14 Mar 2013 08:51:14 GMT"}, {"version": "v6", "created": "Wed, 27 Mar 2013 06:36:07 GMT"}, {"version": "v7", "created": "Wed, 16 Mar 2016 03:31:56 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Hu", "Zhen", ""], ["Fu", "Kun", ""], ["Zhang", "Changshui", ""]]}, {"id": "1301.3461", "submitter": "Cheng Zhang", "authors": "Cheng Zhang and Carl Henrik Ek and Andreas Damianou and Hedvig\n  Kjellstrom", "title": "Factorized Topic Models", "comments": "ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a modification to a latent topic model, which makes\nthe model exploit supervision to produce a factorized representation of the\nobserved data. The structured parameterization separately encodes variance that\nis shared between classes from variance that is private to each class by the\nintroduction of a new prior over the topic space. The approach allows for a\nmore eff{}icient inference and provides an intuitive interpretation of the data\nin terms of an informative signal together with structured noise. The\nfactorized representation is shown to enhance inference performance for image,\ntext, and video classification.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 19:32:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 11:05:05 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2013 09:50:28 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2013 14:16:39 GMT"}, {"version": "v5", "created": "Fri, 15 Mar 2013 17:14:58 GMT"}, {"version": "v6", "created": "Wed, 10 Apr 2013 20:15:04 GMT"}, {"version": "v7", "created": "Tue, 23 Apr 2013 08:13:55 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Zhang", "Cheng", ""], ["Ek", "Carl Henrik", ""], ["Damianou", "Andreas", ""], ["Kjellstrom", "Hedvig", ""]]}, {"id": "1301.3488", "submitter": "Mathieu Raffinot", "authors": "Djamal Belazzougui, Roman Kolpakov, Mathieu Raffinot", "title": "Various improvements to text fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let s = s_1 .. s_n be a text (or sequence) on a finite alphabet \\Sigma of\nsize \\sigma. A fingerprint in s is the set of distinct characters appearing in\none of its substrings. The problem considered here is to compute the set {\\cal\nF} of all fingerprints of all substrings of s in order to answer efficiently\ncertain questions on this set. A substring s_i .. s_j is a maximal location for\na fingerprint f in F (denoted by <i,j>) if the alphabet of s_i .. s_j is f and\ns_{i-1}, s_{j+1}, if defined, are not in f. The set of maximal locations ins is\n{\\cal L} (it is easy to see that |{\\cal L}| \\leq n \\sigma). Two maximal\nlocations <i,j> and <k,l> such that s_i .. s_j = s_k .. s_l are named {\\em\ncopies}, and the quotient set of {\\cal L} according to the copy relation is\ndenoted by {\\cal L}_C. We present new exact and approximate efficient\nalgorithms and data structures for the following three problems: (1) to compute\n{\\cal F}; (2) given f as a set of distinct characters in \\Sigma, to answer if f\nrepresents a fingerprint in {\\cal F}; (3) given f, to find all maximal\nlocations of f in s.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 20:59:43 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Belazzougui", "Djamal", ""], ["Kolpakov", "Roman", ""], ["Raffinot", "Mathieu", ""]]}, {"id": "1301.3862", "submitter": "David Heckerman", "authors": "David Heckerman, David Maxwell Chickering, Christopher Meek, Robert\n  Rounthwaite, Carl Kadie", "title": "Dependency Networks for Collaborative Filtering and Data Visualization", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-264-273", "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a graphical model for probabilistic relationships---an\nalternative to the Bayesian network---called a dependency network. The graph of\na dependency network, unlike a Bayesian network, is potentially cyclic. The\nprobability component of a dependency network, like a Bayesian network, is a\nset of conditional distributions, one for each node given its parents. We\nidentify several basic properties of this representation and describe a\ncomputationally efficient procedure for learning the graph and probability\ncomponents from data. We describe the application of this representation to\nprobabilistic inference, collaborative filtering (the task of predicting\npreferences), and the visualization of acausal predictive relationships.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:50:38 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Heckerman", "David", ""], ["Chickering", "David Maxwell", ""], ["Meek", "Christopher", ""], ["Rounthwaite", "Robert", ""], ["Kadie", "Carl", ""]]}, {"id": "1301.3885", "submitter": "David M Pennock", "authors": "David M. Pennock, Eric J. Horvitz, Steve Lawrence, C. Lee Giles", "title": "Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and\n  Model-Based Approach", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-473-480", "categories": "cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of Internet commerce has stimulated the use of collaborative\nfiltering (CF) algorithms as recommender systems. Such systems leverage\nknowledge about the known preferences of multiple users to recommend items of\ninterest to other users. CF methods have been harnessed to make recommendations\nabout such items as web pages, movies, books, and toys. Researchers have\nproposed and evaluated many approaches for generating recommendations. We\ndescribe and evaluate a new method called emph{personality diagnosis (PD)}.\nGiven a user's preferences for some items, we compute the probability that he\nor she is of the same \"personality type\" as other users, and, in turn, the\nprobability that he or she will like new items. PD retains some of the\nadvantages of traditional similarity-weighting techniques in that all data is\nbrought to bear on each prediction and new data can be added easily and\nincrementally. Additionally, PD has a meaningful probabilistic interpretation,\nwhich may be leveraged to justify, explain, and augment results. We report\nempirical results on the EachMovie database of movie ratings, and on user\nprofile data collected from the CiteSeer digital library of Computer Science\nresearch papers. The probabilistic framework naturally supports a variety of\ndescriptive measurements - in particular, we consider the applicability of a\nvalue of information (VOI) computation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:52:09 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Pennock", "David M.", ""], ["Horvitz", "Eric J.", ""], ["Lawrence", "Steve", ""], ["Giles", "C. Lee", ""]]}, {"id": "1301.4171", "submitter": "Jason  Weston", "authors": "Jason Weston, Ron Weiss, Hector Yee", "title": "Affinity Weighted Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised (linear) embedding models like Wsabie and PSI have proven\nsuccessful at ranking, recommendation and annotation tasks. However, despite\nbeing scalable to large datasets they do not take full advantage of the extra\ndata due to their linear nature, and typically underfit. We propose a new class\nof models which aim to provide improved performance while retaining many of the\nbenefits of the existing class of embedding models. Our new approach works by\niteratively learning a linear embedding model where the next iteration's\nfeatures and labels are reweighted as a function of the previous iteration. We\ndescribe several variants of the family, and give some initial results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 17:46:27 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Weston", "Jason", ""], ["Weiss", "Ron", ""], ["Yee", "Hector", ""]]}, {"id": "1301.4351", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Applying machine learning techniques to improve user acceptance on\n  ubiquitous environement", "comments": null, "journal-ref": null, "doi": null, "report-no": "urn:nbn:de:0074-731-7 Vol-731", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous information access becomes more and more important nowadays and\nresearch is aimed at making it adapted to users. Our work consists in applying\nmachine learning techniques in order to adapt the information access provided\nby ubiquitous systems to users when the system only knows the user social\ngroup, without knowing anything about the user interest. The adaptation\nprocedures associate actions to perceived situations of the user. Associations\nare based on feedback given by the user as a reaction to the behavior of the\nsystem. Our method brings a solution to some of the problems concerning the\nacceptance of the system by users when applying machine learning techniques to\nsystems at the beginning of the interaction between the system and the user.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 11:26:54 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1301.4781", "submitter": "Christophe Cruz", "authors": "David Werner (Le2i), Christophe Cruz (Le2i), Christophe Nicolle (Le2i)", "title": "Ontology-based Recommender System of Economic Articles", "comments": null, "journal-ref": "8th International Conference on Web Information Systems and\n  Technologies, Porto : Portugal (2013)", "doi": null, "report-no": null, "categories": "cs.IR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision makers need economical information to drive their decisions. The\nCompany Actualis SARL is specialized in the production and distribution of a\npress review about French regional economic actors. This economic review\nrepresents for a client a prospecting tool on partners and competitors. To\nreduce the overload of useless information, the company is moving towards a\ncustomized review for each customer. Three issues appear to achieve this goal.\nFirst, how to identify the elements in the text in order to extract objects\nthat match with the recommendation's criteria presented? Second, How to define\nthe structure of these objects, relationships and articles in order to provide\na source of knowledge usable by the extraction process to produce new knowledge\nfrom articles? The latter issue is the feedback on customer experience to\nidentify the quality of distributed information in real-time and to improve the\nrelevance of the recommendations. This paper presents a new type of\nrecommendation based on the semantic description of both articles and user\nprofile.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 08:15:16 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Werner", "David", "", "Le2i"], ["Cruz", "Christophe", "", "Le2i"], ["Nicolle", "Christophe", "", "Le2i"]]}, {"id": "1301.4916", "submitter": "Denzil Correa", "authors": "Denzil Correa, Ashish Sureka", "title": "Solutions to Detect and Analyze Online Radicalization : A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Radicalization (also called Cyber-Terrorism or Extremism or\nCyber-Racism or Cyber- Hate) is widespread and has become a major and growing\nconcern to the society, governments and law enforcement agencies around the\nworld. Research shows that various platforms on the Internet (low barrier to\npublish content, allows anonymity, provides exposure to millions of users and a\npotential of a very quick and widespread diffusion of message) such as YouTube\n(a popular video sharing website), Twitter (an online micro-blogging service),\nFacebook (a popular social networking website), online discussion forums and\nblogosphere are being misused for malicious intent. Such platforms are being\nused to form hate groups, racist communities, spread extremist agenda, incite\nanger or violence, promote radicalization, recruit members and create virtual\norgani- zations and communities. Automatic detection of online radicalization\nis a technically challenging problem because of the vast amount of the data,\nunstructured and noisy user-generated content, dynamically changing content and\nadversary behavior. There are several solutions proposed in the literature\naiming to combat and counter cyber-hate and cyber-extremism. In this survey, we\nreview solutions to detect and analyze online radicalization. We review 40\npapers published at 12 venues from June 2003 to November 2011. We present a\nnovel classification scheme to classify these papers. We analyze these\ntechniques, perform trend analysis, discuss limitations of existing techniques\nand find out research gaps.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 16:25:19 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Correa", "Denzil", ""], ["Sureka", "Ashish", ""]]}, {"id": "1301.5177", "submitter": "Andrea Scharnhorst", "authors": "Linda Reijnhoudt, Rodrigo Costas, Ed Noyons, Katy Boerner, Andrea\n  Scharnhorst", "title": "\"Seed+Expand\": A validated methodology for creating high quality\n  publication oeuvres of individual researchers", "comments": "Paper accepted for the ISSI 2013, small changes in the text due to\n  referee comments, one figure added (Fig 3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The study of science at the individual micro-level frequently requires the\ndisambiguation of author names. The creation of author's publication oeuvres\ninvolves matching the list of unique author names to names used in publication\ndatabases. Despite recent progress in the development of unique author\nidentifiers, e.g., ORCID, VIVO, or DAI, author disambiguation remains a key\nproblem when it comes to large-scale bibliometric analysis using data from\nmultiple databases. This study introduces and validates a new methodology\ncalled seed+expand for semi-automatic bibliographic data collection for a given\nset of individual authors. Specifically, we identify the oeuvre of a set of\nDutch full professors during the period 1980-2011. In particular, we combine\nauthor records from the National Research Information System (NARCIS) with\npublication records from the Web of Science. Starting with an initial list of\n8,378 names, we identify \"seed publications\" for each author using five\ndifferent approaches. Subsequently, we \"expand\" the set of publication in three\ndifferent approaches. The different approaches are compared and resulting\noeuvres are evaluated on precision and recall using a \"gold standard\" dataset\nof authors for which verified publications in the period 2001-2010 are\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 13:16:15 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2013 11:01:55 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Reijnhoudt", "Linda", ""], ["Costas", "Rodrigo", ""], ["Noyons", "Ed", ""], ["Boerner", "Katy", ""], ["Scharnhorst", "Andrea", ""]]}, {"id": "1301.5871", "submitter": "Pierre-Francois Marteau", "authors": "Muhammad Marwan Muhammad Fuad (VALORIA), Pierre-Fran\\c{c}ois Marteau\n  (VALORIA)", "title": "Towards a faster symbolic aggregate approximation method", "comments": "ICSOFT 2010 - Fifth International Conference on Software and Data\n  Technologies, Athens : Greece (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The similarity search problem is one of the main problems in time series data\nmining. Traditionally, this problem was tackled by sequentially comparing the\ngiven query against all the time series in the database, and returning all the\ntime series that are within a predetermined threshold of that query. But the\nlarge size and the high dimensionality of time series databases that are in use\nnowadays make that scenario inefficient. There are many representation\ntechniques that aim at reducing the dimensionality of time series so that the\nsearch can be handled faster at a lower-dimensional space level. The symbolic\naggregate approximation (SAX) is one of the most competitive methods in the\nliterature. In this paper we present a new method that improves the performance\nof SAX by adding to it another exclusion condition that increases the exclusion\npower. This method is based on using two representations of the time series:\none of SAX and the other is based on an optimal approximation of the time\nseries. Pre-computed distances are calculated and stored offline to be used\nonline to exclude a wide range of the search space using two exclusion\nconditions. We conduct experiments which show that the new method is faster\nthan SAX.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 19:24:33 GMT"}], "update_date": "2013-01-25", "authors_parsed": [["Fuad", "Muhammad Marwan Muhammad", "", "VALORIA"], ["Marteau", "Pierre-Fran\u00e7ois", "", "VALORIA"]]}, {"id": "1301.6191", "submitter": "Elizeu Santos-Neto", "authors": "Elizeu Santos-Neto, David Condon, Nazareno Andrade, Adriana Iamnitchi,\n  Matei Ripeanu", "title": "Reuse, Temporal Dynamics, Interest Sharing, and Collaboration in Social\n  Tagging Systems", "comments": "Part of this work has been publish in the ACM International\n  Conference on Hypertext'2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  User-generated content is shaping the dynamics of the World Wide Web. Indeed,\nan increasingly large number of systems provide mechanisms to support the\ngrowing demand for content creation, sharing, and management. Tagging systems\nare a particular class of these systems where users share and collaboratively\nannotate content such as photos and URLs. This collaborative behavior and the\npool of user-generated metadata create opportunities to improve existing\nsystems and to design new mechanisms. However, to realize this potential, it is\nnecessary to understand the usage characteristics of current systems. This work\naddresses this issue characterizing three tagging systems (CiteULike, Connotea\nand del.icio.us) while focusing on three aspects: i) the patterns of\ninformation (tags and items) production; ii) the temporal dynamics of users'\ntag vocabularies; and, iii) the social aspects of tagging systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 23:31:28 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Santos-Neto", "Elizeu", ""], ["Condon", "David", ""], ["Andrade", "Nazareno", ""], ["Iamnitchi", "Adriana", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1301.6277", "submitter": "Jeon-Hyung Kang", "authors": "Jeon-Hyung Kang, Kristina Lerman, Lise Getoor", "title": "LA-LDA: A Limited Attention Topic Model for Social Recommendation", "comments": "The 2013 International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction (SBP 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media users have finite attention which limits the number of incoming\nmessages from friends they can process. Moreover, they pay more attention to\nopinions and recommendations of some friends more than others. In this paper,\nwe propose LA-LDA, a latent topic model which incorporates limited,\nnon-uniformly divided attention in the diffusion process by which opinions and\ninformation spread on the social network. We show that our proposed model is\nable to learn more accurate user models from users' social network and item\nadoption behavior than models which do not take limited attention into account.\nWe analyze voting on news items on the social news aggregator Digg and show\nthat our proposed model is better able to predict held out votes than\nalternative models. Our study demonstrates that psycho-socially motivated\nmodels have better ability to describe and predict observed behavior than\nmodels which only consider topics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 18:26:36 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Kang", "Jeon-Hyung", ""], ["Lerman", "Kristina", ""], ["Getoor", "Lise", ""]]}, {"id": "1301.6591", "submitter": "Leon Abdillah", "authors": "Leon Andretti Abdillah", "title": "PDF articles metadata harvester", "comments": "6 Pages, 9 images, 1 table", "journal-ref": "Jurnal Komputer dan Informatika (JKI). 10 (2012) 1-7", "doi": null, "report-no": null, "categories": "cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific journals are very important in recording the finding from\nresearchers around the world. The recent media to disseminate scientific\njournals is PDF. On scheme to find the scientific journals over the internet is\nvia metadata. Metadata stores information about article summary. Embedding\nmetadata into PDF of scientific article will grant the consistency of metadata\nreadness. Harvesting the metadata from scientific journal is very interesting\nfield at the moment. This paper will discuss about scientific journal metadata\nharvesters involving XMP.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 16:18:05 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Abdillah", "Leon Andretti", ""]]}, {"id": "1301.6705", "submitter": "Thomas Hofmann", "authors": "Thomas Hofmann", "title": "Probabilistic Latent Semantic Analysis", "comments": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1999-PG-289-296", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Latent Semantic Analysis is a novel statistical technique for\nthe analysis of two-mode and co-occurrence data, which has applications in\ninformation retrieval and filtering, natural language processing, machine\nlearning from text, and in related areas. Compared to standard Latent Semantic\nAnalysis which stems from linear algebra and performs a Singular Value\nDecomposition of co-occurrence tables, the proposed method is based on a\nmixture decomposition derived from a latent class model. This results in a more\nprincipled approach which has a solid foundation in statistics. In order to\navoid overfitting, we propose a widely applicable generalization of maximum\nlikelihood model fitting by tempered EM. Our approach yields substantial and\nconsistent improvements over Latent Semantic Analysis in a number of\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 15:58:43 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Hofmann", "Thomas", ""]]}, {"id": "1301.6728", "submitter": "Hien Nguyen", "authors": "Hien Nguyen, Peter Haddawy", "title": "The Decision-Theoretic Interactive Video Advisor", "comments": "Appears in Proceedings of the Fifteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1999)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1999-PG-494-501", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to help people choose among large numbers of items and to filter\nthrough large amounts of information has led to a flood of research in\nconstruction of personal recommendation agents. One of the central issues in\nconstructing such agents is the representation and elicitation of user\npreferences or interests. This topic has long been studied in Decision Theory,\nbut surprisingly little work in the area of recommender systems has made use of\nformal decision-theoretic techniques. This paper describes DIVA, a\ndecision-theoretic agent for recommending movies that contains a number of\nnovel features. DIVA represents user preferences using pairwise comparisons\namong items, rather than numeric ratings. It uses a novel similarity measure\nbased on the concept of the probability of conflict between two orderings of\nitems. The system has a rich representation of preference, distinguishing\nbetween a user's general taste in movies and his immediate interests. It takes\nan incremental approach to preference elicitation in which the user can provide\nfeedback if not satisfied with the recommendation list. We empirically evaluate\nthe performance of the system using the EachMovie collaborative filtering\ndatabase.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 16:00:14 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Nguyen", "Hien", ""], ["Haddawy", "Peter", ""]]}, {"id": "1301.6770", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang (Eddie) Xu, Minmin Chen, Kilian Q. Weinberger, Fei Sha", "title": "An alternative text representation to TF-IDF and Bag-of-Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In text mining, information retrieval, and machine learning, text documents\nare commonly represented through variants of sparse Bag of Words (sBoW) vectors\n(e.g. TF-IDF). Although simple and intuitive, sBoW style representations suffer\nfrom their inherent over-sparsity and fail to capture word-level synonymy and\npolysemy. Especially when labeled data is limited (e.g. in document\nclassification), or the text documents are short (e.g. emails or abstracts),\nmany features are rarely observed within the training corpus. This leads to\noverfitting and reduced generalization accuracy. In this paper we propose Dense\nCohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW\ndocument features. dCoT explicitly models absent words by removing and\nreconstructing random sub-sets of words in the unlabeled corpus. With this\napproach, dCoT learns to reconstruct frequent words from co-occurring\ninfrequent words and maps the high dimensional sparse sBoW vectors into a\nlow-dimensional dense representation. We show that the feature removal can be\nmarginalized out and that the reconstruction can be solved for in closed-form.\nWe demonstrate empirically, on several benchmark datasets, that dCoT features\nsignificantly improve the classification accuracy across several document\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 21:04:45 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Zhixiang", "", "", "Eddie"], ["Xu", "", ""], ["Chen", "Minmin", ""], ["Weinberger", "Kilian Q.", ""], ["Sha", "Fei", ""]]}, {"id": "1301.6822", "submitter": "Latanya Sweeney", "authors": "Latanya Sweeney", "title": "Discrimination in Online Ad Delivery", "comments": null, "journal-ref": null, "doi": null, "report-no": "1071-1", "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Google search for a person's name, such as \"Trevon Jones\", may yield a\npersonalized ad for public records about Trevon that may be neutral, such as\n\"Looking for Trevon Jones?\", or may be suggestive of an arrest record, such as\n\"Trevon Jones, Arrested?\". This writing investigates the delivery of these\nkinds of ads by Google AdSense using a sample of racially associated names and\nfinds statistically significant discrimination in ad delivery based on searches\nof 2184 racially associated personal names across two websites. First names,\nassigned at birth to more black or white babies, are found predictive of race\n(88% black, 96% white), and those assigned primarily to black babies, such as\nDeShawn, Darnell and Jermaine, generated ads suggestive of an arrest in 81 to\n86 percent of name searches on one website and 92 to 95 percent on the other,\nwhile those assigned at birth primarily to whites, such as Geoffrey, Jill and\nEmma, generated more neutral copy: the word \"arrest\" appeared in 23 to 29\npercent of name searches on one site and 0 to 60 percent on the other. On the\nmore ad trafficked website, a black-identifying name was 25% more likely to get\nan ad suggestive of an arrest record. A few names did not follow these\npatterns. All ads return results for actual individuals and ads appear\nregardless of whether the name has an arrest record in the company's database.\nThe company maintains Google received the same ad text for groups of last names\n(not first names), raising questions as to whether Google's technology exposes\nracial bias.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 02:59:47 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Sweeney", "Latanya", ""]]}, {"id": "1301.6916", "submitter": "Vincent Gripon", "authors": "Vincent Gripon and Michael Rabbat", "title": "Reconstructing a Graph from Path Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of inferring the structure of a network from\nindirect observations. Each observation (a \"trace\") is the unordered set of\nnodes which are activated along a path through the network. Since a trace does\nnot convey information about the order of nodes within the path, there are many\nfeasible orders for each trace observed, and thus the problem of inferring the\nnetwork from traces is, in general, illposed. We propose and analyze an\nalgorithm which inserts edges by ordering each trace into a path according to\nwhich pairs of nodes in the path co-occur most frequently in the observations.\nWhen all traces involve exactly 3 nodes, we derive necessary and sufficient\nconditions for the reconstruction algorithm to exactly recover the graph.\nFinally, for a family of random graphs, we present expressions for\nreconstruction error probabilities (false discoveries and missed detections).\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 13:13:02 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Gripon", "Vincent", ""], ["Rabbat", "Michael", ""]]}, {"id": "1301.6917", "submitter": "Vincent Gripon", "authors": "Vincent Gripon and Michael Rabbat", "title": "Maximum Likelihood Associative Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.IR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Associative memories are structures that store data in such a way that it can\nlater be retrieved given only a part of its content -- a sort-of\nerror/erasure-resilience property. They are used in applications ranging from\ncaches and memory management in CPUs to database engines. In this work we study\nassociative memories built on the maximum likelihood principle. We derive\nminimum residual error rates when the data stored comes from a uniform binary\nsource. Second, we determine the minimum amount of memory required to store the\nsame data. Finally, we bound the computational complexity for message\nretrieval. We then compare these bounds with two existing associative memory\narchitectures: the celebrated Hopfield neural networks and a neural network\narchitecture introduced more recently by Gripon and Berrou.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 13:15:09 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2013 16:15:09 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Gripon", "Vincent", ""], ["Rabbat", "Michael", ""]]}, {"id": "1301.7363", "submitter": "John S. Breese", "authors": "John S. Breese, David Heckerman, Carl Kadie", "title": "Empirical Analysis of Predictive Algorithms for Collaborative Filtering", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-43-52", "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering or recommender systems use a database about user\npreferences to predict additional topics or products a new user might like. In\nthis paper we describe several algorithms designed for this task, including\ntechniques based on correlation coefficients, vector-based similarity\ncalculations, and statistical Bayesian methods. We compare the predictive\naccuracy of the various methods in a set of representative problem domains. We\nuse two basic classes of evaluation metrics. The first characterizes accuracy\nover a set of individual predictions in terms of average absolute deviation.\nThe second estimates the utility of a ranked list of suggested items. This\nmetric uses an estimate of the probability that a user will see a\nrecommendation in an ordered list. Experiments were run for datasets associated\nwith 3 application areas, 4 experimental protocols, and the 2 evaluation\nmetrics for the various algorithms. Results indicate that for a wide range of\nconditions, Bayesian networks with decision trees at each node and correlation\nmethods outperform Bayesian-clustering and vector-similarity methods. Between\ncorrelation and Bayesian networks, the preferred method depends on the nature\nof the dataset, nature of the application (ranked versus one-by-one\npresentation), and the availability of votes with which to make predictions.\nOther considerations include the size of database, speed of predictions, and\nlearning time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:02:44 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Breese", "John S.", ""], ["Heckerman", "David", ""], ["Kadie", "Carl", ""]]}, {"id": "1301.7364", "submitter": "Luis M. de Campos", "authors": "Luis M. de Campos, Juan M. Fernandez-Luna, Juan F. Huete", "title": "Query Expansion in Information Retrieval Systems using a Bayesian\n  Network-Based Thesaurus", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-53-60", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Retrieval (IR) is concerned with the identification of documents\nin a collection that are relevant to a given information need, usually\nrepresented as a query containing terms or keywords, which are supposed to be a\ngood description of what the user is looking for. IR systems may improve their\neffectiveness (i.e., increasing the number of relevant documents retrieved) by\nusing a process of query expansion, which automatically adds new terms to the\noriginal query posed by an user. In this paper we develop a method of query\nexpansion based on Bayesian networks. Using a learning algorithm, we construct\na Bayesian network that represents some of the relationships among the terms\nappearing in a given document collection; this network is then used as a\nthesaurus (specific for that collection). We also report the results obtained\nby our method on three standard test collections.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:02:49 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["de Campos", "Luis M.", ""], ["Fernandez-Luna", "Juan M.", ""], ["Huete", "Juan F.", ""]]}, {"id": "1301.7382", "submitter": "David Heckerman", "authors": "David Heckerman, Eric J. Horvitz", "title": "Inferring Informational Goals from Free-Text Queries: A Bayesian\n  Approach", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-230-237", "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People using consumer software applications typically do not use technical\njargon when querying an online database of help topics. Rather, they attempt to\ncommunicate their goals with common words and phrases that describe software\nfunctionality in terms of structure and objects they understand. We describe a\nBayesian approach to modeling the relationship between words in a user's query\nfor assistance and the informational goals of the user. After reviewing the\ngeneral method, we describe several extensions that center on integrating\nadditional distinctions and structure about language usage and user goals into\nthe Bayesian models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:04:21 GMT"}, {"version": "v2", "created": "Sat, 16 May 2015 23:11:26 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Heckerman", "David", ""], ["Horvitz", "Eric J.", ""]]}, {"id": "1301.7738", "submitter": "Flavio Coelho", "authors": "Fl\\'avio Code\\c{c}o Coelho and Renato Rocha Souza and \\'Alvaro Justen\n  and Fl\\'avio Amieiro and Heliana Mello", "title": "PyPLN: a Distributed Platform for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a distributed platform for Natural Language Processing\ncalled PyPLN. PyPLN leverages a vast array of NLP and text processing open\nsource tools, managing the distribution of the workload on a variety of\nconfigurations: from a single server to a cluster of linux servers. PyPLN is\ndeveloped using Python 2.7.3 but makes it very easy to incorporate other\nsoftwares for specific tasks as long as a linux version is available. PyPLN\nfacilitates analyses both at document and corpus level, simplifying management\nand publication of corpora and analytical results through an easy to use web\ninterface. In the current (beta) release, it supports English and Portuguese\nlanguages with support to other languages planned for future releases. To\nsupport the Portuguese language PyPLN uses the PALAVRAS parser\\citep{Bick2000}.\nCurrently PyPLN offers the following features: Text extraction with encoding\nnormalization (to UTF-8), part-of-speech tagging, token frequency, semantic\nannotation, n-gram extraction, word and sentence repertoire, and full-text\nsearch across corpora. The platform is licensed as GPL-v3.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 20:21:52 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 11:54:23 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Coelho", "Fl\u00e1vio Code\u00e7o", ""], ["Souza", "Renato Rocha", ""], ["Justen", "\u00c1lvaro", ""], ["Amieiro", "Fl\u00e1vio", ""], ["Mello", "Heliana", ""]]}]