[{"id": "2103.00022", "submitter": "Qiongwen Xu", "authors": "Qiongwen Xu, Michael D. Wong, Tanvi Wagle, Srinivas Narayana, Anirudh\n  Sivaraman", "title": "Synthesizing Safe and Efficient Kernel Extensions for Packet Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extended Berkeley Packet Filter (BPF) has emerged as a powerful method to\nextend packet-processing functionality in the Linux operating system. BPF\nallows users to write code in high-level languages (like C or Rust) and execute\nthem at specific hooks in the kernel, such as the network device driver. To\nensure safe execution of a user-developed BPF program in kernel context, Linux\nuses an in-kernel static checker. The checker allows a program to execute only\nif it can prove that the program is crash-free, always accesses memory within\nsafe bounds, and avoids leaking kernel data.\n  BPF programming is not easy. One, even modest-sized BPF programs are deemed\ntoo large to analyze and rejected by the kernel checker. Two, the kernel\nchecker may incorrectly determine that a BPF program exhibits unsafe behaviors.\nThree, even small performance optimizations to BPF code (e.g., 5% gains) must\nbe meticulously hand-crafted by expert developers. Traditional optimizing\ncompilers for BPF are often inadequate since the kernel checker's safety\nconstraints are incompatible with rule-based optimizations.\n  We present K2, a program-synthesis-based compiler that automatically\noptimizes BPF bytecode with formal correctness and safety guarantees. K2\nproduces code with 6--26% reduced size, 1.36%--55.03% lower average\npacket-processing latency, and 0--4.75% higher throughput (packets per second\nper core) relative to the best clang-compiled program, across benchmarks drawn\nfrom Cilium, Facebook, and the Linux kernel. K2 incorporates several\ndomain-specific techniques to make synthesis practical by accelerating\nequivalence-checking of BPF programs by 6 orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:09:04 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 18:09:06 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Qiongwen", ""], ["Wong", "Michael D.", ""], ["Wagle", "Tanvi", ""], ["Narayana", "Srinivas", ""], ["Sivaraman", "Anirudh", ""]]}, {"id": "2103.00227", "submitter": "Emadeldin Abbas Mazied Abdrabou", "authors": "EmadElDin A Mazied, Lingjia Liu, Scott F. Midkiff", "title": "Towards Intelligent RAN Slicing for B5G: Opportunities and Challenges", "comments": "9 pages, 5 figures, 1 table, 15 references, Magazine article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the diverse demands for wireless communication, fifth-generation (5G)\nnetworks and beyond (B5G) embrace the concept of network slicing by forging\nvirtual instances (slices) of its physical infrastructure. While network\nslicing constitutes dynamic allocation of core network and radio access network\n(RAN) resources, this article emphasizes RAN slicing (RAN-S) design. Forming\non-demand RAN-S that can be flexibly (re)-configured while ensuring slice\nisolation is challenging. A variety of machine learning (ML) techniques have\nbeen recently introduced for traffic forecasting and classification, resource\nusage prediction, admission control, scheduling, and dynamic resource\nallocation in RAN-S. Albeit these approaches grant opportunities towards\nintelligent RAN-S design, they raise critical challenges that need to be\nexamined. This article underlines the opportunities and the challenges of\nincorporating ML into RAN-S by reviewing the cutting-edge ML-based techniques\nfor RAN-S. It also draws few directions for future research towards intelligent\nRAN-S (iRAN-S).\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 14:24:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mazied", "EmadElDin A", ""], ["Liu", "Lingjia", ""], ["Midkiff", "Scott F.", ""]]}, {"id": "2103.00428", "submitter": "Yuqi Han", "authors": "Yuqi Han, Rui Wang, Jun Wu, Dian Liu, and Haoqi Ren", "title": "Cache Placement Optimization in Mobile Edge Computing Networks with\n  Unaware Environment -- An Extended Multi-armed Bandit Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Caching high-frequency reuse contents at the edge servers in the mobile edge\ncomputing (MEC) network omits the part of backhaul transmission and further\nreleases the pressure of data traffic. However, how to efficiently decide the\ncaching contents for edge servers is still an open problem, which refers to the\ncache capacity of edge servers, the popularity of each content, and the\nwireless channel quality during transmission. In this paper, we discuss the\ninfluence of unknown user density and popularity of content on the cache\nplacement solution at the edge server. Specifically, towards the implementation\nof the cache placement solution in the practical network, there are two\nproblems needing to be solved. First, the estimation of unknown users'\npreference needs a huge amount of records of users' previous requests. Second,\nthe overlapping serving regions among edge servers cause the wrong estimation\nof users' preference, which hinders the individual decision of caching\nplacement. To address the first issue, we propose a learning-based solution to\nadaptively optimize the cache placement policy. We develop the extended\nmulti-armed bandit (Extended MAB), which combines the generalized global bandit\n(GGB) and Standard Multi-armed bandit (MAB). For the second problem, a\nmulti-agent Extended MAB-based solution is presented to avoid the\nmis-estimation of parameters and achieve the decentralized cache placement\npolicy. The proposed solution determines the primary time slot and secondary\ntime slot for each edge server. The proposed strategies are proven to achieve\nthe bounded regret according to the mathematical analysis. Extensive\nsimulations verify the optimality of the proposed strategies when comparing\nwith baselines.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:50:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Han", "Yuqi", ""], ["Wang", "Rui", ""], ["Wu", "Jun", ""], ["Liu", "Dian", ""], ["Ren", "Haoqi", ""]]}, {"id": "2103.00433", "submitter": "Steffen Wendzel", "authors": "Wojciech Mazurczyk and Steffen Wendzel and Mehdi Chourib and J\\\"org\n  Keller", "title": "Countering Adaptive Network Covert Communication with Dynamic Wardens", "comments": null, "journal-ref": "Elsevier FGCS, Volume 94, May 2019, Pages 712-725", "doi": "10.1016/j.future.2018.12.047", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network covert channels are hidden communication channels in computer\nnetworks. They influence several factors of the cybersecurity economy. For\ninstance, by improving the stealthiness of botnet communications, they aid and\npreserve the value of darknet botnet sales. Covert channels can also be used to\nsecretly exfiltrate confidential data out of organizations, potentially\nresulting in loss of market/research advantage. Considering the above, efforts\nare needed to develop effective countermeasures against such threats. Thus in\nthis paper, based on the introduced novel warden taxonomy, we present and\nevaluate a new concept of a dynamic warden. Its main novelty lies in the\nmodification of the warden's behavior over time, making it difficult for the\nadaptive covert communication parties to infer its strategy and perform a\nsuccessful hidden data exchange. Obtained experimental results indicate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:31:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mazurczyk", "Wojciech", ""], ["Wendzel", "Steffen", ""], ["Chourib", "Mehdi", ""], ["Keller", "J\u00f6rg", ""]]}, {"id": "2103.00474", "submitter": "Jason R.C. Nurse Dr", "authors": "Jason R. C. Nurse", "title": "Cybersecurity Awareness", "comments": null, "journal-ref": "In: Jajodia S., Samarati P., Yung M. (eds) Encyclopedia of\n  Cryptography, Security and Privacy. Springer, Berlin, Heidelberg (2021)", "doi": "10.1007/978-3-642-27739-9_1596-1", "report-no": null, "categories": "cs.CR cs.CY cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity awareness can be viewed as the level of appreciation,\nunderstanding or knowledge of cybersecurity or information security aspects.\nSuch aspects include cognizance of cyber risks and threats, but also\nappropriate protection measures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 11:54:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nurse", "Jason R. C.", ""]]}, {"id": "2103.00487", "submitter": "Jiaojiao Jiang", "authors": "Jiaojiao Jiang, Sanjay Jha", "title": "Network Growth From Global and Local Influential Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In graph theory and network analysis, node degree is defined as a simple but\npowerful centrality to measure the local influence of node in a complex\nnetwork. Preferential attachment based on node degree has been widely adopted\nfor modeling network growth. However, many evidences exist which show deviation\nof real network growth from what a pure degree-based model suggests. It seems\nthat node degree is not a reliable measure for predicting the preference of\nnewcomers in attaching to the network, or at least, it does not tell the whole\nstory. In this paper, we argue that there is another dimension to network\ngrowth, one that we call node \"coreness\". The new dimension gives insights on\nthe global influence of nodes, in comparison to the local view the degree\nmetric provides. We found that the probability of existing nodes attracting new\nnodes generally follows an exponential dependence on node coreness, while at\nthe same time, follows a power-law dependence on node degree. That is to say,\nhigh-coreness nodes are more powerful than high-degree nodes in attracting\nnewcomers. The new dimension further discloses some hidden phenomena which\nhappen in the process of network growth. The power of node degree in attracting\nnewcomers increases over time while the influence of coreness decreases, and\nfinally, they reach a state of equilibrium in the growth. All these theories\nhave been tested on real-world networks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 23:13:03 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jiang", "Jiaojiao", ""], ["Jha", "Sanjay", ""]]}, {"id": "2103.00499", "submitter": "Steffen Wendzel", "authors": "Steffen Wendzel", "title": "Protocol-independent Detection of \"Messaging Ordering\" Network Covert\n  Channels", "comments": null, "journal-ref": "Published in Proc. ARES 2019 (CUING Workshop)", "doi": "10.1145/3339252.3341477", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection methods are available for several known covert channels. However, a\ntype of covert channel that received little attention within the last decade is\nthe \"message ordering\" channel. Such a covert channel changes the order of PDUs\n(protocol data units, i.e. packets) transferred over the network to encode\nhidden information. The advantage of these channels is that they cannot be\nblocked easily as they do not modify header content but instead mimic typical\nnetwork behavior such as TCP segments that arrive in a different order than\nthey were sent.\n  Contribution: In this paper, we show a protocol-independent approach to\ndetect message ordering channels. Our approach is based on a modified\ncompressibility score. We analyze the detectability of message ordering\nchannels and whether several types of message ordering channels differ in their\ndetectability.\n  Results: Our results show that the detection of message ordering channels\ndepends on their number of utilized PDUs. First, we performed a rough threshold\nselection by hand, which we later optimized using the C4.5 decision tree\nclassifier. We were able to detect message ordering covert channels with an\naccuracy and F1 score of >= 99.5% and a false-positive rate < 1% and < 0.1% if\nthey use sequences of 3 or 4 PDUs, respectively. Simpler channels that only\nmanipulate a sequence of two PDUs were detectable with an accuracy and F1 score\nof 94.5% and were linked to a false-positive rate of 5.19%. We thus consider\nour approach suitable for real-world detection scenarios with channels\nutilizing 3 or 4 PDUs while the detection of channels utilizing 2 PDUs should\nbe improved further.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:02:07 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wendzel", "Steffen", ""]]}, {"id": "2103.00555", "submitter": "Lakshmi B Narayana V S Ch", "authors": "V S Ch Lakshmi Narayana, Mohit Agarwala, Nikhil Karamchandani, and\n  Sharayu Moharir", "title": "Online Partial Service Hosting at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of service hosting where an application provider can\ndynamically rent edge computing resources and serve user requests from the edge\nto deliver a better quality of service. A key novelty of this work is that we\nallow the service to be hosted partially at the edge which enables a fraction\nof the user query to be served by the edge. We model the total cost for\n(partially) hosting a service at the edge as a combination of the latency in\nserving requests, the bandwidth consumption, and the time-varying cost for\nrenting edge resources. We propose an online policy called\n$\\alpha$-RetroRenting ($\\alpha$-RR) which dynamically determines the fraction\nof the service to be hosted at the edge in any time-slot, based on the history\nof the request arrivals and the rent cost sequence. As our main result, we\nderive an upper bound on $\\alpha$-RR's competitive ratio with respect to the\noffline optimal policy that knows the entire request arrival and rent cost\nsequence in advance. We conduct extensive numerical evaluations to compare the\nperformance of $\\alpha$-RR with various benchmarks for synthetic and\ntrace-based request arrival and rent cost processes, and find several parameter\nregimes where $\\alpha$-RR's ability to store the service partially greatly\nimproves cost-efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:28:50 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:13:08 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Narayana", "V S Ch Lakshmi", ""], ["Agarwala", "Mohit", ""], ["Karamchandani", "Nikhil", ""], ["Moharir", "Sharayu", ""]]}, {"id": "2103.00753", "submitter": "Xindi Wang", "authors": "Xindi Wang, Chi-Tsun Cheng, Lei Deng, Xiaojing Chen, Fu Xiao", "title": "Technical Report for A Joint User Scheduling and Trajectory Planning\n  Data Collection Strategy for the UAV-assisted WSN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned aerial vehicles (UAVs) are usually dispatched as mobile sinks to\nassist data collection in large-scale wireless sensor networks (WSNs). However,\nwhen considering the limitations of UAV's mobility and communication\ncapabilities in a large-scale WSN, some sensor nodes may run out of storage\nspace as they fail to offload their data to the UAV for an extended period of\ntime. To minimize the data loss caused by the above issue, a joint user\nscheduling and trajectory planning data collection strategy is proposed in this\nletter, which is formulated as a non-convex optimization problem. The problem\nis further divided into two sub-problems and solved sequentially. Simulation\nresults show that the proposed strategy is more effective in minimizing data\nloss rate than other strategies.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:58:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Xindi", ""], ["Cheng", "Chi-Tsun", ""], ["Deng", "Lei", ""], ["Chen", "Xiaojing", ""], ["Xiao", "Fu", ""]]}, {"id": "2103.01282", "submitter": "Nuref\\c{s}an Sertba\\c{s} B\\\"ulb\\\"ul", "authors": "Nuref\\c{s}an Sertba\\c{s} B\\\"ulb\\\"ul, Do\\u{g}analp Ergen\\c{c}, Mathias\n  Fischer", "title": "SDN-based Self-Configuration for Time-Sensitive IoT Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence of IT and OT technologies results in the need for efficient\nnetwork management solutions for automotive and industrial automation\nenvironments. However, configuring real-time Ethernet networks while\nmaintaining the desired QoS is challenging due to the dynamic nature of OT\nnetworks and the high configuration parameters. This paper introduces an\nSDN-based self-configuration framework for the fully automated configuration of\nTSN networks. Unlike standard TSN configuration, we remove end-host-related\ndependencies and put flows initially on default paths to extract traffic\ncharacteristics by monitoring network traffic at edge switches. Communicated to\na central SDN controller, these characteristics allow to move the flows to\noptimal paths while maintaining hard real-time guarantees, for which we also\nformulate an optimization problem. Our simulation results indicate that the\nproposed self-configuration approach works properly for different network sizes\nand numbers of end-hosts. Even though it slightly increases the average latency\nof critical frames, it still provides a certain level of real-time guarantee\nwithout any prior knowledge of flows.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:55:59 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["B\u00fclb\u00fcl", "Nuref\u015fan Sertba\u015f", ""], ["Ergen\u00e7", "Do\u011fanalp", ""], ["Fischer", "Mathias", ""]]}, {"id": "2103.01314", "submitter": "Kevin Zhao", "authors": "Kevin Zhao, Prateesh Goyal, Mohammad Alizadeh, Thomas E. Anderson", "title": "SWP: Microsecond Network SLOs Without Priorities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of cloud computing for latency-sensitive applications has\nsparked renewed interest in providing tight bounds on network tail latency.\nAchieving this in practice at reasonable network utilization has proved\nelusive, due to a combination of highly bursty application demand, faster link\nspeeds, and heavy-tailed message sizes. While priority scheduling can be used\nto reduce tail latency for some traffic, this comes at a cost of much worse\ndelay behavior for all other traffic on the network. Most operators choose to\nrun their networks at very low average utilization, despite the added cost, and\nyet still suffer poor tail behavior.\n  This paper takes a different approach. We build a system, swp, to help\noperators (and network designers) to understand and control tail latency\nwithout relying on priority scheduling. As network workload changes, swp is\ndesigned to give real-time advice on the network switch configurations needed\nto maintain tail latency objectives for each traffic class. The core of swp is\nan efficient model for simulating the combined effect of traffic\ncharacteristics, end-to-end congestion control, and switch scheduling on\nservice-level objectives (SLOs), along with an optimizer that adjusts\nswitch-level scheduling weights assigned to each class. Using simulation across\na diverse set of workloads with different SLOs, we show that to meet the same\nSLOs as swp provides, FIFO would require 65% greater link capacity, and 79%\nmore for scenarios with tight SLOs on bursty traffic classes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 21:10:52 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:33:17 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhao", "Kevin", ""], ["Goyal", "Prateesh", ""], ["Alizadeh", "Mohammad", ""], ["Anderson", "Thomas E.", ""]]}, {"id": "2103.01351", "submitter": "Dongzhu Liu", "authors": "Dongzhu Liu and Osvaldo Simeone", "title": "Channel-Driven Monte Carlo Sampling for Bayesian Distributed Learning in\n  Wireless Data Centers", "comments": "Under Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional frequentist learning, as assumed by existing federated learning\nprotocols, is limited in its ability to quantify uncertainty, incorporate prior\nknowledge, guide active learning, and enable continual learning. Bayesian\nlearning provides a principled approach to address all these limitations, at\nthe cost of an increase in computational complexity. This paper studies\ndistributed Bayesian learning in a wireless data center setting encompassing a\ncentral server and multiple distributed workers. Prior work on wireless\ndistributed learning has focused exclusively on frequentist learning, and has\nintroduced the idea of leveraging uncoded transmission to enable \"over-the-air\"\ncomputing. Unlike frequentist learning, Bayesian learning aims at evaluating\napproximations or samples from a global posterior distribution in the model\nparameter space. This work investigates for the first time the design of\ndistributed one-shot, or \"embarrassingly parallel\", Bayesian learning protocols\nin wireless data centers via consensus Monte Carlo (CMC). Uncoded transmission\nis introduced not only as a way to implement \"over-the-air\" computing, but also\nas a mechanism to deploy channel-driven MC sampling: Rather than treating\nchannel noise as a nuisance to be mitigated, channel-driven sampling utilizes\nchannel noise as an integral part of the MC sampling process. A simple wireless\nCMC scheme is first proposed that is asymptotically optimal under Gaussian\nlocal posteriors. Then, for arbitrary local posteriors, a variational\noptimization strategy is introduced. Simulation results demonstrate that, if\nproperly accounted for, channel noise can indeed contribute to MC sampling and\ndoes not necessarily decrease the accuracy level.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:28:54 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 20:54:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Liu", "Dongzhu", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "2103.01711", "submitter": "Alberto Signori", "authors": "Roberto Francescon, Filippo Campagnaro, Emanuele Coccolo, Alberto\n  Signori, Federico Guerra, Federico Favaro, Michele Zorzi", "title": "An Event-Based Stack For Data Transmission Through Underwater Multimodal\n  Networks", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DESERT Underwater framework (http://desert-underwater.dei.unipd.it/),\noriginally designed for simulating and testing underwater acoustic networks in\nsea trials, has recently been extended to support real payload data\ntransmission through underwater multimodal networks. Specifically, the new\nversion of the framework is now able to transmit data in real time through the\nEvoLogics S2C low-rate and high-rate acoustic modems, the SmartPORT low-cost\nacoustic underwater modem prototype (AHOI) for IoT applications, as well as\nEthernet, surface WiFi, and the BlueComm optical modem. The system can also be\ntested in the lab by employing a simulated channel, and the EvoLogics S2C DMAC\nEmulator (DMACE)\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:36:17 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Francescon", "Roberto", ""], ["Campagnaro", "Filippo", ""], ["Coccolo", "Emanuele", ""], ["Signori", "Alberto", ""], ["Guerra", "Federico", ""], ["Favaro", "Federico", ""], ["Zorzi", "Michele", ""]]}, {"id": "2103.01776", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Huansheng Ning, Fadi Farha, Liming Chen, Luigi Atzori\n  and Mahmoud Daneshmand", "title": "IoT-Enabled Social Relationships Meet Artificial Social Intelligence", "comments": "IEEE Internet of Things Journal (2021)", "journal-ref": null, "doi": "10.1109/JIOT.2021.3081556", "report-no": null, "categories": "cs.CY cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances of the Internet of Things, and the increasing\naccessibility of ubiquitous computing resources and mobile devices, the\nprevalence of rich media contents, and the ensuing social, economic, and\ncultural changes, computing technology and applications have evolved quickly\nover the past decade. They now go beyond personal computing, facilitating\ncollaboration and social interactions in general, causing a quick proliferation\nof social relationships among IoT entities. The increasing number of these\nrelationships and their heterogeneous social features have led to computing and\ncommunication bottlenecks that prevent the IoT network from taking advantage of\nthese relationships to improve the offered services and customize the delivered\ncontent, known as relationship explosion. On the other hand, the quick advances\nin artificial intelligence applications in social computing have led to the\nemerging of a promising research field known as Artificial Social Intelligence\n(ASI) that has the potential to tackle the social relationship explosion\nproblem. This paper discusses the role of IoT in social relationships detection\nand management, the problem of social relationships explosion in IoT and\nreviews the proposed solutions using ASI, including social-oriented\nmachine-learning and deep-learning techniques.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 09:07:32 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 19:43:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Ning", "Huansheng", ""], ["Farha", "Fadi", ""], ["Chen", "Liming", ""], ["Atzori", "Luigi", ""], ["Daneshmand", "Mahmoud", ""]]}, {"id": "2103.02085", "submitter": "Spyridon Mastorakis", "authors": "Boubakr Nour and Spyridon Mastorakis and Rehmat Ullah and Nicholas\n  Stergiou", "title": "Information-Centric Networking in Wireless Environments: Security Risks\n  and Challenges", "comments": "This paper has been accepted for publication by the IEEE Wireless\n  Communications Magazine. The final version will be published by the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information-Centric Networking (ICN) has emerged as a paradigm to cope with\nthe lack of built-in security primitives and efficient mechanisms for content\ndistribution of today's Internet. However, deploying ICN in a wireless\nenvironment poses a different set of challenges compared to a wired\nenvironment, especially when it comes to security. In this paper, we present\nthe security issues that may arise and the attacks that may occur from\ndifferent points of view when ICN is deployed in wireless environments. The\ndiscussed attacks may target both applications and the ICN network itself by\nexploiting elements of the ICN architecture, such as content names and\nin-network content caches. Furthermore, we discuss potential solutions to the\npresented issues and countermeasures to the presented attacks. Finally, we\nidentify future research opportunities and directions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:17:45 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Nour", "Boubakr", ""], ["Mastorakis", "Spyridon", ""], ["Ullah", "Rehmat", ""], ["Stergiou", "Nicholas", ""]]}, {"id": "2103.02139", "submitter": "Pedro Henrique Juliano Nardelli", "authors": "Shiva Kazemi Taskou, Mehdi Rasti, Pedro H. J. Nardelli", "title": "Energy and Cost Efficient Resource Allocation for Blockchain-Enabled NFV", "comments": null, "journal-ref": "IEEE Transactions on Services Computing, 2021", "doi": "10.1109/TSC.2021.3050717", "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network function virtualization (NFV) is a promising technology to make 5G\nnetworks flexible and agile. NFV decreases operators' OPEX and CAPEX by\ndecoupling the physical hardware from the functions they perform. In NFV,\nusers' service request can be viewed as a service function chain (SFC)\nconsisting of several virtual network functions (VNFs) which are connected\nthrough virtual links. Resource allocation in NFV is done through a centralized\nauthority called NFV Orchestrator (NFVO). This centralized authority suffers\nfrom some drawbacks such as single point of failure and security. Blockchain\n(BC) technology is able to address these problems by decentralizing resource\nallocation. The drawbacks of NFVO in NFV architecture and the exceptional BC\ncharacteristics to address these problems motivate us to focus on NFV resource\nallocation to users' SFCs without the need for an NFVO based on BC technology.\nTo this end, we assume there are two types of users: users who send SFC\nrequests (SFC requesting users) and users who perform mining process (miner\nusers). For SFC requesting users, we formulate NFV resource allocation (NFV-RA)\nproblem as a multi-objective problem to minimize the energy consumption and\nutilized resource cost, simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:44:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Taskou", "Shiva Kazemi", ""], ["Rasti", "Mehdi", ""], ["Nardelli", "Pedro H. J.", ""]]}, {"id": "2103.02176", "submitter": "Shaoshan Liu", "authors": "Shaoshan Liu, Bo Yu, Jie Tang, Qi Zhu", "title": "Towards Fully Intelligent Transportation through Infrastructure-Vehicle\n  Cooperative Autonomous Driving: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The infrastructure-vehicle cooperative autonomous driving approach depends on\nthe cooperation between intelligent roads and intelligent vehicles. This\napproach is not only safer but also more economical compared to the traditional\non-vehicle-only autonomous driving approach. In this paper, we introduce our\nreal-world deployment experiences of cooperative autonomous driving, and delve\ninto the details of new challenges and opportunities. Specifically, based on\nour progress towards commercial deployment, we follow a three-stage development\nroadmap of the cooperative autonomous driving approach:infrastructure-augmented\nautonomous driving (IAAD), infrastructure-guided autonomous driving (IGAD), and\ninfrastructure-planned autonomous driving (IPAD).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:50:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Shaoshan", ""], ["Yu", "Bo", ""], ["Tang", "Jie", ""], ["Zhu", "Qi", ""]]}, {"id": "2103.02260", "submitter": "David Fischer", "authors": "Jiali Xing, David Fischer, Nitya Labh, Ryan Piersma, Benjamin C. Lee,\n  Yu Amy Xia, Tuhin Sahai, Vahid Tarokh", "title": "Talaria: A Framework for Simulation of Permissioned Blockchains for\n  Logistics and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Talaria, a novel permissioned blockchain simulator\nthat supports numerous protocols and use cases, most notably in supply chain\nmanagement. Talaria extends the capability of BlockSim, an existing blockchain\nsimulator, to include permissioned blockchains and serves as a foundation for\nfurther private blockchain assessment. Talaria is designed with both practical\nByzantine Fault Tolerance (pBFT) and simplified version of Proof-of-Authority\nconsensus protocols, but can be revised to include other permissioned protocols\nwithin its modular framework. Moreover, Talaria is able to simulate different\ntypes of malicious authorities and a variable daily transaction load at each\nnode. In using Talaria, business practitioners and policy planners have an\nopportunity to measure, evaluate, and adapt a range of blockchain solutions for\ncommercial operations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:43:30 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 00:22:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Xing", "Jiali", ""], ["Fischer", "David", ""], ["Labh", "Nitya", ""], ["Piersma", "Ryan", ""], ["Lee", "Benjamin C.", ""], ["Xia", "Yu Amy", ""], ["Sahai", "Tuhin", ""], ["Tarokh", "Vahid", ""]]}, {"id": "2103.02282", "submitter": "Milan Stute", "authors": "Alexander Heinrich, Milan Stute, Tim Kornhuber, Matthias Hollick", "title": "Who Can Find My Devices? Security and Privacy of Apple's Crowd-Sourced\n  Bluetooth Location Tracking System", "comments": "Accepted at Privacy Enhancing Technologies Symposium (PETS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Overnight, Apple has turned its hundreds-of-million-device ecosystem into the\nworld's largest crowd-sourced location tracking network called offline finding\n(OF). OF leverages online finder devices to detect the presence of missing\noffline devices using Bluetooth and report an approximate location back to the\nowner via the Internet. While OF is not the first system of its kind, it is the\nfirst to commit to strong privacy goals. In particular, OF aims to ensure\nfinder anonymity, untrackability of owner devices, and confidentiality of\nlocation reports. This paper presents the first comprehensive security and\nprivacy analysis of OF. To this end, we recover the specifications of the\nclosed-source OF protocols by means of reverse engineering. We experimentally\nshow that unauthorized access to the location reports allows for accurate\ndevice tracking and retrieving a user's top locations with an error in the\norder of 10 meters in urban areas. While we find that OF's design achieves its\nprivacy goals, we discover two distinct design and implementation flaws that\ncan lead to a location correlation attack and unauthorized access to the\nlocation history of the past seven days, which could deanonymize users. Apple\nhas partially addressed the issues following our responsible disclosure.\nFinally, we make our research artifacts publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:46:34 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Heinrich", "Alexander", ""], ["Stute", "Milan", ""], ["Kornhuber", "Tim", ""], ["Hollick", "Matthias", ""]]}, {"id": "2103.02299", "submitter": "Giuseppe Rizzelli Martella", "authors": "Giuseppe Rizzelli, Antonino Nespola, Stefano Straullu and Roberto\n  Gaudino", "title": "Scaling Laws for Unamplified Coherent Transmission in Next-generation\n  Short-Reach and Access Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  International standardization bodies (IEEE and ITU-T) working on the\nevolution of transmission technologies are still considering traditional direct\ndetection solutions for the most relevant short reach optical link\napplications, that are Passive Optical Networks (PON) and intra-data center\ninterconnects. Anyway, future jumps towards even higher bit rates per\nwavelength will require a complete paradigm shift, moving towards coherent\ntechnologies. In this paper, we thus study both analytically and experimentally\nthe scaling laws of unamplified coherent transmission in the short-reach\ncommunications ecosystems. We believe that, given the extremely tight\ntechno-economic constraints, such a revolutionary transition towards coherent\nin short-reach first requires a very detailed study of its intrinsic\ncapabilities in largely extending the limitation currently imposed by direct\ndetection systems. To this end, this paper focuses on the ultimate physical\nlayer limitations of unamplified coherent systems in terms of bit rate and\npower budget. The main parameters of our performance estimation model are\nextracted through fitting with a set of experimental characterizations and\nlater used as the starting point of a scaling laws study regarding local\noscillator power, modulator-induced attenuation, bit rate, and maximum\nachievable power budget. The analytically predicted performance is then\nverified through transmission experiments, including a demonstration on a 37-km\ninstalled metropolitan dark fiber in the city of Turin.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:25:38 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Rizzelli", "Giuseppe", ""], ["Nespola", "Antonino", ""], ["Straullu", "Stefano", ""], ["Gaudino", "Roberto", ""]]}, {"id": "2103.02434", "submitter": "Jingya Li", "authors": "Jingya Li, Keerthi Kumar Nagalapur, Erik Stare, Satyam Dwivedi,\n  Shehzad Ali Ashraf, Per-Erik Eriksson, Ulrika Engstr\\\"om, Woonghee Lee,\n  Thorsten Lohmar", "title": "5G New Radio for Public Safety Mission Critical Communications", "comments": "7 pages, 5 figures, 1 table, submitted to IEEE Communications\n  Standards Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driven by increasing demands on connectivity to improve safety, situational\nawareness and operational effectiveness for first responders, more and more\npublic safety agencies are realizing the need of modernization of their\nexisting non-3GPP networks. 3GPP based cellular networks offer the unique\nopportunity of providing fast, reliable, and prioritized communications for\nfirst responders in a shared network. In this article, we give an overview of\nservice requirements of public safety mission critical communications. We\nidentify key technical challenges and explain how 5G NR features are being\nevolved to meet the emerging safety critical requirements, including enabling\nconnectivity everywhere, supporting efficient group communications,\nprioritizing mission critical traffic, and providing accurate positioning for\nfirst responders.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:39:48 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:37:02 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Li", "Jingya", ""], ["Nagalapur", "Keerthi Kumar", ""], ["Stare", "Erik", ""], ["Dwivedi", "Satyam", ""], ["Ashraf", "Shehzad Ali", ""], ["Eriksson", "Per-Erik", ""], ["Engstr\u00f6m", "Ulrika", ""], ["Lee", "Woonghee", ""], ["Lohmar", "Thorsten", ""]]}, {"id": "2103.02587", "submitter": "Tomoyuki Naito Dr.", "authors": "Yoshiyuki R Shiraishi, Hiromichi Sato, Takahisa M Sanada, Tomoyuki\n  Naito", "title": "Reconstructed spatial receptive field structures by reverse correlation\n  technique explains the visual feature selectivity of units in deep\n  convolutional neural networks", "comments": "28 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important issue in dealing with Deep Convolutional Neural Networks (DCNN)\nis the 'black box problem', which represents the unknowns about internal\ninformation representation and processing, especially in the middle and higher\nlayers. In this study, we adopted a systems neuroscience methodology to measure\nthe visual feature selectivity and visualize the spatial receptive field of the\nunits in VGG16. Orientation and spatial frequency tunings of each unit were\nmeasured using sinusoidal grating stimuli. The image category selectivity of\neach unit was also measured using natural image stimuli. The spatial structures\nof the receptive fields of all convolutional units were estimated by\nactivation-weighted average (AWA) and activation-weighted covariance (AWC)\nanalyses. In the middle layers (convolutional layers in block3 and block4), AWC\nanalysis successfully reconstructed the receptive field that predicted the\nvisual feature selectivity of the unit. Those results suggested the possibility\nthat analyzing the reconstructed receptive field structure can be used to\ninterpret the functional significance of the units and layers of a DCNN.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:37:13 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shiraishi", "Yoshiyuki R", ""], ["Sato", "Hiromichi", ""], ["Sanada", "Takahisa M", ""], ["Naito", "Tomoyuki", ""]]}, {"id": "2103.02649", "submitter": "Xiaoyang Wang", "authors": "Xiaoyang Wang, Jonathan D Thomas, Robert J Piechocki, Shipra Kapoor,\n  Raul Santos-Rodriguez, Arjun Parekh", "title": "Self-play Learning Strategies for Resource Assignment in Open-RAN\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Radio Access Network (ORAN) is being developed with an aim to\ndemocratise access and lower the cost of future mobile data networks,\nsupporting network services with various QoS requirements, such as massive IoT\nand URLLC. In ORAN, network functionality is dis-aggregated into remote units\n(RUs), distributed units (DUs) and central units (CUs), which allows flexible\nsoftware on Commercial-Off-The-Shelf (COTS) deployments. Furthermore, the\nmapping of variable RU requirements to local mobile edge computing centres for\nfuture centralized processing would significantly reduce the power consumption\nin cellular networks. In this paper, we study the RU-DU resource assignment\nproblem in an ORAN system, modelled as a 2D bin packing problem. A deep\nreinforcement learning-based self-play approach is proposed to achieve\nefficient RU-DU resource management, with AlphaGo Zero inspired neural\nMonte-Carlo Tree Search (MCTS). Experiments on representative 2D bin packing\nenvironment and real sites data show that the self-play learning strategy\nachieves intelligent RU-DU resource assignment for different network\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:31:29 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wang", "Xiaoyang", ""], ["Thomas", "Jonathan D", ""], ["Piechocki", "Robert J", ""], ["Kapoor", "Shipra", ""], ["Santos-Rodriguez", "Raul", ""], ["Parekh", "Arjun", ""]]}, {"id": "2103.02762", "submitter": "Yansong Gao Dr", "authors": "Yansong Gao, Minki Kim, Chandra Thapa, Sharif Abuadbba, Zhi Zhang,\n  Seyit A. Camtepe, Hyoungshick Kim, Surya Nepal", "title": "Evaluation and Optimization of Distributed Machine Learning Techniques\n  for Internet of Things", "comments": "14 pages. arXiv admin note: text overlap with arXiv:2003.13376", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) and split learning (SL) are state-of-the-art\ndistributed machine learning techniques to enable machine learning training\nwithout accessing raw data on clients or end devices. However, their\n\\emph{comparative training performance} under real-world resource-restricted\nInternet of Things (IoT) device settings, e.g., Raspberry Pi, remains barely\nstudied, which, to our knowledge, have not yet been evaluated and compared,\nrendering inconvenient reference for practitioners. This work firstly provides\nempirical comparisons of FL and SL in real-world IoT settings regarding (i)\nlearning performance with heterogeneous data distributions and (ii) on-device\nexecution overhead. Our analyses in this work demonstrate that the learning\nperformance of SL is better than FL under an imbalanced data distribution but\nworse than FL under an extreme non-IID data distribution. Recently, FL and SL\nare combined to form splitfed learning (SFL) to leverage each of their benefits\n(e.g., parallel training of FL and lightweight on-device computation\nrequirement of SL). This work then considers FL, SL, and SFL, and mount them on\nRaspberry Pi devices to evaluate their performance, including training time,\ncommunication overhead, power consumption, and memory usage. Besides\nevaluations, we apply two optimizations. Firstly, we generalize SFL by\ncarefully examining the possibility of a hybrid type of model training at the\nserver-side. The generalized SFL merges sequential (dependent) and parallel\n(independent) processes of model training and is thus beneficial for a system\nwith large-scaled IoT devices, specifically at the server-side operations.\nSecondly, we propose pragmatic techniques to substantially reduce the\ncommunication overhead by up to four times for the SL and (generalized) SFL.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:55:37 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Gao", "Yansong", ""], ["Kim", "Minki", ""], ["Thapa", "Chandra", ""], ["Abuadbba", "Sharif", ""], ["Zhang", "Zhi", ""], ["Camtepe", "Seyit A.", ""], ["Kim", "Hyoungshick", ""], ["Nepal", "Surya", ""]]}, {"id": "2103.02823", "submitter": "Rongpeng Li", "authors": "Jianjun Wu, Rongpeng Li, Xueli An, Chenghui Peng, Zhe Liu, Jon\n  Crowcroft, and Honggang Zhang", "title": "Toward Native Artificial Intelligence in 6G Networks: System Design,\n  Architectures, and Paradigms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The mobile communication system has transformed to be the fundamental\ninfrastructure to support digital demands from all industry sectors, and 6G is\nenvisioned to go far beyond the communication-only purpose. There is coming to\na consensus that 6G will treat Artificial Intelligence (AI) as the cornerstone\nand has a potential capability to provide \"intelligence inclusion\", which\nimplies to enable the access of AI services at anytime and anywhere by anyone.\nApparently, the intelligent inclusion vision produces far-reaching influence on\nthe corresponding network architecture design in 6G and deserves a clean-slate\nrethink. In this article, we propose an end-to-end system architecture design\nscope for 6G, and talk about the necessity to incorporate an independent data\nplane and a novel intelligent plane with particular emphasis on end-to-end AI\nworkflow orchestration, management and operation. We also highlight the\nadvantages to provision converged connectivity and computing services at the\nnetwork function plane. Benefiting from these approaches, we believe that 6G\nwill turn to an \"everything as a service\" (XaaS) platform with significantly\nenhanced business merits.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 03:56:55 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Jianjun", ""], ["Li", "Rongpeng", ""], ["An", "Xueli", ""], ["Peng", "Chenghui", ""], ["Liu", "Zhe", ""], ["Crowcroft", "Jon", ""], ["Zhang", "Honggang", ""]]}, {"id": "2103.02889", "submitter": "Frederick Ziyang Hong", "authors": "Ziyang Hong and C. Patrick Yue", "title": "Efficient Training Convolutional Neural Networks on Edge Devices with\n  Gradient-pruned Sign-symmetric Feedback Alignment", "comments": "This work is published in the Proceedings of the 9th International\n  Conference on IT Convergence and Security (ICITCS2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prosperity of mobile devices, the distributed learning approach\nenabling model training with decentralized data has attracted wide research.\nHowever, the lack of training capability for edge devices significantly limits\nthe energy efficiency of distributed learning in real life. This paper\ndescribes a novel approach of training DNNs exploiting the redundancy and the\nweight asymmetry potential of conventional backpropagation. We demonstrate that\nwith negligible classification accuracy loss, the proposed approach outperforms\nthe prior arts by 5x in terms of energy efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:23:39 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 02:10:18 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Hong", "Ziyang", ""], ["Yue", "C. Patrick", ""]]}, {"id": "2103.02964", "submitter": "Bahador Bakhshi", "authors": "Bahador Bakhshi, Josep Mangues-Bafalluy", "title": "R-Learning Based Admission Control for Service Federation in\n  Multi-domain 5G Networks", "comments": "This is the draft version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Service federation in 5G/B5G networks enables service providers to\norchestrate network services across multiple domains where admission control is\na key issue. For each demand, without knowing the future ones, the admission\ncontroller either determines the domain to deploy the demand or rejects it in\norder to maximize the long-term average profit. In this paper, at first, under\nthe assumption of knowing the arrival and departure rates of demands, we obtain\nthe optimal admission control policy by formulating the problem as a Markov\ndecision process that is solved by the policy iteration method. As a practical\nsolution, where the rates are not known, we apply the Q-Learning and R-Learning\nalgorithms to approximate the optimal policy. The extensive simulation results\nshow the learning approaches outperform the greedy policy, and while the\nperformance of Q-Learning depends on the discount factor, the optimality gap of\nthe R-Learning algorithm is at most 3-5% independent of the system\nconfiguration.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:34:40 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bakhshi", "Bahador", ""], ["Mangues-Bafalluy", "Josep", ""]]}, {"id": "2103.03022", "submitter": "Guiying Huang", "authors": "Victoria Huang, Gang Chen, Qiang Fu", "title": "Multi-Agent Deep Reinforcement Learning for Request Dispatching in\n  Distributed-Controller Software-Defined Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distributed controller architectures have been quickly gaining\npopularity in Software-Defined Networking (SDN). However, the use of\ndistributed controllers introduces a new and important Request Dispatching (RD)\nproblem with the goal for every SDN switch to properly dispatch their requests\namong all controllers so as to optimize network performance. This goal can be\nfulfilled by designing an RD policy to guide distribution of requests at each\nswitch. In this paper, we propose a Multi-Agent Deep Reinforcement Learning\n(MA-DRL) approach to automatically design RD policies with high adaptability\nand performance. This is achieved through a new problem formulation in the form\nof a Multi-Agent Markov Decision Process (MA-MDP), a new adaptive RD policy\ndesign and a new MA-DRL algorithm called MA-PPO. Extensive simulation studies\nshow that our MA-DRL technique can effectively train RD policies to\nsignificantly outperform man-made policies, model-based policies, as well as RD\npolicies learned via single-agent DRL algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 09:49:27 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Huang", "Victoria", ""], ["Chen", "Gang", ""], ["Fu", "Qiang", ""]]}, {"id": "2103.03043", "submitter": "Sajib Mistry", "authors": "Athman Bouguettaya, Quan Z. Sheng, Boualem Benatallah, Azadeh Ghari\n  Neiat, Sajib Mistry, Aditya Ghose, Surya Nepal, Lina Yao", "title": "An Internet of Things Service Roadmap", "comments": "Accepted for publication in for publication in Communications of the\n  ACM (CACM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a roadmap for leveraging the tremendous opportunities the Internet\nof Things (IoT) has to offer. We argue that the combination of the recent\nadvances in service computing and IoT technology provide a unique framework for\ninnovations not yet envisaged, as well as the emergence of yet-to-be-developed\nIoT applications. This roadmap covers: emerging novel IoT services,\narticulation of major research directions, and suggestion of a roadmap to guide\nthe IoT and service computing community to address key IoT service challenges.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:40:20 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Bouguettaya", "Athman", ""], ["Sheng", "Quan Z.", ""], ["Benatallah", "Boualem", ""], ["Neiat", "Azadeh Ghari", ""], ["Mistry", "Sajib", ""], ["Ghose", "Aditya", ""], ["Nepal", "Surya", ""], ["Yao", "Lina", ""]]}, {"id": "2103.03070", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Esin Guldogan, Moncef\n  Gabbouj", "title": "Convolutional versus Self-Organized Operational Neural Networks for\n  Real-World Blind Image Denoising", "comments": "Submitted for review in IEEE TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-world blind denoising poses a unique image restoration challenge due to\nthe non-deterministic nature of the underlying noise distribution. Prevalent\ndiscriminative networks trained on synthetic noise models have been shown to\ngeneralize poorly to real-world noisy images. While curating real-world noisy\nimages and improving ground truth estimation procedures remain key points of\ninterest, a potential research direction is to explore extensions to the widely\nused convolutional neuron model to enable better generalization with fewer data\nand lower network complexity, as opposed to simply using deeper Convolutional\nNeural Networks (CNNs). Operational Neural Networks (ONNs) and their recent\nvariant, Self-organized ONNs (Self-ONNs), propose to embed enhanced\nnon-linearity into the neuron model and have been shown to outperform CNNs\nacross a variety of regression tasks. However, all such comparisons have been\nmade for compact networks and the efficacy of deploying operational layers as a\ndrop-in replacement for convolutional layers in contemporary deep architectures\nremains to be seen. In this work, we tackle the real-world blind image\ndenoising problem by employing, for the first time, a deep Self-ONN. Extensive\nquantitative and qualitative evaluations spanning multiple metrics and four\nhigh-resolution real-world noisy image datasets against the state-of-the-art\ndeep CNN network, DnCNN, reveal that deep Self-ONNs consistently achieve\nsuperior results with performance gains of up to 1.76dB in PSNR. Furthermore,\nSelf-ONNs with half and even quarter the number of layers that require only a\nfraction of computational resources as that of DnCNN can still achieve similar\nor better results compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:49:17 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 20:21:56 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Guldogan", "Esin", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03288", "submitter": "Vibhaalakshmi Sivaraman", "authors": "Vibhaalakshmi Sivaraman, Weizhao Tang, Shaileshh Bojja\n  Venkatakrishnan, Giulia Fanti, Mohammad Alizadeh", "title": "The Effect of Network Topology on Credit Network Throughput", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Credit networks rely on decentralized, pairwise trust relationships\n(channels) to exchange money or goods. Credit networks arise naturally in many\nfinancial systems, including the recent construct of payment channel networks\nin blockchain systems. An important performance metric for these networks is\ntheir transaction throughput. However, predicting the throughput of a credit\nnetwork is nontrivial. Unlike traditional communication channels, credit\nchannels can become imbalanced; they are unable to support more transactions in\na given direction once the credit limit has been reached. This potential for\nimbalance creates a complex dependency between a network's throughput and its\ntopology, path choices, and the credit balances (state) on every channel. Even\nworse, certain combinations of these factors can lead the credit network to\ndeadlocked states where no transactions can make progress. In this paper, we\nstudy the relationship between the throughput of a credit network and its\ntopology and credit state. We show that the presence of deadlocks completely\ncharacterizes a network's throughput sensitivity to different credit states.\nAlthough we show that identifying deadlocks in an arbitrary topology is\nNP-hard, we propose a peeling algorithm inspired by decoding algorithms for\nerasure codes that upper bounds the severity of the deadlock. We use the\npeeling algorithm as a tool to compare the performance of different topologies\nas well as to aid in the synthesis of topologies robust to deadlocks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:51:45 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 20:52:38 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Sivaraman", "Vibhaalakshmi", ""], ["Tang", "Weizhao", ""], ["Venkatakrishnan", "Shaileshh Bojja", ""], ["Fanti", "Giulia", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2103.03342", "submitter": "Fatemeh Lotfi", "authors": "Fatemeh Lotfi and Omid Semiari", "title": "Performance Analysis and Optimization of Uplink Cellular Networks with\n  Flexible Frame Structure", "comments": "In Proc. of the 2021 IEEE 93rd Vehicular Technology Conference:\n  VTC2021-Spring", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Future wireless cellular networks must support both enhanced mobile broadband\n(eMBB) and ultra reliable low latency communication (URLLC) to manage\nheterogeneous data traffic for emerging wireless services. To achieve this\ngoal, a promising technique is to enable flexible frame structure by\ndynamically changing the data frame's numerology according to the channel\ninformation as well as traffic quality of service requirements. However, due to\nnonorthogonal subcarriers, this technique can result in an interference, known\nas inter numerology interference (INI), thus, degrading the network\nperformance. In this work, a novel framework is proposed to analyze the INI in\nthe uplink cellular communications. In particular, a closed form expression is\nderived for the INI power in the uplink with a flexible frame structure, and a\nnew resource allocation problem is formulated to maximize the network spectral\nefficiency (SE) by jointly optimizing the power allocation and numerology\nselection in a multi user uplink scenario. The simulation results validate the\nderived theoretical INI analyses and provide guidelines for power allocation\nand numerology selection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 21:35:11 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 02:19:48 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Lotfi", "Fatemeh", ""], ["Semiari", "Omid", ""]]}, {"id": "2103.03572", "submitter": "Kirill Glinskiy", "authors": "Kirill Glinskiy, Evgeny Khorov, Alexey Kureev", "title": "SDR-based Testbed for Real-time CQI Prediction for URLLC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-reliable Low-Latency Communication (URLLC) is a key feature of 5G\nsystems. The quality of service (QoS) requirements imposed by URLLC are less\nthan 10ms delay and less than $10^{-5}$ packet loss rate (PLR). To satisfy such\nstrict requirements with minimal channel resource consumption, the devices need\nto accurately predict the channel quality and select Modulation and Coding\nScheme (MCS) for URLLC in a proper way.\n  This paper presents a novel real-time channel prediction system based on\nSoftware-Defined Radio that uses a neural network. The paper also describes and\nshares an open channel measurement dataset that can be used to compare various\nchannel prediction approaches in different mobility scenarios in future\nresearch on URLLC\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:17:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Glinskiy", "Kirill", ""], ["Khorov", "Evgeny", ""], ["Kureev", "Alexey", ""]]}, {"id": "2103.03745", "submitter": "Salvatore D'Oro", "authors": "Salvatore D'Oro, Francesco Restuccia and Tommaso Melodia", "title": "Can You Fix My Neural Network? Real-Time Adaptive Waveform Synthesis for\n  Resilient Wireless Signal Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to its capability of classifying complex phenomena without explicit\nmodeling, deep learning (DL) has been demonstrated to be a key enabler of\nWireless Signal Classification (WSC). Although DL can achieve a very high\naccuracy under certain conditions, recent research has unveiled that the\nwireless channel can disrupt the features learned by the DL model during\ntraining, thus drastically reducing the classification performance in\nreal-world live settings. Since retraining classifiers is cumbersome after\ndeployment, existing work has leveraged the usage of carefully-tailored Finite\nImpulse Response (FIR) filters that, when applied at the transmitter's side,\ncan restore the features that are lost because of the the channel actions,\ni.e., waveform synthesis. However, these approaches compute FIRs using offline\noptimization strategies, which limits their efficacy in highly-dynamic channel\nsettings. In this paper, we improve the state of the art by proposing Chares, a\nDeep Reinforcement Learning (DRL)-based framework for channel-resilient\nadaptive waveform synthesis. Chares adapts to new and unseen channel conditions\nby optimally computing through DRL the FIRs in real-time. Chares is a DRL agent\nwhose architecture is-based upon the Twin Delayed Deep Deterministic Policy\nGradients (TD3), which requires minimal feedback from the receiver and explores\na continuous action space. Chares has been extensively evaluated on two\nwell-known datasets. We have also evaluated the real-time latency of Chares\nwith an implementation on field-programmable gate array (FPGA). Results show\nthat Chares increases the accuracy up to 4.1x when no waveform synthesis is\nperformed, by 1.9x with respect to existing work, and can compute new actions\nwithin 41us.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:17:13 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["D'Oro", "Salvatore", ""], ["Restuccia", "Francesco", ""], ["Melodia", "Tommaso", ""]]}, {"id": "2103.03819", "submitter": "Paolo Testolina", "authors": "Andrea Varischio, Francesco Mandruzzato, Marcello Bullo, Marco\n  Giordani, Paolo Testolina, Michele Zorzi", "title": "Hybrid Point Cloud Semantic Compression for Automotive Sensors: A\n  Performance Evaluation", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a fully autonomous driving framework, where vehicles operate without human\nintervention, information sharing plays a fundamental role. In this context,\nnew network solutions have to be designed to handle the large volumes of data\ngenerated by the rich sensor suite of the cars in a reliable and efficient way.\nAmong all the possible sensors, Light Detection and Ranging (LiDAR) can produce\nan accurate 3D point cloud representation of the surrounding environment, which\nin turn generates high data rates. For this reason, efficient point cloud\ncompression is paramount to alleviate the burden of data transmission over\nbandwidth-constrained channels and to facilitate real-time communications. In\nthis paper, we propose a pipeline to efficiently compress LiDAR observations in\nan automotive scenario. First, we leverage the capabilities of RangeNet++, a\nDeep Neural Network (DNN) used to semantically infer point labels, to reduce\nthe channel load by selecting the most valuable environmental data to be\ndisseminated. Second, we compress the selected points using Draco, a 3D\ncompression algorithm which is able to obtain compression up to the\nquantization error. Our experiments, validated on the Semantic KITTI dataset,\ndemonstrate that it is possible to compress and send the information at the\nframe rate of the LiDAR, thus achieving real-time performance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:34:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Varischio", "Andrea", ""], ["Mandruzzato", "Francesco", ""], ["Bullo", "Marcello", ""], ["Giordani", "Marco", ""], ["Testolina", "Paolo", ""], ["Zorzi", "Michele", ""]]}, {"id": "2103.03866", "submitter": "Abbas Yazdinejad", "authors": "Mostafa Kazemi (Department of Electrical Engineering, Faculty of\n  Engineering, Shahed University, Tehran, Iran) and Abbas Yazdinejad (School of\n  Computer Science, University of Guelph, Ontario, Canada)", "title": "Towards Automated Benchmark Support for Multi-Blockchain\n  Interoperability-Facilitating Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the introduction of the first Bitcoin blockchain in 2008, different\ndecentralized blockchain systems such as Ethereum, Hyperledger Fabric, and\nCorda, have emerged with public and private accessibility. It has been widely\nacknowledged that no single blockchain network will fit all use cases. As a\nresult, we have observed the increasing popularity of multi-blockchain\necosystem in which customers will move toward different blockchains based on\ntheir particular requirements. Hence, the efficiency and security requirements\nof interactions among these heterogeneous blockchains become critical. In\nrealization of this multi-blockchain paradigm, initiatives in building\nInteroperability-Facilitating Platforms (IFPs) that aim at bridging different\nblockchains (a.k.a. blockchain interoperability) have come to the fore. Despite\ncurrent efforts, it is extremely difficult for blockchain customers\n(organizations, governments, companies) to understand the trade-offs between\ndifferent IFPs and their suitability for different application domains before\nadoption. A key reason is due to a lack of fundamental and systematic\napproaches to assess the variables among different IFPs. To fill this gap,\ndeveloping new IFP requirements specification and open-source benchmark tools\nto advance research in distributed, multi-blockchain interoperability, with\nemphasis on IFP performance and security challenges are required. In this\ndocument, we outline a research proposal study to the community to realize this\ngap.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:33:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kazemi", "Mostafa", "", "Department of Electrical Engineering, Faculty of\n  Engineering, Shahed University, Tehran, Iran"], ["Yazdinejad", "Abbas", "", "School of\n  Computer Science, University of Guelph, Ontario, Canada"]]}, {"id": "2103.04092", "submitter": "Israel Leyva-Mayorga", "authors": "Federico Chiariotti, Israel Leyva-Mayorga, \\v{C}edomir Stefanovi\\'c,\n  Anders E. Kal{\\o}r, and Petar Popovski", "title": "RAN Slicing Performance Trade-offs: Timing versus Throughput\n  Requirements", "comments": "Submitted to IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coexistence of diverse services with heterogeneous requirements is a\nmajor component of 5G. This calls for spectrum slicing: efficient sharing of\nthe wireless resources among diverse users while guaranteeing the respective\nrequirements in terms of throughput, timing, and/or reliability. In this paper,\nwe set up a simple system model of a radio access network (RAN) for\ninvestigating spectrum slicing for two user types: (1) broadband users with\nthroughput requirements and (2) intermittently active users with a timing\nrequirement, expressed as either latency-reliability or Age of Information\n(AoI). We evaluate the trade-offs between between the achievable throughput of\nbroadband users and the timing requirement of the intermittent users under\nthree different access modes: Orthogonal Multiple Access (OMA), Non-Orthogonal\nMultiple Access (NOMA), and Partial Non-Orthogonal Multiple Access (PNOMA).\nAnalysis shows that, under a pessimistic model with destructive collisions,\nNOMA schemes can almost match the throughput of OMA without jeopardizing the\nreliability and timeliness performance. This indicates that the capture effect\ncan significantly improve NOMA performance, as confirmed by the numerical\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 10:39:51 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chiariotti", "Federico", ""], ["Leyva-Mayorga", "Israel", ""], ["Stefanovi\u0107", "\u010cedomir", ""], ["Kal\u00f8r", "Anders E.", ""], ["Popovski", "Petar", ""]]}, {"id": "2103.04119", "submitter": "Parham Hadikhani", "authors": "Meysam Yari, Parham Hadikhani, Mohammad Yaghoubi, Raza Nowrozy, Zohreh\n  Asgharzadeh", "title": "An Energy Efficient Routing Algorithm for Wireless Sensor Networks Using\n  Mobile Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing usage of wireless sensor networks in human life is an\nindication of the high importance of this technology. Wireless sensor networks\nhave a vast majority of applications in monitoring and care which are known as\ntarget tracking. In this application, the moving targets are monitored and\ntracked in the environment. One of the most important challenges in this area\nis the limited energy of the sensors. In this paper, we proposed a new\nalgorithm to reduce energy consumption by increasing the load balancing in the\nnetwork. The proposed algorithm consists of four phases. In the first phase,\nwhich is the hole prevention phase, in each cluster, it is checked by the\ncluster heads that if the energy level in an area of the cluster is less than\nthe threshold, a mobile node is sent to that area. The second phase is the\nupdate phase. In this phase, the parameters required to detect a hole are\nupdated. In the third phase, the hole in the cluster is detected, and in the\nfourth phase, the hole is covered by static or moving nodes. A comparison of\nsimulation results with the well-known and successful routing method in\nwireless sensor networks show that the proposed method is suitable and working\nproperly.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 14:25:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Yari", "Meysam", ""], ["Hadikhani", "Parham", ""], ["Yaghoubi", "Mohammad", ""], ["Nowrozy", "Raza", ""], ["Asgharzadeh", "Zohreh", ""]]}, {"id": "2103.04121", "submitter": "Myounggyu Won", "authors": "Pradeep Sambu and Myounggyu Won", "title": "An Experimental Study on Direction Finding of Bluetooth 5.1: Indoor vs\n  Outdoor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bluetooth Special Interest Group (Bluetooth SIG) introduced a new feature\nfor highly accurate localization called the Direction Finding in the Bluetooth\nCore Specification 5.1. Since this new localization feature is relatively new,\ndespite the significant interest of industry and academia in the accurate\npositioning of Bluetooth devices/tags, there are only a handful of experimental\nstudies conducted to evaluate the performance of the new technology.\nFurthermore, these experimental works are constrained to only indoor\nenvironments or performed with hardware emulation of Bluetooth 5.1 via\nUniversal Software Radio Peripherals (USRPs). In this paper, we perform an\nexperimental study on the positioning accuracy of the direction finding using\nCOTS Bluetooth 5.1 devices in booth indoor and outdoor environments to provide\ninsights on the performance gap under these different experimental settings.\nOur results demonstrate that the average angular error in an outdoor\nenvironment is 0.28 degrees, significantly improving the angular error measured\nin an indoor environment by 73%. It is also demonstrated that the average\npositioning accuracy measured in an outdoor environment is 22cm which is 39.7%\nsmaller than that measured in an indoor environment.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 14:34:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sambu", "Pradeep", ""], ["Won", "Myounggyu", ""]]}, {"id": "2103.04303", "submitter": "Nguyen Van Huynh", "authors": "Nguyen Van Huynh, Dinh Thai Hoang, Diep N. Nguyen, and Eryk Dutkiewicz", "title": "Joint Coding and Scheduling Optimization for Distributed Learning over\n  Wireless Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike theoretical distributed learning (DL), DL over wireless edge networks\nfaces the inherent dynamics/uncertainty of wireless connections and edge nodes,\nmaking DL less efficient or even inapplicable under the highly dynamic wireless\nedge networks (e.g., using mmW interfaces). This article addresses these\nproblems by leveraging recent advances in coded computing and the deep dueling\nneural network architecture. By introducing coded structures/redundancy, a\ndistributed learning task can be completed without waiting for straggling\nnodes. Unlike conventional coded computing that only optimizes the code\nstructure, coded distributed learning over the wireless edge also requires to\noptimize the selection/scheduling of wireless edge nodes with heterogeneous\nconnections, computing capability, and straggling effects. However, even\nneglecting the aforementioned dynamics/uncertainty, the resulting joint\noptimization of coding and scheduling to minimize the distributed learning time\nturns out to be NP-hard. To tackle this and to account for the dynamics and\nuncertainty of wireless connections and edge nodes, we reformulate the problem\nas a Markov Decision Process and then design a novel deep reinforcement\nlearning algorithm that employs the deep dueling neural network architecture to\nfind the jointly optimal coding scheme and the best set of edge nodes for\ndifferent learning tasks without explicit information about the wireless\nenvironment and edge nodes' straggling parameters. Simulations show that the\nproposed framework reduces the average learning delay in wireless edge\ncomputing up to 66% compared with other DL approaches. The jointly optimal\nframework in this article is also applicable to any distributed learning scheme\nwith heterogeneous and uncertain computing nodes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 08:57:09 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 04:20:00 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Van Huynh", "Nguyen", ""], ["Hoang", "Dinh Thai", ""], ["Nguyen", "Diep N.", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "2103.04443", "submitter": "Oliver Hohlfeld", "authors": "Daniel Kopp and Christoph Dietzel and Oliver Hohlfeld", "title": "DDoS Never Dies? An IXP Perspective on DDoS Amplification Attacks", "comments": "To appear at PAM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DDoS attacks remain a major security threat to the continuous operation of\nInternet edge infrastructures, web services, and cloud platforms. While a large\nbody of research focuses on DDoS detection and protection, to date we\nultimately failed to eradicate DDoS altogether. Yet, the landscape of DDoS\nattack mechanisms is even evolving, demanding an updated perspective on DDoS\nattacks in the wild. In this paper, we identify up to 2608 DDoS amplification\nattacks at a single day by analyzing multiple Tbps of traffic flows at a major\nIXP with a rich ecosystem of different networks. We observe the prevalence of\nwell-known amplification attack protocols (e.g., NTP, CLDAP), which should no\nlonger exist given the established mitigation strategies. Nevertheless, they\npose the largest fraction on DDoS amplification attacks within our observation\nand we witness the emergence of DDoS attacks using recently discovered\namplification protocols (e.g., OpenVPN, ARMS, Ubiquity Discovery Protocol). By\nanalyzing the impact of DDoS on core Internet infrastructure, we show that DDoS\ncan overload backbone-capacity and that filtering approaches in prior work omit\n97% of the attack traffic.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:22:03 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kopp", "Daniel", ""], ["Dietzel", "Christoph", ""], ["Hohlfeld", "Oliver", ""]]}, {"id": "2103.04536", "submitter": "Medhat Elsayed", "authors": "Medhat Elsayed, Melike Erol-Kantarci", "title": "AI-enabled Future Wireless Networks: Challenges, Opportunities and Open\n  Issues", "comments": null, "journal-ref": "IEEE Vehicular Technology Magazine ( Volume: 14, Issue: 3, Sept.\n  2019)", "doi": "10.1109/MVT.2019.2919236", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A plethora of demanding services and use cases mandate a revolutionary shift\nin the management of future wireless network resources. Indeed, when tight\nquality of service demands of applications are combined with increased\ncomplexity of the network, legacy network management routines will become\nunfeasible in 6G. Artificial Intelligence (AI) is emerging as a fundamental\nenabler to orchestrate the network resources from bottom to top. AI-enabled\nradio access and AI-enabled core will open up new opportunities for automated\nconfiguration of 6G. On the other hand, there are many challenges in AI-enabled\nnetworks that need to be addressed. Long convergence time, memory complexity,\nand complex behaviour of machine learning algorithms under uncertainty as well\nas highly dynamic channel, traffic and mobility conditions of the network\ncontribute to the challenges. In this paper, we survey the state-of-art\nresearch in utilizing machine learning techniques in improving the performance\nof wireless networks. In addition, we identify challenges and open issues to\nprovide a roadmap for the researchers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 03:59:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Elsayed", "Medhat", ""], ["Erol-Kantarci", "Melike", ""]]}, {"id": "2103.04609", "submitter": "Mattia Lecci", "authors": "Mattia Lecci, Andrea Zanella, Michele Zorzi", "title": "An ns-3 Implementation of a Bursty Traffic Framework for Virtual Reality\n  Sources", "comments": "9 pages, 6 figures. Please cite it as: M. Lecci, A. Zanella, M.\n  Zorzi, \"An ns-3 Implementation of a Bursty Traffic Framework for Virtual\n  Reality Sources,\" in Workshop on ns-3 (WNS3), Jun. 2021, Virtual Event, US", "journal-ref": null, "doi": "10.1145/3460797.3460807", "report-no": null, "categories": "cs.NI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation wireless communication technologies will allow users to\nobtain unprecedented performance, paving the way to new and immersive\napplications. A prominent application requiring high data rates and low\ncommunication delay is Virtual Reality (VR), whose presence will become\nincreasingly stronger in the years to come. To the best of our knowledge, we\npropose the first traffic model for VR applications based on traffic traces\nacquired from a commercial VR streaming software, allowing the community to\nfurther study and improve the technology to manage this type of traffic. This\nwork implements ns-3 applications able to generate and process large bursts of\npackets, enabling the possibility of analyzing APP-level end-to-end metrics,\nmaking the source code as well as the acquired VR traffic traces publicly\navailable and open-source.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 08:59:58 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:03:37 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lecci", "Mattia", ""], ["Zanella", "Andrea", ""], ["Zorzi", "Michele", ""]]}, {"id": "2103.04930", "submitter": "Carlos Rea\\~no", "authors": "Jason Kennedy, Blesson Varghese and Carlos Rea\\~no", "title": "AVEC: Accelerator Virtualization in Cloud-Edge Computing for Deep\n  Learning Libraries", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge computing offers the distinct advantage of harnessing compute\ncapabilities on resources located at the edge of the network to run workloads\nof relatively weak user devices. This is achieved by offloading computationally\nintensive workloads, such as deep learning from user devices to the edge. Using\nthe edge reduces the overall communication latency of applications as workloads\ncan be processed closer to where data is generated on user devices rather than\nsending them to geographically distant clouds. Specialised hardware\naccelerators, such as Graphics Processing Units (GPUs) available in the\ncloud-edge network can enhance the performance of computationally intensive\nworkloads that are offloaded from devices on to the edge. The underlying\napproach required to facilitate this is virtualization of GPUs. This paper\ntherefore sets out to investigate the potential of GPU accelerator\nvirtualization to improve the performance of deep learning workloads in a\ncloud-edge environment. The AVEC accelerator virtualization framework is\nproposed that incurs minimum overheads and requires no source-code modification\nof the workload. AVEC intercepts local calls to a GPU on a device and forwards\nthem to an edge resource seamlessly. The feasibility of AVEC is demonstrated on\na real-world application, namely OpenPose using the Caffe deep learning\nlibrary. It is observed that on a lab-based experimental test-bed AVEC delivers\nup to 7.48x speedup despite communication overheads incurred due to data\ntransfers.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:43:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kennedy", "Jason", ""], ["Varghese", "Blesson", ""], ["Rea\u00f1o", "Carlos", ""]]}, {"id": "2103.05024", "submitter": "Jose Saldana", "authors": "Jose Saldana, Omer Topal, Jose Ruiz-Mas, Julian Fernandez-Navajas", "title": "Finding the Sweet Spot for Frame Aggregation in 802.11 WLANs", "comments": null, "journal-ref": null, "doi": "10.1109/LCOMM.2020.3044231", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter proposes an algorithm for the dynamic tuning of the maximum size\nof aggregated frames in 802.11 WLANs. Traffic flows with opposed requirements\nmay coexist in these networks: traditional services as web browsing or file\ndownload that need high throughput, and services with realtime requirements\nthat need low latency. The proposed algorithm allows the network manager to\nfind an optimal balance (i.e. the \"sweet spot\" between throughput and latency:\na \"delay budget\" can be assigned to real-time flows, with the objective of\nkeeping the latency as close as possible to that budget, while penalizing the\nthroughput of traditional services as little as possible.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:24:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Saldana", "Jose", ""], ["Topal", "Omer", ""], ["Ruiz-Mas", "Jose", ""], ["Fernandez-Navajas", "Julian", ""]]}, {"id": "2103.05026", "submitter": "Philipp H. Kindt", "authors": "Philipp H. Kindt and Samarjit Chakraborty", "title": "Performance Limits of Neighbor Discovery in Wireless Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.05220", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighbor Discovery (ND) is the process employed by two wireless devices to\ndiscover each other. There are many different ND protocols, both in the\nscientific literature and also those employed in practice. All ND protocols\ninvolve devices sending beacons, and also listening for them. Protocols differ\nin terms of how the beacon transmissions and reception windows are scheduled,\nand the device sleeps in between consecutive transmissions and reception\nwindows in order to save energy. A successful discovery constitutes a sending\ndevice's beacon overlapping with a receiving device's reception window. The\ngoal of all ND protocols is to minimize the discovery latency. In spite of the\nubiquity of ND protocols and active research on this topic for over two\ndecades, the basic question \"Given an energy budget, what is the minimum\nguaranteed ND latency?\", however, still remains unanswered. Given the different\nkinds of protocols that exist, there has also been no standard way of comparing\nthem and their performance. This paper, for the first time, answers the\nquestion on the best-achievable ND latency for a given energy budget. We derive\ndiscovery latencies for different scenarios, e.g., when both devices have the\nsame energy budgets, and both devices have different energy budgets. We also\nshow that some existing protocols can be parametrized such that they perform\noptimally. The fact that the parametrizations of some other protocols were\noptimal was not known before, and can now be established using our technique.\nOur results are restricted to the case when a few devices discover each other\nat a time, as is the case in most real-life scenarios. When many devices need\nto discover each other simultaneously, packet collisions play a dominant role\nin the discovery latency and how to analyze such scenarios need further study.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:26:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kindt", "Philipp H.", ""], ["Chakraborty", "Samarjit", ""]]}, {"id": "2103.05047", "submitter": "Medhat Elsayed", "authors": "Medhat Elsayed, Melike Erol-Kantarci", "title": "Radio Resource and Beam Management in 5G mmWave Using Clustering and\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": "10.1109/GLOBECOM42002.2020.9322401", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To optimally cover users in millimeter-Wave (mmWave) networks, clustering is\nneeded to identify the number and direction of beams. The mobility of users\nmotivates the need for an online clustering scheme to maintain up-to-date beams\ntowards those clusters. Furthermore, mobility of users leads to varying\npatterns of clusters (i.e., users move from the coverage of one beam to\nanother), causing dynamic traffic load per beam. As such, efficient radio\nresource allocation and beam management is needed to address the dynamicity\nthat arises from mobility of users and their traffic. In this paper, we\nconsider the coexistence of Ultra-Reliable Low-Latency Communication (URLLC)\nand enhanced Mobile BroadBand (eMBB) users in 5G mmWave networks and propose a\nQuality-of-Service (QoS) aware clustering and resource allocation scheme.\nSpecifically, Density-Based Spatial Clustering of Applications with Noise\n(DBSCAN) is used for online clustering of users and the selection of the number\nof beams. In addition, Long Short Term Memory (LSTM)-based Deep Reinforcement\nLearning (DRL) scheme is used for resource block allocation. The performance of\nthe proposed scheme is compared to a baseline that uses K-means and\npriority-based proportional fairness for clustering and resource allocation,\nrespectively. Our simulation results show that the proposed scheme outperforms\nthe baseline algorithm in terms of latency, reliability, and rate of URLLC\nusers as well as rate of eMBB users.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:05:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Elsayed", "Medhat", ""], ["Erol-Kantarci", "Melike", ""]]}, {"id": "2103.05061", "submitter": "Medhat Elsayed", "authors": "Medhat Elsayed, Kevin Shimotakahara, Melike Erol-Kantarci", "title": "Machine Learning-based Inter-Beam Inter-Cell Interference Mitigation in\n  mmWave", "comments": null, "journal-ref": null, "doi": "10.1109/ICC40277.2020.9148711", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address inter-beam inter-cell interference mitigation in 5G\nnetworks that employ millimeter-wave (mmWave), beamforming and non-orthogonal\nmultiple access (NOMA) techniques. Those techniques play a key role in\nimproving network capacity and spectral efficiency by multiplexing users on\nboth spatial and power domains. In addition, the coverage area of multiple\nbeams from different cells can intersect, allowing more flexibility in\nuser-cell association. However, the intersection of coverage areas also implies\nincreased inter-beam inter-cell interference, i.e. interference among beams\nformed by nearby cells. Therefore, joint user-cell association and inter-beam\npower allocation stand as a promising solution to mitigate inter-beam,\ninter-cell interference. In this paper, we consider a 5G mmWave network and\npropose a reinforcement learning algorithm to perform joint user-cell\nassociation and inter-beam power allocation to maximize the sum rate of the\nnetwork. The proposed algorithm is compared to a uniform power allocation that\nequally divides power among beams per cell. Simulation results present a\nperformance enhancement of 13-30% in network's sum-rate corresponding to the\nlowest and highest traffic loads, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 20:36:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Elsayed", "Medhat", ""], ["Shimotakahara", "Kevin", ""], ["Erol-Kantarci", "Melike", ""]]}, {"id": "2103.05091", "submitter": "Ekaterina Tolstaya", "authors": "Ekaterina Tolstaya, Landon Butler, Daniel Mox, James Paulos, Vijay\n  Kumar, Alejandro Ribeiro", "title": "Learning Connectivity for Data Distribution in Robot Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many algorithms for control of multi-robot teams operate under the assumption\nthat low-latency, global state information necessary to coordinate agent\nactions can readily be disseminated among the team. However, in harsh\nenvironments with no existing communication infrastructure, robots must form\nad-hoc networks, forcing the team to operate in a distributed fashion. To\novercome this challenge, we propose a task-agnostic, decentralized, low-latency\nmethod for data distribution in ad-hoc networks using Graph Neural Networks\n(GNN). Our approach enables multi-agent algorithms based on global state\ninformation to function by ensuring it is available at each robot. To do this,\nagents glean information about the topology of the network from packet\ntransmissions and feed it to a GNN running locally which instructs the agent\nwhen and where to transmit the latest state information. We train the\ndistributed GNN communication policies via reinforcement learning using the\naverage Age of Information as the reward function and show that it improves\ntraining stability compared to task-specific reward functions. Our approach\nperforms favorably compared to industry-standard methods for data distribution\nsuch as random flooding and round robin. We also show that the trained policies\ngeneralize to larger teams of both static and mobile agents.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:48:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Tolstaya", "Ekaterina", ""], ["Butler", "Landon", ""], ["Mox", "Daniel", ""], ["Paulos", "James", ""], ["Kumar", "Vijay", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "2103.05311", "submitter": "Syed Waqas Haider Shah", "authors": "Syed Waqas Haider Shah, Adnan Noor Mian, Adnan Aijaz, Junaid Qadir,\n  Jon Crowcroft", "title": "Energy-Efficient MAC for Cellular IoT: State-of-the-Art, Challenges, and\n  Standardization", "comments": null, "journal-ref": "IEEE Transactions on Green Communications and Networking, Feb.\n  2021", "doi": "10.1109/TGCN.2021.3062093", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the modern world, the connectivity-as-we-go model is gaining popularity.\nInternet-of-Things (IoT) envisions a future in which human beings communicate\nwith each other and with devices that have identities and virtual\npersonalities, as well as sensing, processing, and networking capabilities,\nwhich will allow the developing of smart environments that operate with little\nor no human intervention. In such IoT environments, that will have\nbattery-operated sensors and devices, energy efficiency becomes a fundamental\nconcern. Thus, energy-efficient (EE) connectivity is gaining significant\nattention from the industrial and academic communities. This work aims to\nprovide a comprehensive state-of-the-art survey on the energy efficiency of\nmedium access control (MAC) protocols for cellular IoT. we provide a detailed\ndiscussion on the sources of energy dissipation at the MAC layer and then\npropose solutions. In addition to reviewing the proposed MAC designs, we also\nprovide insights and suggestions that can guide practitioners and researchers\nin designing EE MAC protocols that extend the battery life of IoT devices.\nFinally, we identify a range of challenging open problems that should be solved\nfor providing EE MAC services for IoT devices, along with corresponding\nopportunities and future research ideas to address these challenges.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 09:20:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Shah", "Syed Waqas Haider", ""], ["Mian", "Adnan Noor", ""], ["Aijaz", "Adnan", ""], ["Qadir", "Junaid", ""], ["Crowcroft", "Jon", ""]]}, {"id": "2103.05329", "submitter": "Aleksey Kureev", "authors": "Evgeny Khorov, Aleksey Kureev, Vladislav Molodtsov", "title": "FIND: an SDR-based Tool for Fine Indoor Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An indoor localization approach uses Wi-Fi Access Points (APs) to estimate\nthe Direction of Arrival (DoA) of the WiFi signals. This paper demonstrates\nFIND, a tool for Fine INDoor localization based on a software-defined radio,\nwhich receives Wi-Fi frames in the 80 MHz band with four antennas. To the best\nof our knowledge, it is the first-ever prototype that extracts from such frames\ndata in both frequency and time domains to calculate the DoA of Wi-Fi signals\nin real-time. Apart from other prototypes, we retrieve from frames\ncomprehensive information that could be used to DoA estimation: all preamble\nfields in the time domain, Channels State Information, and signal-to-noise\nratio. Using our device, we collect a dataset for comparing different\nalgorithms estimating the angle of arrival in the same scenario. Furthermore,\nwe propose a novel calibration method, eliminating the constant phase shift\nbetween receiving paths caused by hardware imperfections. All calibration data,\nas well as a gathered dataset with various DoA in an anechoic chamber and in a\nclassroom, are provided to facilitate further research in the area of indoor\nlocalization, intelligence surfaces, and multi-user transmissions in dense\ndeployments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:19:10 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Khorov", "Evgeny", ""], ["Kureev", "Aleksey", ""], ["Molodtsov", "Vladislav", ""]]}, {"id": "2103.05391", "submitter": "Elif Uysal", "authors": "Elif Uysal, Onur Kaya, Anthony Ephremides, James Gross, Marian\n  Codreanu, Petar Popovski, Mohamad Assaad, Gianluigi Liva, Andrea Munari,\n  Touraj Soleymani, Beatriz Soret, Karl Henrik Johansson", "title": "Semantic Communications in Networked Systems", "comments": "9 pages, 6 figures, 1500 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT cs.NI cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our vision for a departure from the established way of\narchitecting and assessing communication networks, by incorporating the\nsemantics of information for communications and control in networked systems.\nWe define semantics of information, not as the meaning of the messages, but as\ntheir significance, possibly within a real time constraint, relative to the\npurpose of the data exchange. We argue that research efforts must focus on\nlaying the theoretical foundations of a redesign of the entire process of\ninformation generation, transmission and usage in unison by developing:\nadvanced semantic metrics for communications and control systems; an optimal\nsampling theory combining signal sparsity and semantics, for real-time\nprediction, reconstruction and control under communication constraints and\ndelays; semantic compressed sensing techniques for decision making and\ninference directly in the compressed domain; semantic-aware data generation,\nchannel coding, feedback, multiple and random access schemes that reduce the\nvolume of data and the energy consumption, increasing the number of supportable\ndevices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:22:23 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 12:45:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Uysal", "Elif", ""], ["Kaya", "Onur", ""], ["Ephremides", "Anthony", ""], ["Gross", "James", ""], ["Codreanu", "Marian", ""], ["Popovski", "Petar", ""], ["Assaad", "Mohamad", ""], ["Liva", "Gianluigi", ""], ["Munari", "Andrea", ""], ["Soleymani", "Touraj", ""], ["Soret", "Beatriz", ""], ["Johansson", "Karl Henrik", ""]]}, {"id": "2103.05554", "submitter": "Benjamin Fabian", "authors": "Milena Oehlers and Benjamin Fabian", "title": "Graph Metrics for Internet Robustness -- A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on the robustness of the Internet has gained critical importance in\nthe last decades because more and more individuals, societies and firms rely on\nthis global network infrastructure for communication, knowledge transfer,\nbusiness processes and e-commerce. In particular, modeling the structure of\nInternet has inspired several novel graph metrics for assessing important\ntopological robustness features of large complex networks such as the Internet.\nThis survey provides a comparative overview of these metrics, presents their\nstrengths and limitations for analyzing the robustness of the Internet\ntopology, and outlines a conceptual tool set in order to facilitate future\nanalysis and adoption in research and practice.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:59:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Oehlers", "Milena", ""], ["Fabian", "Benjamin", ""]]}, {"id": "2103.06026", "submitter": "Soeren Becker", "authors": "Ana Juan Ferrer, Soeren Becker, Florian Schmidt, Lauritz Thamsen, Odej\n  Kao", "title": "Towards a Cognitive Compute Continuum: An Architecture for Ad-Hoc\n  Self-Managed Swarms", "comments": "8 pages, CCGrid 2021 Cloud2Things Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce our vision of a Cognitive Computing Continuum to\naddress the changing IT service provisioning towards a distributed,\nopportunistic, self-managed collaboration between heterogeneous devices outside\nthe traditional data center boundaries. The focal point of this continuum are\ncognitive devices, which have to make decisions autonomously using their\non-board computation and storage capacity based on information sensed from\ntheir environment. Such devices are moving and cannot rely on fixed\ninfrastructure elements, but instead realise on-the-fly networking and thus\nfrequently join and leave temporal swarms. All this creates novel demands for\nthe underlying architecture and resource management, which must bridge the gap\nfrom edge to cloud environments, while keeping the QoS parameters within\nrequired boundaries. The paper presents an initial architecture and a resource\nmanagement framework for the implementation of this type of IT service\nprovisioning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:56:00 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ferrer", "Ana Juan", ""], ["Becker", "Soeren", ""], ["Schmidt", "Florian", ""], ["Thamsen", "Lauritz", ""], ["Kao", "Odej", ""]]}, {"id": "2103.06221", "submitter": "Pietro Tedeschi", "authors": "Pietro Tedeschi and Kang Eun Jeon and James She and Simon Wong and\n  Spiridon Bakiras and Roberto Di Pietro", "title": "Privacy-Preserving and Sustainable Contact Tracing Using Batteryless BLE\n  Beacons", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contact tracing with mobile applications is an attractive approach for many\ngovernments and industry initiatives to address the Covid-19 pandemic. However,\nmany approaches today have severe privacy and security issues, and many of them\nalso fail to offer a sustainable contact tracing infrastructure due to the\ndemanding energy consumption. This work makes several contributions towards\novercoming these limitations. First, we propose a privacy-preserving\narchitecture for contact tracing that leverages a fixed infrastructure of BLE\nbeacon transmitters. Second, we evaluate the feasibility of adopting\nbatteryless or energy-harvesting BLE beacons to make this architecture more\nsustainable and green. Finally, we identify practical research challenges and\nopportunities for academia and industry to advance and realize the proposed\nprivacy-preserving and sustainable contact tracing architecture.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:45:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tedeschi", "Pietro", ""], ["Jeon", "Kang Eun", ""], ["She", "James", ""], ["Wong", "Simon", ""], ["Bakiras", "Spiridon", ""], ["Di Pietro", "Roberto", ""]]}, {"id": "2103.06367", "submitter": "Zohre Ranjbar Mojaveri", "authors": "Zohre R. Mojaveri and Andr\\'as Farag\\'o", "title": "Routing for Global Congestion Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling networks as different graph types and researching on route finding\nstrategies, to avoid congestion in dense subnetworks via graph-theoretic\napproaches, contributes to overall blocking probability reduction in networks.\nOur main focus is to study methods for modeling congested subnetworks and graph\ndensity measures, in order to identify routes that avoid dense subgraphs for\nglobal congestion avoidance, along with covering related algorithmic issues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:14:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mojaveri", "Zohre R.", ""], ["Farag\u00f3", "Andr\u00e1s", ""]]}, {"id": "2103.06472", "submitter": "Yipeng Zhou", "authors": "Miao Hu, Xianzhuo Luo, Jiawen Chen, Young Choon Lee, Yipeng Zhou, Di\n  Wu", "title": "Virtual Reality: A Survey of Enabling Technologies and its Applications\n  in IoT", "comments": null, "journal-ref": "Journal of Network and Computer Applications (2021): 102970", "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) has shown great potential to revolutionize the market by\nproviding users immersive experiences with freedom of movement. Compared to\ntraditional video streaming, VR is with ultra high-definition and dynamically\nchanges with users' head and eye movements, which poses significant challenges\nfor the realization of such potential. In this paper, we provide a detailed and\nsystematic survey of enabling technologies of virtual reality and its\napplications in Internet of Things (IoT). We identify major challenges of\nvirtual reality on system design, view prediction, computation, streaming, and\nquality of experience evaluation. We discuss each of them by extensively\nsurveying and reviewing related papers in the recent years. We also introduce\nseveral use cases of VR for IoT. Last, issues and future research directions\nare also identified and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:39:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hu", "Miao", ""], ["Luo", "Xianzhuo", ""], ["Chen", "Jiawen", ""], ["Lee", "Young Choon", ""], ["Zhou", "Yipeng", ""], ["Wu", "Di", ""]]}, {"id": "2103.06579", "submitter": "Zhuo Li", "authors": "Zhuo Li, Xu Zhou, Junruo Gao, Yifang Qin", "title": "SDN Controller Load Balancing Based on Reinforcement Learning", "comments": null, "journal-ref": "2018 IEEE 9th International Conference on Software Engineering and\n  Service Science (ICSESS)", "doi": "10.1109/ICSESS.2018.8663757", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at the local overload of multi-controller deployment in\nsoftware-defined networks, a load balancing mechanism of SDN controller based\non reinforcement learning is designed. The initial paired migrate-out domain\nand migrate-in domain are obtained by calculating the load ratio deviation\nbetween the controllers, a preliminary migration triplet, contains migration\ndomain mentioned above and a group of switches which are subordinated to the\nmigrate-out domain, makes the migration efficiency reach the local optimum.\nUnder the constraint of the best efficiency of migration in the whole and\nwithout migration conflict, selecting multiple sets of triples based on\nreinforcement learning, as the final migration of this round to attain the\nglobal optimal controller load balancing with minimum cost. The experimental\nresults illustrate that the mechanism can make full use of the controllers'\nresources, quickly balance the load between controllers, reduce unnecessary\nmigration overhead and get a faster response rate of the packet-in request.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:08:36 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Li", "Zhuo", ""], ["Zhou", "Xu", ""], ["Gao", "Junruo", ""], ["Qin", "Yifang", ""]]}, {"id": "2103.06611", "submitter": "Zhuo Li", "authors": "Zhuo Li, Xu Zhou, Taixin Li, Yang Liu", "title": "An Optimal-Transport-Based Reinforcement Learning Approach for\n  Computation Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the mass deployment of computing-intensive applications and\ndelay-sensitive applications on end devices, only adequate computing resources\ncan meet differentiated services' delay requirements. By offloading tasks to\ncloud servers or edge servers, computation offloading can alleviate computing\nand storage limitations and reduce delay and energy consumption. However, few\nof the existing offloading schemes take into consideration the cloud-edge\ncollaboration and the constraint of energy consumption and task dependency.\nThis paper builds a collaborative computation offloading model in cloud and\nedge computing and formulates a multi-objective optimization problem.\nConstructed by fusing optimal transport and Policy-Based RL, we propose an\nOptimal-Transport-Based RL approach to resolve the offloading problem and make\nthe optimal offloading decision for minimizing the overall cost of delay and\nenergy consumption. Simulation results show that the proposed approach can\neffectively reduce the cost and significantly outperforms existing optimization\nsolutions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:23:37 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Li", "Zhuo", ""], ["Zhou", "Xu", ""], ["Li", "Taixin", ""], ["Liu", "Yang", ""]]}, {"id": "2103.07151", "submitter": "Zhenyu Kang", "authors": "Changsheng You, Zhenyu Kang, Yong Zeng, and Rui Zhang", "title": "Enabling Smart Reflection in Integrated Air-Ground Wireless Network: IRS\n  Meets UAV", "comments": "In this article, we propose new methods to jointly apply IRS and UAV\n  in integrated air-ground wireless networks by exploiting their complementary\n  advantages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI eess.SP math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent reflecting surface (IRS) and unmanned aerial vehicle (UAV) have\nemerged as two promising technologies to boost the performance of wireless\ncommunication networks, by proactively altering the wireless communication\nchannels via smart signal reflection and maneuver control, respectively.\nHowever, they face different limitations in practice, which restrain their\nfuture applications. In this article, we propose new methods to jointly apply\nIRS and UAV in integrated air-ground wireless networks by exploiting their\ncomplementary advantages. Specifically, terrestrial IRS is used to enhance the\nUAV-ground communication performance, while UAV-mounted IRS is employed to\nassist in the terrestrial communication. We present their promising application\nscenarios, new communication design issues as well as potential solutions. In\nparticular, we show that it is practically beneficial to deploy both the\nterrestrial and aerial IRSs in future wireless networks to reap the benefits of\nsmart reflections in three-dimensional (3D) space.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:56:21 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 08:09:13 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["You", "Changsheng", ""], ["Kang", "Zhenyu", ""], ["Zeng", "Yong", ""], ["Zhang", "Rui", ""]]}, {"id": "2103.07415", "submitter": "Pawel Kulakowski", "authors": "Pawel Kulakowski, Kenan Turbic, Luis M. Correia", "title": "From Nano-Communications to Body Area Networks: A Perspective on Truly\n  Personal Communications", "comments": null, "journal-ref": "IEEE Access, vol. 8, pp. 159839-159853, 2020", "doi": "10.1109/ACCESS.2020.3015825", "report-no": null, "categories": "cs.NI q-bio.MN", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents an overview of future truly personal communications,\nranging from networking inside the human body to the exchange of data with\nexternal wireless devices in the surrounding environment. At the nano- and\nmicro-scales, communications can be realized with the aid of molecular\nmechanisms, Forster resonance energy transfer phenomenon, electromagnetic or\nultrasound waves. At a larger scale, in the domain of Body Area Networks, a\nwide range of communication mechanisms is available, including smart-textiles,\ninductive- and body-couplings, ultrasounds, optical and wireless radio\ntransmissions, a number of mature technologies existing already. The main goal\nof this article is to identify the potential mechanisms that can be exploited\nto provide interfaces in between nano- and micro-scale systems and Body Area\nNetworks. These interfaces have to bridge the existing gap between the two\nworlds, in order to allow for truly personal communication systems to become a\nreality. The extraordinary applications of such systems are also discussed, as\nthey are strong drivers of the research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:22:21 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kulakowski", "Pawel", ""], ["Turbic", "Kenan", ""], ["Correia", "Luis M.", ""]]}, {"id": "2103.07428", "submitter": "Giovanni Iacca Dr.", "authors": "Michela Lorandi, Leonardo Lucio Custode, Giovanni Iacca", "title": "Genetic Improvement of Routing Protocols for Delay Tolerant Networks", "comments": "To be published in ACM Transactions on Evolutionary Learning and\n  Optimization", "journal-ref": null, "doi": "10.1145/3453683", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Routing plays a fundamental role in network applications, but it is\nespecially challenging in Delay Tolerant Networks (DTNs). These are a kind of\nmobile ad hoc networks made of e.g. (possibly, unmanned) vehicles and humans\nwhere, despite a lack of continuous connectivity, data must be transmitted\nwhile the network conditions change due to the nodes' mobility. In these\ncontexts, routing is NP-hard and is usually solved by heuristic \"store and\nforward\" replication-based approaches, where multiple copies of the same\nmessage are moved and stored across nodes in the hope that at least one will\nreach its destination. Still, the existing routing protocols produce relatively\nlow delivery probabilities. Here, we genetically improve two routing protocols\nwidely adopted in DTNs, namely Epidemic and PRoPHET, in the attempt to optimize\ntheir delivery probability. First, we dissect them into their fundamental\ncomponents, i.e., functionalities such as checking if a node can transfer data,\nor sending messages to all connections. Then, we apply Genetic Improvement (GI)\nto manipulate these components as terminal nodes of evolving trees. We apply\nthis methodology, in silico, to six test cases of urban networks made of\nhundreds of nodes, and find that GI produces consistent gains in delivery\nprobability in four cases. We then verify if this improvement entails a\nworsening of other relevant network metrics, such as latency and buffer time.\nFinally, we compare the logics of the best evolved protocols with those of the\nbaseline protocols, and we discuss the generalizability of the results across\ntest cases.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:46:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lorandi", "Michela", ""], ["Custode", "Leonardo Lucio", ""], ["Iacca", "Giovanni", ""]]}, {"id": "2103.07683", "submitter": "Shi Zhou Dr.", "authors": "Jie Li, Shi Zhou, Vasileios Giotsas", "title": "Performance Analysis of Multipath BGP", "comments": "IEEE Global Internet (GI) Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipath BGP (M-BGP) allows a BGP router to install multiple 'equally-good'\npaths, via parallel inter-domain border links, to a destination prefix. M-BGP\ndiffers from the multipath routing techniques in many ways, e.g. M-BGP is only\nimplemented at border routers of Autonomous Systems (ASes); and while it shares\ntraffic to different IP addresses in a destination prefix via different border\nlinks, any traffic to a given destination IP always follows the same border\nlink. Recently we studied Looking Glass data and reported the wide deployment\nof M-BGP in the Internet; in particular, Hurricane Electric (AS6939) has\nimplemented over 1,000 cases of M-BGP to hundreds of its peering ASes.\n  In this paper, we analyzed the performance of M-BGP. We used RIPE Atlas to\nsend traceroute probes to a series of destination prefixes through Hurricane\nElectric's border routers implemented with M-BGP. We examined the distribution\nof Round Trip Time to each probed IP address in a destination prefix and their\nvariation during the measurement. We observed that the deployment of M-BGP can\nguarantee stable routing between ASes and enhance a network's resilience to\ntraffic changes. Our work provides insights into the unique characteristics of\nM-BGP as an effective technique for load balancing.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:47:50 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 14:59:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Li", "Jie", ""], ["Zhou", "Shi", ""], ["Giotsas", "Vasileios", ""]]}, {"id": "2103.07750", "submitter": "Thomas Luinaud", "authors": "Thomas Luinaud, Jeferson Santiago da Silva, J.M. Pierre Langlois, Yvon\n  Savaria", "title": "Design Principles for Packet Deparsers on FPGAs", "comments": "Presented at ISFPGA'21, 2021 Source code available at :\n  https://github.com/luinaudt/deparser/tree/FPGA_paper", "journal-ref": null, "doi": "10.1145/3431920.3439303", "report-no": null, "categories": "cs.AR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The P4 language has drastically changed the networking field as it allows to\nquickly describe and implement new networking applications. Although a large\nvariety of applications can be described with the P4 language, current\nprogrammable switch architectures impose significant constraints on P4\nprograms. To address this shortcoming, FPGAs have been explored as potential\ntargets for P4 applications. P4 applications are described using three\nabstractions: a packet parser, match-action tables, and a packet deparser,\nwhich reassembles the output packet with the result of the match-action tables.\nWhile implementations of packet parsers and match-action tables on FPGAs have\nbeen widely covered in the literature, no general design principles have been\npresented for the packet deparser. Indeed, implementing a high-speed and\nefficient deparser on FPGAs remains an open issue because it requires a large\namount of interconnections and the architecture must be tailored to a P4\nprogram. As a result, in several works where a P4 application is implemented on\nFPGAs, the deparser consumes a significant proportion of chip resources. Hence,\nin this paper, we address this issue by presenting design principles for\nefficient and high-speed deparsers on FPGAs. As an artifact, we introduce a\ntool that generates an efficient vendor-agnostic deparser architecture from a\nP4 program. Our design has been validated and simulated with a cocotb-based\nframework. The resulting architecture is implemented on Xilinx Ultrascale+\nFPGAs and supports a throughput of more than 200 Gbps while reducing resource\nusage by almost 10$\\times$ compared to other solutions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 16:58:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luinaud", "Thomas", ""], ["da Silva", "Jeferson Santiago", ""], ["Langlois", "J. M. Pierre", ""], ["Savaria", "Yvon", ""]]}, {"id": "2103.07797", "submitter": "Tanya Shreedhar", "authors": "Tanya Shreedhar, Sanjit K. Kaul, Roy D. Yates", "title": "An Empirical Study of Ageing in the Cloud", "comments": "Accepted at IEEE INFOCOM Age of Information Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify, over inter-continental paths, the ageing of TCP packets,\nthroughput and delay for different TCP congestion control algorithms containing\na mix of loss-based, delay-based and hybrid congestion control algorithms. In\ncomparing these TCP variants to ACP+, an improvement over ACP, we shed better\nlight on the ability of ACP+ to deliver timely updates over fat pipes and long\npaths. ACP+ estimates the network conditions on the end-to-end path and adapts\nthe rate of status updates to minimize age. It achieves similar average age as\nthe best (age wise) performing TCP algorithm but at end-to-end throughputs that\nare two orders of magnitude smaller. We also quantify the significant\nimprovements that ACP+ brings to age control over a shared multiaccess channel.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 21:42:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shreedhar", "Tanya", ""], ["Kaul", "Sanjit K.", ""], ["Yates", "Roy D.", ""]]}, {"id": "2103.07811", "submitter": "Laha Ale", "authors": "Laha Ale, Ning Zhang, Xiaojie Fang, Xianfu Chen, Shaohua Wu,\n  Longzhuang Li", "title": "Delay-aware and Energy-Efficient Computation Offloading in Mobile Edge\n  Computing Using Deep Reinforcement Learning", "comments": null, "journal-ref": "TCNN 2021 1-12", "doi": "10.1109/TCCN.2021.3066619", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is considered as the enabling platform for a variety\nof promising applications, such as smart transportation and smart city, where\nmassive devices are interconnected for data collection and processing. These\nIoT applications pose a high demand on storage and computing capacity, while\nthe IoT devices are usually resource-constrained. As a potential solution,\nmobile edge computing (MEC) deploys cloud resources in the proximity of IoT\ndevices so that their requests can be better served locally. In this work, we\ninvestigate computation offloading in a dynamic MEC system with multiple edge\nservers, where computational tasks with various requirements are dynamically\ngenerated by IoT devices and offloaded to MEC servers in a time-varying\noperating environment (e.g., channel condition changes over time). The\nobjective of this work is to maximize the completed tasks before their\nrespective deadlines and minimize energy consumption. To this end, we propose\nan end-to-end Deep Reinforcement Learning (DRL) approach to select the best\nedge server for offloading and allocate the optimal computational resource such\nthat the expected long-term utility is maximized. The simulation results are\nprovided to demonstrate that the proposed approach outperforms the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 23:10:40 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ale", "Laha", ""], ["Zhang", "Ning", ""], ["Fang", "Xiaojie", ""], ["Chen", "Xianfu", ""], ["Wu", "Shaohua", ""], ["Li", "Longzhuang", ""]]}, {"id": "2103.07814", "submitter": "Laha Ale", "authors": "Laha Ale, Ning Zhang, Scott A. King, Jose Guardiola", "title": "Spatio-Temporal Bayesian Learning for Mobile Edge Computing Resource\n  Planning in Smart Cities", "comments": null, "journal-ref": null, "doi": "10.1145/3448613", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A smart city improves operational efficiency and comfort of living by\nharnessing techniques such as the Internet of Things (IoT) to collect and\nprocess data for decision making. To better support smart cities, data\ncollected by IoT should be stored and processed appropriately. However, IoT\ndevices are often task-specialized and resource-constrained, and thus, they\nheavily rely on online resources in terms of computing and storage to\naccomplish various tasks. Moreover, these cloud-based solutions often\ncentralize the resources and are far away from the end IoTs and cannot respond\nto users in time due to network congestion when massive numbers of tasks\noffload through the core network. Therefore, by decentralizing resources\nspatially close to IoT devices, mobile edge computing (MEC) can reduce latency\nand improve service quality for a smart city, where service requests can be\nfulfilled in proximity. As the service demands exhibit spatial-temporal\nfeatures, deploying MEC servers at optimal locations and allocating MEC\nresources play an essential role in efficiently meeting service requirements in\na smart city. In this regard, it is essential to learn the distribution of\nresource demands in time and space. In this work, we first propose a\nspatio-temporal Bayesian hierarchical learning approach to learn and predict\nthe distribution of MEC resource demand over space and time to facilitate MEC\ndeployment and resource management. Second, the proposed model is trained and\ntested on real-world data, and the results demonstrate that the proposed method\ncan achieve very high accuracy. Third, we demonstrate an application of the\nproposed method by simulating task offloading. Finally, the simulated results\nshow that resources allocated based upon our models' predictions are exploited\nmore efficiently than the resources are equally divided into all servers in\nunobserved areas.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 23:23:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ale", "Laha", ""], ["Zhang", "Ning", ""], ["King", "Scott A.", ""], ["Guardiola", "Jose", ""]]}, {"id": "2103.08181", "submitter": "Weiheng Jiang", "authors": "Weiheng Jiang and Wanxin Yu", "title": "Multi-Agent Reinforcement Learning based Joint Cooperative Spectrum\n  Sensing and Channel Access for Cognitive UAV Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing clustered unmanned aerial vehicle (UAV) communication networks\nbased on cognitive radio (CR) and reinforcement learning can significantly\nimprove the intelligence level of clustered UAV communication networks and the\nrobustness of the system in a time-varying environment. Among them, designing\nsmarter systems for spectrum sensing and access is a key research issue in CR.\nTherefore, we focus on the dynamic cooperative spectrum sensing and channel\naccess in clustered cognitive UAV (CUAV) communication networks. Due to the\nlack of prior statistical information on the primary user (PU) channel\noccupancy state, we propose to use multi-agent reinforcement learning (MARL) to\nmodel CUAV spectrum competition and cooperative decision-making problem in this\ndynamic scenario, and a return function based on the weighted compound of\nsensing-transmission cost and utility is introduced to characterize the\nreal-time rewards of multi-agent game. On this basis, a time slot multi-round\nrevisit exhaustive search algorithm based on virtual controller (VC-EXH), a\nQ-learning algorithm based on independent learner (IL-Q) and a deep Q-learning\nalgorithm based on independent learner (IL-DQN) are respectively proposed.\nFurther, the information exchange overhead, execution complexity and\nconvergence of the three algorithms are briefly analyzed. Through the numerical\nsimulation analysis, all three algorithms can converge quickly, significantly\nimprove system performance and increase the utilization of idle spectrum\nresources.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:39:32 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Jiang", "Weiheng", ""], ["Yu", "Wanxin", ""]]}, {"id": "2103.08234", "submitter": "Ertan Onur", "authors": "Berke Tezergil and Ertan Onur", "title": "Wireless Backhaul in 5G and Beyond: Issues, Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the introduction of new technologies such as Unmanned Aerial Vehicle\n(UAV), High Altitude Platform Station (HAPS), Millimeter Wave (mmWave)\nfrequencies, Massive Multiple-Input Multiple-Output (mMIMO), and beamforming,\nwireless backhaul is expected to be an integral part of the 5G networks. While\nthis concept is nothing new, it was shortcoming in terms of performance\ncompared to the fiber backhauling. However, with these new technologies, fiber\nis no longer the foremost technology for backhauling. With the projected\ndensification of networks, wireless backhaul has become mandatory to use. There\nare still challenges to be tackled if wireless backhaul is to be used\nefficiently. Resource allocation, deployment, scheduling, power management and\nenergy efficiency are some of these problems. Wireless backhaul also acts as an\nenabler for new technologies and improves some of the existing ones\nsignificantly. To name a few, rural connectivity, satellite communication, and\nmobile edge computing are some concepts for which wireless backhauling acts as\nan enabler. Small cell usage with wireless backhaul presents different security\nchallenges. Governing bodies of cellular networks have standardization efforts\ngoing on especially for the Integrated Acces-Backhaul (IAB) concept, and this\nis briefly mentioned. Finally, wireless backhaul is also projected to be an\nimportant part of the beyond 5G networks, and newly developed concepts such as\ncell-free networking, ultra-massive MIMO, and extremely dense network show this\ntrend as well. In this survey, we present the aforementioned issues,\nchallenges, opportunities, and applications of wireless backhaul in 5G, while\nbriefly mentioning concepts related to wireless backhaul beyond 5G alongside\nwith security and standardization issues.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:34:41 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 05:02:25 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Tezergil", "Berke", ""], ["Onur", "Ertan", ""]]}, {"id": "2103.08247", "submitter": "Pawel Kulakowski", "authors": "Jakub Kmiecik, Pawel Kulakowski, Krzysztof Wojcik, Andrzej Jajszczyk", "title": "Transmitting FRET signals to nerve cells", "comments": null, "journal-ref": "5th ACM/IEEE International Conference on Nanoscale Computing and\n  Communication, September 2018", "doi": "10.1145/3233188.3233223", "report-no": null, "categories": "q-bio.MN cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a novel method allowing communication between\nFRET nanonetworks and nerve cells. It is focused on two system components:\nfluorophores and channelrhodopsins which serve as transmitters and receivers,\nrespectively. Channelrhodopsins are used here also as a FRET signal-to-voltage\nconverter. The trade-off between throughput and bit error rate is also\ninvestigated.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:57:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kmiecik", "Jakub", ""], ["Kulakowski", "Pawel", ""], ["Wojcik", "Krzysztof", ""], ["Jajszczyk", "Andrzej", ""]]}, {"id": "2103.08308", "submitter": "Hui Zhou", "authors": "Hui Zhou, Changyang She, Yansha Deng, Mischa Dohler, and Arumugam\n  Nallanathan", "title": "Machine Learning for Massive Industrial Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Internet of Things (IIoT) revolutionizes the future manufacturing\nfacilities by integrating the Internet of Things technologies into industrial\nsettings. With the deployment of massive IIoT devices, it is difficult for the\nwireless network to support the ubiquitous connections with diverse\nquality-of-service (QoS) requirements. Although machine learning is regarded as\na powerful data-driven tool to optimize wireless network, how to apply machine\nlearning to deal with the massive IIoT problems with unique characteristics\nremains unsolved. In this paper, we first summarize the QoS requirements of the\ntypical massive non-critical and critical IIoT use cases. We then identify\nunique characteristics in the massive IIoT scenario, and the corresponding\nmachine learning solutions with its limitations and potential research\ndirections. We further present the existing machine learning solutions for\nindividual layer and cross-layer problems in massive IIoT. Last but not the\nleast, we present a case study of massive access problem based on deep neural\nnetwork and deep reinforcement learning techniques, respectively, to validate\nthe effectiveness of machine learning in massive IIoT scenario.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:10:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhou", "Hui", ""], ["She", "Changyang", ""], ["Deng", "Yansha", ""], ["Dohler", "Mischa", ""], ["Nallanathan", "Arumugam", ""]]}, {"id": "2103.08408", "submitter": "Mital Raithatha", "authors": "Mital Raithatha, Aizaz U. Chaudhry, Roshdy H.M. Hafez, John W.\n  Chinneck", "title": "A Fast Heuristic for Gateway Location in Wireless Backhaul of 5G\n  Ultra-Dense Networks", "comments": "Accepted Journal paper in IEEE access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 5G Ultra-Dense Networks, a distributed wireless backhaul is an attractive\nsolution for forwarding traffic to the core. The macro-cell coverage area is\ndivided into many small cells. A few of these cells are designated as gateways\nand are linked to the core by high-capacity fiber optic links. Each small cell\nis associated with one gateway and all small cells forward their traffic to\ntheir respective gateway through multi-hop mesh networks. We investigate the\ngateway location problem and show that finding near-optimal gateway locations\nimproves the backhaul network capacity. An exact p-median integer linear\nprogram is formulated for comparison with our novel K-GA heuristic that\ncombines a Genetic Algorithm (GA) with K-means clustering to find near-optimal\ngateway locations. We compare the performance of KGA with six other approaches\nin terms of average number of hops and backhaul network capacity at different\nnode densities through extensive Monte Carlo simulations. All approaches are\ntested in various user distribution scenarios, including uniform distribution,\nbivariate Gaussian distribution, and cluster distribution. In all cases K-GA\nprovides near-optimal results, achieving average number of hops and backhaul\nnetwork capacity within 2% of optimal while saving an average of 95% of the\nexecution time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:34:49 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Raithatha", "Mital", ""], ["Chaudhry", "Aizaz U.", ""], ["Hafez", "Roshdy H. M.", ""], ["Chinneck", "John W.", ""]]}, {"id": "2103.08440", "submitter": "Johannes Krupp", "authors": "Johannes Krupp, Christian Rossow", "title": "BGPeek-a-Boo: Active BGP-based Traceback for Amplification DDoS Attacks", "comments": "6th IEEE European Symposium on Security and Privacy (EuroS&P) 2021 ;\n  \\copyright 2021 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amplification DDoS attacks inherently rely on IP spoofing to steer attack\ntraffic to the victim. At the same time, IP spoofing undermines prosecution, as\nthe originating attack infrastructure remains hidden. Researchers have\ntherefore proposed various mechanisms to trace back amplification attacks (or\nIP-spoofed attacks in general). However, existing traceback techniques require\neither the cooperation of external parties or a priori knowledge about the\nattacker. We propose BGPeek-a-Boo, a BGP-based approach to trace back\namplification attacks to their origin network. BGPeek-a-Boo monitors\namplification attacks with honeypots and uses BGP poisoning to temporarily shut\ndown ingress traffic from selected Autonomous Systems. By systematically\nprobing the entire AS space, we detect systems forwarding and originating\nspoofed traffic. We then show how a graph-based model of BGP route propagation\ncan reduce the search space, resulting in a 5x median speed-up and over 20x for\n1/4 of all cases. BGPeek-a-Boo achieves a unique traceback result 60% of the\ntime in a simulation-based evaluation supported by real-world experiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:10:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Krupp", "Johannes", ""], ["Rossow", "Christian", ""]]}, {"id": "2103.08735", "submitter": "Nariman Torkzaban", "authors": "Nariman Torkzaban, and John S. Baras", "title": "Joint Satellite Gateway Deployment & Controller Placement in\n  Software-Defined 5G-Satellite Integrated Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several challenging optimization problems arise while considering the\ndeployment of the space-air-ground integrated networks (SAGINs), among which\nthe optimal satellite gateway deployment problem is of significant importance.\nMoreover, with the increasing interest in the software-defined integration of\n5G networks and satellites, the existence of an effective scheme for optimal\nplacement of SDN controllers is essential. In this paper, we discuss the\ninterrelation between the two problems above and propose suitable methods to\nsolve them under various network design criteria. We first provide a MILP model\nfor solving the joint problem, and then motivate the decomposition of the model\ninto two disjoint MILPs. We then show that the resulting problems can be\nmodeled as the optimization of submodular set functions and can be solved\nefficiently with provable optimality gaps.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:47:07 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 21:42:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Torkzaban", "Nariman", ""], ["Baras", "John S.", ""]]}, {"id": "2103.08836", "submitter": "Samith Abeywickrama", "authors": "Samith Abeywickrama, Changsheng You, Rui Zhang, and Chau Yuen", "title": "Channel Estimation for Intelligent Reflecting Surface Assisted\n  Backscatter Communication", "comments": "A new and efficient channel estimation scheme has been proposed for\n  the IRS-assisted backscatter communication system", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent reflecting surface (IRS) is a promising technology to improve the\nperformance of backscatter communication systems by smartly reconfiguring the\nmulti-reflection channel. To fully exploit the passive beamforming gain of IRS\nin backscatter communication, channel state information (CSI) is indispensable\nbut more practically challenging to acquire than conventional IRS-assisted\nsystems, since IRS passively reflects signals over both the forward and\nbackward (backscattering) links between the reader and tag. To address this\nissue, we propose in this letter a new and efficient channel estimation scheme\nfor the IRS-assisted backscatter communication system. To minimize the\nmean-square error (MSE) of channel estimation, we formulate and solve an\noptimization problem by designing the IRS training reflection matrix for\nchannel estimation under the constraints of unit-modulus elements and full\nrank. Simulation results verify the effectiveness of the proposed channel\nestimation scheme as compared to other baseline schemes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:42:41 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Abeywickrama", "Samith", ""], ["You", "Changsheng", ""], ["Zhang", "Rui", ""], ["Yuen", "Chau", ""]]}, {"id": "2103.08856", "submitter": "Hussain Ahmad Mr.", "authors": "Hussain Ahmad, Muhammad Zubair Islam, Amir Haider, Rashid Ali, Hyung\n  Seok Kim", "title": "Intelligent Stretch Reduction in Information-CentricNetworking towards\n  5G-Tactile Internet realization", "comments": "6 pages, 4 Figures, 1 Table, International Conference on Green and\n  Human Information Technology (ICGHIT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, 5G is widely used in parallel with IoT networks to enable\nmassive data connectivity and exchange with ultra-reliable and low latency\ncommunication (URLLC) services. The internet requirements from user's\nperspective have shifted from simple human to human interactions to different\ncommunication paradigms and information-centric networking (ICN). ICN\ndistributes the content among the users based on their trending requests. ICN\nis responsible not only for the routing and caching but also for naming the\nnetwork's content. ICN considers several parameters such as cache-hit ratio,\ncontent diversity, content redundancy, and stretch to route the content. ICN\nenables name-based caching of the required content according to the user's\nrequest based on the router's interest table. The stretch shows the path\ncovered while retrieving the content from producer to consumer. Reduction in\npath length also leads to a reduction in end-to-end latency and better data\nrate availability. ICN routers must have the minimum stretch to obtain a better\nsystem efficiency. Reinforcement learning (RL) is widely used in networks\nenvironment to increase agent efficiency to make decisions. In ICN, RL can aid\nto increase caching and stretch efficiency. This paper investigates a stretch\nreduction strategy for ICN routers by formulating the stretch reduction problem\nas a Markov decision process. The evaluation of the proposed stretch reduction\nstrategy's accuracy is done by employing Q-Learning, an RL technique. The\nsimulation results indicate that by using the optimal parameters for the\nproposed stretch reduction strategy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 05:28:44 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ahmad", "Hussain", ""], ["Islam", "Muhammad Zubair", ""], ["Haider", "Amir", ""], ["Ali", "Rashid", ""], ["Kim", "Hyung Seok", ""]]}, {"id": "2103.08875", "submitter": "Asif Ahmed Sardar", "authors": "Asif Ahmed Sardar, Dibbendu Roy, Washim Uddin Mondal and Goutam Das", "title": "Queuing Analysis of Opportunistic Cognitive Radio IoT Network with\n  Imperfect Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze a Cognitive Radio-based Internet-of-Things (CR-IoT)\nnetwork comprising a Primary Network Provider (PNP) and an IoT operator. The\nPNP uses its licensed spectrum to serve its users. The IoT operator identifies\nthe white-space in the licensed band at regular intervals and opportunistically\nexploits them to serve the IoT nodes under its coverage. IoT nodes are\nbattery-operated devices that require periodical energy replenishment. We\nemploy the Microwave Power Transfer (MPT) technique for its superior energy\ntransfer efficiency over long-distance. The white-space detection process is\nnot always perfect and the IoT operator may jeopardize the PNP's transmissions\ndue to misdetection. To reduce the possibility of such interferences, some of\nthe spectrum holes must remain unutilized, even when the IoT nodes have data to\ntransmit. The IoT operator needs to decide what percentage of the white-space\nto keep unutilized and how to judiciously use the rest for data transmission\nand energy-replenishment to maintain an optimal balance between the average\ninterference inflicted on PNP's users and the Quality-of-Service (QoS)\nexperienced by IoT nodes. Due to the periodic nature of the spectrum-sensing\nprocess, Discrete Time Markov Chain (DTMC) method can realistically model this\nframework. In literature, activities of the PNP and IoT operator are assumed to\nbe mutually exclusive, for ease of analysis. Our model incorporates possible\noverlaps between these activities, making the analysis more realistic. Using\nour model, the sustainability region of the CR-IoT network can be obtained. The\naccuracy of our analysis is demonstrated via extensive simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:50:29 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sardar", "Asif Ahmed", ""], ["Roy", "Dibbendu", ""], ["Mondal", "Washim Uddin", ""], ["Das", "Goutam", ""]]}, {"id": "2103.09156", "submitter": "Xingqin Lin", "authors": "Xingqin Lin, Stefan Rommer, Sebastian Euler, Emre A. Yavuz and Robert\n  S. Karlsson", "title": "5G from Space: An Overview of 3GPP Non-Terrestrial Networks", "comments": "8 pages, 5 figures, 1 table, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of the 3rd generation partnership project (3GPP) work\non evolving the 5G wireless technology to support non-terrestrial satellite\nnetworks. Adapting 5G to support non-terrestrial networks entails a holistic\ndesign spanning across multiple areas from radio access network to services and\nsystem aspects to core and terminals. In this article, we describe the main\ntopics of non-terrestrial networks, explain in detail the design aspects, and\nshare various design rationales influencing standardization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:53:39 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 13:35:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Lin", "Xingqin", ""], ["Rommer", "Stefan", ""], ["Euler", "Sebastian", ""], ["Yavuz", "Emre A.", ""], ["Karlsson", "Robert S.", ""]]}, {"id": "2103.09169", "submitter": "Rohit Verma", "authors": "Rohit Verma, Justas Brazauskas, Vadim Safronov, Matthew Danish, Jorge\n  Merino, Xiang Xie, Ian Lewis, Richard Mortier", "title": "SenseRT: A Streaming Architecture for Smart Building Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building Management Systems (BMSs) have evolved in recent years, in ways that\nrequire changes to existing network architectures that follow the\nstore-then-analyse approach. The primary cause is the increasing deployment of\na diverse range of cost-effective sensors and actuators in smart buildings that\ngenerate real-time streaming data. Any in-building system with a large number\nof sensors needs a framework for real-time data collection and concurrent\nstream processing from sensors connected using a range of networks.\n  We present SenseRT, a system for managing and analysing in-building real-time\nstreams of sensor data. SenseRT collects streams of real-time data from sensors\nconnected using a range of network protocols. It supports concurrent modules\nsimultaneously performing stream processing over real-time data, asynchronously\nand non-blocking, with results made available with minimal latency. We describe\na prototype implementation deployed in two University department buildings,\ndemonstrating its effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:11:50 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Verma", "Rohit", ""], ["Brazauskas", "Justas", ""], ["Safronov", "Vadim", ""], ["Danish", "Matthew", ""], ["Merino", "Jorge", ""], ["Xie", "Xiang", ""], ["Lewis", "Ian", ""], ["Mortier", "Richard", ""]]}, {"id": "2103.09323", "submitter": "Kezhi Wang", "authors": "Hong Ren, Kezhi Wang, Cunhua Pan", "title": "Intelligent Reflecting Surface-aided URLLC in a Factory Automation\n  Scenario", "comments": "Submitted for possible journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from conventional wired line connections, industrial control\nthrough wireless transmission is widely regarded as a promising solution due to\nits reduced cost, increased long-term reliability, and enhanced reliability.\nHowever, mission-critical applications impose stringent quality of service\n(QoS) requirements that entail ultra-reliability low-latency communications\n(URLLC). The primary feature of URLLC is that the blocklength of channel codes\nis short, and the conventional Shannon's Capacity is not applicable. In this\npaper, we consider the URLLC in a factory automation (FA) scenario. Due to\ndensely deployed equipment in FA, wireless signal are easily blocked by the\nobstacles. To address this issue, we propose to deploy intelligent reflecting\nsurface (IRS) to create an alternative transmission link when the direct link\nis blocked, which can enhance the transmission reliability. In this paper, we\nfocus on the performance analysis for IRS-aided URLLC-enabled communications in\na FA scenario, where the direct link is blocked. Both the average data rate\n(ADR) and the average decoding error probability (ADEP) are derived under the\ncase with perfect channel state information (CSI) and the case without CSI.\nAsymptotic analysis is performed to obtain more design insights. Extensive\nnumerical results are provided to verify the accuracy of our derived results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 21:00:15 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 23:28:47 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ren", "Hong", ""], ["Wang", "Kezhi", ""], ["Pan", "Cunhua", ""]]}, {"id": "2103.09355", "submitter": "Manoj R. Rege", "authors": "Manoj R. Rege, Vlado Handziski, Adam Wolisz", "title": "Generation of Realistic Cloud Access Times for Mobile Application\n  Testing using Transfer Learning", "comments": "Accepted by Elsevier Computer Communications, 24 pages, 14 figures,\n  corrected typos, figures in grayscale", "journal-ref": null, "doi": "10.1016/j.comcom.2021.03.010", "report-no": null, "categories": "cs.NI cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The network Quality of Service (QoS) metrics such as the access time, the\nbandwidth, and the packet loss play an important role in determining the\nQuality of Experience (QoE) of mobile applications. Various factors like the\nRadio Resource Control (RRC) states, the Mobile Network Operator (MNO) specific\nretransmission configurations, handovers triggered by the user mobility, the\nnetwork load, etc. can cause high variability in these QoS metrics on 4G/LTE,\nand WiFi networks, which can be detrimental to the application QoE. Therefore,\nexposing the mobile application to realistic network QoS metrics is critical\nfor a tester attempting to predict its QoE. A viable approach is testing using\nsynthetic traces. The main challenge in the generation of realistic synthetic\ntraces is the diversity of environments and the lack of wide scope of real\ntraces to calibrate the generators. In this paper, we describe a\nmeasurement-driven methodology based on transfer learning with Long Short Term\nMemory (LSTM) neural nets to solve this problem. The methodology requires a\nrelatively short sample of the targeted environment to adapt the presented\nbasic model to new environments, thus simplifying synthetic traces generation.\nWe present this feature for realistic WiFi and LTE cloud access time models\nadapted for diverse target environments with a trace size of just 6000 samples\nmeasured over a few tens of minutes. We demonstrate that synthetic traces\ngenerated from these models are capable of accurately reproducing application\nQoE metric distributions including their outlier values.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 22:42:34 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 21:26:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Rege", "Manoj R.", ""], ["Handziski", "Vlado", ""], ["Wolisz", "Adam", ""]]}, {"id": "2103.09532", "submitter": "Peng Yang", "authors": "Peng Yang, Xing Xi, Tony Q. S. Quek, and Hyundong Shin", "title": "CoMP-Enabled RAN Slicing for Tactile Internet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tactile Internet (TI) enables the omnipresence and exchange of tactile\nexperiences across the global via the ultra-reliable and ultra-responsive\nconnectivity. This article argues for coordinated multi-point (CoMP) enabled\nradio access network (RAN) slicing as an efficient solution that satisfies the\nstringent reliable and responsive connectivity requirements for supporting\ntactile applications. This article presents the emerging challenges when\naccommodating CoMP-enabled RAN slicing in the TI ecosystem and expounds the\nfunctional split of CoMP-enabled RAN. Besides, this article elaborates on the\nimplementation prototype of CoMP-enabled RAN slicing for TI with the\ncoexistence of diverse vertical applications. Finally, this article studies a\nuse case of enabling TI-included application multiplexing as an example of\nCoMP-enabled RAN slicing for TI.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:38:20 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 07:25:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yang", "Peng", ""], ["Xi", "Xing", ""], ["Quek", "Tony Q. S.", ""], ["Shin", "Hyundong", ""]]}, {"id": "2103.09639", "submitter": "Reza Fotohi", "authors": "Shahram Jamali, Mir Mahmoud Talebi, Reza Fotohi", "title": "Congestion control in high-speed networks using the probabilistic\n  estimation approach", "comments": "19 pages, 14 Figures, 2 Tables, Int J Commun Syst", "journal-ref": "Int J Commun Syst. 2021;e4766", "doi": "10.1002/dac.4766", "report-no": null, "categories": "cs.NI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the bulk of Internet traffic uses TCP protocol for reliable\ntransmission. But the standard TCP's performance is very poor in High Speed\nNetworks (HSN) and hence the core gigabytes links are usually underutilization.\nThis problem has roots in conservative nature of TCP, especially in its\nAdditive Increase Multiplicative Decrease (AIMD) phase. In other words, since\nTCP can't figure out precisely the congestion status of the network, it follows\na conservative strategy to keep the network from overwhelming. We believe that\nprecisely congestion estimation in the network can solve this problem by\navoiding unnecessary conservation. To this end, this paper proposes an\nalgorithm which considers packet loss and delay information jointly and employs\na probabilistic approach to accurately estimation of congestion status in the\nnetwork. To examine the proposed scheme performance, extensive simulations have\nbeen performed in the NS-2 environment. Simulation results reveal that the\nproposed algorithm has better performance than existing algorithms in terms of\nbottleneck utilization, stability, throughput and fairness.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:18:20 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jamali", "Shahram", ""], ["Talebi", "Mir Mahmoud", ""], ["Fotohi", "Reza", ""]]}, {"id": "2103.09860", "submitter": "Esther Max-Onakpoya", "authors": "Oluwashina Madamori, Esther Max-Onakpoya, Gregory D. Erhardt, Corey E.\n  Baker", "title": "Enabling Opportunistic Low-cost Smart Cities By Using Tactical Edge Node\n  Placement", "comments": "Accepted in WONS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smart city projects aim to enhance the management of city infrastructure by\nenabling government entities to monitor, control and maintain infrastructure\nefficiently through the deployment of Internet-of-things (IoT) devices.\nHowever, the financial burden associated with smart city projects is a\ndetriment to prospective smart cities. A noteworthy factor that impacts the\ncost and sustainability of smart city projects is providing cellular Internet\nconnectivity to IoT devices. In response to this problem, this paper explores\nthe use of public transportation network nodes and mules, such as bus-stops as\nbuses, to facilitate connectivity via device-to-device communication in order\nto reduce cellular connectivity costs within a smart city. The data mules\nconvey non-urgent data from IoT devices to edge computing hardware, where data\ncan be processed or sent to the cloud. Consequently, this paper focuses on edge\nnode placement in smart cities that opportunistically leverage public transit\nnetworks for reducing reliance on and thus costs of cellular connectivity. We\nintroduce an algorithm that selects a set of edge nodes that provides maximal\nsensor coverage and explore another that selects a set of edge nodes that\nprovide minimal delivery delay within a budget. The algorithms are evaluated\nfor two public transit network data-sets:Chapel Hill, North Carolina and\nLouisville, Kentucky. Results show that our algorithms consistently outperform\nedge node placement strategies that rely on traditional centrality\nmetrics(betweenness and in-degree centrality) by over 77% reduction in coverage\nbudget and over 20 minutes reduction in latency.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:49:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Madamori", "Oluwashina", ""], ["Max-Onakpoya", "Esther", ""], ["Erhardt", "Gregory D.", ""], ["Baker", "Corey E.", ""]]}, {"id": "2103.09924", "submitter": "Francesca Meneghello", "authors": "Francesca Meneghello, Domenico Garlisi, Nicol\\`o Dal Fabbro, Ilenia\n  Tinnirello, Michele Rossi", "title": "Environment and Person Independent Activity Recognition with a Commodity\n  IEEE 802.11ac Access Point", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here, we propose an original approach for human activity recognition (HAR)\nwith commercial IEEE 802.11ac (WiFi) devices, which generalizes across\ndifferent persons, days and environments. To achieve this, we devise a\ntechnique to extract, clean and process the received phases from the channel\nfrequency response (CFR) of the WiFi channel, obtaining an estimate of the\nDoppler shift at the receiver of the communication link. The Doppler shift\nreveals the presence of moving scatterers in the environment, while not being\naffected by (environment specific) static objects. The proposed HAR framework\nis trained on data collected as a person performs four different activities and\nis tested on unseen setups, to assess its performance as the person, the day\nand/or the environment change with respect to those considered at training\ntime. In the worst case scenario, the proposed HAR technique reaches an average\naccuracy higher than 95%, validating the effectiveness of the extracted Doppler\ninformation, used in conjunction with a learning algorithm based on a neural\nnetwork, in recognizing human activities in a subject and environment\nindependent fashion.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:44:13 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Meneghello", "Francesca", ""], ["Garlisi", "Domenico", ""], ["Fabbro", "Nicol\u00f2 Dal", ""], ["Tinnirello", "Ilenia", ""], ["Rossi", "Michele", ""]]}, {"id": "2103.10120", "submitter": "Pawel Kulakowski", "authors": "Sebastian Canovas-Carrasco, Rafael Asorey-Cacheda, Antonio-Javier\n  Garcia-Sanchez, Joan Garcia-Haro, Krzysztof Wojcik, Pawel Kulakowski", "title": "Understanding the Applicability of Terahertz Flow-guided Nano-Networks\n  for Medical Applications", "comments": null, "journal-ref": "IEEE Access, vol. 8, pp. 214224-214239, 2020", "doi": "10.1109/ACCESS.2020.3041187", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Terahertz-based nano-networks are emerging as a groundbreaking technology\nable to play a decisive role in future medical applications owing to their\nability to precisely quantify figures, such as the viral load in a patient or\nto predict sepsis shock or heart attacks before they occur. Due to the\nextremely limited size of the devices composing these nano-networks, the use of\nthe Terahertz (THz) band has emerged as the enabling technology for their\ncommunication. However, the characteristics of the THz band, which strictly\nreduce the communication range inside the human body, together with the energy\nlimitations of nano-nodes make the in-body deployment of nano-nodes a\nchallenging task. To overcome these problems, we propose a novel in-body\nflow-guided nano-network architecture consisting of three different devices: i)\nnano-node, ii) nano-router, and iii) bio-sensor. As the performance of this\ntype of nano-network has not been previously explored, a theoretical framework\ncapturing all its particularities is derived to properly model its behavior and\nevaluate its feasibility in real medical applications. Employing this\nanalytical model, a thorough sensitivity study of its key parameters is\naccomplished. Finally, we analyze the terahertz flow-guided nano-network design\nto satisfy the requirements of several medical applications of interest.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:46:38 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Canovas-Carrasco", "Sebastian", ""], ["Asorey-Cacheda", "Rafael", ""], ["Garcia-Sanchez", "Antonio-Javier", ""], ["Garcia-Haro", "Joan", ""], ["Wojcik", "Krzysztof", ""], ["Kulakowski", "Pawel", ""]]}, {"id": "2103.10277", "submitter": "Sergio Martiradonna", "authors": "Sergio Martiradonna, Andrea Abrardo, Marco Moretti, Giuseppe Piro,\n  Gennaro Boggia", "title": "Deep Reinforcement Learning-Aided RAN Slicing Enforcement for B5G\n  Latency Sensitive Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of cloud computing capabilities at the network edge and\nartificial intelligence promise to turn future mobile networks into service-\nand radio-aware entities, able to address the requirements of upcoming\nlatency-sensitive applications. In this context, a challenging research goal is\nto exploit edge intelligence to dynamically and optimally manage the Radio\nAccess Network Slicing (that is a less mature and more complex technology than\nfifth-generation Network Slicing) and Radio Resource Management, which is a\nvery complex task due to the mostly unpredictably nature of the wireless\nchannel. This paper presents a novel architecture that leverages Deep\nReinforcement Learning at the edge of the network in order to address Radio\nAccess Network Slicing and Radio Resource Management optimization supporting\nlatency-sensitive applications. The effectiveness of our proposal against\nbaseline methodologies is investigated through computer simulation, by\nconsidering an autonomous-driving use-case.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:18:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Martiradonna", "Sergio", ""], ["Abrardo", "Andrea", ""], ["Moretti", "Marco", ""], ["Piro", "Giuseppe", ""], ["Boggia", "Gennaro", ""]]}, {"id": "2103.10561", "submitter": "Minsung Kim", "authors": "Minsung Kim, Salvatore Mandr\\`a, Davide Venturelli, Kyle Jamieson", "title": "Physics-Inspired Heuristics for Soft MIMO Detection in 5G New Radio and\n  Beyond", "comments": "ACM MobiCom '21 (https://doi.org/10.1145/3447993.3448619)", "journal-ref": null, "doi": "10.1145/3447993.3448619", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overcoming the conventional trade-off between throughput and bit error rate\n(BER) performance, versus computational complexity is a long-term challenge for\nuplink Multiple-Input Multiple-Output (MIMO) detection in base station design\nfor the cellular 5G New Radio roadmap, as well as in next generation wireless\nlocal area networks. In this work, we present ParaMax, a MIMO detector\narchitecture that for the first time brings to bear physics-inspired parallel\ntempering algorithmic techniques [28, 50, 67] on this class of problems.\nParaMax can achieve near optimal maximum-likelihood (ML) throughput performance\nin the Large MIMO regime, Massive MIMO systems where the base station has\nadditional RF chains, to approach the number of base station antennas, in order\nto support even more parallel spatial streams. ParaMax is able to achieve a\nnear ML-BER performance up to 160x160 and 80x80 Large MIMO for low-order\nmodulations such as BPSK and QPSK, respectively, only requiring less than tens\nof processing elements. With respect to Massive MIMO systems, in 12x24 MIMO\nwith 16-QAM at SNR 16 dB, ParaMax achieves 330 Mbits/s near-optimal system\nthroughput with 4--8 processing elements per subcarrier, which is approximately\n1.4x throughput than linear detector-based Massive MIMO systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 23:04:57 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kim", "Minsung", ""], ["Mandr\u00e0", "Salvatore", ""], ["Venturelli", "Davide", ""], ["Jamieson", "Kyle", ""]]}, {"id": "2103.10579", "submitter": "Wanli Xue", "authors": "Cheng Shen and Wanli Xue", "title": "An Experiment Study on Federated LearningTestbed", "comments": "Accepted by SMARTCOM2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Internet of Things (IoT) can benefit from machine learning by\noutsourcing model training on the cloud, user data exposure to an untrusted\ncloud service provider can pose threat to user privacy. Recently, federated\nlearning is proposed as an approach for privacy-preserving machine learning\n(PPML) for the IoT, while its practicability remains unclear. This work\npresents the evaluation on the efficiency and privacy performance of a readily\navailable federated learning framework based on PySyft, a Python library for\ndistributed deep learning. It is observed that the training speed of the\nframework is significantly slower than of the centralized approach due to\ncommunication overhead. Meanwhile, the framework bears some vulnerability to\npotential man-in-the-middle attacks at the network level. The report serves as\na starting point for PPML performance analysis and suggests the future\ndirection for PPML framework development.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 01:07:37 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shen", "Cheng", ""], ["Xue", "Wanli", ""]]}, {"id": "2103.10582", "submitter": "Jianqing Liu", "authors": "Jianqing Liu, Shangjia Dong, Thomas Morris", "title": "Empirical Optimization on Post-Disaster Communication Restoration for\n  Social Equality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Disasters are constant threats to humankind, and beyond losses in lives, they\ncause many implicit yet profound societal issues such as wealth disparity and\ndigital divide. Among those recovery measures in the aftermath of disasters,\nrestoring and improving communication services is of vital importance. Although\nexisting works have proposed many architectural and protocol designs, none of\nthem have taken human factors and social equality into consideration. Recent\nsociological studies have shown that people from marginalized groups (e.g.,\nminority, low income, and poor education) are more vulnerable to communication\noutages. In this work, we take pioneering efforts in integrating human factors\ninto an empirical optimization model to determine strategies for post-disaster\ncommunication restoration. We cast the design into a mix-integer non-linear\nprogramming problem, which is proven too complex to be solved. Through a suite\nof convex relaxations, we then develop heuristic algorithms to efficiently\nsolve the transformed optimization problem. Based on a collected dataset, we\nfurther evaluate and demonstrate how our design will prioritize communication\nservices for vulnerable people and promote social equality compared with an\nexisting modeling benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 01:17:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Jianqing", ""], ["Dong", "Shangjia", ""], ["Morris", "Thomas", ""]]}, {"id": "2103.10650", "submitter": "Do-Yup Kim", "authors": "Do-Yup Kim, Hamid Jafarkhani, Jang-Won Lee", "title": "Low-complexity dynamic resource scheduling for downlink MC-NOMA over\n  fading channels", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate dynamic resource scheduling (i.e., joint user,\nsubchannel, and power scheduling) for downlink multi-channel non-orthogonal\nmultiple access (MC-NOMA) systems over time-varying fading channels.\nSpecifically, we address the weighted average sum rate maximization problem\nwith quality-of-service (QoS) constraints. In particular, to facilitate fast\nresource scheduling, we focus on developing a very low-complexity algorithm. To\nthis end, by leveraging Lagrangian duality and the stochastic optimization\ntheory, we first develop an opportunistic MC-NOMA scheduling algorithm whereby\nthe original problem is decomposed into a series of subproblems, one for each\ntime slot. Accordingly, resource scheduling works in an online manner by\nsolving one subproblem per time slot, making it more applicable to practical\nsystems. Then, we further develop a heuristic joint subchannel assignment and\npower allocation (Joint-SAPA) algorithm with very low computational complexity,\ncalled Joint-SAPA-LCC, that solves each subproblem. Finally, through\nsimulation, we show that our Joint-SAPA-LCC algorithm provides good performance\ncomparable to the existing Joint-SAPA algorithms despite requiring much lower\ncomputational complexity. We also demonstrate that our opportunistic MC-NOMA\nscheduling algorithm in which the Joint-SAPA-LCC algorithm is embedded works\nwell while satisfying given QoS requirements.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:12:15 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 08:57:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kim", "Do-Yup", ""], ["Jafarkhani", "Hamid", ""], ["Lee", "Jang-Won", ""]]}, {"id": "2103.10746", "submitter": "Tomoya Tanaka", "authors": "Tomoya Tanaka, Tomio Kamada and Chikara Ohta", "title": "Topic Allocation Method on Edge Servers for Latency-sensitive\n  Notification Service", "comments": "18 pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The importance of real-time notification has been growing for social services\nand Intelligent Transporting System (ITS). As an advanced version of Pub/Sub\nsystems, publish-process-subscribe systems, where published messages are\nspooled and processed on edge servers, have been proposed to achieve\ndata-driven intelligent notifications. In this paper, we present a system that\nallows a topic to be managed on multiple edge servers so that messages are\nprocessed near the publishers, even when publishers are spread over a wide\narea. Duplicating messages on geographically distributed servers could enable\nimmediate notification to neighboring subscribers. However, the duplicated\nmessage spool may cause exhaustion of resources. We prepare a formal model of\nour publish-process-subscribe system and formulate the topic allocation as an\noptimization problem under the resource constraints of edge servers. As the\noptimization problem is NP-hard, we propose heuristics leveraging the locality\nand the pub/sub relationships observed between clients to use the edge server\nresources efficiently. Our performance evaluation shows that our method reduces\nthe delay to deliver notifications and the effectiveness of the strategy\nexploiting the relationships between clients.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:28:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Tanaka", "Tomoya", ""], ["Kamada", "Tomio", ""], ["Ohta", "Chikara", ""]]}, {"id": "2103.11043", "submitter": "Jianli Pan", "authors": "Ismail Alqerm, Jianyu Wang, Jianli Pan, and Yuanni Liu", "title": "BEHAVE: Behavior-Aware, Intelligent and Fair Resource Management for\n  Heterogeneous Edge-IoT Systems", "comments": null, "journal-ref": "IEEE Transactions on Mobile Computing, 2021", "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing is an emerging solution to support the future Internet of\nThings (IoT) applications that are delay-sensitive, processing-intensive or\nthat require closer intelligence. Machine intelligence and data-driven\napproaches are envisioned to build future Edge-IoT systems that satisfy IoT\ndevices' demands for edge resources. However, significant challenges and\ntechnical barriers exist which complicate the resource management for such\nEdge-IoT systems. IoT devices running various applications can demonstrate a\nwide range of behaviors in the devices' resource demand that are extremely\ndifficult to manage. In addition, the management of multidimensional resources\nfairly and efficiently by the edge in such a setting is a challenging task. In\nthis paper, we develop a novel data-driven resource management framework named\nBEHAVE that intelligently and fairly allocates edge resources to heterogeneous\nIoT devices with consideration of their behavior of resource demand (BRD).\nBEHAVE aims to holistically address the management technical barriers by: 1)\nbuilding an efficient scheme for modeling and assessment of the BRD of IoT\ndevices based on their resource requests and resource usage; 2) expanding a new\nRational, Fair, and Truthful Resource Allocation (RFTA) model that binds the\ndevices' BRD and resource allocation to achieve fair allocation and encourage\ntruthfulness in resource demand; and 3) developing an enhanced deep\nreinforcement learning (EDRL) scheme to achieve the RFTA goals. The evaluation\nresults demonstrate BEHAVE's capability to analyze the IoT devices' BRD and\nadjust its resource management policy accordingly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 22:09:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Alqerm", "Ismail", ""], ["Wang", "Jianyu", ""], ["Pan", "Jianli", ""], ["Liu", "Yuanni", ""]]}, {"id": "2103.11073", "submitter": "Quoc-Viet Pham", "authors": "Quoc-Viet Pham and Ming Zeng and Rukhsana Ruby and Thien Huynh-The and\n  Won-Joo Hwang", "title": "UAV Communications for Sustainable Federated Learning", "comments": "Accepted by IEEE Vehicular Technology correspondence 2021", "journal-ref": null, "doi": "10.1109/TVT.2021.3065084", "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL), invented by Google in 2016, has become a hot\nresearch trend. However, enabling FL in wireless networks has to overcome the\nlimited battery challenge of mobile users. In this regard, we propose to apply\nunmanned aerial vehicle (UAV)-empowered wireless power transfer to enable\nsustainable FL-based wireless networks. The objective is to maximize the UAV\ntransmit power efficiency, via a joint optimization of transmission time and\nbandwidth allocation, power control, and the UAV placement. Directly solving\nthe formulated problem is challenging, due to the coupling of variables. Hence,\nwe leverage the decomposition technique and a successive convex approximation\napproach to develop an efficient algorithm, namely UAV for sustainable FL\n(UAV-SFL). Finally, simulations illustrate the potential of our proposed\nUAV-SFL approach in providing a sustainable solution for FL-based wireless\nnetworks, and in reducing the UAV transmit power by 32.95%, 63.18%, and 78.81%\ncompared with the benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 02:29:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pham", "Quoc-Viet", ""], ["Zeng", "Ming", ""], ["Ruby", "Rukhsana", ""], ["Huynh-The", "Thien", ""], ["Hwang", "Won-Joo", ""]]}, {"id": "2103.11083", "submitter": "Hong-Ning Dai Prof.", "authors": "Ke Zhang, Hanbo Ying, Hong-Ning Dai, Lin Li, Yuangyuang Peng, Keyi\n  Guo, Hongfang Yu", "title": "Compacting Deep Neural Networks for Internet of Things: Methods and\n  Applications", "comments": "25 pages, 11 figures", "journal-ref": "IEEE Internet of Things Journal, 2021", "doi": "10.1109/JIOT.2021.3063497", "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Neural Networks (DNNs) have shown great success in completing complex\ntasks. However, DNNs inevitably bring high computational cost and storage\nconsumption due to the complexity of hierarchical structures, thereby hindering\ntheir wide deployment in Internet-of-Things (IoT) devices, which have limited\ncomputational capability and storage capacity. Therefore, it is a necessity to\ninvestigate the technologies to compact DNNs. Despite tremendous advances in\ncompacting DNNs, few surveys summarize compacting-DNNs technologies, especially\nfor IoT applications. Hence, this paper presents a comprehensive study on\ncompacting-DNNs technologies. We categorize compacting-DNNs technologies into\nthree major types: 1) network model compression, 2) Knowledge Distillation\n(KD), 3) modification of network structures. We also elaborate on the diversity\nof these approaches and make side-by-side comparisons. Moreover, we discuss the\napplications of compacted DNNs in various IoT applications and outline future\ndirections.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:18:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Ke", ""], ["Ying", "Hanbo", ""], ["Dai", "Hong-Ning", ""], ["Li", "Lin", ""], ["Peng", "Yuangyuang", ""], ["Guo", "Keyi", ""], ["Yu", "Hongfang", ""]]}, {"id": "2103.11098", "submitter": "Changmin Lee Ph.D -ing", "authors": "Changmin Lee, Seong-Lyun Kim", "title": "Most Efficient Sensor Network Protocol for a Permanent Natural Disaster\n  Monitoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  To minimize enormous havoc from disasters, permanent environment monitoring\nis necessarily required. Thus we propose a novel energy management protocol for\nenergy harvesting wireless sensor networks (EH-WSNs), named the adaptive sensor\nnode management protocol (ASMP). The proposed protocol makes system components\nto systematically control their performance to conserve the energy. Through\nthis protocol, sensor nodes autonomously activate an additional energy\nconservation algorithm. ASMP embeds three sampling algorithms. For the\noptimized environment sampling, we proposed the adaptive sampling algorithm for\nmonitoring (ASA-m). ASA-m estimates the expected time period to occur\nmeaningful change. The meaningful change refers to the distance between two\ntarget data for the monitoring QoS. Therefore, ASA-m merely gathers the data\nthe system demands. The continuous adaptive sampling algorithm (CASA) solves\nthe problem to be continuously decreasing energy despite of ASA-m. When the\nmonitored environment shows a linear trend property, the sensor node in CASA\nrests a sampling process, and the server generates predicted data at the\nestimated time slot. For guaranteeing the self-sustainability, ASMP uses the\nrecoverable adaptive sampling algorithm (RASA). RASA makes consumed energy\nsmaller than harvested energy by utilizing the predicted data. RASA recharges\nthe energy of the sensor node. Through this method, ASMP achieves both energy\nconservation and service quality.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 04:53:02 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lee", "Changmin", ""], ["Kim", "Seong-Lyun", ""]]}, {"id": "2103.11148", "submitter": "George Papakostas Prof.", "authors": "G.G. Samatas, S.S. Moumgiakmas, G.A. Papakostas", "title": "Predictive Maintenance -- Bridging Artificial Intelligence and IoT", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper highlights the trends in the field of predictive maintenance with\nthe use of machine learning. With the continuous development of the Fourth\nIndustrial Revolution, through IoT, the technologies that use artificial\nintelligence are evolving. As a result, industries have been using these\ntechnologies to optimize their production. Through scientific research\nconducted for this paper, conclusions were drawn about the trends in Predictive\nMaintenance applications with the use of machine learning bridging Artificial\nIntelligence and IoT. These trends are related to the types of industries in\nwhich Predictive Maintenance was applied, the models of artificial intelligence\nwere implemented, mainly of machine learning and the types of sensors that are\napplied through the IoT to the applications. Six sectors were presented and the\nproduction sector was dominant as it accounted for 54.54% of total\npublications. In terms of artificial intelligence models, the most prevalent\namong ten were the Artificial Neural Networks, Support Vector Machine and\nRandom Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve\ncategories of sensors emerged, of which the most widely used were the sensors\nof temperature and vibration with percentages of 60.71% and 46.42%\ncorrespondingly.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 10:01:40 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 17:50:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Samatas", "G. G.", ""], ["Moumgiakmas", "S. S.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2103.11396", "submitter": "Jasenka Dizdarevic", "authors": "Jasenka Dizdarevic and Admela Jukan", "title": "Engineering an IoT-Edge-Cloud Computing System Architecture: Lessons\n  Learnt from An Undergraduate Lab Course", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid advances in IoT, edge and cloud computing solutions, it is\ncritical to educate and train students in computer science and engineering in\nvarious aspects of IoT-edge-cloud (IoT-E-C) system architecture\nimplementations. We outline the design and development of an undergraduate\nlaboratory course that sets the goal of implementing various interfaces and\ncommunication protocols to connect IoT, edge and cloud computing systems and\nevaluating their performance. The lab setup is modular and based on open source\ntools. In the IoT context, it consists of low-cost processing platforms with\nvarious sensors and actuators. In the edge and cloud computing context, we\nimplement and deploy single board computers and Firebase cloud solutions,\nrespectively. The modular lab setup allows students to engineer and integrate\nvarious communication protocol solutions, including MQTT, COAP and HTTP. In\naddition to the system implementation, students can evaluate and benchmark the\nperformance of the entire system.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:46:07 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 09:23:06 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dizdarevic", "Jasenka", ""], ["Jukan", "Admela", ""]]}, {"id": "2103.11439", "submitter": "Jan Janak", "authors": "Pekka Karhula, Jan Janak, Henning Schulzrinne", "title": "Checkpointing and Migration of IoT Edge Functions", "comments": "6 pages", "journal-ref": "EdgeSys '19: Proceedings of the 2nd International Workshop on Edge\n  Systems, Analytics and Networking, March 2019", "doi": "10.1145/3301418.3313947", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The serverless and functions as a service (FaaS) paradigms are currently\ntrending among cloud providers and are now increasingly being applied to the\nnetwork edge, and to the Internet of Things (IoT) devices. The benefits include\nreduced latency for communication, less network traffic and increased privacy\nfor data processing. However, there are challenges as IoT devices have limited\nresources for running multiple simultaneous containerized functions, and also\nFaaS does not typically support long-running functions. Our implementation\nutilizes Docker and CRIU for checkpointing and suspending long-running blocking\nfunctions. The results show that checkpointing is slightly slower than regular\nDocker pause, but it saves memory and allows for more long-running functions to\nbe run on an IoT device. Furthermore, the resulting checkpoint files are small,\nhence they are suitable for live migration and backing up stateful functions,\ntherefore improving availability and reliability of the system.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:15:26 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Karhula", "Pekka", ""], ["Janak", "Jan", ""], ["Schulzrinne", "Henning", ""]]}, {"id": "2103.11674", "submitter": "Young Jin Chun", "authors": "Chao Wang, Young Jin Chun", "title": "Stochastic Geometry Modeling and Analysis for THz-mmWave Hybrid IoT\n  Networks", "comments": "Submitted to IEEE IoT Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Terahertz (THz) band contains abundant spectrum resources that can offer\nultra-high data rates. However, due to the THz band's inherent characteristics,\ni.e., low penetrability, high path loss, and non-negligible molecular\nabsorption effect, THz communication can only provide limited coverage. To\novercome these fundamental obstacles and fully utilize the THz band, we\nconsider a hybrid Internet-of-Things (IoT) network consisting of THz and\nmillimeter wave (mmWave) cells. A hybrid IoT network can dynamically switch\nbetween mmWave and THz links to ensure reliable and ultra-fast data connection.\nWe use a stochastic geometric framework to evaluate the proposed hybrid IoT\nnetwork's coverage probability and spectral efficiency and validate the\nanalysis through numerical simulation. In this paper, we derive a closed-form\nexpression of the Laplace transform of the interference while considering an\naccurate multi-level Flat-top (MLFT) antenna pattern. We observed that a large\nantenna array with a strong bias to the THz base station (TBS) improves the\nend-to-end network performance through numerical results. Furthermore, we\nrecognized a fundamental trade-off relation between the TBS's node density and\nthe bias to mmWave/THz; e.g., high TBS density with a strong bias to the TBS\nmay degrade the network performance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:06:32 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 03:58:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wang", "Chao", ""], ["Chun", "Young Jin", ""]]}, {"id": "2103.11782", "submitter": "Aidong Yang", "authors": "Aidong Yang, Xinlang Yue, Ye Ouyang", "title": "Reinforcement Learning Assisted Beamforming for Inter-cell Interference\n  Mitigation in 5G Massive MIMO Networks", "comments": "There is an error in section 4 about what are the definitions of\n  states and actions, which will affect the performance of the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.NI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beamforming is an essential technology in the 5G massive\nmultiple-input-multiple-output (MMIMO) communications, which are subject to\nmany impairments due to the nature of wireless transmission channel, i.e. the\nair. The inter-cell interference (ICI) is one of the main impairments faced by\n5G communications due to frequency-reuse technologies. In this paper, we\npropose a reinforcement learning (RL) assisted full dynamic beamforming for ICI\nmitigation in 5G downlink. The proposed algorithm is a joint of beamforming and\nfull dynamic Q-learning technology to minimize the ICI, and results in a\nlow-complexity method without channel estimation. Performance analysis shows\nthe quality of service improvement in terms of\nsignal-to-interference-plus-noise-ratio (SINR) and computational complexity\ncompared to other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:18:07 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 02:13:52 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Aidong", ""], ["Yue", "Xinlang", ""], ["Ouyang", "Ye", ""]]}, {"id": "2103.11891", "submitter": "Marcin Hoffmann", "authors": "Marcin Hoffmann, Pawel Kryszkiewicz, Adrian Kliks", "title": "Increasing Energy Efficiency of Massive-MIMO Network via Base Stations\n  Switching using Reinforcement Learning and Radio Environment Maps", "comments": null, "journal-ref": "Computer Communications, Volume 169, 2021, Pages 232-242, ISSN\n  0140-3664", "doi": "10.1016/j.comcom.2021.01.012", "report-no": null, "categories": "eess.SP cs.IT cs.LG cs.NI math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Energy Efficiency (EE) is of high importance while considering Massive\nMultiple-Input Multiple-Output (M-MIMO) networks where base stations (BSs) are\nequipped with an antenna array composed of up to hundreds of elements. M-MIMO\ntransmission, although highly spectrally efficient, results in high energy\nconsumption growing with the number of antennas. This paper investigates EE\nimprovement through switching on/off underutilized BSs. It is proposed to use\nthe location-aware approach, where data about an optimal active BSs set is\nstored in a Radio Environment Map (REM). For efficient acquisition, processing\nand utilization of the REM data, reinforcement learning (RL) algorithms are\nused. State-of-the-art exploration/exploitation methods including e-greedy,\nUpper Confidence Bound (UCB), and Gradient Bandit are evaluated. Then\nanalytical action filtering, and an REM-based Exploration Algorithm (REM-EA)\nare proposed to improve the RL convergence time. Algorithms are evaluated using\nan advanced, system-level simulator of an M-MIMO Heterogeneous Network (HetNet)\nutilizing an accurate 3D-ray-tracing radio channel model. The proposed RL-based\nBSs switching algorithm is proven to provide 70% gains in EE over a\nstate-of-the-art algorithm using an analytical heuristic. Moreover, the\nproposed action filtering and REM-EA can reduce RL convergence time in relation\nto the best-performing state-of-the-art exploration method by 60% and 83%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:57:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Hoffmann", "Marcin", ""], ["Kryszkiewicz", "Pawel", ""], ["Kliks", "Adrian", ""]]}, {"id": "2103.11982", "submitter": "Ahmed Elzanaty Dr.", "authors": "Amanat Kafizov, Ahmed Elzanaty, Lav R. Varshney, Mohamed-Slim Alouini", "title": "Wireless Network Coding with Intelligent Reflecting Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.ET eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conventional wireless techniques are becoming inadequate for beyond\nfifth-generation (5G) networks due to latency and bandwidth considerations. To\nimprove the error performance and throughput of wireless communication systems,\nwe propose physical layer network coding (PNC) in an intelligent reflecting\nsurface (IRS)-assisted environment. We consider an IRS-aided butterfly network,\nwhere we propose an algorithm for obtaining the optimal IRS phases. Also,\nanalytic expressions for the bit error rate (BER) are derived. The numerical\nresults demonstrate that the proposed scheme significantly improves the BER\nperformance. For instance, the BER at the relay in the presence of a 32-element\nIRS is three orders of magnitudes less than that without an IRS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 16:29:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kafizov", "Amanat", ""], ["Elzanaty", "Ahmed", ""], ["Varshney", "Lav R.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2103.12221", "submitter": "Huy Tu", "authors": "Huy Tu, George Papadimitriou, Mariam Kiran, Cong Wang, Anirban Mandal,\n  Ewa Deelman, and Tim Menzies", "title": "Mining Scientific Workflows for Anomalous Data Transfers", "comments": "Accepted for MSR 2021: Working Conference on Mining Software\n  Repositories\n  (https://2021.msrconf.org/details/msr-2021-technical-papers/1/Mining-Workflows-for-Anomalous-Data-Transfers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern scientific workflows are data-driven and are often executed on\ndistributed, heterogeneous, high-performance computing infrastructures.\nAnomalies and failures in the workflow execution cause loss of scientific\nproductivity and inefficient use of the infrastructure. Hence, detecting,\ndiagnosing, and mitigating these anomalies are immensely important for reliable\nand performant scientific workflows. Since these workflows rely heavily on\nhigh-performance network transfers that require strict QoS constraints,\naccurately detecting anomalous network performance is crucial to ensure\nreliable and efficient workflow execution. To address this challenge, we have\ndeveloped X-FLASH, a network anomaly detection tool for faulty TCP workflow\ntransfers. X-FLASH incorporates novel hyperparameter tuning and data mining\napproaches for improving the performance of the machine learning algorithms to\naccurately classify the anomalous TCP packets. X-FLASH leverages XGBoost as an\nensemble model and couples XGBoost with a sequential optimizer, FLASH, borrowed\nfrom search-based Software Engineering to learn the optimal model parameters.\nX-FLASH found configurations that outperformed the existing approach up to\n28\\%, 29\\%, and 40\\% relatively for F-measure, G-score, and recall in less than\n30 evaluations. From (1) large improvement and (2) simple tuning, we recommend\nfuture research to have additional tuning study as a new standard, at least in\nthe area of scientific workflow anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:06:07 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tu", "Huy", ""], ["Papadimitriou", "George", ""], ["Kiran", "Mariam", ""], ["Wang", "Cong", ""], ["Mandal", "Anirban", ""], ["Deelman", "Ewa", ""], ["Menzies", "Tim", ""]]}, {"id": "2103.12296", "submitter": "Bo Yang", "authors": "Xuelin Cao, Bo Yang, Hongliang Zhang, Chongwen Huang, Chau Yuen, Zhu\n  Han", "title": "Reconfigurable Intelligent Surface-Assisted MAC for Wireless Networks:\n  Protocol Design, Analysis, and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable intelligent surface (RIS) is a promising reflective radio\ntechnology for improving the coverage and rate of future wireless systems by\nreconfiguring the wireless propagation environment. The current work mainly\nfocuses on the physical layer design of RIS. However, enabling multiple devices\nto communicate with the assistance of RIS is a crucial challenging problem.\nMotivated by this, we explore RIS-assisted communications at the medium access\ncontrol (MAC) layer and propose an RIS-assisted MAC framework. In particular,\nRISassisted transmissions are implemented by pre-negotiation and a\nmulti-dimension reservation (MDR) scheme. Based on this, we investigate\nRIS-assisted single-channel multi-user (SCMU) communications. Wherein the RIS\nregarded as a whole unity can be reserved by one user to support the multiple\ndata transmissions, thus achieving high efficient RIS-assisted connections at\nthe user. Moreover, under frequency-selective channels, implementing the MDR\nscheme on the RIS group division, RISassisted multi-channel multi-user (MCMU)\ncommunications are further explored to improve the service efficiency of the\nRIS and decrease the computation complexity. Besides, a Markov chain is built\nbased on the proposed RIS-assisted MAC framework to analyze the system\nperformance of SCMU/MCMU. Then the optimization problem is formulated to\nmaximize the overall system capacity of SCMU/MCMU with energy-efficient\nconstraint. The performance evaluations demonstrate the feasibility and\neffectiveness of each\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:20:03 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cao", "Xuelin", ""], ["Yang", "Bo", ""], ["Zhang", "Hongliang", ""], ["Huang", "Chongwen", ""], ["Yuen", "Chau", ""], ["Han", "Zhu", ""]]}, {"id": "2103.12870", "submitter": "Arnau Rovira Sugranes", "authors": "Arnau Rovira-Sugranes, Fatemeh Afghah, Junsuo Qu, Abolfazl Razi", "title": "Fully-echoed Q-routing with Simulated Annealing Inference for Flying\n  Adhoc Networks", "comments": "13 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current networking protocols deem inefficient in accommodating the two key\nchallenges of Unmanned Aerial Vehicle (UAV) networks, namely the network\nconnectivity loss and energy limitations. One approach to solve these issues is\nusing learning-based routing protocols to make close-to-optimal local decisions\nby the network nodes, and Q-routing is a bold example of such protocols.\nHowever, the performance of the current implementations of Q-routing algorithms\nis not yet satisfactory, mainly due to the lack of adaptability to continued\ntopology changes. In this paper, we propose a full-echo Q-routing algorithm\nwith a self-adaptive learning rate that utilizes Simulated Annealing (SA)\noptimization to control the exploration rate of the algorithm through the\ntemperature decline rate, which in turn is regulated by the experienced\nvariation rate of the Q-values. Our results show that our method adapts to the\nnetwork dynamicity without the need for manual re-initialization at transition\npoints (abrupt network topology changes). Our method exhibits a reduction in\nthe energy consumption ranging from 7% up to 82%, as well as a 2.6 fold gain in\nsuccessful packet delivery rate}, compared to the state of the art Q-routing\nprotocols\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:28:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Rovira-Sugranes", "Arnau", ""], ["Afghah", "Fatemeh", ""], ["Qu", "Junsuo", ""], ["Razi", "Abolfazl", ""]]}, {"id": "2103.13055", "submitter": "Oliver Gasser", "authors": "Aniss Maghsoudlou, Oliver Gasser, Anja Feldmann", "title": "Zeroing in on Port 0 Traffic in the Wild", "comments": "Proceedings of the 2021 Passive and Active Measurement Conference\n  (PAM '21)", "journal-ref": null, "doi": "10.1007/978-3-030-72582-2_32", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet services leverage transport protocol port numbers to specify the\nsource and destination application layer protocols. While using port 0 is not\nallowed in most transport protocols, we see a non-negligible share of traffic\nusing port 0 in the Internet. In this study, we dissect port 0 traffic to infer\nits possible origins and causes using five complementing flow-level and\npacket-level datasets. We observe 73 GB of port 0 traffic in one week of IXP\ntraffic, most of which we identify as an artifact of packet fragmentation. In\nour packet-level datasets, most traffic is originated from a small number of\nhosts and while most of the packets have no payload, a major fraction of\npackets containing payload belong to the BitTorrent protocol. Moreover, we find\nunique traffic patterns commonly seen in scanning. In addition to analyzing\npassive traces, we also conduct an active measurement campaign to study how\ndifferent networks react to port 0 traffic. We find an unexpectedly high\nresponse rate for TCP port 0 probes in IPv4, with very low response rates with\nother protocol types. Finally, we will be running continuous port 0\nmeasurements and providing the results to the measurement community.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:06:23 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 10:19:27 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Maghsoudlou", "Aniss", ""], ["Gasser", "Oliver", ""], ["Feldmann", "Anja", ""]]}, {"id": "2103.13212", "submitter": "Brian McCarthy", "authors": "Brian McCarthy, Andres Burbano-Abril, Victor Rangel Licea and Aisling\n  O'Driscoll", "title": "OpenCV2X: Modelling of the V2X Cellular Sidelink and Performance\n  Evaluation for Aperiodic Traffic", "comments": "18 Pages, 20 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents OpenCV2X, the first publicly available, open-source\nsimulation model of the Third Generation Partnership Project (3GPP) Release 14\nCellular Vehicle to Everything (C-V2X) sidelink, which forms the basis for 5G\nNR Mode 2 under later releases. This model is fully compliant with the existing\nvehicular service and application layers, including messaging sets as defined\nby the automotive and standards communities providing a fully standardised,\ncross-layer communication model. Using this model, we show how the current\nsidelink scheduling mechanism performs poorly when scheduling applications with\nhighly aperiodic communication characteristics, such as ETSI Cooperative\nAwareness Messages (CAMs). We then provide the first indepth evaluation of\ndedicated per-packet aperiodic scheduling mechanisms, in contrast to schemes\nthat parameterise the existing algorithm. This paper highlights that the level\nof aperiodicity exhibited by the application model greatly impacts scheduling\nperformance. Finally, we analyse how such scheduling mechanisms might co-exist.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:18:13 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["McCarthy", "Brian", ""], ["Burbano-Abril", "Andres", ""], ["Licea", "Victor Rangel", ""], ["O'Driscoll", "Aisling", ""]]}, {"id": "2103.13263", "submitter": "Marco Faltelli Mr.", "authors": "Marco Faltelli, Giacomo Belocchi, Francesco Quaglia, Salvatore\n  Pontarelli, Giuseppe Bianchi", "title": "Metronome: adaptive and precise intermittent packet retrieval in DPDK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing performance requirements of modern applications place a\nsignificant burden on software-based packet processing. Most of today's\nsoftware input/output accelerations achieve high performance at the expense of\nreserving CPU resources dedicated to continuously poll the Network Interface\nCard. This is specifically the case with DPDK (Data Plane Development Kit),\nprobably the most widely used framework for software-based packet processing\ntoday. The approach presented in this paper, descriptively called Metronome,\nhas the dual goals of providing CPU utilization proportional to the load, and\nallowing flexible sharing of CPU resources between I/O tasks and applications.\nMetronome replaces DPDK's continuous polling with an intermittent sleep&wake\nmode, and revolves around a new multi-threaded operation, which improves\nservice continuity. Since the proposed operation trades CPU usage with\nbuffering delay, we propose an analytical model devised to dynamically adapt\nthe sleep&wake parameters to the actual traffic load, meanwhile providing a\ntarget average latency. Our experimental results show a significant reduction\nof the CPU cycles, improvements in power usage, and robustness to CPU sharing\neven when challenged with CPU-intensive applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:27:24 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 08:34:29 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 15:40:00 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Faltelli", "Marco", ""], ["Belocchi", "Giacomo", ""], ["Quaglia", "Francesco", ""], ["Pontarelli", "Salvatore", ""], ["Bianchi", "Giuseppe", ""]]}, {"id": "2103.13293", "submitter": "Chit Wutyee Zaw", "authors": "Chit Wutyee Zaw, Shashi Raj Pandey, Kitae Kim, and Choong Seon Hong", "title": "Energy-aware Resource Management for Federated Learning in Multi-access\n  Edge Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Federated Learning (FL), a global statistical model is developed by\nencouraging mobile users to perform the model training on their local data and\naggregating the output local model parameters in an iterative manner. However,\ndue to limited energy and computation capability at the mobile devices, the\nperformance of the model training is always at stake to meet the objective of\nlocal energy minimization. In this regard, Multi-access Edge Computing\n(MEC)-enabled FL addresses the tradeoff between the model performance and the\nenergy consumption of the mobile devices by allowing users to offload a portion\nof their local dataset to an edge server for the model training. Since the edge\nserver has high computation capability, the time consumption of the model\ntraining at the edge server is insignificant. However, the time consumption for\ndataset offloading from mobile users to the edge server has a significant\nimpact on the total time consumption. Thus, resource management in MEC-enabled\nFL is challenging, where the objective is to reduce the total time consumption\nwhile saving the energy consumption of the mobile devices. In this paper, we\nformulate an energy-aware resource management for MEC-enabled FL in which the\nmodel training loss and the total time consumption are jointly minimized, while\nconsidering the energy limitation of mobile devices. In addition, we recast the\nformulated problem as a Generalized Nash Equilibrium Problem (GNEP) to capture\nthe coupling constraints between the radio resource management and dataset\noffloading. We then analyze the impact of the dataset offloading and computing\nresource allocation on the model training loss, time, and the energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 07:00:54 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zaw", "Chit Wutyee", ""], ["Pandey", "Shashi Raj", ""], ["Kim", "Kitae", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2103.13298", "submitter": "Jianyu Zhao", "authors": "Jianyu Zhao, Chenyang Yang", "title": "Deep Reinforcement Learning with Symmetric Prior for Predictive Power\n  Allocation to Mobile Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning has been applied for a variety of wireless tasks,\nwhich is however known with high training and inference complexity. In this\npaper, we resort to deep deterministic policy gradient (DDPG) algorithm to\noptimize predictive power allocation among K mobile users requesting video\nstreaming, which minimizes the energy consumption of the network under the\nno-stalling constraint of each user. To reduce the sampling complexity and\nmodel size of the DDPG, we exploit a kind of symmetric prior inherent in the\nactor and critic networks: permutation invariant and equivariant properties, to\ndesign the neural networks. Our analysis shows that the free model parameters\nof the DDPG can be compressed by 2/K^2. Simulation results demonstrate that the\nepisodes required by the learning model with the symmetric prior to achieve the\nsame performance as the vanilla policy reduces by about one third when K = 10.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:24:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zhao", "Jianyu", ""], ["Yang", "Chenyang", ""]]}, {"id": "2103.13303", "submitter": "Mohamad Reza Sadeghi Moghadam", "authors": "Taha Mansouri, Mohammad Reza Sadeghi Moghadam, Fatemeh Monshizadeh,\n  Ahad Zareravasan", "title": "IoT Data Quality Issues and Potential Solutions: A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet of Things (IoT) is a paradigm that connects everyday items to\nthe Internet. In the recent decade, the IoT's spreading popularity is a\npromising opportunity for people and industries. IoT utilizes in a wide range\nof respects such as agriculture, healthcare, smart cities, and manufacturing\nsectors. IoT data quality is crucial in IoT real-life applications. IoT data\nquality dimensions and issues should be considered because we require data to\nmake accurate and timely decisions, produce commodities, and gain insights\nabout events, people, and the environment. It is essential to point out that we\ncannot reach valuable results by using poor quality data. This paper aims to\ndevelop a new category for IoT data quality. Hence, we examine existing IoT\ndata quality dimensions and IoT data quality issues in general and specific\ndomains and IoT data quality dimensions' categories. It is worth considering\nthat categories in the context of IoT are not many. We developed a new category\nin which IoT data quality dimensions and issues are separated. Concerning this\ncategory, we can get familiar with related dimensions and issues in each\ncategory. To enhance data quality dimensions and minimize data quality issues,\nwe suggest potential solutions using Blockchain to overcome IoT's security\nissues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:35:29 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mansouri", "Taha", ""], ["Moghadam", "Mohammad Reza Sadeghi", ""], ["Monshizadeh", "Fatemeh", ""], ["Zareravasan", "Ahad", ""]]}, {"id": "2103.13306", "submitter": "Jie Chen", "authors": "Dibyajyoti Guha, Jie Chen, Abhijit Dutta Banik, Biplab Sikdar", "title": "Delay and Power consumption Analysis for Queue State Dependent Service\n  Rate Control in WirelessHart System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To solve the problem of power supply limitation of machines working in\nwireless industry automation, we evaluated the workload aware service rate\ncontrol design implanted in the medium access control component of these small\ndevices and proposed a bio-intelligence based algorithm to optimise the design\nregarding the delay constraint while minimizing power consumption. To achieve\nthis, we provide an accurate analysis of the delay cost of this design and for\nthe first time pinpoint an exact departure process model in order to evaluate\nthe overall delay cost in consideration of the medium access time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 13:49:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Guha", "Dibyajyoti", ""], ["Chen", "Jie", ""], ["Banik", "Abhijit Dutta", ""], ["Sikdar", "Biplab", ""]]}, {"id": "2103.13317", "submitter": "Frank Phillipson", "authors": "Frank Phillipson", "title": "End-to-End Quality of Service Management in the Physical Internet: a\n  Digital Internet Inspired Multi-Domain Approach", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the layer 'System Level Functionality' of the Phyisical Internet, it is\nneeded to estimate end-to-end performance characteristics of transportations\nthat visit multiple logistic domains. This paper proposes an approach based on\na Digital Internet functionality: a combination of a Service Level Agreement\nregistry and a Quality of Service processor. Existing SLA-calculus gives tools\nfor the QoS-processor to combine the SLA-parameters for all possible end-to-end\npaths and gives the QoS-processor the possibility to propose the best path\ngiven the required performance. A realistic implementation is proposed using a\nmulti objective/constraint approach and a related communication form between\nthe domain owner and the QoS Processor.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 13:28:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Phillipson", "Frank", ""]]}, {"id": "2103.13321", "submitter": "Arun Venkatesh Ramesh", "authors": "Arun Venkatesh Ramesh and Xingpeng Li", "title": "Network Reconfiguration Impact on Renewable Energy System and Energy\n  Storage System in Day-Ahead Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable energy sources (RES) has gained significant interest in recent\nyears. However, due to favourable weather conditions, the RES is installed in\nremote locations with limited transmission capacity. As a result, it can lead\nto major curtailments of the free resource when the network is congested.\nTherefore, energy storage system (ESS) is considered as a viable solution to\nstore energy and address the intermittent nature of RES though ESS is often\ndistributed and may not be geographically close to RES. Therefore, ESS may also\nsuffer from limited transmission capacity due to network congestion. Currently,\ngrid operators overlook network flexibility as a congestion management tool in\nday-ahead scheduling. This paper addresses these issues and studies the\nbenefits of introducing network reconfiguration (NR) as a preventive and\ncorrective action for transmission flexibility in day-ahead stochastic\nsecurity-constrained unit-commitment (SSCUC-PC) while considering a\nmulti-scenario RES output. Simulation results demonstrate that NR can lower\ntotal system cost, reduce RES curtailments and utilize ESS for better impact by\nalleviating network congestion in both base-case and post-contingency networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:51:29 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ramesh", "Arun Venkatesh", ""], ["Li", "Xingpeng", ""]]}, {"id": "2103.13346", "submitter": "Andrea Munari", "authors": "Andrea Munari and Gianluigi Liva", "title": "Information Freshness Analysis of Slotted ALOHA in Gilbert-Elliot\n  Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter analyzes a class of information freshness metrics for large IoT\nsystems in which terminals employ slotted ALOHA to access a common channel.\nConsidering a Gilbert- Elliot channel model, information freshness is evaluated\nthrough a penalty function that follows a power law of the time elapsed since\nthe last received update, in contrast with the linear growth of age of\ninformation. By means of a signal flow graph analysis of Markov processes, we\nprovide exact closed form expressions for the average penalty and for the peak\npenalty violation probability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:12:44 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 13:36:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Munari", "Andrea", ""], ["Liva", "Gianluigi", ""]]}, {"id": "2103.13348", "submitter": "Alessandro Guidotti", "authors": "Alessandro Guidotti, Matteo Conti, Alessandro Vanelli-Coralli", "title": "Beamforming in LEO Constellations for NB-IoT Services in 6G\n  Communications", "comments": "Submitted to IEEE ICC'21 Workshop - SatMegaConst 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the first commercializations of 5G networks, Beyond 5G (B5G), or 6G,\nsystems are starting to be defined. In this context, Internet of Things (IoT)\nservices will be even more impactful with respect to 5G systems. In order to\ncope with the huge amount of IoT devices, and the potentially large capacity\nrequirements for the most advanced of them (e.g., live camera feeds from first\nresponders in emergency scenarios), non-terrestrial systems will be pivotal to\nassist and complement the terrestrial networks. In this paper, we propose a\nmulti-layer non-terrestrial architecture for NarrowBand-IoT (NB-IoT) services\nbased on Geosynchronous Orbit (GSO) and Non GSO (NGSO) nodes. To cope with both\nthe number of devices and the potentially large capacities, we propose to\nimplement Minimum Mean Square Error (MMSE) beamforming in an aggressive full\nfrequency reuse scenario. The performance assessment shows the significant\nbenefits of the proposed solution.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:26:50 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Guidotti", "Alessandro", ""], ["Conti", "Matteo", ""], ["Vanelli-Coralli", "Alessandro", ""]]}, {"id": "2103.13424", "submitter": "Luxi Zhao", "authors": "Luxi Zhao, Paul Pop, Sebastian Steinhorst", "title": "Quantitative Performance Comparison of Various Traffic Shapers in\n  Time-Sensitive Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owning to the sub-standards being developed by IEEE Time-Sensitive Networking\n(TSN) Task Group, the traditional IEEE 802.1 Ethernet is enhanced to support\nreal-time dependable communications for future time- and safety-critical\napplications. Several sub-standards have been recently proposed that introduce\nvarious traffic shapers (e.g., Time-Aware Shaper (TAS), Asynchronous Traffic\nShaper (ATS), Credit-Based Shaper (CBS), Strict Priority (SP)) for flow control\nmechanisms of queuing and scheduling, targeting different application\nrequirements. These shapers can be used in isolation or in combination and\nthere is limited work that analyzes, evaluates and compares their performance,\nwhich makes it challenging for end-users to choose the right combination for\ntheir applications. This paper aims at (i) quantitatively comparing various\ntraffic shapers and their combinations, (ii) summarizing, classifying and\nextending the architectures of individual and combined traffic shapers and\ntheir Network calculus (NC)-based performance analysis methods and (iii)\nfilling the gap in the timing analysis research on handling two novel hybrid\narchitectures of combined traffic shapers, i.e., TAS+ATS+SP and TAS+ATS+CBS. A\nlarge number of experiments, using both synthetic and realistic test cases, are\ncarried out for quantitative performance comparisons of various individual and\ncombined traffic shapers, from the perspective of upper bounds of delay,\nbacklog and jitter. To the best of our knowledge, we are the first to\nquantitatively compare the performance of the main traffic shapers in TSN. The\npaper aims at supporting the researchers and practitioners in the selection of\nsuitable TSN sub-protocols for their use cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:10:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhao", "Luxi", ""], ["Pop", "Paul", ""], ["Steinhorst", "Sebastian", ""]]}, {"id": "2103.13794", "submitter": "Maurilio Matracia", "authors": "Maurilio Matracia, Mustafa A. Kishk, Mohamed-Slim Alouini", "title": "Coverage Analysis for UAV-Assisted Cellular Networks in Rural Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite coverage enhancement in rural areas is one of the main requirements\nin next generations of wireless networks (i.e., 5G and 6G), the low expected\nprofit prevents telecommunication providers from investing in such sparsely\npopulated areas. Hence, it is required to design and deploy cost efficient\nalternatives for extending the cellular infrastructure to these regions. A\nconcrete mathematical model that characterizes and clearly captures the\naforementioned problem might be a key-enabler for studying the efficiency of\nany potential solution. Unfortunately, the commonly used mathematical tools\nthat model large scale wireless networks are not designed to capture the\nunfairness, in terms of cellular coverage, suffered by exurban and rural areas.\nIn big cities, in fact, cellular deployment is essentially capacity driven and\nthus cellular base station densities are maximum in the town centers and\ndecline when getting far from them. In this paper, a new stochastic\ngeometry-based model is implemented in order to show the coverage spatial\nvariation among urban, suburban, and exurban settlements. Indeed, by\nimplementing inhomogeneous Poisson point processes (PPPs) it is possible to\nstudy the performance metrics in a realistic scenario where terrestrial base\nstations (TBSs) are clustered around the urban center while outer aerial base\nstations (ABSs) are uniformly distributed outside an urban exclusion zone.\nBased on this, our simulation results can quantify the improvement, in terms of\ncoverage probability, that even a surprisingly low density of ABSs can bring to\nperipheral regions depending on the extension of the exclusion zone, enabling\nus to draw insightful considerations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:43:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Matracia", "Maurilio", ""], ["Kishk", "Mustafa A.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2103.13832", "submitter": "Stefan Hristozov", "authors": "Stefan Hristozov, Manuel Huber, Lei Xu, Jaro Fietz, Marco Liess, Georg\n  Sigl", "title": "The Cost of OSCORE and EDHOC for Constrained Devices", "comments": "A short version of this paper will appear on CODASPY 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern IoT applications rely on the Constrained Application Protocol\n(CoAP) because of its efficiency and seamless integrability in the existing\nInternet infrastructure. One of the strategies that CoAP leverages to achieve\nthese characteristics is the usage of proxies. Unfortunately, in order for a\nproxy to operate, it needs to terminate the (D)TLS channels between clients and\nservers. Therefore, end-to-end confidentiality, integrity and authenticity of\nthe exchanged data cannot be achieved. In order to overcome this problem, an\nalternative to (D)TLS was recently proposed by the Internet Engineering Task\nForce (IETF). This alternative consists of two novel protocols: 1) Object\nSecurity for Constrained RESTful Environments (OSCORE) providing authenticated\nencryption for the payload data and 2) Ephemeral Diffie-Hellman Over COSE\n(EDHOC) providing the symmetric session keys required for OSCORE. In this\npaper, we present the design of four firmware libraries for these protocols\nespecially targeted for constrained microcontrollers and their detailed\nevaluation. More precisely, we present the design of uOSCORE and uEDHOC\nlibraries for regular microcontrollers and uOSCORE-TEE and uEDHOC-TEE libraries\nfor microcontrollers with a Trusted Execution Environment (TEE), such as\nmicrocontrollers featuring ARM TrustZone-M. Our firmware design for the later\nclass of devices concerns the fact that attackers may exploit common software\nvulnerabilities, e.g., buffer overflows in the protocol logic, OS or\napplication to compromise the protocol security. uOSCORE-TEE and uEDHOC-TEE\nachieve separation of the cryptographic operations and keys from the remainder\nof the firmware, which could be vulnerable. We present an evaluation of our\nimplementations in terms of RAM/FLASH requirements, execution speed and energy\non a broad range of microcontrollers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:21:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hristozov", "Stefan", ""], ["Huber", "Manuel", ""], ["Xu", "Lei", ""], ["Fietz", "Jaro", ""], ["Liess", "Marco", ""], ["Sigl", "Georg", ""]]}, {"id": "2103.13989", "submitter": "Brian Kim", "authors": "Brian Kim and Yalin E. Sagduyu and Tugba Erpek and Sennur Ulukus", "title": "Adversarial Attacks on Deep Learning Based mmWave Beam Prediction in 5G\n  and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning provides powerful means to learn from spectrum data and solve\ncomplex tasks in 5G and beyond such as beam selection for initial access (IA)\nin mmWave communications. To establish the IA between the base station (e.g.,\ngNodeB) and user equipment (UE) for directional transmissions, a deep neural\nnetwork (DNN) can predict the beam that is best slanted to each UE by using the\nreceived signal strengths (RSSs) from a subset of possible narrow beams. While\nimproving the latency and reliability of beam selection compared to the\nconventional IA that sweeps all beams, the DNN itself is susceptible to\nadversarial attacks. We present an adversarial attack by generating adversarial\nperturbations to manipulate the over-the-air captured RSSs as the input to the\nDNN. This attack reduces the IA performance significantly and fools the DNN\ninto choosing the beams with small RSSs compared to jamming attacks with\nGaussian or uniform noise.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:25:21 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kim", "Brian", ""], ["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2103.14071", "submitter": "Roy Friedman", "authors": "Yamit Barshatz-Schneor and Roy Friedman", "title": "Accelerating Big-Data Sorting Through Programmable Switches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sorting is a fundamental and well studied problem that has been studied\nextensively. Sorting plays an important role in the area of databases, as many\nqueries can be served much faster if the relations are first sorted. One of the\nmost popular sorting algorithm in databases is merge sort.\n  In modern data-centers, data is stored in storage servers, while processing\ntakes place in compute servers. Hence, in order to compute queries on the data,\nit must travel through the network from the storage servers to the compute\nservers. This creates a potential for utilizing programmable switches to\nperform partial sorting in order to accelerate the sorting process at the\nserver side. This is possible because, as mentioned above, data packets pass\nthrough the switch in any case on their way to the server. Alas, programmable\nswitches offer a very restricted and non-intuitive programming model, which is\nwhy realizing this is not-trivial.\n  We devised a novel partial sorting algorithm that fits the programming model\nand restrictions of programmable switches and can expedite merge sort at the\nserver. We also utilize built-in parallelism in the switch to divide the data\ninto sequential ranges. Thus, the server needs to sort each range separately\nand then concatenate them to one sorted stream. This way, the server needs to\nsort smaller sections and each of these sections is already partially sorted.\nHence, the server does less work, and the access pattern becomes more\nvirtual-memory friendly.\n  We evaluated the performance improvements obtained when utilizing our partial\nsorting algorithm over several data stream compositions with various switch\nconfigurations. Our study exhibits an improvement of 20%-75% in the sorting\nrun-time when using our approach compared to plain sorting on the original\nstream.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:46:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Barshatz-Schneor", "Yamit", ""], ["Friedman", "Roy", ""]]}, {"id": "2103.14191", "submitter": "Marcelo De Abranches", "authors": "Marcelo Abranches, Karl Olson and Eric Keller", "title": "Infinity: A Scalable Infrastructure for In-Network Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network programmability is an area of research both defined by its potential\nand its current limitations. While programmable hardware enables customization\nof device operation, tailoring processing to finely tuned objectives, limited\nresources stifle much of the capability and scalability desired for future\ntechnologies. Current solutions to overcome these limitations simply shift the\nproblem, temporarily offloading memory needs or processing to other systems\nwhile incurring both round-trip time and complexity costs. To overcome these\nunnecessary costs, we introduce Infinity, a resource disaggregation method to\nmove processing to capable devices while continuing to forward as the original\nowner, limiting unnecessary buffering and round-trip processing. By forwarding\nboth the processing need and associated data simultaneously we are able to\nscale operation with minimal overhead and delay, improving both capability and\nperformance objectives for in-network processing.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 00:55:08 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Abranches", "Marcelo", ""], ["Olson", "Karl", ""], ["Keller", "Eric", ""]]}, {"id": "2103.14225", "submitter": "Shih-Chun Lin", "authors": "Shih-Chun Lin, Kwang-Cheng Chen, and Ali Karimoddini", "title": "SD-VEC: Software-Defined Vehicular Edge Computing with Ultra-Low Latency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New paradigm shifts and 6G technological revolution in vehicular services\nhave emerged toward unmanned driving, automated transportation, and\nself-driving vehicles. As the technology for autonomous vehicles becomes\nmature, real challenges come from reliable, safe, real-time connected\ntransportation operations to achieve ubiquitous and prompt information\nexchanges with massive connected and autonomous vehicles. This article aims at\nintroducing novel wireless distributed architectures that embed the edge\ncomputing capability inside software-defined vehicular networking\ninfrastructure. Such edge networks consist of open-loop grant-free\ncommunications and computing-based control frameworks, which enable dynamic\neco-routing with ultra-low latency and mobile data-driven orchestration. Thus,\nthis work advances the frontiers of machine learning potentials and\nnext-generation mobile system realization in vehicular networking applications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:25:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lin", "Shih-Chun", ""], ["Chen", "Kwang-Cheng", ""], ["Karimoddini", "Ali", ""]]}, {"id": "2103.14362", "submitter": "Jiachen Zhang", "authors": "Jiachen Zhang and Xingquan zuo and Mingying Xu and Jing Han and\n  Baisheng Zhang", "title": "Base Station Network Traffic Prediction Approach Based on LMA-DeepAR", "comments": "7 pages,4 figures,2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate network traffic prediction of base station cell is very vital for\nthe expansion and reduction of wireless devices in base station cell. The burst\nand uncertainty of base station cell network traffic makes the network traffic\nnonlinear and non-stationary, which brings challenges to the long-term\nprediction of network traffic. In this paper, the traffic model LMA-DeepAR for\nbase station network is established based on DeepAR. Acordding to the\ndistribution characteristics of network traffic, this paper proposes an\nartificial feature sequence calculation method based on local moving average\n(LMA). The feature sequence is input into DeepAR as covariant, which makes the\nstatistical characteristics of network traffic near a period of time in the\npast be considered when updating parameters, and the interference of\nnon-stationary network traffic on model training will be reduced. Experimental\nresults show that the proposed prediction approach (LMA-DeepAR) outperforms\nother methods in the overall long-term prediction performance and stability of\nmulti cell network traffic.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:08:35 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Jiachen", ""], ["zuo", "Xingquan", ""], ["Xu", "Mingying", ""], ["Han", "Jing", ""], ["Zhang", "Baisheng", ""]]}, {"id": "2103.14497", "submitter": "Ahmad Sawalmeh Dr.", "authors": "Ahmad H. Sawalmeh and Noor Shamsiah Othman", "title": "An Overview of Collision Avoidance Approaches and Network Architecture\n  of Unmanned Aerial Vehicles (UAVs)", "comments": "24 pages, 13 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As an autonomous vehicles, Unmanned Aerial Vehicles (UAVs) are subjected to\nseveral challenges. One of the challenges is for UAV to be able to avoid\ncollision. Many collision avoidance methods have been proposed to address this\nissue. Furthermore, in a multi-UAV system, it is also important to address\ncommunication issue among UAVs for cooperation and collaboration. This issue\ncan be addressed by setting up an ad-hoc network among UAVs. There is also a\nneed to consider the challenges in the deployment of UAVs, as well as, in the\ndevelopment of collision avoidance methods and the establishment of\ncommunication for cooperation and collaboration in a multi-UAV system. In this\npaper, we present general challenges in the deployment of UAV and comparison of\nUAV communication services based on its operating frequency. We also present\nmajor collision avoidance approaches, and specifically discuss collision\navoidance approaches that are suitable for indoor applications. We also present\nthe Flying Ad-hoc Networks (FANET) network architecture, communication and\nrouting protocols for each Open System Interconnection (OSI) communication\nlayers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:38:17 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sawalmeh", "Ahmad H.", ""], ["Othman", "Noor Shamsiah", ""]]}, {"id": "2103.14596", "submitter": "Martina Capuzzo", "authors": "Martina Capuzzo, Carmen Delgado, Jeroen Famaey, Andrea Zanella", "title": "An ns-3 implementation of a battery-less node for energy-harvesting\n  Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things (IoT), thousands of devices can be deployed to\nacquire data from the environment and provide service to several applications\nin different fields. In many cases, it is desirable that devices are\nself-sustainable in terms of energy. Therefore,the research community is\nexploring the possibility of employing battery-less devices, where the energy\nis derived solely from external and/or environmental sources, such as solar\npanels. In this work, we propose an ns-3 model of a (super) capacitor, which\ncan be used as the storage of the harvested energy in a battery-less IoT\ndevice, and add the support for the intermittent behavior of devices, turning\noff/on according to their energy level. To exemplify the use of the model, we\napply it to a LoRaWAN node, and compare the simulation outcomes with results in\nthe literature obtained with mathematical analysis, confirming the accuracy of\nthe implementation. Then, we show the importance of analyzing the interaction\nbetween energy availability and communication performance, paving the way for\nmore accurate and realistic simulations in the field. The implemented code is\nmade available as open source.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:58:46 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Capuzzo", "Martina", ""], ["Delgado", "Carmen", ""], ["Famaey", "Jeroen", ""], ["Zanella", "Andrea", ""]]}, {"id": "2103.14699", "submitter": "Favyen Bastani", "authors": "Favyen Bastani, Songtao He, Ziwen Jiang, Osbert Bastani, Michael\n  Cafarella, Tim Kraska, Sam Madden", "title": "SkyQuery: An Aerial Drone Video Sensing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video-based sensing from aerial drones, especially small multirotor drones,\ncan provide rich data for numerous applications, including traffic analysis\n(computing traffic flow volumes), precision agriculture (periodically\nevaluating plant health), and wildlife population management (estimating\npopulation sizes). However, aerial drone video sensing applications must handle\na surprisingly wide range of tasks: video frames must be aligned so that we can\nequate coordinates of objects that appear in different frames, video data must\nbe analyzed to extract application-specific insights, and drone routes must be\ncomputed that maximize the value of newly captured video. To address these\nchallenges, we built SkyQuery, a novel aerial drone video sensing platform that\nprovides an expressive, high-level programming language to make it\nstraightforward for users to develop complex long-running sensing applications.\nSkyQuery combines novel methods for fast video frame alignment and detection of\nsmall objects in top-down aerial drone video to efficiently execute\napplications with diverse video analysis workflows and data distributions,\nthereby allowing application developers to focus on the unique qualities of\ntheir particular application rather than general video processing, data\nanalysis, and drone routing tasks. We conduct diverse case studies using\nSkyQuery in parking monitoring, pedestrian activity mapping, and traffic hazard\ndetection scenarios to demonstrate the generalizability and effectiveness of\nour system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:09:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bastani", "Favyen", ""], ["He", "Songtao", ""], ["Jiang", "Ziwen", ""], ["Bastani", "Osbert", ""], ["Cafarella", "Michael", ""], ["Kraska", "Tim", ""], ["Madden", "Sam", ""]]}, {"id": "2103.14902", "submitter": "Samuele Zoppi", "authors": "Samuele Zoppi, Jaya Prakash Champati, James Gross, and Wolfgang\n  Kellerer", "title": "Scheduling of Wireless Edge Networks for Feedback-Based Interactive\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactive applications with automated feedback will largely influence the\ndesign of future networked infrastructures. In such applications, status\ninformation about an environment of interest is captured and forwarded to a\ncompute node, which analyzes the information and generates a feedback message.\nTimely processing and forwarding must ensure the feedback information to be\nstill applicable; thus, the quality-of-service parameter for such applications\nis the end-to-end latency over the entire loop. By modelling the communication\nof a feedback loop as a two-hop network, we address the problem of allocating\nnetwork resources in order to minimize the delay violation probability (DVP),\ni.e. the probability of the end-to-end latency exceeding a target value. We\ninvestigate the influence of the network queue states along the network path on\nthe performance of semi-static and dynamic scheduling policies. The former\ndetermine the schedule prior to the transmission of the packet, while the\nlatter benefit from feedback on the queue states as time evolves and reallocate\ntime slots depending on the queue's evolution. The performance of the proposed\npolicies is evaluated for variations in several system parameters and\ncomparison baselines. Results show that the proposed semi-static policy\nachieves close-to-optimal DVP and the dynamic policy outperforms the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:09:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zoppi", "Samuele", ""], ["Champati", "Jaya Prakash", ""], ["Gross", "James", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "2103.14917", "submitter": "Lei Deng", "authors": "Danzhou Wu, Lei Deng, Zilong Liu, Yijin Zhang, Yunghsiang S. Han", "title": "Reinforcement Learning Random Access for Delay-Constrained Heterogeneous\n  Wireless Networks: A Two-User Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the random access problem for a\ndelay-constrained heterogeneous wireless network. As a first attempt to study\nthis new problem, we consider a network with two users who deliver\ndelay-constrained traffic to an access point (AP) via a common unreliable\ncollision wireless channel. We assume that one user (called user 1) adopts\nALOHA and we optimize the random access scheme of the other user (called user\n2). The most intriguing part of this problem is that user 2 does not know the\ninformation of user 1 but needs to maximize the system timely throughput. Such\na paradigm of collaboratively sharing spectrum is envisioned by DARPA to better\ndynamically match the supply and demand in the future [1], [2]. We first\npropose a Markov Decision Process (MDP) formulation to derive a modelbased\nupper bound, which can quantify the performance gap of any designed schemes. We\nthen utilize reinforcement learning (RL) to design an R-learning-based [3]-[5]\nrandom access scheme, called TSRA. We finally carry out extensive simulations\nto show that TSRA achieves close-to-upper-bound performance and better\nperformance than the existing baseline DLMA [6], which is our counterpart\nscheme for delay-unconstrained heterogeneous wireless network. All source code\nis publicly available in https://github.com/DanzhouWu/TSRA.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:34:38 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 06:45:07 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 15:10:13 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wu", "Danzhou", ""], ["Deng", "Lei", ""], ["Liu", "Zilong", ""], ["Zhang", "Yijin", ""], ["Han", "Yunghsiang S.", ""]]}, {"id": "2103.14918", "submitter": "Francesco Restuccia", "authors": "Francesco Restuccia", "title": "IEEE 802.11bf: Toward Ubiquitous Wi-Fi Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wi-Fi is among the most successful wireless technologies ever invented. As\nWi-Fi becomes more and more present in public and private spaces, it becomes\nnatural to leverage its ubiquitousness to implement groundbreaking wireless\nsensing applications such as human presence detection, activity recognition,\nand object tracking, just to name a few. This paper reports ongoing efforts by\nthe IEEE 802.11bf Task Group (TGbf), which is defining the appropriate\nmodifications to existing Wi-Fi standards to enhance sensing capabilities\nthrough 802.11-compliant waveforms. We summarize objectives and timeline of\nTGbf, and discuss some of the most interesting proposed technical features\ndiscussed so far. We also introduce a roadmap of research challenges pertaining\nto Wi-Fi sensing and its integration with future Wi-Fi technologies and\nemerging spectrum bands, hoping to elicit further activities by both the\nresearch community and TGbf.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:52:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Restuccia", "Francesco", ""]]}, {"id": "2103.14941", "submitter": "Spyridon Mastorakis", "authors": "Spyridon Mastorakis and Andreas Skiadopoulos and Susmit Shannigrahi\n  and Aaron Likens and Boubakr Nour and Nicholas Stergiou", "title": "Networking and Computing in Biomechanical Research: Challenges and\n  Directions", "comments": "Accepted for publication by the IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomechanics is a scientific discipline that studies the forces acting on a\nbody and the effects they produce. In this paper, we bring together\nbiomechanists and networking researchers to shed light into how research\nefforts in biomechanics, primarily related to the study of the human body, can\nbe facilitated through networking and computing technologies, such as edge and\ncloud computing, Software Defined Networking, and Information-Centric\nNetworking. We first present challenges related to networking and computing\nthat biomechanists face today and we then describe how networking and computing\ntechnologies can address them. Finally, we identify directions for future\nnetworking research with a focus on biomechanics to facilitate and encourage\ninterdisciplinary collaborations between biomechanists and networking\nresearchers.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:31:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mastorakis", "Spyridon", ""], ["Skiadopoulos", "Andreas", ""], ["Shannigrahi", "Susmit", ""], ["Likens", "Aaron", ""], ["Nour", "Boubakr", ""], ["Stergiou", "Nicholas", ""]]}, {"id": "2103.15222", "submitter": "Chia-Hung Lin", "authors": "Chia-Hung Lin, Shih-Chun Lin, and Erik Blasch", "title": "TULVCAN: Terahertz Ultra-broadband Learning Vehicular Channel-Aware\n  Networking", "comments": "This paper was already accepted by the IEEE 2021 INFOCOM Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to spectrum scarcity and increasing wireless capacity demands, terahertz\n(THz) communications at 0.1-10THz and the corresponding spectrum\ncharacterization have emerged to meet diverse service requirements in future 5G\nand 6G wireless systems. However, conventional compressed sensing techniques to\nreconstruct the original wideband spectrum with under-sampled measurements\nbecome inefficient as local spectral correlation is deliberately omitted.\nRecent works extend communication methods with deep learning-based algorithms\nbut lack strong ties to THz channel properties. This paper introduces novel THz\nchannel-aware spectrum learning solutions that fully disclose the uniqueness of\nTHz channels when performing such ultra-broadband sensing in vehicular\nenvironments. Specifically, a joint design of spectrum compression and\nreconstruction is proposed through a structured sensing matrix and two-phase\nreconstruction based on high spreading loss and molecular absorption at THz\nfrequencies. An end-to-end learning framework, namely compression and\nreconstruction network (CRNet), is further developed with the mean-square-error\nloss function to improve sensing accuracy while significantly reducing\ncomputational complexity. Numerical results show that the CRNet solutions\noutperform the latest generative adversarial network (GAN) realization with a\nmuch higher cosine and structure similarity measures, smaller learning errors,\nand 56% less required training overheads. This THz Ultra-broadband Learning\nVehicular Channel-Aware Networking (TULVCAN) work successfully achieves\neffective THz spectrum learning and hence allows frequency-agile access.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 21:16:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lin", "Chia-Hung", ""], ["Lin", "Shih-Chun", ""], ["Blasch", "Erik", ""]]}, {"id": "2103.15367", "submitter": "Xiangyu Zhang", "authors": "Xiangyu Zhang, Zhengming Zhang, and Luxi Yang", "title": "Joint User Association and Power Allocation in Heterogeneous Ultra Dense\n  Network via Semi-Supervised Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Ultra-Dense Network (HUDN) is one of the vital networking\narchitectures due to its ability to enable higher connectivity density and\nultra-high data rates. Rational user association and power control schedule in\nHUDN can reduce wireless interference. This paper proposes a novel idea for\nresolving the joint user association and power control problem: the optimal\nuser association and Base Station transmit power can be represented by channel\ninformation. Then, we solve this problem by formulating an optimal\nrepresentation function. We model the HUDNs as a heterogeneous graph and train\na Graph Neural Network (GNN) to approach this representation function by using\nsemi-supervised learning, in which the loss function is composed of the\nunsupervised part that helps the GNN approach the optimal representation\nfunction and the supervised part that utilizes the previous experience to\nreduce useless exploration. We separate the learning process into two parts,\nthe generalization-representation learning (GRL) part and the\nspecialization-representation learning (SRL) part, which train the GNN for\nlearning representation for generalized scenario quasi-static user distribution\nscenario, respectively. Simulation results demonstrate that the proposed\nGRL-based solution has higher computational efficiency than the traditional\noptimization algorithm, and the performance of SRL outperforms the GRL.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:39:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zhang", "Zhengming", ""], ["Yang", "Luxi", ""]]}, {"id": "2103.15374", "submitter": "Zhenzhen Gong", "authors": "Zhenzhen Gong, Qimei Cui, Christina Chaccour, Bo Zhou, Mingzhe Chen\n  and Walid Saad", "title": "Lifelong Learning for Minimizing Age of Information in Internet of\n  Things Networks", "comments": "6 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a lifelong learning problem is studied for an Internet of\nThings (IoT) system. In the considered model, each IoT device aims to balance\nits information freshness and energy consumption tradeoff by controlling its\ncomputational resource allocation at each time slot under dynamic environments.\nAn unmanned aerial vehicle (UAV) is deployed as a flying base station so as to\nenable the IoT devices to adapt to novel environments. To this end, a new\nlifelong reinforcement learning algorithm, used by the UAV, is proposed in\norder to adapt the operation of the devices at each visit by the UAV. By using\nthe experience from previously visited devices and environments, the UAV can\nhelp devices adapt faster to future states of their environment. To do so, a\nknowledge base shared by all devices is maintained at the UAV. Simulation\nresults show that the proposed algorithm can converge $25\\%$ to $50\\%$ faster\nthan a policy gradient baseline algorithm that optimizes each device's decision\nmaking problem in isolation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:02:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gong", "Zhenzhen", ""], ["Cui", "Qimei", ""], ["Chaccour", "Christina", ""], ["Zhou", "Bo", ""], ["Chen", "Mingzhe", ""], ["Saad", "Walid", ""]]}, {"id": "2103.15450", "submitter": "Emmanouil Fountoulakis", "authors": "Emmanouil Fountoulakis, Marian Codreanu, Anthony Ephremides, Nikolaos\n  Pappas", "title": "Joint Sampling and Transmission Policies for Minimizing Cost under AoI\n  Constraints", "comments": "30 pages, submitted on a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we consider the problem of jointly minimizing the average cost\nof sampling and transmitting status updates by users over a wireless channel\nsubject to average Age of Information (AoI) constraints. Errors in the\ntransmission may occur and a scheduling policy has to decide if the users\nsample a new packet or attempt for retransmission of the packet sampled\npreviously. The cost consists of both sampling and transmission costs. The\nsampling of a new packet after a failure imposes an additional cost on the\nsystem. We formulate a stochastic optimization problem with the average cost in\nthe objective under average AoI constraints. To solve this problem, we propose\nthree scheduling policies; a) a dynamic policy, that is centralized and\nrequires full knowledge of the state of the system, b) two stationary\nrandomized policies that require no knowledge of the state of the system. We\nutilize tools from Lyapunov optimization theory in order to provide the dynamic\npolicy, and we prove that its solution is arbitrary close to the optimal one.\nIn order to provide the randomized policies, we model the system by utilizing\nDiscrete Time Markov Chain (DTMC). We provide closed-form and approximated\nexpressions for the average AoI and its distribution, for each randomized\npolicy. Simulation results show the importance of providing the option to\ntransmit an old packet in order to minimize the total average cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:33:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fountoulakis", "Emmanouil", ""], ["Codreanu", "Marian", ""], ["Ephremides", "Anthony", ""], ["Pappas", "Nikolaos", ""]]}, {"id": "2103.15458", "submitter": "Dimitris Tsakalidis", "authors": "George Domalis, Nikos Karacapilidis, Dimitris Tsakalidis, Anastasios\n  Giannaros", "title": "A trustable and interoperable decentralized solution for citizen-centric\n  and cross-border eGovernance: A conceptual approach", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aiming to support a cross-sector and cross-border eGovernance paradigm for\nsharing common public services, this paper introduces an AI-enhanced solution\nthat enables beneficiaries to participate in a decenntralized network for\neffective big data exchange and service delivery that promotes the once-only\npriority and is by design digital, efficient, cost-effective, interoperable and\nsecure. The solution comprises (i) a reliable and efficient decentralized\nmechanism for data sharing, capable of addressing the complexity of the\nprocesses and their high demand of resources; (ii) an ecosystem for delivering\nmobile services tailored to the needs of stakeholders; (iii) a single sign-on\nWallet mechanism to manage the transactions with multiple services; and (iv) an\nintercommunication layer, responsible for the secure exchange of information\namong existing eGovernment systems with newly developed ones. An indicative\napplication scenario showcases the potential of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:44:52 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:25:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Domalis", "George", ""], ["Karacapilidis", "Nikos", ""], ["Tsakalidis", "Dimitris", ""], ["Giannaros", "Anastasios", ""]]}, {"id": "2103.15591", "submitter": "Vanlin Sathya", "authors": "Vanlin Sathya, Muhammad Iqbal Rochman, and Monisha Ghosh", "title": "Hidden-nodes in coexisting LAA & Wi-Fi: a measurement study of real\n  deployments", "comments": "IEEE ICC 2021 Workshop on Spectrum Sharing Technology for\n  Next-Generation Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LTE-Licensed Assisted Access (LAA) networks are beginning to be deployed\nwidely in major metropolitan areas in the US in the unlicensed 5 GHz bands,\nwhich have existing dense deployments of Wi-Fi. This provides a real-world\nopportunity to study the problems due to hidden-node scenarios between LAA and\nWi-Fi. The hidden node problem has been well studied in the context of\noverlapping Wi-Fi APs. However, when Wi-Fi coexists with LAA, the hidden node\nproblem is exacerbated since LAA cannot use the well-known Request-to-Send\n(RTS)/Clear to-Send (CTS) mechanism to resolve contentions, resulting in\nthroughput degradation for Wi-Fi. In this paper, we describe detailed\nmeasurements and conclusions from experiments on the campus of the University\nof Chicago which presents a perfect hidden node scenario where Wi-Fi access\npoints (APs) controlled by us and an LAA base-station (BS) deployed by AT&T are\nhidden from each other, but the clients are not. We performed careful\nexperiments in three different regions of the coexistence area: (i) clients\nmidway between LAA & Wi-Fi; (ii) clients close to the Wi-Fi AP; and (iii)\nclients close to the LAA BS. Our results show that in a situation where LAA\nuses an aggregate of three unlicensed channels (60 MHz bandwidth) which overlap\nwith an 80 MHz Wi-Fi transmission, the Wi-Fi throughput at client devices\nsuffers considerably. Overall, Wi-Fi performance is impacted by the hidden node\nproblem more severely than LAA. In the best outdoor conditions, the throughput\nof LAA and Wi-Fi is reduced by 35% and 97% respectively when coexisting with\neach other as compared when the other system is not present. Furthermore, we\nconclude that when both LAA and Wi-Fi use multiple 20 MHz channels and there\nare multiple Wi-Fi APs coexisting with LAA on the same set of channels, the\nchoice of Wi-Fi primary channels can have a significant impact on LAA\nthroughput.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:13:45 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 19:13:02 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sathya", "Vanlin", ""], ["Rochman", "Muhammad Iqbal", ""], ["Ghosh", "Monisha", ""]]}, {"id": "2103.15757", "submitter": "Celso Carvalho", "authors": "Thales Ruano Barros de Souza, Gabriel Goes Rodrigues, Luan da Silva\n  Serrao, Renata do Nascimento Mota Macambira, Celso Barbosa Carvalho", "title": "Residential smart plug with bluetooth communication", "comments": null, "journal-ref": "ITEGAM-JETIA, 6(21), p. 20-30 (2020)", "doi": "10.5935/2447-0228.20200003", "report-no": null, "categories": "eess.SP cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electricity forms the backbone of the modern world but increasing energy\ndemand with the growth of urban areas in recent decades has overwhelmed the\ncurrent power grid ecosystem. So, there is a need to move towards a more\nefficient and interconnected smart grid infrastructure. The growing popularity\nof the Internet of Things(IoT) has increased the demand for smart and connected\ndevices. In this work we developed a hardware device based on the ATmega2560\nmicrocontroller that can estimate the power consumption and control the state\nof electro-electronic devices interconnected to it through Bluetooth wireless\ntechnology. The developed hardware is a smart plug focusing on smart home\napplications. As a result, by using a smartphone device with Bluetooth\ncommunication, one can control and measure electrical parameters of the\ninterconnected electro-electronic hardware such as the RMS (Root Mean Square)\ncurrent and RMS power been consumed. The obtained results showed the technical\nviability in the construction of energy consumption measuring device using\nmodules and components available in the Brazilian market.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 21:00:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["de Souza", "Thales Ruano Barros", ""], ["Rodrigues", "Gabriel Goes", ""], ["Serrao", "Luan da Silva", ""], ["Macambira", "Renata do Nascimento Mota", ""], ["Carvalho", "Celso Barbosa", ""]]}, {"id": "2103.15896", "submitter": "Petros Spachos", "authors": "Marc Jayson Baucas, Stephen Andrew Gadsden, and Petros Spachos", "title": "IoT-based Smart Home Device Monitor Using Private Blockchain Technology\n  and Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of Things (IoT)-based smart home applications are rising in\npopularity. However, this trend attracts malicious activity, which causes\ncost-efficient security to be in high demand. This paper proposes a low-end\ndesign that reinforces the security of a home network. It uses private\nblockchain technology and localization via RSSI-based trilateration. We\ninvestigated the benefits of private blockchains over their public counterpart,\nand we improve the precision of the localization algorithm by testing it\nagainst different wireless technologies. The results conclude that using a\nprivate blockchain with a WiFi-based communication system produces the most\nefficient iteration of the proposed design.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:07:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Baucas", "Marc Jayson", ""], ["Gadsden", "Stephen Andrew", ""], ["Spachos", "Petros", ""]]}, {"id": "2103.15924", "submitter": "Boubakr Nour", "authors": "Boubakr Nour and Soumaya Cherkaoui", "title": "How Far Can We Go in Compute-less Networking: Computation Correctness\n  and Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emerging applications such as augmented reality and the tactile Internet are\ncompute-intensive and latency-sensitive, which hampers their running in\nconstrained end devices alone or in the distant cloud. The stringent\nrequirements of such application drove to the realization of Edge computing in\nwhich computation is offloaded near to users. Moreover, these applications are\noften executed with similar input data that yield the same output. Compute-less\nnetworking is an extension of edge computing that aims at reducing computation\nand abridging communication by adopting in-network computing and computation\nreuse. Calculation-reuse aims to cache the result of calculations, and use them\nto perform similar tasks in the future and, therefore, avoid redundant\ncalculations and optimize the use of resources. Since the input might not be\nidentical but similar, the reuse of previous computation raises questions on\nthe correctness and accuracy of the final results. In this paper, we study the\ncorrectness of output data in the computation reuse concept and gauge the\neffectiveness and efficiency of compute-less networking.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:57:16 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Nour", "Boubakr", ""], ["Cherkaoui", "Soumaya", ""]]}, {"id": "2103.16117", "submitter": "Sharvari N P", "authors": "Sharvari N P, Dibakar Das, Jyotsna Bapat, Debabrata Das", "title": "Connectivity and Collision Constrained Opportunistic Routing for\n  Emergency Communication using UAV", "comments": "11 pages, 14 figures, submitted to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency communication is extremely important to aid rescue and search\noperation in the aftermath of any disaster. In such scenario, Unmanned Aerial\nVehicle (UAV) networks may be used to complement the damaged cellular networks\nover large areas. However, in such UAV networks, routing is a challenge, owing\nto high UAV mobility, intermittent link quality between UAVs, dynamic three\ndimensional (3D) UAV topology and resource constraints. Though several UAV\nrouting approaches have been proposed, none of them so far have addressed inter\nUAV coverage, collision and routing in an integrated manner. In this paper, we\nconsider a scenario where network of UAVs, operating at different heights from\nground, with inter UAV coverage and collision constraints, are sent on a\nmission to collect disaster surveillance data and route it to Terrestrial Base\nStation via multi-hop UAV path. Analytical expressions for coverage probability\n(Pcov) and collision probability (Pcoll) are derived and minimum (Rmin) and\nmaximum (Rmax) distance between UAVs are empirically calculated. We then\npropose a novel Multi-hop Opportunistic 3D Routing (MO3DR) algorithm with inter\nUAV coverage and collision constraints such that at every hop expected progress\nof data packet is maximized. The numerical results obtained from closed form\nmathematical modelling are validated through extensive simulation and their\ntrade-off with variation in network parameters such as path loss component,\ntrajectory divergence etc. are demonstrated. Finally, we obtain empirical\noptimality condition for inter UAV distance for the given application\nrequirement.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:08:44 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["P", "Sharvari N", ""], ["Das", "Dibakar", ""], ["Bapat", "Jyotsna", ""], ["Das", "Debabrata", ""]]}, {"id": "2103.16295", "submitter": "Siamak Layeghy", "authors": "Seyedehfaezeh Hosseininoorbin, Siamak Layeghy, Mohanad Sarhan, Raja\n  Jurdak, Marius Portmann", "title": "Exploring Edge TPU for Network Intrusion Detection in IoT", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores Google's Edge TPU for implementing a practical network\nintrusion detection system (NIDS) at the edge of IoT, based on a deep learning\napproach. While there are a significant number of related works that explore\nmachine learning based NIDS for the IoT edge, they generally do not consider\nthe issue of the required computational and energy resources. The focus of this\npaper is the exploration of deep learning-based NIDS at the edge of IoT, and in\nparticular the computational and energy efficiency. In particular, the paper\nstudies Google's Edge TPU as a hardware platform, and considers the following\nthree key metrics: computation (inference) time, energy efficiency and the\ntraffic classification performance. Various scaled model sizes of two major\ndeep neural network architectures are used to investigate these three metrics.\nThe performance of the Edge TPU-based implementation is compared with that of\nan energy efficient embedded CPU (ARM Cortex A53). Our experimental evaluation\nshows some unexpected results, such as the fact that the CPU significantly\noutperforms the Edge TPU for small model sizes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:43:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hosseininoorbin", "Seyedehfaezeh", ""], ["Layeghy", "Siamak", ""], ["Sarhan", "Mohanad", ""], ["Jurdak", "Raja", ""], ["Portmann", "Marius", ""]]}, {"id": "2103.16329", "submitter": "Siamak Layeghy", "authors": "Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher, Marius\n  Portmann", "title": "E-GraphSAGE: A Graph Neural Network based Intrusion Detection System", "comments": "13 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new network intrusion detection system (NIDS) based on\nGraph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep\nneural networks, which have the unique ability to leverage the inherent\nstructure of graph-based data. Training and evaluation data for NIDSs are\ntypically represented as flow records, which can naturally be represented in a\ngraph format. This establishes the potential and motivation for exploring GNNs\nfor the purpose of network intrusion detection, which is the focus of this\npaper. E-GraphSAGE, our proposed new approach is based on the established\nGraphSAGE model, but provides the necessary modifications in order to support\nedge features for edge classification, and hence the classification of network\nflows into benign and attack classes. An extensive experimental evaluation\nbased on six recent NIDS benchmark datasets shows the excellent performance of\nour E-GraphSAGE based NIDS in comparison with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:21:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:43:02 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:01:00 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 01:03:35 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lo", "Wai Weng", ""], ["Layeghy", "Siamak", ""], ["Sarhan", "Mohanad", ""], ["Gallagher", "Marcus", ""], ["Portmann", "Marius", ""]]}, {"id": "2103.16437", "submitter": "Simon Kassing", "authors": "Simon Kassing, Hussain Abbas, Laurent Vanbever, Ankit Singla", "title": "Order P4-66: Characterizing and mitigating surreptitious programmable\n  network device exploitation", "comments": "14 pages, 13 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial efforts are invested in improving network security, but the\nthreat landscape is rapidly evolving, particularly with the recent interest in\nprogrammable network hardware. We explore a new security threat, from an\nattacker who has gained control of such devices. While it should be obvious\nthat such attackers can trivially cause substantial damage, the challenge and\nnovelty are in doing so while preventing quick diagnosis by the operator.\n  We find that compromised programmable devices can easily degrade networked\napplications by orders of magnitude, while evading diagnosis by even the most\nsophisticated network diagnosis methods in deployment. Two key observations\nyield this result: (a) targeting a small number of packets is often enough to\ncause disproportionate performance degradation; and (b) new programmable\nhardware is an effective enabler of careful, selective targeting of packets.\nOur results also point to recommendations for minimizing the damage from such\nattacks, ranging from known, easy to implement techniques like encryption and\nredundant requests, to more complex considerations that would potentially limit\nsome intended uses of programmable hardware. For data center contexts, we also\ndiscuss application-aware monitoring and response as a potential mitigation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:34:37 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:51:53 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kassing", "Simon", ""], ["Abbas", "Hussain", ""], ["Vanbever", "Laurent", ""], ["Singla", "Ankit", ""]]}, {"id": "2103.16444", "submitter": "Roberto Doriguzzi Corin", "authors": "Sajad Khorsandroo, Adrian Gallego Sanchez, Ali Saman Tosun, Jose'\n  Manuel Arco Rodriguez, Roberto Doriguzzi-Corin", "title": "Hybrid SDN Evolution: A Comprehensive Survey of the State-of-the-Art", "comments": null, "journal-ref": null, "doi": "10.1016/j.comnet.2021.107981", "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-Defined Networking (SDN) is an evolutionary networking paradigm\nwhich has been adopted by large network and cloud providers, among which are\nTech Giants. However, embracing a new and futuristic paradigm as an alternative\nto well-established and mature legacy networking paradigm requires a lot of\ntime along with considerable financial resources and technical expertise.\nConsequently, many enterprises can not afford it. A compromise solution then is\na hybrid networking environment (a.k.a. Hybrid SDN (hSDN)) in which SDN\nfunctionalities are leveraged while existing traditional network\ninfrastructures are acknowledged. Recently, hSDN has been seen as a viable\nnetworking solution for a diverse range of businesses and organizations.\nAccordingly, the body of literature on hSDN research has improved remarkably.\nOn this account, we present this paper as a comprehensive state-of-the-art\nsurvey which expands upon hSDN from many different perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:44:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Khorsandroo", "Sajad", ""], ["Sanchez", "Adrian Gallego", ""], ["Tosun", "Ali Saman", ""], ["Rodriguez", "Jose' Manuel Arco", ""], ["Doriguzzi-Corin", "Roberto", ""]]}, {"id": "2103.16683", "submitter": "Roza Goscien", "authors": "R\\'o\\.za Go\\'scie\\'n", "title": "On the Efficient Design of Network Resilient to Electro-Magnetic Pulse\n  Attack -- Elastic Optical Network Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The telecommunication networks have become an indispensable part of our\neveryday life, providing support for such important areas as business,\neducation, health care, finances, entertainment and social life. Alongside\ntheir continuous and uninterrupted operation is required while numerous new\nthreats and attack scenarios emerge. The international security organisations\nwarn against increasing likelihood of nuclear weapon or electro-magnetic pulse\n(EMP) attacks, which can be extremely harmful also for transport networks. On\nthat background, we study efficient design of network resilient to EMP attack\nwherein the required protection level is provided by the application of\nmultipath routing and military grade bunkers (advanced electro-magnetic\nradiation resilient approaches protecting whole network node) implementation.\nFormally, we define and study problem of bunkers location, routing and spectrum\nallocation (BLRSA) in elastic optical network (EON). In the problem objective\nwe address two criteria - network resilience (measured by the average lost flow\nper potential attack) and spectrum usage. For that problem we propose integer\nlinear programming (ILP) model and two dedicated heuristics - 1S-RSA and\n2S-RSA. Then, we perform extensive numerical experiments divided into three\nparts: (i) tuning of the proposed approaches, (ii) comparison with reference\nmethods, (iii) realistic case study - efficient EMP-resilient network design.\nIn the case study we analyze benefits and costs of the proposed protection\nscheme. Moreover, we also analyze vulnerabilities of three realistic network\ntopologies to EMP attacks and identify their critical nodes. The investigation\nproves high efficiency of the proposed approaches and shows that they allow to\nsave up to 90% of traffic lost in the case of no protection against these types\nof attacks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:11:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Go\u015bcie\u0144", "R\u00f3\u017ca", ""]]}, {"id": "2103.16717", "submitter": "Derya Malak", "authors": "Derya Malak", "title": "Applications of Common Information to Computing Functions", "comments": "submitted for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a low complexity distributed compression scheme for computing\narbitrary functions of sources with discrete alphabets. We use a helper-based\nmethod that extends the definition of the G{\\'a}cs-K{\\\"o}rner-Witsenhausen\n(GKW) common information to functional common information. The helper relaxes\nthe combinatorial structure of GKW by partitioning the joint source\ndistribution into nests imposed by the function, which ensures hierarchical\ncooperation between the sources for effectively distributed computing. By\ncontrasting our approach's performance with existing efficient techniques, we\ndemonstrate the rate savings in recovering function and source data.\n  Permutation invariant functions are prevalent in learning and combinatorial\noptimization fields and most recently applied to graph neural networks. We\nconsider the efficient compression for computing permutation invariant\nfunctions in a network with two sources and one decoder. We use a bipartite\ngraph representation, where two disjoint sets of vertices (parts) denote the\nindividual source graphs and the edge weights capture the joint source\ndistribution. We compress bipartite graphs by creating connected components\ndetermined by the function's distribution, accounting for the edge symmetries,\nand eliminating the low probability edges. We separately encode those edges and\nsend them as refinements. Our approach can substitute high complexity joint\ndecoding techniques and inform neural networks to reduce the computing time and\nreduce complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:10:36 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Malak", "Derya", ""]]}, {"id": "2103.16766", "submitter": "Yu Wang Mr", "authors": "Yu Wang, Ying Chen, Yunfei Qiao, Hejia Luo, Xiaolu Wang, Rong Li, Jun\n  Wang", "title": "Cooperative Beam Hopping for Accurate Positioning in Ultra-Dense LEO\n  Satellite Networks", "comments": "6 pages, Accepted by ICC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ultra-dense LEO satellite networks, conventional communication-oriented\nbeam pattern design cannot provide multiple favorable signals from different\nsatellites simultaneously, and thus leads to poor positioning performance. To\ntackle this issue, in this paper, we propose a novel cooperative beam hopping\n(BH) framework to adaptively tune beam layouts suitable for multi-satellite\ncoordinated positioning. On this basis, a joint user association, BH design and\npower allocation optimization problem is formulated to minimize average\nCram\\'er-Rao lower bound (CRLB). An efficient flexible BH control algorithm\n(FBHCA) is then proposed to solve the problem. Finally, a thorough experimental\nplatform is built following the Third Generation Partnership Project (3GPP)\ndefined non-terrestrial network (NTN) simulation parameters to validate the\nperformance gain of the devised algorithm. The numerical results demonstrate\nthat FBHCA can significantly improve CRLB performance over the benchmark\nscheme.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:14:53 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Yu", ""], ["Chen", "Ying", ""], ["Qiao", "Yunfei", ""], ["Luo", "Hejia", ""], ["Wang", "Xiaolu", ""], ["Li", "Rong", ""], ["Wang", "Jun", ""]]}, {"id": "2103.16920", "submitter": "Hossein Pakdel", "authors": "Reza Fotohi and Hossein Pakdel", "title": "A Lightweight and Scalable Physical Layer Attack Detection Mechanism for\n  the Internet of Things (IoT) Using Hybrid Security Schema", "comments": "20 pages, 6 Figures, 8 Tables, Wireless Pers Commun (2021)", "journal-ref": null, "doi": "10.1007/s11277-021-08388-1", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Internet of Things, also known as the IoT, refers to the billions of\ndevices around the world that are now connected to the Internet, collecting and\nsharing data. The amount of data collected through IoT sensors must be\ncompletely securely controlled. To protect the information collected by IoT\nsensors, a lightweight method called Discover the Flooding Attack-RPL (DFA-RPL)\nhas been proposed. The proposed DFA-RPL method identifies intrusive nodes in\nseveral steps to exclude them from continuing routing operations. Thus, in the\nDFA-RPL method, it first builds a cluster and selects the most appropriate node\nas a cluster head in DODAG, then, due to the vulnerability of the RPL protocol\nto Flooding attacks, it uses an ant colony algorithm (ACO) using five steps to\ndetect attacks. Use Flooding to prevent malicious activity on the IoT network.\nIn other words, if it detects a node as malicious, it puts that node on the\ndetention list and quarantines it for a certain period of time. The results\nobtained from the simulation show the superiority of the proposed method in\nterms of Packet Delivery Rate, Detection Rate, False Positive Rate, and False\nNegative Rate compared to IRAD and REATO methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:18:06 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Fotohi", "Reza", ""], ["Pakdel", "Hossein", ""]]}, {"id": "2103.16981", "submitter": "Bjoern Annighoefer", "authors": "Bjoern Annighoefer, Adrian Zeyher, Johannes Reinhart", "title": "Multi-core Fiber and Power-limited Optical Network Topology Optimization\n  with MILP", "comments": "changes: typos corrected, figure and table formating improved 18\n  pages, 15 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical networks with multi-core fibers can replace several electronics\nnetworks with a single topology. Each electronic link is replaced by a single\nfiber, which can save space, weight, and cost, while having better segregation\nand EMI resistance. This is, for instance, of high interest in safety-critical\ncyber-physical systems, such as aircraft avionics networks. Finding the optimal\ntopology requires finding the optimal number of components, component\nlocations, inter-meshing, and signal routing, while assuring the appropriate\noptical power level at each participating device. A Mixed-integer Linear\nProgramming (MILP) representation is presented for the optimization of the\ntopology of optical multi-core fiber networks. The optimization approach\nretrieves a globally optimal topology with respect to weight or cost, i.e. it\nbuilds the optimal network topology from a set of switch and cable types, which\ndiffer in the number of fibers, attenuation, connectors, and other properties.\nThe novelty of our approach is the consideration of translucent and opaque\noptical switches as well as the representation of cable and device attenuation\ndirectly in the MILP constraints. Moreover, arbitrary installation and routing\nresource restrictions are considered. The application of the approach to five\ndedicated scenarios yields in each case an optimal solution and validates our\nmethod. The application to an excerpt of an aircraft cabin network shows the\nretrieval of the global optimum in less 30 min for a topology with 48 signals\nand 23 components.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 10:59:39 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 21:53:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Annighoefer", "Bjoern", ""], ["Zeyher", "Adrian", ""], ["Reinhart", "Johannes", ""]]}, {"id": "2103.16985", "submitter": "Mohamed Sana", "authors": "Mohamed Sana, Mattia Merluzzi, Nicola di Pietro, Emilio Calvanese\n  Strinati", "title": "Energy Efficient Edge Computing: When Lyapunov Meets Distributed\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of energy-efficient computation offloading\nenabled by edge computing. In the considered scenario, multiple users\nsimultaneously compete for limited radio and edge computing resources to get\noffloaded tasks processed under a delay constraint, with the possibility of\nexploiting low power sleep modes at all network nodes. The radio resource\nallocation takes into account inter- and intra-cell interference, and the duty\ncycles of the radio and computing equipment have to be jointly optimized to\nminimize the overall energy consumption. To address this issue, we formulate\nthe underlying problem as a dynamic long-term optimization. Then, based on\nLyapunov stochastic optimization tools, we decouple the formulated problem into\na CPU scheduling problem and a radio resource allocation problem to be solved\nin a per-slot basis. Whereas the first one can be optimally and efficiently\nsolved using a fast iterative algorithm, the second one is solved using\ndistributed multi-agent reinforcement learning due to its non-convexity and\nNP-hardness. The resulting framework achieves up to 96.5% performance of the\noptimal strategy based on exhaustive search, while drastically reducing\ncomplexity. The proposed solution also allows to increase the network's energy\nefficiency compared to a benchmark heuristic approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:02:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sana", "Mohamed", ""], ["Merluzzi", "Mattia", ""], ["di Pietro", "Nicola", ""], ["Strinati", "Emilio Calvanese", ""]]}, {"id": "2103.17091", "submitter": "David M\\\"odinger", "authors": "David M\\\"odinger and Alexander He{\\ss} and Franz J. Hauck", "title": "Arbitrary Length k-Anonymous Dining-Cryptographers Communication", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dining-cryptographers networks (DCN) can achieve information-theoretical\nprivacy. Unfortunately, they are not well suited for peer-to-peer networks as\nthey are used in blockchain applications to disseminate transactions and blocks\namong participants. In previous but preliminary work, we proposed a threephase\napproach with an initial phase based on a DCN with a group size of k while\nlater phases take care of the actual broadcast within a peer-to-peer network.\nThis paper describes our DCN protocol in detail and adds a performance\nevaluation powered by our proof-of-concept implementation. Our contributions\nare (i) an extension of the DCN protocol by von Ahn for fair delivery of\narbitrarily long messages sent by potentially multiple senders, (ii) a privacy\nand security analysis of this extension, (iii) various performance optimisation\nespecially for best-case operation, and (iv) a performance evaluation. The\nlatter uses a latency of 100 ms and a bandwidth limit of 50 Mbit/s between\nparticipants. The interquartile range of the largest test of the highly secured\nversion took 35s+-1.25s for a full run. All tests of the optimized common-case\nmode show the dissemination of a message within 0.5s+-0.1s. These results\ncompare favourably to previously established protocols for k-anonymous\ntransmission of fixed size messages, outperforming the original protocol for\nmessages as small as 2 KiB.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:03:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["M\u00f6dinger", "David", ""], ["He\u00df", "Alexander", ""], ["Hauck", "Franz J.", ""]]}, {"id": "2103.17207", "submitter": "Nikolaos Papadis", "authors": "Nikolaos Papadis, Leandros Tassiulas", "title": "State-Dependent Processing in Payment Channel Networks for Throughput\n  Optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.NI cs.SI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Payment channel networks (PCNs) have emerged as a scalability solution for\nblockchains built on the concept of a payment channel: a setting that allows\ntwo nodes to safely transact between themselves in high frequencies based on\npre-committed peer-to-peer balances. Transaction requests in these networks may\nbe declined because of unavailability of funds due to temporary uneven\ndistribution of the channel balances. In this paper, we investigate how to\nalleviate unnecessary payment blockage via proper prioritization of the\ntransaction execution order. Specifically, we consider the scheduling problem\nin PCNs: as transactions continuously arrive on both sides of a channel, nodes\nneed to decide which ones to process and when in order to maximize their\nobjective, which in our case is the channel throughput. We introduce a\nstochastic model to capture the dynamics of a payment channel under random\narrivals, and propose that channels can hold incoming transactions in buffers\nup to some deadline in order to enable more elaborate processing decisions. We\ndescribe a policy that maximizes the channel success rate/throughput for\nuniform transaction requests of fixed amounts, both in the presence and absence\nof buffering capabilities, and formally prove its optimality. We also develop a\ndiscrete event simulator of a payment channel, and evaluate different heuristic\nscheduling policies in the more general heterogeneous amounts case, with the\nresults showing superiority of the heuristic extension of our policy in this\ncase as well. Our work opens the way for more formal research on improving PCN\nperformance via joint consideration of routing and scheduling decisions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:40:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Papadis", "Nikolaos", ""], ["Tassiulas", "Leandros", ""]]}, {"id": "2103.17221", "submitter": "Viviana Arrigoni", "authors": "Viviana Arrigoni, Novella Bartolini, Annalisa Massini, Federico\n  Trombetti", "title": "Static and Dynamic Failure Localization through Progressive Network\n  Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We aim at assessing the states of the nodes in a network by means of\nend-to-end monitoring paths. The contribution of this paper is twofold. First,\nwe consider a static failure scenario. In this context, we aim at minimizing\nthe number of probes to obtain failure identification. To face this problem, we\npropose a progressive approach to failure localization based on stochastic\noptimization, whose solution is the optimal sequence of monitoring paths to\nprobe.\n  We address the complexity of the problem by proposing a greedy strategy in\ntwo variants: one considers exact calculation of posterior probabilities of\nnode failures, given the observation, whereas the other approximates these\nvalues by means of a novel failure centrality metric. Secondly, we adapt these\ntwo strategies to a dynamic failure scenario where nodes states can change\nthroughout a monitoring period. By means of numerical experiments conducted on\nreal network topologies, we demonstrate the practical applicability of our\napproach. Our performance evaluation evidences the superiority of our\nalgorithms with respect to state of the art solutions based on classic Boolean\nNetwork Tomography as well as approaches based on sequential group testing.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:14:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Arrigoni", "Viviana", ""], ["Bartolini", "Novella", ""], ["Massini", "Annalisa", ""], ["Trombetti", "Federico", ""]]}]