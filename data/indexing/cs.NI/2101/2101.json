[{"id": "2101.00172", "submitter": "Daniel Szelogowski", "authors": "Daniel Szelogowski", "title": "Chunk List: Concurrent Data Structures", "comments": "20 pages, 3 figures A full implementation can be found at\n  https://github.com/danielathome19/Chunk-List Update: Revised format to align\n  closer to IEEE standards", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chunking data is obviously no new concept; however, I had never found any\ndata structures that used chunking as the basis of their implementation. I\nfigured that by using chunking alongside concurrency, I could create an\nextremely fast run-time in regards to particular methods as searching and/or\nsorting. By using chunking and concurrency to my advantage, I came up with the\nchunk list - a dynamic list-based data structure that would separate large\namounts of data into specifically sized chunks, each of which should be able to\nbe searched at the exact same time by searching each chunk on a separate\nthread. As a result of implementing this concept into its own class, I was able\nto create something that almost consistently gives around 20x-300x faster\nresults than a regular ArrayList. However, should speed be a particular issue\neven after implementation, users can modify the size of the chunks and\nbenchmark the speed of using smaller or larger chunks, depending on the amount\nof data being stored.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 05:45:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:32:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Szelogowski", "Daniel", ""]]}, {"id": "2101.00191", "submitter": "Yuris Mulya Saputra", "authors": "Yuris Mulya Saputra, Dinh Thai Hoang, Diep N. Nguyen, Le-Nam Tran,\n  Shimin Gong, and Eryk Dutkiewicz", "title": "Dynamic Federated Learning-Based Economic Framework for\n  Internet-of-Vehicles", "comments": "18 pages, 12 figures, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) can empower Internet-of-Vehicles (IoV) networks by\nleveraging smart vehicles (SVs) to participate in the learning process with\nminimum data exchanges and privacy disclosure. The collected data and learned\nknowledge can help the vehicular service provider (VSP) improve the global\nmodel accuracy, e.g., for road safety as well as better profits for both VSP\nand participating SVs. Nonetheless, there exist major challenges when\nimplementing the FL in IoV networks, such as dynamic activities and diverse\nquality-of-information (QoI) from a large number of SVs, VSP's limited payment\nbudget, and profit competition among SVs. In this paper, we propose a novel\ndynamic FL-based economic framework for an IoV network to address these\nchallenges. Specifically, the VSP first implements an SV selection method to\ndetermine a set of the best SVs for the FL process according to the\nsignificance of their current locations and information history at each\nlearning round. Then, each selected SV can collect on-road information and\noffer a payment contract to the VSP based on its collected QoI. For that, we\ndevelop a multi-principal one-agent contract-based policy to maximize the\nprofits of the VSP and learning SVs under the VSP's limited payment budget and\nasymmetric information between the VSP and SVs. Through experimental results\nusing real-world on-road datasets, we show that our framework can converge 57%\nfaster (even with only 10% of active SVs in the network) and obtain much higher\nsocial welfare of the network (up to 27.2 times) compared with those of other\nbaseline FL methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 08:11:18 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 06:32:58 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Saputra", "Yuris Mulya", ""], ["Hoang", "Dinh Thai", ""], ["Nguyen", "Diep N.", ""], ["Tran", "Le-Nam", ""], ["Gong", "Shimin", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "2101.00256", "submitter": "Pengyuan Zhou", "authors": "Pengyuan Zhou, Benjamin Finley, Xuebing Li, Sasu Tarkoma, Jussi\n  Kangasharju, Mostafa Ammar, Pan Hui", "title": "5G MEC Computation Handoff for Mobile Augmented Reality", "comments": "Submitted to Mobihoc'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The combination of 5G and Multi-access Edge Computing (MEC) can significantly\nreduce application delay by lowering transmission delay and bringing\ncomputational capabilities closer to the end user. Therefore, 5G MEC could\nenable excellent user experience in applications like Mobile Augmented Reality\n(MAR), which are computation-intensive, and delay and jitter-sensitive.\nHowever, existing 5G handoff algorithms often do not consider the computational\nload of MEC servers, are too complex for real-time execution, or do not\nintegrate easily with the standard protocol stack. Thus they can impair the\nperformance of 5G MEC. To address this gap, we propose Comp-HO, a handoff\nalgorithm that finds a local solution to the joint problem of optimizing signal\nstrength and computational load. Additionally, Comp-HO can easily be integrated\ninto current LTE and 5G base stations thanks to its simplicity and\nstandard-friendly deployability. Specifically, we evaluate Comp-HO through a\ncustom NS-3 simulator which we calibrate via MAR prototype measurements from a\nreal-world 5G testbed. We simulate both Comp-HO and several classic handoff\nalgorithms. The results show that, even without a global optimum, the proposed\nalgorithm still significantly reduces the number of large delays, caused by\ncongestion at MECs, at the expense of a small increase in transmission delay.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 15:56:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Pengyuan", ""], ["Finley", "Benjamin", ""], ["Li", "Xuebing", ""], ["Tarkoma", "Sasu", ""], ["Kangasharju", "Jussi", ""], ["Ammar", "Mostafa", ""], ["Hui", "Pan", ""]]}, {"id": "2101.00257", "submitter": "Bin Li", "authors": "Bin Li", "title": "Efficient Learning-based Scheduling for Information Freshness in\n  Wireless Networks", "comments": "This paper has been accepted by IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the recent trend of integrating artificial intelligence into the\nInternet-of-Things (IoT), we consider the problem of scheduling packets from\nmultiple sensing sources to a central controller over a wireless network. Here,\npackets from different sensing sources have different values or degrees of\nimportance to the central controller for intelligent decision making. In such a\nsetup, it is critical to provide timely and valuable information for the\ncentral controller. In this paper, we develop a parameterized maximum-weight\ntype scheduling policy that combines both the AoI metrics and Upper Confidence\nBound (UCB) estimates in its weight measure with parameter $\\eta$. Here, UCB\nestimates balance the tradeoff between exploration and exploitation in learning\nand are critical for yielding a small cumulative regret. We show that our\nproposed algorithm yields the running average total age at most by\n$O(N^2\\eta)$. We also prove that our proposed algorithm achieves the cumulative\nregret over time horizon $T$ at most by $O(NT/\\eta+\\sqrt{NT\\log T})$. This\nreveals a tradeoff between the cumulative regret and the running average total\nage: when increasing $\\eta$, the cumulative regret becomes smaller, but is at\nthe cost of increasing running average total age. Simulation results are\nprovided to evaluate the efficiency of our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 15:59:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Bin", ""]]}, {"id": "2101.00341", "submitter": "Jihong Park", "authors": "Hyesung Kim, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, M\\'erouane\n  Debbah", "title": "Mean-Field Game-Theoretic Edge Caching", "comments": "26 pages, 9 figures; This chapter is written for the forthcoming\n  book, Edge Caching for Mobile Networks (IET), edited by W. Chen and H. V.\n  Poor. arXiv admin note: substantial text overlap with arXiv:1801.07367", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this book chapter, we study a problem of distributed content caching in an\nultra-dense edge caching network (UDCN), in which a large number of small base\nstations (SBSs) prefetch popular files to cope with the ever-growing user\ndemand in 5G and beyond. In a UDCN, even a small misprediction of user demand\nmay render a large amount of prefetched data obsolete. Furtherproacmore, the\ninterference variance is high due to the short inter-SBS distances, making it\ndifficult to quantify data downloading rates. Lastly, since the caching\ndecision of each SBS interacts with those of all other SBSs, the problem\ncomplexity of exponentially increases with the number of SBSs, which is unfit\nfor UDCNs. To resolve such challenging issues while reflecting time-varying and\nlocation-dependent user demand, we leverage mean-field game (MFG) theory\nthrough which each SBS interacts only with a single virtual SBS whose state is\ndrawn from the state distribution of the entire SBS population, i.e.,\nmean-field (MF) distribution. This MF approximation asymptotically guarantees\nachieving the epsilon Nash equilibrium as the number of SBSs approaches\ninfinity. To describe such an MFG-theoretic caching framework, this chapter\naims to provide a brief review of MFG, and demonstrate its effectiveness for\nUDCNs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 23:56:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kim", "Hyesung", ""], ["Park", "Jihong", ""], ["Bennis", "Mehdi", ""], ["Kim", "Seong-Lyun", ""], ["Debbah", "M\u00e9rouane", ""]]}, {"id": "2101.00378", "submitter": "Wang Taotao", "authors": "Lihao Zhang, Taotao Wang, and Soung Chang Liew", "title": "Speeding up Block Propagation in Blockchain Network: Uncoded and Coded\n  Designs", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and validate new block propagation protocols for the peer-to-peer\n(P2P) network of the Bitcoin blockchain. Despite its strong protection for\nsecurity and privacy, the current Bitcoin blockchain can only support a low\nnumber of transactions per second (TPS). In this work, we redesign the current\nBitcoin's networking protocol to increase TPS without changing vital components\nin its consensus-building protocol. In particular, we improve the compact-block\nrelaying protocol to enable the propagation of blocks containing a massive\nnumber of transactions without inducing extra propagation latencies. Our\nimprovements consist of (i) replacing the existing store-and-forward\ncompact-block relaying scheme with a cut-through compact-block relaying scheme;\n(ii) exploiting rateless erasure codes for P2P networks to increase\nblock-propagation efficiency. Since our protocols only need to rework the\ncurrent Bitcoin's networking protocol and does not modify the data structures\nand crypto-functional components, they can be seamlessly incorporated into the\nexisting Bitcoin blockchain. To validate our designs, we perform analysis on\nour protocols and implement a Bitcoin network simulator on NS3 to run different\nblock propagation protocols. The analysis and experimental results confirm that\nour new block propagation protocols could increase the TPS of the Bitcoin\nblockchain by 100x without compromising security and consensus-building.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 05:43:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Lihao", ""], ["Wang", "Taotao", ""], ["Liew", "Soung Chang", ""]]}, {"id": "2101.00382", "submitter": "Bohai Li", "authors": "Bohai Li, Qian Wang, He Chen, Yong Zhou, Yonghui Li", "title": "Optimizing Information Freshness for Cooperative IoT Systems with\n  Stochastic Arrivals", "comments": "16 pages. This work has been accepted by IEEE Internet of Things\n  Journal. arXiv admin note: substantial text overlap with arXiv:2001.04084", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a cooperative Internet of Things (IoT) system with a\nsource aiming to transmit randomly generated status updates to a designated\ndestination as timely as possible under the help of a relay. We adopt a\nrecently proposed concept, the age of information (AoI), to characterize the\ntimeliness of the status updates. In the considered system, delivering the\nstatus updates via the one-hop direct link will have a shorter transmission\ntime at the cost of incurring a higher error probability, while the delivery of\nstatus updates through the two-hop relay link could be more reliable at the\ncost of suffering longer transmission time. Thus, it is important to design the\nrelaying protocol of the considered system for optimizing the information\nfreshness. Considering the limited capabilities of IoT devices, we propose two\nlow-complexity age-oriented relaying (AoR) protocols, i.e., the\nsource-prioritized AoR (SP-AoR) protocol and the relay-prioritized AoR (RP-AoR)\nprotocol, to reduce the AoI of the considered system. By carefully analyzing\nthe evolution of the instantaneous AoI, we derive closed-form expressions of\nthe average AoI for both proposed AoR protocols. We further optimize the\ngeneration probability of the status updates at the source in both protocols.\nSimulation results validate our theoretical analysis, and demonstrate that the\ntwo proposed protocols outperform each other under various system parameters.\nMoreover, the protocol with better performance can achieve near-optimal\nperformance compared with the optimal scheduling policy attained by applying\nthe Markov decision process (MDP) tool.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 06:00:08 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Bohai", ""], ["Wang", "Qian", ""], ["Chen", "He", ""], ["Zhou", "Yong", ""], ["Li", "Yonghui", ""]]}, {"id": "2101.00464", "submitter": "Sami Khairy", "authors": "Sami Khairy, Prasanna Balaprakash, Lin X. Cai, H. Vincent Poor", "title": "Data-Driven Random Access Optimization in Multi-Cell IoT Networks with\n  NOMA", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-orthogonal multiple access (NOMA) is a key technology to enable massive\nmachine type communications (mMTC) in 5G networks and beyond. In this paper,\nNOMA is applied to improve the random access efficiency in high-density\nspatially-distributed multi-cell wireless IoT networks, where IoT devices\ncontend for accessing the shared wireless channel using an adaptive\np-persistent slotted Aloha protocol. To enable a capacity-optimal network, a\nnovel formulation of random channel access management is proposed, in which the\ntransmission probability of each IoT device is tuned to maximize the geometric\nmean of users' expected capacity. It is shown that the network optimization\nobjective is high dimensional and mathematically intractable, yet it admits\nfavourable mathematical properties that enable the design of efficient\ndata-driven algorithmic solutions which do not require a priori knowledge of\nthe channel model or network topology. A centralized model-based algorithm and\na scalable distributed model-free algorithm, are proposed to optimally tune the\ntransmission probabilities of IoT devices and attain the maximum capacity. The\nconvergence of the proposed algorithms to the optimal solution is further\nestablished based on convex optimization and game-theoretic analysis. Extensive\nsimulations demonstrate the merits of the novel formulation and the efficacy of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:21:08 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 14:44:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Khairy", "Sami", ""], ["Balaprakash", "Prasanna", ""], ["Cai", "Lin X.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2101.00573", "submitter": "Junaid Qadir", "authors": "Usman Ashraf, Amir Khwaja, Junaid Qadir, Stefano Avallone, Chau Yuen", "title": "WiMesh: Leveraging Mesh Networking For Disaster Communication in Poor\n  Regions of the World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the design, implementation and field trials of WiMesh -\na resilient Wireless Mesh Network (WMN) based disaster communication system\npurpose-built for underdeveloped and rural parts of the world. Mesh networking\nis a mature area, and the focus of this paper is not on proposing novel models,\nprotocols or other mesh solutions. Instead, the paper focuses on the\nidentification of important design considerations and justifications for\nseveral design trade offs in the context of mesh networking for disaster\ncommunication in developing countries with very limited resources. These\ntrade-offs are discussed in the context of key desirable traits including\nsecurity, low cost, low power, size, availability, customization, portability,\nease of installation and deployment, and coverage area among others. We discuss\nat length the design, implementation, and field trial results of the WiMesh\nsystem which enables users spread over large geographical regions, to\ncommunicate with each other despite the lack of cellular coverage, power, and\nother communication infrastructure by leveraging multi-hop mesh networking and\nWi-Fi equipped handheld devices. Lessons learned along with real-world results\nare shared for WiMesh deployment in a remote rural mountainous village of\nPakistan, and the source code is shared with the research community.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 07:58:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ashraf", "Usman", ""], ["Khwaja", "Amir", ""], ["Qadir", "Junaid", ""], ["Avallone", "Stefano", ""], ["Yuen", "Chau", ""]]}, {"id": "2101.00678", "submitter": "Haonan Tong", "authors": "Haonan Tong, Tao Wang, Yujiao Zhu, Xuanlin Liu, Sihua Wang, and\n  Changchuan Yin", "title": "Mobility-Aware Seamless Handover with MPTCP in Software-Defined HetNets", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of vertical handover in software-defined network\n(SDN) based heterogeneous networks (HetNets) is studied. In the studied model,\nHetNets are required to offer diverse services for mobile users. Using an SDN\ncontroller, HetNets have the capability of managing users' access and mobility\nissues but still have the problems of ping-pong effect and service interruption\nduring vertical handover. To solve these problems, a mobility-aware seamless\nhandover method based on multipath transmission control protocol (MPTCP) is\nproposed. The proposed handover method is executed in the controller of the\nsoftware-defined HetNets (SDHetNets) and consists of three steps: location\nprediction, network selection, and handover execution. In particular, the\nmethod first predicts the user's location in the next moment with an echo state\nnetwork (ESN). Given the predicted location, the SDHetNet controller can\ndetermine the candidate network set for the handover to pre-allocate network\nwireless resources. Second, the target network is selected through fuzzy\nanalytic hierarchical process (FAHP) algorithm, jointly considering user\npreferences, service requirements, network attributes, and user mobility\npatterns. Then, seamless handover is realized through the proposed MPTCP-based\nhandover mechanism. Simulations using real-world user trajectory data from\nKorea Advanced Institute of Science & Technology show that the proposed method\ncan reduce the handover times by 10.85% to 29.12% compared with traditional\nmethods. The proposed method also maintains at least one MPTCP subflow\nconnected during the handover process and achieves a seamless handover.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:09:06 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Tong", "Haonan", ""], ["Wang", "Tao", ""], ["Zhu", "Yujiao", ""], ["Liu", "Xuanlin", ""], ["Wang", "Sihua", ""], ["Yin", "Changchuan", ""]]}, {"id": "2101.00687", "submitter": "Joberto Martins Prof. Dr.", "authors": "Carlos E. Arruda, Pedro F. Moraes, Nazim Agoulmine, Joberto S. B.\n  Martins", "title": "Enhanced Pub/Sub Communications for Massive IoT Traffic with SARSA\n  Reinforcement Learning", "comments": "3rd International Conference on Machine Learning for Networking - MLN\n  2020, Paris, 20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sensors are being extensively deployed and are expected to expand at\nsignificant rates in the coming years. They typically generate a large volume\nof data on the internet of things (IoT) application areas like smart cities,\nintelligent traffic systems, smart grid, and e-health. Cloud, edge and fog\ncomputing are potential and competitive strategies for collecting, processing,\nand distributing IoT data. However, cloud, edge, and fog-based solutions need\nto tackle the distribution of a high volume of IoT data efficiently through\nconstrained and limited resource network infrastructures. This paper addresses\nthe issue of conveying a massive volume of IoT data through a network with\nlimited communications resources (bandwidth) using a cognitive communications\nresource allocation based on Reinforcement Learning (RL) with SARSA algorithm.\nThe proposed network infrastructure (PSIoTRL) uses a Publish/ Subscribe\narchitecture to access massive and highly distributed IoT data. It is\ndemonstrated that the PSIoTRL bandwidth allocation for buffer flushing based on\nSARSA enhances the IoT aggregator buffer occupation and network link\nutilization. The PSIoTRL dynamically adapts the IoT aggregator traffic flushing\naccording to the Pub/Sub topic's priority and network constraint requirements.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 18:46:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Arruda", "Carlos E.", ""], ["Moraes", "Pedro F.", ""], ["Agoulmine", "Nazim", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "2101.00787", "submitter": "Su Wang", "authors": "Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito,\n  Mung Chiang, and Christopher G. Brinton", "title": "Device Sampling for Heterogeneous Federated Learning: Theory,\n  Algorithms, and Implementation", "comments": "This paper is accepted for publication in the proceedings of 2021\n  IEEE International Conference on Computer Communications (INFOCOM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional federated learning (FedL) architecture distributes machine\nlearning (ML) across worker devices by having them train local models that are\nperiodically aggregated by a server. FedL ignores two important characteristics\nof contemporary wireless networks, however: (i) the network may contain\nheterogeneous communication/computation resources, while (ii) there may be\nsignificant overlaps in devices' local data distributions. In this work, we\ndevelop a novel optimization methodology that jointly accounts for these\nfactors via intelligent device sampling complemented by device-to-device (D2D)\noffloading. Our optimization aims to select the best combination of sampled\nnodes and data offloading configuration to maximize FedL training accuracy\nsubject to realistic constraints on the network topology and device\ncapabilities. Theoretical analysis of the D2D offloading subproblem leads to\nnew FedL convergence bounds and an efficient sequential convex optimizer. Using\nthis result, we develop a sampling methodology based on graph convolutional\nnetworks (GCNs) which learns the relationship between network attributes,\nsampled nodes, and resulting offloading that maximizes FedL accuracy. Through\nevaluation on real-world datasets and network measurements from our IoT\ntestbed, we find that our methodology while sampling less than 5% of all\ndevices outperforms conventional FedL substantially both in terms of trained\nmodel accuracy and required resource utilization.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:59:50 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Wang", "Su", ""], ["Lee", "Mengyuan", ""], ["Hosseinalipour", "Seyyedali", ""], ["Morabito", "Roberto", ""], ["Chiang", "Mung", ""], ["Brinton", "Christopher G.", ""]]}, {"id": "2101.00798", "submitter": "Quoc-Viet Pham", "authors": "Parimala M and Swarna Priya R M and Quoc-Viet Pham and Kapal Dev and\n  Praveen Kumar Reddy Maddikunta and Thippa Reddy Gadekallu and Thien Huynh-The", "title": "Fusion of Federated Learning and Industrial Internet of Things: A Survey", "comments": "This work has been submitted for possible publication. Any comments\n  and suggestions are appreciated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Internet of Things (IIoT) lays a new paradigm for the concept of\nIndustry 4.0 and paves an insight for new industrial era. Nowadays smart\nmachines and smart factories use machine learning/deep learning based models\nfor incurring intelligence. However, storing and communicating the data to the\ncloud and end device leads to issues in preserving privacy. In order to address\nthis issue, federated learning (FL) technology is implemented in IIoT by the\nresearchers nowadays to provide safe, accurate, robust and unbiased models.\nIntegrating FL in IIoT ensures that no local sensitive data is exchanged, as\nthe distribution of learning models over the edge devices has become more\ncommon with FL. Therefore, only the encrypted notifications and parameters are\ncommunicated to the central server. In this paper, we provide a thorough\noverview on integrating FL with IIoT in terms of privacy, resource and data\nmanagement. The survey starts by articulating IIoT characteristics and\nfundamentals of distributive and FL. The motivation behind integrating IIoT and\nFL for achieving data privacy preservation and on-device learning are\nsummarized. Then we discuss the potential of using machine learning, deep\nlearning and blockchain techniques for FL in secure IIoT. Further we analyze\nand summarize the ways to handle the heterogeneous and huge data. Comprehensive\nbackground on data and resource management are then presented, followed by\napplications of IIoT with FL in healthcare and automobile industry. Finally, we\nshed light on challenges, some possible solutions and potential directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:28:32 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["M", "Parimala", ""], ["M", "Swarna Priya R", ""], ["Pham", "Quoc-Viet", ""], ["Dev", "Kapal", ""], ["Maddikunta", "Praveen Kumar Reddy", ""], ["Gadekallu", "Thippa Reddy", ""], ["Huynh-The", "Thien", ""]]}, {"id": "2101.00817", "submitter": "Xinghua Sun", "authors": "Xinghua Sun, Fangming Zhao, Howard H. Yang, Wen Zhan, Xijun Wang and\n  Tony Q. S. Quek", "title": "Optimizing Age of Information in Random-Access Poisson Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timeliness is an emerging requirement for many Internet of Things (IoT)\napplications. In IoT networks, where a large-number of nodes are distributed,\nsevere interference may incur during the transmission phase which causes age of\ninformation (AoI) degradation. It is therefore important to study the\nperformance limit of AoI as well as how to achieve such limit. In this paper,\nwe aim to optimize the AoI in random access Poisson networks. By taking into\naccount the spatio-temporal interactions amongst the transmitters, an\nexpression of the peak AoI is derived, based on explicit expressions of the\noptimal peak AoI and the corresponding optimal system parameters including the\npacket arrival rate and the channel access probability are further derived. It\nis shown that with a given packet arrival rate (resp. a given channel access\nprobability), the optimal channel access probability (resp. the optimal packet\narrival rate), is equal to one under a small node deployment density, and\ndecrease monotonically as the spatial deployment density increases due to the\nsevere interference caused by spatio-temproal coupling between transmitters.\nWhen joint tuning of the packet arrival rate and channel access probability is\nperformed, the optimal channel access probability is always set to be one.\nMoreover, with the sole tuning of the channel access probability, it is found\nthat the optimal peak AoI performance can be improved with a smaller packet\narrival rate only when the node deployment density is high, which is contrast\nto the case of the sole tuning of the packet arrival rate, where a higher\nchannel access probability always leads to better optimal peak AoI regardless\nof the node deployment density. In all the cases of optimal tuning of system\nparameters, the optimal peak AoI linearly grows with the node deployment\ndensity as opposed to an exponential growth with fixed system parameters.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:48:23 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 06:09:19 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Sun", "Xinghua", ""], ["Zhao", "Fangming", ""], ["Yang", "Howard H.", ""], ["Zhan", "Wen", ""], ["Wang", "Xijun", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2101.00845", "submitter": "Daniel Menasche", "authors": "Felipe Ribas Coutinho and Victor Pires and Claudio Miceli and Daniel\n  Sadoc Menasche", "title": "Crypto-Hotwire: Illegal Blockchain Mining at Zero Cost Using Public\n  Infrastructures", "comments": null, "journal-ref": "Symposium on Cryptocurrency Analysis (SOCCA) 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Blockchains and cryptocurrencies disrupted the conversion of energy into a\nmedium of exchange. Numerous applications for blockchains and cryptocurrencies\nare now envisioned for purposes ranging from inventory control to banking\napplications. Naturally, in order to mine in an economically viable way,\nregions where energy is plentiful and cheap, e.g., close to hydroelectric\nplants, are sought. The possibility of converting energy into cash, however,\nalso opens up opportunities for a new kind of cyber attack aimed at illegally\nmining cryptocurrencies by stealing energy. In this work, we indicate, using\ndata from January and February of 2018 from our university, that such a threat\nis real, and present a projection of the gains derived from these attacks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:34:51 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Coutinho", "Felipe Ribas", ""], ["Pires", "Victor", ""], ["Miceli", "Claudio", ""], ["Menasche", "Daniel Sadoc", ""]]}, {"id": "2101.00847", "submitter": "Qiumei Cheng", "authors": "Qiumei Cheng, Chunming WU, Haifeng Zhou, Dezhang Kong, Dong Zhang,\n  Junchi Xing, Wei Ruan", "title": "Machine Learning based Malicious Payload Identification in\n  Software-Defined Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep packet inspection (DPI) has been extensively investigated in\nsoftware-defined networking (SDN) as complicated attacks may intractably inject\nmalicious payloads in the packets. Existing proprietary pattern-based or\nport-based third-party DPI tools can suffer from limitations in efficiently\nprocessing a large volume of data traffic. In this paper, a novel\nOpenFlow-enabled deep packet inspection (OFDPI) approach is proposed based on\nthe SDN paradigm to provide adaptive and efficient packet inspection. First,\nOFDPI prescribes an early detection at the flow-level granularity by checking\nthe IP addresses of each new flow via OpenFlow protocols. Then, OFDPI allows\nfor deep packet inspection at the packet-level granularity: (i) for unencrypted\npackets, OFDPI extracts the features of accessible payloads, including tri-gram\nfrequency based on Term Frequency and Inverted Document Frequency (TF-IDF) and\nlinguistic features. These features are concatenated into a sparse matrix\nrepresentation and are then applied to train a binary classifier with logistic\nregression rather than matching with specific pattern combinations. In order to\nbalance the detection accuracy and performance bottleneck of the SDN\ncontroller, OFDPI introduces an adaptive packet sampling window based on the\nlinear prediction; and (ii) for encrypted packets, OFDPI extracts notable\nfeatures of packets and then trains a binary classifier with a decision tree,\ninstead of decrypting the encrypted traffic to weaken user privacy. A prototype\nof OFDPI is implemented on the Ryu SDN controller and the Mininet platform. The\nperformance and the overhead of the proposed sulotion are assessed using the\nreal-world datasets through experiments. The numerical results indicate that\nOFDPI can provide a significant improvement in detection accuracy with\nacceptable overheads.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:36:42 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheng", "Qiumei", ""], ["WU", "Chunming", ""], ["Zhou", "Haifeng", ""], ["Kong", "Dezhang", ""], ["Zhang", "Dong", ""], ["Xing", "Junchi", ""], ["Ruan", "Wei", ""]]}, {"id": "2101.00851", "submitter": "Daniel Menasche", "authors": "Jefferson E. Simoes and Eduardo Ferreira and Daniel S. Menasche and\n  Carlos A. V. Campos", "title": "Blockchain Privacy Through Merge Avoidance and Mixing Services: a\n  Hardness and an Impossibility Result", "comments": null, "journal-ref": "Symposium on Cryptocurrency Analysis (SOCCA) 2020", "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptocurrencies typically aim at preserving the privacy of their users.\nDifferent cryptocurrencies preserve privacy at various levels, some of them\nrequiring users to rely on strategies to raise the privacy level to their\nneeds. Among those strategies, we focus on two of them: merge avoidance and\nmixing services. Such strategies may be adopted on top of virtually any\nblockchain-based cryptocurrency. In this paper, we show that whereas optimal\nmerge avoidance leads to an NP-hard optimization problem, incentive-compatible\nmixing services are subject to a certain class of impossibility results.\nTogether, our results contribute to the body of work on fundamental limits of\nprivacy mechanisms in blockchain-based cryptocurrencies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 09:45:35 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Simoes", "Jefferson E.", ""], ["Ferreira", "Eduardo", ""], ["Menasche", "Daniel S.", ""], ["Campos", "Carlos A. V.", ""]]}, {"id": "2101.00902", "submitter": "Chathura Sarathchandra Magurawalage", "authors": "Chathura Sarathchandra", "title": "REACT: Distributed Mobile Microservice Execution Enabled by Efficient\n  Inter-Process Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increased mobile connectivity, the range and number of services available\nin various computing environments in the network, demand mobile applications to\nbe highly dynamic to be able to efficiently incorporate those services into\napplications, along with other local capabilities on mobile devices. However,\nthe monolithic structure and mostly static configuration of mobile application\ncomponents today limit application's ability to dynamically manage internal\ncomponents, to be able to adapt to the user and the environment, and utilize\nvarious services in the network for improving the application experience.\n  In this paper, we present REACT, a new Android-based framework that enables\napps to be developed as a collection of loosely coupled microservices (MS). It\nallows individual distribution, dynamic management and offloading of MS to be\nexecuted by services in the network, based on contextual changes. REACT aims to\nprovide i) a framework as an Android Library for creating MS-based apps that\nadapt to contextual changes ii) a unified HTTP-based communication mechanism,\nusing Android Inter-Process Communication (IPC) for transporting requests\nbetween locally running MS, while allowing flexible and transparent switching\nbetween network and IPC requests, when offloading. We evaluate REACT by\nimplementing a video streaming app that dynamically offloads MS to web services\nin the network, adapting to contextual changes. The evaluation shows the\nadaptability to contextual changes and reductions in power consumption when\noffloading, while our communication mechanism overcomes performance limitations\nof Android IPC by enabling efficient transferring of large payloads between\nmobile MS.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 11:47:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Sarathchandra", "Chathura", ""]]}, {"id": "2101.01022", "submitter": "Brigitte Jaumard Prof.", "authors": "Brigitte Jaumard and Yan Wang", "title": "A Two Sub-problem Decomposition for the Optimal Design of Filterless\n  Optical Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Filterless optical transport networks relies on passive optical\ninterconnections between nodes, i.e., on splitters/couplers and amplifiers.\nWhile different studies have investigated their design, none of them offer a\nsolution for an optimal design. We propose a one step solution scheme, which\ncombines network provisioning, i.e., routing and wavelength assignment within a\nsingle mathematical model. Decomposition into two different types sub-problems\nis then used in order to conceive an exact solution scheme. The first type of\nsubproblem relies on the generation of filterless subnetworks while the second\none takes care of their wavelength assignment.\n  Numerical experiments demonstrate the improved performance of the proposed\noptimization model and algorithm over the state of the art, with the\nimprovement of the solution for several open source data sets.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 15:13:59 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Jaumard", "Brigitte", ""], ["Wang", "Yan", ""]]}, {"id": "2101.01043", "submitter": "Estefania Recayte", "authors": "Estefan\\'ia Recayte and Andrea Munari", "title": "Caching at the Edge: Outage Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching at the edge of wireless networks is a keytechnology to reduce traffic\nin the backhaul link. However, aconcentrated amount of requests during\npeak-periods may causethe outage of the system, meaning that the network is not\nableto serve the whole set of demands. The outage probability is afundamental\nmetric to take into account during the networkdesign. In this paper, we derive\nthe analytical expression ofthe outage probability as a function of the total\namount ofusers requests, library size, requests distribution, cache size\nandcapacity constraints on the backhaul resources. In particular, wefocus on a\nscenario where end-users have no direct connectionto the master node which\nholds the complete library of contentthat can be requested. A general\nformulation of the outage isderived and studied for two relevant caching\nschemes, i.e. therandom caching scheme and the most popular caching schemes.The\nexact closed form expressions presented in this paper provideuseful insights on\nhow requests, memory and resources can bebalanced when the parameters of a\ncache-enabled network haveto designed\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:07:53 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Recayte", "Estefan\u00eda", ""], ["Munari", "Andrea", ""]]}, {"id": "2101.01048", "submitter": "Sanjay Goyal", "authors": "Sanjay Goyal, Hussain Elkotby, Ravikumar Pragada, Tanbir Haque", "title": "Reducing the Paging Overhead in Highly Directional Systems", "comments": "7 pages, 5 figures, accepted at IEEE VTC Spring 2021, April 2021,\n  Helsinki, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New Radio (NR) supports operations at high-frequency bands (e.g.,\nmillimeter-wave frequencies) by using narrow beam based directional\ntransmissions to compensate high propagation losses at such frequencies. Due to\nthe limited spatial coverage with each beam, the broadcast transmission of\npaging in NR is performed using beam sweeping, which takes multiple time slots.\nThus, the paging procedure used in NR would substantially increase the downlink\nresource overhead of the network with directional transmissions. Such overhead\nwould further increase as we move higher in the frequency bands, such as\nterahertz bands, which is being viewed as one of the potential candidates for\nfuture generation networks. Therefore, the NR based paging solution is\ninfeasible for supporting highly directional systems. In this paper, we propose\na novel minimal feedback enabled paging mechanism, which instead of using all\nthe beams for paging transmissions, only activates sub-set of beams having one\nor more UEs under the coverage. UE presence indications are implemented to\nidentify the correct set of beams to be activated. Our analytical analysis and\nsimulations show that the proposed solution significantly reduces the downlink\npaging overhead compared to the NR based solution (e.g., more than 80% gain for\na system supporting 64 number of beams at a UE density of 200 UEs per paging\noccasion) while incurring minimal energy cost at the UE side.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:14:48 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 22:17:01 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Goyal", "Sanjay", ""], ["Elkotby", "Hussain", ""], ["Pragada", "Ravikumar", ""], ["Haque", "Tanbir", ""]]}, {"id": "2101.01062", "submitter": "Bin Han", "authors": "Bin Han, Wei Jiang, Mohammad Asif Habibi, Hans D. Schotten", "title": "An Abstracted Survey on 6G: Drivers, Requirements, Efforts, and Enablers", "comments": null, "journal-ref": "Published at NGNA 2020", "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of today, 5G mobile systems have been already widely rolled out, it is the\nright time for academia and industry to explore the next generation mobile\ncommunication system beyond 5G. To this end, this paper provides an abstracted\nsurvey for the 6G mobile system. We shed light on the key driving factors for\n6G through predicting the growth trend of mobile traffic and mobile service\nsubscriptions until the year of 2030, envisioning the potential use cases and\napplications, as well as deriving the potential use scenarios. Then, a number\nof key performance indicators to support the 6G use cases are identified and\ntheir target values are estimated in a quantitatively manner, which is compared\nwith those of 5G clearly in a visualized way. An investigation of the efforts\nspent on 6G research in different countries and institutions until now is\nsummarized, and a potential roadmap in terms of the definition, specification,\nstandardization, and spectrum regulation is given. Finally, an introduction to\npotential key 6G technologies is provided. The principle, technical advantages,\nchallenges, and open research issues for each identified technology are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:22:58 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Han", "Bin", ""], ["Jiang", "Wei", ""], ["Habibi", "Mohammad Asif", ""], ["Schotten", "Hans D.", ""]]}, {"id": "2101.01064", "submitter": "Estefania Recayte", "authors": "Estefan\\'ia Recayte and Francisco L\\'azaro and Gianluigi Liva", "title": "Caching in Heterogeneous Satellite Networks with Fountain Codes", "comments": "arXiv admin note: substantial text overlap with arXiv:1807.05619", "journal-ref": "International journal of satellite communications and networking,\n  2019", "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the performance of caching schemes based on\nfountain codes in a heterogeneous satellite network. We consider multiple\ncache-aided hubs which are connected to a geostationary satellite through\nbackhaul links. With the aimof reducing the average number of transmissions\nover the satellite backhaul link, we propose the use of a caching scheme based\non fountain codes. We derive a simple analytical expression of the average\nbackhaul transmission rate and provide a tightupper bound on it. Furthermore,\nwe show how the performance of the fountain code based caching scheme is\nsimilar to that of a caching scheme based on maximum distance separable codes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 16:24:04 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Recayte", "Estefan\u00eda", ""], ["L\u00e1zaro", "Francisco", ""], ["Liva", "Gianluigi", ""]]}, {"id": "2101.01081", "submitter": "Liang Ma", "authors": "Liang Ma, Ting He, Kin K. Leung, Don Towsley, Ananthram Swami", "title": "Additive Link Metrics Identification: Proof of Selected Lemmas and\n  Propositions", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.12190", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a technical report, containing all the lemma and proposition proofs\nin paper \"Topological Constraints on Identifying Additive Link Metrics via\nEnd-to-end Paths Measurements\" by Liang Ma, Ting He, Kin K. Leung, Don Towsley,\nand Ananthram Swami, published in Annual Conference of The International\nTechnology Alliance (ACITA), 2012.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 00:25:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ma", "Liang", ""], ["He", "Ting", ""], ["Leung", "Kin K.", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""]]}, {"id": "2101.01102", "submitter": "Adnan Aijaz", "authors": "Mehdi Toumi, Adnan Aijaz", "title": "System Performance Insights into Design of RIS-assisted Smart Radio\n  Environments for 6G", "comments": "To appear in IEEE WCNC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state-of-the-art wireless networks, the radio environment is beyond the\ncontrol of the operators which leads to several issues. For example, signal\nattenuation limits radio connectivity, multi-path propagation results in fading\nphenomena and reflections/refractions from large objects are the main sources\nof uncontrollable interference. To overcome these issues, a new technology\nreferred to as reconfigurable intelligent surfaces (RISs) has been brought to\nlight. RISs, whose interactions with the electromagnetic waves are\nreconfigurable, are at the heart of the smart radio environments for beyond 5G\n(or 6G) wireless systems. Research on design of smart radio environments is\nstill in infancy. To this end, this paper provides performance insights for\ndesign of RIS-assisted smart radio environments. By extending a recent\nsystem-level simulator and through extensive simulations, it investigates the\nimpact of positioning/placement of RISs, the number of reflecting elements and\nthe tilt/rotation of RISs in both single and multi-RIS environments. Results\nreveal that these aspects are crucial to achieving capacity and reliability\nenhancements in smart radio environments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 17:19:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Toumi", "Mehdi", ""], ["Aijaz", "Adnan", ""]]}, {"id": "2101.01279", "submitter": "Elisa Bertino", "authors": "Elisa Bertino, Daniel Bliss, Daniel Lopresti, Larry Peterson, and\n  Henning Schulzrinne", "title": "Computing Research Challenges in Next Generation Wireless Networking", "comments": "A Computing Community Consortium (CCC) white paper, 5 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020whitepaper_16", "categories": "cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By all measures, wireless networking has seen explosive growth over the past\ndecade. Fourth Generation Long Term Evolution (4G LTE) cellular technology has\nincreased the bandwidth available for smartphones, in essence, delivering\nbroadband speeds to mobile devices. The most recent 5G technology is further\nenhancing the transmission speeds and cell capacity, as well as, reducing\nlatency through the use of different radio technologies and is expected to\nprovide Internet connections that are an order of magnitude faster than 4G LTE.\nTechnology continues to advance rapidly, however, and the next generation, 6G,\nis already being envisioned. 6G will make possible a wide range of powerful,\nnew applications including holographic telepresence, telehealth, remote\neducation, ubiquitous robotics and autonomous vehicles, smart cities and\ncommunities (IoT), and advanced manufacturing (Industry 4.0, sometimes referred\nto as the Fourth Industrial Revolution), to name but a few. The advances we\nwill see begin at the hardware level and extend all the way to the top of the\nsoftware \"stack.\"\n  Artificial Intelligence (AI) will also start playing a greater role in the\ndevelopment and management of wireless networking infrastructure by becoming\nembedded in applications throughout all levels of the network. The resulting\nbenefits to society will be enormous.\n  At the same time these exciting new wireless capabilities are appearing\nrapidly on the horizon, a broad range of research challenges loom ahead. These\nstem from the ever-increasing complexity of the hardware and software systems,\nalong with the need to provide infrastructure that is robust and secure while\nsimultaneously protecting the privacy of users. Here we outline some of those\nchallenges and provide recommendations for the research that needs to be done\nto address them.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:27:19 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bertino", "Elisa", ""], ["Bliss", "Daniel", ""], ["Lopresti", "Daniel", ""], ["Peterson", "Larry", ""], ["Schulzrinne", "Henning", ""]]}, {"id": "2101.01286", "submitter": "Yongqiang Zhang", "authors": "Yongqiang Zhang, Mustafa A. Kishk, Mohamed-Slim Alouini", "title": "A Survey on Integrated Access and Backhaul Networks", "comments": "47 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the usage of the high-frequency band, utilizing part of the\nlarge available bandwidth for wireless backhauling is feasible without\nconsiderable performance sacrifice. In this context, integrated access and\nbackhaul (IAB) was proposed by 3GPP to reduce the fiber optics deployment cost\nof 5G and beyond networks. In this paper, we first give a brief introduction of\nIAB based on the 3GPP release. After that, we survey existing research on IAB\nnetworks, the integrations of IAB to cache-enabled network, optical\ncommunication transport network, and the non-terrestrial network. Finally, we\ndiscuss the challenges and opportunities that might arise while developing and\ncommercializing IAB networks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:50:45 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Yongqiang", ""], ["Kishk", "Mustafa A.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2101.01406", "submitter": "Prabhu Chandhar", "authors": "Prabhu Chandhar, Sathish Babu, Tamizh Elakkiya", "title": "Tutorial I: Learning the Principles of Mobile Radio Propagation through\n  Smartphone and CRFO", "comments": "Tutorial (8 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial, we present three simple smartphone based experiments for\nunderstanding the basic concepts of mobile communications such as pathloss,\nShadow fading, and small scale fading. We also explain the use of Collaborative\nRadio Frequency Observatory (CRFO), an online platform, for visualizing radio\ncoverage maps.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:33:10 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Chandhar", "Prabhu", ""], ["Babu", "Sathish", ""], ["Elakkiya", "Tamizh", ""]]}, {"id": "2101.01514", "submitter": "Gian Pietro Picco", "authors": "Timofei Istomin, Elia Leoni, Davide Molteni, Amy L. Murphy, Gian\n  Pietro Picco, Maurizio Griva", "title": "Janus: Efficient and Accurate Dual-radio Social Contact Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining when two individuals are within close distance is key to contain\na pandemic, e.g., to alert individuals in real-time and trace their social\ncontacts. Common approaches rely on either Bluetooth Low Energy (BLE) or\nultra-wideband (UWB) radios, that nonetheless strike opposite tradeoffs for\nenergy efficiency vs. accuracy of distance estimates.\n  Janus reconciles these dimensions with a dual-radio protocol enabling\nefficient and accurate social contact detection. Measurements show that Janus\nachieves weeks to months of autonomous operation, depending on the\nconfiguration. Several large-scale campaigns in real-world contexts confirm its\nreliability and practical usefulness in enabling insightful analysis of contact\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:01:16 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 15:35:01 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Istomin", "Timofei", ""], ["Leoni", "Elia", ""], ["Molteni", "Davide", ""], ["Murphy", "Amy L.", ""], ["Picco", "Gian Pietro", ""], ["Griva", "Maurizio", ""]]}, {"id": "2101.01518", "submitter": "Abusayeed Saifullah", "authors": "Dali Ismail and Abusayeed Saifullah", "title": "Handling Mobility in Low-Power Wide-Area Network", "comments": "arXiv admin note: text overlap with arXiv:2011.03169,\n  arXiv:2008.12845 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the proliferation of mobile devices in various wide-area Internet of\nThings applications (e.g., smart city, smart farming), current Low-Power\nWide-Area Networks (LPWANs) are not designed to effectively support mobile\nnodes. In this paper, we propose to handle mobility in SNOW (Sensor Network\nOver White spaces), an LPWAN that operates in the TV white spaces. SNOW\nsupports massive concurrent communication between a base station (BS) and\nnumerous low-power nodes through a distributed implementation of OFDM. In SNOW,\ninter-carrier interference (ICI) is more pronounced under mobility due to its\nOFDM based design. Geospatial variation of white spaces also raises challenges\nin both intra- and inter-network mobility as the low-power nodes are not\nequipped to determine white spaces. To handle mobility impacts on ICI, we\npropose a dynamic carrier frequency offset estimation and compensation\ntechnique which takes into account Doppler shifts without requiring to know the\nspeed of the nodes. We also propose to circumvent the mobility impacts on\ngeospatial variation of white space through a mobility-aware spectrum\nassignment to nodes. To enable mobility of the nodes across different SNOWs, we\npropose an efficient handoff management through a fast and energy-efficient BS\ndiscovery and quick association with the BS by combining time and frequency\ndomain energy-sensing. Experiments through SNOW deployments in a large\nmetropolitan city and indoors show that our proposed approaches enable mobility\nacross multiple different SNOWs and provide robustness in terms of reliability,\nlatency, and energy consumption under mobility.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 14:07:36 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Ismail", "Dali", ""], ["Saifullah", "Abusayeed", ""]]}, {"id": "2101.01588", "submitter": "Bomin Mao", "authors": "Bomin Mao, Fengxiao Tang, Kawamoto Yuichi, and Nei Kato", "title": "AI based Service Management for 6G Green Communications", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Green communications have always been a target for the information industry\nto alleviate energy overhead and reduce fossil fuel usage. In current 5G and\nfuture 6G era, there is no doubt that the volume of network infrastructure and\nthe number of connected terminals will keep exponentially increasing, which\nresults in the surging energy cost. It becomes growing important and urgent to\ndrive the development of green communications. However, 6G will inevitably have\nincreasingly stringent and diversified requirements for Quality of Service\n(QoS), security, flexibility, and even intelligence, all of which challenge the\nimprovement of energy efficiency. Moreover, the dynamic energy harvesting\nprocess, which will be adopted widely in 6G, further complicates the power\ncontrol and network management. To address these challenges and reduce human\nintervene, Artificial Intelligence (AI) has been widely recognized and\nacknowledged as the only solution. Academia and industry have conducted\nextensive research to alleviate energy demand, improve energy efficiency, and\nmanage energy harvesting in various communication scenarios. In this paper, we\npresent the main considerations for green communications and survey the related\nresearch on AI-based green communications. We focus on how AI techniques are\nadopted to manage the network and improve energy harvesting toward the green\nera. We analyze how state-of-the-art Machine Learning (ML) and Deep Learning\n(DL) techniques can cooperate with conventional AI methods and mathematical\nmodels to reduce the algorithm complexity and optimize the accuracy rate to\naccelerate the applications in 6G. Finally, we discuss the existing problems\nand envision the challenges for these emerging techniques in 6G.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 15:26:14 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 10:58:45 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mao", "Bomin", ""], ["Tang", "Fengxiao", ""], ["Yuichi", "Kawamoto", ""], ["Kato", "Nei", ""]]}, {"id": "2101.01768", "submitter": "Zhibo Meng", "authors": "Zhibo Meng, Hongwei Zhang", "title": "Multi-Cell, Multi-Channel URLLC with Probabilistic Per-Packet Real-Time\n  Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-reliable, low-latency communication (URLLC) represents a new focus in\n5G-and-beyond networks, and it is expected to enable mission-critical sensing\nand control as well as AR/VR applications. URLLC requires controlling the\ncommunication quality of individual packets. Prior studies have considered\nprobabilistic per-packet real-time guarantees for single-cell, single-channel\nnetworks with implicit deadline constraints, but they have not considered\nreal-world complexities such as inter-cell interference and multiple\ncommunication channels. Towards ensuring URLLC in multi-cell, multi-channel\nwireless networks, we propose a real-time scheduling algorithm based on\nlocal-deadline-partition (LDP). The LDP algorithm is suitable for distributed\nimplementation, and it ensures probabilistic per-packet real-time guarantee for\nmulti-cell, multi-channel networks with general deadline constraints. We also\naddress the associated challenge of the schedulability test of URLLC traffic.\nIn particular, we propose the concept of feasible set and identify a\nclosed-form sufficient condition for the schedulability of URLLC traffic. We\npropose a distributed algorithm for the schedulability test, and the algorithm\nincludes a procedure for finding the minimum sum work density of feasible sets\nwhich is of interest by itself. We also identify a necessary condition for the\nschedulability of URLLC traffic, and use numerical studies to understand a\nlower bound on the approximation ratio of the LDP algorithm. We experimentally\nstudy the properties of the LDP algorithm and observe that the URLLC traffic\nsupportable by the LDP algorithm is significantly higher than that of a\nstate-of-the-art algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:14:26 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 22:23:47 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Meng", "Zhibo", ""], ["Zhang", "Hongwei", ""]]}, {"id": "2101.01847", "submitter": "Tugba Erpek", "authors": "Tarun S. Cousik, Vijay K. Shah, Tugba Erpek, Yalin E. Sagduyu, Jeffrey\n  H. Reed", "title": "Deep Learning for Fast and Reliable Initial Access in AI-Driven 6G\n  mmWave Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.12653", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepIA, a deep neural network (DNN) framework for enabling fast\nand reliable initial access for AI-driven beyond 5G and 6G millimeter (mmWave)\nnetworks. DeepIA reduces the beam sweep time compared to a conventional\nexhaustive search-based IA process by utilizing only a subset of the available\nbeams. DeepIA maps received signal strengths (RSSs) obtained from a subset of\nbeams to the beam that is best oriented to the receiver. In both line of sight\n(LoS) and non-line of sight (NLoS) conditions, DeepIA reduces the IA time and\noutperforms the conventional IA's beam prediction accuracy. We show that the\nbeam prediction accuracy of DeepIA saturates with the number of beams used for\nIA and depends on the particular selection of the beams. In LoS conditions, the\nselection of the beams is consequential and improves the accuracy by up to 70%.\nIn NLoS situations, it improves accuracy by up to 35%. We find that, averaging\nmultiple RSS snapshots further reduces the number of beams needed and achieves\nmore than 95% accuracy in both LoS and NLoS conditions. Finally, we evaluate\nthe beam prediction time of DeepIA through embedded hardware implementation and\nshow the improvement over the conventional beam sweeping.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 02:59:49 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Cousik", "Tarun S.", ""], ["Shah", "Vijay K.", ""], ["Erpek", "Tugba", ""], ["Sagduyu", "Yalin E.", ""], ["Reed", "Jeffrey H.", ""]]}, {"id": "2101.01901", "submitter": "Dimitris Chatzopoulos", "authors": "Christodoulos Pappas, Dimitris Chatzopoulos, Spyros Lalis, Manolis\n  Vavalis", "title": "IPLS : A Framework for Decentralized Federated Learning", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of resourceful mobile devices that store rich,\nmultidimensional and privacy-sensitive user data motivate the design of\nfederated learning (FL), a machine-learning (ML) paradigm that enables mobile\ndevices to produce an ML model without sharing their data. However, the\nmajority of the existing FL frameworks rely on centralized entities. In this\nwork, we introduce IPLS, a fully decentralized federated learning framework\nthat is partially based on the interplanetary file system (IPFS). By using IPLS\nand connecting into the corresponding private IPFS network, any party can\ninitiate the training process of an ML model or join an ongoing training\nprocess that has already been started by another party. IPLS scales with the\nnumber of participants, is robust against intermittent connectivity and dynamic\nparticipant departures/arrivals, requires minimal resources, and guarantees\nthat the accuracy of the trained model quickly converges to that of a\ncentralized FL framework with an accuracy drop of less than one per thousand.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:44:51 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Pappas", "Christodoulos", ""], ["Chatzopoulos", "Dimitris", ""], ["Lalis", "Spyros", ""], ["Vavalis", "Manolis", ""]]}, {"id": "2101.01952", "submitter": "Filip Lemic", "authors": "Filip Lemic, Sergi Abadal, Aleksandar Stevanovic, Eduard Alarc\\'on,\n  Jeroen Famaey", "title": "Toward Location-aware In-body Terahertz Nanonetworks with Energy\n  Harvesting", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nanoscale wireless networks are expected to revolutionize a variety of\ndomains, with significant advances conceivable in in-body healthcare. In\nhealthcare, these nanonetworks will consist of energy-harvesting nanodevices\npassively flowing through the bloodstream, taking actions at certain locations,\nand communicating results to more powerful Body Area Network (BAN) nodes.\nAssuming such a setup and electromagnetic nanocommunication in the Terahertz\n(THz) frequencies, we propose a network architecture that can support\nfine-grained localization of the energy-harvesting in-body nanonodes, as well\nas their two-way communication with the outside world. The main novelties of\nour proposal lie in the introduction of location-aware and Wake-up Radio\n(WuR)-based wireless nanocommunication paradigms, as well as Software-Defined\nMetamaterials (SDMs), to THz-operating energy-harvesting in-body nanonetworks.\nWe argue that, on a high level, the proposed architecture can handle (and\nactually benefits from) a large number of nanonodes, while simultaneously\ndealing with a short range of THz in-body propagation and highly constrained\nnanonodes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 10:19:56 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 13:27:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lemic", "Filip", ""], ["Abadal", "Sergi", ""], ["Stevanovic", "Aleksandar", ""], ["Alarc\u00f3n", "Eduard", ""], ["Famaey", "Jeroen", ""]]}, {"id": "2101.01995", "submitter": "Francesco Malandrino", "authors": "Francesco Malandrino and Carla Fabiana Chiasserini", "title": "Federated Learning at the Network Edge: When Not All Nodes are Created\n  Equal", "comments": null, "journal-ref": "IEEE Communications Magazine, 2021", "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the federated learning paradigm, a set of nodes can cooperatively train\na machine learning model with the help of a centralized server. Such a server\nis also tasked with assigning a weight to the information received from each\nnode, and often also to drop too-slow nodes from the learning process. Both\ndecisions have major impact on the resulting learning performance, and can\ninterfere with each other in counterintuitive ways. In this paper, we focus on\nedge networking scenarios and investigate existing and novel approaches to such\nmodel-weighting and node-dropping decisions. Leveraging a set of real-world\nexperiments, we find that popular, straightforward decision-making approaches\nmay yield poor performance, and that considering the quality of data in\naddition to its quantity can substantially improve learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 12:51:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 13:00:56 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Malandrino", "Francesco", ""], ["Chiasserini", "Carla Fabiana", ""]]}, {"id": "2101.02029", "submitter": "Mohammad Tabrez Quasim", "authors": "Mohammad Meraj, Surendra Pal Singh, Prashant Johri, Mohammad Tabrez\n  Quasim", "title": "Detection and Prediction of Infectious Diseases Using IoT Sensors: A\n  Review", "comments": "7 pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An infectious kind of disease affects a huge number of human beings. A lot of\ninvestigation being conducted throughout the world. There are many interactive\nhardware platform packages like IoT in healthcare including smart tracking,\nsmart sensors, and clinical device integration available in the market.\nEmerging technology like IoT has a notable ability to hold patients secure and\nhealthful and also enhance how physicians supply care. Healthcare IoT also can\nbolster affected person pride by permitting patients to spend more time\ninteracting with their medical doctors due to the fact docs aren't as taken\nwith the mundane and rote aspects of their career. The most considerable\nadvantage to IoT in healthcare is that it supports doctors in undertaking extra\nsignificant clinical work in a profession that already is experiencing a\nworldwide professional hard work shortage. This paper investigates the basis\nexploration of the applicability of IoT in the healthcare System.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 15:59:00 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Meraj", "Mohammad", ""], ["Singh", "Surendra Pal", ""], ["Johri", "Prashant", ""], ["Quasim", "Mohammad Tabrez", ""]]}, {"id": "2101.02030", "submitter": "Sunder Ali Khowaja", "authors": "Sunder Ali Khowaja, Parus Khuwaja, Kapal Dev", "title": "Internet of Everything enabled solution for COVID-19, its new variants\n  and future pandemics: Framework, Challenges, and Research Directions", "comments": "14 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  After affecting the world in unexpected ways, COVID-19 has started mutating\nwhich is evident with the insurgence of its new variants. The governments,\nhospitals, schools, industries, and humans, in general, are looking for a\npotential solution in the vaccine which will eventually be available but its\ntimeline for eradicating the virus is yet unknown. Several researchers have\nencouraged and recommended the use of good practices such as physical\nhealthcare monitoring, immunity-boosting, personal hygiene, mental healthcare,\nand contact tracing for slowing down the spread of the virus. In this article,\nwe propose the use of wearable/mobile sensors integrated with the Internet of\nEverything to cover the spectrum of good practices in an automated manner. We\npresent hypothetical frameworks for each of the good practice modules and\npropose the COvid-19 Resistance Framework using the Internet of Everything\n(CORFIE) to tie all the individual modules in a unified architecture. We\nenvision that CORFIE would be influential in assisting people with the new\nnormal for current and future pandemics as well as instrumental in halting the\neconomic losses, respectively. We also provide potential challenges and their\nprobable solutions in compliance with the proposed CORFIE.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 07:27:34 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 09:49:07 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2021 09:11:56 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Khowaja", "Sunder Ali", ""], ["Khuwaja", "Parus", ""], ["Dev", "Kapal", ""]]}, {"id": "2101.02086", "submitter": "Laurent Viennot", "authors": "Filippo Brunelli (Inria, IRIF (UMR\\_8243), UP), Pierluigi Crescenzi\n  (GSSI), Laurent Viennot (Inria, IRIF (UMR\\_8243), UP)", "title": "On Computing Pareto Optimal Paths in Weighted Time-Dependent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weighted point-availability time-dependent network is a list of temporal\nedges, where each temporal edge has an appearing time value, a travel time\nvalue, and a cost value. In this paper we consider the single source Pareto\nproblem in weighted point-availability time-dependent networks, which consists\nof computing, for any destination d, all Pareto optimal pairs (t, c), where t\nand c are the arrival time and the cost of a path from s to d, respectively (a\npair (t, c) is Pareto optimal if there is no path with arrival time smaller\nthan t and cost no worse than c or arrival time no greater than t and better\ncost). We design and analyse a general algorithm for solving this problem,\nwhose time complexity is O(M log P), where M is the number of temporal edges\nand P is the maximum number of Pareto optimal pairs for each node of the\nnetwork. This complexity significantly improves the time complexity of the\npreviously known solution. Our algorithm can be used to solve several different\nminimum cost path problems in weighted point-availability time-dependent\nnetworks with a vast variety of cost definitions, and it can be easily modified\nin order to deal with the single destination Pareto problem. All our results\napply to directed networks, but they can be easily adapted to undirected\nnetworks with no edges with zero travel time.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:16:11 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Brunelli", "Filippo", "", "Inria, IRIF"], ["Crescenzi", "Pierluigi", "", "GSSI"], ["Viennot", "Laurent", "", "Inria, IRIF"]]}, {"id": "2101.02184", "submitter": "Anees Al-Najjar Dr.", "authors": "Anees Al-Najjar, Nageswara S. V. Rao, Neena Imam, Thomas Naughton,\n  Seth Hitefield, Lawrence Sorrillo, James Kohl, Wael Elwasif, Jean-Christophe\n  Bilheux, Hassina Bilheux, Swen Boehm, Jason Kincl", "title": "VFSIE -- Development and Testing Framework for Federated Science\n  Instruments", "comments": "Earlier Version of VFSIE framework for emulating science workflows at\n  a single site", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in softwarization of networked infrastructures combined\nwith containerization of computing workflows promise unprecedented compute\nanywhere and everywhere capabilities for federations of edge and remote\ncomputing systems and science instruments. The development and testing of\nsoftware stacks that implement these capabilities over physical production\nfederations, however, is not very practical nor cost-effective. In response, we\ndevelop a digital twin of the physical infrastructure, called the Virtual\nFederated Science Instrument Environment (VFSIE). This framework emulates the\nfederation using containers and hosts connected over an emulated network, and\nsupports the development and testing of federation stacks and workflows. We\nillustrate its use in a case study involving Jupiter Notebook computations and\ninstrument control.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:34:14 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 19:00:51 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 17:01:04 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Al-Najjar", "Anees", ""], ["Rao", "Nageswara S. V.", ""], ["Imam", "Neena", ""], ["Naughton", "Thomas", ""], ["Hitefield", "Seth", ""], ["Sorrillo", "Lawrence", ""], ["Kohl", "James", ""], ["Elwasif", "Wael", ""], ["Bilheux", "Jean-Christophe", ""], ["Bilheux", "Hassina", ""], ["Boehm", "Swen", ""], ["Kincl", "Jason", ""]]}, {"id": "2101.02198", "submitter": "Cong Shen", "authors": "Xizixiang Wei and Cong Shen", "title": "Federated Learning over Noisy Channels: Convergence Analysis and Design\n  Examples", "comments": "30 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does Federated Learning (FL) work when both uplink and downlink\ncommunications have errors? How much communication noise can FL handle and what\nis its impact to the learning performance? This work is devoted to answering\nthese practically important questions by explicitly incorporating both uplink\nand downlink noisy channels in the FL pipeline. We present several novel\nconvergence analyses of FL over simultaneous uplink and downlink noisy\ncommunication channels, which encompass full and partial clients participation,\ndirect model and model differential transmissions, and non-independent and\nidentically distributed (IID) local datasets. These analyses characterize the\nsufficient conditions for FL over noisy channels to have the same convergence\nbehavior as the ideal case of no communication error. More specifically, in\norder to maintain the O(1/T) convergence rate of FedAvg with perfect\ncommunications, the uplink and downlink signal-to-noise ratio (SNR) for direct\nmodel transmissions should be controlled such that they scale as O(t^2) where t\nis the index of communication rounds, but can stay constant for model\ndifferential transmissions. The key insight of these theoretical results is a\n\"flying under the radar\" principle - stochastic gradient descent (SGD) is an\ninherent noisy process and uplink/downlink communication noises can be\ntolerated as long as they do not dominate the time-varying SGD noise. We\nexemplify these theoretical findings with two widely adopted communication\ntechniques - transmit power control and diversity combining - and further\nvalidating their performance advantages over the standard methods via extensive\nnumerical experiments using several real-world FL tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 18:57:39 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Wei", "Xizixiang", ""], ["Shen", "Cong", ""]]}, {"id": "2101.02434", "submitter": "Michael Gundall", "authors": "Michael Gundall, Christopher Huber, Sergiy Melnyk", "title": "Integration of IEEE 802.1AS-based Time Synchronization in IEEE 802.11 as\n  an Enabler for Novel Industrial Use Cases", "comments": "arXiv admin note: text overlap with arXiv:2011.06313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 introduces new use cases, with more and more mobile devices\nappearing in the industrial landscape. These applications require both new\ntechnologies and smooth integration into existing brownfield deployments.\nEmerging mobile use cases can be divided into optional mobile and mandatory\nmobile, where the first point considers the use of wireless communications due\nto soft criteria such as cost savings and the second means use cases that\ncannot be covered by wireline technologies due to their movement, such as AGVs.\nFor most industrial applications, high determinism, E2E latency and\nsynchronicity are most important. Therefore, we provide a common table, based\non these requirements, listing both existing and emerging mobile use cases.\nSince time synchronization is particularly demanding for wireless use cases, we\npropose a concept for a simple but precise synchronization in IEEE 802.11 WLAN\nand a suitable integration using TSN in combination with OPC UA technology as\nexamples. Furthermore, the concept is evaluated with the help of a testbed\nutilizing state-of-the-art hardware. This means that this concept can be\ndirectly applied in existing industry solutions. It can be shown that the\nconcept is already suitable for a wide range of the mandatory mobile\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 08:54:43 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gundall", "Michael", ""], ["Huber", "Christopher", ""], ["Melnyk", "Sergiy", ""]]}, {"id": "2101.02436", "submitter": "Michael Gundall", "authors": "Michael Gundall, Calvin Glas, Hans D. Schotten", "title": "Feasibility Study on Virtual Process Controllers as Basis for Future\n  Industrial Automation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 offers many possibilities for creating highly efficient and\nflexible manufacturing. To create such advantages, highly automated and thus\ndigitized processes and systems are required. Here, most technologies known\nfrom the office floor are basically suitable for these tasks, but cannot meet\nthe high demands of industrial use cases. Therefore, they cannot replace\nindustrial technologies and devices that have performed well over decades \"out\nof the box\". For this reason, many technologies known from the office floor are\nbeing investigated and adapted for industrial environments. An important task\nis the virtualization of process controls, as more and more devices use\ncomputation offloading, e.g. due to limited resources. In this paper we extend\nthe work on our novel architecture that enables numerous use cases and meets\nindustrial requirements by virtualizing process controllers. In addition, a\ntestbed based on a factory scenario is proposed to evaluate the most important\nfeatures of the presented architecture.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 09:04:38 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Gundall", "Michael", ""], ["Glas", "Calvin", ""], ["Schotten", "Hans D.", ""]]}, {"id": "2101.02615", "submitter": "Cao Vien Phung", "authors": "Cao Vien Phung, Mounir Bensalem and Admela Jukan", "title": "On the Expected Value of Buffer Size in IoT Devices Deploying REST HTTP", "comments": "This paper is uploaded here for research community, thus it is for\n  non-commercial purposes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the expected value of buffer size in IoT devices deploying REST\nHTTP, which is critical to communication performance and hardware architecture\nof IoT devices.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 16:31:47 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 08:31:37 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Phung", "Cao Vien", ""], ["Bensalem", "Mounir", ""], ["Jukan", "Admela", ""]]}, {"id": "2101.02656", "submitter": "Tugba Erpek", "authors": "Yalin E. Sagduyu, Tugba Erpek, Yi Shi", "title": "Adversarial Machine Learning for 5G Communications Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning provides automated means to capture complex dynamics of\nwireless spectrum and support better understanding of spectrum resources and\ntheir efficient utilization. As communication systems become smarter with\ncognitive radio capabilities empowered by machine learning to perform critical\ntasks such as spectrum awareness and spectrum sharing, they also become\nsusceptible to new vulnerabilities due to the attacks that target the machine\nlearning applications. This paper identifies the emerging attack surface of\nadversarial machine learning and corresponding attacks launched against\nwireless communications in the context of 5G systems. The focus is on attacks\nagainst (i) spectrum sharing of 5G communications with incumbent users such as\nin the Citizens Broadband Radio Service (CBRS) band and (ii) physical layer\nauthentication of 5G User Equipment (UE) to support network slicing. For the\nfirst attack, the adversary transmits during data transmission or spectrum\nsensing periods to manipulate the signal-level inputs to the deep learning\nclassifier that is deployed at the Environmental Sensing Capability (ESC) to\nsupport the 5G system. For the second attack, the adversary spoofs wireless\nsignals with the generative adversarial network (GAN) to infiltrate the\nphysical layer authentication mechanism based on a deep learning classifier\nthat is deployed at the 5G base station. Results indicate major vulnerabilities\nof 5G systems to adversarial machine learning. To sustain the 5G system\noperations in the presence of adversaries, a defense mechanism is presented to\nincrease the uncertainty of the adversary in training the surrogate model used\nfor launching its subsequent attacks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 17:52:17 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Shi", "Yi", ""]]}, {"id": "2101.02666", "submitter": "Abbas Mirzaei", "authors": "Abbas Mirzaei, Morteza Barari, Houman Zarrabi", "title": "An Optimal Load Balanced Resource Allocation Scheme for Heterogeneous\n  Wireless Networks based on Big Data Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important issue in heterogeneous wireless networks is how to optimally\nutilize various radio resources. While many methods have been proposed for\nmanaging radio resources in each network, these methods are not suitable for\nheterogeneous wireless networks. In this study, a new management method is\nproposed which provides acceptable service quality and roaming rate, reduces\nthe cost of the service, and utilizes big data technology for its operation. In\nour proposed scheme, by considering various parameters such as the type of the\nservice, the information related to the location of the user, the movement\ndirection of the user, the cost of the service, and a number of other\nstatistical measures, the most suitable technology for radio access will be\nselected. It is expected that besides improving the decision making accuracy in\nselecting the radio access technology and the balanced distribution of network\nresources, the proposed method provides lower roaming and lower probability of\nstopping roaming requests entering the network. By considering the various\nservice classes and various quality of service requirements regarding delay,\nvibration and so on, this can be useful in optimal implementation of\nheterogeneous wireless networks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:17:18 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Mirzaei", "Abbas", ""], ["Barari", "Morteza", ""], ["Zarrabi", "Houman", ""]]}, {"id": "2101.02772", "submitter": "Sheng Yue", "authors": "Sheng Yue, Ju Ren, Nan Qiao, Yongmin Zhang, Hongbo Jiang, Yaoxue Zhang", "title": "TODG: Distributed Task Offloading with Delay Guarantees for Edge\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing has been an efficient way to provide prompt and near-data\ncomputing services for resource-and-delay sensitive IoT applications via\ncomputation offloading. Effective computation offloading strategies need to\ncomprehensively cope with several major issues, including the allocation of\ndynamic communication and computational resources, the deadline constraints of\nheterogeneous tasks, and the requirements for computationally inexpensive and\ndistributed algorithms. However, most of the existing works mainly focus on\npart of these issues, which would not suffice to achieve expected performance\nin complex and practical scenarios. To tackle this challenge, in this paper, we\nsystematically study a distributed computation offloading problem with hard\ndelay constraints, where heterogeneous computational tasks require continually\noffloading to a set of edge servers via a limiting number of stochastic\ncommunication channels. The task offloading problem is then cast as a\ndelay-constrained long-term stochastic optimization problem under unknown\npriori statistical knowledge. To resolve this problem, we first provide a\ntechnical path to transform and decompose it into several slot-level\nsubproblems, then we develop a distributed online algorithm, namely TODG, to\nefficiently allocate the resources and schedule the offloading tasks with delay\nguarantees. Further, we present a comprehensive analysis for TODG, in terms of\nthe optimality gap, the delay guarantees, and the impact of system parameters.\nExtensive simulation results demonstrate the effectiveness and efficiency of\nTODG.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:43:23 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 20:13:02 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yue", "Sheng", ""], ["Ren", "Ju", ""], ["Qiao", "Nan", ""], ["Zhang", "Yongmin", ""], ["Jiang", "Hongbo", ""], ["Zhang", "Yaoxue", ""]]}, {"id": "2101.02963", "submitter": "Zhihui Gao", "authors": "Zhihui Gao, Ang Li, Yunfan Gao, Yu Wang, Yiran Chen", "title": "Hermes: Decentralized Dynamic Spectrum Access System for Massive Devices\n  Deployment in 5G", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the incoming 5G network, the ubiquitous Internet of Things (IoT) devices\ncan benefit our daily life, such as smart cameras, drones, etc. With the\nintroduction of the millimeter-wave band and the thriving number of IoT\ndevices, it is critical to design new dynamic spectrum access (DSA) system to\ncoordinate the spectrum allocation across massive devices in 5G. In this paper,\nwe present Hermes, the first decentralized DSA system for massive devices\ndeployment. Specifically, we propose an efficient multi-agent reinforcement\nlearning algorithm and introduce a novel shuffle mechanism, addressing the\ndrawbacks of collision and fairness in existing decentralized systems. We\nimplement Hermes in 5G network via simulations. Extensive evaluations show that\nHermes significantly reduces collisions and improves fairness compared to the\nstate-of-the-art decentralized methods. Furthermore, Hermes is able to adapt\nthe environmental changes within 0.5 seconds, showing its deployment\npracticability in dynamic environment of 5G.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 11:11:10 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Gao", "Zhihui", ""], ["Li", "Ang", ""], ["Gao", "Yunfan", ""], ["Wang", "Yu", ""], ["Chen", "Yiran", ""]]}, {"id": "2101.02980", "submitter": "Stefan Runeson", "authors": "Stefan Runeson, Ali Zaidi and Ana Cantarero", "title": "Coverage Enhancement for Vehicles", "comments": "4 pages, 6 figures, submitted to IEEE Communications Standards\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Third Generation Partnership Project (3GPP) has standardized Coverage\nEnhancement (CE) for Internet-of-Things (IoT) to connect devices in challenging\nradio conditions with cellular networks. CE is based on the principle of\nprolonged transmission time that exploits the fact that many IoT applications\nhave relaxed requirements on data rate and latency, and the coverage can be\nsignificantly boosted by repeating transmissions for such applications.\nHowever, CE consumes a lot of radio resources and should be implemented\ncarefully for different applications. This paper presents an end-to-end concept\nfor realizing dynamic use of CE for connected vehicles in a resource efficient\nway. The proposed framework has the potential of improving coverage by around\n10dB for low data rate connected vehicle applications, based on the 3GPP Long\nTerm Evolution (LTE) standard.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 12:18:10 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Runeson", "Stefan", ""], ["Zaidi", "Ali", ""], ["Cantarero", "Ana", ""]]}, {"id": "2101.03022", "submitter": "AKM Bahalul Haque", "authors": "A K M Bahalul Haque, Sonia Tasmin", "title": "Security Threats and Research Challenges of IoT-A Review", "comments": "13 pages. 1 table", "journal-ref": "Journal of Engineering Advancements Vol. 01(04) 2020, pp 170-182", "doi": "10.38032/jea.2020.04.008", "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Internet of things (IoT) is the epitome of sustainable development. It has\nfacilitated the development of smart systems, industrialization, and the\nstate-of-the-art quality of life. IoT architecture is one of the essential\nbaselines of understanding the widespread adoption. Security issues are very\ncrucial for any technical infrastructure. Since IoT comprises heterogeneous\ndevices, its security issues are diverse too. Various security attacks can be\nresponsible for compromising confidentiality, integrity, and availability. In\nthis paper, at first, the IoT architecture is described briefly. After that,\nthe components of IoT are explained with perspective to various IoT based\napplications and services. Finally, various security issues, including\nrecommended solutions, are elaborately described and the potential research\nchallenges and future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:42:05 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Haque", "A K M Bahalul", ""], ["Tasmin", "Sonia", ""]]}, {"id": "2101.03072", "submitter": "Sebastian Euler", "authors": "Sebastian Euler, Xingqin Lin, Erika Tejedor, Evanny Obregon", "title": "A Primer on HIBS -- High Altitude Platform Stations as IMT Base Stations", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile communication via high-altitude platforms operating in the\nstratosphere is an idea that has been on the table for decades. In the past few\nyears, however, with recent advances in technology and parallel progress in\nstandardization and regulatory bodies like 3GPP and ITU, these ideas have\ngained considerable momentum. In this article, we present a comprehensive\noverview of HIBS - High Altitude Platform Stations as IMT Base Stations. We lay\nout possible use cases and summarize the current status of the development,\nfrom a technological point of view as well as from standardization in 3GPP, and\nregarding spectrum aspects. We then present preliminary system level simulation\nresults to shed light on the performance of HIBS. We conclude with pointing out\nseveral directions for future research.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:04:02 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Euler", "Sebastian", ""], ["Lin", "Xingqin", ""], ["Tejedor", "Erika", ""], ["Obregon", "Evanny", ""]]}, {"id": "2101.03109", "submitter": "Taoufik Yeferny", "authors": "Sofian Hamad and Taoufik Yeferny", "title": "Routing Approach for P2P Systems Over MANET Network", "comments": null, "journal-ref": "IJCSNS International Journal of Computer Science and Network\n  Security, VOL.20 No.3, March 2020", "doi": null, "report-no": null, "categories": "cs.NI cs.OS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Thanks to the great progress in mobile and wireless technologies,\nInternet-distributed applications like P2P file sharing are nowadays deployed\nover MANET (i.e., P2P mobile systems). These applications allow users to search\nand share diverse multimedia resources over MANET. Due the nature of MANET, P2P\nmobile systems brought up many new thriving challenges regarding the query\nrouting issue. To tackle this problem, we introduce a novel context-aware query\nrouting protocol for unstructured P2P mobile file sharing systems. Our protocol\n(i) locates relevant peers sharing pertinent resources for user's query and\n(ii) ensures that those peers would be reached by considering different MANET\nconstraints (e.g., query content, peer mobility, battery energy, peer load). In\norder to consider all these constraints for choosing the relevant peers, we are\nbased on the technique for order preferences by similarity to ideal solution\n(TOPSIS). We implemented the proposed protocol and compared its routing\nefficiency and retrieval effectiveness with another protocol taken from the\nliterature. Experimental results show that our scheme carries out better than\nthe baseline protocol with respect to accuracy\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:27:31 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Hamad", "Sofian", ""], ["Yeferny", "Taoufik", ""]]}, {"id": "2101.03212", "submitter": "Roberto Mag\\'an-Carri\\'on Dr.", "authors": "Roberto Mag\\'an-Carri\\'on and Alberto Abell\\'an-Galera and Gabriel\n  Maci\\'a-Fern\\'andez and Pedro Garc\\'ia-Teodoro", "title": "Unveiling the I2P web structure: a connectivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web is a primary and essential service to share information among users and\norganizations at present all over the world. Despite the current significance\nof such a kind of traffic on the Internet, the so-called Surface Web traffic\nhas been estimated in just about 5% of the total. The rest of the volume of\nthis type of traffic corresponds to the portion of Web known as Deep Web. These\ncontents are not accessible by search engines because they are authentication\nprotected contents or pages that are only reachable through the well known as\ndarknets. To browse through darknets websites special authorization or specific\nsoftware and configurations are needed. Despite TOR is the most used darknet\nnowadays, there are other alternatives such as I2P or Freenet, which offer\ndifferent features for end users. In this work, we perform an analysis of the\nconnectivity of websites in the I2P network (named eepsites) aimed to discover\nif different patterns and relationships from those used in legacy web are\nfollowed in I2P, and also to get insights about its dimension and structure.\nFor that, a novel tool is specifically developed by the authors and deployed on\na distributed scenario. Main results conclude the decentralized nature of the\nI2P network, where there is a structural part of interconnected eepsites while\nother several nodes are isolated probably due to their intermittent presence in\nthe network.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:35:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Mag\u00e1n-Carri\u00f3n", "Roberto", ""], ["Abell\u00e1n-Galera", "Alberto", ""], ["Maci\u00e1-Fern\u00e1ndez", "Gabriel", ""], ["Garc\u00eda-Teodoro", "Pedro", ""]]}, {"id": "2101.03218", "submitter": "Olakunle Ibitoye", "authors": "Olakunle Ibitoye, M. Omair Shafiq, Ashraf Matrawy", "title": "DiPSeN: Differentially Private Self-normalizing Neural Networks For\n  Adversarial Robustness in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The need for robust, secure and private machine learning is an important goal\nfor realizing the full potential of the Internet of Things (IoT). Federated\nlearning has proven to help protect against privacy violations and information\nleakage. However, it introduces new risk vectors which make machine learning\nmodels more difficult to defend against adversarial samples. In this study, we\nexamine the role of differential privacy and self-normalization in mitigating\nthe risk of adversarial samples specifically in a federated learning\nenvironment. We introduce DiPSeN, a Differentially Private Self-normalizing\nNeural Network which combines elements of differential privacy noise with\nself-normalizing techniques. Our empirical results on three publicly available\ndatasets show that DiPSeN successfully improves the adversarial robustness of a\ndeep learning classifier in a federated learning environment based on several\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 20:49:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Ibitoye", "Olakunle", ""], ["Shafiq", "M. Omair", ""], ["Matrawy", "Ashraf", ""]]}, {"id": "2101.03273", "submitter": "Saeed Kaviani", "authors": "Saeed Kaviani, Bo Ryu, Ejaz Ahmed, Kevin A. Larson, Anh Le, Alex\n  Yahja, Jae H. Kim", "title": "Robust and Scalable Routing with Multi-Agent Deep Reinforcement Learning\n  for MANETs", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly dynamic mobile ad-hoc networks (MANETs) are continuing to serve as one\nof the most challenging environments to develop and deploy robust, efficient,\nand scalable routing protocols. In this paper, we present DeepCQ+ routing\nwhich, in a novel manner, integrates emerging multi-agent deep reinforcement\nlearning (MADRL) techniques into existing Q-learning-based routing protocols\nand their variants, and achieves persistently higher performance across a wide\nrange of MANET configurations while training only on a limited range of network\nparameters and conditions. Quantitatively, DeepCQ+ shows consistently higher\nend-to-end throughput with lower overhead compared to its Q-learning-based\ncounterparts with the overall gain of 10-15% in its efficiency. Qualitatively\nand more significantly, DeepCQ+ maintains remarkably similar performance gains\nunder many scenarios that it was not trained for in terms of network sizes,\nmobility conditions, and traffic dynamics. To the best of our knowledge, this\nis the first successful demonstration of MADRL for the MANET routing problem\nthat achieves and maintains a high degree of scalability and robustness even in\nthe environments that are outside the trained range of scenarios. This implies\nthat the proposed hybrid design approach of DeepCQ+ that combines MADRL and\nQ-learning significantly increases its practicality and explainability because\nthe real-world MANET environment will likely vary outside the trained range of\nMANET scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 02:26:14 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 02:53:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kaviani", "Saeed", ""], ["Ryu", "Bo", ""], ["Ahmed", "Ejaz", ""], ["Larson", "Kevin A.", ""], ["Le", "Anh", ""], ["Yahja", "Alex", ""], ["Kim", "Jae H.", ""]]}, {"id": "2101.03300", "submitter": "Hang Chen", "authors": "Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis", "title": "Robust Blockchained Federated Learning with Model Validation and\n  Proof-of-Stake Inspired Consensus", "comments": "8 pages, 7 figures, AAAI 2021 Workshop - Towards Robust, Secure and\n  Efficient Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising distributed learning solution that\nonly exchanges model parameters without revealing raw data. However, the\ncentralized architecture of FL is vulnerable to the single point of failure. In\naddition, FL does not examine the legitimacy of local models, so even a small\nfraction of malicious devices can disrupt global training. To resolve these\nrobustness issues of FL, in this paper, we propose a blockchain-based\ndecentralized FL framework, termed VBFL, by exploiting two mechanisms in a\nblockchained architecture. First, we introduced a novel decentralized\nvalidation mechanism such that the legitimacy of local model updates is\nexamined by individual validators. Second, we designed a dedicated\nproof-of-stake consensus mechanism where stake is more frequently rewarded to\nhonest devices, which protects the legitimate local model updates by increasing\ntheir chances of dictating the blocks appended to the blockchain. Together,\nthese solutions promote more federation within legitimate devices, enabling\nrobust FL. Our emulation results of the MNIST classification corroborate that\nwith 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher\nthan Vanilla FL.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 06:30:38 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Hang", ""], ["Asif", "Syed Ali", ""], ["Park", "Jihong", ""], ["Shen", "Chien-Chung", ""], ["Bennis", "Mehdi", ""]]}, {"id": "2101.03441", "submitter": "Khashayar Kamran", "authors": "Khashayar Kamran, Armin Moharrer, Stratis Ioannidis, and Edmund Yeh", "title": "Rate Allocation and Content Placement in Cache Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the problem of optimal congestion control in cache networks,\nwhereby \\emph{both} rate allocations and content placements are optimized\n\\emph{jointly}. We formulate this as a maximization problem with non-convex\nconstraints, and propose solving this problem via (a) a Lagrangian barrier\nalgorithm and (b) a convex relaxation. We prove different optimality guarantees\nfor each of these two algorithms; our proofs exploit the fact that the\nnon-convex constraints of our problem involve DR-submodular functions.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 23:19:55 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 18:44:18 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Kamran", "Khashayar", ""], ["Moharrer", "Armin", ""], ["Ioannidis", "Stratis", ""], ["Yeh", "Edmund", ""]]}, {"id": "2101.03627", "submitter": "Jie Xu", "authors": "Jie Xu, Heqiang Wang, Lixing Chen", "title": "Bandwidth Allocation for Multiple Federated Learning Services in\n  Wireless Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a federated learning (FL) system, where \\textit{multiple}\nFL services co-exist in a wireless network and share common wireless resources.\nIt fills the void of wireless resource allocation for multiple simultaneous FL\nservices in the existing literature. Our method designs a two-level resource\nallocation framework comprising \\emph{intra-service} resource allocation and\n\\emph{inter-service} resource allocation. The intra-service resource allocation\nproblem aims to minimize the length of FL rounds by optimizing the bandwidth\nallocation among the clients of each FL service. Based on this, an\ninter-service resource allocation problem is further considered, which\ndistributes bandwidth resources among multiple simultaneous FL services. We\nconsider both cooperative and selfish providers of the FL services. For\ncooperative FL service providers, we design a distributed bandwidth allocation\nalgorithm to optimize the overall performance of multiple FL services,\nmeanwhile cater to the fairness among FL services and the privacy of clients.\nFor selfish FL service providers, a new auction scheme is designed with the FL\nservice owners as the bidders and the network provider as the auctioneer. The\ndesigned auction scheme strikes a balance between the overall FL performance\nand fairness. Our simulation results show that the proposed algorithms\noutperform other benchmarks under various network conditions.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 20:57:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Xu", "Jie", ""], ["Wang", "Heqiang", ""], ["Chen", "Lixing", ""]]}, {"id": "2101.03641", "submitter": "Jian Li", "authors": "Guojun Xiong, Rahul Singh, Jian Li", "title": "Learning Augmented Index Policy for Optimal Service Placement at the\n  Network Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of service placement at the network edge, in which a\ndecision maker has to choose between $N$ services to host at the edge to\nsatisfy the demands of customers. Our goal is to design adaptive algorithms to\nminimize the average service delivery latency for customers. We pose the\nproblem as a Markov decision process (MDP) in which the system state is given\nby describing, for each service, the number of customers that are currently\nwaiting at the edge to obtain the service. However, solving this $N$-services\nMDP is computationally expensive due to the curse of dimensionality. To\novercome this challenge, we show that the optimal policy for a single-service\nMDP has an appealing threshold structure, and derive explicitly the Whittle\nindices for each service as a function of the number of requests from customers\nbased on the theory of Whittle index policy.\n  Since request arrival and service delivery rates are usually unknown and\npossibly time-varying, we then develop efficient learning augmented algorithms\nthat fully utilize the structure of optimal policies with a low learning\nregret. The first of these is UCB-Whittle, and relies upon the principle of\noptimism in the face of uncertainty. The second algorithm, Q-learning-Whittle,\nutilizes Q-learning iterations for each service by using a two time scale\nstochastic approximation. We characterize the non-asymptotic performance of\nUCB-Whittle by analyzing its learning regret, and also analyze the convergence\nproperties of Q-learning-Whittle. Simulation results show that the proposed\npolicies yield excellent empirical performance.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 23:54:59 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 04:01:37 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Xiong", "Guojun", ""], ["Singh", "Rahul", ""], ["Li", "Jian", ""]]}, {"id": "2101.04003", "submitter": "Florian Meyer", "authors": "Florian Meyer and Volker Turau", "title": "QMA: A Ressource-efficient, Q-Learning-based Multiple Access Scheme for\n  the IIoT", "comments": "23 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contention-based wireless channel access methods like CSMA and ALOHA paved\nthe way for the rise of the Internet of Things in industrial applications\n(IIoT). However, to cope with increasing demands for reliability and\nthroughput, several mostly TDMA-based protocols like IEEE 802.15.4 and its\nextensions were proposed. Nonetheless, many of these IIoT-protocols still\nrequire contention-based communication, e.g., for slot allocation and broadcast\ntransmission. In many cases, subtle but hidden patterns characterize this\nsecondary traffic. Present contention-based protocols are unaware of these\nhidden patterns and can therefore not exploit this information. Especially in\ndense networks, they often do not provide sufficient reliability for primary\ntraffic, e.g., they are unable to allocate transmission slots in time. In this\npaper, we propose QMA, a contention-based multiple access scheme based on\nQ-learning, which dynamically adapts transmission times to avoid collisions by\nlearning patterns in the contention-based traffic. QMA is designed to be\nresource-efficient and targets small embedded devices. We show that QMA solves\nthe hidden node problem without the additional overhead of RTS / CTS messages\nand verify the behaviour of QMA in the FIT IoT-LAB testbed. Finally, QMA's\nscalability is studied by simulation, where it is used for GTS allocation in\nIEEE 802.15.4 DSME. Results show that QMA considerably increases reliability\nand throughput in comparison to CSMA/CA, especially in networks with a high\nload.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 16:24:19 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 08:39:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Meyer", "Florian", ""], ["Turau", "Volker", ""]]}, {"id": "2101.04216", "submitter": "Veronica Quintuna Rodriguez", "authors": "Veronica Quintuna Rodriguez, Fabrice Guillemin, Alexandre Ferrieux and\n  Laurent Thomas", "title": "Cloud-RAN functional split for an efficient fronthaul network", "comments": "Added to IEEE Xplore: 27 July 2020", "journal-ref": null, "doi": null, "report-no": "ISBN:978-1-7281-3130-6", "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of telecommunication network towards cloud-native environments\nenables flexible centralization of the base band processing of radio signals.\nThere is however a trade-off between the centralization benefits and the\nfronthaul cost for carrying the radio data between distributed antennas and\ndata processing centers, which host the virtual RAN functions. In this paper,\nwe present a specific split solution for an efficient fronthaul, which enables\nreducing the consumed bandwidth while being compliant with advanced cooperative\nradio technologies (interference reduction and data rate improvements). The\nproposed split has been implemented on the basis of Open Air Interface code and\nshows important gains in the required fronthaul bandwidth as well as\nsignificant latency reduction in the processing of radio frames. \\\\\n\\textbf{Publisher:} IEEE \\\\ \\textbf{ISBN:}978-1-7281-3130-6\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:15:19 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rodriguez", "Veronica Quintuna", ""], ["Guillemin", "Fabrice", ""], ["Ferrieux", "Alexandre", ""], ["Thomas", "Laurent", ""]]}, {"id": "2101.04224", "submitter": "Shruti Jadon", "authors": "Shruti Jadon, Jan Kanty Milczek, Ajit Patankar", "title": "Challenges and approaches to time-series forecasting in data center\n  telemetry: A Survey", "comments": "13 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Time-series forecasting has been an important research domain for so many\nyears. Its applications include ECG predictions, sales forecasting, weather\nconditions, even COVID-19 spread predictions. These applications have motivated\nmany researchers to figure out an optimal forecasting approach, but the\nmodeling approach also changes as the application domain changes. This work has\nfocused on reviewing different forecasting approaches for telemetry data\npredictions collected at data centers. Forecasting of telemetry data is a\ncritical feature of network and data center management products. However, there\nare multiple options of forecasting approaches that range from a simple linear\nstatistical model to high capacity deep learning architectures. In this paper,\nwe attempted to summarize and evaluate the performance of well known time\nseries forecasting techniques. We hope that this evaluation provides a\ncomprehensive summary to innovate in forecasting approaches for telemetry data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 22:36:21 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 21:55:24 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Jadon", "Shruti", ""], ["Milczek", "Jan Kanty", ""], ["Patankar", "Ajit", ""]]}, {"id": "2101.04249", "submitter": "Ish Jain", "authors": "Ish Kumar Jain, Raghav Subbaraman, Dinesh Bharadia", "title": "Two beams are better than one: Enabling reliable and high throughput\n  mmWave links", "comments": "17 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Millimeter-wave communication with high throughput and high reliability is\npoised to be a gamechanger for V2X and VR applications. However, mmWave links\nare notorious for low reliability since they suffer from frequent outages due\nto blockage and user mobility. Traditional mmWave systems are hardly reliable\nfor two reasons. First, they create a highly directional link that acts as a\nsingle point of failure and cannot be sustained for high user mobility. Second,\nthey follow a `reactive' approach, which reacts after the link has already\nsuffered an outage. We build mmReliable, a reliable mmWave system that\nimplements smart analog beamforming and user tracking to handle environmental\nvulnerabilities. It creates custom beam patterns with multiple lobes and\noptimizes their angle, phase, and amplitude to maximize the signal strength at\nthe receiver. Such phase-coherent multi-beam patterns allow the signal to\ntravel along multiple paths and add up constructively at the receiver to\nimprove throughput. Of course, multi-beam links are resilient to occasional\nblockages of few beams in multi-beam compared to a single-beam system. With\nuser mobility, mmReliable proactively tracks the motion in the background by\nleveraging continuous channel estimates without affecting the data rates. We\nimplement mmReliable on a 28 GHz testbed with 400 MHz bandwidth and a 64\nelement phased-array supporting 5G NR waveforms. Rigorous indoor and outdoor\nexperiments demonstrate that mmReliable achieves close to 100% reliability\nproviding 1.5 times better throughput than traditional single-beam systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 00:59:16 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Jain", "Ish Kumar", ""], ["Subbaraman", "Raghav", ""], ["Bharadia", "Dinesh", ""]]}, {"id": "2101.04338", "submitter": "Geong Sen Poh", "authors": "Geong Sen Poh, Dinil Mon Divakaran, Hoon Wei Lim, Jianting Ning,\n  Achintya Desai", "title": "A Survey of Privacy-Preserving Techniques for Encrypted Traffic\n  Inspection over Network Middleboxes", "comments": "17 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Middleboxes in a computer network system inspect and analyse network traffic\nto detect malicious communications, monitor system performance and provide\noperational services. However, encrypted traffic hinders the ability of\nmiddleboxes to perform such services. A common practice in addressing this\nissue is by employing a \"Man-in-the-Middle\" (MitM) approach, wherein an\nencrypted traffic flow between two endpoints is interrupted, decrypted and\nanalysed by the middleboxes. The MitM approach is straightforward and is used\nby many organisations, but there are both practical and privacy concerns. Due\nto the cost of the MitM appliances and the latency incurred in the\nencrypt-decrypt processes, enterprises continue to seek solutions that are less\ncostly. There were discussion on the many efforts required to configure MitM.\nBesides, MitM violates end-to-end privacy guarantee, raising privacy concerns\nand issues on compliance especially with the rising awareness on user privacy.\nFurthermore, some of the MitM implementations were found to be flawed.\nConsequently, new practical and privacy-preserving techniques for inspection\nover encrypted traffic were proposed. We examine them to compare their\nadvantages, limitations and challenges. We categorise them into four main\ncategories by defining a framework that consist of system architectures, use\ncases, trust and threat models. These are searchable encryption, access\ncontrol, machine learning and trusted hardware. We first discuss the\nman-in-the-middle approach as a baseline, then discuss in details each of them,\nand provide an in-depth comparisons of their advantages and limitations. By\ndoing so we describe practical constraints, advantages and pitfalls towards\nadopting the techniques. We also give insights on the gaps between research\nwork and industrial deployment, which leads us to the discussion on the\nchallenges and research directions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 07:59:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Poh", "Geong Sen", ""], ["Divakaran", "Dinil Mon", ""], ["Lim", "Hoon Wei", ""], ["Ning", "Jianting", ""], ["Desai", "Achintya", ""]]}, {"id": "2101.04403", "submitter": "Nicola Galesi", "authors": "Nicola Galesi, Fariba Ranjbar", "title": "Counting and localizing defective nodes by Boolean network tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying defective items in larger sets is a main problem with many\napplications in real life situations. We consider the problem of localizing\ndefective nodes in networks through an approach based on boolean network\ntomography (BNT), which is grounded on inferring informations from the boolean\noutcomes of end-to-end measurements paths. {\\em Identifiability} conditions on\nthe set of paths which guarantee discovering or counting unambiguously the\ndefective nodes are of course very relevant. We investigate old and introduce\nnew identifiability conditions contributing this problem both from a\ntheoretical and applied perspective. (1) What is the precise tradeoff between\nnumber of nodes and number of paths such that at most $k$ nodes can be\nidentified unambiguously ? The answer is known only for $k=1$ and we answer the\nquestion for any $k$, setting a problem implicitly left open in previous works.\n(2) We study upper and lower bounds on the number of unambiguously identifiable\nnodes, introducing new identifiability conditions which strictly imply and are\nstrictly implied by unambiguous identifiability; (3) We use these new\nconditions on one side to design algorithmic heuristics to count defective\nnodes in a fine-grained way, on the other side to prove the first complexity\nhardness results on the problem of identifying defective nodes in networks via\nBNT. (4) We introduce a random model where we study lower bounds on the number\nof unambiguously identifiable defective nodes and we use this model to estimate\nthat number on real networks by a maximum likelihood estimate approach\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:53:20 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Galesi", "Nicola", ""], ["Ranjbar", "Fariba", ""]]}, {"id": "2101.04427", "submitter": "Amoldeep Singh Mr.", "authors": "Amoldeep Singh, Kapal Dev, Harun Siljak, Hem Dutt Joshi and Maurizio\n  Magarini", "title": "Quantum Internet- Applications, Functionalities, Enabling Technologies,\n  Challenges, and Research Directions", "comments": "This survey paper is submitted in IEEE Communications Surveys and\n  Tutorials and revised on 27th May 2021. It includes 31 pages, 14 figures, and\n  5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The advanced notebooks, mobile phones, and internet applications in today's\nworld that we use are all entrenched in classical communication bits of zeros\nand ones. Classical internet has laid its foundation originating from the\namalgamation of mathematics and Claude Shannon's theory of information. But\ntoday's internet technology is a playground for eavesdroppers. This poses a\nserious challenge to various applications that relies on classical internet\ntechnology. This has motivated the researchers to switch to new technologies\nthat are fundamentally more secure. Exploring the quantum effects, researchers\npaved the way into quantum networks that provide security, privacy and range of\ncapabilities such as quantum computation, communication and metrology. The\nrealization of quantum internet requires quantum communication between various\nremote nodes through quantum channels guarded by quantum cryptographic\nprotocols. Such networks rely upon quantum bits (qubits) that can\nsimultaneously take the value of zeros and ones. Due to extraordinary\nproperties of qubits such as entanglement, teleportation and superposition, it\ngives an edge to quantum networks over traditional networks in many ways. But\nat the same time transmitting qubits over long distances is a formidable task\nand extensive research is going on quantum teleportation over such distances,\nwhich will become a breakthrough in physically realizing quantum internet in\nnear future. In this paper, quantum internet functionalities, technologies,\napplications and open challenges have been extensively surveyed to help readers\ngain a basic understanding of infrastructure required for the development of\nglobal quantum internet.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 11:57:04 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 17:03:20 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Singh", "Amoldeep", ""], ["Dev", "Kapal", ""], ["Siljak", "Harun", ""], ["Joshi", "Hem Dutt", ""], ["Magarini", "Maurizio", ""]]}, {"id": "2101.04539", "submitter": "Taoufik Yeferny", "authors": "Taoufik Yeferny and Sofian Hamad", "title": "Vehicular Ad-hoc Networks: Architecture, Applications and Challenges", "comments": null, "journal-ref": "IJCSNS International Journal of Computer Science and Network\n  Security, VOL.20 No.2, February 2020", "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the emergence of Information and Communication Technologies (ICT) and\nwireless embedded sensing devices into modern vehicles, Intelligent Transport\nSystem (ITS) becomes a reality and an indispensable component of smart cities.\nThe purpose of ITS is to improve road safety and traffic efficiency as well as\noffering infotainments services. In fact, warning drivers in the right time\nabout dangerous situations on the road and providing them with prior\ninformation about traffic will undoubtedly leads to enhance driver's safety and\nreduce traffic congestion. Technically speaking, ITS is based on\nself-organizing wireless networks, known as vehicular ad-hoc networks (VANETs).\nMobile vehicles in VANET might play the role of stationary sensors in\ninfrastructure-based networks. They can detect, gather and disseminate\nreal-time data about traffic, driving conditions and potential hazards on\nroads. In this respect, we review in this study, recent developments on the\ndesign of VANET protocols and applications. We first introduce the architecture\nof VANETs then we review their unique characteristics and applications.\nThereafter, we discuss the main research challenges and open issues to be\nconsidered for designing efficient and a cost-effective VANET protocols and\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:15:08 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Yeferny", "Taoufik", ""], ["Hamad", "Sofian", ""]]}, {"id": "2101.04556", "submitter": "Manjesh Kumar Hanawal", "authors": "Vinod S. Khandkar and Manjesh K. Hanawal", "title": "Masking Host Identity on Internet: Encrypted TLS/SSL Handshake", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network middle-boxes often classify the traffic flows on the Internet to\nperform traffic management or discriminate one traffic against the other. As\nthe widespread adoption of HTTPS protocol has made it difficult to classify the\ntraffic looking into the content field, one of the fields the middle-boxes look\nfor is Server Name Indicator (SNI), which goes in plain text. SNI field\ncontains information about the host and can, in turn, reveal the type of\ntraffic. This paper presents a method to mask the server host identity by\nencrypting the SNI. We develop a simple method that completes the SSL/TLS\nconnection establishment over two handshakes - the first handshake establishes\na secure channel without sharing SNI information, and the second handshake\nshares the encrypted SNI. Our method makes it mandatory for fronting servers to\nalways accept the handshake request without the SNI and respond with a valid\nSSL certificate.\n  As there is no modification in already proven SSL/TLS encryption mechanism\nand processing of handshake messages, the new method enjoys all security\nbenefits of existing secure channel establishment and needs no modification in\nexisting routers/middle-boxes. Using customized client-server over the live\nInternet, we demonstrate the feasibility of our method. Moreover, the impact\nanalysis shows that the method adheres to almost all SSL/TLS related Internet\nstandards requirements.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:48:36 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Khandkar", "Vinod S.", ""], ["Hanawal", "Manjesh K.", ""]]}, {"id": "2101.04627", "submitter": "Majid Raeis", "authors": "Majid Raeis, Ali Tizghadam, Alberto Leon-Garcia", "title": "Queue-Learning: A Reinforcement Learning Approach for Providing Quality\n  of Service", "comments": "8 pages, Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end delay is a critical attribute of quality of service (QoS) in\napplication domains such as cloud computing and computer networks. This metric\nis particularly important in tandem service systems, where the end-to-end\nservice is provided through a chain of services. Service-rate control is a\ncommon mechanism for providing QoS guarantees in service systems. In this\npaper, we introduce a reinforcement learning-based (RL-based) service-rate\ncontroller that provides probabilistic upper-bounds on the end-to-end delay of\nthe system, while preventing the overuse of service resources. In order to have\na general framework, we use queueing theory to model the service systems.\nHowever, we adopt an RL-based approach to avoid the limitations of\nqueueing-theoretic methods. In particular, we use Deep Deterministic Policy\nGradient (DDPG) to learn the service rates (action) as a function of the queue\nlengths (state) in tandem service systems. In contrast to existing RL-based\nmethods that quantify their performance by the achieved overall reward, which\ncould be hard to interpret or even misleading, our proposed controller provides\nexplicit probabilistic guarantees on the end-to-end delay of the system. The\nevaluations are presented for a tandem queueing system with non-exponential\ninter-arrival and service times, the results of which validate our controller's\ncapability in meeting QoS constraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:28:57 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raeis", "Majid", ""], ["Tizghadam", "Ali", ""], ["Leon-Garcia", "Alberto", ""]]}, {"id": "2101.04790", "submitter": "Wilder Castellanos", "authors": "Wilder Castellanos", "title": "Evaluation of quality scalability techniques for video transmission", "comments": "12 pages, 11 figures. Language Spanish. This work is a Chapter of the\n  eBook: \"Avances de la Ingenieria Bonaventuriana\". ISBN 978-958-8928-85-2\n  Bogota. Colombia. 2019", "journal-ref": "Avances de la Ingenieria Bonaventuriana. Editorial Bonaventuriana.\n  ISBN 978-958-8928-85-2. Bogota. Colombia. (2019)", "doi": null, "report-no": null, "categories": "cs.NI cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The significant increase of the transmission of multimedia content over\nInternet are demanded new delivery strategies to assure a good quality of\nexperience of the users. Transmission of video over packet networks is not an\neasy task due to multiple fluctuations of the network conditions. One\npossibility to improve the quality of some video streaming services is the\ncombinate use of the scalable video coding and cross layer mechanisms that\nallow applications to adapt its traffic stream to the resources network that\nare available. In this paper, it is presented a performance evaluation of the\nthree main scalability techniques: CGS (Coarse-Grained Scalability), FGS (Fine\nGrain scalability) and MGS (Medium Grain Scalability). In particular, we focus\non determining what method is more appropriated for video transmission taking\ninto account some video quality metrics like PSNR (Peak Signal-to-Noise Ratio)\nand decoded frame rate. The results reveal that the rate-adaptive strategy and\nthe MGS technique help avoid or reduce the congestion in networks obtaining a\nbetter quality in the received videos.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 22:53:10 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Castellanos", "Wilder", ""]]}, {"id": "2101.04815", "submitter": "Christos Tsanikidis", "authors": "Christos Tsanikidis, Javad Ghaderi", "title": "Randomized Scheduling of Real-Time Traffic in Wireless Networks Over\n  Fading Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.IT math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the rich literature on scheduling algorithms for wireless networks,\nalgorithms that can provide deadline guarantees on packet delivery for general\ntraffic and interference models are very limited. In this paper, we study the\nproblem of scheduling real-time traffic under a conflict-graph interference\nmodel with unreliable links due to channel fading. Packets that are not\nsuccessfully delivered within their deadlines are of no value. We consider\ntraffic (packet arrival and deadline) and fading (link reliability) processes\nthat evolve as an unknown finite-state Markov chain. The performance metric is\nefficiency ratio which is the fraction of packets of each link which are\ndelivered within their deadlines compared to that under the optimal (unknown)\npolicy. We first show a conversion result that shows classical non-real-time\nscheduling algorithms can be ported to the real-time setting and yield a\nconstant efficiency ratio, in particular, Max-Weight Scheduling (MWS) yields an\nefficiency ratio of 1/2. We then propose randomized algorithms that achieve\nefficiency ratios strictly higher than 1/2, by carefully randomizing over the\nmaximal schedules. We further propose low-complexity and myopic distributed\nrandomized algorithms, and characterize their efficiency ratio. Simulation\nresults are presented that verify that randomized algorithms outperform\nclassical algorithms such as MWS and GMS.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:11:33 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Tsanikidis", "Christos", ""], ["Ghaderi", "Javad", ""]]}, {"id": "2101.04825", "submitter": "Dimitris Chatzopoulos", "authors": "Dimitris Chatzopoulos, Anurag Jain, Sujit Gujar, Boi Faltings, Pan Hui", "title": "Towards Mobile Distributed Ledgers", "comments": "Part of it was presented in IEEE Infocom 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in mobile computing have paved the way for new types of distributed\napplications that can be executed solely by mobile devices on device-to-device\n(D2D) ecosystems (e.g., crowdsensing). Sophisticated applications, like\ncryptocurrencies, need distributed ledgers to function. Distributed ledgers,\nsuch as blockchains and directed acyclic graphs (DAGs), employ consensus\nprotocols to add data in the form of blocks. However, such protocols are\ndesigned for resourceful devices that are interconnected via the Internet.\nMoreover, existing distributed ledgers are not deployable to D2D ecosystems\nsince their storage needs are continuously increasing. In this work, we\nintroduce and analyse Mneme, a DAG-based distributed ledger that can be\nmaintained solely by mobile devices. Mneme utilizes two novel consensus\nprotocols: Proof-of-Context (PoC) and Proof-of-Equivalence (PoE). PoC employs\nusers' context to add data on Mneme. PoE is executed periodically to summarize\ndata and produce equivalent blocks that require less storage. We analyze\nMneme's security and justify the ability of PoC and PoE to guarantee the\ncharacteristics of distributed ledgers: persistence and liveness. Furthermore,\nwe analyze potential attacks from malicious users and prove that the\nprobability of a successful attack is inversely proportional to the square of\nthe number of mobile users who maintain Mneme.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 01:26:36 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Chatzopoulos", "Dimitris", ""], ["Jain", "Anurag", ""], ["Gujar", "Sujit", ""], ["Faltings", "Boi", ""], ["Hui", "Pan", ""]]}, {"id": "2101.04866", "submitter": "Dian Shi", "authors": "Dian Shi, Liang Li, Rui Chen, Pavana Prakash, Miao Pan, Yuguang Fang", "title": "Towards Energy Efficient Federated Learning over 5G+ Mobile Devices", "comments": "submitted to IEEE Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The continuous convergence of machine learning algorithms, 5G and beyond\n(5G+) wireless communications, and artificial intelligence (AI) hardware\nimplementation hastens the birth of federated learning (FL) over 5G+ mobile\ndevices, which pushes AI functions to mobile devices and initiates a new era of\non-device AI applications. Despite the remarkable progress made in FL, huge\nenergy consumption is one of the most significant obstacles restricting the\ndevelopment of FL over battery-constrained 5G+ mobile devices. To address this\nissue, in this paper, we investigate how to develop energy efficient FL over\n5G+ mobile devices by making a trade-off between energy consumption for\n\"working\" (i.e., local computing) and that for \"talking\" (i.e., wireless\ncommunications) in order to boost the overall energy efficiency. Specifically,\nwe first examine energy consumption models for graphics processing unit (GPU)\ncomputation and wireless transmissions. Then, we overview the state of the art\nof integrating FL procedure with energy-efficient learning techniques (e.g.,\ngradient sparsification, weight quantization, pruning, etc.). Finally, we\npresent several potential future research directions for FL over 5G+ mobile\ndevices from the perspective of energy efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 04:13:54 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Shi", "Dian", ""], ["Li", "Liang", ""], ["Chen", "Rui", ""], ["Prakash", "Pavana", ""], ["Pan", "Miao", ""], ["Fang", "Yuguang", ""]]}, {"id": "2101.05030", "submitter": "Ahmed Elmokashfi", "authors": "Ahmed Elmokashfi, Alfred Arouna, Ioana Livadariu, Mah-Rukh Fida, Amund\n  Kvalbein, Anas Al-Selwi, Thomas Dreibholz, Haakon Bryhni", "title": "A Multi-Perspective Study of Internet Performance during the COVID-19\n  Outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid spread of the novel corona virus, SARS-CoV-2, has prompted an\nunprecedented response from governments across the world. A third of the world\npopulation have been placed in varying degrees of lockdown, and the Internet\nhas become the primary medium for conducting most businesses and schooling\nactivities. This paper aims to provide a multi-prospective account of Internet\nperformance during the first wave of the pandemic. We investigate the\nperformance of the Internet control plane and data plane from a number of\nglobally spread vantage points. We also look closer at two case studies. First,\nwe look at growth in video traffic during the pandemic, using traffic logs from\na global video conferencing provider. Second, we leverage a country-wide\ndeployment of measurement probes to assess the performance of mobile networks\nduring the outbreak. We find that the lockdown has visibly impacted almost all\naspects of Internet performance. Access networks have experienced an increase\nin peak and off-peak end to end latency. Mobile networks exhibit significant\nchanges in download speed, while certain types of video traffic has increased\nby an order of magnitude. Despite these changes, the Internet seems to have\ncoped reasonably well with the lockdown traffic.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:29:42 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Elmokashfi", "Ahmed", ""], ["Arouna", "Alfred", ""], ["Livadariu", "Ioana", ""], ["Fida", "Mah-Rukh", ""], ["Kvalbein", "Amund", ""], ["Al-Selwi", "Anas", ""], ["Dreibholz", "Thomas", ""], ["Bryhni", "Haakon", ""]]}, {"id": "2101.05102", "submitter": "Roberto Natella", "authors": "Roberto Natella and Van-Thuan Pham", "title": "ProFuzzBench: A Benchmark for Stateful Protocol Fuzzing", "comments": "The source code of ProFuzzBench is available online on GitHub at:\n  https://github.com/profuzzbench/profuzzbench", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new benchmark (ProFuzzBench) for stateful fuzzing of network\nprotocols. The benchmark includes a suite of representative open-source network\nservers for popular protocols, and tools to automate experimentation. We\ndiscuss challenges and potential directions for future research based on this\nbenchmark.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 14:36:48 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Natella", "Roberto", ""], ["Pham", "Van-Thuan", ""]]}, {"id": "2101.05323", "submitter": "S\\'ebastien Vaucher", "authors": "S\\'ebastien Vaucher, Niloofar Yazdani, Pascal Felber, Daniel E.\n  Lucani, Valerio Schiavoni", "title": "ZipLine: In-Network Compression at Line Speed", "comments": null, "journal-ref": "2020. Proceedings of the 16th International Conference on emerging\n  Networking EXperiments and Technologies. Association for Computing Machinery,\n  New York, NY, USA, 399-405", "doi": "10.1145/3386367.3431302", "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network appliances continue to offer novel opportunities to offload\nprocessing from computing nodes directly into the data plane. One popular\nconcern of network operators and their customers is to move data increasingly\nfaster. A common technique to increase data throughput is to compress it before\nits transmission. However, this requires compression of the data -- a time and\nenergy demanding pre-processing phase -- and decompression upon reception -- a\nsimilarly resource consuming operation. Moreover, if multiple nodes transfer\nsimilar data chunks across the network hop (e.g., a given pair of switches),\neach node effectively wastes resources by executing similar steps. This paper\nproposes ZipLine, an approach to design and implement (de)compression at line\nspeed leveraging the Tofino hardware platform which is programmable using the\nP4_16 language. We report on lessons learned while building the system and show\nthroughput, latency and compression measurements on synthetic and real-world\ntraces, showcasing the benefits and trade-offs of our design.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:59:25 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vaucher", "S\u00e9bastien", ""], ["Yazdani", "Niloofar", ""], ["Felber", "Pascal", ""], ["Lucani", "Daniel E.", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2101.05462", "submitter": "Haiwen Du Dr.", "authors": "Dongjie Zhu, Haiwen Du, Yundong Sun, Zhaoshuo Tian", "title": "Leader Confirmation Replication for Millisecond Consensus in\n  Geo-distributed Systems", "comments": "Prepared to submit to SRDS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed private chain and database have created higher performance\nrequirements for consistency models. However, with millisecond network latency\nbetween nodes, the widely used leader-based SMR models cause frequent\nretransmission of logs since they cannot know the logs replication status in\ntime, which resulting in the leader costing high network and computing\nresource. To address the problem, we proposed a Leader Confirmation based\nReplication (LCR) model. First, we demonstrate the efficacy of the approach by\ndesigning the Future-Log Replication model, which the followers are responsible\nfor non-transactional log replication. It reduces the leader's network load\nusing the signal log. Secondly, we designed a Generation Re-replication\nstrategy, which can ensure the security and consistency of future-logs when the\nnumber of nodes changes. Finally, we implemented LCR-Raft and designed\nexperiments. The results show that in the single-ms network latency\nenvironments, LCR-Raft can provide 1.5X higher TPS, reduces transactional data\nresponse time 40%-60%, and network traffic by 20%-30% with acceptable network\ntraffic and CPU cost on followers. Besides, LCR can provide high portability\nsince it does not change the number of leader and election process.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 05:25:04 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:27:47 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhu", "Dongjie", ""], ["Du", "Haiwen", ""], ["Sun", "Yundong", ""], ["Tian", "Zhaoshuo", ""]]}, {"id": "2101.05607", "submitter": "Mahdi Jafari Siavoshani", "authors": "Mahdi Jafari Siavoshani and Seyed Pooya Shariatpanahi and Naeimeh\n  Omidvar", "title": "Intelligent Reflecting Surfaces for Compute-and-Forward", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute-and-forward is a promising strategy to tackle interference and obtain\nhigh rates between the transmitting users in a wireless network. However, the\nquality of the wireless channels between the users substantially limits the\nachievable computation rate in such systems. In this paper, we introduce the\nidea of using intelligent reflecting surfaces (IRSs) to enhance the computing\ncapability of the compute-and-forward systems. For this purpose, we consider a\nmultiple access channel(MAC) where a number of users aim to send data to a base\nstation (BS) in a wireless network, where the BS is interested in decoding a\nlinear combination of the data from different users in the corresponding finite\nfield. Considering the compute-and-forward framework, we show that through\ncarefully designing the IRS parameters, such a scenario's computation rate can\nbe significantly improved. More specifically, we formulate an optimization\nproblem which aims to maximize the computation rate of the system through\noptimizing the IRS phase shift parameters. We then propose an alternating\noptimization (AO) approach to solve the formulated problem with low complexity.\nFinally, via various numerical results, we demonstrate the effectiveness of the\nIRS technology for enhancing the performance of the compute-and-forward\nsystems, which indicates its great potential for future wireless networks with\nmassive computation requirements, such as 6G.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 14:22:44 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 10:54:46 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Siavoshani", "Mahdi Jafari", ""], ["Shariatpanahi", "Seyed Pooya", ""], ["Omidvar", "Naeimeh", ""]]}, {"id": "2101.05706", "submitter": "Wilder Castellanos", "authors": "Harold Pinilla, Jose Macias, Emmanuel Lescano, Jose David Alvarado,\n  Wilder Castellanos", "title": "Environmental Variable Monitoring with IoT Technology", "comments": "14 pages, in Spanish. 12 figures. Work presented in TechFest 2019\n  Conference. organized by Universidad de San Buenaventura. Bogota. Colombia.\n  https://www.usbbog.edu.co/images/pdf/educontinua/tech_fest_agenda.pdf\n  Proceedings in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article describes the design of a flexible and low-cost platform for\nmonitoring environmental variables applied to agriculture. For the construction\nof this platform, technologies based on the communication protocol, Wi-Fi,\nBluetooth, and Zigbee were used, using the embedded Raspberry pi 3 b + system\nand sensors to quantify different environmental variables, using different open\nsource hardware and software tools. The network is made up of a central node\n(gateway), implemented on Samsung's Artik 1020 card, and two nodes where the\nsensors for reading environmental variables are connected. Finally, the data is\ncollected by the gateway, which will be in charge of processing and storing it\nin a database so that the user in the future can access the information in real\ntime from anywhere.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:31:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pinilla", "Harold", ""], ["Macias", "Jose", ""], ["Lescano", "Emmanuel", ""], ["Alvarado", "Jose David", ""], ["Castellanos", "Wilder", ""]]}, {"id": "2101.05768", "submitter": "Yi Shi", "authors": "Yi Shi, Yalin E. Sagduyu, Tugba Erpek, M. Cenk Gursoy", "title": "How to Attack and Defend 5G Radio Access Network Slicing with\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) for network slicing is considered in the 5G radio\naccess network, where the base station, gNodeB, allocates resource blocks (RBs)\nto the requests of user equipments and maximizes the total reward of accepted\nrequests over time. Based on adversarial machine learning, a novel over-the-air\nattack is introduced to manipulate the RL algorithm and disrupt 5G network\nslicing. Subject to an energy budget, the adversary observes the spectrum and\nbuilds its own RL-based surrogate model that selects which RBs to jam with the\nobjective of maximizing the number of failed network slicing requests due to\njammed RBs. By jamming the RBs, the adversary reduces the RL algorithm's\nreward. As this reward is used as the input to update the RL algorithm, the\nperformance does not recover even after the adversary stops jamming. This\nattack is evaluated in terms of the recovery time and the (maximum and total)\nreward loss, and it is shown to be much more effective than benchmark (random\nand myopic) jamming attacks. Different reactive and proactive defense\nmechanisms (protecting the RL algorithm's updates or misleading the adversary's\nlearning process) are introduced to show that it is viable to defend 5G network\nslicing against this attack.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:19:05 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Shi", "Yi", ""], ["Sagduyu", "Yalin E.", ""], ["Erpek", "Tugba", ""], ["Gursoy", "M. Cenk", ""]]}, {"id": "2101.05792", "submitter": "Batuhan Arasli", "authors": "Batuhan Arasli and Sennur Ulukus", "title": "Group Testing with a Graph Infection Spread Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CY cs.DS cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel infection spread model based on a random connection graph\nwhich represents connections between $n$ individuals. Infection spreads via\nconnections between individuals and this results in a probabilistic cluster\nformation structure as well as a non-i.i.d. (correlated) infection status for\nindividuals. We propose a class of two-step sampled group testing algorithms\nwhere we exploit the known probabilistic infection spread model. We investigate\nthe metrics associated with two-step sampled group testing algorithms. To\ndemonstrate our results, for analytically tractable exponentially split cluster\nformation trees, we calculate the required number of tests and the expected\nnumber of false classifications in terms of the system parameters, and identify\nthe trade-off between them. For such exponentially split cluster formation\ntrees, for zero-error construction, we prove that the required number of tests\nis $O(\\log_2n)$. Thus, for such cluster formation trees, our algorithm\noutperforms any zero-error non-adaptive group test, binary splitting algorithm,\nand Hwang's generalized binary splitting algorithm. Our results imply that, by\nexploiting probabilistic information on the connections of individuals, group\ntesting can be used to reduce the number of required tests significantly even\nwhen infection rate is high, contrasting the prevalent belief that group\ntesting is useful only when infection rate is low.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:51:32 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Arasli", "Batuhan", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2101.05865", "submitter": "Roch Guerin", "authors": "Chong Li, Jiangnan Liu, Chenyang Lu, Roch Guerin, Christopher D. Gill", "title": "Impact of Distributed Rate Limiting on Load Distribution in a\n  Latency-sensitive Messaging Service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The cloud's flexibility and promise of seamless auto-scaling notwithstanding,\nits ability to meet service level objectives (SLOs) typically calls for some\nform of control in resource usage. This seemingly traditional problem gives\nrise to new challenges in a cloud setting, and in particular a subtle yet\nsignificant trade-off involving load-distribution decisions (the distribution\nof workload across available cloud resources to optimize performance), and rate\nlimiting (the capping of individual workloads to prevent global\nover-commitment). This paper investigates that trade-off through the design and\nimplementation of a real-time messaging system motivated by Internet-of-Things\n(IoT) applications, and demonstrates a solution capable of realizing an\neffective compromise. The paper's contributions are in both explicating the\nsource of this trade-off, and in demonstrating a possible solution.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 20:47:32 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Li", "Chong", ""], ["Liu", "Jiangnan", ""], ["Lu", "Chenyang", ""], ["Guerin", "Roch", ""], ["Gill", "Christopher D.", ""]]}, {"id": "2101.05885", "submitter": "Tongyu Zong", "authors": "Tongyu Zong, Chen Li, Yuanyuan Lei, Guangyu Li, Houwei Cao, Yong Liu", "title": "Cocktail Edge Caching: Ride Dynamic Trends of Content Popularity with\n  Ensemble Learning", "comments": "INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge caching will play a critical role in facilitating the emerging\ncontent-rich applications. However, it faces many new challenges, in\nparticular, the highly dynamic content popularity and the heterogeneous caching\nconfigurations. In this paper, we propose Cocktail Edge Caching, that tackles\nthe dynamic popularity and heterogeneity through ensemble learning. Instead of\ntrying to find a single dominating caching policy for all the caching\nscenarios, we employ an ensemble of constituent caching policies and adaptively\nselect the best-performing policy to control the cache. Towards this goal, we\nfirst show through formal analysis and experiments that different variations of\nthe LFU and LRU policies have complementary performance in different caching\nscenarios. We further develop a novel caching algorithm that enhances LFU/LRU\nwith deep recurrent neural network (LSTM) based time-series analysis. Finally,\nwe develop a deep reinforcement learning agent that adaptively combines base\ncaching policies according to their virtual hit ratios on parallel virtual\ncaches. Through extensive experiments driven by real content requests from two\nlarge video streaming platforms, we demonstrate that CEC not only consistently\noutperforms all single policies, but also improves the robustness of them. CEC\ncan be well generalized to different caching scenarios with low computation\noverheads for deployment.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 21:59:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zong", "Tongyu", ""], ["Li", "Chen", ""], ["Lei", "Yuanyuan", ""], ["Li", "Guangyu", ""], ["Cao", "Houwei", ""], ["Liu", "Yong", ""]]}, {"id": "2101.05952", "submitter": "Beibei Zhang", "authors": "Beibei Zhang, Tian Xiang, Hongxuan Zhang, Te Li, Shiqiang Zhu, Jianjun\n  Gu", "title": "Dynamic DNN Decomposition for Lossless Synergistic Inference", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) sustain high performance in today's data\nprocessing applications. DNN inference is resource-intensive thus is difficult\nto fit into a mobile device. An alternative is to offload the DNN inference to\na cloud server. However, such an approach requires heavy raw data transmission\nbetween the mobile device and the cloud server, which is not suitable for\nmission-critical and privacy-sensitive applications such as autopilot. To solve\nthis problem, recent advances unleash DNN services using the edge computing\nparadigm. The existing approaches split a DNN into two parts and deploy the two\npartitions to computation nodes at two edge computing tiers. Nonetheless, these\nmethods overlook collaborative device-edge-cloud computation resources.\nBesides, previous algorithms demand the whole DNN re-partitioning to adapt to\ncomputation resource changes and network dynamics. Moreover, for\nresource-demanding convolutional layers, prior works do not give a parallel\nprocessing strategy without loss of accuracy at the edge side. To tackle these\nissues, we propose D3, a dynamic DNN decomposition system for synergistic\ninference without precision loss. The proposed system introduces a heuristic\nalgorithm named horizontal partition algorithm to split a DNN into three parts.\nThe algorithm can partially adjust the partitions at run time according to\nprocessing time and network conditions. At the edge side, a vertical separation\nmodule separates feature maps into tiles that can be independently run on\ndifferent edge nodes in parallel. Extensive quantitative evaluation of five\npopular DNNs illustrates that D3 outperforms the state-of-the-art counterparts\nup to 3.4 times in end-to-end DNN inference time and reduces backbone network\ncommunication overhead up to 3.68 times.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 03:18:53 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Zhang", "Beibei", ""], ["Xiang", "Tian", ""], ["Zhang", "Hongxuan", ""], ["Li", "Te", ""], ["Zhu", "Shiqiang", ""], ["Gu", "Jianjun", ""]]}, {"id": "2101.06015", "submitter": "Jeroen Keiren", "authors": "Anna Stramaglia, Jeroen J.A. Keiren and Hans Zantema", "title": "Deadlock in packet switching networks", "comments": "This is a version with full proofs of the preprint that was submitted\n  to FSEN 2021, and accepted for publication in that conference (to appear in\n  Springer LNCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deadlock in a packet switching network is a state in which one or more\nmessages have not yet reached their target, yet cannot progress any further. We\nformalize three different notions of deadlock in the context of packet\nswitching networks, to which we refer as global, local and weak deadlock. We\nestablish the precise relations between these notions, and prove they\ncharacterize different sets of deadlocks. Moreover, we implement checking of\ndeadlock freedom of packet switching networks using the symbolic model checker\nnuXmv. We show experimentally that the implementation is effective at finding\nsubtle deadlock situations in packet switching networks.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 08:42:35 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Stramaglia", "Anna", ""], ["Keiren", "Jeroen J. A.", ""], ["Zantema", "Hans", ""]]}, {"id": "2101.06028", "submitter": "Pengfei Ma", "authors": "Pengfei Ma, Hancheng Lu, Ming Zhang, Jinxue Liu, Ruoyun Chen", "title": "QoS-Driven Video Uplinking in NOMA-Based IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the explosive growth of visual sensors and a large\nnumber of related video applications in Internet of Things (IoT), massive video\ndata is generated by IoT devices. Since the volume of video data is far greater\nthan traditional data in IoT, it is challenging to ensure high Quality of\nService (QoS) for video uplinking in IoT. To address this challenge, we\nintegrate non-orthogonal multiple access (NOMA) and scalable video coding (SVC)\nin IoT. To improve the video quality, we formulate a power allocation problem\nto maximize the average QoS in the proposed integrated system. Due to that the\nproblem is non-convex, we transform it into a monotonic problem based on its\nhidden monotonicity. Then a power allocation algorithm based on polyblock outer\napproximation is proposed to solve the problem effectively. Finally, simulation\nresults demonstrate that the proposed algorithm outperforms existing OMA and\nNOMA based schemes for video uplinking in IoT in terms of QoS and energy\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:32:05 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ma", "Pengfei", ""], ["Lu", "Hancheng", ""], ["Zhang", "Ming", ""], ["Liu", "Jinxue", ""], ["Chen", "Ruoyun", ""]]}, {"id": "2101.06056", "submitter": "Xu Chen", "authors": "Shuai Yu and Xiaowen Gong and Qian Shi and Xiaofei Wang and Xu Chen", "title": "EC-SAGINs: Edge Computing-enhanced Space-Air-Ground Integrated Networks\n  for Internet of Vehicles", "comments": "The paper is accepted by IEEE IoTJ, Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing-enhanced Internet of Vehicles (EC-IoV) enables ubiquitous data\nprocessing and content sharing among vehicles and terrestrial edge computing\n(TEC) infrastructures (e.g., 5G base stations and roadside units) with little\nor no human intervention, plays a key role in the intelligent transportation\nsystems. However, EC-IoV is heavily dependent on the connections and\ninteractions between vehicles and TEC infrastructures, thus will break down in\nsome remote areas where TEC infrastructures are unavailable (e.g., desert,\nisolated islands and disaster-stricken areas). Driven by the ubiquitous\nconnections and global-area coverage, space-air-ground integrated networks\n(SAGINs) efficiently support seamless coverage and efficient resource\nmanagement, represent the next frontier for edge computing. In light of this,\nwe first review the state-of-the-art edge computing research for SAGINs in this\narticle. After discussing several existing orbital and aerial edge computing\narchitectures, we propose a framework of edge computing-enabled\nspace-air-ground integrated networks (EC-SAGINs) to support various IoV\nservices for the vehicles in remote areas. The main objective of the framework\nis to minimize the task completion time and satellite resource usage. To this\nend, a pre-classification scheme is presented to reduce the size of action\nspace, and a deep imitation learning (DIL) driven offloading and caching\nalgorithm is proposed to achieve real-time decision making. Simulation results\nshow the effectiveness of our proposed scheme. At last, we also discuss some\ntechnology challenges and future directions.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:56:23 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Yu", "Shuai", ""], ["Gong", "Xiaowen", ""], ["Shi", "Qian", ""], ["Wang", "Xiaofei", ""], ["Chen", "Xu", ""]]}, {"id": "2101.06129", "submitter": "Marie Siew", "authors": "Marie Siew, Kun Guo, Desmond Cai, Lingxiang Li, Tony Q.S. Quek", "title": "Let's Share VMs: Optimal Placement and Pricing across Base Stations in\n  MEC Systems", "comments": "Accepted at IEEE INFOCOM 2021 - IEEE Conference on Computer\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI cs.SY eess.SY math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mobile edge computing (MEC) systems, users offload computationally\nintensive tasks to edge servers at base stations. However, with unequal demand\nacross the network, there might be excess demand at some locations and\nunderutilized resources at other locations. To address such load-unbalanced\nproblem in MEC systems, in this paper we propose virtual machines (VMs) sharing\nacross base stations. Specifically, we consider the joint VM placement and\npricing problem across base stations to match demand and supply and maximize\nrevenue at the network level. To make this problem tractable, we decompose it\ninto master and slave problems. For the placement master problem, we propose a\nMarkov approximation algorithm MAP on the design of a continuous time Markov\nchain. As for the pricing slave problem, we propose OPA - an optimal VM pricing\nauction, where all users are truthful. Furthermore, given users' potential\nuntruthful behaviors, we propose an incentive compatible auction iCAT along\nwith a partitioning mechanism PUFF, for which we prove incentive compatibility\nand revenue guarantees. Finally, we combine MAP and OPA or PUFF to solve the\noriginal problem, and analyze the optimality gap. Simulation results show that\ncollaborative base stations increases revenue by up to 50%.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:20:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Siew", "Marie", ""], ["Guo", "Kun", ""], ["Cai", "Desmond", ""], ["Li", "Lingxiang", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2101.06210", "submitter": "Konstantinos Stylianou", "authors": "Konstantinos Stylianou, Leonhard Spiegelberg, Maurice Herlihy, Nic\n  Carter", "title": "Cryptoasset Competition and Market Concentration in the Presence of\n  Network Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When network products and services become more valuable as their userbase\ngrows (network effects), this tendency can become a major determinant of how\nthey compete with each other in the market and how the market is structured.\nNetwork effects are traditionally linked to high market concentration,\nearly-mover advantages, and entry barriers, and in the cryptoasset market they\nhave been used as a valuation tool too. The recent resurgence of Bitcoin has\nbeen partly attributed to network effects too. We study the existence of\nnetwork effects in six cryptoassets from their inception to obtain a high-level\noverview of the application of network effects in the cryptoasset market. We\nshow that contrary to the usual implications of network effects, they do not\nserve to concentrate the cryptoasset market, nor do they accord any one\ncryptoasset a definitive competitive advantage, nor are they consistent enough\nto be reliable valuation tools. Therefore, while network effects do occur in\ncryptoasset networks, they are not a defining feature of the cryptoasset market\nas a whole.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 16:55:45 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Stylianou", "Konstantinos", ""], ["Spiegelberg", "Leonhard", ""], ["Herlihy", "Maurice", ""], ["Carter", "Nic", ""]]}, {"id": "2101.06256", "submitter": "Davide Villa", "authors": "Davide Villa, Xinchao Song, Matthew Heim, Liangshe Li", "title": "Internet of Robotic Things: Current Technologies, Applications,\n  Challenges and Future Directions", "comments": "8 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the Internet of Things (IoT) concept is gaining more and more\nnotoriety bringing the number of connected devices to reach the order of\nbillion units. Its smart technology is influencing the research and\ndevelopments of advanced solutions in many areas. This paper focuses on the\nmerger between the IoT and robotics named the Internet of Robotic Things\n(IoRT). Allowing robotic systems to communicate over the internet at a minimal\ncost is an important technological opportunity. Robots can use the cloud to\nimprove the overall performance and for offloading demanding tasks. Since\ncommunicating to the cloud results in latency, data loss, and energy loss,\nfinding efficient techniques is a concern that can be addressed with current\nmachine learning methodologies. Moreover, the use of robotic generates ethical\nand regulation questions that should be answered for a proper coexistence\nbetween humans and robots. This paper aims at providing a better understanding\nof the new concept of IoRT with its benefits and limitations, as well as\nguidelines and directions for future research and studies.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:42:15 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Villa", "Davide", ""], ["Song", "Xinchao", ""], ["Heim", "Matthew", ""], ["Li", "Liangshe", ""]]}, {"id": "2101.06301", "submitter": "Edward Oughton", "authors": "Edward J Oughton, Julius Kusuma, Thibault Peyronel, Jon Crowcroft", "title": "Wi-Fi Wardriving Studies Must Account for Important Statistical Issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge of Wi-Fi networks helps to guide future engineering and spectrum\npolicy decisions. However, due to its unlicensed nature, the deployment of\nWi-Fi Access Points is undocumented meaning researchers are left making\neducated guesses as to the prevalence of these assets through remotely\ncollected or passively sensed measurements. One commonly used method is\nreferred to as `wardriving` essentially where a vehicle is used to collect\ngeospatial statistical data on wireless networks to inform mobile computing and\nnetworking security research. Surprisingly, there has been very little\nexamination of the statistical issues with wardriving data, despite the vast\nnumber of analyses being published in the literature using this approach. In\nthis paper, a sample of publicly collected wardriving data is compared to a\npredictive model for Wi-Fi Access Points. The results demonstrate several\nstatistical issues which future wardriving studies must account for, including\nselection bias, sample representativeness and the modifiable areal unit\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:13:11 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 09:08:02 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Oughton", "Edward J", ""], ["Kusuma", "Julius", ""], ["Peyronel", "Thibault", ""], ["Crowcroft", "Jon", ""]]}, {"id": "2101.06311", "submitter": "Mohammed Salman", "authors": "Mohammed I. Salman, Bin Wang", "title": "Boosting performance for software defined networks from traffic\n  engineering perspective", "comments": null, "journal-ref": null, "doi": "10.1016/j.comcom.2020.12.018", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Paths selection algorithms and rate adaptation objective functions are\nusually studied separately. In contrast, this paper evaluates some traffic\nengineering (TE) systems for software defined networking obtained by combining\npath selection techniques with average delay and load balancing, the two most\npopular TE objective functions. Based on TE simulation results, the best TE\nsystem suitable for software defined networks is a system where the paths are\ncalculated using an oblivious routing model and its adaptation rate calculated\nusing an average delay objective function. Thus, we propose the RACKE+AD system\ncombining path sets computed using Racke's oblivious routing and traffic\nsplitting objective function using average delay. This model outperforms\ncurrent state-of-the-art models, maximizes throughput, achieves better network\nresource utilization, and minimizes delay. The proposed system outperformed\nSMORE and SWAN by 4.2% and 9.6% respectively, achieving 27% better utilization\nand delivering 34% more traffic with 50% less latency compared with both\nsystems on a GEANT network.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 22:04:05 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Salman", "Mohammed I.", ""], ["Wang", "Bin", ""]]}, {"id": "2101.06454", "submitter": "Daoyuan Wu", "authors": "Mengjie Chen, Daoyuan Wu, Xiao Yi, Jianliang Xu", "title": "AGChain: A Blockchain-based Gateway for Permanent, Distributed, and\n  Secure App Delegation from Existing Mobile App Markets", "comments": "This is a technical report from The Chinese Chinese University of\n  Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile app markets are emerging with the popularity of smartphones. However,\nthey fall short in several aspects, including no transparent app listing, no\nworld-wide app access, and even insecure app downloading. To address these\nproblems, we propose a novel blockchain-based gateway, AGChain, to bridge end\nusers and app markets so that existing app markets could still provide services\nwhile users enjoy permanent, distributed, and secure app delegation from\nAGChain. To this end, we identify two previously under-estimated challenges and\npropose mechanisms to significantly reduce gas costs in our smart contract and\nmake IPFS (Inter-planetary File System) based file storage really distributed.\nWe also address three AGChain-specific system challenges to make it secure and\nsustainable. We have implemented an AGChain prototype\n(https://www.agchain.ltd/) on Ethereum. The evaluation shows that it achieves\nsecurity and decentralization with minimal gas costs and reasonable\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:19:21 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chen", "Mengjie", ""], ["Wu", "Daoyuan", ""], ["Yi", "Xiao", ""], ["Xu", "Jianliang", ""]]}, {"id": "2101.06466", "submitter": "Jianfeng Wang", "authors": "Jianfeng Wang, Tam\\'as L\\'evai, Zhuojin Li, Marcos A. M. Vieira,\n  Ramesh Govindan, Barath Raghavan", "title": "Galleon: Reshaping the Square Peg of NFV", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software is often used for Network Functions (NFs) -- such as firewalls, NAT,\ndeep packet inspection, and encryption -- that are applied to traffic in the\nnetwork. The community has hoped that NFV would enable rapid development of new\nNFs and leverage commodity computing infrastructure. However, the challenge for\nresearchers and operators has been to align the square peg of high-speed packet\nprocessing with the round hole of cloud computing infrastructures and\nabstractions, all while delivering performance, scalability, and isolation.\nPast work has led to the belief that NFV is different enough that it requires\nnovel, custom approaches that deviate from today's norms. To the contrary, we\nshow that we can achieve performance, scalability, and isolation in NFV\njudiciously using mechanisms and abstractions of FaaS, the Linux kernel, NIC\nhardware, and OpenFlow switches. As such, with our system Galleon, NFV can be\npractically-deployable today in conventional cloud environments while\ndelivering up to double the performance per core compared to the state of the\nart.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 15:55:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Wang", "Jianfeng", ""], ["L\u00e9vai", "Tam\u00e1s", ""], ["Li", "Zhuojin", ""], ["Vieira", "Marcos A. M.", ""], ["Govindan", "Ramesh", ""], ["Raghavan", "Barath", ""]]}, {"id": "2101.06495", "submitter": "Livia Elena Chatzieleftheriou", "authors": "Livia Elena Chatzieleftheriou, Apostolos Destounis, Georgios Paschos\n  and Iordanis Koutsopoulos", "title": "Blind Optimal User Association in Small-Cell Networks", "comments": "To appear in IEEE International Conference on Computer Communication\n  (INFOCOM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We learn optimal user association policies for traffic from different\nlocations to Access Points(APs), in the presence of unknown dynamic traffic\ndemand. We aim at minimizing a broad family of $\\alpha$-fair cost functions\nthat express various objectives in load assignment in the wireless downlink,\nsuch as total load or total delay minimization. Finding an optimal user\nassociation policy in dynamic environments is challenging because traffic\ndemand fluctuations over time are non-stationary and difficult to characterize\nstatistically, which obstructs the computation of cost-efficient associations.\nAssuming arbitrary traffic patterns over time, we formulate the problem of\nonline learning of optimal user association policies using the Online Convex\nOptimization (OCO) framework. We introduce a periodic benchmark for OCO\nproblems that generalizes state-of-the-art benchmarks. We exploit inherent\nproperties of the online user association problem and propose PerOnE, a simple\nonline learning scheme that dynamically adapts the association policy to\narbitrary traffic demand variations. We compare PerOnE against our periodic\nbenchmark and prove that it enjoys the no-regret property, with additional\nsublinear dependence of the network size. To the best of our knowledge, this is\nthe first work that introduces a periodic benchmark for OCO problems and a\nno-regret algorithm for the online user association problem. Our theoretical\nfindings are validated through results on a real-trace dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 18:12:39 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Chatzieleftheriou", "Livia Elena", ""], ["Destounis", "Apostolos", ""], ["Paschos", "Georgios", ""], ["Koutsopoulos", "Iordanis", ""]]}, {"id": "2101.06522", "submitter": "Mao Ye", "authors": "Yong Zhang, Mao Ye, Lin Guan", "title": "Overlap-Minimization Scheduling Strategy for Data Transmission in VANET", "comments": "6 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The vehicular ad-hoc network (VANET) based on dedicated short-range\ncommunication (DSRC) is a distributed communication system, in which all the\nnodes share the wireless channel with carrier sense multiple access/collision\navoid (CSMA/CA) protocol. However, the competition and backoff mechanisms of\nCSMA/CA often bring additional delays and data packet collisions, which may\nhardly meet the QoS requirements in terms of delay and packets delivery ratio\n(PDR). Moreover, because of the distribution nature of security information in\nbroadcast mode, the sender cannot know whether the receivers have received the\ninformation successfully. Similarly, this problem also exists in no-acknowledge\n(non-ACK) transmissions of VANET. Therefore, the probability of packet\ncollisions should be considered in broadcast or non-ACK working modes. This\npaper presents a connection-level scheduling algorithm overlaid on CSMA/CA to\nschedule the start sending time of each transmission. By converting the object\nof reducing collision probability to minimizing the overlap of transmission\ndurations of connections, the probability of backoff-activation can be greatly\ndecreased. Then the delay and the probability of packet collisions can also be\ndecreased. Numerical simulations have been conducted in our unified platform\ncontaining SUMO, Veins and Omnet++. The result shows that the proposed\nalgorithm can effectively improve the PDR and reduce the packets collision in\nVANET.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 20:57:03 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhang", "Yong", ""], ["Ye", "Mao", ""], ["Guan", "Lin", ""]]}, {"id": "2101.06537", "submitter": "Yanfang Le", "authors": "Yanfang Le, Radhika Niranjan Mysore, Lalith Suresh, Gerd Zellweger,\n  Sujata Banerjee, Aditya Akella, Michael Swift", "title": "PL2: Towards Predictable Low Latency in Rack-Scale Networks", "comments": "13 pages, 23 pdf figures, use acmart.cls", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance rack-scale offerings package disaggregated pools of compute,\nmemory and storage hardware in a single rack to run diverse workloads with\nvarying requirements, including applications that need low and predictable\nlatency. The intra-rack network is typically high speed Ethernet, which can\nsuffer from congestion leading to packet drops and may not satisfy the\nstringent tail latency requirements for some workloads (including remote\nmemory/storage accesses). In this paper, we design a Predictable Low\nLatency(PL2) network architecture for rack-scale systems with Ethernet as\ninterconnecting fabric. PL2 leverages programmable Ethernet switches to\ncarefully schedule packets such that they incur no loss with NIC and switch\nqueues maintained at small, near-zero levels. In our 100 Gbps rack-prototype,\nPL2 keeps 99th-percentile memcached RPC latencies under 60us even when the RPCs\ncompete with extreme offered-loads of 400%, without losing traffic. Network\ntransfers for a machine learning training task complete 30% faster than a\nreceiver-driven scheme implementation modeled after Homa (222ms vs 321ms 99%ile\nlatency per iteration).\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:42:33 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 20:38:15 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Le", "Yanfang", ""], ["Mysore", "Radhika Niranjan", ""], ["Suresh", "Lalith", ""], ["Zellweger", "Gerd", ""], ["Banerjee", "Sujata", ""], ["Akella", "Aditya", ""], ["Swift", "Michael", ""]]}, {"id": "2101.06548", "submitter": "Ghayoor Shah", "authors": "Ghayoor Shah, MD Saifuddin, Yaser P. Fallah, Somak Datta Gupta", "title": "RVE-CV2X: A Scalable Emulation Framework for Real-Time Evaluation of\n  CV2X-Based Connected Vehicle Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle-to-Everything (V2X) communication has become an integral component of\nIntelligent Transportation Systems (ITS) due to its ability to connect\nvehicles, pedestrians, infrastructure, and create situational awareness among\nvehicles. Cellular-Vehicle-to-Everything (C-V2X), based on 3rd Generation\nPartnership Project (3GPP) Release 14, is one such communication technology\nthat has recently gained significant attention to cater the needs of V2X\ncommunication. However, for a successful deployment of C-V2X, it is of\nparamount significance to thoroughly test the performance of this technology.\nIt is unfeasible to physically conduct a V2X communication experiment to test\nthe performance of C-V2X by arranging hundreds of real vehicles and their\ntransceiving on-board units. Although multiple simulators based on frameworks\nsuch as NS-3, OMNET++ and OPNET have proven to be reliable and economic\nalternatives to using real vehicles, all these simulators are time-consuming\nand require several orders of magnitudes longer than the actual simulation\ntime. As opposed to physical field- and simulation-based testing, network\nemulators can provide more realistic and repeatable results for testing\nvehicular communication. This paper proposes a real-time, high-fidelity,\nhardware-in-the-loop network emulator (RVE-CV2X) based on C-V2X mode 4 that can\nprovide scalable, reliable and repeatable testing scenarios for V2X\ncommunication. The accuracy of this emulator is verified by comparing it to an\nalready validated C-V2X simulator based on the NS-3 framework.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:22:50 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Shah", "Ghayoor", ""], ["Saifuddin", "MD", ""], ["Fallah", "Yaser P.", ""], ["Gupta", "Somak Datta", ""]]}, {"id": "2101.06564", "submitter": "Sharare Zehtabian", "authors": "Sharare Zehtabian, Siavash Khodadadeh, Ladislau B\\\"ol\\\"oni and Damla\n  Turgut", "title": "Privacy-Preserving Learning of Human Activity Predictors in Smart\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The daily activities performed by a disabled or elderly person can be\nmonitored by a smart environment, and the acquired data can be used to learn a\npredictive model of user behavior. To speed up the learning, several\nresearchers designed collaborative learning systems that use data from multiple\nusers. However, disclosing the daily activities of an elderly or disabled user\nraises privacy concerns. In this paper, we use state-of-the-art deep neural\nnetwork-based techniques to learn predictive human activity models in the\nlocal, centralized, and federated learning settings. A novel aspect of our work\nis that we carefully track the temporal evolution of the data available to the\nlearner and the data shared by the user. In contrast to previous work where\nusers shared all their data with the centralized learner, we consider users\nthat aim to preserve their privacy. Thus, they choose between approaches in\norder to achieve their goals of predictive accuracy while minimizing the shared\ndata. To help users make decisions before disclosing any data, we use machine\nlearning to predict the degree to which a user would benefit from collaborative\nlearning. We validate our approaches on real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 01:04:53 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zehtabian", "Sharare", ""], ["Khodadadeh", "Siavash", ""], ["B\u00f6l\u00f6ni", "Ladislau", ""], ["Turgut", "Damla", ""]]}, {"id": "2101.06602", "submitter": "Mohammed Gharib Dr.", "authors": "Mohammed Gharib and Fatemeh Afghah and Elizabeth Bentley", "title": "OPAR: Optimized Predictive and Adaptive Routing for Cooperative UAV\n  Networks", "comments": "6 pages, 6 figures, Infocom workshop accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative UAV networks are becoming increasingly popular in military and\ncivilian applications. Alas, the typical ad-hoc routing protocols, which aim at\nfinding the shortest path, lead to significant performance degradation because\nof the 3-dimension highly-dynamic nature of UAV networks and the uneven\ndistribution of nodes across the network. This paper proposes OPAR, an\noptimized predictive and adaptive routing protocol, to face this challenging\nproblem. We model the routing problem with linear programming (LP), where the\ngoal is to maximize network performance, considering the path lifetime and\npath-length together. This model relies on a precise link lifetime prediction\nmechanism. We support the LP problem with a lightweight algorithm to find the\noptimized solution with a computation complexity of $O(|E|^2)$, where $|E|$ is\nthe number of network links. We evaluate the OPAR performance and compare it\nwith the well-known routing algorithms AODV, DSDV, and OLSR to cover a wide\nrange of proactive and reactive protocols as well as distance vector and\nlink-state techniques. We performed extensive simulations for different network\ndensities and mobility patterns using the ns-3 simulator. Results show that\nOPAR prevents a high volume of routing traffic, increases the successful\ndelivery by more than $30\\%$, improves the throughput $25\\%$ on average, and\ndecreases the flow completion time by an average of $35\\%$.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 06:30:07 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Gharib", "Mohammed", ""], ["Afghah", "Fatemeh", ""], ["Bentley", "Elizabeth", ""]]}, {"id": "2101.06617", "submitter": "Farhad Rezazadeh", "authors": "Farhad Rezazadeh, Hatim Chergui, Luis Alonso, Christos Verikoukis", "title": "Continuous Multi-objective Zero-touch Network Slicing via Twin Delayed\n  DDPG and OpenAI Gym", "comments": "6 pages, 4 figures, accepted for publication at IEEE GLOBECOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI)-driven zero-touch network slicing (NS) is a new\nparadigm enabling the automation of resource management and orchestration\n(MANO) in multi-tenant beyond 5G (B5G) networks. In this paper, we tackle the\nproblem of cloud-RAN (C-RAN) joint slice admission control and resource\nallocation by first formulating it as a Markov decision process (MDP). We then\ninvoke an advanced continuous deep reinforcement learning (DRL) method called\ntwin delayed deep deterministic policy gradient (TD3) to solve it. In this\nintent, we introduce a multi-objective approach to make the central unit (CU)\nlearn how to re-configure computing resources autonomously while minimizing\nlatency, energy consumption and virtual network function (VNF) instantiation\ncost for each slice. Moreover, we build a complete 5G C-RAN network slicing\nenvironment using OpenAI Gym toolkit where, thanks to its standardized\ninterface, it can be easily tested with different DRL schemes. Finally, we\npresent extensive experimental results to showcase the gain of TD3 as well as\nthe adopted multi-objective strategy in terms of achieved slice admission\nsuccess rate, latency, energy saving and CPU utilization.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 08:18:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rezazadeh", "Farhad", ""], ["Chergui", "Hatim", ""], ["Alonso", "Luis", ""], ["Verikoukis", "Christos", ""]]}, {"id": "2101.06654", "submitter": "Farhad Rezazadeh", "authors": "Farhad Rezazadeh, Hatim Chergui, Christos Verikoukis", "title": "Zero-touch Continuous Network Slicing Control via Scalable Actor-Critic\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI)-driven zero-touch network slicing is envisaged\nas a promising cutting-edge technology to harness the full potential of\nheterogeneous 5G and beyond 5G (B5G) communication systems and enable the\nautomation of demand-aware resource management and orchestration (MANO). In\nthis paper, we tackle the issue of B5G radio access network (RAN) joint slice\nadmission control and resource allocation according to proposed slice-enabling\ncell-free massive multiple-input multiple-output (mMIMO) setup by invoking a\ncontinuous deep reinforcement learning (DRL) method. We present a novel\nActor-Critic-based network slicing approach called, prioritized twin delayed\ndistributional deep deterministic policy gradient (D-TD3)}. The paper defines\nand corroborates via extensive experimental results a zero-touch network\nslicing scheme with a multi-objective approach where the central server learns\ncontinuously to accumulate the knowledge learned in the past to solve future\nproblems and re-configure computing resources autonomously while minimizing\nlatency, energy consumption, and virtual network function (VNF) instantiation\ncost for each slice. Moreover, we pursue a state-action return distribution\nlearning approach with the proposed replay policy and reward-penalty\nmechanisms. Finally, we present numerical results to showcase the gain of the\nadopted multi-objective strategy and verify the performance in terms of\nachieved slice admission rate, latency, energy, CPU utilization, and time\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:57:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Rezazadeh", "Farhad", ""], ["Chergui", "Hatim", ""], ["Verikoukis", "Christos", ""]]}, {"id": "2101.06661", "submitter": "Dibakar Das", "authors": "Dibakar Das, Mohammad Fahad Imteyaz, Jyotsna Bapat, Debabrata Das", "title": "A Non-intrusive Failure Prediction Mechanism for Deployed Optical\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failures in optical network backbone can lead to major disruption of internet\ndata traffic. Hence, minimizing such failures is of paramount importance for\nthe network operators. Even better, if the network failures can be predicted\nand preventive steps can be taken in advance to avoid any disruption in\ntraffic. Various data driven and machine learning techniques have been proposed\nin literature for failure prediction. Most of these techniques need real time\ndata from the networks and also need different monitors to measure key optical\nparameters. This means provision for failure prediction has to be available in\nnetwork nodes, e.g., routers and network management systems. However, sometimes\ndeployed networks do not have failure prediction built into their initial\ndesign but subsequently need arises for such mechanisms. For such systems,\nthere are two key challenges. Firstly, statistics of failure distribution,\ndata, etc., are not readily available. Secondly, major changes cannot be made\nto the network nodes which are already commercially deployed. This paper\nproposes a novel implementable non-intrusive failure prediction mechanism for\ndeployed network nodes using information from log files of those devices.\nNumerical results show that the mechanism has near perfect accuracy in\npredicting failures of individual network nodes.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 12:58:28 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Das", "Dibakar", ""], ["Imteyaz", "Mohammad Fahad", ""], ["Bapat", "Jyotsna", ""], ["Das", "Debabrata", ""]]}, {"id": "2101.07158", "submitter": "Roman Kovalchukov Mr", "authors": "Roman Kovalchukov (1), Dmitri Moltchanov (1), Juho Pirskanen (2),\n  Joonas Sae (1), Jussi Numminen (2), Yevgeni Koucheryavy (1), and Mikko\n  Valkama (1) ((1) Tampere University, Finland, (2) Wirepas Oy, Finland)", "title": "DECT-2020 New Radio: The Next Step Towards 5G Massive Machine-Type\n  Communications", "comments": "Author-Submitted Paper to IEEE Communications Magazine, 7 pages, 5\n  figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive machine type communications (mMTC) is one of the cornerstone services\nthat have to be supported by 5G systems. 3GPP has already introduced LTE-M and\nNB-IoT, often referred to as cellular IoT, in 3GPP Releases 13, 14, and 15 and\nsubmitted these technologies as part of 3GPP IMT-2020 (i.e., 5G) technology\nsubmission to ITU-R. Even though NB-IoT and LTE-M have shown to satisfy 5G mMTC\nrequirements defined by ITU-R, it is expected that these cellular IoT solutions\nwill not address all aspects of IoT and ongoing digitalization, including the\nsupport for direct communication between \"things\" with flexible deployments,\ndifferent business models, as well as support for even higher node densities\nand enhanced coverage. In this paper, we introduce the DECT-2020 standard\nrecently published by ETSI for mMTC communications. We evaluate its performance\nand compare it to the existing LPWAN solutions showing that it outperforms\nthose in terms of supported density of nodes while still keeping delay and loss\nguarantees at the required level.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 16:49:56 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 21:08:53 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 16:51:39 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kovalchukov", "Roman", "", "Tampere University, Finland"], ["Moltchanov", "Dmitri", "", "Tampere University, Finland"], ["Pirskanen", "Juho", "", "Wirepas Oy, Finland"], ["Sae", "Joonas", "", "Tampere University, Finland"], ["Numminen", "Jussi", "", "Wirepas Oy, Finland"], ["Koucheryavy", "Yevgeni", "", "Tampere University, Finland"], ["Valkama", "Mikko", "", "Tampere University, Finland"]]}, {"id": "2101.07327", "submitter": "Hung-Wei Tseng", "authors": "Alec Rohloff and Zackary Allen and Kung-Min Lin and Joshua Okrend and\n  Chengyi Nie and Yu-Chia Liu and Hung-Wei Tseng", "title": "OpenUVR: an Open-Source System Framework for Untethered Virtual Reality\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in heterogeneous computing technologies enable the significant\npotential of virtual reality (VR) applications. To offer the best user\nexperience (UX), a system should adopt an untethered, wireless-network-based\narchitecture to transfer VR content between the user and the content generator.\nHowever, modern wireless network technologies make implementing such an\narchitecture challenging, as VR applications require superior video quality --\nwith high resolution, high frame rates, and very low latency.\n  This paper presents OpenUVR, an open-source framework that uses commodity\nhardware components to satisfy the demands of interactive, real-time VR\napplications. OpenUVR significantly improves UX through a redesign of the\nsystem stack and addresses the most time-sensitive issues associated with\nredundant memory copying in modern computing systems. OpenUVR presents a\ncross-layered VR datapath to avoid redundant data operations and computation\namong system components, OpenUVR customizes the network stack to eliminate\nunnecessary memory operations incurred by mismatching data formats in each\nlayer, and OpenUVR uses feedback from mobile devices to remove memory buffers.\n  Together, these modifications allow OpenUVR to reduce VR application delays\nto 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As\nan open-source system that is fully compatible with commodity hardware, OpenUVR\noffers the research community an opportunity to develop, investigate, and\noptimize applications for untethered, high-performance VR architectures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:02:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rohloff", "Alec", ""], ["Allen", "Zackary", ""], ["Lin", "Kung-Min", ""], ["Okrend", "Joshua", ""], ["Nie", "Chengyi", ""], ["Liu", "Yu-Chia", ""], ["Tseng", "Hung-Wei", ""]]}, {"id": "2101.07534", "submitter": "Federico Chiariotti", "authors": "Siddharth Chandak, Federico Chiariotti, Petar Popovski", "title": "Hidden Markov Model-Based Encoding for Time-Correlated IoT Sources", "comments": "Preprint version of the paper published in IEEE Communications\n  Letters", "journal-ref": null, "doi": "10.1109/LCOMM.2020.3044210", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the use of Internet of Things (IoT) devices for monitoring purposes\nbecomes ubiquitous, the efficiency of sensor communication is a major issue for\nthe modern Internet. Channel coding is less efficient for extremely short\npackets, and traditional techniques that rely on source compression require\nextensive signaling or pre-existing knowledge of the source dynamics. In this\nwork, we propose an encoding and decoding scheme that learns source dynamics\nonline using a Hidden Markov Model (HMM), puncturing a short packet code to\noutperform existing compression-based approaches. Our approach shows\nsignificant performance improvements for sources that are highly correlated in\ntime, with no additional complexity on the sender side.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 09:45:27 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 11:45:48 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Chandak", "Siddharth", ""], ["Chiariotti", "Federico", ""], ["Popovski", "Petar", ""]]}, {"id": "2101.07541", "submitter": "Malisa Vucinic", "authors": "Jelena Kova\\v{c} (UCG), Jovan Crnogorac (UCG), Enis Ko\\v{c}an (UCG),\n  Malisa Vucinic (EVA)", "title": "Sniffing Multi-hop Multi-channel Wireless Sensor Networks", "comments": "2020 28th Telecommunications Forum (TELFOR), Nov 2020, Belgrade,\n  Serbia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As wireless sensor networks grow larger, more complex and their role more\nsignificant, it becomes necessary to have an insight into the network traffic.\nFor this purpose, sniffers play an irreplaceable role. Since a sniffer is a\ndevice of limited range, to cover a multi-hop network it is necessary to\nconsider the deployment of multiple sniffers. This motivates the research on\nthe optimal number and position of sniffers in the network. We present a\nsolution based on a minimal dominant set from graph theory. We evaluate the\nproposed solution and implement it as an extension of the 6TiSCH simulator. Our\nsolution assumes a 50-nodes scenario, deployed in 2x2 km outdoor area, with 10%\nof packet drops over all channels, when 10 sniffers are used.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 10:02:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Kova\u010d", "Jelena", "", "UCG"], ["Crnogorac", "Jovan", "", "UCG"], ["Ko\u010dan", "Enis", "", "UCG"], ["Vucinic", "Malisa", "", "EVA"]]}, {"id": "2101.07562", "submitter": "Douglas Leith", "authors": "Francesco Gringoli, Douglas J. Leith", "title": "Modelling Downlink Packet Aggregation in Paced 802.11ac WLANs", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.09651", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an analytic model of packet aggregation on the the downlink of an\n802.11ac WLAN when packet arrivals are paced. The model is closed-form and so\nsuitable for both analysis and design of next generation edge architectures\nthat aim to achieve high rate and low delay. The model is validated against\nboth simulations and experimental measurements and found to be remarkably\naccurate despite its simplicity.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:10:20 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Gringoli", "Francesco", ""], ["Leith", "Douglas J.", ""]]}, {"id": "2101.07676", "submitter": "Milan Groshev", "authors": "Milan Groshev, Jorge Mart\\'in-P\\'erez, Kiril Antevski, Antonio de la\n  Oliva, Carlos J. Bernardos", "title": "COTORRA: COntext-aware Testbed fOR Robotic Applications", "comments": "4 pages, 4 figures, submitted to IEEE Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge & Fog computing have received considerable attention as promising\ncandidates for the evolution of robotic systems. In this letter, we propose\nCOTORRA, an Edge & Fog driven robotic testbed that combines context information\nwith robot sensor data to validate innovative concepts for robotic systems\nprior to being applied in a production environment. In lab/university, we\nestablished COTORRA as an easy applicable and modular testbed on top of\nheterogeneous network infrastructure. COTORRA is open for pluggable robotic\napplications. To verify its feasibility and assess its performance, we ran set\nof experiments that show how autonomous navigation applications can achieve\ntarget latencies bellow 15ms or perform an inter-domain (DLT) federation within\n19 seconds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:15:21 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Groshev", "Milan", ""], ["Mart\u00edn-P\u00e9rez", "Jorge", ""], ["Antevski", "Kiril", ""], ["de la Oliva", "Antonio", ""], ["Bernardos", "Carlos J.", ""]]}, {"id": "2101.07727", "submitter": "Bob Briscoe", "authors": "Bob Briscoe", "title": "Improving DCTCP/Prague Congestion Control Responsiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-BB-2020-002", "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report explains how DCTCP introduces 1--2 rounds of unnecessary lag, due\nto the way it processes congestion feedback. To solve this, a per-ACK moving\naverage is proposed. It always cuts out 1 RTT of this lag and, paradoxically,\nit cuts out most of the rest of the lag by spreading the congestion response\nover a round. The EWMA still averages out variations in the feedback signal\nover the same set number of round trips, even though it is clocked per-ACK.\nThis version of the report is released prior to full evaluation, in order to\nelicit early feedback on the design.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:59:52 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2021 16:18:55 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Briscoe", "Bob", ""]]}, {"id": "2101.07820", "submitter": "Edward Oughton", "authors": "Edward J Oughton and Niccol\\`o Comini and Vivien Foster and Jim W Hall", "title": "Policy choices can help keep 4G and 5G universal broadband affordable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CY cs.NI q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The United Nations Broadband Commission has committed the international\ncommunity to accelerate universal broadband. However, the cost of meeting this\nobjective, and the feasibility of doing so on a commercially viable basis, are\nnot well understood. Using scenario analysis, this paper compares the global\ncost-effectiveness of different infrastructure strategies for the developing\nworld to achieve universal 4G or 5G mobile broadband. Utilizing remote sensing\nand demand forecasting, least-cost network designs are developed for eight\nrepresentative low and middle-income countries (Malawi, Uganda, Kenya, Senegal,\nPakistan, Albania, Peru and Mexico), the results from which form the basis for\naggregation to the global level. The cost of meeting a minimum 10 Mbps per user\nis estimated at USD 1.7 trillion using 5G Non-Standalone, approximately 0.6% of\nannual GDP for the developing world over the next decade. However, by creating\na favorable regulatory environment, governments can bring down these costs by\nas much as three quarters, to USD 0.5 trillion (approximately 0.2% of annual\nGDP), and avoid the need for public subsidy. Providing governments make\njudicious choices, adopting fiscal and regulatory regimes conducive to lowering\ncosts, universal broadband may be within reach of most developing countries\nover the next decade.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 19:04:41 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 15:16:57 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2021 17:23:26 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2021 21:32:50 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Oughton", "Edward J", ""], ["Comini", "Niccol\u00f2", ""], ["Foster", "Vivien", ""], ["Hall", "Jim W", ""]]}, {"id": "2101.07877", "submitter": "Manuel Steve Mbankeu Patchou", "authors": "Manuel Patchou and Benjamin Sliwa and Christian Wietfeld", "title": "Flying Robots for Safe and Efficient Parcel Delivery Within the COVID-19\n  Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of small-scale Unmanned Aerial Vehicles (UAVs) into\nIntelligent Transportation Systems (ITSs) will empower novel smart-city\napplications and services. After the unforeseen outbreak of the COVID-19\npandemic, the public demand for delivery services has multiplied. Mobile\nrobotic systems inherently offer the potential for minimizing the amount of\ndirect human-to-human interactions with the parcel delivery process. The\nproposed system-of-systems consists of various complex aspects such as\nassigning and distributing delivery jobs, establishing and maintaining reliable\ncommunication links between the vehicles, as well as path planning and mobility\ncontrol. In this paper, we apply a system-level perspective for identifying key\nchallenges and promising solution approaches for modeling, analysis, and\noptimization of UAV-aided parcel delivery. We present a system-of-systems model\nfor UAV-assisted parcel delivery to cope with higher capacity requirements\ninduced by the COVID-19. To demonstrate the benefits of hybrid vehicular\ndelivery, we present a case study focusing on the prioritization of\ntime-critical deliveries such as medical goods. The results further confirm\nthat the capacity of traditional delivery fleets can be upgraded with drone\nusage. Furthermore, we observe that the delay incurred by prioritizing\ntime-critical deliveries can be compensated with drone deployment. Finally,\ncentralized and decentralized communication approaches for data transmission\ninside hybrid delivery fleets are compared.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 21:54:22 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Patchou", "Manuel", ""], ["Sliwa", "Benjamin", ""], ["Wietfeld", "Christian", ""]]}, {"id": "2101.07930", "submitter": "Zhen Qin", "authors": "Zhen Qin, Hai Wang, Yuben Qu, Haipeng Dai, and Zhenhua Wei", "title": "Air-Ground Collaborative Mobile Edge Computing: Architecture,\n  Challenges, and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By pushing computation, cache, and network control to the edge, mobile edge\ncomputing (MEC) is expected to play a leading role in fifth generation (5G) and\nfuture sixth generation (6G). Nevertheless, facing ubiquitous fast-growing\ncomputational demands, it is impossible for a single MEC paradigm to\neffectively support high-quality intelligent services at end user equipments\n(UEs). To address this issue, we propose an air-ground collaborative MEC\n(AGC-MEC) architecture in this article. The proposed AGC-MEC integrates all\npotentially available MEC servers within air and ground in the envisioned 6G,\nby a variety of collaborative ways to provide computation services at their\nbest for UEs. Firstly, we introduce the AGC-MEC architecture and elaborate\nthree typical use cases. Then, we discuss four main challenges in the AGC-MEC\nas well as their potential solutions. Next, we conduct a case study of\ncollaborative service placement for AGC-MEC to validate the effectiveness of\nthe proposed collaborative service placement strategy. Finally, we highlight\nseveral potential research directions of the AGC-MEC.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 02:21:35 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Qin", "Zhen", ""], ["Wang", "Hai", ""], ["Qu", "Yuben", ""], ["Dai", "Haipeng", ""], ["Wei", "Zhenhua", ""]]}, {"id": "2101.08014", "submitter": "Iacovos Ioannou I.I.", "authors": "Iacovos Ioannou, Christophoros Christophorou, Vasos Vassiliou, Andreas\n  Pitsillides", "title": "5G D2D Transmission Mode Selection Performance & Cluster Limits\n  Evaluation of Distributed Artificial Intelligence and Machine Learning\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  5G D2D Communication promises improvements in energy and spectral efficiency,\noverall system capacity, and higher data rates. However, to achieve optimum\nresults it is important to select wisely the Transmission mode of the D2D\nDevice to form clusters in the most fruitful positions in terms of Sum Rate and\nPower Consumption. Towards this end, this paper investigates the use of\nDistributed Artificial Intelligence (DAI) and innovative to D2D, Machine\nLearning (ML) approaches to achieve satisfactory results in terms of Spectral\nEfficiency (SE), Power Consumption (PC) and execution time, with the creation\nof clusters and backhauling D2D network under existing Base Station/Small Cell.\nAdditionally, one of the major factors that affect the creation of high-quality\nclusters under a D2D network is the number of the Devices. Therefore, this\npaper focuses on a small (<=200) number of Devices, with the purpose to\nidentify the limits of each approach in terms of number of devices.\nSpecifically, to identify where it is beneficial to form a cluster, investigate\nthe critical point that gains increases rapidly and at the end examine the\napplicability of 5G requirements. Additionally, prior work presented a\nDistributed Artificial Intelligence (DAI) Solution/Framework in D2D and a DAIS\nTransmission Mode Selection (TMS) plan was proposed. In this paper DAIS is\nfurther examined, improved in terms of thresholds evaluation, evaluated, and\ncompared with other approaches (AI/ML). The results obtained demonstrate the\nexceptional performance of DAIS, compared to all other related approaches in\nterms of SE, PC, execution time and cluster formation efficiency. Also, results\nshow that the investigated AI/ML approaches are also beneficial for\nTransmission Mode Selection (TMS) in 5G D2D communication, even with a smaller\n(i.e., >=5 D2D Relay,>=50 D2D Multi Hop Relay) numbers of devices as a lower\nlimits.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:11:08 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:59:35 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 16:22:46 GMT"}, {"version": "v4", "created": "Wed, 28 Apr 2021 05:55:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Ioannou", "Iacovos", ""], ["Christophorou", "Christophoros", ""], ["Vassiliou", "Vasos", ""], ["Pitsillides", "Andreas", ""]]}, {"id": "2101.08221", "submitter": "Muhammad Waseem Akhtar", "authors": "Muhammad Waseem Akhtar and Syed Ali Hassan", "title": "TaNTIN: Terrestrial and Non-Terrestrial Integrated Networks-A\n  collaborative technologies perspective for beyond 5G and 6G", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is moving toward globalization rapidly. Everybody has easy access\nto information with the spread of Internet technology. Businesses are growing\nbeyond national borders. Internationalization affects every aspect of life. In\nthis scenario, by dispersing functions and tasks across organizational borders,\ntime and space, global organizations have higher requirements for\ncollaboration. In order to allow decision-makers and knowledge workers,\nsituated at different times and spaces, to work more efficiently, collaborative\ntechnologies are needed. In this paper, we give an overview of potential\ncollaborative technologies, their benefits, risks and challenges, types, and\nelements. Based on the conceptualization of terrestrial and non-terrestrial\nintegrated networks (TaNTIN), we highlight artificial intelligence (AI),\nblockchains, tactile Internet, mobile edge computing (MEC)/fog computing,\naugmented reality and virtual reality, and so forth as the key features to\nensure quality-of-service (QoS) guarantee of futuristic collaborative services\nsuch as telemedicine, e-education, online gaming, online businesses, the\nentertainment industry. We also discuss how these technologies will impact\nhuman life in the near future.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 17:17:05 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Akhtar", "Muhammad Waseem", ""], ["Hassan", "Syed Ali", ""]]}, {"id": "2101.08244", "submitter": "Jaafar Elmirghani", "authors": "Opeyemi O. Ajibola, Taisir E. H. El-Gorashi, and Jaafar M. H.\n  Elmirghani", "title": "Energy Efficient Placement of Workloads in Composable Data Center\n  Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JLT.2021.3063325", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper studies the energy efficiency of composable datacentre (DC)\ninfrastructures over network topologies. Using a mixed integer linear\nprogramming (MILP) model, we compare the performance of disaggregation at\nrack-scale and pod-scale over selected electrical, optical and hybrid network\ntopologies relative to a traditional DC. Relative to a pod-scale DC, the\nresults show that physical disaggregation at rack-scale is sufficient for\noptimal efficiency when the optical network topology is adopted and resource\ncomponents are allocated in a suitable manner. The optical network topology\nalso enables optimal energy efficiency in composable DCs. The paper also\nstudies logical disaggregation of traditional DC servers over an optical\nnetwork topology. Relative to physical disaggregation at rack-scale, logical\ndisaggregation of server resources within each rack enables marginal fall in\nthe total DC power consumption (TDPC) due to improved resource demands\nplacement. Hence, an adaptable composable infrastructure that can support both\nin memory (access) latency sensitive and insensitive workloads is enabled. We\nalso conduct a study of the adoption of micro-service architecture in both\ntraditional and composable DCs. Our results show that increasing the modularity\nof workloads improves the energy efficiency in traditional DCs, but\ndisproportionate utilization of DC resources persists. A combination of\ndisaggregation and micro-services achieved up to 23% reduction in the TDPC of\nthe traditional DC by enabling optimal resources utilization and energy\nefficiencies. Finally, we propose a heuristic for energy efficient placement of\nworkloads in composable DCs which replicates the trends produced by the MILP\nmodel formulated in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:36:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ajibola", "Opeyemi O.", ""], ["El-Gorashi", "Taisir E. H.", ""], ["Elmirghani", "Jaafar M. H.", ""]]}, {"id": "2101.08262", "submitter": "Iacovos Ioannou I.I.", "authors": "Iacovos Ioannou, Christophoros Christophorou, Vasos Vassiliou, Andreas\n  Pitsillides", "title": "Performance Evaluation of Transmission Mode Selection in D2D\n  communication", "comments": "arXiv admin note: text overlap with arXiv:2101.08014", "journal-ref": "2021 11th IFIP International Conference on New Technologies,\n  Mobility and Security (NTMS)", "doi": "10.1109/NTMS49979.2021.9432648", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Device to Device (D2D) Communication is expected to be a core part of the\nforthcoming 5G Mobile Communication Networks as it promises improvements in\nenergy efficiency, spectral efficiency, overall system capacity, and higher\ndata rates with the use of the same frequencies for different D2D transmissions\nin short communication distances within the Cell. However, in order to achieve\noptimum results, it is important, among others, to select wisely the\nTransmission Mode of the D2D Device. Towards this end, our previous work\nproposed an intelligent Transmission mode selection approach in a framework\nthat is utilizing Artificial Intelligence (AI) BDIx agents to collectively\nsatisfy the D2D challenges in a Distributed Artificial Intelligent (DAI) manner\nautonomously and independently. In this paper, as a first step, a literature\nreview focused on related Transmission mode approaches, is performed. Then, our\ninvestigated Transmission mode selection approach is further explained with\nformulas and evaluated based on different threshold values and investigated how\nthese can affect the overall spectral efficiency and power usage of the network\nin order to achieve the maximum performance. The investigated thresholds(i.e.\nD2D Device Weighted Data Rate (WDR) and the D2D Device Battery Power Level) and\nmetrics(i.e. WDR) are also further analyzed and formulated. In addition, the\neffect the transmission power of the D2D links has on the total spectral\nefficiency and total power consumption of the network, is also examined. This\nevaluation results arise some interesting findings that can contribute in other\napproaches that utilized similar or same thresholds. Also, the results obtained\ndemonstrate that with the right tuning of the thresholds and transmission\npower, one can achieve a significant improvement in the network power usage and\ntotal spectral efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:37:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Ioannou", "Iacovos", ""], ["Christophorou", "Christophoros", ""], ["Vassiliou", "Vasos", ""], ["Pitsillides", "Andreas", ""]]}, {"id": "2101.08336", "submitter": "Tasneem Darwish Dr.", "authors": "Tasneem Darwish, Gunes Kurt, Halim Yanikomeroglu, Guillaume\n  Lamontagne, Michel Bellemare", "title": "Location Management in IP-based Future LEO Satellite Networks: A Review", "comments": "Submitted to the Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future integrated terrestrial, aerial, and space networks will involve\nthousands of Low Earth Orbit (LEO) satellites forming a network of\nmega-constellations, which will play a significant role in providing\ncommunication and Internet services everywhere, at any time, and for\neverything. Due to its very large scale and highly dynamic nature, future LEO\nsatellite networks (SatNets) management is a very complicated and crucial\nprocess, especially the mobility management aspect and its two components\nlocation management and handover management. In this article, we present a\ncomprehensive and critical review of the state-of-the-art research in LEO\nSatNets location management. First, we give an overview of the Internet\nEngineering Task Force (IETF) mobility management standards (e.g., Mobile IPv6\nand Proxy Mobile IPv6) and discuss their location management techniques\nlimitations in the environment of future LEO SatNets. We highlight future LEO\nSatNets mobility characteristics and their challenging features and describe\ntwo unprecedented future location management scenarios. A taxonomy of the\navailable location management solutions for LEO SatNets is presented, where the\nsolutions are classified into three approaches. The \"Issues to consider\"\nsection draws attention to critical points related to each of the reviewed\napproaches that should be considered in future LEO SatNets location management.\nTo identify the gaps, the current state of LEO SatNets location management is\nsummarized. Noteworthy future research directions are recommended. This article\nis providing a road map for researchers and industry to shape the future of LEO\nSatNets location management.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 21:53:46 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:13:16 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Darwish", "Tasneem", ""], ["Kurt", "Gunes", ""], ["Yanikomeroglu", "Halim", ""], ["Lamontagne", "Guillaume", ""], ["Bellemare", "Michel", ""]]}, {"id": "2101.08391", "submitter": "Xu Chen", "authors": "Qiong Wu and Xu Chen and Zhi Zhou and Liang Chen and Junshan Zhang", "title": "Deep Reinforcement Learning with Spatio-temporal Traffic Forecasting for\n  Data-Driven Base Station Sleep Control", "comments": "Accepted by IEEE/ACM Transactions on Networking, Jan. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To meet the ever increasing mobile traffic demand in 5G era, base stations\n(BSs) have been densely deployed in radio access networks (RANs) to increase\nthe network coverage and capacity. However, as the high density of BSs is\ndesigned to accommodate peak traffic, it would consume an unnecessarily large\namount of energy if BSs are on during off-peak time. To save the energy\nconsumption of cellular networks, an effective way is to deactivate some idle\nbase stations that do not serve any traffic demand. In this paper, we develop a\ntraffic-aware dynamic BS sleep control framework, named DeepBSC, which presents\na novel data-driven learning approach to determine the BS active/sleep modes\nwhile meeting lower energy consumption and satisfactory Quality of Service\n(QoS) requirements. Specifically, the traffic demands are predicted by the\nproposed GS-STN model, which leverages the geographical and semantic\nspatial-temporal correlations of mobile traffic. With accurate mobile traffic\nforecasting, the BS sleep control problem is cast as a Markov Decision Process\nthat is solved by Actor-Critic reinforcement learning methods. To reduce the\nvariance of cost estimation in the dynamic environment, we propose a benchmark\ntransformation method that provides robust performance indicator for policy\nupdate. To expedite the training process, we adopt a Deep Deterministic Policy\nGradient (DDPG) approach, together with an explorer network, which can\nstrengthen the exploration further. Extensive experiments with a real-world\ndataset corroborate that our proposed framework significantly outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 01:39:42 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Wu", "Qiong", ""], ["Chen", "Xu", ""], ["Zhou", "Zhi", ""], ["Chen", "Liang", ""], ["Zhang", "Junshan", ""]]}, {"id": "2101.08479", "submitter": "Xiao Hong", "authors": "Yue Hong Gao, Xiao Hong, Hao Tian Yang, Lu Chen, Xiao Nan Zhang", "title": "Comparison and Improvement for Delay Analysis Approaches: Theoretical\n  Models and Experimental Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer network tends to be subjected to the proliferation of mobile demands\nand increasingly multifarious, therefore it poses a great challenge to\nguarantee the quality of network service. By designing the model according to\ndifferent requirements, we may get some related indicators such as delay and\npacket loss rate in order to evaluate the quality of network service and verify\nthe user data surface and capacity of the network environment. In this paper,\nwe describe an analytical model based on the measurement for the delay of each\npacket passing through the single existing routers in the network environment.\nIn previous studies, the emulation of real network service behaviors was\ngenerally under ideal condition. In our work, the test environment is built to\nget the relevant test results of the actual network, and the corresponding\ntheoretical results are obtained by our model. The test results are compared\nwith the theoretical results, analyzed and corrected, in order to verify the\nfeasibility of our analysis model for the performance analysis of the actual\nnetwork. With this concern, calculation results are modified with different\nschemes to realize more precise calculation of delay boundary with the\ncomparison with the experimental test results. The results show the analysis\nmethods after the amendment can realistically estimate the performance of\nnetwork element.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 07:36:31 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Gao", "Yue Hong", ""], ["Hong", "Xiao", ""], ["Yang", "Hao Tian", ""], ["Chen", "Lu", ""], ["Zhang", "Xiao Nan", ""]]}, {"id": "2101.08676", "submitter": "Marco Antonio Sotelo-Monge", "authors": "Marco Antonio Sotelo Monge and Jorge Maestre Vidal", "title": "Conceptualization and cases of study on cyber operations against the\n  sustainability of the tactical edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last decade consolidated the cyberspace as fifth domain of operations,\nwhich extends its preliminarily intelligence and information exchange purposes\ntowards enabling complex offensive and defensive operations\nsupported/supportively of parallel kinetic domain actuations. Although there is\na plethora of well documented cases on strategic and operational interventions\nof cyber commands, the cyber tactical military edge is still a challenge, where\ncyber fires barely integrate to the traditional joint targeting cycle due among\nothers to long planning/development times, asymmetric effects, strict target\nreachability requirements, or the fast propagation of collateral damage; the\nlatter rapidly deriving on hybrid impacts (political, economic, social, etc.)\nand evidencing significant socio-technical gaps. In this context, it is\nexpected that tactical clouds disruptively facilitate cyber operations at the\nedge while exposing the rest of the digital assets of the operation to them. On\nthese grounds, the main purpose of the conducted research is to review and in\ndepth analyze the risks and opportunities of jeopardizing the sustainability of\nthe military tactical clouds at the edge by cyber operations. Along with a 1)\ncomprehensively formulation of the researched problematic, the study 2)\nformalizes the Tactical Denial of Sustainability (TDoS) concept; 3) introduces\nthe phasing, potential attack surfaces, terrains and impact of TDoS attacks; 4)\nemphasizes the related human and socio-technical aspects; 5) analyzes the\nthreats/opportunities inherent to their impact on the cloud energy efficiency;\n6) reviews their implications at the military cyber thinking for tactical\noperations; 7) illustrates five extensive CONOPS that facilitate the\nunderstanding of the TDoS concept; and given the high novelty of the discussed\ntopics, it 8) paves the way for further research and development actions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:42:43 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Monge", "Marco Antonio Sotelo", ""], ["Vidal", "Jorge Maestre", ""]]}, {"id": "2101.08681", "submitter": "Lorenzo Bertizzolo", "authors": "Lorenzo Bertizzolo, Tuyen X. Tran, John Buczek, Bharath\n  Balasubramanian, Rittwik Jana, Yu Zhou, Tommaso Melodia", "title": "Streaming From the Air: Enabling High Data-rate 5G Cellular Links for\n  Drone Streaming Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling high data-rate uplink cellular connectivity for drones is a\nchallenging problem, since a flying drone has a higher likelihood of having\nline-of-sight propagation to base stations that terrestrial UEs normally do not\nhave line-of-sight to. This may result in uplink inter-cell interference and\nuplink performance degradation for the neighboring ground UEs when drones\ntransmit at high data-rates (e.g., video streaming). We address this problem\nfrom a cellular operator's standpoint to support drone-sourced video streaming\nof a point of interest. We propose a low-complexity, closed-loop control system\nfor Open-RAN architectures that jointly optimizes the drone's location in space\nand its transmission directionality to support video streaming and minimize its\nuplink interference impact on the network. We prototype and experimentally\nevaluate the proposed control system on an outdoor multi-cell RAN testbed.\nFurthermore, we perform a large-scale simulation assessment of the proposed\nsystem on the actual cell deployment topologies and cell load profiles of a\nmajor US cellular carrier. The proposed Open-RAN-based control achieves an\naverage 19% network capacity gain over traditional BS-constrained control\nsolutions and satisfies the application data-rate requirements of the drone\n(e.g., to stream an HD video).\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 15:48:20 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 20:09:34 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 19:56:59 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bertizzolo", "Lorenzo", ""], ["Tran", "Tuyen X.", ""], ["Buczek", "John", ""], ["Balasubramanian", "Bharath", ""], ["Jana", "Rittwik", ""], ["Zhou", "Yu", ""], ["Melodia", "Tommaso", ""]]}, {"id": "2101.08724", "submitter": "Yi Shi", "authors": "Yi Shi and Yalin E. Sagduyu", "title": "Adversarial Machine Learning for Flooding Attacks on 5G Radio Access\n  Network Slicing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network slicing manages network resources as virtual resource blocks (RBs)\nfor the 5G Radio Access Network (RAN). Each communication request comes with\nquality of experience (QoE) requirements such as throughput and\nlatency/deadline, which can be met by assigning RBs, communication power, and\nprocessing power to the request. For a completed request, the achieved reward\nis measured by the weight (priority) of this request. Then, the reward is\nmaximized over time by allocating resources, e.g., with reinforcement learning\n(RL). In this paper, we introduce a novel flooding attack on 5G network\nslicing, where an adversary generates fake network slicing requests to consume\nthe 5G RAN resources that would be otherwise available to real requests. The\nadversary observes the spectrum and builds a surrogate model on the network\nslicing algorithm through RL that decides on how to craft fake requests to\nminimize the reward of real requests over time. We show that the portion of the\nreward achieved by real requests may be much less than the reward that would be\nachieved when there was no attack. We also show that this flooding attack is\nmore effective than other benchmark attacks such as random fake requests and\nfake requests with the minimum resource requirement (lowest QoE requirement).\nFake requests may be detected due to their fixed weight. As an attack\nenhancement, we present schemes to randomize weights of fake requests and show\nthat it is still possible to reduce the reward of real requests while\nmaintaining the balance on weight distributions.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:05:31 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 14:39:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shi", "Yi", ""], ["Sagduyu", "Yalin E.", ""]]}, {"id": "2101.08819", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Ziliang Lai, Liana Patel, Boon Thau Loo, Eric\n  Lo, Wenchao Zhou", "title": "Saguaro: Efficient Processing of Transactions in Wide Area Networks\n  using a Hierarchical Permissioned Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The next frontier for the Internet leading by innovations in mobile\ncomputing, in particular, 5G, together with blockchains' transparency,\nimmutability, provenance, and authenticity, indicates the potentials of running\na new generation of applications on the mobile internet. A 5G-enabled\nblockchain system is structured as a hierarchy and needs to deal with different\nchallenges such as maintaining blockchain ledger at different spatial domains\nand various levels of the networks, efficient processing of cross-domain\ntransactions, establishing consensus among heterogeneous nodes, and supporting\ndelay-tolerant mobile transactions. In this paper, we present Saguaro, a\nhierarchical permissioned blockchain designed specifically for Internet-scale\nmobile networks. Saguaro benefits from the hierarchical structure of mobile\nnetwork infrastructure to address the aforementioned challenges. Our extensive\nexperimental results demonstrate the high potential of Saguaro being the first\n5G-enabled permissioned blockchain system in the community.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 19:16:22 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Lai", "Ziliang", ""], ["Patel", "Liana", ""], ["Loo", "Boon Thau", ""], ["Lo", "Eric", ""], ["Zhou", "Wenchao", ""]]}, {"id": "2101.08874", "submitter": "Gabor Fodor", "authors": "Gabor Fodor, Julia Vinogradova, Peter Hammarberg, Keerthi Kumar\n  Nagalapur, Zhiqiang (Tyler) Qi, Hieu Do, Ricardo Blasco, Mirza Uzair Baig", "title": "5G New Radio for Automotive, Rail, and Air Transport", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent and upcoming releases of the 3rd Generation Partnership Project's\n5G New Radio specifications include features that are motivated by providing\nconnectivity services to a broad set of verticals, including the automotive,\nrail, and air transport industries. Currently, several radio access network\nfeatures are being further enhanced or newly introduced in NR to improve 5G's\ncapability to provide fast, reliable, and non-limiting connectivity for\ntransport applications. In this article, we review the most important\ncharacteristics and requirements of a wide range of services that are driven by\nthe desire to help the transport sector to become more sustainable,\neconomically viable, safe, and secure. These requirements will be supported by\nthe evolving and entirely new features of 5G NR systems, including accurate\npositioning, reference signal design to enable multi-transmission and reception\npoints, service-specific scheduling configuration, and service quality\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:33:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Fodor", "Gabor", "", "Tyler"], ["Vinogradova", "Julia", "", "Tyler"], ["Hammarberg", "Peter", "", "Tyler"], ["Nagalapur", "Keerthi Kumar", "", "Tyler"], ["Zhiqiang", "", "", "Tyler"], ["Qi", "", ""], ["Do", "Hieu", ""], ["Blasco", "Ricardo", ""], ["Baig", "Mirza Uzair", ""]]}, {"id": "2101.08891", "submitter": "Geunsik Lim", "authors": "Geunsik Lim, Donghwa Lee, and Sang-Bum Suh", "title": "Cloud-Based Content Cooperation System to Assist Collaborative Learning\n  Environment", "comments": null, "journal-ref": null, "doi": "10.1109/TALE.2014.7062593", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online educational systems running on smart devices have the advantage of\nallowing users to learn online regardless of the location of the users. In\nparticular, data synchronization enables users to cooperate on contents in real\ntime anywhere by sharing their files via cloud storage. However, users cannot\ncollaborate by simultaneously modifying files that are shared with each other.\nIn this paper, we propose a content collaboration method and a history\nmanagement technique that are based on distributed system structure and can\nsynchronize data shared in the cloud for multiple users and multiple devices.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:46:53 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Lim", "Geunsik", ""], ["Lee", "Donghwa", ""], ["Suh", "Sang-Bum", ""]]}, {"id": "2101.08976", "submitter": "Zhiyuan Jiang", "authors": "Zhiyuan Jiang and Wei Zhang and Zixu Cao and Shan Cao and Shunqing\n  Zhang and Shugong Xu", "title": "Predictive Wireless Based Status Update for Communication-Agnostic\n  Sampling", "comments": "Submitted to IEEE TWC for possible publication. An earlier version\n  was presented at INFOCOM 2020. arXiv admin note: substantial text overlap\n  with arXiv:2002.01255", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wireless network that conveys status updates from sources (i.e.,\nsensors) to destinations, one of the key issues studied by existing literature\nis how to design an optimal source sampling strategy on account of the\ncommunication constraints which are often modeled as queues. In this paper, an\nalternative perspective is presented -- a novel status-aware communication\nscheme, namely \\emph{parallel communications}, is proposed which allows sensors\nto be communication-agnostic. Specifically, the proposed scheme can determine,\nbased on an online prediction functionality, whether a status packet is worth\ntransmitting considering both the network condition and status prediction, such\nthat sensors can generate status packets without communication constraints. We\nevaluate the proposed scheme on a Software-Defined-Radio (SDR) test platform,\nwhich is integrated with a collaborative autonomous driving simulator, i.e.,\nSimulation-of-Urban-Mobility (SUMO), to produce realistic vehicle control\nmodels and road conditions. The results show that with online status\npredictions, the channel occupancy is significantly reduced, while guaranteeing\nlow status recovery error. Then the framework is applied to two scenarios: a\nmulti-density platooning scenario, and a flight formation control scenario.\nSimulation results show that the scheme achieves better performance on the\nnetwork level, in terms of keeping the minimum safe distance in both vehicle\nplatooning and flight control.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 07:25:49 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Jiang", "Zhiyuan", ""], ["Zhang", "Wei", ""], ["Cao", "Zixu", ""], ["Cao", "Shan", ""], ["Zhang", "Shunqing", ""], ["Xu", "Shugong", ""]]}, {"id": "2101.09002", "submitter": "Jean-Romain Luttringer", "authors": "Jean-Romain Luttringer (UNISTRA), Quentin Bramas (UNISTRA), Cristel\n  Pelsser (UNISTRA), Pascal M\\'erindol (UNISTRA)", "title": "A Fast-Convergence Routing of the Hot-Potato", "comments": "IEEE INFOCOM 2021, May 2021, Online, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between the intra- and inter-domain routing protocols received\nlittle attention despite playing an important role in forwarding transit\ntraffic. More precisely, by default, IGP distances are taken into account by\nBGP to select the closest exit gateway for the transit traffic (hot-potato\nrouting). Upon an IGP update, the new best gateway may change and should be\nupdated through the (full) re-convergence of BGP, causing superfluous BGP\nprocessing and updates in many cases. We propose OPTIC (Optimal Protection\nTechnique for Inter-intra domain Convergence), an efficient way to assemble\nboth protocols without losing the hot-potato property. OPTIC pre-computes sets\nof gateways (BGP next-hops) shared by groups of prefixes. Such sets are\nguaranteed to contain the post-convergence gateway after any single IGP event\nfor the grouped prefixes. The new optimal exits can be found through a single\nwalk-through of each set, allowing the transit traffic to benefit from optimal\nBGP routes almost as soon as the IGP converges. Compared to vanilla BGP,\nOPTIC's structures allow it to consider a reduced number of entries: this\nnumber can be reduced by 99\\% for stub networks. The update of OPTIC's\nstructures, which is not required as long as border routers remain at least\nbi-connected, scales linearly in time with its number of groups.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 08:44:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Luttringer", "Jean-Romain", "", "UNISTRA"], ["Bramas", "Quentin", "", "UNISTRA"], ["Pelsser", "Cristel", "", "UNISTRA"], ["M\u00e9rindol", "Pascal", "", "UNISTRA"]]}, {"id": "2101.09163", "submitter": "Aidong Yang", "authors": "Ye Ouyang (1), Lilei Wang (1), Aidong Yang (1), Maulik Shah (2), David\n  Belanger (3 and 4), Tongqing Gao (5), Leping Wei (6), Yaqin Zhang (7) ((1)\n  AsiaInfo Technologies, (2) Verizon, (3) AT&T, (4) Stevens Institute of\n  Technology, (5) China Mobile, (6) China Telecom, (7) Tsinghua University)", "title": "The Next Decade of Telecommunications Artificial Intelligence", "comments": "19 pages, in Chinese, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has been an exciting journey since the mobile communications and\nartificial intelligence were conceived 37 years and 64 years ago. While both\nfields evolved independently and profoundly changed communications and\ncomputing industries, the rapid convergence of 5G and deep learning is\nbeginning to significantly transform the core communication infrastructure,\nnetwork management and vertical applications. The paper first outlines the\nindividual roadmaps of mobile communications and artificial intelligence in the\nearly stage, with a concentration to review the era from 3G to 5G when AI and\nmobile communications started to converge. With regard to telecommunications\nartificial intelligence, the paper further introduces in detail the progress of\nartificial intelligence in the ecosystem of mobile communications. The paper\nthen summarizes the classifications of AI in telecom ecosystems along with its\nevolution paths specified by various international telecommunications\nstandardization bodies. Towards the next decade, the paper forecasts the\nprospective roadmap of telecommunications artificial intelligence. In line with\n3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network\nintelligence following 3GPP and ORAN routes respectively, experience and\nintention driven network management and operation, network AI signalling\nsystem, intelligent middle-office based BSS, intelligent customer experience\nmanagement and policy control driven by BSS and OSS convergence, evolution from\nSLA to ELA, and intelligent private network for verticals. The paper is\nconcluded with the vision that AI will reshape the future B5G or 6G landscape\nand we need pivot our R&D, standardizations, and ecosystem to fully take the\nunprecedented opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 07:33:44 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 02:25:23 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 10:19:47 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 14:41:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ouyang", "Ye", "", "3 and 4"], ["Wang", "Lilei", "", "3 and 4"], ["Yang", "Aidong", "", "3 and 4"], ["Shah", "Maulik", "", "3 and 4"], ["Belanger", "David", "", "3 and 4"], ["Gao", "Tongqing", ""], ["Wei", "Leping", ""], ["Zhang", "Yaqin", ""]]}, {"id": "2101.09343", "submitter": "Bin Han", "authors": "Amina Lejla Ibrahimpasic, Bin Han, and Hans D. Schotten", "title": "AI-Empowered VNF Migration as a Cost-Loss-Effective Solution for Network\n  Resilience", "comments": "Accepted by the IEEE WCNC 2021 Workshop on Intelligent Computing and\n  Caching at the Network Edge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a wide deployment of Multi-Access Edge Computing (MEC) in the Fifth\nGeneration (5G) mobile networks, virtual network functions (VNF) can be\nflexibly migrated between difference locations, and therewith significantly\nenhances the network resilience to counter the degradation in quality of\nservice (QoS) due to network function outages. A balance has to be taken\ncarefully, between the loss reduced by VNF migration and the operations cost\ngenerated thereby. To achieve this in practical scenarios with realistic user\nbehavior, it calls for models of both cost and user mobility. This paper\nproposes a novel cost model and a AI-empowered approach for a rational\nmigration of stateful VNFs, which minimizes the sum of operations cost and\npotential loss caused by outages, and is capable to deal with the complex\nrealistic user mobility patterns.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 21:47:41 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ibrahimpasic", "Amina Lejla", ""], ["Han", "Bin", ""], ["Schotten", "Hans D.", ""]]}, {"id": "2101.09365", "submitter": "Vasudevan Nagendra", "authors": "Vasudevan Nagendra, Abhishek Pokala, Arani Bhattacharya, Samir Das", "title": "MAVERICK: Proactively detecting network control plane bugs using\n  structural outlierness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactive detection of network configuration bugs is important to ensure its\nproper functioning and reduce cost of network administrator. In this research,\nwe propose to build the control plane verification engine MAVERICK that detects\nthe bugs in the network control plane i.e., network device configurations and\ncontrol plane states. MAVERICK automatically infers signatures for the control\nplane configurations (e.g., ACLs, route-maps, route-policies and so on) and\nstates that allows administrators to automatically detect bugs with minimal\nhuman intervention. MAVERICK achieves this by effectively leveraging any\nstructural deviation i.e., outliers in the network configurations that is\norganized as simple or complexly nested key-value pairs. The outliers that are\ncalculated using signature-based outlier detection mechanism are further\ncharacterized for its severity and ranked or re-prioritized according to their\ncriticality. We consider a wide set of heuristics and domain expertise factors\nfor effectively to reduce both false positives and false negatives.Our\nevaluation on four medium to large-scale enterprise networks show that MAVERICK\ncan automatically detect the bugs present in the network with approximately 75%\naccuracy. Further-more, With minimal administrator input i.e., with a few\nminutes of signature re-tuning, MAVERICK allows the administrators to\neffectively detect approximately 94 - 100% of the bugs present in the network,\nthereby ranking down less severe bugs and removing false positives.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 22:23:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Nagendra", "Vasudevan", ""], ["Pokala", "Abhishek", ""], ["Bhattacharya", "Arani", ""], ["Das", "Samir", ""]]}, {"id": "2101.09515", "submitter": "Dheryta Jaisinghani", "authors": "Dheryta Jaisinghani, Vinayak Naik, Rajesh Balan, Archan Misra, and\n  Youngki Lee", "title": "Experiences & Challenges with Server-Side WiFi Indoor Localization Using\n  Existing Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world deployments of WiFi-based indoor localization in large public\nvenues are few and far between as most state-of-the-art solutions require\neither client or infrastructure-side changes. Hence, even though high location\naccuracy is possible with these solutions, they are not practical due to cost\nand/or client adoption reasons. Majority of the public venues use commercial\ncontroller-managed WLAN solutions, %provided by Aruba, Cisco, etc., that\nneither allow client changes nor infrastructure changes. In fact, for such\nvenues we have observed highly heterogeneous devices with very low adoption\nrates for client-side apps.\n  In this paper, we present our experiences in deploying a scalable location\nsystem for such venues. We show that server-side localization is not trivial\nand present two unique challenges associated with this approach, namely\nCardinality Mismatch and High Client Scan Latency. The \"Mismatch\" challenge\nresults in a significant mismatch between the set of access points (APs)\nreporting a client in the offline and online phases, while the \"Latency\"\nchallenge results in a low number of APs reporting data for any particular\nclient. We collect three weeks of detailed ground truth data (~200 landmarks),\nfrom a WiFi setup that has been deployed for more than four years, to provide\nevidences for the extent and understanding the impact of these problems. Our\nanalysis of real-world client devices reveal that the current trend for the\nclients is to reduce scans, thereby adversely impacting their localization\naccuracy. We analyze how localization is impacted when scans are minimal. We\npropose heuristics to alleviate reduction in the accuracy despite lesser scans.\nBesides the number of scans, we summarize the other challenges and pitfalls of\nreal deployments which hamper the localization accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 14:48:17 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 04:58:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Jaisinghani", "Dheryta", ""], ["Naik", "Vinayak", ""], ["Balan", "Rajesh", ""], ["Misra", "Archan", ""], ["Lee", "Youngki", ""]]}, {"id": "2101.09547", "submitter": "Chun-Hung Liu", "authors": "Chun-Hung Liu, Di-Chun Liang, Rung-Hung Gau", "title": "A 3D Modeling Approach to Tractable Analysis in UAV-Enabled Cellular\n  Networks", "comments": "6 pages, 4 figures, conference paper. arXiv admin note: substantial\n  text overlap with arXiv:2007.09866", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to propose a three-dimensional (3D) point process that can be\nemployed to generally deploy unmanned aerial vehicles (UAVs) in a large-scale\ncellular network and tractably analyze the fundamental network-wide\nperformances of the network. This 3D point process is devised based on a 2D\nmarked Poisson point process in which each point and its random mark uniquely\ncorrespond to the projection and the altitude of each point in the 3D point\nprocess, respectively. We elaborate on some important statistical properties of\nthe proposed 3D point process and use them to tractably analyze the coverage\nperformances of a UAV-enabled cellular network wherein all the UAVs equipped\nwith multiple antennas are served as aerial base stations. The downlink\ncoverage of the UAV-enabled cellular network is found and its closed-form\nresults for some special cases are explicitly derived as well. Furthermore, the\nfundamental limits achieved by cell-free massive antenna array are\ncharacterized when coordinating all the UAVs to jointly perform non-coherent\ndownlink transmission. These findings are validated by numerical simulation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 18:02:10 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Liu", "Chun-Hung", ""], ["Liang", "Di-Chun", ""], ["Gau", "Rung-Hung", ""]]}, {"id": "2101.09589", "submitter": "Yuhang Ye", "authors": "Yuhang Ye, Brian Lee, Yuansong Qiao", "title": "R2P2: Reactive Routing and Payment Protocol for Named Data Network using\n  Blockchain", "comments": "4 pages excluding reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous emergence of new mobile devices which support new\ncommunication paradigms such as D2D and V2V, Internet users can take advantage\nof these devices to achieve better Internet connectivity and improve service\nquality. Meanwhile, packet forwarding brings extra costs to devices (e.g.\nelectricity consumption), that hinders the realisation of successful ad-hoc\nnetworks. This paper proposes Reactive Routing and Payment Protocol (R2P2) to\nincentivise mobile devices to contribute idle networking resources and gain\nmonetary returns. The routing and payment protocol is developed for Named-Data\nNetwork (NDN) because its content-centric nature can better support the\nintermittent and ephemeral communication requirements in ad-hoc networks.\nBlockchain is used as the settlement platform for transactions between devices\nbecause of its neutrality, robustness and trust. R2P2 is still an on-going\nproject. The content of this paper focuses on the design of R2P2.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 21:46:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ye", "Yuhang", ""], ["Lee", "Brian", ""], ["Qiao", "Yuansong", ""]]}, {"id": "2101.09701", "submitter": "Igor Donevski", "authors": "Igor Donevski, Jimmy Jessen Nielsen, Petar Popovski", "title": "Standalone Deployment of a Dynamic Drone Cell for Wireless Connectivity\n  of Two Services", "comments": "To be published in proceedings of WCNC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat a setting in which two priority wireless service classes are offered\nin a given area by a drone small cell (DSC). Specifically, we consider\nbroadband (BB) user with high priority and reliability requirements that\ncoexists with random access machine-type-communications (MTC) devices. The\ndrone serves both connectivity types with a combination of orthogonal slicing\nof the wireless resources and dynamic horizontal opportunistic positioning\n(D-HOP). We treat the D-HOP as a computational geometry function over\nstochastic BB user locations which requires careful adjustment in the\ndeployment parameters to ensure MTC service at all times. Using an information\ntheoretic approach, we optimize DSC deployment properties and radio resource\nallocation for the purpose of maximizing the average rate of BB users. While\nrespecting the strict dual service requirements we analyze how system\nperformance is affected by stochastic user positioning and density, topology,\nand reliability constraints combinations. The numerical results show that this\napproach outperforms static DSCs that fit the same coverage constraints, with\noutstanding performance in the urban setting.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:23:43 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Donevski", "Igor", ""], ["Nielsen", "Jimmy Jessen", ""], ["Popovski", "Petar", ""]]}, {"id": "2101.09716", "submitter": "TianZhang He", "authors": "TianZhang He, Adel N. Toosi, Rajkumar Buyya", "title": "SLA-Aware Multiple Migration Planning and Scheduling in SDN-NFV-enabled\n  Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Software-Defined Networking (SDN)-enabled cloud data centers, live\nmigration is a key approach used for the reallocation of Virtual Machines (VMs)\nin cloud services and Virtual Network Functions (VNFs) in Service Function\nChaining (SFC). Using live migration methods, cloud providers can address their\ndynamic resource management and fault tolerance objectives without interrupting\nthe service of users. However, in cloud data centers, performing multiple live\nmigrations in arbitrary order can lead to service degradation. Therefore,\nefficient migration planning is essential to reduce the impact of live\nmigration overheads. In addition, to prevent Quality of Service (QoS)\ndegradations and Service Level Agreement (SLA) violations, it is necessary to\nset priorities for different live migration requests with various urgency. In\nthis paper, we propose SLAMIG, a set of algorithms that composes the\ndeadline-aware multiple migration grouping algorithm and on-line migration\nscheduling to determine the sequence of VM/VNF migrations. The experimental\nresults show that our approach with reasonable algorithm runtime can\nefficiently reduce the number of deadline misses and has a good migration\nperformance compared with the one-by-one scheduling and two state-of-the-art\nalgorithms in terms of total migration time, average execution time, downtime,\nand transferred data. We also evaluate and analyze the impact of multiple\nmigration planning and scheduling on QoS and energy consumption.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 13:15:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["He", "TianZhang", ""], ["Toosi", "Adel N.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2101.09818", "submitter": "Ali Rasteh", "authors": "Ali Rasteh, Florian Delpech, Carlos Aguilar-Melchor, Romain Zimmer,\n  Saeed Bagheri Shouraki and Timoth\\'ee Masquelier", "title": "Encrypted Internet traffic classification using a supervised Spiking\n  Neural Network", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet traffic recognition is an essential tool for access providers since\nrecognizing traffic categories related to different data packets transmitted on\na network help them define adapted priorities. That means, for instance, high\npriority requirements for an audio conference and low ones for a file transfer,\nto enhance user experience. As internet traffic becomes increasingly encrypted,\nthe mainstream classic traffic recognition technique, payload inspection, is\nrendered ineffective. This paper uses machine learning techniques for encrypted\ntraffic classification, looking only at packet size and time of arrival.\nSpiking neural networks (SNN), largely inspired by how biological neurons\noperate, were used for two reasons. Firstly, they are able to recognize\ntime-related data packet features. Secondly, they can be implemented\nefficiently on neuromorphic hardware with a low energy footprint. Here we used\na very simple feedforward SNN, with only one fully-connected hidden layer, and\ntrained in a supervised manner using the newly introduced method known as\nSurrogate Gradient Learning. Surprisingly, such a simple SNN reached an\naccuracy of 95.9% on ISCX datasets, outperforming previous approaches. Besides\nbetter accuracy, there is also a very significant improvement on simplicity:\ninput size, number of neurons, trainable parameters are all reduced by one to\nfour orders of magnitude. Next, we analyzed the reasons for this good accuracy.\nIt turns out that, beyond spatial (i.e. packet size) features, the SNN also\nexploits temporal ones, mostly the nearly synchronous (within a 200ms range)\narrival times of packets with certain sizes. Taken together, these results show\nthat SNNs are an excellent fit for encrypted internet traffic classification:\nthey can be more accurate than conventional artificial neural networks (ANN),\nand they could be implemented efficiently on low power embedded systems.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 22:46:08 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rasteh", "Ali", ""], ["Delpech", "Florian", ""], ["Aguilar-Melchor", "Carlos", ""], ["Zimmer", "Romain", ""], ["Shouraki", "Saeed Bagheri", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "2101.09878", "submitter": "Ajesh Koyatan Chathoth", "authors": "Ajesh Koyatan Chathoth (1), Abhyuday Jagannatha (2), Stephen Lee (1)\n  ((1) University of Pittsburgh, (2) University of Massachusetts Amherst)", "title": "Federated Intrusion Detection for IoT with Heterogeneous Cohort Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) devices are becoming increasingly popular and are\ninfluencing many application domains such as healthcare and transportation.\nThese devices are used for real-world applications such as sensor monitoring,\nreal-time control. In this work, we look at differentially private (DP) neural\nnetwork (NN) based network intrusion detection systems (NIDS) to detect\nintrusion attacks on networks of such IoT devices. Existing NN training\nsolutions in this domain either ignore privacy considerations or assume that\nthe privacy requirements are homogeneous across all users. We show that the\nperformance of existing differentially private stochastic methods degrade for\nclients with non-identical data distributions when clients' privacy\nrequirements are heterogeneous. We define a cohort-based $(\\epsilon,\\delta)$-DP\nframework that models the more practical setting of IoT device cohorts with\nnon-identical clients and heterogeneous privacy requirements. We propose two\nnovel continual-learning based DP training methods that are designed to improve\nmodel performance in the aforementioned setting. To the best of our knowledge,\nours is the first system that employs a continual learning-based approach to\nhandle heterogeneity in client privacy requirements. We evaluate our approach\non real datasets and show that our techniques outperform the baselines. We also\nshow that our methods are robust to hyperparameter changes. Lastly, we show\nthat one of our proposed methods can easily adapt to post-hoc relaxations of\nclient privacy requirements.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:33:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chathoth", "Ajesh Koyatan", "", "University of Pittsburgh"], ["Jagannatha", "Abhyuday", "", "University of Massachusetts Amherst"], ["Lee", "Stephen", "", "University of Pittsburgh"]]}, {"id": "2101.09904", "submitter": "Shivang Singh", "authors": "Sai Koppula, Shivang Singh", "title": "Using Angle of Arrival for Improving Indoor Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.NI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we primarily explore the improvement of single stream audio\nsystems using Angle of Arrival calculations in both simulation and real life\ngathered data. We wanted to learn how to discern the direction of an audio\nsource from gathered signal data to ultimately incorporate into a multi modal\nsecurity system. We focused on the MUSIC algorithm for the estimation of the\nangle of arrival but briefly experimented with other techniques such as\nBartlett and Capo. We were able to implement our own MUSIC algorithm on\nstimulated data from Cornell. In addition, we demonstrated how we are able to\ncalculate the angle of arrival over time in a real life scene. Finally, we are\nable to detect the direction of arrival for two separate and simultaneous audio\nsources in a real life scene. Eventually, we could incorporate this tracking\ninto a multi modal system combined with video. Overall, we are able to produce\ncompelling results for angle of arrival calculations that could be the stepping\nstones for a better system to detect events in a scene.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 05:52:19 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Koppula", "Sai", ""], ["Singh", "Shivang", ""]]}, {"id": "2101.10225", "submitter": "Vishrant Tripathi", "authors": "Vishrant Tripathi, Eytan Modiano", "title": "Age Debt: A General Framework For Minimizing Age of Information", "comments": "To be presented at the 4th AoI Workshop, IEEE INFOCOM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing age of information in general\nsingle-hop and multihop wireless networks. First, we formulate a way to convert\nAoI optimization problems into equivalent network stability problems. Then, we\npropose a heuristic low complexity approach for achieving stability that can\nhandle general network topologies; unicast, multicast and broadcast flows;\ninterference constraints; link reliabilities; and AoI cost functions. We\nprovide numerical results to show that our proposed algorithms behave as well\nas the best known scheduling and routing schemes available in the literature\nfor a wide variety of network settings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:36:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tripathi", "Vishrant", ""], ["Modiano", "Eytan", ""]]}, {"id": "2101.10254", "submitter": "Jithin Jagannath", "authors": "Anu Jagannath, Jithin Jagannath", "title": "Multi-task Learning Approach for Automatic Modulation and Wireless\n  Signal Classification", "comments": "To appear in Proc. of IEEE International Conference on Communications\n  (ICC) 2021. Open data set also included in the reference", "journal-ref": "IEEE International Conference on Communications (ICC) 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Wireless signal recognition is becoming increasingly more significant for\nspectrum monitoring, spectrum management, and secure communications.\nConsequently, it will become a key enabler with the emerging fifth-generation\n(5G) and beyond 5G communications, Internet of Things networks, among others.\nState-of-the-art studies in wireless signal recognition have only focused on a\nsingle task which in many cases is insufficient information for a system to act\non. In this work, for the first time in the wireless communication domain, we\nexploit the potential of deep neural networks in conjunction with multi-task\nlearning (MTL) framework to simultaneously learn modulation and signal\nclassification tasks. The proposed MTL architecture benefits from the mutual\nrelation between the two tasks in improving the classification accuracy as well\nas the learning efficiency with a lightweight neural network model.\nAdditionally, we consider the problem of heterogeneous wireless signals such as\nradar and communication signals in the electromagnetic spectrum. Accordingly,\nwe have shown how the proposed MTL model outperforms several state-of-the-art\nsingle-task learning classifiers while maintaining a lighter architecture and\nperforming two signal characterization tasks simultaneously. Finally, we also\nrelease the only known open heterogeneous wireless signals dataset that\ncomprises of radar and communication signals with multiple labels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:43:42 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 21:14:59 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jagannath", "Anu", ""], ["Jagannath", "Jithin", ""]]}, {"id": "2101.10424", "submitter": "Hao Yin", "authors": "Liu Cao, Hao Yin", "title": "Resource Allocation for Vehicle Platooning in 5G NR-V2X via Deep\n  Reinforcement Learning", "comments": "To be published in Blackseacom2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle platooning, one of the advanced services supported by 5G NR-V2X,\nimproves traffic efficiency in the connected intelligent transportation systems\n(C-ITSs). However, the packet delivery ratio of platoon communication,\nespecially in the out-of-coverage area, is significantly impacted by the random\nselection algorithms employed in the current resource allocation scheme. In\nthis paper, we first analyze the collision probability via the random selection\nalgorithm adopted in the current standard. Subsequently, we then investigate\nthe deep reinforcement learning (DRL) algorithm that decreases the collision\nprobability by letting the agent (vehicle) learn from the communication\nenvironment. Monte Carlo simulation is utilized to verify the results obtained\nin the analytical model and to compare the results between the two discussed\nalgorithms. Numerical results show that the proposed DRL algorithm outperforms\nthe random selection algorithm in terms of different vehicle density, which at\nleast lowering the collision probability by 73% and 45% in low and high vehicle\ndensity respectively.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 21:28:09 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 23:09:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Cao", "Liu", ""], ["Yin", "Hao", ""]]}, {"id": "2101.10447", "submitter": "Seungmo Kim", "authors": "Seungmo Kim, Byung-Jun Kim, and B. Brian Park", "title": "Environment-Adaptive Multiple Access for Distributed V2X Network: A\n  Reinforcement Learning Framework", "comments": "This manuscript will be submitted to IEEE Vehicular Technology\n  Conference 2021 Spring", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The huge research interest in cellular vehicle-to-everything (C-V2X)\ncommunications in recent days is attributed to their ability to schedule\nmultiple access more efficiently as compared to its predecessor technology,\ni.e., dedicated short-range communications (DSRC). However, one of the foremost\nissues still remaining is the need for the V2X to operate stably in a highly\ndynamic environment. This paper proposes a way to exploit the dynamicity. That\nis, we propose a resource allocation mechanism adaptive to the environment,\nwhich can be an efficient solution for air interface congestion that a V2X\nnetwork often suffers from. Specifically, the proposed mechanism aims at\ngranting a higher chance of transmission to a vehicle with a higher crash risk.\nAs such, the channel access is prioritized to those with urgent needs. The\nproposed framework is established based on reinforcement learning (RL), which\nis modeled as a contextual multi-armed bandit (MAB). Importantly, the framework\nis designed to operate at a vehicle autonomously without any assistance from a\ncentral entity, which, henceforth, is expected to make a particular fit to\ndistributed V2X network such as C-V2X mode 4.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 14:21:28 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Kim", "Seungmo", ""], ["Kim", "Byung-Jun", ""], ["Park", "B. Brian", ""]]}, {"id": "2101.10451", "submitter": "Vanlin Sathya", "authors": "Vanlin Sathya", "title": "Evolution of Small Cell from 4G to 6G: Past, Present, and Future", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To boost the capacity of the cellular system, the operators have started to\nreuse the same licensed spectrum by deploying 4G LTE small cells (Femto Cells)\nin the past. But in time, these small cell licensed spectrum is not sufficient\nto satisfy future applications like augmented reality (AR)and virtual reality\n(VR). Hence, cellular operators look for alternate unlicensed spectrum in Wi-Fi\n5 GHz band, later 3GPP named as LTE Licensed Assisted Access (LAA). The recent\nand current rollout of LAA deployments (in developed nations like the US)\nprovides an opportunity to understand coexistence profound ground truth. This\npaper discusses a high-level overview of my past, present, and future research\nworks in the direction of small cell benefits. In the future, we shift the\nfocus onto the latest unlicensed band: 6 GHz, where the latest Wi-Fi version,\n802.11ax, will coexist with the latest cellular technology, 5G New Radio(NR) in\nunlicensed\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 17:28:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sathya", "Vanlin", ""]]}, {"id": "2101.10453", "submitter": "Abhilash Singh", "authors": "Abhilash Singh, Sandeep Sharma, Jitenda Singh", "title": "Nature-Inspired Algorithms for Wireless Sensor Networks: A Comprehensive\n  Survey", "comments": null, "journal-ref": "Computer Science Review (2020) 100342", "doi": "10.1016/j.cosrev.2020.100342", "report-no": null, "categories": "cs.NI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to solve the critical issues in Wireless Sensor Networks (WSNs),\nwith concern for limited sensor lifetime, nature-inspired algorithms are\nemerging as a suitable method. Getting optimal network coverage is one of those\nchallenging issues that need to be examined critically before any network\nsetup. Optimal network coverage not only minimizes the consumption of limited\nenergy of battery-driven sensors but also reduce the sensing of redundant\ninformation. In this paper, we focus on nature-inspired optimization algorithms\nconcerning the optimal coverage in WSNs. In the first half of the paper, we\nhave briefly discussed the taxonomy of the optimization algorithms along with\nthe problem domains in WSNs. In the second half of the paper, we have compared\nthe performance of two nature-inspired algorithms for getting optimal coverage\nin WSNs. The first one is a combined Improved Genetic Algorithm and Binary Ant\nColony Algorithm (IGABACA), and the second one is Lion Optimization (LO). The\nsimulation results confirm that LO gives better network coverage, and the\nconvergence rate of LO is faster than that of IGA-BACA. Further, we observed\nthat the optimal coverage is achieved at a lesser number of generations in LO\nas compared to IGA-BACA. This review will help researchers to explore the\napplications in this field as well as beyond this area. Keywords: Optimal\nCoverage, Bio-inspired Algorithm, Lion Optimization, WSNs.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:30:04 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Singh", "Abhilash", ""], ["Sharma", "Sandeep", ""], ["Singh", "Jitenda", ""]]}, {"id": "2101.10454", "submitter": "Aksh Garg", "authors": "Aksh Garg", "title": "Machine Learning Coupled Trajectory and Communication Design for\n  UAV-Facilitated Wireless Networks", "comments": "16 pages. 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Augmenting wireless networks with Unmanned Aerial Vehicles (UAVs), commonly\nreferred to as drones, offers a promising avenue for providing reliable,\ncost-effective, and on-demand wireless services to desired areas. However,\nexisting UAV communication and trajectory schemes are inefficient as they\nassume limited drone mobility and static transmission power. Furthermore, they\ntend to rely upon convex approximations to highly non-linear functions and fail\nto adopt a combination of heuristic and convex methods. This paper considers a\nMulti-UAV system where UAV-mounted mobile base stations serve users on the\nground. An iterative approach using block gradient descent is used to jointly\noptimize user scheduling, UAV trajectories, and transmission power for\nmaximizing throughput over all users. Subsequently, an innovative technique for\ninitial trajectory predictions was developed using a K-means clustering\nalgorithm for partitioning users into subgroups and a genetic algorithm for\ninitializing shortest flight paths within clusters. Finally, convex\noptimization solvers such as MATLAB's Fmincon are used for fine-tuning\nparameters. Extensive simulation and optimization results demonstrate a 33.57%,\n87.4%, and 53.2% increase in system throughput for the 1, 2, and 3 UAV\nscenarios respectively when compared to existing trajectory and communication\ndesign schemes. Furthermore, the K-means and genetic algorithm reveal\nadditional improvements in throughput by around 15%. Our results note\ndiminished increases in throughput for increases in UAV trajectory period as\nthe period approaches higher values. Further research into joint adoption of\nconvex and non-convex schemes as well as consideration of environment-dependent\nchannel models would allow for a faster and more optimal deployment of UAVs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 01:04:16 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Garg", "Aksh", ""]]}, {"id": "2101.10455", "submitter": "Trung-Kien Le", "authors": "Trung-Kien Le, Umer Salim, Florian Kaltenberger", "title": "Frame based equipment channel access enhancements in NR unlicensed\n  spectrum for the URLLC transmissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ultra-reliable low-latency communication (URLLC) in 5G New Radio has been\noriginally defined only for licensed spectrum. However, due to new use cases in\nthe Industry 4.0 scenarios, URLLC operation is currently being extended to\nunlicensed spectrum in the ongoing Release 17 of the 3rd Generation Partnership\nProject. Although in such controlled environments we can guarantee the absence\nof any other technology sharing the channel on a long-term basis, the\nuncertainty of obtaining channel access through load based equipment (LBE) or\nframe based equipment (FBE) can impede with the latency requirements of URLLC.\nIn FBE, the transmitters can be prioritized to support data with different\nrequirements and have lower energy consumption and latency compared to LBE with\na big contention window size. In this paper we analyze the performance of FBE\nin an unlicensed controlled environment through a Markov chain. Based on this\nanalysis, we propose two schemes to improve the URLLC performance in FBE: The\nfirst scheme allows the transmitters to use multiple fixed frame period (FFP)\nconfigurations while the second scheme configures the FFP's starting point of\neach transmitter based on its priority. The simulations show the benefits of\nthese schemes compared to the URLLC transmission of existing schemes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 08:48:02 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 20:34:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Le", "Trung-Kien", ""], ["Salim", "Umer", ""], ["Kaltenberger", "Florian", ""]]}, {"id": "2101.10464", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo, V\\'ictor\n  Rodr\\'iguez-Doncel", "title": "Personal Data Access Control Through Distributed Authorization", "comments": null, "journal-ref": "2020 IEEE 19th International Symposium on Network Computing and\n  Applications (NCA)", "doi": "10.1109/NCA51143.2020.9306721", "report-no": null, "categories": "cs.CR cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture of a Personal Information Management\nSystem, in which individuals can define the access to their personal data by\nmeans of smart contracts. These smart contracts, running on the Ethereum\nblockchain, implement access control lists and grant immutability, traceability\nand verifiability of the references to personal data, which is stored itself in\na (possibly distributed) file system. A distributed authorization mechanism is\ndevised, where trust from multiple network nodes is necessary to grant the\naccess to the data. To this aim, two possible alternatives are described: a\nSecret Sharing scheme and Threshold Proxy Re-Encryption scheme. The performance\nof these alternatives is experimentally compared in terms of execution time.\nThreshold Proxy Re-Encryption appears to be faster in different scenarios, in\nparticular when increasing message size, number of nodes and the threshold\nvalue, i.e. number of nodes needed to grant the data disclosure.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""], ["Rodr\u00edguez-Doncel", "V\u00edctor", ""]]}, {"id": "2101.10558", "submitter": "Zohre Ranjbar Mojaveri", "authors": "Zohre Ranjbar Mojaveri and Andras Farago", "title": "Routing Packet Traffic via Enhanced Access Control List for Network\n  Congestion Avoidance", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Filtering packet traffic and rules of permit/denial of data packets into\nnetwork nodes are granted by facilitating Access Control Lists (ACL). This\npaper proposes a procedure of adding a link load threshold value to the access\ncontrol list rules option, which acts on the basis of threshold value. The\nultimate goal of this enhanced ACL is to avoid congestion in targeted\nsubnetworks. The link load threshold value allows to decide that packet traffic\nis rerouted by the router to avoid congestion, or packet drop happens on the\nbasis of packet priorities. The packet rerouting in case of high traffic loads,\nbased on new packet filtering procedure for congestion avoidance, will result\nin the reduction of the overall packet drop ratio, and of over-subscription in\ncongested subnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 05:03:31 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Mojaveri", "Zohre Ranjbar", ""], ["Farago", "Andras", ""]]}, {"id": "2101.10632", "submitter": "Frederik Hauser", "authors": "Frederik Hauser, Marco H\\\"aberle, Daniel Merling, Steffen Lindner,\n  Vladimir Gurevich, Florian Zeiger, Reinhard Frank, Michael Menth", "title": "A Survey on Data Plane Programming with P4: Fundamentals, Advances, and\n  Applied Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable data planes allow users to define their own data plane\nalgorithms for network devices including appropriate data plane application\nprogramming interfaces (APIs) which may be leveraged by user-defined\nsoftware-defined networking (SDN) control. This offers great flexibility for\nnetwork customization, be it for specialized, commercial appliances, e.g., in\n5G or data center networks, or for rapid prototyping in industrial and academic\nresearch. Programming protocol-independent packet processors (P4) has emerged\nas the currently most widespread abstraction, programming language, and concept\nfor data plane programming. It is developed and standardized by an open\ncommunity, and it is supported by various software and hardware platforms. In\nthe first part of this paper we give a tutorial of data plane programming\nmodels, the P4 programming language, architectures, compilers, targets, and\ndata plane APIs. We also consider research efforts to advance P4 technology. In\nthe second part, we categorize a large body of literature of P4-based applied\nresearch into different research domains, summarize the contributions of these\npapers, and extract prototypes, target platforms, and source code availability.\nFor each research domain, we analyze how the reviewed works benefit from P4's\ncore features. Finally, we discuss potential next steps based on our findings.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 08:42:41 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 08:52:24 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hauser", "Frederik", ""], ["H\u00e4berle", "Marco", ""], ["Merling", "Daniel", ""], ["Lindner", "Steffen", ""], ["Gurevich", "Vladimir", ""], ["Zeiger", "Florian", ""], ["Frank", "Reinhard", ""], ["Menth", "Michael", ""]]}, {"id": "2101.10753", "submitter": "Zainab Zaidi", "authors": "Zainab R. Zaidi, Hazer Inaltekin, Jamie Evans", "title": "Separation of Control and Data Transmissions in 5G Networks may not be\n  Beneficial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The logical separation of control signaling from data transmission in a\nmobile cellular network has been shown to have significant energy saving\npotential compared with the legacy systems. As a result, there has been a lot\nof focus in recent years on development and realization of separation\narchitectures. Our study, however, shows that the energy savings of separation\narchitecture remain under 16-17% when compared with legacy systems and this\ngain falls to a mere 7% when both architectures are realized under a CloudRAN\n(CRAN) setting. Moreover, when we strategically place some small base-stations\n(SBSs) to cover the area in a densely deployed scenario and allow all other\nbase-stations (BSs) to be used only on-demand, the system consumes much less\nenergy than the separation architecture. While we expected that most equipment\nwould be shut down during nights, our study shows that around 70% of the small\ncells are required to be active to serve randomly distributed minimum data\nload, i.e., active mobile equipment. Contemporary mobile traffic is\npredominantly data which does not go to extremely low levels during nights. We\ndiscuss, in detail, the assumptions, their implications, and the effects of\nsystem parameter values on our conclusions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:50:46 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zaidi", "Zainab R.", ""], ["Inaltekin", "Hazer", ""], ["Evans", "Jamie", ""]]}, {"id": "2101.10758", "submitter": "Tarachand Amgoth", "authors": "Dinesh Kumar Sah, Praveen Kumar Donta and Tarachand Amgoth", "title": "EDGF: Empirical dataset generation framework for wireless network\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In wireless sensor networks (WSNs), simulation practices, system models,\nalgorithms, and protocols have been published worldwide based on the assumption\nof randomness. The applied statistics used for randomness in WSNs are broad in\nnature, e.g., random deployment, activity tracking, packet generation, etc.\nEven though with adequate formal and informal information provided and pledge\nby authors, validation of the proposal became a challenging issue. The\nminuscule information alteration in implementation and validation can reflect\nthe enormous effect on eventual results. In this proposal, we show how the\nresults are affected by the generalized assumption made on randomness. In\nsensor node deployment, ambiguity arises due to node error-value ($\\epsilon$),\nand it's upper bound in the relative position is estimated to understand the\ndelicacy of diminutives changes. Moreover, the effect of uniformity in the\ntraffic and contribution of scheduling position of nodes also generalized. We\npropose an algorithm to generate the unified dataset for the general and some\nspecific applications system models in WSNs. The results produced by our\nalgorithm reflects the pseudo-randomness and can efficiently regenerate through\nseed value for validation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 12:59:54 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Sah", "Dinesh Kumar", ""], ["Donta", "Praveen Kumar", ""], ["Amgoth", "Tarachand", ""]]}, {"id": "2101.10812", "submitter": "Yasameen Sajid Razoki", "authors": "Yasameen Sajid Razoki, Muntasir Al-Asfoor", "title": "Intelligent Routing to Enhance Energy Consumption in Wireless Sensor\n  Network: A survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the network and the Internet applications have gained a substantial\nimportance in the digital world, due to the great impact which it provides for\nhealth and community services. Among the most important services that have been\nprovided are smart devices and vital factor measurement devices for patients,\nwhether in a hospital or outside the hospital. Furthermore, sensors that\ncollect medical data or measurements of temperature and humidity, in various\ncritical environments. The proper types of network that may be used in such\ndifficult environment are Wireless Sensor Networks, that used to sense and\nprocess data. Additionally, the Wireless Sensor Networks have been used in the\nenvironment of Internet of Things and smart cities in the general services and\nhealth fields. All these reasons have made researchers focus on Wireless Sensor\nNetworks and addressing the challenges that face them. The most important\nchallenge facing this type of network is energy consumption and increase\nbattery life. This paper discusses the methodologies used in energy\nconservation in Wireless Sensor Networks, such as data reduction technology,\nshortest path selection and artificial intelligence algorithms used in smart\nrouting and energy saving. Besides, we have introduced comparisons between the\nstandard algorithms which are suggested by the researchers, to make a clear\npicture of the energy consumption problem and propose some effective solutions\nin Wireless Sensor Networks field.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 21:39:18 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Razoki", "Yasameen Sajid", ""], ["Al-Asfoor", "Muntasir", ""]]}, {"id": "2101.10816", "submitter": "Steven Platt", "authors": "Steven Platt", "title": "Application Layer Modeling in Vehicle Networks: Cooperative Maneuver Use\n  Case", "comments": "Master thesis, Universitat Pompeu Fabra, 63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, network function virtualization and a software defined focus\nhas allowed networks to become flexible and extensible in ways not possible\npreviously. Although network modeling tools such as NS-2, NS-3, and OMNet++\nhave been extended with modules and code to support the absolute latest\nwireless protocols and medium access standards - there has been a growing gap\nin simulation of layers above medium access which recent 5G use cases are\ndesigned to support. In this thesis, I extend research into the topic of\napplication layer modeling in wireless networks, with focus on upcoming\ndeployments of enhanced vehicle services. In particular, I begin in discussing\nthe evolution of vehicle network standards and use cases, along with the most\nrecent initiative `Project 5GCar`; funded by the European Commission. I go\nnarrower, from the macro trend of vehicular network standards evolution, to\ncompare the state of the art in simulation stacks, designed to support\napplication models in vehicle networks. Within this comparison, simulation\nenvironments OVNIS, Veins, iTetris, and VSimRTI are compared for their\ncapabilities. Finally, to measure the qualitative performance of application\nlayer modeling in vehicle networks, I take the cooperative maneuver use case,\npresented under Project 5GCar; to design an autonomous merge algorithm -\ncompleting the steps required to program and model the application in vehicles\nusing the VSimRTI simulation stack.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:37:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Platt", "Steven", ""]]}, {"id": "2101.10817", "submitter": "Syed Mohsan Raza", "authors": "Syed Mohsan Raza", "title": "Reliability Aware Multiple Path Installation in Software Defined\n  Networking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being a state-of-the-art network, Software Defined Networking (SDN) decouples\ncontrol and management planes from data plane of the forwarding devices by\nimplementing both the control and management planes at logically centralized\nentity, called controller. This helps to make simple and easy both the network\ncontrol and management. Failure of links occurs frequently in a computer\nnetwork. To deal with the link failures, the existing approaches computes and\ninstalls multiple paths for a flow at the switches in SDN without considering\nthe reliability value of the primary path. This incurs extra computation to\ncompute multiple paths, and both increased computation time and traffic to\ninstall extra flow rules in the network. In this research work we propose a new\napproach that calculates the link reliability and then installs the number of\nmultiple paths based on the reliability value of the primary path. More\nspecifically, if a primary path has higher reliability then a smaller number of\nalternative paths should be installed. This shall decrease the path\ncomputational time and flow rule installation load at controller. Resultantly\nthere shall be less flow rule entries in switch flow table which in turn will\navoid the overflow of the flow table. Through simulation results, our proposed\napproach performs better as compared to the existing approach in term of\ncomputational overhead at controller, end-to-end delay for packet deliver and\nthe traffic overhead for flow rule installation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 11:35:19 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Raza", "Syed Mohsan", ""]]}, {"id": "2101.10833", "submitter": "Amr Hilal", "authors": "Amr E Hilal, Ismail Arai, Samy El-Tawab", "title": "DataLoc+: A Data Augmentation Technique for Machine Learning in\n  Room-Level Indoor Localization", "comments": "7 pages, 7 figures, 1 table, 1 algorithm. Accepted at IEEE WCNC 2021,\n  and final version is submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor localization has been a hot area of research over the past two\ndecades. Since its advent, it has been steadily utilizing the emerging\ntechnologies to improve accuracy, and machine learning has been at the heart of\nthat. Machine learning has been increasingly used in fingerprint-based indoor\nlocalization to replace or emulate the radio map that is used to predict\nlocations given a location signature. The prediction quality of a machine\nlearning model primarily depends on how well the model was trained, which\nrelies on the amount and quality of data used to train it. Data augmentation\nhas been used to improve quality of the trained models by synthetically\nproducing more training data, and several approaches were used in the\nliterature that tackles the problem of lack of training data from different\nangles. In this paper, we propose DataLoc+, a data augmentation technique for\nroom-level indoor localization that combines different approaches in a simple\nalgorithm. We evaluate the technique by comparing it to the typical direct\nsnapshot approach using data collected from a field experiment conducted in a\nhospital. Our evaluation shows that the model trained using the proposed\ntechnique achieves higher accuracy. We also show that the technique adapts to\nlarger problems using a limited dataset while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:41:41 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hilal", "Amr E", ""], ["Arai", "Ismail", ""], ["El-Tawab", "Samy", ""]]}, {"id": "2101.10856", "submitter": "Hao Xu", "authors": "Hao Xu, Lei Zhang, Yunqing Sun, and Chih-Lin I", "title": "BE-RAN: Blockchain-enabled Open RAN with Decentralized Identity\n  Management and Privacy-Preserving Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio Access Networks (RAN) tends to be more distributed in the 5G and\nbeyond, in order to provide low latency and flexible on-demanding services. In\nthis paper, Blockchain-enabled Radio Access Networks (BE-RAN) is proposed as a\nnovel decentralized RAN architecture to facilitate enhanced security and\nprivacy on identification and authentication. It can offer user-centric\nidentity management for User Equipment (UE) and RAN elements, and enable mutual\nauthentication to all entities while enabling on-demand point-to-point\ncommunication with accountable billing service add-on on public network. Also,\na potential operating model with thorough decentralization of RAN is\nenvisioned. The paper also proposed a distributed privacy-preserving P2P\ncommunication approach, as one of the core use cases for future mobile\nnetworks, is presented as an essential complement to the existing core\nnetwork-based security and privacy management. The results show that BE-RAN\nsignificantly improves communication and computation overheads compared to the\nexisting communication authentication protocols.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 15:24:22 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 19:43:45 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 12:24:26 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Hao", ""], ["Zhang", "Lei", ""], ["Sun", "Yunqing", ""], ["I", "Chih-Lin", ""]]}, {"id": "2101.10961", "submitter": "Gian Pietro Picco", "authors": "Matteo Trobinger, Gabriel de Albuquerque Gleizer, Timofei Istomin,\n  Manuel Mazo Jr., Amy L. Murphy, Gian Pietro Picco", "title": "The Wireless Control Bus: Enabling Efficient Multi-hop Event-Triggered\n  Control with Concurrent Transmissions", "comments": "25 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.NI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-triggered control (ETC) holds the potential to significantly improve\nthe efficiency of wireless networked control systems. Unfortunately, its\nreal-world impact has hitherto been hampered by the lack of a network stack\nable to transfer its benefits from theory to practice specifically by\nsupporting the latency and reliability requirements of the aperiodic\ncommunication ETC induces. This is precisely the contribution of this paper.\n  Our Wireless Control Bus (WCB) exploits carefully orchestrated network-wide\nfloods of concurrent transmissions to minimize overhead during quiescent,\nsteady-state periods, and ensures timely and reliable collection of sensor\nreadings and dissemination of actuation commands when an ETC triggering\ncondition is violated. Using a cyber-physical testbed emulating a water\ndistribution system controlled over a real-world multi-hop wireless network, we\nshow that ETC over WCB achieves the same quality of periodic control at a\nfraction of the energy costs, therefore unleashing and concretely demonstrating\nits full potential for the first time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 17:38:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Trobinger", "Matteo", ""], ["Gleizer", "Gabriel de Albuquerque", ""], ["Istomin", "Timofei", ""], ["Mazo", "Manuel", "Jr."], ["Murphy", "Amy L.", ""], ["Picco", "Gian Pietro", ""]]}, {"id": "2101.11147", "submitter": "Mohammad Mukhtaruzzaman", "authors": "Mohammad Mukhtaruzzaman and Mohammed Atiquzzaman", "title": "Cloud based VANET Simulator (CVANETSIM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vehicular ad hoc network (VANET) is an integral part of vehicular\ncommunication. VANET suffers many problems such as scalability. To solve\nscalability and other problems of VANET, clustering is proposed. VANET\nclustering is different than any other kind of clustering due to the high\nmobility of the vehicles. Likewise, VANET and VANET clustering, VANET simulator\nrequires some unique features such as internet based real-time data processing,\nhuge data analysis, the complex calculation to maintain hierarchy among the\nvehicles, etc.; however, neither web based VANET simulator nor clustering\nmodule available in the existing simulators. Therefore, a simulator that will\nbe able to simulate any feature of VANET equipped with a clustering module and\naccessible via the internet is a growing need in vehicular communication\nresearch. At the Telecom and Network Research Lab (TNRL), University of\nOklahoma, we have developed a fully functional discrete-event VANET simulator\nthat includes all the features of VANET clustering. Moreover, the cloud based\nVANET simulator (CVANETSIM) is coming with an easy and interactive web\ninterface. To our best of our knowledge, CVANETSIM is the first of its kind\nwhich integrates features of the VANET simulator, built-in VANET clustering\nmodule, and accessible through the internet.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 00:49:06 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mukhtaruzzaman", "Mohammad", ""], ["Atiquzzaman", "Mohammed", ""]]}, {"id": "2101.11211", "submitter": "Vinayak Naik", "authors": "Vinayak Naik and Anish Arora", "title": "Harvest: A Reliable and Energy Efficient Bulk Data Collection Service\n  for Large Scale Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a bulk data collection service, Harvest, for energy constrained\nwireless sensor nodes. To increase spatial reuse and thereby decrease latency,\nHarvest performs concurrent, pipelined exfiltration from multiple nodes to a\nbase station. To this end, it uses a distance-k coloring of the nodes, notably\nwith a constant number of colors, which yields a TDMA schedule whereby nodes\ncan communicate concurrently with low packet losses due to collision. This\ncoloring is based on a randomized CSMA approach which does not exploit location\nknowledge. Given a bounded degree of the network, each node waits only O$(1)$\ntime to obtain a unique color among its distance-k neighbors, in contrast to\nthe traditional deterministic distributed distance-k vertex coloring wherein\neach node waits O$(\\Delta^{2})$ time to obtain a color.\n  Harvest offers the option of limiting memory use to only a small constant\nnumber of bytes or of improving latency with increased memory use; it can be\nused with or without additional mechanisms for reliability of message\nforwarding. We experimentally evaluate the performance of Harvest using 51\nmotes in the Kansei testbed. We also provide theoretical as well as\nTOSSIM-based comparison of Harvest with Straw, an extant data collection\nservice implemented for TinyOS platforms that use one-node at a time\nexfiltration. For networks with more than 3-hops, Harvest reduces the latency\nby at least 33% as compared to that of Straw.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 05:38:17 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Naik", "Vinayak", ""], ["Arora", "Anish", ""]]}, {"id": "2101.11246", "submitter": "David Lopez", "authors": "David Lopez-Perez, Antonio De Domenico, Nicola Piovesan, Harvey\n  Baohongqiang, Geng Xinli, Song Qitao and Merouane Debbah", "title": "A Survey on 5G Energy Efficiency: Massive MIMO, Lean Carrier Design,\n  Sleep Modes, and Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular networks have changed the world we are living in, and the fifth\ngeneration (5G) of radio technology is expected to further revolutionise our\neveryday lives, by enabling a high degree of automation, through its larger\ncapacity, massive connectivity, and ultra-reliable low latency communications.\nIn addition, the third generation partnership project (3GPP) new radio (NR)\nspecification also provides tools to significantly decrease the energy\nconsumption and the green house emissions of next generations networks, thus\ncontributing towards information and communication technology (ICT)\nsustainability targets. In this survey paper, we thoroughly review the\nstate-of-the-art on current energy efficiency research. We first categorise and\ncarefully analyse the different power consumption models and energy efficiency\nmetrics, which have helped to make progress on the understanding of green\nnetworks. Then, as a main contribution, we survey in detail -- from a\ntheoretical and a practical viewpoint -- the main energy efficiency enabling\nfeatures that 3GPP NR provides, together with their main benefits and\nchallenges. Special attention is paid to four key technology features, i.e.,\nmassive multiple-input multiple-output (MIMO), lean carrier design, and\nadvanced idle modes, together with the role of artificial intelligence\ncapabilities. We dive into their implementation and operational details, and\nthoroughly discuss their optimal operation points and theoretical-trade-offs\nfrom an energy consumption perspective. This will help the reader to grasp the\nfundamentals of -- and the status on -- green networking. Finally, the areas of\nresearch where more effort is needed to make future networks greener are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:02:40 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lopez-Perez", "David", ""], ["De Domenico", "Antonio", ""], ["Piovesan", "Nicola", ""], ["Baohongqiang", "Harvey", ""], ["Xinli", "Geng", ""], ["Qitao", "Song", ""], ["Debbah", "Merouane", ""]]}, {"id": "2101.11315", "submitter": "Siamak Layeghy", "authors": "Mohanad Sarhan, Siamak Layeghy, Marius Portmann", "title": "Towards a Standard Feature Set for Network Intrusion Detection System\n  Datasets", "comments": "13 pages, 4 figures, 13 tables. arXiv admin note: substantial text\n  overlap with arXiv:2011.09144", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network Intrusion Detection Systems (NIDSs) are important tools for the\nprotection of computer networks against increasingly frequent and sophisticated\ncyber attacks. Recently, a lot of research effort has been dedicated to the\ndevelopment of Machine Learning (ML) based NIDSs. As in any ML-based\napplication, the availability of high-quality datasets is critical for the\ntraining and evaluation of ML-based NIDS. One of the key problems with the\ncurrently available datasets is the lack of a standard feature set. The use of\na unique and proprietary set of features for each of the publicly available\ndatasets makes it virtually impossible to compare the performance of ML-based\ntraffic classifiers on different datasets, and hence to evaluate the ability of\nthese systems to generalise across different network scenarios. To address that\nlimitation, this paper proposes and evaluates standard NIDS feature sets based\non the NetFlow network meta-data collection protocol and system. We evaluate\nand compare two NetFlow-based feature set variants, a version with 12 features,\nand another one with 43 features.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:00:55 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 01:53:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sarhan", "Mohanad", ""], ["Layeghy", "Siamak", ""], ["Portmann", "Marius", ""]]}, {"id": "2101.11728", "submitter": "Abhishek Kumar Mishra", "authors": "Abhishek Kumar Mishra, Aline Carneiro Viana, Nadjib Achir", "title": "SimBle: Generating privacy preserving real-world BLE traces with ground\n  truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bluetooth has become critical as many IoT devices are arriving in the market.\nMost of the current literature focusing on Bluetooth simulation concentrates on\nthe network protocols' performances and completely neglects the privacy\nprotection recommendations introduced in the BLE standard. Indeed, privacy\nprotection is one of the main issues handled in the Bluetooth standard. For\ninstance, the current standard forces devices to change the identifier they\nembed within the public and private packets, known as MAC address\nrandomization. Although randomizing MAC addresses is intended to preserve\ndevice privacy, recent literature shows many challenges that are still present.\nOne of them is the correlation between the public packets and the emitters.\nUnfortunately, existing evaluation tools such as NS-3 are not designed to\nreproduce this Bluetooth standard's essential functionality. This makes it\nimpossible to test solutions for different device-fingerprinting strategies as\nthere is a lack of ground truth for large-scale scenarios with the majority of\ncurrent BLE devices implementing MAC address randomization. In this paper, we\nfirst introduce a solution of standard-compliant MAC address randomization in\nthe NS-3 framework, capable of emulating any real BLE device in the simulation\nand generating real-world Bluetooth traces. In addition, since the simulation\nrun-time for trace-collection grows exponentially with the number of devices,\nwe introduce an optimization to linearize public-packet sniffing. This made the\nlarge-scale trace-collection practically feasible. Then, we use the generated\ntraces and associated ground truth to do a case study on the evaluation of a\ngeneric MAC address association available in the literature. Our case study\nreveals that close to 90 percent of randomized addresses could be correctly\nlinked even in highly dense and mobile scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:32:43 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:52:57 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Mishra", "Abhishek Kumar", ""], ["Viana", "Aline Carneiro", ""], ["Achir", "Nadjib", ""]]}, {"id": "2101.11800", "submitter": "Sicong Liu", "authors": "Sicong Liu, Bin Guo, Ke Ma, Zhiwen Yu, Junzhao Du", "title": "AdaSpring: Context-adaptive and Runtime-evolutionary Deep Model\n  Compression for Mobile Applications", "comments": "Ubicomp 2021", "journal-ref": null, "doi": "10.1145/3448125", "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are many deep learning (e.g., DNN) powered mobile and wearable\napplications today continuously and unobtrusively sensing the ambient\nsurroundings to enhance all aspects of human lives. To enable robust and\nprivate mobile sensing, DNN tends to be deployed locally on the\nresource-constrained mobile devices via model compression. The current practice\neither hand-crafted DNN compression techniques, i.e., for optimizing\nDNN-relative performance (e.g., parameter size), or on-demand DNN compression\nmethods, i.e., for optimizing hardware-dependent metrics (e.g., latency),\ncannot be locally online because they require offline retraining to ensure\naccuracy. Also, none of them have correlated their efforts with runtime\nadaptive compression to consider the dynamic nature of the deployment context\nof mobile applications. To address those challenges, we present AdaSpring, a\ncontext-adaptive and self-evolutionary DNN compression framework. It enables\nthe runtime adaptive DNN compression locally online. Specifically, it presents\nthe ensemble training of a retraining-free and self-evolutionary network to\nintegrate multiple alternative DNN compression configurations (i.e., compressed\narchitectures and weights). It then introduces the runtime search strategy to\nquickly search for the most suitable compression configurations and evolve the\ncorresponding weights. With evaluation on five tasks across three platforms and\na real-world case study, experiment outcomes show that AdaSpring obtains up to\n3.1x latency reduction, 4.2 x energy efficiency improvement in DNNs, compared\nto hand-crafted compression techniques, while only incurring <= 6.2ms\nruntime-evolution latency.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 03:30:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Liu", "Sicong", ""], ["Guo", "Bin", ""], ["Ma", "Ke", ""], ["Yu", "Zhiwen", ""], ["Du", "Junzhao", ""]]}, {"id": "2101.11871", "submitter": "Pengwei Zhan", "authors": "Pengwei Zhan, Liming Wang, Yi Tang", "title": "Website Fingerprinting on Early QUIC Traffic", "comments": "30 pages, 7 figures, submitted to Elsevier Computer Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptographic protocols have been widely used to protect the user's privacy\nand avoid exposing private information. QUIC (Quick UDP Internet Connections),\nas an alternative to traditional HTTP, demonstrates its unique transmission\ncharacteristics: based on UDP for encrypted resource transmission, accelerating\nweb page rendering. However, existing encrypted transmission schemes based on\nTCP are vulnerable to website fingerprinting (WFP) attacks, allowing\nadversaries to infer the users' visited websites by eavesdropping on the\ntransmission channel. Whether QUIC protocol can effectively resisting to such\nattacks is worth investigating. In this work, we demonstrated the extreme\nvulnerability of QUIC under WFP attacks by comparing attack results under\nwell-designed conditions. We also study the transferability of features, which\nenable the adversary to use proven effective features on a special protocol\nattacking a new protocol. This study shows that QUIC is more vulnerable to WFP\nattacks than HTTPS in the early traffic scenario but is similar in the normal\nscenario. The maximum attack accuracy on QUIC is 56.8 % and 73 % higher than on\nHTTPS utilizing Simple features and Transfer features. The insecurity\ncharacteristic of QUIC explains the dramatic gap. We also find that features\nare transferable between protocols, and the feature importance is partially\ninherited on normal traffic due to the relatively fixed browser rendering\nsequence and the similar request-response model of protocols. However, the\ntransferability is inefficient when on early traffic, as QUIC and HTTPS show\nsignificantly different vulnerability when considering early traffic. We also\nshow that attack accuracy on QUIC could reach 95.4 % with only 40 packets and\njust using simple features, whereas only 60.7 % when on HTTPS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:53:51 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Zhan", "Pengwei", ""], ["Wang", "Liming", ""], ["Tang", "Yi", ""]]}, {"id": "2101.11984", "submitter": "George Papakostas Prof.", "authors": "V.N. Tsakalidou, P. Mitsou, G.A. Papakostas", "title": "Machine learning for cloud resources management -- An overview", "comments": "13 pages, 3 figures, to be published in proceedings of International\n  Conference on Expert Clouds and Applications (ICOECA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nowadays, an important topic that is considered a lot is how to integrate\nMachine Learning(ML) to cloud resources management. In this study, our goal is\nto explore the most important cloud resources management issues that have been\ncombined with ML and which present many promising results. To accomplish this,\nwe used chronological charts based on some keywords that we considered\nimportant and tried to answer the question: is ML suitable for resources\nmanagement problems in the cloud? Furthermore, a short discussion takes place\non the data that are available and the open challenges on it. A big collection\nof researches is used to make sensible comparisons between the ML techniques\nthat are used in the different kinds of cloud resources management fields and\nwe propose the most suitable ML model for each field. 1\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 13:23:00 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Tsakalidou", "V. N.", ""], ["Mitsou", "P.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2101.12036", "submitter": "Miroslav Bures", "authors": "Miroslav Bures, Bestoun S. Ahmed, Vaclav Rechtberger, Matej Klima,\n  Michal Trnka, Miroslav Jaros, Xavier Bellekens, Dani Almog, Pavel Herout", "title": "PatrIoT: IoT Automated Interoperability and Integration Testing\n  Framework", "comments": "Paper accepted at IEEE International Conference on Software Testing,\n  Verification and Validation 2021, Testing Tools Track (ICST 2021), virtual\n  conference, April 12-16, 2021.\n  htpps://icst2021.icmc.usp.br/track/icst-2021-Testing-Tool-Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of the contemporary Internet of Things (IoT) market,\nthe established systems raise a number of concerns regarding the reliability\nand the potential presence of critical integration defects. In this paper, we\npresent a PatrIoT framework that aims to provide flexible support to construct\nan effective IoT system testbed to implement automated interoperability and\nintegration testing. The framework allows scaling from a pure physical testbed\nto a simulated environment using a number of predefined modules and elements to\nsimulate an IoT device or part of the tested infrastructure. PatrIoT also\ncontains a set of reference example testbeds and several sets of example\nautomated tests for a smart street use case.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:42:45 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bures", "Miroslav", ""], ["Ahmed", "Bestoun S.", ""], ["Rechtberger", "Vaclav", ""], ["Klima", "Matej", ""], ["Trnka", "Michal", ""], ["Jaros", "Miroslav", ""], ["Bellekens", "Xavier", ""], ["Almog", "Dani", ""], ["Herout", "Pavel", ""]]}, {"id": "2101.12160", "submitter": "Daan Rutten", "authors": "Daan Rutten, Debankur Mukherjee", "title": "A New Approach to Capacity Scaling Augmented With Unreliable Machine\n  Learning Predictions", "comments": "47 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.NI cs.PF math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers suffer from immense power consumption. The erratic\nbehavior of internet traffic forces data centers to maintain excess capacity in\nthe form of idle servers in case the workload suddenly increases. As an idle\nserver still consumes a significant fraction of the peak energy, data center\noperators have heavily invested in capacity scaling solutions. In simple terms,\nthese aim to deactivate servers if the demand is low and to activate them again\nwhen the workload increases. To do so, an algorithm needs to strike a delicate\nbalance between power consumption, flow-time, and switching costs. Over the\nlast decade, the research community has developed competitive online algorithms\nwith worst-case guarantees. In the presence of historic data patterns,\nprescription from Machine Learning (ML) predictions typically outperform such\ncompetitive algorithms. This, however, comes at the cost of sacrificing the\nrobustness of performance, since unpredictable surges in the workload are not\nuncommon. The current work builds on the emerging paradigm of augmenting\nunreliable ML predictions with online algorithms to develop novel robust\nalgorithms that enjoy the benefits of both worlds.\n  We analyze a continuous-time model for capacity scaling, where the goal is to\nminimize the weighted sum of flow-time, switching cost, and power consumption\nin an online fashion. We propose a novel algorithm, called Adaptive Balanced\nCapacity Scaling (ABCS), that has access to black-box ML predictions, but is\ncompletely oblivious to the accuracy of these predictions. In particular, if\nthe predictions turn out to be accurate in hindsight, we prove that ABCS is\n$(1+\\varepsilon)$-competitive. Moreover, even when the predictions are\ninaccurate, ABCS guarantees a bounded competitive ratio. The performance of the\nABCS algorithm on a real-world dataset positively support the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:14:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rutten", "Daan", ""], ["Mukherjee", "Debankur", ""]]}, {"id": "2101.12328", "submitter": "Yong Niu", "authors": "Yibing Wang, Hao Wu, Yong Niu, Zhu Han, Bo Ai, Zhangdui Zhong", "title": "Coalition Game Based Full-duplex Popular Content Distribution in mmWave\n  Vehicular Networks", "comments": "12 pages, 8 figures, IEEE Transactions on Vehicular Technology", "journal-ref": "IEEE Transactions on Vehicular Technology, vol. 69, no. 11, pp.\n  13836-13848, Nov. 2020", "doi": "10.1109/TVT.2020.3028389", "report-no": null, "categories": "cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The millimeter wave (mmWave) communication has drawn intensive attention with\nabundant band resources. In this paper, we consider the popular content\ndistribution (PCD) problem in the mmWave vehicular network. In order to offload\nthe communication burden of base stations (BSs), vehicle-to-vehicle (V2V)\ncommunication is introduced into the PCD problem to transmit contents between\non-board units (OBUs) and improve the transmission efficiency. We propose a\nfull-duplex (FD) cooperative scheme based on coalition formation game, and the\nutility function is provided based on the maximization of the number of\nreceived contents. The contribution of each member in the coalition can be\ntransferable to its individual profit. While maximizing the number of received\ncontents in the fixed time, the cooperative scheme also ensures the individual\nprofit of each OBU in the coalition. We evaluate the proposed scheme by\nextensive simulations in mmWave vehicular networks. Compared with other\nexisting schemes, the proposed scheme has superior performances on the number\nof possessed contents and system fairness. Besides, the low complexity of the\nproposed algorithm is demonstrated by the switch operation number and CPU time.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 00:15:47 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wang", "Yibing", ""], ["Wu", "Hao", ""], ["Niu", "Yong", ""], ["Han", "Zhu", ""], ["Ai", "Bo", ""], ["Zhong", "Zhangdui", ""]]}, {"id": "2101.12375", "submitter": "Zehua Wang", "authors": "Xi Li, Zehua Wang, Victor C.M. Leung, Hong Ji, Yiming Liu, Heli Zhang", "title": "Blockchain-empowered Data-driven Networks: A Survey and Outlook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paths leading to future networks are pointing towards a data-driven\nparadigm to better cater to the explosive growth of mobile services as well as\nthe increasing heterogeneity of mobile devices, many of which generate and\nconsume large volumes and variety of data. These paths are also hampered by\nsignificant challenges in terms of security, privacy, services provisioning,\nand network management. Blockchain, which is a technology for building\ndistributed ledgers that provide an immutable log of transactions recorded in a\ndistributed network, has become prominent recently as the underlying technology\nof cryptocurrencies and is revolutionizing data storage and processing in\ncomputer network systems. For future data-driven networks (DDNs), blockchain is\nconsidered as a promising solution to enable the secure storage, sharing, and\nanalytics of data, privacy protection for users, robust, trustworthy network\ncontrol, and decentralized routing and resource managements. However, many\nimportant challenges and open issues remain to be addressed before blockchain\ncan be deployed widely to enable future DDNs. In this article, we present a\nsurvey on the existing research works on the application of blockchain\ntechnologies in computer networks, and identify challenges and potential\nsolutions in the applications of blockchains in future DDNs. We identify\napplication scenarios in which future blockchain-empowered DDNs could improve\nthe efficiency and security, and generally the effectiveness of network\nservices.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 03:18:05 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Li", "Xi", ""], ["Wang", "Zehua", ""], ["Leung", "Victor C. M.", ""], ["Ji", "Hong", ""], ["Liu", "Yiming", ""], ["Zhang", "Heli", ""]]}, {"id": "2101.12472", "submitter": "Shunpu Tang", "authors": "Shunpu Tang, Wenqi Zhou, Lunyuan Chen, Lijia Lai, Junjuan Xia and\n  Liseng Fan", "title": "Battery-constrained Federated Edge Learning in UAV-enabled IoT for\n  B5G/6G Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.phycom.2021.101381", "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study how to optimize the federated edge learning (FEEL) in\nUAV-enabled Internet of things (IoT) for B5G/6G networks, from a deep\nreinforcement learning (DRL) approach. The federated learning is an effective\nframework to train a shared model between decentralized edge devices or servers\nwithout exchanging raw data, which can help protect data privacy. In\nUAV-enabled IoT networks, latency and energy consumption are two important\nmetrics limiting the performance of FEEL. Although most of existing works have\nstudied how to reduce the latency and improve the energy efficiency, few works\nhave investigated the impact of limited batteries at the devices on the FEEL.\nMotivated by this, we study the battery-constrained FEEL, where the UAVs can\nadjust their operating CPU-frequency to prolong the battery life and avoid\nwithdrawing from federated learning training untimely. We optimize the system\nby jointly allocating the computational resource and wireless bandwidth in\ntime-varying environments. To solve this optimization problem, we employ a deep\ndeterministic policy gradient (DDPG) based strategy, where a linear combination\nof latency and energy consumption is used to evaluate the system cost.\nSimulation results are finally demonstrated to show that the proposed strategy\noutperforms the conventional ones. In particular, it enables all the devices to\ncomplete all rounds of FEEL with limited batteries and meanwhile reduce the\nsystem cost effectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:46:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Tang", "Shunpu", ""], ["Zhou", "Wenqi", ""], ["Chen", "Lunyuan", ""], ["Lai", "Lijia", ""], ["Xia", "Junjuan", ""], ["Fan", "Liseng", ""]]}, {"id": "2101.12475", "submitter": "Amin Shahraki", "authors": "Amin Shahraki, Mahmoud Abbasi, Md. Jalil Piran and Amir Taherkordi", "title": "A Comprehensive Survey on 6G Networks:Applications, Core Services,\n  Enabling Technologies, and Future Challenges", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cellular Internet of Things (IoT) is considered as de facto paradigm to\nimprove the communication and computation systems. Cellular IoT connects\nmassive number of physical and virtual objects to the Internet using cellular\nnetworks. The latest generation of cellular networks, e.g. fifth-generation\n(5G), use evolutionary and revolutionary technologies to notably improve the\nperformance of wireless networks. However, given the envisioned new use-cases,\ne.g., holographic communication, and the ever-increasing deployment of massive\nsmart-physical end-devices in IoT, the volume of network traffic has\nconsiderably raised, and therefore, the current generation of mobile networks\ncannot wholly meet the ever-increasing demands. Hence, it is envisioned that\nthe next generation, sixth generation (6G) networks, need to play a critical\nrole to alleviate such challenges in IoT by providing new communication\nservices, network capacity, and ultra-low latency communications (uRLLC). In\nthis paper, first, the need for 6G networks is discussed. Then, the potential\n6G requirements and trends, as well as the latest research activities related\nto 6G are introduced e.g., Tactile Internet and Terahertz (THz). Furthermore,\nthe key performance indicators, applications, new services, and the potential\nkey enabling technologies for 6G networks are presented. Finally, several\npotential unresolved challenges for future 6G networks are presented.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 08:56:59 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 05:21:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shahraki", "Amin", ""], ["Abbasi", "Mahmoud", ""], ["Piran", "Md. Jalil", ""], ["Taherkordi", "Amir", ""]]}, {"id": "2101.12512", "submitter": "Bj{\\o}rn Ivar Teigen", "authors": "Bj{\\o}rn Ivar Teigen, Neil Davies, Kai Olav Ellefsen, Tor Skeie, Jim\n  Torresen", "title": "A Model of WiFi Performance With Bounded Latency", "comments": "12 pages, 19 figures, Submitted for review to SIGCOMM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In September 2020, the Broadband Forum published a new industry standard for\nmeasuring network quality. The standard centers on the notion of quality\nattenuation. Quality attenuation is a measure of the distribution of latency\nand packet loss between two points connected by a network path. A vital feature\nof the quality attenuation idea is that we can express detailed application\nrequirements and network performance measurements in the same mathematical\nframework. Performance requirements and measurements are both modeled as\nlatency distributions. To the best of our knowledge, existing models of the\n802.11 WiFi protocol do not permit the calculation of complete latency\ndistributions without assuming steady-state operation. We present a novel model\nof the WiFi protocol. Instead of computing throughput numbers from a\nsteady-state analysis of a Markov chain, we explicitly model latency and packet\nloss. Explicitly modeling latency and loss allows for both transient and\nsteady-state analysis of latency distributions, and we can derive throughput\nnumbers from the latency results. Our model is, therefore, more general than\nthe standard Markov chain methods. We reproduce several known results with this\nmethod. Using transient analysis, we derive bounds on WiFi throughput under the\nrequirement that latency and packet loss must be bounded.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:44:19 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Teigen", "Bj\u00f8rn Ivar", ""], ["Davies", "Neil", ""], ["Ellefsen", "Kai Olav", ""], ["Skeie", "Tor", ""], ["Torresen", "Jim", ""]]}, {"id": "2101.12588", "submitter": "Tareq Si Salem", "authors": "Tareq Si Salem, Giovanni Neglia and Stratis Ioannidis", "title": "No-Regret Caching via Online Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an online caching problem in which requests can be served by a local\ncache to avoid retrieval costs from a remote server. The cache can update its\nstate after a batch of requests and store an arbitrarily small fraction of each\ncontent. We study no-regret algorithms based on Online Mirror Descent (OMD)\nstrategies. We show that the optimal OMD strategy depends on the request\ndiversity present in a batch. We also prove that, when the cache must store the\nentire content, rather than a fraction, OMD strategies can be coupled with a\nrandomized rounding scheme that preserves regret guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 13:56:51 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 08:14:49 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2021 10:52:42 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Salem", "Tareq Si", ""], ["Neglia", "Giovanni", ""], ["Ioannidis", "Stratis", ""]]}, {"id": "2101.12644", "submitter": "Matteo Nerini", "authors": "Matteo Nerini, David Palma", "title": "5G Network Slicing for Wi-Fi Networks", "comments": "9 pages, 8 figures, to be published in the 17th IFIP/IEEE\n  International Symposium on Integrated Network Management (IM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Future networks will pave the way for a myriad of applications with different\nrequirements and Wi-Fi will play an important role in local area networks. This\nis why network slicing is proposed by 5G networks, allowing to offer multiple\nlogical networks tailored to the different user requirements, over a common\ninfrastructure. However, this is not supported by current Wi-Fi networks. In\nthis paper, we propose a standard-compliant network slicing approach for the\nradio access segment of Wi-Fi by defining multiple Service Set Identifiers\n(SSIDs) per Access Point (AP). We present two algorithms, one that assigns\nresources according to the requirements of slices in a static way, and another\nthat dynamically configures the slices according to the network's conditions\nand relevant Key Performance Indicators (KPIs). The proposed algorithms were\nvalidated through extensive simulations, conducted in the ns-3 network\nsimulator, and complemented by theoretical assessments. The obtained results\nreveal that the two proposed slicing approaches outperform today's Wi-Fi access\ntechnique, reaching lower error probability for bandwidth intensive slices and\nlower latency for time-critical slices. Simultaneously, the proposed approach\nis up to 32 times more energy efficient, when considering slices tailored for\nlow-power and low-bandwidth devices, while increasing the overall spectrum\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:35:31 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Nerini", "Matteo", ""], ["Palma", "David", ""]]}, {"id": "2101.12649", "submitter": "Xiaodan Song", "authors": "Guangming Shi, Dahua Gao, Xiaodan Song, Jingxuan Chai, Minxi Yang,\n  Xuemei Xie, Leida Li and Xuyang Li", "title": "A new communication paradigm: from bit accuracy to semantic fidelity", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless communication has achieved great success in the past several\ndecades. The challenge is of improving bandwidth with limited spectrum and\npower consumption, which however has gradually become a bottleneck with\nevolution going on. The intrinsic problem is that communication is modeled as a\nmessage transportation from sender to receiver and pursues for an exact message\nreplication in Shannon's information theory, which certainly leads to large\nbandwidth and power requirements with data explosion. However, the goal for\ncommunication among intelligent agents, entities with intelligence including\nhumans, is to understand the meaning or semantics underlying data, not an\naccurate recovery of the transmitted messages. The separate first transmission\nand then understanding is a waste on bandwidth. In this article, we deploy\nsemantics to solve the spectrum and power bottleneck and propose a first\nunderstanding and then transmission framework with high semantic fidelity. We\nfirst give a brief introduction of semantics covering the definition and\nproperties to show the insights and scope of this paper. Then the proposed\ncommunication towards semantic fidelity framework is introduced, which takes\nthe above mentioned properties into account to further improve efficiency.\nSpecially, a semantic transformation is introduced to transform the input into\nsemantic symbols. Different from the conventional transformations in signal\nprocessing area, for example discrete cosine transform, the transformation is\nwith data loss, which is also the reason that the proposed framework can\nachieve large bandwidth saving with high semantic fidelity. Besides, we also\ndiscuss semantic noise and performance measurement. To evaluate the\neffectiveness, a case study of audio transmission is carried out. Finally, we\ndiscuss the typical applications and open challenges.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:41:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Shi", "Guangming", ""], ["Gao", "Dahua", ""], ["Song", "Xiaodan", ""], ["Chai", "Jingxuan", ""], ["Yang", "Minxi", ""], ["Xie", "Xuemei", ""], ["Li", "Leida", ""], ["Li", "Xuyang", ""]]}, {"id": "2101.12691", "submitter": "Tao Wang", "authors": "Tao Wang, Xiangrui Yang, Gianni Antichi, Anirudh Sivaraman, Aurojit\n  Panda", "title": "Isolation mechanisms for high-speed packet-processing pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-plane programmability is now mainstream, both in the form of\nprogrammable switches and smart network-interface cards (SmartNICs). As the\nnumber of use cases for programmable network devices grows, each device will\nneed to support multiple packet-processing modules simultaneously. These\nmodules are likely to be independently developed, e.g., measurement and\nsecurity modules developed by different teams, or cloud tenants offloading\npacket processing to a NIC. Hence, we need isolation mechanisms to ensure that\nmodules on the same device do not interfere with each other. This paper\npresents a system, Menshen, for inter-module isolation on programmable\npacket-processing pipelines similar to the RMT/PISA architecture. Menshen\nconsists of a set of lightweight hardware primitives that can be added to an\nRMT pipeline and a compiler to take advantage of these primitives. We prototype\nthe Menshen hardware using the NetFPGA switch and Corundum FPGA NIC platforms\nand the Menshen software using the open-source P4-16 reference compiler. We\nshow that Menshen supports multiple modules simultaneously, allows one module\nto be quickly updated without disrupting other modules, and consumes a modest\namount of additional hardware resources relative to an RMT pipeline. We have\nopen sourced the code for Menshen's hardware and software at\nhttps://github.com/anonymous-submission-855. Although we do not have an ASIC\nimplementation of Menshen, we believe its primitives are simple enough that\nthey can be added to an ASIC realization of RMT as well.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:21:27 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 20:48:20 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Wang", "Tao", ""], ["Yang", "Xiangrui", ""], ["Antichi", "Gianni", ""], ["Sivaraman", "Anirudh", ""], ["Panda", "Aurojit", ""]]}, {"id": "2101.12704", "submitter": "Hong Xing", "authors": "Hong Xing and Osvaldo Simeone and Suzhi Bi", "title": "Federated Learning over Wireless Device-to-Device Networks: Algorithms\n  and Convergence Analysis", "comments": "41 pages, 10 figures, submitted for possible journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.NI eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of Internet-of-Things (IoT) devices and cloud-computing\napplications over siloed data centers is motivating renewed interest in the\ncollaborative training of a shared model by multiple individual clients via\nfederated learning (FL). To improve the communication efficiency of FL\nimplementations in wireless systems, recent works have proposed compression and\ndimension reduction mechanisms, along with digital and analog transmission\nschemes that account for channel noise, fading, and interference. This prior\nart has mainly focused on star topologies consisting of distributed clients and\na central server. In contrast, this paper studies FL over wireless\ndevice-to-device (D2D) networks by providing theoretical insights into the\nperformance of digital and analog implementations of decentralized stochastic\ngradient descent (DSGD). First, we introduce generic digital and analog\nwireless implementations of communication-efficient DSGD algorithms, leveraging\nrandom linear coding (RLC) for compression and over-the-air computation\n(AirComp) for simultaneous analog transmissions. Next, under the assumptions of\nconvexity and connectivity, we provide convergence bounds for both\nimplementations. The results demonstrate the dependence of the optimality gap\non the connectivity and on the signal-to-noise ratio (SNR) levels in the\nnetwork. The analysis is corroborated by experiments on an image-classification\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 17:42:26 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Xing", "Hong", ""], ["Simeone", "Osvaldo", ""], ["Bi", "Suzhi", ""]]}]