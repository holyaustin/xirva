[{"id": "1804.00705", "submitter": "Soramichi Akiyama", "authors": "Shinsuke Hamada, Soramichi Akiyama, Mitaro Namiki", "title": "Reactive NaN Repair for Applying Approximate Memory to Numerical\n  Applications", "comments": "Presented in the 8th Workshop on Systems for Multi-core and\n  Heterogeneous Architectures (SFMA), co-located with EuroSys'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.ET cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in the AI and HPC fields require much memory capacity, and the\namount of energy consumed by main memory of server machines is ever increasing.\nEnergy consumption of main memory can be greatly reduced by applying\napproximate computing in exchange for increased bit error rates. AI and HPC\napplications are to some extent robust to bit errors because small numerical\nerrors are amortized by their iterative nature. However, a single occurrence of\na NaN due to bit-flips corrupts the whole calculation result. The issue is that\nfixing every bit-flip using ECC incurs too much overhead because the bit error\nrate is much higher than in normal environments. We propose a low-overhead\nmethod to fix NaNs when approximate computing is applied to main memory. The\nmain idea is to reactively repair NaNs while leaving other non-fatal numerical\nerrors as-is to reduce the overhead. We implemented a prototype by leveraging\nfloating-point exceptions of x86 CPUs, and the preliminary evaluations showed\nthat our method incurs negligible overhead.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 03:52:05 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Hamada", "Shinsuke", ""], ["Akiyama", "Soramichi", ""], ["Namiki", "Mitaro", ""]]}, {"id": "1804.01226", "submitter": "Hongyu Liu", "authors": "Hongyu Liu, Sam Silvestro, Wei Wang, Chen Tian, and Tongping Liu", "title": "iReplayer: In-situ and Identical Record-and-Replay for Multithreaded\n  Applications", "comments": "16 pages, 5 figures, to be published at PLDI'18", "journal-ref": null, "doi": "10.1145/3192366.3192380", "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducing executions of multithreaded programs is very challenging due to\nmany intrinsic and external non-deterministic factors. Existing RnR systems\nachieve significant progress in terms of performance overhead, but none targets\nthe in-situ setting, in which replay occurs within the same process as the\nrecording process. Also, most existing work cannot achieve identical replay,\nwhich may prevent the reproduction of some errors.\n  This paper presents iReplayer, which aims to identically replay multithreaded\nprograms in the original process (under the \"in-situ\" setting). The novel\nin-situ and identical replay of iReplayer makes it more likely to reproduce\nerrors, and allows it to directly employ debugging mechanisms (e.g.\nwatchpoints) to aid failure diagnosis. Currently, iReplayer only incurs 3%\nperformance overhead on average, which allows it to be always enabled in the\nproduction environment. iReplayer enables a range of possibilities, and this\npaper presents three examples: two automatic tools for detecting buffer\noverflows and use-after-free bugs, and one interactive debugging tool that is\nintegrated with GDB.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 03:26:13 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Liu", "Hongyu", ""], ["Silvestro", "Sam", ""], ["Wang", "Wei", ""], ["Tian", "Chen", ""], ["Liu", "Tongping", ""]]}, {"id": "1804.11265", "submitter": "Rachata Ausavarungnirun", "authors": "Rachata Ausavarungnirun, Joshua Landgraf, Vance Miller, Saugata Ghose,\n  Jayneel Gandhi, Christopher J. Rossbach, Onur Mutlu", "title": "Mosaic: An Application-Transparent Hardware-Software Cooperative Memory\n  Manager for GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern GPUs face a trade-off on how the page size used for memory management\naffects address translation and demand paging. Support for multiple page sizes\ncan help relax the page size trade-off so that address translation and demand\npaging optimizations work together synergistically. However, existing page\ncoalescing and splintering policies require costly base page migrations that\nundermine the benefits multiple page sizes provide. In this paper, we observe\nthat GPGPU applications present an opportunity to support multiple page sizes\nwithout costly data migration, as the applications perform most of their memory\nallocation en masse (i.e., they allocate a large number of base pages at once).\nWe show that this en masse allocation allows us to create intelligent memory\nallocation policies which ensure that base pages that are contiguous in virtual\nmemory are allocated to contiguous physical memory pages. As a result,\ncoalescing and splintering operations no longer need to migrate base pages.\n  We introduce Mosaic, a GPU memory manager that provides\napplication-transparent support for multiple page sizes. Mosaic uses base pages\nto transfer data over the system I/O bus, and allocates physical memory in a\nway that (1) preserves base page contiguity and (2) ensures that a large page\nframe contains pages from only a single memory protection domain. This\nmechanism allows the TLB to use large pages, reducing address translation\noverhead. During data transfer, this mechanism enables the GPU to transfer only\nthe base pages that are needed by the application over the system I/O bus,\nkeeping demand paging overhead low.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 15:08:54 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Ausavarungnirun", "Rachata", ""], ["Landgraf", "Joshua", ""], ["Miller", "Vance", ""], ["Ghose", "Saugata", ""], ["Gandhi", "Jayneel", ""], ["Rossbach", "Christopher J.", ""], ["Mutlu", "Onur", ""]]}]