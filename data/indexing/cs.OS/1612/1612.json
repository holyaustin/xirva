[{"id": "1612.00445", "submitter": "Javier Picorel", "authors": "Javier Picorel, Djordje Jevdjic and Babak Falsafi", "title": "Near-Memory Address Translation", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory and logic integration on the same chip is becoming increasingly cost\neffective, creating the opportunity to offload data-intensive functionality to\nprocessing units placed inside memory chips. The introduction of memory-side\nprocessing units (MPUs) into conventional systems faces virtual memory as the\nfirst big showstopper: without efficient hardware support for address\ntranslation MPUs have highly limited applicability. Unfortunately, conventional\ntranslation mechanisms fall short of providing fast translations as\ncontemporary memories exceed the reach of TLBs, making expensive page walks\ncommon.\n  In this paper, we are the first to show that the historically important\nflexibility to map any virtual page to any page frame is unnecessary in today's\nservers. We find that while limiting the associativity of the\nvirtual-to-physical mapping incurs no penalty, it can break the\ntranslate-then-fetch serialization if combined with careful data placement in\nthe MPU's memory, allowing for translation and data fetch to proceed\nindependently and in parallel. We propose the Distributed Inverted Page Table\n(DIPTA), a near-memory structure in which the smallest memory partition keeps\nthe translation information for its data share, ensuring that the translation\ncompletes together with the data fetch. DIPTA completely eliminates the\nperformance overhead of translation, achieving speedups of up to 3.81x and\n2.13x over conventional translation using 4KB and 1GB pages respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 17:45:18 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 20:09:02 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Picorel", "Javier", ""], ["Jevdjic", "Djordje", ""], ["Falsafi", "Babak", ""]]}, {"id": "1612.06830", "submitter": "Carl Nettelblad", "authors": "Jessica Nettelblad, Carl Nettelblad", "title": "CannyFS: Opportunistically Maximizing I/O Throughput Exploiting the\n  Transactional Nature of Batch-Mode Data Processing", "comments": "8 pages, 3 figures, 1 table. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a user mode file system, CannyFS, that hides latency by assuming\nall I/O operations will succeed. The user mode process will in turn report\nerrors, allowing proper cleanup and a repeated attempt to take place. We\ndemonstrate benefits for the model tasks of extracting archives and removing\ndirectory trees in a real-life HPC environment, giving typical reductions in\ntime use of over 80%.\n  This approach can be considered a view of HPC jobs and their I/O activity as\ntransactions. In general, file systems lack clearly defined transaction\nsemantics. Over time, the competing trends to add cache and maintain data\nintegrity have resulted in different practical tradeoffs.\n  High-performance computing is a special case where overall throughput demands\nare high. Latency can also be high, with non-local storage. In addition, a\ntheoretically possible I/O error (like permission denied, loss of connection,\nexceeding disk quota) will frequently warrant the resubmission of a full job or\ntask, rather than traditional error reporting or handling. Therefore,\nopportunistically treating each I/O operation as successful, and part of a\nlarger transaction, can speed up some applications that do not leverage\nasynchronous I/O.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 20:14:35 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Nettelblad", "Jessica", ""], ["Nettelblad", "Carl", ""]]}]