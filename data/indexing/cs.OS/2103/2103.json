[{"id": "2103.07092", "submitter": "Joel Mandebi Mbongue", "authors": "Joel Mandebi Mbongue, Danielle Tchuinkou Kwadjo, Christophe Bobda", "title": "Performance Exploration of Virtualization Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtualization has gained astonishing popularity in recent decades. It is\napplied in several application domains, including mainframes, personal\ncomputers, data centers, and embedded systems. While the benefits of\nvirtualization are no longer to be demonstrated, it often comes at the price of\nperformance degradation compared to native execution. In this work, we conduct\na comparative study on the performance outcome of VMWare, KVM, and Docker\nagainst compute-intensive, IO-intensive, and system benchmarks. The experiments\nreveal that containers are the way-to-go for the fast execution of\napplications. It also shows that VMWare and KVM perform similarly on most of\nthe benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 05:14:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mbongue", "Joel Mandebi", ""], ["Kwadjo", "Danielle Tchuinkou", ""], ["Bobda", "Christophe", ""]]}, {"id": "2103.10779", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Aravinda Prasad, Smruti R. Sarangi, Sreenivas\n  Subramoney", "title": "Page Table Management for Heterogeneous Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern enterprise servers are increasingly embracing tiered memory systems\nwith a combination of low latency DRAMs and large capacity but high latency\nnon-volatile main memories (NVMMs) such as Intel's Optane DC PMM. Prior works\nhave focused on efficient placement and migration of data on a tiered memory\nsystem, but have not studied the optimal placement of page tables.\n  Explicit and efficient placement of page tables is crucial for large memory\nfootprint applications with high TLB miss rates because they incur dramatically\nhigher page walk latency when page table pages are placed in NVMM. We show that\n(i) page table pages can end up on NVMM even when enough DRAM memory is\navailable and (ii) page table pages that spill over to NVMM due to DRAM memory\npressure are not migrated back later when memory is available in DRAM.\n  We study the performance impact of page table placement in a tiered memory\nsystem and propose an efficient and transparent page table management technique\nthat (i) applies different placement policies for data and page table pages,\n(ii) introduces a differentiating policy for page table pages by placing a\nsmall but critical part of the page table in DRAM, and (iii) dynamically and\njudiciously manages the rest of the page table by transparently migrating the\npage table pages between DRAM and NVMM. Our implementation on a real system\nequipped with Intel's Optane NVMM running Linux reduces the page table walk\ncycles by 12% and total cycles by 20% on an average. This improves the runtime\nby 20% on an average for a set of synthetic and real-world large memory\nfootprint applications when compared with various default Linux kernel\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:46:59 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kumar", "Sandeep", ""], ["Prasad", "Aravinda", ""], ["Sarangi", "Smruti R.", ""], ["Subramoney", "Sreenivas", ""]]}, {"id": "2103.14951", "submitter": "Jose Martins", "authors": "Bruno S\\'a, Jos\\'e Martins, Sandro Pinto", "title": "A First Look at RISC-V Virtualization from an Embedded Systems\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the first public implementation and evaluation of the\nlatest version of the RISC-V hypervisor extension (H-extension v0.6.1)\nspecification in a Rocket chip core. To perform a meaningful evaluation for\nmodern multi-core embedded and mixedcriticality systems, we have ported Bao, an\nopen-source static partitioning hypervisor, to RISC-V. We have also extended\nthe RISC-V platformlevel interrupt controller (PLIC) to enable direct guest\ninterrupt injection with low and deterministic latency and we have enhanced the\ntimer infrastructure to avoid trap and emulation overheads. Experiments were\ncarried out in FireSim, a cycle-accurate, FPGA-accelerated simulator, and the\nsystem was also successfully deployed and tested in a Zynq UltraScale+ MPSoC\nZCU104. Our hardware implementation was opensourced and is currently in use by\nthe RISC-V community towards the ratification of the H-extension specification.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:44:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["S\u00e1", "Bruno", ""], ["Martins", "Jos\u00e9", ""], ["Pinto", "Sandro", ""]]}]