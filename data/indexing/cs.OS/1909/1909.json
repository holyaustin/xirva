[{"id": "1909.00805", "submitter": "Liu Yimeng", "authors": "Yimeng Liu, Zhiwen Yu, Bin Guo, Qi Han, Jiangbin Su, Jiahao Liao", "title": "CrowdOS: A Ubiquitous Operating System for Crowdsourcing and Mobile\n  Crowd Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.OS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of crowdsourcing and mobile crowdsensing techniques, a large\nnumber of crowdsourcing applications or platforms (CAP) have appeared. In the\nmean time, CAP-related models and frameworks based on different research\nhypotheses are rapidly emerging, and they usually address specific issues from\na certain perspective. Due to different settings and conditions, different\nmodels are not compatible with each other. However, CAP urgently needs to\ncombine these techniques to form a unified framework. In addition, these models\nneeds to be learned and updated online with the extension of crowdsourced data\nand task types, thus requiring a unified architecture that integrates lifelong\nlearning concepts and breaks down the barriers between different modules. This\npaper draws on the idea of ubiquitous operating systems and proposes a novel OS\n(CrowdOS), which is an abstract software layer running between native OS and\napplication layer. In particular, based on an in-depth analysis of the complex\ncrowd environment and diverse characteristics of heterogeneous tasks, we\nconstruct the OS kernel and three core frameworks including Task Resolution and\nAssignment Framework (TRAF), Integrated Resource Management (IRM), and Task\nResult quality Optimization (TRO). In addition, we validate the usability of\nCrowdOS, module correctness and development efficiency. Our evaluation further\nreveals TRO brings enormous improvement in efficiency and a reduction in energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 17:28:05 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Liu", "Yimeng", ""], ["Yu", "Zhiwen", ""], ["Guo", "Bin", ""], ["Han", "Qi", ""], ["Su", "Jiangbin", ""], ["Liao", "Jiahao", ""]]}, {"id": "1909.01843", "submitter": "Anup Das", "authors": "Adarsha Balaji, Anup Das, Yuefeng Wu, Khanh Huynh, Francesco\n  Dell'Anna, Giacomo Indiveri, Jeffrey L. Krichmar, Nikil Dutt, Siebren\n  Schaafsma, and Francky Catthoor", "title": "Mapping Spiking Neural Networks to Neuromorphic Hardware", "comments": "14 pages, 14 images, 69 references, Accepted in IEEE Transactions on\n  Very Large Scale Integration (VLSI) Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.LG cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphic hardware platforms implement biological neurons and synapses to\nexecute spiking neural networks (SNNs) in an energy-efficient manner. We\npresent SpiNeMap, a design methodology to map SNNs to crossbar-based\nneuromorphic hardware, minimizing spike latency and energy consumption.\nSpiNeMap operates in two steps: SpiNeCluster and SpiNePlacer. SpiNeCluster is a\nheuristic-based clustering technique to partition SNNs into clusters of\nsynapses, where intracluster local synapses are mapped within crossbars of the\nhardware and inter-cluster global synapses are mapped to the shared\ninterconnect. SpiNeCluster minimizes the number of spikes on global synapses,\nwhich reduces spike congestion on the shared interconnect, improving\napplication performance. SpiNePlacer then finds the best placement of local and\nglobal synapses on the hardware using a meta-heuristic-based approach to\nminimize energy consumption and spike latency. We evaluate SpiNeMap using\nsynthetic and realistic SNNs on the DynapSE neuromorphic hardware. We show that\nSpiNeMap reduces average energy consumption by 45% and average spike latency by\n21%, compared to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:39:47 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Balaji", "Adarsha", ""], ["Das", "Anup", ""], ["Wu", "Yuefeng", ""], ["Huynh", "Khanh", ""], ["Dell'Anna", "Francesco", ""], ["Indiveri", "Giacomo", ""], ["Krichmar", "Jeffrey L.", ""], ["Dutt", "Nikil", ""], ["Schaafsma", "Siebren", ""], ["Catthoor", "Francky", ""]]}, {"id": "1909.04658", "submitter": "Jie Gao", "authors": "Jie Gao, Lian Zhao, Xuemin (Sherman) Shen", "title": "The Study of Dynamic Caching via State Transition Field -- the Case of\n  Time-Invariant Popularity", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This two-part paper investigates cache replacement schemes with the objective\nof developing a general model to unify the analysis of various replacement\nschemes and illustrate their features. To achieve this goal, we study the\ndynamic process of caching in the vector space and introduce the concept of\nstate transition field (STF) to model and characterize replacement schemes. In\nthe first part of this work, we consider the case of time-invariant content\npopularity based on the independent reference model (IRM). In such case, we\ndemonstrate that the resulting STFs are static, and each replacement scheme\nleads to a unique STF. The STF determines the expected trace of the dynamic\nchange in the cache state distribution, as a result of content requests and\nreplacements, from any initial point. Moreover, given the replacement scheme,\nthe STF is only determined by the content popularity. Using four example\nschemes including random replacement (RR) and least recently used (LRU), we\nshow that the STF can be used to analyze replacement schemes such as finding\ntheir steady states, highlighting their differences, and revealing insights\nregarding the impact of knowledge of content popularity. Based on the above\nresults, STF is shown to be useful for characterizing and illustrating\nreplacement schemes. Extensive numeric results are presented to demonstrate\nanalytical STFs and STFs from simulations for the considered example\nreplacement schemes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:49:00 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Gao", "Jie", "", "Sherman"], ["Zhao", "Lian", "", "Sherman"], ["Xuemin", "", "", "Sherman"], ["Shen", "", ""]]}, {"id": "1909.04659", "submitter": "Jie Gao", "authors": "Jie Gao, Lian Zhao, Xuemin (Sherman) Shen", "title": "The Study of Dynamic Caching via State Transition Field -- the Case of\n  Time-Varying Popularity", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the second part of this two-part paper, we extend the study of dynamic\ncaching via state transition field (STF) to the case of time-varying content\npopularity. The objective of this part is to investigate the impact of\ntime-varying content popularity on the STF and how such impact accumulates to\naffect the performance of a replacement scheme. Unlike the case in the first\npart, the STF is no longer static over time, and we introduce instantaneous STF\nto model it. Moreover, we demonstrate that many metrics, such as instantaneous\nstate caching probability and average cache hit probability over an arbitrary\nsequence of requests, can be found using the instantaneous STF. As a steady\nstate may not exist under time-varying content popularity, we characterize the\nperformance of replacement schemes based on how the instantaneous STF of a\nreplacement scheme after a content request impacts on its cache hit probability\nat the next request. From this characterization, insights regarding the\nrelations between the pattern of change in the content popularity, the\nknowledge of content popularity exploited by the replacement schemes, and the\neffectiveness of these schemes under time-varying popularity are revealed. In\nthe simulations, different patterns of time-varying popularity, including the\nshot noise model, are experimented. The effectiveness of example replacement\nschemes under time-varying popularity is demonstrated, and the numerical\nresults support the observations from the analytic results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:59:45 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Gao", "Jie", "", "Sherman"], ["Zhao", "Lian", "", "Sherman"], ["Xuemin", "", "", "Sherman"], ["Shen", "", ""]]}, {"id": "1909.05349", "submitter": "Ayoosh Bansal", "authors": "Ayoosh Bansal, Jayati Singh, Yifan Hao, Jen-Yang Wen, Renato Mancuso,\n  and Marco Caccamo", "title": "Cache Where you Want! Reconciling Predictability and Coherent Caching", "comments": "13 pages, 10 figures, v2 update includes overview section with formal\n  solution definition. This is a long version of a prior publication", "journal-ref": "2020 9th Mediterranean Conference on Embedded Computing (MECO),\n  2020, pp. 1-6", "doi": "10.1109/MECO49872.2020.9134262", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time and cyber-physical systems need to interact with and respond to\ntheir physical environment in a predictable time. While multicore platforms\nprovide incredible computational power and throughput, they also introduce new\nsources of unpredictability. Large fluctuations in latency to access data\nshared between multiple cores is an important contributor to the overall\nexecution-time variability. In addition to the temporal unpredictability\nintroduced by caching, parallel applications with data shared across multiple\ncores also pay additional latency overheads due to data coherence. Analyzing\nthe impact of data coherence on the worst-case execution-time of real-time\napplications is challenging because only scarce implementation details are\nrevealed by manufacturers. This paper presents application level control for\ncaching data at different levels of the cache hierarchy. The rationale is that\nby caching data only in shared cache it is possible to bypass private caches.\nThe access latency to data present in caches becomes independent of its\ncoherence state. We discuss the existing architectural support as well as the\nrequired hardware and OS modifications to support the proposed cacheability\ncontrol. We evaluate the system on an architectural simulator. We show that the\nworst case execution time for a single memory write request is reduced by 52%.\nBenchmark evaluations show that proposed technique has a minimal impact on\naverage performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 20:47:58 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 23:34:25 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bansal", "Ayoosh", ""], ["Singh", "Jayati", ""], ["Hao", "Yifan", ""], ["Wen", "Jen-Yang", ""], ["Mancuso", "Renato", ""], ["Caccamo", "Marco", ""]]}, {"id": "1909.07820", "submitter": "Sisheng Liang Liang", "authors": "Sisheng Liang, Zhou Yang, Fang Jin, Yong Chen", "title": "Data Centers Job Scheduling with Deep Reinforcement Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient job scheduling on data centers under heterogeneous complexity is\ncrucial but challenging since it involves the allocation of multi-dimensional\nresources over time and space. To adapt the complex computing environment in\ndata centers, we proposed an innovative Advantage Actor-Critic (A2C) deep\nreinforcement learning based approach called A2cScheduler for job scheduling.\nA2cScheduler consists of two agents, one of which, dubbed the actor, is\nresponsible for learning the scheduling policy automatically and the other one,\nthe critic, reduces the estimation error. Unlike previous policy gradient\napproaches, A2cScheduler is designed to reduce the gradient estimation variance\nand to update parameters efficiently. We show that the A2cScheduler can achieve\ncompetitive scheduling performance using both simulated workloads and real data\ncollected from an academic data center.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 02:09:58 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 20:05:26 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liang", "Sisheng", ""], ["Yang", "Zhou", ""], ["Jin", "Fang", ""], ["Chen", "Yong", ""]]}, {"id": "1909.09294", "submitter": "Christopher Jelesnianski", "authors": "Christopher Jelesnianski, Jinwoo Yom, Changwoo Min, Yeongjin Jang", "title": "Making Code Re-randomization Practical with MARDU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defense techniques such as Data Execution Prevention (DEP) and Address Space\nLayout Randomization (ASLR) were the early role models preventing primitive\ncode injection and return-oriented programming (ROP) attacks. Notably, these\ntechniques did so in an elegant and utilitarian manner, keeping performance and\nscalability in the forefront, making them one of the few widely-adopted defense\ntechniques. As code re-use has evolved in complexity from JIT-ROP, to BROP and\ndata-only attacks, defense techniques seem to have tunneled on defending at all\ncosts, losing-their-way in pragmatic defense design. Some fail to provide\ncomprehensive coverage, being too narrow in scope, while others provide\nunrealistic overheads leaving users willing to take their chances to maintain\nperformance expectations.\n  We present Mardu, an on-demand system-wide re-randomization technique that\nimproves re-randomization and refocuses efforts to simultaneously embrace key\ncharacteristics of defense techniques: security, performance, and scalability.\nOur code sharing with diversification is achieved by implementing reactive and\nscalable, rather than continuous or one-time diversification while the use of\nhardware supported eXecute-only Memory (XoM) and shadow stack prevent memory\ndisclosure; entwining and enabling code sharing further minimizes needed\ntracking, patching costs, and memory overhead. Mardu's evaluation shows\nperformance and scalability to have low average overhead in both\ncompute-intensive (5.5% on SPEC) and real-world applications (4.4% on NGINX).\nWith this design, Mardu demonstrates that strong and scalable security\nguarantees are possible to achieve at a practical cost to encourage deployment.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 02:08:24 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Jelesnianski", "Christopher", ""], ["Yom", "Jinwoo", ""], ["Min", "Changwoo", ""], ["Jang", "Yeongjin", ""]]}, {"id": "1909.09600", "submitter": "Bj\\\"orn Brandenburg", "authors": "Bj\\\"orn B. Brandenburg", "title": "Multiprocessor Real-Time Locking Protocols: A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We systematically survey the literature on analytically sound multiprocessor\nreal-time locking protocols from 1988 until 2018, covering the following\ntopics: progress mechanisms that prevent the lock-holder preemption problem,\nspin-lock protocols, binary semaphore protocols, independence-preserving (or\nfully preemptive) locking protocols, reader-writer and k-exclusion\nsynchronization, support for nested critical sections, and implementation and\nsystem-integration aspects. A special focus is placed on the\nsuspension-oblivious and suspension-aware analysis approaches for semaphore\nprotocols, their respective notions of priority inversion, optimality criteria,\nlower bounds on maximum priority-inversion blocking, and matching\nasymptotically optimal locking protocols.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 16:26:25 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Brandenburg", "Bj\u00f6rn B.", ""]]}, {"id": "1909.10123", "submitter": "Rohan Kadekodi", "authors": "Rohan Kadekodi, Se Kwon Lee, Sanidhya Kashyap, Taesoo Kim, Aasheesh\n  Kolli, Vijay Chidambaram", "title": "SplitFS: Reducing Software Overhead in File Systems for Persistent\n  Memory", "comments": null, "journal-ref": null, "doi": "10.1145/3341301.3359631", "report-no": null, "categories": "cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SplitFS, a file system for persistent memory (PM) that reduces\nsoftware overhead significantly compared to state-of-the-art PM file systems.\nSplitFS presents a novel split of responsibilities between a user-space library\nfile system and an existing kernel PM file system. The user-space library file\nsystem handles data operations by intercepting POSIX calls, memory-mapping the\nunderlying file, and serving the read and overwrites using processor loads and\nstores. Metadata operations are handled by the kernel PM file system (ext4\nDAX). SplitFS introduces a new primitive termed relink to efficiently support\nfile appends and atomic data operations. SplitFS provides three consistency\nmodes, which different applications can choose from, without interfering with\neach other. SplitFS reduces software overhead by up-to 4x compared to the NOVA\nPM file system, and 17x compared to ext4-DAX. On a number of micro-benchmarks\nand applications such as the LevelDB key-value store running the YCSB\nbenchmark, SplitFS increases application performance by up to 2x compared to\next4 DAX and NOVA while providing similar consistency guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 02:03:13 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Kadekodi", "Rohan", ""], ["Lee", "Se Kwon", ""], ["Kashyap", "Sanidhya", ""], ["Kim", "Taesoo", ""], ["Kolli", "Aasheesh", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "1909.10377", "submitter": "Shesha Sreenivasamurthy", "authors": "Shesha Sreenivasamurthy, Ethan Miller", "title": "SIVSHM: Secure Inter-VM Shared Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report UCSC-SSRC-16-01", "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With wide spread acceptance of virtualization, virtual machines (VMs) find\ntheir presence in various applications such as Network Address Translation\n(NAT) servers, firewall servers and MapReduce applications. Typically, in these\napplications a data manager collects data from the external world and\ndistributes it to multiple workers for further processing. Currently, data\nmanagers distribute data with workers either using inter-VM shared memory\n(IVSHMEM) or network communication. IVSHMEM provides better data distribution\nthroughput sacrificing security as all untrusted workers have full access to\nthe shared memory region and network communication provides better security at\nthe cost of throughput. Secondly, IVSHMEM uses a central distributor to\nexchange eventfd - a file descriptor to an event queue of length one, which is\nused for inter-VM signaling. This central distributor becomes a bottleneck and\nincreases boot time of VMs. Secure Inter-VM Shared Memory (SIVSHM) provided\nboth security and better throughout by segmenting inter-VM shared memory, so\nthat each worker has access to segment that belong only to it, thereby enabling\nsecurity without sacrificing throughput. SIVSHM boots VMs in 30% less time\ncompared to IVSHMEM by eliminating central distributor from its architecture\nand enabling direct exchange of eventfds amongst VMs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 14:10:16 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Sreenivasamurthy", "Shesha", ""], ["Miller", "Ethan", ""]]}, {"id": "1909.11644", "submitter": "arXiv Admin", "authors": "Reza Fotohi, Mehdi Effatparvar, Fateme Sarkohaki, Shahram Behzad,\n  Jaber Hoseini balov", "title": "An Improvement Over Threads Communications on Multi-Core Processors", "comments": "This submission has been withdrawn by arXiv administrators due to\n  inappropriate text reuse from external sources", "journal-ref": "2012, Volume 6, Issue 12, pp 379-384", "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multicore is an integrated circuit chip that uses two or more computational\nengines (cores) places in a single processor. This new approach is used to\nsplit the computational work of a threaded application and spread it over\nmultiple execution cores, so that the computer system can benefits from a\nbetter performance and better responsiveness of the system. A thread is a unit\nof execution inside a process that is created and maintained to execute a set\nof actions/ instructions. Threads can be implemented differently from an\noperating system to another, but the operating system is in most cases\nresponsible to schedule the execution of different threads. Multi-threading\nimproving efficiency of processor performance with a cost-effective memory\nsystem. In this paper, we explore one approach to improve communications for\nmultithreaded. Pre-send is a software Controlled data forwarding technique that\nsends data to destination's cache before it is needed, eliminating cache misses\nin the destination's cache as well as reducing the coherence traffic on the\nbus. we show how we could improve the overall system performance by addition of\nthese architecture optimizations to multi-core processors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:44:55 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 18:15:48 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Fotohi", "Reza", ""], ["Effatparvar", "Mehdi", ""], ["Sarkohaki", "Fateme", ""], ["Behzad", "Shahram", ""], ["balov", "Jaber Hoseini", ""]]}]