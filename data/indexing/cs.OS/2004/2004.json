[{"id": "2004.00402", "submitter": "Simson Garfinkel", "authors": "Simson L. Garfinkel and J. Spencer Love", "title": "A File System For Write-Once Media", "comments": "MIT Media Laboratory Tech Report, 1985", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A file system standard for use with write-once media such as digital compact\ndisks is proposed. The file system is designed to work with any operating\nsystem and a variety of physical media. Although the implementation is simple,\nit provides a a full-featured and high-performance alternative to conventional\nfile systems on traditional, multiple-write media such as magnetic disks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 01:44:14 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Garfinkel", "Simson L.", ""], ["Love", "J. Spencer", ""]]}, {"id": "2004.02400", "submitter": "Arvind Easwaran", "authors": "Xiaozhe Gu, Arvind Easwaran, Kieu-My Phan, Insik Shin", "title": "Resource Efficient Isolation Mechanisms in Mixed-Criticality Scheduling", "comments": "\\copyright 2015 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "Euromicro Conference on Real-Time Systems (ECRTS), Lund, 2015, pp.\n  13-24", "doi": "10.1109/ECRTS.2015.9", "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-criticality real-time scheduling has been developed to improve resource\nutilization while guaranteeing safe execution of critical applications. These\nstudies use optimistic resource reservation for all the applications to improve\nutilization, but prioritize critical applications when the reservations become\ninsufficient at runtime. Many of them however share an impractical assumption\nthat all the critical applications will simultaneously demand additional\nresources. As a consequence, they under-utilize resources by penalizing all the\nlow-criticality applications. In this paper we overcome this shortcoming using\na novel mechanism that comprises a parameter to model the expected number of\ncritical applications simultaneously demanding more resources, and an execution\nstrategy based on the parameter to improve resource utilization. Since most\nmixed-criticality systems in practice are component-based, we design our\nmechanism such that the component boundaries provide the isolation necessary to\nsupport the execution of low-criticality applications, and at the same time\nprotect the critical ones. We also develop schedulability tests for the\nproposed mechanism under both a flat as well as a hierarchical scheduling\nframework. Finally, through simulations, we compare the performance of the\nproposed approach with existing studies in terms of schedulability and the\ncapability to support low-criticality applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 04:39:58 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Gu", "Xiaozhe", ""], ["Easwaran", "Arvind", ""], ["Phan", "Kieu-My", ""], ["Shin", "Insik", ""]]}, {"id": "2004.02439", "submitter": "Arvind Easwaran", "authors": "Arvind Easwaran, Insik Shin, Insup Lee", "title": "Optimal Virtual Cluster-based Multiprocessor Scheduling", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in Springer Real-Time Systems journal. The final authenticated\n  version is available online at: https://doi.org/10.1007/s11241-009-9073-x", "journal-ref": "Springer Real-Time Systems, Volume 43, Pages 25-59, July 2009", "doi": "10.1007/s11241-009-9073-x", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling of constrained deadline sporadic task systems on multiprocessor\nplatforms is an area which has received much attention in the recent past. It\nis widely believed that finding an optimal scheduler is hard, and therefore\nmost studies have focused on developing algorithms with good processor\nutilization bounds. These algorithms can be broadly classified into two\ncategories: partitioned scheduling in which tasks are statically assigned to\nindividual processors, and global scheduling in which each task is allowed to\nexecute on any processor in the platform. In this paper we consider a third,\nmore general, approach called cluster-based scheduling. In this approach each\ntask is statically assigned to a processor cluster, tasks in each cluster are\nglobally scheduled among themselves, and clusters in turn are scheduled on the\nmultiprocessor platform. We develop techniques to support such cluster-based\nscheduling algorithms, and also consider properties that minimize total\nprocessor utilization of individual clusters. In the last part of this paper,\nwe develop new virtual cluster-based scheduling algorithms. For implicit\ndeadline sporadic task systems, we develop an optimal scheduling algorithm that\nis neither Pfair nor ERfair. We also show that the processor utilization bound\nof US-EDF{m/(2m-1)} can be improved by using virtual clustering. Since neither\npartitioned nor global strategies dominate over the other, cluster-based\nscheduling is a natural direction for research towards achieving improved\nprocessor utilization bounds.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 07:24:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Easwaran", "Arvind", ""], ["Shin", "Insik", ""], ["Lee", "Insup", ""]]}, {"id": "2004.03244", "submitter": "Christian Hakert", "authors": "Christian Hakert, Kuan-Hsun Chen, Paul R. Genssler, Georg von der\n  Br\\\"uggen, Lars Bauer, Hussam Amrouch, Jian-Jia Chen, J\\\"org Henkel", "title": "SoftWear: Software-Only In-Memory Wear-Leveling for Non-Volatile Main\n  Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several emerging technologies for byte-addressable non-volatile memory (NVM)\nhave been considered to replace DRAM as the main memory in computer systems\nduring the last years. The disadvantage of a lower write endurance, compared to\nDRAM, of NVM technologies like Phase-Change Memory (PCM) or Ferroelectric RAM\n(FeRAM) has been addressed in the literature. As a solution, in-memory\nwear-leveling techniques have been proposed, which aim to balance the\nwear-level over all memory cells to achieve an increased memory lifetime.\nGenerally, to apply such advanced aging-aware wear-leveling techniques proposed\nin the literature, additional special hardware is introduced into the memory\nsystem to provide the necessary information about the cell age and thus enable\naging-aware wear-leveling decisions.\n  This paper proposes software-only aging-aware wear-leveling based on common\nCPU features and does not rely on any additional hardware support from the\nmemory subsystem. Specifically, we exploit the memory management unit (MMU),\nperformance counters, and interrupts to approximate the memory write counts as\nan aging indicator. Although the software-only approach may lead to slightly\nworse wear-leveling, it is applicable on commonly available hardware. We\nachieve page-level coarse-grained wear-leveling by approximating the current\ncell age through statistical sampling and performing physical memory remapping\nthrough the MMU. This method results in non-uniform memory usage patterns\nwithin a memory page. Hence, we further propose a fine-grained wear-leveling in\nthe stack region of C / C++ compiled software.\n  By applying both wear-leveling techniques, we achieve up to $78.43\\%$ of the\nideal memory lifetime, which is a lifetime improvement of more than a factor of\n$900$ compared to the lifetime without any wear-leveling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:33:37 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 17:05:35 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Hakert", "Christian", ""], ["Chen", "Kuan-Hsun", ""], ["Genssler", "Paul R.", ""], ["von der Br\u00fcggen", "Georg", ""], ["Bauer", "Lars", ""], ["Amrouch", "Hussam", ""], ["Chen", "Jian-Jia", ""], ["Henkel", "J\u00f6rg", ""]]}, {"id": "2004.04760", "submitter": "Sudarsun Kannan", "authors": "Sudarsun Kannan, Yujie Ren, Abhishek Bhatacharjee", "title": "Efficient Kernel Object Management for Tiered Memory Systems with KLOC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-controlled heterogeneous memory systems have the potential to\nimprove performance, efficiency, and cost tradeoffs in emerging systems.\nDelivering on this promise requires an efficient operating system (OS)\nmechanisms and policies for data management. Unfortunately, modern OSes do not\nsupport efficient tiering of data between heterogeneous memories. While this\nproblem is known (and is being studied) for application-level data pages, the\nquestion of how best to tier OS kernel objects has largely been ignored. We\nshow that careful kernel object management is vital to the performance of\nsoftware-controlled tiered memory systems. We find that the state-of-the-art OS\npage management research leaves considerable performance on the table by\noverlooking how best to tier, migrate, and manage kernel objects like inodes,\ndentry caches, journal blocks, network socket buffers, etc., associated with\nthe filesystem and networking stack. In response, we characterize hotness,\nreuse, and liveness properties of kernel objects to develop appropriate\ntiering/migration mechanisms and policies. We evaluate our proposal using a\nreal-system emulation framework on large-scale workloads like RocksDB, Redis,\nCassandra, and Spark and achieve 1.4X to 4X higher throughput compared to the\nprior art.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:04:25 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kannan", "Sudarsun", ""], ["Ren", "Yujie", ""], ["Bhatacharjee", "Abhishek", ""]]}, {"id": "2004.04846", "submitter": "Zahra Tarkhani", "authors": "Zahra Tarkhani, Anil Madhavapeddy", "title": "$\\mu$Tiles: Efficient Intra-Process Privilege Enforcement of Memory\n  Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the alarming rate of security advisories and privacy concerns on\nconnected devices, there is an urgent need for strong isolation guarantees in\nresource-constrained devices that demand very lightweight solutions. However,\nthe status quo is that Unix-like operating systems do not offer privilege\nseparation inside a process. Lack of practical fine-grained\ncompartmentalization inside a shared address space leads to private data\nleakage through applications' untrusted dependencies and compromised threads.\nTo this end, we propose $\\mu$Tiles, a lightweight kernel abstraction and set of\nsecurity primitives based on mutual distrust for intra-process privilege\nseparation, memory protection, and secure multithreading. $\\mu$Tiles takes\nadvantage of hardware support for virtual memory tagging (e.g., ARM memory\ndomains) to achieve significant performance gain while eliminating various\nhardware limitations. Our results (based on OpenSSL, the Apache HTTP server,\nand LevelDB) show that $\\mu$Tiles is extremely lightweight (adds $\\approx 10KB$\nto kernel image) for IoT use cases. It adds negligible runtime overhead\n($\\approx 0.5\\%-3.5\\%$) and is easy to integrate with existing applications for\nproviding strong privilege separation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:07:44 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Tarkhani", "Zahra", ""], ["Madhavapeddy", "Anil", ""]]}, {"id": "2004.05518", "submitter": "Fei Wen", "authors": "Fei Wen, Mian Qin, Paul Gratz, Narasimha Reddy", "title": "Hardware Memory Management for Future Mobile Hybrid Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current mobile applications have rapidly growing memory footprints,\nposing a great challenge for memory system design. Insufficient DRAM main\nmemory will incur frequent data swaps between memory and storage, a process\nthat hurts performance, consumes energy and deteriorates the write endurance of\ntypical flash storage devices. Alternately, a larger DRAM has higher leakage\npower and drains the battery faster. Further, DRAM scaling trends make further\ngrowth of DRAMin the mobile space prohibitive due to cost. Emerging\nnon-volatile memory (NVM) has the potential to alleviate these issues due to\nits higher capacity per cost than DRAM and mini-mal static power. Recently, a\nwide spectrum of NVM technologies, including phase-change memories (PCM),\nmemristor, and 3D XPoint have emerged. Despite the mentioned advantages, NVM\nhas longer access latency compared to DRAMand NVM writes can incur higher\nlatencies and wear costs. Therefore integration of these new memory\ntechnologies in the memory hierarchy requires a fundamental rearchitect-ing of\ntraditional system designs. In this work, we propose a hardware-accelerated\nmemory manager (HMMU) that addresses both types of memory in a flat space\naddress space. We design a set of data placement and data migration policies\nwithin this memory manager, such that we may exploit the advantages of each\nmemory technology. By augmenting the system with this HMMU, we reduce the\noverall memory latency while also reducing writes to the NVM. Experimental\nresults show that our design achieves a 39% reduction in energy consumption\nwith only a 12% performance degradation versus an all-DRAM baseline that is\nlikely untenable in the future.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 01:25:04 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Wen", "Fei", ""], ["Qin", "Mian", ""], ["Gratz", "Paul", ""], ["Reddy", "Narasimha", ""]]}, {"id": "2004.05524", "submitter": "David Domingo", "authors": "David Domingo, Kyle Stratton, Sudarsun Kannan", "title": "Accelerating Filesystem Checking and Repair with pFSCK", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  File system checking and recovery (C/R) tools play a pivotal role in\nincreasing the reliability of storage software, identifying and correcting file\nsystem inconsistencies. However, with increasing disk capacity and data\ncontent, file system C/R tools notoriously suffer from long runtimes. We posit\nthat current file system checkers fail to exploit CPU parallelism and high\nthroughput offered by modern storage devices. To overcome these challenges, we\npropose pFSCK, a tool that redesigns C/R to enable fine-grained parallelism at\nthe granularity of inodes without impacting the correctness of C/R's\nfunctionality. To accelerate C/R, pFSCK first employs data parallelism by\nidentifying functional operations in each stage of the checker and isolating\ndependent operation and their shared data structures. However, fully isolating\nshared structures is infeasible, consequently requiring serialization that\nlimits scalability. To reduce the impact of synchronization bottlenecks and\nexploit CPU parallelism, pFSCK designs pipeline parallelism allowing multiple\nstages of C/R to run simultaneously without impacting correctness. To realize\nefficient pipeline parallelism for different file system data configurations,\npFSCK provides techniques for ordering updates to global data structures,\nefficient per-thread I/O cache management, and dynamic thread placement across\ndifferent passes of a C/R. Finally, pFSCK designs a resource-aware scheduler\naimed towards reducing the impact of C/R on other applications sharing CPUs and\nthe file system. Evaluation of pFSCK shows more than 2.6x gains of e2fsck and\nmore than 1.8x over XFS's checker that provides coarse-grained parallelism.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 01:59:20 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Domingo", "David", ""], ["Stratton", "Kyle", ""], ["Kannan", "Sudarsun", ""]]}, {"id": "2004.06354", "submitter": "Aleix Roca Nonell", "authors": "Aleix Roca, Samuel Rodr\\'iguez, Albert Segura, Kevin Marquet,\n  Vicen\\c{c} Beltran", "title": "A Linux Kernel Scheduler Extension for Multi-core Systems", "comments": "10 pages, 5 figures, conference", "journal-ref": null, "doi": "10.1109/HiPC.2019.00050", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linux kernel is mostly designed for multi-programed environments, but\nhigh-performance applications have other requirements. Such applications are\nrun standalone, and usually rely on runtime systems to distribute the\napplication's workload on worker threads, one per core. However, due to current\nOSes limitations, it is not feasible to track whether workers are actually\nrunning or blocked due to, for instance, a requested resource. For I/O\nintensive applications, this leads to a significant performance degradation\ngiven that the core of a blocked thread becomes idle until it is able to run\nagain. In this paper, we present the proof-of-concept of a Linux kernel\nextension denoted User-Monitored Threads (UMT) which tackles this problem. Our\nextension allows a user-space process to be notified of when the selected\nthreads become blocked or unblocked, making it possible for a runtime to\nschedule additional work on the idle core. We implemented the extension on the\nLinux Kernel 5.1 and adapted the Nanos6 runtime of the OmpSs-2 programming\nmodel to take advantage of it. The whole prototype was tested on two\napplications which, on the tested hardware and the appropriate conditions,\nreported speedups of almost 2x.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 08:35:33 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Roca", "Aleix", ""], ["Rodr\u00edguez", "Samuel", ""], ["Segura", "Albert", ""], ["Marquet", "Kevin", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "2004.09252", "submitter": "Marco Cesati", "authors": "Pierpaolo Santucci, Emiliano Ingrassia, Giulio Picierro, Marco Cesati", "title": "MemShield: GPU-assisted software memory encryption", "comments": "14 pages, 2 figures. In proceedings of the 18th International\n  Conference on Applied Cryptography and Network Security, ACNS 2020, October\n  19-22 2020, Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptographic algorithm implementations are vulnerable to Cold Boot attacks,\nwhich consist in exploiting the persistence of RAM cells across reboots or\npower down cycles to read the memory contents and recover precious sensitive\ndata. The principal defensive weapon against Cold Boot attacks is memory\nencryption. In this work we propose MemShield, a memory encryption framework\nfor user space applications that exploits a GPU to safely store the master key\nand perform the encryption/decryption operations. We developed a prototype that\nis completely transparent to existing applications and does not require changes\nto the OS kernel. We discuss the design, the related works, the implementation,\nthe security analysis, and the performances of MemShield.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:03:14 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Santucci", "Pierpaolo", ""], ["Ingrassia", "Emiliano", ""], ["Picierro", "Giulio", ""], ["Cesati", "Marco", ""]]}, {"id": "2004.09619", "submitter": "Rajat Kateja", "authors": "Rajat Kateja, Andy Pavlo, Gregory R. Ganger", "title": "Vilamb: Low Overhead Asynchronous Redundancy for Direct Access NVM", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-PDL-20-101", "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vilamb provides efficient asynchronous systemredundancy for direct access\n(DAX) non-volatile memory (NVM) storage. Production storage deployments often\nuse system-redundancy in form of page checksums and cross-page parity.\nState-of-the-art solutions for maintaining system-redundancy for DAX NVM either\nincur a high performance overhead or require specialized hardware. The Vilamb\nuser-space library maintains system-redundancy with low overhead by delaying\nand amortizing the system-redundancy updates over multiple data writes. As a\nresult, Vilamb provides 3--5x the throughput of the state-of-the-art software\nsolution at high operation rates. For applications that need system-redundancy\nwith high performance, and can tolerate some delaying of data redundancy,\nVilamb provides a tunable knob between performance and quicker redundancy. Even\nwith the delayed coverage, Vilamb increases the mean time to data loss due to\nfirmware-induced corruptions by up to two orders of magnitude in comparison to\nmaintaining no system-redundancy.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:35:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kateja", "Rajat", ""], ["Pavlo", "Andy", ""], ["Ganger", "Gregory R.", ""]]}]