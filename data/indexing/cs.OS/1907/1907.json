[{"id": "1907.00271", "submitter": "Kartik Hegde", "authors": "Kartik Hegde, Abhishek Srivastava, Rohit Agrawal", "title": "HTS: A Hardware Task Scheduler for Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the Moore's scaling era comes to an end, application specific hardware\naccelerators appear as an attractive way to improve the performance and power\nefficiency of our computing systems. A massively heterogeneous system with a\nlarge number of hardware accelerators along with multiple general purpose CPUs\nis a promising direction, but pose several challenges in terms of the run-time\nscheduling of tasks on the accelerators and design granularity of accelerators.\nThis paper addresses these challenges by developing an example heterogeneous\nsystem to enable multiple applications to share the available accelerators. We\npropose to design accelerators at a lower abstraction to enable applications to\nbe broken down into tasks that can be mapped on several accelerators. We\nobserve that several real-life workloads can be broken down into common\nprimitives that are shared across many workloads. Finally, we propose and\ndesign a hardware task scheduler inspired by the hardware schedulers in\nout-of-order superscalar processors to efficiently utilize the accelerators in\nthe system by scheduling tasks in out-of-order and even speculatively. We\nevaluate the proposed system on both real-life and synthetic benchmarks based\non Digital Signal Processing~(DSP) applications. Compared to executing the\nbenchmark on a system with sequential scheduling, proposed scheduler achieves\nup to 12x improvement in performance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 19:59:21 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hegde", "Kartik", ""], ["Srivastava", "Abhishek", ""], ["Agrawal", "Rohit", ""]]}, {"id": "1907.02064", "submitter": "Mark Hill", "authors": "Mark D. Hill and Vijay Janapa Reddi", "title": "Accelerator-level Parallelism", "comments": "6 pages, 3 figures, & 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future applications demand more performance, but technology advances have\nbeen faltering. A promising approach to further improve computer system\nperformance under energy constraints is to employ hardware accelerators.\nAlready today, mobile systems concurrently employ multiple accelerators in what\nwe call accelerator-level parallelism (ALP). To spread the benefits of ALP more\nbroadly, we charge computer scientists to develop the science needed to best\nachieve the performance and cost goals of ALP hardware and software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:04:47 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:53:53 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 20:11:05 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 14:58:25 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hill", "Mark D.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1907.03356", "submitter": "Petr Ro\\v{c}kai", "authors": "Petr Ro\\v{c}kai, Zuzana Baranov\\'a, Jan Mr\\'azek, Katar\\'ina\n  Kejstov\\'a, Ji\\v{r}\\'i Barnat", "title": "Reproducible Execution of POSIX Programs with DiOS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe DiOS, a lightweight model operating system which\ncan be used to execute programs that make use of POSIX APIs. Such executions\nare fully reproducible: running the same program with the same inputs twice\nwill result in two exactly identical instruction traces, even if the program\nuses threads for parallelism.\n  DiOS is implemented almost entirely in portable C and C++: although its\nprimary platform is DiVM, a verification-oriented virtual machine, it can be\nconfigured to also run in KLEE, a symbolic executor. Finally, it can be\ncompiled into machine code to serve as a user-mode kernel.\n  Additionally, DiOS is modular and extensible. Its various components can be\ncombined to match both the capabilities of the underlying platform and to\nprovide services required by a particular program. New components can be added\nto cover additional system calls or APIs.\n  The experimental evaluation has two parts. DiOS is first evaluated as a\ncomponent of a program verification platform based on DiVM. In the second part,\nwe consider its portability and modularity by combining it with the symbolic\nexecutor KLEE.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 22:26:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ro\u010dkai", "Petr", ""], ["Baranov\u00e1", "Zuzana", ""], ["Mr\u00e1zek", "Jan", ""], ["Kejstov\u00e1", "Katar\u00edna", ""], ["Barnat", "Ji\u0159\u00ed", ""]]}, {"id": "1907.05308", "submitter": "Michael Greenberg", "authors": "Michael Greenberg and Austin J. Blatt", "title": "Executable formal semantics for the POSIX shell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The POSIX shell is a widely deployed, powerful tool for managing computer\nsystems. The shell is the expert's control panel, a necessary tool for\nconfiguring, compiling, installing, maintaining, and deploying systems. Even\nthough it is powerful, critical infrastructure, the POSIX shell is maligned and\nmisunderstood. Its power and its subtlety are a dangerous combination.\n  We define a formal, mechanized, executable small-step semantics for the POSIX\nshell, which we call Smoosh. We compared Smoosh against seven other shells that\naim for some measure of POSIX compliance (bash, dash, zsh, OSH, mksh, ksh93,\nand yash). Using three test suites---the POSIX test suite, the Modernish test\nsuite and shell diagnosis, and a test suite of our own device---we found\nSmoosh's semantics to be the most conformant to the POSIX standard. Modernish\njudges Smoosh to have the fewest bugs (just one, from using dash's parser) and\nno quirks. To show that our semantics is useful beyond yielding a conformant,\nexecutable shell, we also implemented a symbolic stepper to illuminate the\nsubtle behavior of the shell.\n  Smoosh will serve as a foundation for formal study of the POSIX shell,\nsupporting research on and development of new shells, new tooling for shells,\nand new shell designs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:33:34 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Greenberg", "Michael", ""], ["Blatt", "Austin J.", ""]]}, {"id": "1907.10119", "submitter": "Dayeol Lee", "authors": "Dayeol Lee, David Kohlbrenner, Shweta Shinde, Dawn Song, Krste\n  Asanovi\\'c", "title": "Keystone: An Open Framework for Architecting TEEs", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trusted execution environments (TEEs) are being used in all the devices from\nembedded sensors to cloud servers and encompass a range of cost, power\nconstraints, and security threat model choices. On the other hand, each of the\ncurrent vendor-specific TEEs makes a fixed set of trade-offs with little room\nfor customization. We present Keystone -- the first open-source framework for\nbuilding customized TEEs. Keystone uses simple abstractions provided by the\nhardware such as memory isolation and a programmable layer underneath untrusted\ncomponents (e.g., OS). We build reusable TEE core primitives from these\nabstractions while allowing platform-specific modifications and application\nfeatures. We showcase how Keystone-based TEEs run on unmodified RISC-V hardware\nand demonstrate the strengths of our design in terms of security, TCB size,\nexecution of a range of benchmarks, applications, kernels, and deployment\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:24:19 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 05:54:07 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Lee", "Dayeol", ""], ["Kohlbrenner", "David", ""], ["Shinde", "Shweta", ""], ["Song", "Dawn", ""], ["Asanovi\u0107", "Krste", ""]]}, {"id": "1907.11825", "submitter": "Viacheslav Dubeyko", "authors": "Viacheslav Dubeyko", "title": "SSDFS: Towards LFS Flash-Friendly File System without GC operation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solid state drives have a number of interesting characteristics. However,\nthere are numerous file system and storage design issues for SSDs that impact\nthe performance and device endurance. Many flash-oriented and flash-friendly\nfile systems introduce significant write amplification issue and GC overhead\nthat results in shorter SSD lifetime and necessity to use the NAND flash\noverprovisioning. SSDFS file system introduces several authentic concepts and\nmechanisms: logical segment, logical extent, segment's PEBs pool,\nMain/Diff/Journal areas in the PEB's log, Diff-On-Write approach, PEBs\nmigration scheme, hot/warm data self-migration, segment bitmap, hybrid b-tree,\nshared dictionary b-tree, shared extents b-tree. Combination of all suggested\nconcepts are able: (1) manage write amplification in smart way, (2) decrease GC\noverhead, (3) prolong SSD lifetime, and (4) provide predictable file system's\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 01:36:43 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dubeyko", "Viacheslav", ""]]}, {"id": "1907.12014", "submitter": "Takahiro Hirofuchi", "authors": "Takahiro Hirofuchi and Ryousei Takano", "title": "The Preliminary Evaluation of a Hypervisor-based Virtualization\n  Mechanism for Intel Optane DC Persistent Memory Module", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM) technologies, being accessible in the same manner\nas DRAM, are considered indispensable for expanding main memory capacities.\nIntel Optane DCPMM is a long-awaited product that drastically increases main\nmemory capacities. However, a substantial performance gap exists between DRAM\nand DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and\n407% higher than those of DRAM, respectively. The read/write bandwidths were\n37% and 8% of those of DRAM. This performance gap in main memory presents a new\nchallenge to researchers; we need a new system software technology supporting\nemerging hybrid memory architecture. In this paper, we present RAMinate, a\nhypervisor-based virtualization mechanism for hybrid memory systems, and a key\ntechnology to address the performance gap in main memory systems. It provides\ngreat flexibility in memory management and maximizes the performance of virtual\nmachines (VMs) by dynamically optimizing memory mappings. Through experiments,\nwe confirmed that even though a VM has only 1% of DRAM in its RAM, the\nperformance degradation of the VM was drastically alleviated by memory mapping\noptimization. The elapsed time to finish the build of Linux Kernel in the VM\nwas 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495\nseconds). When the optimization mechanism was disabled, the elapsed time\nincreased to 624 seconds (i.e. 26% increase from the 100% DRAM case).\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 05:06:24 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Hirofuchi", "Takahiro", ""], ["Takano", "Ryousei", ""]]}, {"id": "1907.13245", "submitter": "Marcela Melara", "authors": "Marcela S. Melara, Michael J. Freedman, Mic Bowman", "title": "EnclaveDom: Privilege Separation for Large-TCB Applications in Trusted\n  Execution Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trusted executions environments (TEEs) such as Intel(R) SGX provide\nhardware-isolated execution areas in memory, called enclaves. By running only\nthe most trusted application components in the enclave, TEEs enable developers\nto minimize the TCB of their applications thereby helping to protect sensitive\napplication data. However, porting existing applications to TEEs often requires\nconsiderable refactoring efforts, as TEEs provide a restricted interface to\nstandard OS features. To ease development efforts, TEE application developers\noften choose to run their unmodified application in a library OS container that\nprovides a full in-enclave OS interface. Yet, this large-TCB development\napproach now leaves sensitive in-enclave data exposed to potential bugs or\nvulnerabilities in third-party code imported into the application. Importantly,\nbecause the TEE libOS and the application run in the same enclave address\nspace, even the libOS management data structures (e.g. file descriptor table)\nmay be vulnerable to attack, where in traditional OSes these data structures\nmay be protected via privilege isolation.\n  We present EnclaveDom, a privilege separation system for large-TCB TEE\napplications that partitions an enclave into tagged memory regions, and\nenforces per-region access rules at the granularity of individual in-enclave\nfunctions. EnclaveDom is implemented on Intel SGX using Memory Protection Keys\n(MPK) for memory tagging. To evaluate the security and performance impact of\nEnclaveDom, we integrated EnclaveDom with the Graphene-SGX library OS. While no\nproduct or component can be absolutely secure, our prototype helps protect\ninternal libOS management data structures against tampering by\napplication-level code. At every libOS system call, EnclaveDom then only grants\naccess to those internal data structures which the syscall needs to perform its\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 22:07:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 01:55:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Melara", "Marcela S.", ""], ["Freedman", "Michael J.", ""], ["Bowman", "Mic", ""]]}]