[{"id": "2011.01024", "submitter": "Chen Chen", "authors": "Chen Chen, Wenshao Zhong, Xingbo Wu", "title": "Efficient Data Management with Flexible File Address Space", "comments": "14 pages incl. references; 13 figures; 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data management applications store their data using structured files in which\ndata are usually sorted to serve indexing and queries. In order to insert or\nremove a record in a sorted file, the positions of existing data need to be\nshifted. To this end, the existing data after the insertion or removal point\nmust be rewritten to admit the change in place, which can be unaffordable for\napplications that make frequent updates. As a result, applications often employ\nextra layers of indirections to admit changes out-of-place. However, it causes\nincreased access costs and excessive complexity.\n  This paper presents a novel file abstraction, FlexFile, that provides a\nflexible file address space where in-place updates of arbitrary-sized data,\nsuch as insertions and removals, can be performed efficiently. With FlexFile,\napplications can manage their data in a linear file address space with minimal\ncomplexity. Extensive evaluation results show that a simple key-value store\nbuilt on top of this abstraction can achieve high performance for both reads\nand writes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:48:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:07:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Chen", ""], ["Zhong", "Wenshao", ""], ["Wu", "Xingbo", ""]]}, {"id": "2011.02455", "submitter": "Butler Lampson", "authors": "Butler Lampson", "title": "Hints and Principles for Computer System Design", "comments": "There is also a short version of this paper, about half the length of\n  this one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GL cs.OS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This new long version of my 1983 paper suggests the goals you might have for\nyour system -- Simple, Timely, Efficient, Adaptable, Dependable, Yummy (STEADY)\n-- and techniques for achieving them -- Approximate, Incremental, Divide &\nConquer (AID). It also gives some principles for system design that are more\nthan just hints, and many examples of how to apply the ideas.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:40:36 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 19:09:21 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 15:45:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lampson", "Butler", ""]]}, {"id": "2011.07160", "submitter": "Nan Wu", "authors": "Nan Wu, Pengcheng Li", "title": "Phoebe: Reuse-Aware Online Caching with Reinforcement Learning for\n  Emerging Storage Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.LG cs.OS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With data durability, high access speed, low power efficiency and byte\naddressability, NVMe and SSD, which are acknowledged representatives of\nemerging storage technologies, have been applied broadly in many areas.\nHowever, one key issue with high-performance adoption of these technologies is\nhow to properly define intelligent cache layers such that the performance gap\nbetween emerging technologies and main memory can be well bridged. To this end,\nwe propose Phoebe, a reuse-aware reinforcement learning framework for the\noptimal online caching that is applicable for a wide range of emerging storage\nmodels. By continuous interacting with the cache environment and the data\nstream, Phoebe is capable to extract critical temporal data dependency and\nrelative positional information from a single trace, becoming ever smarter over\ntime. To reduce training overhead during online learning, we utilize periodical\ntraining to amortize costs. Phoebe is evaluated on a set of Microsoft cloud\nstorage workloads. Experiment results show that Phoebe is able to close the gap\nof cache miss rate from LRU and a state-of-the-art online learning based cache\npolicy to the Belady's optimal policy by 70.3% and 52.6%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 22:55:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wu", "Nan", ""], ["Li", "Pengcheng", ""]]}, {"id": "2011.10249", "submitter": "Tuo Li", "authors": "Tuo Li, Bradley Hopkins, Sri Parameswaran", "title": "SIMF: Single-Instruction Multiple-Flush Mechanism for Processor Temporal\n  Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarchitectural timing attacks are a type of information leakage attack,\nwhich exploit the time-shared microarchitectural components, such as caches,\ntranslation look-aside buffers (TLBs), branch prediction unit (BPU), and\nspeculative execution, in modern processors to leak critical information from a\nvictim process or thread. To mitigate such attacks, the mechanism for flushing\nthe on-core state is extensively used by operating-system-level solutions,\nsince on-core state is too expensive to partition. In these systems, the\nflushing operations are implemented in software (using cache maintenance\ninstructions), which severely limit the efficiency of timing attack protection.\n  To bridge this gap, we propose specialized hardware support, a\nsingle-instruction multiple-flush (SIMF) mechanism to flush the core-level\nstate, which consists of L1 caches, BPU, TLBs, and register file. We\ndemonstrate SIMF by implementing it as an ISA extension, i.e., flushx\ninstruction, in scalar in-order RISC-V processor. The resultant processor is\nprototyped on Xilinx ZCU102 FPGA and validated with state-of-art seL4\nmicrokernel, Linux kernel in multi-core scenarios, and a cache timing attack.\nOur evaluation shows that SIMF significantly alleviates the overhead of\nflushing by more than a factor of two in execution time and reduces dynamic\ninstruction count by orders-of-magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 07:48:27 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Li", "Tuo", ""], ["Hopkins", "Bradley", ""], ["Parameswaran", "Sri", ""]]}, {"id": "2011.12047", "submitter": "Emmanuel Baccelli", "authors": "Koen Zandberg, Emmanuel Baccelli", "title": "Minimal Virtual Machines on IoT Microcontrollers: The Case of Berkeley\n  Packet Filters with rBPF", "comments": null, "journal-ref": "In proceedings of IFIP/IEEE PEMWN 2020", "doi": null, "report-no": null, "categories": "cs.NI cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machines (VM) are widely used to host and isolate software modules.\nHowever, extremely small memory and low-energy budgets have so far prevented\nwide use of VMs on typical microcontroller-based IoT devices. In this paper, we\nexplore the potential of two minimal VM approaches on such low-power hardware.\nWe design rBPF, a register-based VM based on extended Berkeley Packet Filters\n(eBPF). We compare it with a stack-based VM based on WebAssembly (Wasm) adapted\nfor embedded systems. We implement prototypes of each VM, hosted in the IoT\noperating system RIOT. We perform measurements on commercial off-the-shelf IoT\nhardware. Unsurprisingly, we observe that both Wasm and rBPF virtual machines\nyield execution time and memory overhead, compared to not using a VM. We show\nhowever that this execution time overhead is tolerable for low-throughput,\nlow-energy IoT devices. We further show that, while using a VM based on Wasm\nentails doubling the memory budget for a simple networked IoT application using\na 6LoWPAN/CoAP stack, using a VM based on rBPF requires only negligible memory\noverhead (less than 10% more memory). rBPF is thus a promising approach to host\nsmall software modules, isolated from OS software, and updatable on-demand,\nover low-power networks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:46:00 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 08:15:30 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zandberg", "Koen", ""], ["Baccelli", "Emmanuel", ""]]}, {"id": "2011.12092", "submitter": "Ashish Panwar", "authors": "Venkat Sri Sai Ram, Ashish Panwar, Arkaprava Basu", "title": "Leveraging Architectural Support of Three Page Sizes with Trident", "comments": "13 pages, 16 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large pages are commonly deployed to reduce address translation overheads for\nbig-memory workloads. Modern x86-64 processors from Intel and AMD support two\nlarge page sizes -- 1GB and 2MB. However, previous works on large pages have\nprimarily focused on 2MB pages, partly due to lack of substantial evidence on\nthe profitability of 1GB pages to real-world applications. We argue that in\nfact, inadequate system software support is responsible for a decade of\nunderutilized hardware support for 1GB pages.\n  Through extensive experimentation on a real system, we demonstrate that 1GB\npages can improve performance over 2MB pages, and when used in tandem with 2MB\npages for an important set of applications; the support for the latter is\ncrucial but missing in current systems. Our design and implementation of\n\\trident{} in Linux fully exploit hardware supported large pages by dynamically\nand transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable.\n\\trident{} speeds up eight memory-intensive applications by {$18\\%$}, on\naverage, over Linux's use of 2MB pages. We also propose \\tridentpv{}, an\nextension to \\trident{} that effectively virtualizes 1GB pages via copy-less\npromotion and compaction in the guest OS. Overall, this paper shows that even\nGB-sized pages have considerable practical significance with adequate software\nenablement, in turn motivating architects to continue investing/innovating in\nlarge pages.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 13:54:55 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Ram", "Venkat Sri Sai", ""], ["Panwar", "Ashish", ""], ["Basu", "Arkaprava", ""]]}, {"id": "2011.13432", "submitter": "Aleix Roca Nonell", "authors": "Aleix Roca Nonell, Balazs Gerofi, Leonardo Bautista-Gomez, Dominique\n  Martinet, Vicen\\c{c} Beltran Querol, Yutaka Ishikawa", "title": "On the Applicability of PEBS based Online Memory Access Tracking for\n  Heterogeneous Memory Management at Scale", "comments": "8 pages, 16 figures, conference", "journal-ref": "Proceedings of the Workshop on Memory Centric High Performance\n  Computing (2018) 50-57", "doi": "10.1145/3286475.3286477", "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operating systems have historically had to manage only a single type of\nmemory device. The imminent availability of heterogeneous memory devices based\non emerging memory technologies confronts the classic single memory model and\nopens a new spectrum of possibilities for memory management. Transparent data\nmovement between different memory devices based on access patterns of\napplications is a desired feature to make optimal use of such devices and to\nhide the complexity of memory management to the end-user. However, capturing\nmemory access patterns of an application at runtime comes at a cost, which is\nparticularly challenging for large scale parallel applications that may be\nsensitive to system noise.\n  In this work, we focus on the access pattern profiling phase prior to the\nactual memory relocation. We study the feasibility of using Intel's Processor\nEvent-Based Sampling (PEBS) feature to record memory accesses by sampling at\nruntime and study the overhead at scale. We have implemented a custom PEBS\ndriver in the IHK/McKernel lightweight multi-kernel operating system, one of\nwhose advantages is minimal system interference due to the lightweight kernel's\nsimple design compared to other OS kernels such as Linux. We present the PEBS\noverhead of a set of scientific applications and show the access patterns\nidentified in noise-sensitive HPC applications. Our results show that clear\naccess patterns can be captured with a 10% overhead in the worst-case and 1% in\nthe best case when running on up to 128k CPU cores (2,048 Intel Xeon Phi\nKnights Landing nodes). We conclude that online memory access profiling using\nPEBS at large scale is promising for memory management in heterogeneous memory\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:39:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Nonell", "Aleix Roca", ""], ["Gerofi", "Balazs", ""], ["Bautista-Gomez", "Leonardo", ""], ["Martinet", "Dominique", ""], ["Querol", "Vicen\u00e7 Beltran", ""], ["Ishikawa", "Yutaka", ""]]}, {"id": "2011.15065", "submitter": "Olivier Nicole", "authors": "Olivier Nicole, Matthieu Lemerre, S\\'ebastien Bardin, Xavier Rival", "title": "No Crash, No Exploit: Automated Verification of Embedded Kernels", "comments": "Published in IEEE Real-Time and Embedded Technology and Applications\n  Symposium (RTAS'21)", "journal-ref": null, "doi": "10.1109/RTAS52030.2021.00011", "report-no": null, "categories": "cs.CR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel is the most safety- and security-critical component of many\ncomputer systems, as the most severe bugs lead to complete system crash or\nexploit. It is thus desirable to guarantee that a kernel is free from these\nbugs using formal methods, but the high cost and expertise required to do so\nare deterrent to wide applicability. We propose a method that can verify both\nabsence of runtime errors (i.e. crashes) and absence of privilege escalation\n(i.e. exploits) in embedded kernels from their binary executables. The method\ncan verify the kernel runtime independently from the application, at the\nexpense of only a few lines of simple annotations. When given a specific\napplication, the method can verify simple kernels without any human\nintervention. We demonstrate our method on two different use cases: we use our\ntool to help the development of a new embedded real-time kernel, and we verify\nan existing industrial real-time kernel executable with no modification.\nResults show that the method is fast, simple to use, and can prevent real\nerrors and security vulnerabilities.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:03:28 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 21:47:51 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nicole", "Olivier", ""], ["Lemerre", "Matthieu", ""], ["Bardin", "S\u00e9bastien", ""], ["Rival", "Xavier", ""]]}]