[{"id": "2007.00706", "submitter": "Maolin Yang", "authors": "Maolin Yang, Zewei Chen, Xu Jiang, Nan Guan, Hang Lei", "title": "DPCP-p: A Distributed Locking Protocol for Parallel Real-Time Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scheduling and locking protocols are fundamental facilities to\nconstruct time-critical systems. For parallel real-time tasks, predictable\nlocking protocols are required when concurrent sub-jobs mutually exclusive\naccess to shared resources. This paper for the first time studies the\ndistributed synchronization framework of parallel real-time tasks, where both\ntasks and global resources are partitioned to designated processors, and\nrequests to each global resource are conducted on the processor on which the\nresource is partitioned. We extend the Distributed Priority Ceiling Protocol\n(DPCP) for parallel tasks under federated scheduling, with which we proved that\na request can be blocked by at most one lower-priority request. We develop task\nand resource partitioning heuristics and propose analysis techniques to safely\nbound the task response times. Numerical evaluation (with heavy tasks on 8-,\n16-, and 32-core processors) indicates that the proposed methods improve the\nschedulability significantly compared to the state-of-the-art locking protocols\nunder federated scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:13:26 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yang", "Maolin", ""], ["Chen", "Zewei", ""], ["Jiang", "Xu", ""], ["Guan", "Nan", ""], ["Lei", "Hang", ""]]}, {"id": "2007.04552", "submitter": "Yifan Yuan", "authors": "Yifan Yuan, Mohammad Alian, Yipeng Wang, Ilia Kurakin, Ren Wang,\n  Charlie Tai, Nam Sung Kim", "title": "IOCA: High-Speed I/O-Aware LLC Management for Network-Centric\n  Multi-Tenant Platform", "comments": "Accepted by the 48th IEEE/ACM International Symposium on Computer\n  Architecture (ISCA'21). The title is \"Don't Forget the I/O When Allocating\n  Your LLC\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern server CPUs, last-level cache (LLC) is a critical hardware resource\nthat exerts significant influence on the performance of the workloads, and how\nto manage LLC is a key to the performance isolation and QoS in the cloud with\nmulti-tenancy. In this paper, we argue that besides CPU cores, high-speed\nnetwork I/O is also important for LLC management. This is because of an Intel\narchitectural innovation -- Data Direct I/O (DDIO) -- that directly injects the\ninbound I/O traffic to (part of) the LLC instead of the main memory. We\nsummarize two problems caused by DDIO and show that (1) the default DDIO\nconfiguration may not always achieve optimal performance, (2) DDIO can decrease\nthe performance of non-I/O workloads which share LLC with it by as high as 32%.\n  We then present IOCA, the first LLC management mechanism for network-centric\nplatforms that treats the I/O as the first-class citizen. IOCA monitors and\nanalyzes the performance of the cores, LLC, and DDIO using CPU's hardware\nperformance counters, and adaptively adjusts the number of LLC ways for DDIO or\nthe tenants that demand more LLC capacity. In addition, IOCA dynamically\nchooses the tenants that share its LLC resource with DDIO, to minimize the\nperformance interference by both the tenants and the I/O. Our experiments with\nmultiple microbenchmarks and real-world applications in two major end-host\nnetwork models demonstrate that IOCA can effectively reduce the performance\ndegradation caused by DDIO, with minimal overhead.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 04:45:54 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 16:14:42 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yuan", "Yifan", ""], ["Alian", "Mohammad", ""], ["Wang", "Yipeng", ""], ["Kurakin", "Ilia", ""], ["Wang", "Ren", ""], ["Tai", "Charlie", ""], ["Kim", "Nam Sung", ""]]}, {"id": "2007.05136", "submitter": "Yaswanth Yadlapalli", "authors": "Zelun Kong, Yaswanth Yadlapalli, Soroush Bateni, Junfeng Guo, Cong Liu", "title": "LINTS^RT: A Learning-driven Testbed for Intelligent Scheduling in\n  Embedded Systems", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing complexity seen in both workloads and hardware\nresources in state-of-the-art embedded systems, developing efficient real-time\nschedulers and the corresponding schedulability tests becomes rather\nchallenging. Although close to optimal schedulability performance can be\nachieved for supporting simple system models in practice, adding any small\ncomplexity element into the problem context such as non-preemption or resource\nheterogeneity would cause significant pessimism, which may not be eliminated by\nany existing scheduling technique. In this paper, we present LINTS^RT, a\nlearning-based testbed for intelligent real-time scheduling, which has the\npotential to handle various complexities seen in practice. The design of\nLINTS^RT is fundamentally motivated by AlphaGo Zero for playing the board game\nGo, and specifically addresses several critical challenges due to the real-time\nscheduling context. We first present a clean design of LINTS^RT for supporting\nthe basic case: scheduling sporadic workloads on a homogeneous multiprocessor,\nand then demonstrate how to easily extend the framework to handle further\ncomplexities such as non-preemption and resource heterogeneity. Both\napplication and OS-level implementation and evaluation demonstrate that\nLINTS^RT is able to achieve significantly higher runtime schedulability under\ndifferent settings compared to perhaps the most commonly applied schedulers,\nglobal EDF, and RM. To our knowledge, this work is the first attempt to design\nand implement an extensible learning-based testbed for autonomously making\nreal-time scheduling decisions.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 02:03:52 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Kong", "Zelun", ""], ["Yadlapalli", "Yaswanth", ""], ["Bateni", "Soroush", ""], ["Guo", "Junfeng", ""], ["Liu", "Cong", ""]]}, {"id": "2007.06775", "submitter": "Jayashree Mohan", "authors": "Jayashree Mohan, Amar Phanishayee, Ashish Raniwala, Vijay Chidambaram", "title": "Analyzing and Mitigating Data Stalls in DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Deep Neural Networks (DNNs) is resource-intensive and\ntime-consuming. While prior research has explored many different ways of\nreducing DNN training time, the impact of input data pipeline, i.e., fetching\nraw data items from storage and performing data pre-processing in memory, has\nbeen relatively unexplored. This paper makes the following contributions: (1)\nWe present the first comprehensive analysis of how the input data pipeline\naffects the training time of widely-used computer vision and audio Deep Neural\nNetworks (DNNs), that typically involve complex data preprocessing. We analyze\nnine different models across three tasks and four datasets while varying\nfactors such as the amount of memory, number of CPU threads, storage device,\nGPU generation etc on servers that are a part of a large production cluster at\nMicrosoft. We find that in many cases, DNN training time is dominated by data\nstall time: time spent waiting for data to be fetched and preprocessed. (2) We\nbuild a tool, DS-Analyzer to precisely measure data stalls using a differential\ntechnique, and perform predictive what-if analysis on data stalls. (3) Finally,\nbased on the insights from our analysis, we design and implement three simple\nbut effective techniques in a data-loading library, CoorDL, to mitigate data\nstalls. Our experiments on a range of DNN tasks, models, datasets, and hardware\nconfigs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI\ndata loading library, DNN training time is reduced significantly (by as much as\n5x on a single server).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:16:56 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 16:20:47 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 18:35:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Mohan", "Jayashree", ""], ["Phanishayee", "Amar", ""], ["Raniwala", "Ashish", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "2007.08302", "submitter": "Jian-Jia Chen", "authors": "Jian-Jia Chen, Junjie Shi, Georg von der Br\\\"uggen, Niklas Ueter", "title": "Scheduling of Real-Time Tasks with Multiple Critical Sections in\n  Multiprocessor Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multiprocessor synchronization and locking protocols is a\nkey factor to utilize the computation power of multiprocessor systems under\nreal-time constraints. While multiple protocols have been developed in the past\ndecades, their performance highly depends on the task partition and\nprioritization. The recently proposed Dependency Graph Approach showed its\nadvantages and attracted a lot of interest. It is, however, restricted to task\nsets where each task has at most one critical section. In this paper, we remove\nthis restriction and demonstrate how to utilize algorithms for the classical\njob shop scheduling problem to construct a dependency graph for tasks with\nmultiple critical sections. To show the applicability, we discuss the\nimplementation in Litmus^{RT} and report the overheads. Moreover, we provide\nextensive numerical evaluations under different configurations, which in many\nsituations show significant improvement compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 12:43:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Jian-Jia", ""], ["Shi", "Junjie", ""], ["von der Br\u00fcggen", "Georg", ""], ["Ueter", "Niklas", ""]]}, {"id": "2007.11112", "submitter": "Christos Kozyrakis", "authors": "Michael Cafarella and David DeWitt and Vijay Gadepally and Jeremy\n  Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and\n  Matei Zaharia", "title": "DBOS: A Proposal for a Data-Centric Operating System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current operating systems are complex systems that were designed before\ntoday's computing environments. This makes it difficult for them to meet the\nscalability, heterogeneity, availability, and security challenges in current\ncloud and parallel computing environments. To address these problems, we\npropose a radically new OS design based on data-centric architecture: all\noperating system state should be represented uniformly as database tables, and\noperations on this state should be made via queries from otherwise stateless\ntasks. This design makes it easy to scale and evolve the OS without\nwhole-system refactoring, inspect and debug system state, upgrade components\nwithout downtime, manage decisions using machine learning, and implement\nsophisticated security features. We discuss how a database OS (DBOS) can\nimprove the programmability and performance of many of today's most important\napplications and propose a plan for the development of a DBOS proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:01:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cafarella", "Michael", ""], ["DeWitt", "David", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Kozyrakis", "Christos", ""], ["Kraska", "Tim", ""], ["Stonebraker", "Michael", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.12112", "submitter": "Frank Slomka", "authors": "Frank Slomka and Mohammadreza Sadeghi", "title": "HeRTA: Heaviside Real-Time Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the mathematical properties of event bound functions as they\nare used in the worst-case response time analysis and utilization tests. We\nfigure out the differences and similarities between the two approaches. Based\non this analysis, we derive a more general form do describe events and event\nbounds. This new unified approach gives clear new insights in the investigation\nof real-time systems, simplifies the models and will support algebraic proofs\nin future work. In the end, we present a unified analysis which allows the\nalgebraic definition of any scheduler. Introducing such functions to the\nreal-time scheduling theory will lead two a more systematic way to integrate\nnew concepts and applications to the theory. Last but not least, we show how\nthe response time analysis in dynamic scheduling can be improved.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:33:23 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Slomka", "Frank", ""], ["Sadeghi", "Mohammadreza", ""]]}]