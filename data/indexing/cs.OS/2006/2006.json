[{"id": "2006.00380", "submitter": "Boris Teabe Dr.", "authors": "Boris Teabe, Peterson Yuhala, Alain Tchana, Fabien Hermenier, Daniel\n  Hagimont, Gilles Muller", "title": "Memory virtualization in virtualized systems: segmentation is better\n  than paging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilization of paging for virtual machine (VM) memory management is the\nroot cause of memory virtualization overhead. This paper shows that paging is\nnot necessary in the hypervisor. In fact, memory fragmentation, which explains\npaging utilization, is not an issue in virtualized datacenters thanks to VM\nmemory demand patterns. Our solution Compromis, a novel Memory Management Unit,\nuses direct segment for VM memory management combined with paging for VM's\nprocesses. The paper presents a systematic methodology for implementing\nCompromis in the hardware, the hypervisor and the datacenter scheduler.\nEvaluation results show that Compromis outperforms the two popular memory\nvirtualization solutions: shadow paging and Extended Page Table by up to 30%\nand 370% respectively.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 22:39:46 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Teabe", "Boris", ""], ["Yuhala", "Peterson", ""], ["Tchana", "Alain", ""], ["Hermenier", "Fabien", ""], ["Hagimont", "Daniel", ""], ["Muller", "Gilles", ""]]}, {"id": "2006.01354", "submitter": "Tan N. Le", "authors": "Tan N. Le, Zhenhua Liu", "title": "Flex: Closing the Gaps between Usage and Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers are giant factories of Internet data and services. Worldwide\ndata centers consume energy and emit emissions more than airline industry.\nUnfortunately, most of data centers are significantly underutilized. One of the\nmajor reasons is the big gaps between the real usage and the provisioned\nresources because users tend to over-estimate their demand and data center\noperators often rely on users' requests for resource allocation. In this paper,\nwe first conduct an in-depth analysis of a Google cluster trace to unveil the\nroot causes for low utilization and highlight the great potential to improve\nit. We then developed an online resource manager Flex to maximize the cluster\nutilization while satisfying the Quality of Service (QoS). Large-scale\nevaluations based on real-world traces show that Flex admits up to 1.74x more\nrequests and 1.6x higher utilization compared to tradition schedulers while\nmaintaining the QoS.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:41:39 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Le", "Tan N.", ""], ["Liu", "Zhenhua", ""]]}, {"id": "2006.02055", "submitter": "Mohsen Amini Salehi", "authors": "Davood Ghatreh Samani, Chavit Denninnart, Josef Bacik, Mohsen Amini\n  Salehi", "title": "The Art of CPU-Pinning: Evaluating and Improving the Performance of\n  Virtualization and Containerization Platforms", "comments": null, "journal-ref": "The 49th International Conference on Parallel Processing (ICPP\n  2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers offer a variety of execution platforms in form of bare-metal,\nVM, and containers. However, due to the pros and cons of each execution\nplatform, choosing the appropriate platform for a specific cloud-based\napplication has become a challenge for solution architects. The possibility to\ncombine these platforms (e.g. deploying containers within VMs) offers new\ncapacities that makes the challenge even further complicated. However, there is\na little study in the literature on the pros and cons of deploying different\napplication types on various execution platforms. In particular, evaluation of\ndiverse hardware configurations and different CPU provisioning methods, such as\nCPU pinning, have not been sufficiently studied in the literature. In this\nwork, the performance overhead of container, VM, and bare-metal execution\nplatforms are measured and analyzed for four categories of real-world\napplications, namely video processing, parallel processing (MPI), web\nprocessing, and No-SQL, respectively representing CPU intensive, parallel\nprocessing, and two IO intensive processes. Our analyses reveal a set of\ninteresting and sometimes counterintuitive findings that can be used as best\npractices by the solution architects to efficiently deploy cloud-based\napplications. Here are some notable mentions: (A) Under specific circumstances,\ncontainers can impose a higher overhead than VMs; (B) Containers on top of VMs\ncan mitigate the overhead of VMs for certain applications; (C) Containers with\na large number of cores impose a lower overhead than those with a few cores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:47:14 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Samani", "Davood Ghatreh", ""], ["Denninnart", "Chavit", ""], ["Bacik", "Josef", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2006.07086", "submitter": "Sahil Dhoked", "authors": "Sahil Dhoked and Neeraj Mittal", "title": "An Adaptive Approach to Recoverable Mutual Exlcusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual exclusion (ME) is one of the most commonly used techniques to handle\nconflicts in concurrent systems. Traditionally, mutual exclusion algorithms\nhave been designed under the assumption that a process does not fail while\nacquiring/releasing a lock or while executing its critical section. However,\nfailures do occur in real life, potentially leaving the lock in an inconsistent\nstate. This gives rise to the problem of \\emph{recoverable mutual exclusion\n(RME)} that involves designing a mutual exclusion algorithm that can tolerate\nfailures, while maintaining safety and liveness properties.\n  One of the important measures of performance of any ME algorithm, including\nan RME algorithm, is the number of \\emph{remote memory references (RMRs)} made\nby a process (for acquiring and releasing a lock as well as recovering the lock\nstructure after a failure). The best known RME algorithm solves the problem for\n$n$ processes in sub-logarithmic number of RMRs, given by\n$\\mathcal{O}(\\frac{\\log n}{\\log \\log n})$, irrespective of the number of\nfailures in the system.\n  In this work, we present a new algorithm for solving the RME problem whose\nRMR complexity gradually \\emph{adapts} to the number of failures that have\noccurred in the system \"recently\". In the absence of failures, our algorithm\ngenerates only $\\mathcal{O}(1)$ RMRs. Furthermore, its RMR complexity is given\nby $\\mathcal{O}(\\min\\{ \\sqrt{F}, \\frac{\\log n}{\\log \\log n} \\})$ where $F$ is\nthe total number of failures in the \"recent\" past. In addition to read and\nwrite instructions, our algorithm uses compare-and-swap (\\CAS{}) and\nfetch-and-store (\\FAS{}) hardware instructions, both of which are commonly\navailable in most modern processors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 11:18:04 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 06:04:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dhoked", "Sahil", ""], ["Mittal", "Neeraj", ""]]}, {"id": "2006.07163", "submitter": "Wolfgang John", "authors": "Mina Sedaghat, Pontus Sk\\\"oldstr\\\"om, Daniel Turull, Vinay Yadhav,\n  Joacim Hal\\'en, Madhubala Ganesan, Amardeep Mehta and Wolfgang John", "title": "Nefele: Process Orchestration for the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization, either at OS- or hardware level, plays an important role in\ncloud computing. It enables easier automation and faster deployment in\ndistributed environments. While virtualized infrastructures provide a level of\nmanagement flexibility, they lack practical abstraction of the distributed\nresources. A developer in such an environment still needs to deal with all the\ncomplications of building a distributed software system. Different\norchestration systems are built to provide that abstraction; however, they do\nnot solve the inherent challenges of distributed systems, such as\nsynchronization issues or resilience to failures. This paper introduces Nefele,\na decentralized process orchestration system that automatically deploys and\nmanages individual processes, rather than containers/VMs, within a cluster.\nNefele is inspired by the Single System Image (SSI) vision of mitigating the\nintricacies of remote execution, yet it maintains the flexibility and\nperformance of virtualized infrastructures. Nefele offers a set of APIs for\nbuilding cloud-native applications that lets the developer easily build,\ndeploy, and scale applications in a cloud environment. We have implemented and\ndeployed Nefele on a cluster in our datacenter and evaluated its performance.\nOur evaluations show that Nefele can effectively deploy, scale, and monitor\nprocesses across a distributed environment, while it incorporates essential\nprimitives to build a distributed software system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:21:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:32:14 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Sedaghat", "Mina", ""], ["Sk\u00f6ldstr\u00f6m", "Pontus", ""], ["Turull", "Daniel", ""], ["Yadhav", "Vinay", ""], ["Hal\u00e9n", "Joacim", ""], ["Ganesan", "Madhubala", ""], ["Mehta", "Amardeep", ""], ["John", "Wolfgang", ""]]}, {"id": "2006.08966", "submitter": "Myoungsoo Jung", "authors": "Jie Zhang, Miryeong Kwon, Sanghyun Han, Nam Sung Kim, Mahmut Kandemir,\n  Myoungsoo Jung", "title": "FastDrain: Removing Page Victimization Overheads in NVMe Storage Stack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Host-side page victimizations can easily overflow the SSD internal buffer,\nwhich interferes I/O services of diverse user applications thereby degrading\nuser-level experiences. To address this, we propose FastDrain, a co-design of\nOS kernel and flash firmware to avoid the buffer overflow, caused by page\nvictimizations. Specifically, FastDrain can detect a triggering point where a\nnear-future page victimization introduces an overflow of the SSD internal\nbuffer. Our new flash firmware then speculatively scrubs the buffer space to\naccommodate the requests caused by the page victimization. In parallel, our new\nOS kernel design controls the traffic of page victimizations by considering the\ntarget device buffer status, which can further reduce the risk of buffer\noverflow. To secure more buffer spaces, we also design a latency-aware FTL,\nwhich dumps the dirty data only to the fast flash pages. Our evaluation results\nreveal that FastDrain reduces the 99th response time of user applications by\n84%, compared to a conventional system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 07:45:22 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:11:11 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Jie", ""], ["Kwon", "Miryeong", ""], ["Han", "Sanghyun", ""], ["Kim", "Nam Sung", ""], ["Kandemir", "Mahmut", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "2006.12133", "submitter": "Safdar Jamil Mr", "authors": "Taeuk Kim, Safdar Jamil, Joongeon Park, Youngjae Kim", "title": "Optimizing Placement of Heap Memory Objects in Energy-Constrained Hybrid\n  Memory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main memory (DRAM) significantly impacts the power and energy utilization of\nthe overall server system. Non-Volatile Memory (NVM) devices, such as Phase\nChange Memory and Spin-Transfer Torque RAM, are suitable candidates for main\nmemory to reduce energy consumption. But unlike DRAM, NVMs access latencies are\nhigher than DRAM and NVM writes are more energy sensitive than DRAM write\noperations. Thus, Hybrid Main Memory Systems (HMMS) employing DRAM and NVM have\nbeen proposed to reduce the overall energy depletion of main memory while\noptimizing the performance of NVM. This paper proposes eMap, an optimal heap\nmemory object placement planner in HMMS. eMap considers the object-level access\npatterns and energy consumption at the application level and provides an ideal\nplacement strategy for each object to augment performance and energy\nutilization. eMap is equipped with two modules, eMPlan and eMDyn. Specifically,\neMPlan is a static placement planner which provides one time placement policies\nfor memory object to meet the energy budget while eMDyn is a runtime placement\nplanner to consider the change in energy limiting constraint during the runtime\nand shuffles the memory objects by taking into account the access patterns as\nwell as the migration cost in terms of energy and performance. The evaluation\nshows that our proposed solution satisfies both the energy limiting constraint\nand the performance. We compare our methodology with the state-of-the-art\nmemory object classification and allocation (MOCA) framework. Our extensive\nevaluation shows that our proposed solution, eMPlan meets the energy constraint\nwith 4.17 times less costly and reducing the energy consumption up to 14% with\nthe same performance. eMDyn also satisfies the performance and energy\nrequirement while considering the migration cost in terms of time and energy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 10:37:40 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 01:28:19 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kim", "Taeuk", ""], ["Jamil", "Safdar", ""], ["Park", "Joongeon", ""], ["Kim", "Youngjae", ""]]}, {"id": "2006.12144", "submitter": "Shady Issa", "authors": "Alex Kogan, Dave Dice, Shady Issa", "title": "Scalable Range Locks for Scalable Address Spaces and Beyond", "comments": "17 pages, 9 figures, Eurosys 2020", "journal-ref": null, "doi": "10.1145/3342195.3387533", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range locks are a synchronization construct designed to provide concurrent\naccess to multiple threads (or processes) to disjoint parts of a shared\nresource. Originally conceived in the file system context, range locks are\ngaining increasing interest in the Linux kernel community seeking to alleviate\nbottlenecks in the virtual memory management subsystem. The existing\nimplementation of range locks in the kernel, however, uses an internal spin\nlock to protect the underlying tree structure that keeps track of acquired and\nrequested ranges. This spin lock becomes a point of contention on its own when\nthe range lock is frequently acquired. Furthermore, where and exactly how\nspecific (refined) ranges can be locked remains an open question.\n  In this paper, we make two independent, but related contributions. First, we\npropose an alternative approach for building range locks based on linked lists.\nThe lists are easy to maintain in a lock-less fashion, and in fact, our range\nlocks do not use any internal locks in the common case. Second, we show how the\nrange of the lock can be refined in the mprotect operation through a\nspeculative mechanism. This refinement, in turn, allows concurrent execution of\nmprotect operations on non-overlapping memory regions. We implement our new\nalgorithms and demonstrate their effectiveness in user-space and kernel-space,\nachieving up to 9$\\times$ speedup compared to the stock version of the Linux\nkernel. Beyond the virtual memory management subsystem, we discuss other\napplications of range locks in parallel software. As a concrete example, we\nshow how range locks can be used to facilitate the design of scalable\nconcurrent data structures, such as skip lists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:12:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kogan", "Alex", ""], ["Dice", "Dave", ""], ["Issa", "Shady", ""]]}]