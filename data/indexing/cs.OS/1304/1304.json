[{"id": "1304.3557", "submitter": "Asmaa Hamo Dr.", "authors": "Radhwan Y Ameen and Asmaa Y. Hamo", "title": "Survey of Server Virtualization", "comments": "10 pages 14 figures. arXiv admin note: text overlap with\n  arXiv:1010.3233 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization is a term that refers to the abstraction of computer\nresources. The purpose of virtual computing environment is to improve resource\nutilization by providing a unified integrated operating platform for users and\napplications based on aggregation of heterogeneous and autonomous resources.\nMore recently, virtualization at all levels (system, storage, and network)\nbecame important again as a way to improve system security, reliability and\navailability, reduce costs, and provide greater flexibility. Virtualization has\nrapidly become a go-to technology for increasing efficiency in the data center.\nWith virtualization technologies providing tremendous flexibility, even\ndisparate architectures may be deployed on a single machine without\ninterference This paper explains the basics of server virtualization and\naddresses pros and cons of virtualization\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 07:33:42 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Ameen", "Radhwan Y", ""], ["Hamo", "Asmaa Y.", ""]]}, {"id": "1304.3771", "submitter": "Ardalan Amiri Sani", "authors": "Ardalan Amiri Sani, Sreekumar Nair, Lin Zhong, Quinn Jacobson", "title": "Making I/O Virtualization Easy with Device Files", "comments": null, "journal-ref": null, "doi": null, "report-no": "Rice University ECE Technical Report 2013-04-13", "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal computers have diverse and fast-evolving I/O devices, making their\nI/O virtualization different from that of servers and data centers. In this\npaper, we present our recent endeavors in simplifying I/O virtualization for\npersonal computers. Our key insight is that many operating systems, including\nUnix-like ones, abstract I/O devices as device files. There is a small and\nstable set of operations on device files, therefore, I/O virtualization at the\ndevice file boundary requires a one-time effort to support various I/O devices.\n  We present devirtualization, our design of I/O virtualization at the device\nfile boundary and its implementation for Linux/x86 systems. We are able to\nvirtualize various GPUs, input devices, cameras, and audio devices with fewer\nthan 4900 LoC, of which only about 300 are specific to I/O device classes. Our\nmeasurements show that devirtualized devices achieve interactive performance\nindistinguishable from native ones by human users, even when running 3D HD\ngames.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 04:04:08 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Sani", "Ardalan Amiri", ""], ["Nair", "Sreekumar", ""], ["Zhong", "Lin", ""], ["Jacobson", "Quinn", ""]]}, {"id": "1304.6007", "submitter": "Enoch Peserico", "authors": "Enoch Peserico", "title": "Paging with dynamic memory capacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a generalization of the classic paging problem that allows the\namount of available memory to vary over time - capturing a fundamental property\nof many modern computing realities, from cloud computing to multi-core and\nenergy-optimized processors. It turns out that good performance in the\n\"classic\" case provides no performance guarantees when memory capacity\nfluctuates: roughly speaking, moving from static to dynamic capacity can mean\nthe difference between optimality within a factor 2 in space and time, and\nsuboptimality by an arbitrarily large factor. More precisely, adopting the\ncompetitive analysis framework, we show that some online paging algorithms,\ndespite having an optimal (h,k)-competitive ratio when capacity remains\nconstant, are not (3,k)-competitive for any arbitrarily large k in the presence\nof minimal capacity fluctuations. In this light it is surprising that several\nclassic paging algorithms perform remarkably well even if memory capacity\nchanges adversarially - even without taking those changes into explicit\naccount! In particular, we prove that LFD still achieves the minimum number of\nfaults, and that several classic online algorithms such as LRU have a \"dynamic\"\n(h,k)-competitive ratio that is the best one can achieve without knowledge of\nfuture page requests, even if one had perfect knowledge of future capacity\nfluctuations (an exact characterization of this ratio shows it is almost,\nalbeit not quite, equal to the \"classic\" ratio k/(k-h+1)). In other words, with\ncareful management, knowing/predicting future memory resources appears far less\ncrucial to performance than knowing/predicting future data accesses.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 16:23:24 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Peserico", "Enoch", ""]]}, {"id": "1304.6067", "submitter": "Stefan Wallentowitz", "authors": "J\\\"urgen Teich, Wolfgang Schr\\\"oder-Preikschat, Andreas Herkersdorf", "title": "Invasive Computing - Common Terms and Granularity of Invasion", "comments": null, "journal-ref": null, "doi": null, "report-no": "DPA-13052", "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future MPSoCs with 1000 or more processor cores on a chip require new means\nfor resource-aware programming in order to deal with increasing imperfections\nsuch as process variation, fault rates, aging effects, and power as well as\nthermal problems. On the other hand, predictable program executions are\nthreatened if not impossible if no proper means of resource isolation and\nexclusive use may be established on demand. In view of these problems and\nmenaces, invasive computing enables an application programmer to claim for\nprocessing resources and spread computations to claimed processors dynamically\nat certain points of the program execution.\n  Such decisions may be depending on the degree of application parallelism and\nthe state of the underlying resources such as utilization, load, and\ntemperature, but also with the goal to provide predictable program execution on\nMPSoCs by claiming processing resources exclusively as the default and thus\neliminating interferences and creating the necessary isolation between multiple\nconcurrently running applications. For achieving this goal, invasive computing\nintroduces new programming constructs for resource-aware programming that\nmeanwhile, for testing purpose, have been embedded into the parallel computing\nlanguage X10 as developed by IBM using a library-based approach.\n  This paper presents major ideas and common terms of invasive computing as\ninvestigated by the DFG Transregional Collaborative Research Centre TR89.\nMoreoever, a reflection is given on the granularity of resources that may be\nrequested by invasive programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 19:29:05 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Teich", "J\u00fcrgen", ""], ["Schr\u00f6der-Preikschat", "Wolfgang", ""], ["Herkersdorf", "Andreas", ""]]}, {"id": "1304.7001", "submitter": "Urmila Shrawankar Ms", "authors": "Deepika Bhatia, Urmila Shrawankar", "title": "Network Control Systems RTAI framework A Review", "comments": "Pages: 4 Figures : 1", "journal-ref": "International Journal of Computer Science and Information\n  Technologies (IJCSIT),Vol. 2(5) , 2011, 2380-2383", "doi": null, "report-no": null, "categories": "cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in the automation industry, to perform complex remote\noperations is required. Advancements in the networking technology has led to\nthe development of different architectures to implement control from a large\ndistance. In various control applications of the modern industry, the agents,\nsuch as sensors, actuators, and controllers are basically geographically\ndistributed. For efficient working of a control application, all of the agents\nhave to exchange information through a communication media. At present, an\nincreasing number of distributed control systems are based on platforms made up\nof conventional PCs running open-source real-time operating systems. Often,\nthese systems needed to have networked devices supporting synchronized\noperations with respect to each node. A framework is studied that relies on\nstandard software and protocol as RTAI, EtherCAT, RTnet and IEEE 1588. RTAI and\nits various protocols are studied in network control systems environment.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 11:10:38 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Bhatia", "Deepika", ""], ["Shrawankar", "Urmila", ""]]}]