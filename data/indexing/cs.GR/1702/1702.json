[{"id": "1702.00182", "submitter": "Ryuji Hirayama", "authors": "Ryuji Hirayama, Tomotaka Suzuki, Tomoyoshi Shimobaba, Atsushi Shiraki,\n  Makoto Naruse, Hirotaka Nakayama, Takashi Kakue and Tomoyoshi Ito", "title": "Inkjet printing-based volumetric display projecting multiple full-colour\n  2D patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a method to construct a full-colour volumetric display is\npresented using a commercially available inkjet printer. Photoreactive\nluminescence materials are minutely and automatically printed as the volume\nelements, and volumetric displays are constructed with high resolution using\neasy-to-fabricate means that exploit inkjet printing technologies. The results\nexperimentally demonstrate the first prototype of an inkjet printing-based\nvolumetric display composed of multiple layers of transparent films that yield\na full-colour three-dimensional (3D) image. Moreover, we propose a design\nalgorithm with 3D structures that provide multiple different 2D full-colour\npatterns when viewed from different directions and experimentally demonstrates\nprototypes. It is considered that these types of 3D volumetric structures and\ntheir fabrication methods based on widely deployed existing printing\ntechnologies can be utilised as novel information display devices and systems,\nincluding digital signage, media art, entertainment and security.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 10:01:44 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Hirayama", "Ryuji", ""], ["Suzuki", "Tomotaka", ""], ["Shimobaba", "Tomoyoshi", ""], ["Shiraki", "Atsushi", ""], ["Naruse", "Makoto", ""], ["Nakayama", "Hirotaka", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1702.00737", "submitter": "Jun Tao", "authors": "Jun Tao, Jian Xu, Chaoli Wang, Nitesh V. Chawla", "title": "HoNVis: Visualizing and Exploring Higher-Order Networks", "comments": "Accepted for publication and presentation at IEEE PacificVis 2017, to\n  be held in Seoul, Korea during April 18 to 21, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the conventional first-order network (FoN), the higher-order network\n(HoN) provides a more accurate description of transitions by creating\nadditional nodes to encode higher-order dependencies. However, there exists no\nvisualization and exploration tool for the HoN. For applications such as the\ndevelopment of strategies to control species invasion through global shipping\nwhich is known to exhibit higher-order dependencies, the existing FoN\nvisualization tools are limited. In this paper, we present HoNVis, a novel\nvisual analytics framework for exploring higher-order dependencies of the\nglobal ocean shipping network. Our framework leverages coordinated multiple\nviews to reveal the network structure at three levels of detail (i.e., the\nglobal, local, and individual port levels). Users can quickly identify ports of\ninterest at the global level and specify a port to investigate its higher-order\nnodes at the individual port level. Investigating a larger-scale impact is\nenabled through the exploration of HoN at the local level. Using the global\nocean shipping network data, we demonstrate the effectiveness of our approach\nwith a real-world use case conducted by domain experts specializing in species\ninvasion. Finally, we discuss the generalizability of this framework to other\nreal-world applications such as information diffusion in social networks and\nepidemic spreading through air transportation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 16:18:39 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Tao", "Jun", ""], ["Xu", "Jian", ""], ["Wang", "Chaoli", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1702.01530", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, S. A. Zori", "title": "Ray tracing method for stereo image synthesis using CUDA", "comments": "8 pages, 8 Figures, International Journal of Engineering Science and\n  Technology 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a realization of the approach to spatial 3D stereo of\nvisualization of 3D images with use parallel Graphics processing unit (GPU).\nThe experiments of realization of synthesis of images of a 3D stage by a method\nof trace of beams on GPU with Compute Unified Device Architecture (CUDA) have\nshown that 60 % of the time is spent for the decision of a computing problem\napproximately, the major part of time (40 %) is spent for transfer of data\nbetween the central processing unit and GPU for computations and the\norganization process of visualization. The study of the influence of increase\nin the size of the GPU network at the speed of computations showed importance\nof the correct task of structure of formation of the parallel computer network\nand general mechanism of parallelization. Keywords: Volumetric 3D\nvisualization, stereo 3D visualization, ray tracing, parallel computing on GPU,\nCUDA\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 08:36:10 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 18:56:42 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Zori", "S. A.", ""]]}, {"id": "1702.01537", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Aladdein M. Amro, S. A. Zori, Anas M. Al-Oraiqat", "title": "Conceptual and algorithmic development of Pseudo 3D Graphics and Video\n  Content Visualization", "comments": "12 pages, 10 Figures", "journal-ref": "International Publisher for Advanced Scientific Journals 2013", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a general concept of the organization of pseudo three\ndimension visualization of graphics and video content for three dimension\nvisualization systems. The steps of algorithms for solving the problem of\nsynthesis of three dimension stereo images based on two dimension images are\nintroduced. The features of synthesis organization of standard format of three\ndimension stereo frame are presented. Moreover, the performed experimental\nsimulation for generating complete stereoframes and the results of its time\ncomplexity are shown. Keywords:Three dimension visualization, pseudo three\ndimension stereo, a stereo pair, three dimension stereo format, algorithm,\nmodeling, time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 09:24:02 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Amro", "Aladdein M.", ""], ["Zori", "S. A.", ""], ["Al-Oraiqat", "Anas M.", ""]]}, {"id": "1702.01540", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, E.A. Bashkov, S.A. Zori, Aladdein M. Amro", "title": "Generalized 3D Voxel Image Synthesis Architecture for Volumetric Spatial\n  Visualization", "comments": "9 pages, 4 Figures, International Publisher for Advanced Scientific\n  Journals 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general concept of 3D volumetric visualization systems is described based\non 3D discrete voxel scenes (worlds) representation. Definitions of 3D discrete\nvoxel scene (world) basic elements and main steps of the image synthesis\nalgorithm are formulated. An algorithm for solving the problem of the voxelized\nworld 3D image synthesis, intended for the systems of volumetric spatial\nvisualization, is proposed. A computer-based architecture for 3D volumetric\nvisualization of 3D discrete voxel world is presented. On the basis of the\nproposed overall concept of discrete voxel representation, the proposed\narchitecture successfully adapts the ray tracing technique for the synthesis of\n3D volumetric images. Since it is algorithmically simple and effectively\nsupports parallelism, it can efficiently be implemented.\n  Key words:Volumetric spatial visualization, 3D volumetric imagesynthesis,\ndiscrete voxel world, ray tracing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 09:38:20 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Bashkov", "E. A.", ""], ["Zori", "S. A.", ""], ["Amro", "Aladdein M.", ""]]}, {"id": "1702.01961", "submitter": "Renato Budinich", "authors": "Renato Budinich", "title": "A Region Based Easy Path Wavelet Transform For Sparse Image\n  Representation", "comments": "Fixed use of HaarPSI - see Figure 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Easy Path Wavelet Transform is an adaptive transform for bivariate\nfunctions (in particular natural images) which has been proposed in [1]. It\nprovides a sparse representation by finding a path in the domain of the\nfunction leveraging the local correlations of the function values. It then\napplies a one dimensional wavelet transform to the obtained vector, decimates\nthe points and iterates the procedure. The main drawback of such method is the\nneed to store, for each level of the transform, the path which vectorizes the\ntwo dimensional data. Here we propose a variation on the method which consists\nof firstly applying a segmentation procedure to the function domain,\npartitioning it into regions where the variation in the function values is low;\nin a second step, inside each such region, a path is found in some\ndeterministic way, i.e. not data-dependent. This circumvents the need to store\nthe paths at each level, while still obtaining good quality lossy compression.\nThis method is particularly well suited to encode a Region of Interest in the\nimage with different quality than the rest of the image.\n  [1] Gerlind Plonka. The easy path wavelet transform: A new adaptive wavelet\ntransform for sparse representation of two-dimensional data. Multiscale\nModeling & Simulation, 7(3):1474$-$1496, 2008.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 11:13:08 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 10:41:38 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Budinich", "Renato", ""]]}, {"id": "1702.02463", "submitter": "Ziwei Liu", "authors": "Ziwei Liu, Raymond A. Yeh, Xiaoou Tang, Yiming Liu, Aseem Agarwala", "title": "Video Frame Synthesis using Deep Voxel Flow", "comments": "To appear in ICCV 2017 as an oral paper. More details at the project\n  page: https://liuziwei7.github.io/projects/VoxelFlow.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of synthesizing new video frames in an existing video,\neither in-between existing frames (interpolation), or subsequent to them\n(extrapolation). This problem is challenging because video appearance and\nmotion can be highly complex. Traditional optical-flow-based solutions often\nfail where flow estimation is challenging, while newer neural-network-based\nmethods that hallucinate pixel values directly often produce blurry results. We\ncombine the advantages of these two methods by training a deep network that\nlearns to synthesize video frames by flowing pixel values from existing ones,\nwhich we call deep voxel flow. Our method requires no human supervision, and\nany video can be used as training data by dropping, and then learning to\npredict, existing frames. The technique is efficient, and can be applied at any\nvideo resolution. We demonstrate that our method produces results that both\nquantitatively and qualitatively improve upon the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 15:20:14 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 04:43:44 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Liu", "Ziwei", ""], ["Yeh", "Raymond A.", ""], ["Tang", "Xiaoou", ""], ["Liu", "Yiming", ""], ["Agarwala", "Aseem", ""]]}, {"id": "1702.02514", "submitter": "Markus H\\\"oll BSc", "authors": "Markus H\\\"oll, Vincent Lepetit", "title": "Monocular LSD-SLAM Integration within AR System", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.10054.27205", "report-no": "ICG--CVARLab-TR--ICGCVARLab-TR003", "categories": "cs.CV cs.GR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we cover the process of integrating Large-Scale Direct\nSimultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR\nstereo engine, developed for our modified \"Augmented Reality Oculus Rift\". With\nthat, we are able to track one of our realworld cameras which are mounted on\nthe rift, within a complete unknown environment. This makes it possible to\nachieve a constant and full augmentation, synchronizing our 3D movement (x, y,\nz) in both worlds, the real world and the virtual world. The development for\nthe basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully\ndocumented in our previous paper. After an introduction to image-based\nregistration, we detail the LSD-SLAM algorithm and document our code\nimplementing our integration. The AR stereo engine with Oculus Rift support can\nbe accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and\nthe modified LSD-SLAM project used for the integration is available here\nhttps://github.com/MaXvanHeLL/LSD-SLAM.git.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 16:52:19 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["H\u00f6ll", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1702.02878", "submitter": "Leonardo Fernandez-Jambrina", "authors": "L. Fern\\'andez-Jambrina", "title": "Bezier developable surfaces", "comments": "26 pages, 10 figures, Computer Aided Geometric Design special number\n  in memoriam Professor Gerald Farin", "journal-ref": "Computer Aided Geometric Design 55, 15-28 (2017)", "doi": "10.1016/j.cagd.2017.02.001", "report-no": null, "categories": "cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the issue of designing developable surfaces with\nBezier patches. We show that developable surfaces with a polynomial edge of\nregression are the set of developable surfaces which can be constructed with\nAumann's algorithm. We also obtain the set of polynomial developable surfaces\nwhich can be constructed using general polynomial curves. The conclusions can\nbe extended to spline surfaces as well.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 16:23:36 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Fern\u00e1ndez-Jambrina", "L.", ""]]}, {"id": "1702.03246", "submitter": "Christos Mousas", "authors": "Christos Mousas", "title": "Towards Developing an Easy-To-Use Scripting Environment for Animating\n  Virtual Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the three scripting commands and main functionalities of\na novel character animation environment called CHASE. CHASE was developed for\nenabling inexperienced programmers, animators, artists, and students to animate\nin meaningful ways virtual reality characters. This is achieved by scripting\nsimple commands within CHASE. The commands identified, which are associated\nwith simple parameters, are responsible for generating a number of predefined\nmotions and actions of a character. Hence, the virtual character is able to\nanimate within a virtual environment and to interact with tasks located within\nit. An additional functionality of CHASE is supplied. It provides the ability\nto generate multiple tasks of a character, such as providing the user the\nability to generate scenario-related animated sequences. However, since\nmultiple characters may require simultaneous animation, the ability to script\nactions of different characters at the same time is also provided.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 16:37:55 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Mousas", "Christos", ""]]}, {"id": "1702.04852", "submitter": "Philippe Pebay", "authors": "Gu\\'enol\\'e Harel, Jacques-Bernard Lekien, Philippe P. P\\'eba\\\"y", "title": "Visualization and Analysis of Large-Scale, Tree-Based, Adaptive Mesh\n  Refinement Simulations with Arbitrary Rectilinear Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here the first systematic treatment of the problems posed by the\nvisualization and analysis of large-scale, parallel adaptive mesh refinement\n(AMR) simulations on an Eulerian grid. When compared to those obtained by\nconstructing an intermediate unstructured mesh with fully described\nconnectivity, our primary results indicate a gain of at least 80\\% in terms of\nmemory footprint, with a better rendering while retaining similar execution\nspeed. In this article, we describe the key concepts that allow us to obtain\nthese results, together with the methodology that facilitates the design,\nimplementation, and optimization of algorithms operating directly on such\nrefined meshes. This native support for AMR meshes has been contributed to the\nopen source Visualization Toolkit (VTK). This work pertains to a broader\nlong-term vision, with the dual goal to both improve interactivity when\nexploring such data sets in 2 and 3 dimensions, and optimize resource\nutilization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 03:49:46 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Harel", "Gu\u00e9nol\u00e9", ""], ["Lekien", "Jacques-Bernard", ""], ["P\u00e9ba\u00ff", "Philippe P.", ""]]}, {"id": "1702.06228", "submitter": "Yanwei  Fu", "authors": "Yu-ting Qiang, Yanwei Fu, Xiao Yu, Yanwen Guo, Zhi-Hua Zhou and Leonid\n  Sigal", "title": "Learning to Generate Posters of Scientific Papers by Probabilistic\n  Graphical Models", "comments": "10 pages, submission to IEEE TPAMI. arXiv admin note: text overlap\n  with arXiv:1604.01219", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often summarize their work in the form of scientific posters.\nPosters provide a coherent and efficient way to convey core ideas expressed in\nscientific papers. Generating a good scientific poster, however, is a complex\nand time consuming cognitive task, since such posters need to be readable,\ninformative, and visually aesthetic. In this paper, for the first time, we\nstudy the challenging problem of learning to generate posters from scientific\npapers. To this end, a data-driven framework, that utilizes graphical models,\nis proposed. Specifically, given content to display, the key elements of a good\nposter, including attributes of each panel and arrangements of graphical\nelements are learned and inferred from data. During the inference stage, an MAP\ninference framework is employed to incorporate some design principles. In order\nto bridge the gap between panel attributes and the composition within each\npanel, we also propose a recursive page splitting algorithm to generate the\npanel layout for a poster. To learn and validate our model, we collect and\nrelease a new benchmark dataset, called NJU-Fudan Paper-Poster dataset, which\nconsists of scientific papers and corresponding posters with exhaustively\nlabelled panels and attributes. Qualitative and quantitative results indicate\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:02:56 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Qiang", "Yu-ting", ""], ["Fu", "Yanwei", ""], ["Yu", "Xiao", ""], ["Guo", "Yanwen", ""], ["Zhou", "Zhi-Hua", ""], ["Sigal", "Leonid", ""]]}, {"id": "1702.07619", "submitter": "Amy Tabb", "authors": "Amy Tabb and Henry Medeiros", "title": "Fast and robust curve skeletonization for real-world elongated objects", "comments": "47 pages; IEEE WACV 2018, main paper and supplementary material", "journal-ref": null, "doi": "10.1109/WACV.2018.00214", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of extracting curve skeletons of three-dimensional,\nelongated objects given a noisy surface, which has applications in agricultural\ncontexts such as extracting the branching structure of plants. We describe an\nefficient and robust method based on breadth-first search that can determine\ncurve skeletons in these contexts. Our approach is capable of automatically\ndetecting junction points as well as spurious segments and loops. All of that\nis accomplished with only one user-adjustable parameter. The run time of our\nmethod ranges from hundreds of milliseconds to less than four seconds on large,\nchallenging datasets, which makes it appropriate for situations where real-time\ndecision making is needed. Experiments on synthetic models as well as on data\nfrom real world objects, some of which were collected in challenging field\nconditions, show that our approach compares favorably to classical thinning\nalgorithms as well as to recent contributions to the field.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 15:01:22 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:03:31 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 17:35:54 GMT"}, {"version": "v4", "created": "Mon, 19 Mar 2018 14:44:28 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Tabb", "Amy", ""], ["Medeiros", "Henry", ""]]}, {"id": "1702.08680", "submitter": "Jie Zhu", "authors": "Jie Zhu, Yanwen Guo and Han Ma", "title": "A Data-driven Approach for Furniture and Indoor Scene Colorization", "comments": "13 pages, 16 figures, submission to IEEE TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven approach that colorizes 3D furniture models and\nindoor scenes by leveraging indoor images on the internet. Our approach is able\nto colorize the furniture automatically according to an example image. The core\nis to learn image-guided mesh segmentation to segment the model into different\nparts according to the image object. Given an indoor scene, the system supports\ncolorization-by-example, and has the ability to recommend the colorization\nscheme that is consistent with a user-desired color theme. The latter is\nrealized by formulating the problem as a Markov random field model that imposes\nuser input as an additional constraint. We contribute to the community a\nhierarchically organized image-model database with correspondences between each\nimage and the corresponding model at the part-level. Our experiments and a user\nstudy show that our system produces perceptually convincing results comparable\nto those generated by interior designers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 07:54:21 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Zhu", "Jie", ""], ["Guo", "Yanwen", ""], ["Ma", "Han", ""]]}]