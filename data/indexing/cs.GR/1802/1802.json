[{"id": "1802.02120", "submitter": "Eugene d'Eon", "authors": "Eugene d'Eon and M. M. R. Williams", "title": "Isotropic Scattering in a Flatland Half-Space", "comments": "final version, accepted to JCTT", "journal-ref": null, "doi": "10.1080/23324309.2018.1544566", "report-no": null, "categories": "physics.class-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the Milne, constant-source and albedo problems for isotropic\nscattering in a two-dimensional \"Flatland\" half-space via the Wiener-Hopf\nmethod. The Flatland $H$-function is derived and benchmark values and some\nidentities unique to Flatland are presented. A number of the derivations are\nsupported by Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 03:06:54 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 10:41:33 GMT"}, {"version": "v3", "created": "Sat, 11 Aug 2018 03:33:51 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["d'Eon", "Eugene", ""], ["Williams", "M. M. R.", ""]]}, {"id": "1802.02673", "submitter": "Tomer Weiss", "authors": "Tomer Weiss, Alan Litteneker, Chenfanfu Jiang, Demetri Terzopoulos", "title": "Position-Based Multi-Agent Dynamics for Real-Time Crowd Simulation (MiG\n  paper)", "comments": "9 pages", "journal-ref": "MIG 2017 Proceedings of the Tenth International Conference on\n  Motion in Games", "doi": "10.1145/3136457.3136462", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exploiting the efficiency and stability of Position-Based Dynamics (PBD), we\nintroduce a novel crowd simulation method that runs at interactive rates for\nhundreds of thousands of agents. Our method enables the detailed modeling of\nper-agent behavior in a Lagrangian formulation. We model short-range and\nlong-range collision avoidance to simulate both sparse and dense crowds. On the\nparticles representing agents, we formulate a set of positional constraints\nthat can be readily integrated into a standard PBD solver. We augment the\ntentative particle motions with planning velocities to determine the preferred\nvelocities of agents, and project the positions onto the constraint manifold to\neliminate colliding configurations. The local short-range interaction is\nrepresented with collision and frictional contact between agents, as in the\ndiscrete simulation of granular materials. We incorporate a cohesion model for\nmodeling collective behaviors and propose a new constraint for dealing with\npotential future collisions. Our new method is suitable for use in interactive\ngames.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 23:37:20 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 01:58:15 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Weiss", "Tomer", ""], ["Litteneker", "Alan", ""], ["Jiang", "Chenfanfu", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1802.02731", "submitter": "Julien Tierny", "authors": "Maxime Soler and Melanie Plainchault and Bruno Conche and Julien\n  Tierny", "title": "Topologically Controlled Lossy Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for the lossy compression of scalar data\ndefined on 2D or 3D regular grids, with topological control. Certain techniques\nallow users to control the pointwise error induced by the compression. However,\nin many scenarios it is desirable to control in a similar way the preservation\nof higher-level notions, such as topological features , in order to provide\nguarantees on the outcome of post-hoc data analyses. This paper presents the\nfirst compression technique for scalar data which supports a strictly\ncontrolled loss of topological features. It provides users with specific\nguarantees both on the preservation of the important features and on the size\nof the smaller features destroyed during compression. In particular, we present\na simple compression strategy based on a topologically adaptive quantization of\nthe range. Our algorithm provides strong guarantees on the bottleneck distance\nbetween persistence diagrams of the input and decompressed data, specifically\nthose associated with extrema. A simple extension of our strategy additionally\nenables a control on the pointwise error. We also show how to combine our\napproach with state-of-the-art compressors, to further improve the geometrical\nreconstruction. Extensive experiments, for comparable compression rates,\ndemonstrate the superiority of our algorithm in terms of the preservation of\ntopological features. We show the utility of our approach by illustrating the\ncompatibility between the output of post-hoc topological data analysis\npipelines, executed on the input and decompressed data, for simulated or\nacquired data sets. We also provide a lightweight VTK-based C++ implementation\nof our approach for reproduction purposes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 07:35:43 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Soler", "Maxime", ""], ["Plainchault", "Melanie", ""], ["Conche", "Bruno", ""], ["Tierny", "Julien", ""]]}, {"id": "1802.03168", "submitter": "Young Jin Oh", "authors": "Young Jin Oh, Tae Min Lee, In-Kwon Lee", "title": "Hierarchical Cloth Simulation using Deep Neural Networks", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and reliable physically-based simulation techniques are essential for\nproviding flexible visual effects for computer graphics content. In this paper,\nwe propose a fast and reliable hierarchical cloth simulation method, which\ncombines conventional physically-based simulation with deep neural networks\n(DNN). Simulations of the coarsest level of the hierarchical model are\ncalculated using conventional physically-based simulations, and more detailed\nlevels are generated by inference using DNN models. We demonstrate that our\nmethod generates reliable and fast cloth simulation results through experiments\nunder various conditions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 08:32:29 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Oh", "Young Jin", ""], ["Lee", "Tae Min", ""], ["Lee", "In-Kwon", ""]]}, {"id": "1802.04706", "submitter": "Bin Liu", "authors": "Xiao-Nan Fang, Bin Liu, and Ariel Shamir", "title": "Automatic thread painting generation", "comments": "20 pages, 8 figures", "journal-ref": "Communications in Information and Systems, Volume 16 (2016),\n  Number 4", "doi": "10.4310/CIS.2016.v16.n4.a3", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ThreadTone is an NPR representation of an input image by half-toning using\nthreads on a circle. Current approaches to create ThreadTone paintings greedily\ndraw the chords on the circle. We introduce the concept of chord space, and\ndesign a new algorithm to improve the quality of the thread painting. We use an\noptimization process that estimates the fitness of every chord in the chord\nspace, and an error-diffusion based sampling process that selects a moderate\nnumber of chords to produce the output painting. We used an image similarity\nmeasure to evaluate the quality of our thread painting and also conducted a\nuser study. Our approach can produce high quality results on portraits,\nsketches as well as cartoon pictures.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:21:59 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Fang", "Xiao-Nan", ""], ["Liu", "Bin", ""], ["Shamir", "Ariel", ""]]}, {"id": "1802.06852", "submitter": "Zeeshan Bhatti", "authors": "Zeeshan Bhatti, Ahsan Abro, Abdul Rehman Gillal, Mostafa Karbasi", "title": "Be-Educated: Multimedia Learning through 3D Animation", "comments": "10 pages, 32 figures", "journal-ref": "INTERNATIONAL JOURNAL OF COMPUTER SCIENCE AND EMERGING\n  TECHNOLOGIES,(IJCET)- VOL1(1) DECEMBER 2017- 13-22", "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multimedia learning tools and techniques are placing its importance with\nlarge scale in education sector. With the help of multimedia learning, various\ncomplex phenomenon and theories can be explained and taught easily and\nconveniently. This project aims to teach and spread the importance of education\nand respecting the tools of education: pen, paper, pencil, rubber. To achieve\nthis cognitive learning, a 3D animated movie has been developed using\nprinciples of multimedia learning with 3D cartoon characters resembling the\nactual educational objects, where the buildings have also been modelled to\nresemble real books and diaries. For modelling and animation of these\ncharacters, polygon mesh tools are used in 3D Studio Max. Additionally, the\nfinal composition of video and audio is performed in adobe premiere. This 3D\nanimated video aims to highlight a message of importance for education and\nstationary. The Moral of movie is that do not waste your stationary material,\nuse your Pen and Paper for the purpose they are made for. To be a good citizen\nyou have to Be-Educated yourself and for that you need to give value to Pen.\nThe final rendered and composited 3D animated video reflects this moral and\nportrays the intended message with very vibrant visuals\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 21:08:50 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Bhatti", "Zeeshan", ""], ["Abro", "Ahsan", ""], ["Gillal", "Abdul Rehman", ""], ["Karbasi", "Mostafa", ""]]}, {"id": "1802.07487", "submitter": "Stephane Guinard", "authors": "Stephane Guinard and Bruno Vallet", "title": "Sensor-topology based simplicial complex reconstruction", "comments": "8 pages, 14 figures, ISPRS Technical Commission II Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for the reconstruction of simplicial complexes\n(combining points, edges and triangles) from 3D point clouds from Mobile Laser\nScanning (MLS). Our main goal is to produce a reconstruction of a scene that is\nadapted to the local geometry of objects. Our method uses the inherent topology\nof the MLS sensor to define a spatial adjacency relationship between points. We\nthen investigate each possible connexion between adjacent points and filter\nthem by searching collinear structures in the scene, or structures\nperpendicular to the laser beams. Next, we create triangles for each triplet of\nself-connected edges. Last, we improve this method with a regularization based\non the co-planarity of triangles and collinearity of remaining edges. We\ncompare our results to a naive simplicial complexes reconstruction based on\nedge length.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:00:09 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 07:27:15 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Guinard", "Stephane", ""], ["Vallet", "Bruno", ""]]}, {"id": "1802.07490", "submitter": "Shan Luo Dr", "authors": "Shan Luo, Wenzhen Yuan, Edward Adelson, Anthony G. Cohn and Raul\n  Fuentes", "title": "ViTac: Feature Sharing between Vision and Tactile Sensing for Cloth\n  Texture Recognition", "comments": "6 pages, 5 figures, Accepted for 2018 IEEE International Conference\n  on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision and touch are two of the important sensing modalities for humans and\nthey offer complementary information for sensing the environment. Robots could\nalso benefit from such multi-modal sensing ability. In this paper, addressing\nfor the first time (to the best of our knowledge) texture recognition from\ntactile images and vision, we propose a new fusion method named Deep Maximum\nCovariance Analysis (DMCA) to learn a joint latent space for sharing features\nthrough vision and tactile sensing. The features of camera images and tactile\ndata acquired from a GelSight sensor are learned by deep neural networks. But\nthe learned features are of a high dimensionality and are redundant due to the\ndifferences between the two sensing modalities, which deteriorates the\nperception performance. To address this, the learned features are paired using\nmaximum covariance analysis. Results of the algorithm on a newly collected\ndataset of paired visual and tactile data relating to cloth textures show that\na good recognition performance of greater than 90\\% can be achieved by using\nthe proposed DMCA framework. In addition, we find that the perception\nperformance of either vision or tactile sensing can be improved by employing\nthe shared representation space, compared to learning from unimodal data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 10:06:14 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 14:57:30 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Luo", "Shan", ""], ["Yuan", "Wenzhen", ""], ["Adelson", "Edward", ""], ["Cohn", "Anthony G.", ""], ["Fuentes", "Raul", ""]]}, {"id": "1802.07555", "submitter": "Evgeny Lipovetsky", "authors": "Evgeny Lipovetsky, Nira Dyn", "title": "$C^1$ analysis of 2D subdivision schemes refining point-normal pairs\n  with the circle average", "comments": "Incomplete proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article continues the investigation started in [9] on subdivision\nschemes refining 2D point-normal pairs, obtained by modifying linear\nsubdivision schemes using the circle average. While in [9] the convergence of\nthe Modified Lane-Riesenfeld algorithm and the Modified 4-Point schemes is\nproved, here we show that the curves generated by these two schemes are $C^1$.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:16:56 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 21:04:30 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Lipovetsky", "Evgeny", ""], ["Dyn", "Nira", ""]]}, {"id": "1802.07591", "submitter": "Vaclav Skala", "authors": "Vaclav Skala", "title": "Least Square Error Method Robustness of Computation: What is not usually\n  considered and taught", "comments": null, "journal-ref": null, "doi": "10.15439/978-83-946253-7-5", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many practical applications based on the Least Square Error (LSE)\napproximation. It is based on a square error minimization 'on a vertical' axis.\nThe LSE method is simple and easy also for analytical purposes. However, if\ndata span is large over several magnitudes or non-linear LSE is used, severe\nnumerical instability can be expected. The presented contribution describes a\nsimple method for large span of data LSE computation. It is especially\nconvenient if large span of data are to be processed, when the 'standard'\npseudoinverse matrix is ill conditioned. It is actually based on a LSE solution\nusing orthogonal basis vectors instead of orthonormal basis vectors. The\npresented approach has been used for a linear regression as well as for\napproximation using radial basis functions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 13:55:09 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Skala", "Vaclav", ""]]}, {"id": "1802.07592", "submitter": "Ioannis Tamvakis Mr", "authors": "Ioannis Tamvakis", "title": "\"How to squash a mathematical tomato\", Rubic's cube-like surfaces and\n  their connection to reversible computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Here we show how reversible computation processes, like Margolus diffusion,\ncan be envisioned as physical turning operations on a 2-dimensional rigid\nsurface that is cut by a regular pattern of intersecting circles. We then\nbriefly explore the design-space of these patterns, and report on the discovery\nof an interesting fractal subdivision of space by iterative circle packings. We\ndevise two different ways for creating this fractal, both showing interesting\nproperties, some resembling properties of the dragon curve. The patterns\npresented here can have interesting applications to the engineering of modular,\nkinetic, active surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:14:01 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Tamvakis", "Ioannis", ""]]}, {"id": "1802.07710", "submitter": "Wenhui Zhang", "authors": "Wenhui Zhang", "title": "Medical Volume Reconstruction Techniques", "comments": "arXiv admin note: text overlap with arXiv:1206.1148 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical visualization is the use of computers to create 3D images from\nmedical imaging data sets, almost all surgery and cancer treatment in the\ndeveloped world relies on it.Volume visualization techniques includes\niso-surface visualization, mesh visualization and point cloud visualization\ntechniques, these techniques have revolutionized medicine. Much of modern\nmedicine relies on the 3D imaging that is possible with magnetic resonance\nimaging (MRI) scanners, functional magnetic resonance imaging (fMRI)scanners,\npositron emission tomography (PET) scanners, ultrasound imaging (US) scanners,\nX-Ray scanners, bio-marker microscopy imaging scanners and computed tomography\n(CT) scanners, which make 3D images out of 2D slices. The primary goal of this\nreport is the application-oriented optimization of existing volume rendering\nmethods providing interactive frame rates. Techniques are presented for\ntraditional alpha-blending rendering, surface-shaded display, maximum intensity\nprojection (MIP), and fast previewing with fully interactive parameter control.\nDifferent preprocessing strategies are proposed for interactive iso-surface\nrendering and fast previewing, such as the well-known marching cube algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 18:33:04 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Zhang", "Wenhui", ""]]}, {"id": "1802.08022", "submitter": "Stefan Eilemann", "authors": "Stefan Eilemann, David Steiner and Renato Pajarola", "title": "Equalizer 2.0 - Convergence of a Parallel Rendering Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing complex, real world graphics applications which leverage multiple\nGPUs and computers for interactive 3D rendering tasks is a complex task. It\nrequires expertise in distributed systems and parallel rendering in addition to\nthe application domain itself. We present a mature parallel rendering framework\nwhich provides a large set of features, algorithms and system integration for a\nwide range of real-world research and industry applications. Using the\nEqualizer parallel rendering framework, we show how a wide set of generic\nalgorithms can be integrated in the framework to help application scalability\nand development in many different domains, highlighting how concrete\napplications benefit from the diverse aspects and use cases of Equalizer. We\npresent novel parallel rendering algorithms, powerful abstractions for large\nvisualization setups and virtual reality, as well as new experimental results\nfor parallel rendering and data distribution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:04:42 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Eilemann", "Stefan", ""], ["Steiner", "David", ""], ["Pajarola", "Renato", ""]]}, {"id": "1802.08091", "submitter": "Miao Wang", "authors": "Miao Wang, Guo-Ye Yang, Jin-Kun Lin, Ariel Shamir, Song-Hai Zhang,\n  Shao-Ping Lu and Shi-Min Hu", "title": "Deep Online Video Stabilization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video stabilization technique is essential for most hand-held captured videos\ndue to high-frequency shakes. Several 2D-, 2.5D- and 3D-based stabilization\ntechniques are well studied, but to our knowledge, no solutions based on deep\nneural networks had been proposed. The reason for this is mostly the shortage\nof training data, as well as the challenge of modeling the problem using neural\nnetworks. In this paper, we solve the video stabilization problem using a\nconvolutional neural network (ConvNet). Instead of dealing with offline\nholistic camera path smoothing based on feature matching, we focus on\nlow-latency real-time camera path smoothing without explicitly representing the\ncamera path. Our network, called StabNet, learns a transformation for each\ninput unsteady frame progressively along the time-line, while creating a more\nstable latent camera path. To train the network, we create a dataset of\nsynchronized steady/unsteady video pairs via a well designed hand-held\nhardware. Experimental results shows that the proposed online method (without\nusing future frames) performs comparatively to traditional offline video\nstabilization methods, while running about 30 times faster. Further, the\nproposed StabNet is able to handle night-time and blurry videos, where existing\nmethods fail in robust feature matching.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 15:12:11 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Wang", "Miao", ""], ["Yang", "Guo-Ye", ""], ["Lin", "Jin-Kun", ""], ["Shamir", "Ariel", ""], ["Zhang", "Song-Hai", ""], ["Lu", "Shao-Ping", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1802.08275", "submitter": "Hang Su", "authors": "Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji, Evangelos\n  Kalogerakis, Ming-Hsuan Yang, and Jan Kautz", "title": "SPLATNet: Sparse Lattice Networks for Point Cloud Processing", "comments": "Camera-ready, accepted to CVPR 2018 (oral); project website:\n  http://vis-www.cs.umass.edu/splatnet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a network architecture for processing point clouds that directly\noperates on a collection of points represented as a sparse set of samples in a\nhigh-dimensional lattice. Naively applying convolutions on this lattice scales\npoorly, both in terms of memory and computational cost, as the size of the\nlattice increases. Instead, our network uses sparse bilateral convolutional\nlayers as building blocks. These layers maintain efficiency by using indexing\nstructures to apply convolutions only on occupied parts of the lattice, and\nallow flexible specifications of the lattice structure enabling hierarchical\nand spatially-aware feature learning, as well as joint 2D-3D reasoning. Both\npoint-based and image-based representations can be easily incorporated in a\nnetwork with such layers and the resulting model can be trained in an\nend-to-end manner. We present results on 3D segmentation tasks where our\napproach outperforms existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 19:30:09 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 22:16:31 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 18:36:16 GMT"}, {"version": "v4", "created": "Wed, 9 May 2018 14:22:41 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Su", "Hang", ""], ["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Maji", "Subhransu", ""], ["Kalogerakis", "Evangelos", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1802.09012", "submitter": "Min Chen", "authors": "Min Chen, Kelly Gaither, Nigel W. John, and Brian McCann", "title": "Cost-benefit Analysis of Visualization in Virtual Environments", "comments": "Submitted to SciVis 2017 on 31 March 2017 and was not accepted.\n  Authors' feedback about the SciVis 2017 reviews can be found in the cover\n  letter accompanying the EuroVis submission. Revised with a major extension\n  and submitted to EuroVis 2018 on 13 December 2017, but was not accepted", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 25(1),\n  2019", "doi": "10.1109/TVCG.2018.2865025", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization and virtual environments (VEs) have been two interconnected\nparallel strands in visual computing for decades. Some VEs have been purposely\ndeveloped for visualization applications, while many visualization applications\nare exemplary showcases in general-purpose VEs. Because of the development and\noperation costs of VEs, the majority of visualization applications in practice\nare yet to benefit from the capacity of VEs. In this paper, we examine this\nperplexity from an information-theoretic perspective. Our objectives are to\nconduct cost-benefit analysis on typical VE systems (including augmented and\nmixed reality, theatre-based systems, and large powerwalls), to explain why\nsome visualization applications benefit more from VEs than others, and to\nsketch out pathways for the future development of visualization applications in\nVEs. We support our theoretical propositions and analysis using theories and\ndiscoveries in the literature of cognitive sciences and the practical evidence\nreported in the literatures of visualization and VEs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 14:14:42 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 09:30:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chen", "Min", ""], ["Gaither", "Kelly", ""], ["John", "Nigel W.", ""], ["McCann", "Brian", ""]]}, {"id": "1802.10123", "submitter": "Steffen Wiewel", "authors": "Steffen Wiewel, Moritz Becher, Nils Thuerey", "title": "Latent-space Physics: Towards Learning the Temporal Evolution of Fluid\n  Flow", "comments": "Eurographics 2019, additional materials:\n  https://ge.in.tum.de/publications/latent-space-physics/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the data-driven inference of temporal evolutions of\nphysical functions with deep learning. More specifically, we target fluid\nflows, i.e. Navier-Stokes problems, and we propose a novel LSTM-based approach\nto predict the changes of pressure fields over time. The central challenge in\nthis context is the high dimensionality of Eulerian space-time data sets. We\ndemonstrate for the first time that dense 3D+time functions of physics system\ncan be predicted within the latent spaces of neural networks, and we arrive at\na neural-network based simulation algorithm with significant practical\nspeed-ups. We highlight the capabilities of our method with a series of complex\nliquid simulations, and with a set of single-phase buoyancy simulations. With a\nset of trained networks, our method is more than two orders of magnitudes\nfaster than a traditional pressure solver. Additionally, we present and discuss\na series of detailed evaluations for the different components of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 19:18:43 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 16:04:35 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 09:58:44 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Wiewel", "Steffen", ""], ["Becher", "Moritz", ""], ["Thuerey", "Nils", ""]]}]