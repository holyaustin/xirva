[{"id": "2004.00121", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Mohamed Elgharib, Gaurav Bharaj, Florian Bernard,\n  Hans-Peter Seidel, Patrick P\\'erez, Michael Zollh\\\"ofer, Christian Theobalt", "title": "StyleRig: Rigging StyleGAN for 3D Control over Portrait Images", "comments": "CVPR 2020 (Oral). Project page:\n  https://gvv.mpi-inf.mpg.de/projects/StyleRig/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StyleGAN generates photorealistic portrait images of faces with eyes, teeth,\nhair and context (neck, shoulders, background), but lacks a rig-like control\nover semantic face parameters that are interpretable in 3D, such as face pose,\nexpressions, and scene illumination. Three-dimensional morphable face models\n(3DMMs) on the other hand offer control over the semantic parameters, but lack\nphotorealism when rendered and only model the face interior, not other parts of\na portrait image (hair, mouth interior, background). We present the first\nmethod to provide a face rig-like control over a pretrained and fixed StyleGAN\nvia a 3DMM. A new rigging network, RigNet is trained between the 3DMM's\nsemantic parameters and StyleGAN's input. The network is trained in a\nself-supervised manner, without the need for manual annotations. At test time,\nour method generates portrait images with the photorealism of StyleGAN and\nprovides explicit control over the 3D semantic parameters of the face.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 21:20:34 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 09:40:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tewari", "Ayush", ""], ["Elgharib", "Mohamed", ""], ["Bharaj", "Gaurav", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["P\u00e9rez", "Patrick", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "2004.00214", "submitter": "Boyi Jiang", "authors": "Boyi Jiang, Juyong Zhang, Yang Hong, Jinhao Luo, Ligang Liu and Hujun\n  Bao", "title": "BCNet: Learning Body and Cloth Shape from A Single Image", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem to automatically reconstruct garment\nand body shapes from a single near-front view RGB image. To this end, we\npropose a layered garment representation on top of SMPL and novelly make the\nskinning weight of garment independent of the body mesh, which significantly\nimproves the expression ability of our garment model. Compared with existing\nmethods, our method can support more garment categories and recover more\naccurate geometry. To train our model, we construct two large scale datasets\nwith ground truth body and garment geometries as well as paired color images.\nCompared with single mesh or non-parametric representation, our method can\nachieve more flexible control with separate meshes, makes applications like\nre-pose, garment transfer, and garment texture mapping possible. Code and some\ndata is available at https://github.com/jby1993/BCNet.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:41:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 10:03:24 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Jiang", "Boyi", ""], ["Zhang", "Juyong", ""], ["Hong", "Yang", ""], ["Luo", "Jinhao", ""], ["Liu", "Ligang", ""], ["Bao", "Hujun", ""]]}, {"id": "2004.00326", "submitter": "Dan Casas", "authors": "Igor Santesteban, Elena Garces, Miguel A. Otaduy, Dan Casas", "title": "SoftSMPL: Data-driven Modeling of Nonlinear Soft-tissue Dynamics for\n  Parametric Humans", "comments": "Accepted at Eurographics 2020. Project website:\n  http://dancasas.github.io/projects/SoftSMPL", "journal-ref": null, "doi": "10.1111/cgf.13912", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SoftSMPL, a learning-based method to model realistic soft-tissue\ndynamics as a function of body shape and motion. Datasets to learn such task\nare scarce and expensive to generate, which makes training models prone to\noverfitting. At the core of our method there are three key contributions that\nenable us to model highly realistic dynamics and better generalization\ncapabilities than state-of-the-art methods, while training on the same data.\nFirst, a novel motion descriptor that disentangles the standard pose\nrepresentation by removing subject-specific features; second, a\nneural-network-based recurrent regressor that generalizes to unseen shapes and\nmotions; and third, a highly efficient nonlinear deformation subspace capable\nof representing soft-tissue deformations of arbitrary shapes. We demonstrate\nqualitative and quantitative improvements over existing methods and,\nadditionally, we show the robustness of our method on a variety of motion\ncapture databases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 10:35:06 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Santesteban", "Igor", ""], ["Garces", "Elena", ""], ["Otaduy", "Miguel A.", ""], ["Casas", "Dan", ""]]}, {"id": "2004.00403", "submitter": "Mark Boss", "authors": "Mark Boss, Varun Jampani, Kihwan Kim, Hendrik P.A. Lensch, Jan Kautz", "title": "Two-shot Spatially-varying BRDF and Shape Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00404", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the shape and spatially-varying appearance (SVBRDF) of an object\nfrom images is a challenging task that has applications in both computer vision\nand graphics. Traditional optimization-based approaches often need a large\nnumber of images taken from multiple views in a controlled environment. Newer\ndeep learning-based approaches require only a few input images, but the\nreconstruction quality is not on par with optimization techniques. We propose a\nnovel deep learning architecture with a stage-wise estimation of shape and\nSVBRDF. The previous predictions guide each estimation, and a joint refinement\nnetwork later refines both SVBRDF and shape. We follow a practical mobile image\ncapture setting and use unaligned two-shot flash and no-flash images as input.\nBoth our two-shot image capture and network inference can run on mobile\nhardware. We also create a large-scale synthetic training dataset with\ndomain-randomized geometry and realistic materials. Extensive experiments on\nboth synthetic and real-world datasets show that our network trained on a\nsynthetic dataset can generalize well to real-world images. Comparisons with\nrecent approaches demonstrate the superior performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:56:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Boss", "Mark", ""], ["Jampani", "Varun", ""], ["Kim", "Kihwan", ""], ["Lensch", "Hendrik P. A.", ""], ["Kautz", "Jan", ""]]}, {"id": "2004.00452", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Tomas Simon, Jason Saragih, Hanbyul Joo", "title": "PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution\n  3D Human Digitization", "comments": "project page: https://shunsukesaito.github.io/PIFuHD", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image-based 3D human shape estimation have been driven by\nthe significant improvement in representation power afforded by deep neural\nnetworks. Although current approaches have demonstrated the potential in real\nworld settings, they still fail to produce reconstructions with the level of\ndetail often present in the input images. We argue that this limitation stems\nprimarily form two conflicting requirements; accurate predictions require large\ncontext, but precise predictions require high resolution. Due to memory\nlimitations in current hardware, previous approaches tend to take low\nresolution images as input to cover large spatial context, and produce less\nprecise (or low resolution) 3D estimates as a result. We address this\nlimitation by formulating a multi-level architecture that is end-to-end\ntrainable. A coarse level observes the whole image at lower resolution and\nfocuses on holistic reasoning. This provides context to an fine level which\nestimates highly detailed geometry by observing higher-resolution images. We\ndemonstrate that our approach significantly outperforms existing\nstate-of-the-art techniques on single image human shape reconstruction by fully\nleveraging 1k-resolution input images.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 13:52:53 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Saito", "Shunsuke", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Joo", "Hanbyul", ""]]}, {"id": "2004.00663", "submitter": "Tolga Birdal", "authors": "Tolga Birdal, Michael Arbel, Umut \\c{S}im\\c{s}ekli, and Leonidas\n  Guibas", "title": "Synchronizing Probability Measures on Rotations via Optimal Transport", "comments": "Accepted for publication at CVPR 2020, includes supplementary\n  material. Project website: https://github.com/SynchInVision/probsync", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new paradigm, $\\textit{measure synchronization}$, for\nsynchronizing graphs with measure-valued edges. We formulate this problem as\nmaximization of the cycle-consistency in the space of probability measures over\nrelative rotations. In particular, we aim at estimating marginal distributions\nof absolute orientations by synchronizing the $\\textit{conditional}$ ones,\nwhich are defined on the Riemannian manifold of quaternions. Such graph\noptimization on distributions-on-manifolds enables a natural treatment of\nmultimodal hypotheses, ambiguities and uncertainties arising in many computer\nvision applications such as SLAM, SfM, and object pose estimation. We first\nformally define the problem as a generalization of the classical rotation graph\nsynchronization, where in our case the vertices denote probability measures\nover rotations. We then measure the quality of the synchronization by using\nSinkhorn divergences, which reduces to other popular metrics such as\nWasserstein distance or the maximum mean discrepancy as limit cases. We propose\na nonparametric Riemannian particle optimization approach to solve the problem.\nEven though the problem is non-convex, by drawing a connection to the recently\nproposed sparse optimization methods, we show that the proposed algorithm\nconverges to the global optimum in a special case of the problem under certain\nconditions. Our qualitative and quantitative experiments show the validity of\nour approach and we bring in new perspectives to the study of synchronization.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:44:18 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Birdal", "Tolga", ""], ["Arbel", "Michael", ""], ["\u015eim\u015fekli", "Umut", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2004.01228", "submitter": "Mikaela Angelina Uy", "authors": "Mikaela Angelina Uy and Jingwei Huang and Minhyuk Sung and Tolga\n  Birdal and Leonidas Guibas", "title": "Deformation-Aware 3D Model Embedding and Retrieval", "comments": "Accepted for publication at ECCV 2020. Project page under\n  https://deformscan2cad.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new problem of retrieving 3D models that are deformable to a\ngiven query shape and present a novel deep deformation-aware embedding to solve\nthis retrieval task. 3D model retrieval is a fundamental operation for\nrecovering a clean and complete 3D model from a noisy and partial 3D scan.\nHowever, given a finite collection of 3D shapes, even the closest model to a\nquery may not be satisfactory. This motivates us to apply 3D model deformation\ntechniques to adapt the retrieved model so as to better fit the query. Yet,\ncertain restrictions are enforced in most 3D deformation techniques to preserve\nimportant features of the original model that prevent a perfect fitting of the\ndeformed model to the query. This gap between the deformed model and the query\ninduces asymmetric relationships among the models, which cannot be handled by\ntypical metric learning techniques. Thus, to retrieve the best models for\nfitting, we propose a novel deep embedding approach that learns the asymmetric\nrelationships by leveraging location-dependent egocentric distance fields. We\nalso propose two strategies for training the embedding network. We demonstrate\nthat both of these approaches outperform other baselines in our experiments\nwith both synthetic and real data. Our project page can be found at\nhttps://deformscan2cad.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 19:10:57 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 02:38:26 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2020 05:10:25 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Uy", "Mikaela Angelina", ""], ["Huang", "Jingwei", ""], ["Sung", "Minhyuk", ""], ["Birdal", "Tolga", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2004.01661", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Maks Ovsjanikov", "title": "Intrinsic Point Cloud Interpolation via Dual Latent Space Navigation", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for interpolating and manipulating 3D\nshapes represented as point clouds, that is explicitly designed to preserve\nintrinsic shape properties. Our approach is based on constructing a dual\nencoding space that enables shape synthesis and, at the same time, provides\nlinks to the intrinsic shape information, which is typically not available on\npoint cloud data. Our method works in a single pass and avoids expensive\noptimization, employed by existing techniques. Furthermore, the strong\nregularization provided by our dual latent space approach also helps to improve\nshape recovery in challenging settings from noisy point clouds across different\ndatasets. Extensive experiments show that our method results in more realistic\nand smoother interpolations compared to baselines.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 16:28:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2004.02460", "submitter": "Zhe Li", "authors": "Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin Liu", "title": "Robust 3D Self-portraits in Seconds", "comments": "CVPR 2020, oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient method for robust 3D self-portraits\nusing a single RGBD camera. Benefiting from the proposed PIFusion and\nlightweight bundle adjustment algorithm, our method can generate detailed 3D\nself-portraits in seconds and shows the ability to handle subjects wearing\nextremely loose clothes. To achieve highly efficient and robust reconstruction,\nwe propose PIFusion, which combines learning-based 3D recovery with volumetric\nnon-rigid fusion to generate accurate sparse partial scans of the subject.\nMoreover, a non-rigid volumetric deformation method is proposed to continuously\nrefine the learned shape prior. Finally, a lightweight bundle adjustment\nalgorithm is proposed to guarantee that all the partial scans can not only\n\"loop\" with each other but also remain consistent with the selected live key\nobservations. The results and experiments show that the proposed method\nachieves more robust and efficient 3D self-portraits compared with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:02:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Li", "Zhe", ""], ["Yu", "Tao", ""], ["Pan", "Chuanyu", ""], ["Zheng", "Zerong", ""], ["Liu", "Yebin", ""]]}, {"id": "2004.02546", "submitter": "Erik H\\\"ark\\\"onen", "authors": "Erik H\\\"ark\\\"onen, Aaron Hertzmann, Jaakko Lehtinen, Sylvain Paris", "title": "GANSpace: Discovering Interpretable GAN Controls", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple technique to analyze Generative Adversarial\nNetworks (GANs) and create interpretable controls for image synthesis, such as\nchange of viewpoint, aging, lighting, and time of day. We identify important\nlatent directions based on Principal Components Analysis (PCA) applied either\nin latent space or feature space. Then, we show that a large number of\ninterpretable controls can be defined by layer-wise perturbation along the\nprincipal directions. Moreover, we show that BigGAN can be controlled with\nlayer-wise inputs in a StyleGAN-like manner. We show results on different GANs\ntrained on various datasets, and demonstrate good qualitative matches to edit\ndirections found through earlier supervised approaches.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:41:44 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 11:10:27 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 10:13:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["H\u00e4rk\u00f6nen", "Erik", ""], ["Hertzmann", "Aaron", ""], ["Lehtinen", "Jaakko", ""], ["Paris", "Sylvain", ""]]}, {"id": "2004.02711", "submitter": "Bernhard Egger", "authors": "William A.P. Smith, Alassane Seck, Hannah Dee, Bernard Tiddeman,\n  Joshua Tenenbaum, Bernhard Egger", "title": "A Morphable Face Albedo Model", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we bring together two divergent strands of research:\nphotometric face capture and statistical 3D face appearance modelling. We\npropose a novel lightstage capture and processing pipeline for acquiring\near-to-ear, truly intrinsic diffuse and specular albedo maps that fully factor\nout the effects of illumination, camera and geometry. Using this pipeline, we\ncapture a dataset of 50 scans and combine them with the only existing publicly\navailable albedo dataset (3DRFE) of 23 scans. This allows us to build the first\nmorphable face albedo model. We believe this is the first statistical analysis\nof the variability of facial specular albedo maps. This model can be used as a\nplug in replacement for the texture model of the Basel Face Model (BFM) or\nFLAME and we make the model publicly available. We ensure careful spectral\ncalibration such that our model is built in a linear sRGB space, suitable for\ninverse rendering of images taken by typical cameras. We demonstrate our model\nin a state of the art analysis-by-synthesis 3DMM fitting pipeline, are the\nfirst to integrate specular map estimation and outperform the BFM in albedo\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:49:40 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:39:22 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Smith", "William A. P.", ""], ["Seck", "Alassane", ""], ["Dee", "Hannah", ""], ["Tiddeman", "Bernard", ""], ["Tenenbaum", "Joshua", ""], ["Egger", "Bernhard", ""]]}, {"id": "2004.02867", "submitter": "Dongdong Chen", "authors": "Zhentao Tan, Dongdong Chen, Qi Chu, Menglei Chai, Jing Liao, Mingming\n  He, Lu Yuan, Nenghai Yu", "title": "Rethinking Spatially-Adaptive Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-adaptive normalization is remarkably successful recently in\nconditional semantic image synthesis, which modulates the normalized activation\nwith spatially-varying transformations learned from semantic layouts, to\npreserve the semantic information from being washed away. Despite its\nimpressive performance, a more thorough understanding of the true advantages\ninside the box is still highly demanded, to help reduce the significant\ncomputation and parameter overheads introduced by these new structures. In this\npaper, from a return-on-investment point of view, we present a deep analysis of\nthe effectiveness of SPADE and observe that its advantages actually come mainly\nfrom its semantic-awareness rather than the spatial-adaptiveness. Inspired by\nthis point, we propose class-adaptive normalization (CLADE), a lightweight\nvariant that is not adaptive to spatial positions or layouts. Benefited from\nthis design, CLADE greatly reduces the computation cost while still being able\nto preserve the semantic information during the generation. Extensive\nexperiments on multiple challenging datasets demonstrate that while the\nresulting fidelity is on par with SPADE, its overhead is much cheaper than\nSPADE. Take the generator for ADE20k dataset as an example, the extra parameter\nand computation cost introduced by CLADE are only 4.57% and 0.07% while that of\nSPADE are 39.21% and 234.73% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:58:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Tan", "Zhentao", ""], ["Chen", "Dongdong", ""], ["Chu", "Qi", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""], ["He", "Mingming", ""], ["Yuan", "Lu", ""], ["Yu", "Nenghai", ""]]}, {"id": "2004.03028", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Giorgio Gori, Duygu Ceylan, Radomir Mech, Nathan\n  Carr, Tamy Boubekeur, Rui Wang, Subhransu Maji", "title": "Learning Generative Models of Shape Handles", "comments": "11 pages, 11 figures, accepted do CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative model to synthesize 3D shapes as sets of handles --\nlightweight proxies that approximate the original 3D shape -- for applications\nin interactive editing, shape parsing, and building compact 3D representations.\nOur model can generate handle sets with varying cardinality and different types\nof handles (Figure 1). Key to our approach is a deep architecture that predicts\nboth the parameters and existence of shape handles, and a novel similarity\nmeasure that can easily accommodate different types of handles, such as cuboids\nor sphere-meshes. We leverage the recent advances in semantic 3D annotation as\nwell as automatic shape summarizing techniques to supervise our approach. We\nshow that the resulting shape representations are intuitive and achieve\nsuperior quality than previous state-of-the-art. Finally, we demonstrate how\nour method can be used in applications such as interactive shape editing,\ncompletion, and interpolation, leveraging the latent space learned by our model\nto guide these tasks. Project page: http://mgadelha.me/shapehandles.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:35:55 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Gadelha", "Matheus", ""], ["Gori", "Giorgio", ""], ["Ceylan", "Duygu", ""], ["Mech", "Radomir", ""], ["Carr", "Nathan", ""], ["Boubekeur", "Tamy", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2004.03179", "submitter": "Seiichi Uchida", "authors": "Takuro Karamatsu, Gibran Benitez-Garcia, Keiji Yanai, Seiichi Uchida", "title": "Iconify: Converting Photographs into Icons", "comments": "to appear at 2020 Joint Workshop on Multimedia Artworks Analysis and\n  Attractiveness Computing in Multimedia (MMArt-ACM'20)", "journal-ref": null, "doi": "10.1145/3379173.3393708", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle a challenging domain conversion task between photo\nand icon images. Although icons often originate from real object images (i.e.,\nphotographs), severe abstractions and simplifications are applied to generate\nicon images by professional graphic designers. Moreover, there is no one-to-one\ncorrespondence between the two domains, for this reason we cannot use it as the\nground-truth for learning a direct conversion function. Since generative\nadversarial networks (GAN) can undertake the problem of domain conversion\nwithout any correspondence, we test CycleGAN and UNIT to generate icons from\nobjects segmented from photo images. Our experiments with several image\ndatasets prove that CycleGAN learns sufficient abstraction and simplification\nability to generate icon-like images.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:01:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Karamatsu", "Takuro", ""], ["Benitez-Garcia", "Gibran", ""], ["Yanai", "Keiji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "2004.03450", "submitter": "Chenming Wu", "authors": "Chenming Wu, Yong-Jin Liu, Charlie C.L. Wang", "title": "Learning to Accelerate Decomposition for Multi-Directional 3D Printing", "comments": "8 pages, accepted by IEEE Robotics and Automation Letters 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-directional 3D printing has the capability of decreasing or eliminating\nthe need for support structures. Recent work proposed a beam-guided search\nalgorithm to find an optimized sequence of plane-clipping, which gives volume\ndecomposition of a given 3D model. Different printing directions are employed\nin different regions to fabricate a model with tremendously less support (or\neven no support in many cases).To obtain optimized decomposition, a large beam\nwidth needs to be used in the search algorithm, leading to a very\ntime-consuming computation. In this paper, we propose a learning framework that\ncan accelerate the beam-guided search by using a smaller number of the original\nbeam width to obtain results with similar quality. Specifically, we use the\nresults of beam-guided search with large beam width to train a scoring function\nfor candidate clipping planes based on six newly proposed feature metrics. With\nthe help of these feature metrics, both the current and the sequence-dependent\ninformation are captured by the neural network to score candidates of clipping.\nAs a result, we can achieve around 3x computational speed. We test and\ndemonstrate our accelerated decomposition on a large dataset of models for 3D\nprinting.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 18:37:44 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 06:00:49 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 04:50:55 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wu", "Chenming", ""], ["Liu", "Yong-Jin", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "2004.03590", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Tianhao Zhang, Jitendra Malik", "title": "Multimodal Image Synthesis with Conditional Implicit Maximum Likelihood\n  Estimation", "comments": "To appear in International Journal of Computer Vision (IJCV). arXiv\n  admin note: text overlap with arXiv:1811.12373", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in computer vision and graphics fall within the framework of\nconditional image synthesis. In recent years, generative adversarial nets\n(GANs) have delivered impressive advances in quality of synthesized images.\nHowever, it remains a challenge to generate both diverse and plausible images\nfor the same input, due to the problem of mode collapse. In this paper, we\ndevelop a new generic multimodal conditional image synthesis method based on\nImplicit Maximum Likelihood Estimation (IMLE) and demonstrate improved\nmultimodal image synthesis performance on two tasks, single image\nsuper-resolution and image synthesis from scene layouts. We make our\nimplementation publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 03:06:55 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "2004.03805", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Ohad Fried, Justus Thies, Vincent Sitzmann, Stephen\n  Lombardi, Kalyan Sunkavalli, Ricardo Martin-Brualla, Tomas Simon, Jason\n  Saragih, Matthias Nie{\\ss}ner, Rohit Pandey, Sean Fanello, Gordon Wetzstein,\n  Jun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli Shechtman, Dan B\n  Goldman, Michael Zollh\\\"ofer", "title": "State of the Art on Neural Rendering", "comments": "Eurographics 2020 survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient rendering of photo-realistic virtual worlds is a long standing\neffort of computer graphics. Modern graphics techniques have succeeded in\nsynthesizing photo-realistic images from hand-crafted scene representations.\nHowever, the automatic generation of shape, materials, lighting, and other\naspects of scenes remains a challenging problem that, if solved, would make\nphoto-realistic computer graphics more widely accessible. Concurrently,\nprogress in computer vision and machine learning have given rise to a new\napproach to image synthesis and editing, namely deep generative models. Neural\nrendering is a new and rapidly emerging field that combines generative machine\nlearning techniques with physical knowledge from computer graphics, e.g., by\nthe integration of differentiable rendering into network training. With a\nplethora of applications in computer graphics and vision, neural rendering is\npoised to become a new area in the graphics community, yet no survey of this\nemerging field exists. This state-of-the-art report summarizes the recent\ntrends and applications of neural rendering. We focus on approaches that\ncombine classic computer graphics techniques with deep generative models to\nobtain controllable and photo-realistic outputs. Starting with an overview of\nthe underlying computer graphics and machine learning concepts, we discuss\ncritical aspects of neural rendering approaches. This state-of-the-art report\nis focused on the many important use cases for the described algorithms such as\nnovel view synthesis, semantic photo manipulation, facial and body reenactment,\nrelighting, free-viewpoint video, and the creation of photo-realistic avatars\nfor virtual and augmented reality telepresence. Finally, we conclude with a\ndiscussion of the social implications of such technology and investigate open\nresearch problems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 04:36:31 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Tewari", "Ayush", ""], ["Fried", "Ohad", ""], ["Thies", "Justus", ""], ["Sitzmann", "Vincent", ""], ["Lombardi", "Stephen", ""], ["Sunkavalli", "Kalyan", ""], ["Martin-Brualla", "Ricardo", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Nie\u00dfner", "Matthias", ""], ["Pandey", "Rohit", ""], ["Fanello", "Sean", ""], ["Wetzstein", "Gordon", ""], ["Zhu", "Jun-Yan", ""], ["Theobalt", "Christian", ""], ["Agrawala", "Maneesh", ""], ["Shechtman", "Eli", ""], ["Goldman", "Dan B", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "2004.04242", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Rui Wang, Subhransu Maji", "title": "Deep Manifold Prior", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prior for manifold structured data, such as surfaces of 3D\nshapes, where deep neural networks are adopted to reconstruct a target shape\nusing gradient descent starting from a random initialization. We show that\nsurfaces generated this way are smooth, with limiting behavior characterized by\nGaussian processes, and we mathematically derive such properties for\nfully-connected as well as convolutional networks. We demonstrate our method in\na variety of manifold reconstruction applications, such as point cloud\ndenoising and interpolation, achieving considerably better results against\ncompetitive baselines while requiring no training data. We also show that when\ntraining data is available, our method allows developing alternate\nparametrizations of surfaces under the framework of AtlasNet, leading to a\ncompact network architecture and better reconstruction results on standard\nimage to shape reconstruction benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 20:47:56 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2004.04351", "submitter": "Lan Chen", "authors": "Lan Chen, Juntao Ye, Xiaopeng Zhang", "title": "Multi-feature super-resolution network for cloth wrinkle synthesis", "comments": null, "journal-ref": "JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 36(3): 478-493 May\n  2021. DOI 10.1007/s11390-021-1331-y", "doi": "10.1007/s11390-021-1331-y", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing physical cloth simulators suffer from expensive computation and\ndifficulties in tuning mechanical parameters to get desired wrinkling\nbehaviors. Data-driven methods provide an alternative solution. It typically\nsynthesizes cloth animation at a much lower computational cost, and also\ncreates wrinkling effects that highly resemble the much controllable training\ndata. In this paper we propose a deep learning based method for synthesizing\ncloth animation with high resolution meshes. To do this we first create a\ndataset for training: a pair of low and high resolution meshes are simulated\nand their motions are synchronized. As a result the two meshes exhibit similar\nlarge-scale deformation but different small wrinkles. Each simulated mesh pair\nare then converted into a pair of low and high resolution \"images\" (a 2D array\nof samples), with each sample can be interpreted as any of three features: the\ndisplacement, the normal and the velocity. With these image pairs, we design a\nmulti-feature super-resolution (MFSR) network that jointly train an upsampling\nsynthesizer for the three features. The MFSR architecture consists of two key\ncomponents: a sharing module that takes multiple features as input to learn\nlow-level representations from corresponding super-resolution tasks\nsimultaneously; and task-specific modules focusing on various high-level\nsemantics. Frame-to-frame consistency is well maintained thanks to the proposed\nkinematics-based loss function. Our method achieves realistic results at high\nframe rates: 12-14 times faster than traditional physical simulation. We\ndemonstrate the performance of our method with various experimental scenes,\nincluding a dressed character with sophisticated collisions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 03:37:57 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 09:27:46 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Chen", "Lan", ""], ["Ye", "Juntao", ""], ["Zhang", "Xiaopeng", ""]]}, {"id": "2004.04572", "submitter": "Tony Tung", "authors": "Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung", "title": "ARCH: Animatable Reconstruction of Clothed Humans", "comments": "10 pages, 10 figures, CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose ARCH (Animatable Reconstruction of Clothed Humans),\na novel end-to-end framework for accurate reconstruction of animation-ready 3D\nclothed humans from a monocular image. Existing approaches to digitize 3D\nhumans struggle to handle pose variations and recover details. Also, they do\nnot produce models that are animation ready. In contrast, ARCH is a learned\npose-aware model that produces detailed 3D rigged full-body human avatars from\na single unconstrained RGB image. A Semantic Space and a Semantic Deformation\nField are created using a parametric 3D body estimator. They allow the\ntransformation of 2D/3D clothed humans into a canonical space, reducing\nambiguities in geometry caused by pose variations and occlusions in training\ndata. Detailed surface geometry and appearance are learned using an implicit\nfunction representation with spatial local features. Furthermore, we propose\nadditional per-pixel supervision on the 3D reconstruction using opacity-aware\ndifferentiable rendering. Our experiments indicate that ARCH increases the\nfidelity of the reconstructed humans. We obtain more than 50% lower\nreconstruction errors for standard metrics compared to state-of-the-art methods\non public datasets. We also show numerous qualitative examples of animated,\nhigh-quality reconstructed avatars unseen in the literature so far.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 14:23:08 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 19:14:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Huang", "Zeng", ""], ["Xu", "Yuanlu", ""], ["Lassner", "Christoph", ""], ["Li", "Hao", ""], ["Tung", "Tony", ""]]}, {"id": "2004.04977", "submitter": "Evangelos Ntavelis", "authors": "Evangelos Ntavelis, Andr\\'es Romero, Iason Kastanis, Luc Van Gool and\n  Radu Timofte", "title": "SESAME: Semantic Editing of Scenes by Adding, Manipulating or Erasing\n  Objects", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58542-6_24", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image generation gave rise to powerful tools for semantic\nimage editing. However, existing approaches can either operate on a single\nimage or require an abundance of additional information. They are not capable\nof handling the complete set of editing operations, that is addition,\nmanipulation or removal of semantic concepts. To address these limitations, we\npropose SESAME, a novel generator-discriminator pair for Semantic Editing of\nScenes by Adding, Manipulating or Erasing objects. In our setup, the user\nprovides the semantic labels of the areas to be edited and the generator\nsynthesizes the corresponding pixels. In contrast to previous methods that\nemploy a discriminator that trivially concatenates semantics and image as an\ninput, the SESAME discriminator is composed of two input streams that\nindependently process the image and its semantics, using the latter to\nmanipulate the results of the former. We evaluate our model on a diverse set of\ndatasets and report state-of-the-art performance on two tasks: (a) image\nmanipulation and (b) image generation conditioned on semantic labels.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:19:19 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 14:52:01 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ntavelis", "Evangelos", ""], ["Romero", "Andr\u00e9s", ""], ["Kastanis", "Iason", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2004.05571", "submitter": "Pan Zhang", "authors": "Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, Fang Wen", "title": "Cross-domain Correspondence Learning for Exemplar-based Image\n  Translation", "comments": "Accepted as a CVPR 2020 oral paper", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for exemplar-based image translation, which\nsynthesizes a photo-realistic image from the input in a distinct domain (e.g.,\nsemantic segmentation mask, or edge map, or pose keypoints), given an exemplar\nimage. The output has the style (e.g., color, texture) in consistency with the\nsemantically corresponding objects in the exemplar. We propose to jointly learn\nthe crossdomain correspondence and the image translation, where both tasks\nfacilitate each other and thus can be learned with weak supervision. The images\nfrom distinct domains are first aligned to an intermediate domain where dense\ncorrespondence is established. Then, the network synthesizes images based on\nthe appearance of semantically corresponding patches in the exemplar. We\ndemonstrate the effectiveness of our approach in several image translation\ntasks. Our method is superior to state-of-the-art methods in terms of image\nquality significantly, with the image style faithful to the exemplar with\nsemantic consistency. Moreover, we show the utility of our method for several\napplications\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 09:10:57 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Zhang", "Pan", ""], ["Zhang", "Bo", ""], ["Chen", "Dong", ""], ["Yuan", "Lu", ""], ["Wen", "Fang", ""]]}, {"id": "2004.05679", "submitter": "Qian Xie", "authors": "Qian Xie, Yu-Kun Lai, Jing Wu, Zhoutao Wang, Yiming Zhang, Kai Xu, Jun\n  Wang", "title": "MLCVNet: Multi-Level Context VoteNet for 3D Object Detection", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the 3D object detection task by capturing\nmulti-level contextual information with the self-attention mechanism and\nmulti-scale feature fusion. Most existing 3D object detection methods recognize\nobjects individually, without giving any consideration on contextual\ninformation between these objects. Comparatively, we propose Multi-Level\nContext VoteNet (MLCVNet) to recognize 3D objects correlatively, building on\nthe state-of-the-art VoteNet. We introduce three context modules into the\nvoting and classifying stages of VoteNet to encode contextual information at\ndifferent levels. Specifically, a Patch-to-Patch Context (PPC) module is\nemployed to capture contextual information between the point patches, before\nvoting for their corresponding object centroid points. Subsequently, an\nObject-to-Object Context (OOC) module is incorporated before the proposal and\nclassification stage, to capture the contextual information between object\ncandidates. Finally, a Global Scene Context (GSC) module is designed to learn\nthe global scene context. We demonstrate these by capturing contextual\ninformation at patch, object and scene levels. Our method is an effective way\nto promote detection accuracy, achieving new state-of-the-art detection\nperformance on challenging 3D object detection datasets, i.e., SUN RGBD and\nScanNet. We also release our code at https://github.com/NUAAXQ/MLCVNet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 19:10:24 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Xie", "Qian", ""], ["Lai", "Yu-Kun", ""], ["Wu", "Jing", ""], ["Wang", "Zhoutao", ""], ["Zhang", "Yiming", ""], ["Xu", "Kai", ""], ["Wang", "Jun", ""]]}, {"id": "2004.05980", "submitter": "Timothy Jeruzalski", "authors": "Timothy Jeruzalski, David I.W. Levin, Alec Jacobson, Paul Lalonde,\n  Mohammad Norouzi, Andrea Tagliasacchi", "title": "NiLBS: Neural Inverse Linear Blend Skinning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we investigate efficient representations of\narticulated objects (e.g. human bodies), which is an important problem in\ncomputer vision and graphics. To deform articulated geometry, existing\napproaches represent objects as meshes and deform them using \"skinning\"\ntechniques. The skinning operation allows a wide range of deformations to be\nachieved with a small number of control parameters. This paper introduces a\nmethod to invert the deformations undergone via traditional skinning techniques\nvia a neural network parameterized by pose. The ability to invert these\ndeformations allows values (e.g., distance function, signed distance function,\noccupancy) to be pre-computed at rest pose, and then efficiently queried when\nthe character is deformed. We leave empirical evaluation of our approach to\nfuture work.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:46:37 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jeruzalski", "Timothy", ""], ["Levin", "David I. W.", ""], ["Jacobson", "Alec", ""], ["Lalonde", "Paul", ""], ["Norouzi", "Mohammad", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2004.06718", "submitter": "Zhang Qian", "authors": "Zhang Qian, Wang Bo, Wen Wei, Li Hai, Liu Jun Hui", "title": "Line Art Correlation Matching Feature Transfer Network for Automatic\n  Animation Colorization", "comments": "8pages,6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic animation line art colorization is a challenging computer vision\nproblem, since the information of the line art is highly sparse and abstracted\nand there exists a strict requirement for the color and style consistency\nbetween frames. Recently, a lot of Generative Adversarial Network (GAN) based\nimage-to-image translation methods for single line art colorization have\nemerged. They can generate perceptually appealing results conditioned on line\nart images. However, these methods can not be adopted for the purpose of\nanimation colorization because there is a lack of consideration of the\nin-between frame consistency. Existing methods simply input the previous\ncolored frame as a reference to color the next line art, which will mislead the\ncolorization due to the spatial misalignment of the previous colored frame and\nthe next line art especially at positions where apparent changes happen. To\naddress these challenges, we design a kind of correlation matching feature\ntransfer model (called CMFT) to align the colored reference feature in a\nlearnable way and integrate the model into an U-Net based generator in a\ncoarse-to-fine manner. This enables the generator to transfer the layer-wise\nsynchronized features from the deep semantic code to the content progressively.\nExtension evaluation shows that CMFT model can effectively improve the\nin-between consistency and the quality of colored frames especially when the\nmotion is intense and diverse.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 06:50:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 13:21:23 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 09:03:16 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Qian", "Zhang", ""], ["Bo", "Wang", ""], ["Wei", "Wen", ""], ["Hai", "Li", ""], ["Hui", "Liu Jun", ""]]}, {"id": "2004.06848", "submitter": "Kyle Olszewski", "authors": "Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen,\n  Weikai Chen, Hao Li", "title": "Intuitive, Interactive Beard and Hair Synthesis with Generative Models", "comments": "To be presented in the 2020 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020, Oral Presentation). Supplementary video can be seen\n  at: https://www.youtube.com/watch?v=v4qOtBATrvM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive approach to synthesizing realistic variations in\nfacial hair in images, ranging from subtle edits to existing hair to the\naddition of complex and challenging hair in images of clean-shaven subjects. To\ncircumvent the tedious and computationally expensive tasks of modeling,\nrendering and compositing the 3D geometry of the target hairstyle using the\ntraditional graphics pipeline, we employ a neural network pipeline that\nsynthesizes realistic and detailed images of facial hair directly in the target\nimage in under one second. The synthesis is controlled by simple and sparse\nguide strokes from the user defining the general structural and color\nproperties of the target hairstyle. We qualitatively and quantitatively\nevaluate our chosen method compared to several alternative approaches. We show\ncompelling interactive editing results with a prototype user interface that\nallows novice users to progressively refine the generated image to match their\ndesired hairstyle, and demonstrate that our approach also allows for flexible\nand high-fidelity scalp hair synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 01:20:10 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Olszewski", "Kyle", ""], ["Ceylan", "Duygu", ""], ["Xing", "Jun", ""], ["Echevarria", "Jose", ""], ["Chen", "Zhili", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "2004.07016", "submitter": "He Wang", "authors": "Zheyan Zhang, Yongxing Wang, Peter K. Jimack, and He Wang", "title": "MeshingNet: A New Mesh Generation Method based on Deep Learning", "comments": "Accepted in International Conference on Computational Science 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.LG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to automatic unstructured mesh generation using\nmachine learning to predict an optimal finite element mesh for a previously\nunseen problem. The framework that we have developed is based around training\nan artificial neural network (ANN) to guide standard mesh generation software,\nbased upon a prediction of the required local mesh density throughout the\ndomain. We describe the training regime that is proposed, based upon the use of\n\\emph{a posteriori} error estimation, and discuss the topologies of the ANNs\nthat we have considered. We then illustrate performance using two standard test\nproblems, a single elliptic partial differential equation (PDE) and a system of\nPDEs associated with linear elasticity. We demonstrate the effective generation\nof high quality meshes for arbitrary polygonal geometries and a range of\nmaterial parameters, using a variety of user-selected error norms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:29:00 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Zhang", "Zheyan", ""], ["Wang", "Yongxing", ""], ["Jimack", "Peter K.", ""], ["Wang", "He", ""]]}, {"id": "2004.07414", "submitter": "Jungtaek Kim", "authors": "Jungtaek Kim, Hyunsoo Chung, Jinhwi Lee, Minsu Cho, Jaesik Park", "title": "Combinatorial 3D Shape Generation via Sequential Assembly", "comments": "14 pages, 20 figures, 1 table, presented at NeurIPS 2020 Workshop on\n  Machine Learning for Engineering Modeling, Simulation, and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential assembly with geometric primitives has drawn attention in robotics\nand 3D vision since it yields a practical blueprint to construct a target\nshape. However, due to its combinatorial property, a greedy method falls short\nof generating a sequence of volumetric primitives. To alleviate this\nconsequence induced by a huge number of feasible combinations, we propose a\ncombinatorial 3D shape generation framework. The proposed framework reflects an\nimportant aspect of human generation processes in real life -- we often create\na 3D shape by sequentially assembling unit primitives with geometric\nconstraints. To find the desired combination regarding combination evaluations,\nwe adopt Bayesian optimization, which is able to exploit and explore\nefficiently the feasible regions constrained by the current primitive\nplacements. An evaluation function conveys global structure guidance for an\nassembly process and stability in terms of gravity and external forces\nsimultaneously. Experimental results demonstrate that our method successfully\ngenerates combinatorial 3D shapes and simulates more realistic generation\nprocesses. We also introduce a new dataset for combinatorial 3D shape\ngeneration. All the codes are available at\n\\url{https://github.com/POSTECH-CVLab/Combinatorial-3D-Shape-Generation}.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 01:23:14 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 03:51:49 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Kim", "Jungtaek", ""], ["Chung", "Hyunsoo", ""], ["Lee", "Jinhwi", ""], ["Cho", "Minsu", ""], ["Park", "Jaesik", ""]]}, {"id": "2004.07484", "submitter": "Christoph Lassner", "authors": "Christoph Lassner, Michael Zollh\\\"ofer", "title": "Pulsar: Efficient Sphere-based Neural Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Pulsar, an efficient sphere-based differentiable renderer that is\norders of magnitude faster than competing techniques, modular, and easy-to-use\ndue to its tight integration with PyTorch. Differentiable rendering is the\nfoundation for modern neural rendering approaches, since it enables end-to-end\ntraining of 3D scene representations from image observations. However,\ngradient-based optimization of neural mesh, voxel, or function representations\nsuffers from multiple challenges, i.e., topological inconsistencies, high\nmemory footprints, or slow rendering speeds. To alleviate these problems,\nPulsar employs: 1) a sphere-based scene representation, 2) an efficient\ndifferentiable rendering engine, and 3) neural shading. Pulsar executes orders\nof magnitude faster than existing techniques and allows real-time rendering and\noptimization of representations with millions of spheres. Using spheres for the\nscene representation, unprecedented speed is obtained while avoiding topology\nproblems. Pulsar is fully differentiable and thus enables a plethora of\napplications, ranging from 3D reconstruction to general neural rendering.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 06:57:26 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 17:53:52 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 06:30:56 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Lassner", "Christoph", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "2004.08475", "submitter": "Ingo Wald", "authors": "Ingo Wald", "title": "A Simple, General, and GPU Friendly Method for Computing Dual Mesh and\n  Iso-Surfaces of Adaptive Mesh Refinement (AMR) Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to extracting crack-free iso-surfaces from\nStructured AMR data that is more general than previous techniques, is trivially\nsimple to implement, requires no information other than the list of AMR cells,\nand works, in particular, for different AMR formats including octree AMR,\nblock-structured AMR with arbitrary level differences at level boundaries, and\nAMR data that consist of individual cells without any existing grid structure.\nWe describe both the technique itself and a CUDA-based GPU implementation of\nthis technique, and evaluate it on several non-trivial AMR data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:41:01 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wald", "Ingo", ""]]}, {"id": "2004.09038", "submitter": "Pengbo Bo", "authors": "Zixuan Hu and Pengbo Bo", "title": "Developable B-spline surface generation from control rulings", "comments": "13 pages, 12 figrues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intuitive design method is proposed for generating developable ruled\nB-spline surfaces from a sequence of straight line segments indicating the\nsurface shape. The first and last line segments are enforced to be the head and\ntail ruling lines of the resulting surface while the interior lines are\nrequired to approximate rulings on the resulting surface as much as possible.\nThis manner of developable surface design is conceptually similar to the\npopular way of the freeform curve and surface design in the CAD community,\nobserving that a developable ruled surface is a single parameter family of\nstraight lines. This new design mode of the developable surface also provides\nmore flexibility than the widely employed way of developable surface design\nfrom two boundary curves of the surface. The problem is treated by numerical\noptimization methods with which a particular level of distance error is\nallowed. We thus provide an effective tool for creating surfaces with a high\ndegree of developability when the input control rulings do not lie in exact\ndevelopable surfaces. We consider this ability as the superiority over\nanalytical methods in that it can deal with arbitrary design inputs and find\npractically useful results.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 03:22:23 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 04:19:05 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Hu", "Zixuan", ""], ["Bo", "Pengbo", ""]]}, {"id": "2004.09136", "submitter": "Qiang Zou", "authors": "Qiang Zou", "title": "Robust and efficient tool path generation for poor-quality triangular\n  mesh surface machining", "comments": "10 pages, 9 figures, Under journal review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to generate iso-scallop tool paths for\ntriangular mesh surfaces. With the popularity of 3D scanning techniques,\nscanning-derived mesh surfaces have seen a significant increase in their\napplication to machining. Quite often, such mesh surfaces exhibit defects such\nas noises, which differentiate them from the good-quality mesh surfaces\nprevious research work focuses on. To generate tool paths for such poor-quality\nmesh surfaces, the primary challenge lies in robustness against the defects. In\nthis work, a robust tool path generation method is proposed for poor-quality\nmesh surfaces. In addition to robustness, the method is quite efficient,\nproviding the benefit of faster iterations and improved integration between\nscanning and machining. The fundamental principle of the method is to convert\nthe tool path generation problem to the heat diffusion problem that has robust\nand efficient algorithms available. The effectiveness of the method will be\ndemonstrated by a series of case studies and comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 08:58:50 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zou", "Qiang", ""]]}, {"id": "2004.09169", "submitter": "Andrei-Timotei Ardelean", "authors": "Andrei-Timotei Ardelean, Lucian Mircea Sasu", "title": "Pose Manipulation with Identity Preservation", "comments": "9 pages, journal article", "journal-ref": "International Journal of Computers Communications & Control, Vol\n  15, Nr 2, 3862, 2020", "doi": "10.15837/ijccc.2020.2.3862", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new model which generates images in novel poses e.g.\nby altering face expression and orientation, from just a few instances of a\nhuman subject. Unlike previous approaches which require large datasets of a\nspecific person for training, our approach may start from a scarce set of\nimages, even from a single image. To this end, we introduce Character Adaptive\nIdentity Normalization GAN (CainGAN) which uses spatial characteristic features\nextracted by an embedder and combined across source images. The identity\ninformation is propagated throughout the network by applying conditional\nnormalization. After extensive adversarial training, CainGAN receives figures\nof faces from a certain individual and produces new ones while preserving the\nperson's identity. Experimental results show that the quality of generated\nimages scales with the size of the input set used during inference.\nFurthermore, quantitative measurements indicate that CainGAN performs better\ncompared to other methods when training data is limited.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 09:51:31 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ardelean", "Andrei-Timotei", ""], ["Sasu", "Lucian Mircea", ""]]}, {"id": "2004.09190", "submitter": "Juyong Zhang", "authors": "Hongrui Cai, Yudong Guo, Zhuang Peng, Juyong Zhang", "title": "Landmark Detection and 3D Face Reconstruction for Caricature using a\n  Nonlinear Parametric Model", "comments": "The code is available at https://github.com/Juyong/CaricatureFace", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an artistic abstraction of the human face by distorting or\nexaggerating certain facial features, while still retains a likeness with the\ngiven face. Due to the large diversity of geometric and texture variations,\nautomatic landmark detection and 3D face reconstruction for caricature is a\nchallenging problem and has rarely been studied before. In this paper, we\npropose the first automatic method for this task by a novel 3D approach. To\nthis end, we first build a dataset with various styles of 2D caricatures and\ntheir corresponding 3D shapes, and then build a parametric model on vertex\nbased deformation space for 3D caricature face. Based on the constructed\ndataset and the nonlinear parametric model, we propose a neural network based\nmethod to regress the 3D face shape and orientation from the input 2D\ncaricature image. Ablation studies and comparison with state-of-the-art methods\ndemonstrate the effectiveness of our algorithm design. Extensive experimental\nresults demonstrate that our method works well for various caricatures. Our\nconstructed dataset, source code and trained model are available at\nhttps://github.com/Juyong/CaricatureFace.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:34:52 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 12:52:28 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cai", "Hongrui", ""], ["Guo", "Yudong", ""], ["Peng", "Zhuang", ""], ["Zhang", "Juyong", ""]]}, {"id": "2004.09484", "submitter": "Dongdong Chen", "authors": "Ziyu Wan and Bo Zhang and Dongdong Chen and Pan Zhang and Dong Chen\n  and Jing Liao and Fang Wen", "title": "Bringing Old Photos Back to Life", "comments": "CVPR 2020 Oral, project website: http://raywzy.com/Old_Photo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to restore old photos that suffer from severe degradation through\na deep learning approach. Unlike conventional restoration tasks that can be\nsolved through supervised learning, the degradation in real photos is complex\nand the domain gap between synthetic images and real old photos makes the\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\ntranslation network by leveraging real photos along with massive synthetic\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\nrespectively transform old photos and clean photos into two latent spaces. And\nthe translation between these two latent spaces is learned with synthetic\npaired data. This translation generalizes well to real photos because the\ndomain gap is closed in the compact latent space. Besides, to address multiple\ndegradations mixed in one old photo, we design a global branch with a partial\nnonlocal block targeting to the structured defects, such as scratches and dust\nspots, and a local branch targeting to the unstructured defects, such as noises\nand blurriness. Two branches are fused in the latent space, leading to improved\ncapability to restore old photos from multiple defects. The proposed method\noutperforms state-of-the-art methods in terms of visual quality for old photos\nrestoration.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:59:23 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Bo", ""], ["Chen", "Dongdong", ""], ["Zhang", "Pan", ""], ["Chen", "Dong", ""], ["Liao", "Jing", ""], ["Wen", "Fang", ""]]}, {"id": "2004.10354", "submitter": "Jon McCormack", "authors": "Jon McCormack, Ben Porter and James Wetter", "title": "A scriptable, generative modelling system for dynamic 3D meshes", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a flexible, script-based system for the procedural generation and\nanimation of 3D geometry. Dynamic triangular meshes are generated through the\nreal-time execution of scripts written in the Lua programming language. Tight\nintegration between the programming environment, runtime engine and graphics\nvisualisation enables a workflow between coding and visual results that\nencourages experimentation and rapid prototyping. The system has been used\nsuccessfully to generate a variety of complex, dynamic organic forms including\ncomplex branching structures, scalable symmetric manifolds and abstract organic\nforms. We use examples in each of these areas to detail the main features of\nthe system, which include a set of flexible 3D mesh operations integrated with\na Lua-based L-system interpreter that creates geometry using generalised\ncylinders.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 00:59:07 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["McCormack", "Jon", ""], ["Porter", "Ben", ""], ["Wetter", "James", ""]]}, {"id": "2004.10696", "submitter": "Demetris Marnerides", "authors": "Demetris Marnerides, Thomas Bashford-Rogers and Kurt Debattista", "title": "Spectrally Consistent UNet for High Fidelity Image Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are the current de-facto models used for\nmany imaging tasks due to their high learning capacity as well as their\narchitectural qualities. The ubiquitous UNet architecture provides an efficient\nand multi-scale solution that combines local and global information. Despite\nthe success of UNet architectures, the use of upsampling layers can cause\nartefacts. In this work, a method for assessing the structural biases of UNets\nand the effects these have on the outputs is presented, characterising their\nimpact in the Fourier domain. A new upsampling module is proposed, based on a\nnovel use of the Guided Image Filter, that provides spectrally consistent\noutputs when used in a UNet architecture, forming the Guided UNet (GUNet). The\nGUNet architecture is applied and evaluated for example applications of inverse\ntone mapping/dynamic range expansion and colourisation from grey-scale images\nand is shown to provide higher fidelity outputs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:04:02 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 09:32:09 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Marnerides", "Demetris", ""], ["Bashford-Rogers", "Thomas", ""], ["Debattista", "Kurt", ""]]}, {"id": "2004.10904", "submitter": "Zhengqin Li", "authors": "Zhengqin Li, Yu-Ying Yeh, Manmohan Chandraker", "title": "Through the Looking Glass: Neural 3D Reconstruction of Transparent\n  Shapes", "comments": "Accepted by CVPR 2020 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D shape of transparent objects using a small number of\nunconstrained natural images is an ill-posed problem. Complex light paths\ninduced by refraction and reflection have prevented both traditional and deep\nmultiview stereo from solving this challenge. We propose a physically-based\nnetwork to recover 3D shape of transparent objects using a few images acquired\nwith a mobile phone camera, under a known but arbitrary environment map. Our\nnovel contributions include a normal representation that enables the network to\nmodel complex light transport through local computation, a rendering layer that\nmodels refractions and reflections, a cost volume specifically designed for\nnormal refinement of transparent shapes and a feature mapping based on\npredicted normals for 3D point cloud reconstruction. We render a synthetic\ndataset to encourage the model to learn refractive light transport across\ndifferent views. Our experiments show successful recovery of high-quality 3D\ngeometry for complex transparent shapes using as few as 5-12 natural images.\nCode and data are publicly released.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 23:51:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 05:54:49 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Li", "Zhengqin", ""], ["Yeh", "Yu-Ying", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2004.11364", "submitter": "Richard Tucker", "authors": "Richard Tucker and Noah Snavely", "title": "Single-View View Synthesis with Multiplane Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent strand of work in view synthesis uses deep learning to generate\nmultiplane images (a camera-centric, layered 3D representation) given two or\nmore input images at known viewpoints. We apply this representation to\nsingle-view view synthesis, a problem which is more challenging but has\npotentially much wider application. Our method learns to predict a multiplane\nimage directly from a single image input, and we introduce scale-invariant view\nsynthesis for supervision, enabling us to train on online video. We show this\napproach is applicable to several different datasets, that it additionally\ngenerates reasonable depth maps, and that it learns to fill in content behind\nthe edges of foreground objects in background layers.\n  Project page at https://single-view-mpi.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:59:19 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Tucker", "Richard", ""], ["Snavely", "Noah", ""]]}, {"id": "2004.11563", "submitter": "Xuequan Lu", "authors": "Dening Lu, Xuequan Lu, Yangxing Sun, Jun Wang", "title": "Deep Feature-preserving Normal Estimation for Point Cloud Filtering", "comments": "accepted to Computer Aided Design (Symposium on Solid and Physical\n  Modeling 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud filtering, the main bottleneck of which is removing noise\n(outliers) while preserving geometric features, is a fundamental problem in 3D\nfield. The two-step schemes involving normal estimation and position update\nhave been shown to produce promising results. Nevertheless, the current normal\nestimation methods including optimization ones and deep learning ones, often\neither have limited automation or cannot preserve sharp features. In this\npaper, we propose a novel feature-preserving normal estimation method for point\ncloud filtering with preserving geometric features. It is a learning method and\nthus achieves automatic prediction for normals. For training phase, we first\ngenerate patch based samples which are then fed to a classification network to\nclassify feature and non-feature points. We finally train the samples of\nfeature and non-feature points separately, to achieve decent results. Regarding\ntesting, given a noisy point cloud, its normals can be automatically estimated.\nFor further point cloud filtering, we iterate the above normal estimation and a\ncurrent position update algorithm for a few times. Various experiments\ndemonstrate that our method outperforms state-of-the-art normal estimation\nmethods and point cloud filtering techniques, in terms of both quality and\nquantity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 07:05:48 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Lu", "Dening", ""], ["Lu", "Xuequan", ""], ["Sun", "Yangxing", ""], ["Wang", "Jun", ""]]}, {"id": "2004.11702", "submitter": "Ojaswa Sharma", "authors": "Aradhya Neeraj Mathur, Apoorv Khattar, Ojaswa Sharma", "title": "Multimodal Medical Volume Colorization from 2D Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorization involves the synthesis of colors on a target image while\npreserving structural content as well as the semantics of the target image.\nThis is a well-explored problem in 2D with many state-of-the-art solutions. We\npropose a novel deep learning-based approach for the colorization of 3D medical\nvolumes. Our system is capable of directly mapping the colors of a 2D\nphotograph to a 3D MRI volume in real-time, producing a high-fidelity color\nvolume suitable for photo-realistic visualization. Since this work is first of\nits kind, we discuss the full pipeline in detail and the challenges that it\nbrings for 3D medical data. The colorization of medical MRI volume also entails\nmodality conversion that highlights the robustness of our approach in handling\nmulti-modal data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 10:53:25 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mathur", "Aradhya Neeraj", ""], ["Khattar", "Apoorv", ""], ["Sharma", "Ojaswa", ""]]}, {"id": "2004.11897", "submitter": "Ulrik G\\\"unther", "authors": "Ulrik G\\\"unther, Kyle I.S. Harrington", "title": "Tales from the Trenches: Developing sciview, a new 3D viewer for the\n  ImageJ community", "comments": "7 pages, 3 figures. As submitted to VisGap workshop at Eurovis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  ImageJ/Fiji is a widely-used tool in the biomedical community for performing\neveryday image analysis tasks. However, its 3D viewer component (aptly named 3D\nViewer) has become dated and is no longer actively maintained. We set out to\ncreate an alternative tool that not only brings modern concepts and APIs from\ncomputer graphics to ImageJ, but is designed to be robust to long-term,\nopen-source development. To achieve this we divided the visualization logic\ninto two parts: the rendering framework, scenery, and the user-facing\napplication, sciview. In this paper we describe the development process and\ndesign decisions made, putting an emphasis on sustainable development,\ncommunity building, and software engineering best practises. We highlight the\nmotivation for the Java Virtual Machine (JVM) as a target platform for\nvisualisation applications. We conclude by discussing the remaining milestones\nand strategy for long-term sustainability.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 18:49:55 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["G\u00fcnther", "Ulrik", ""], ["Harrington", "Kyle I. S.", ""]]}, {"id": "2004.12069", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Zexiang Xu, Henrik Wann Jensen, Hao Su, Ravi Ramamoorthi", "title": "Deep Photon Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning-based denoising approaches have led to dramatic\nimprovements in low sample-count Monte Carlo rendering. These approaches are\naimed at path tracing, which is not ideal for simulating challenging light\ntransport effects like caustics, where photon mapping is the method of choice.\nHowever, photon mapping requires very large numbers of traced photons to\nachieve high-quality reconstructions. In this paper, we develop the first deep\nlearning-based method for particle-based rendering, and specifically focus on\nphoton density estimation, the core of all particle-based methods. We train a\nnovel deep neural network to predict a kernel function to aggregate photon\ncontributions at shading points. Our network encodes individual photons into\nper-photon features, aggregates them in the neighborhood of a shading point to\nconstruct a photon local context vector, and infers a kernel function from the\nper-photon and photon local context features. This network is easy to\nincorporate in many previous photon mapping methods (by simply swapping the\nkernel density estimator) and can produce high-quality reconstructions of\ncomplex global illumination effects like caustics with an order of magnitude\nfewer photons compared to previous photon mapping methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 06:59:10 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhu", "Shilin", ""], ["Xu", "Zexiang", ""], ["Jensen", "Henrik Wann", ""], ["Su", "Hao", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2004.12992", "submitter": "Yang Zhou", "authors": "Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevarria, Evangelos\n  Kalogerakis, Dingzeyu Li", "title": "MakeItTalk: Speaker-Aware Talking-Head Animation", "comments": "SIGGRAPH Asia 2020, 15 pages, 13 figures", "journal-ref": null, "doi": "10.1145/3414685.3417774", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that generates expressive talking heads from a single\nfacial image with audio as the only input. In contrast to previous approaches\nthat attempt to learn direct mappings from audio to raw pixels or points for\ncreating talking faces, our method first disentangles the content and speaker\ninformation in the input audio signal. The audio content robustly controls the\nmotion of lips and nearby facial regions, while the speaker information\ndetermines the specifics of facial expressions and the rest of the talking head\ndynamics. Another key component of our method is the prediction of facial\nlandmarks reflecting speaker-aware dynamics. Based on this intermediate\nrepresentation, our method is able to synthesize photorealistic videos of\nentire talking heads with full range of motion and also animate artistic\npaintings, sketches, 2D cartoon characters, Japanese mangas, stylized\ncaricatures in a single unified framework. We present extensive quantitative\nand qualitative evaluation of our method, in addition to user studies,\ndemonstrating generated talking heads of significantly higher quality compared\nto prior state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:56:15 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 16:31:29 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 17:57:03 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Zhou", "Yang", ""], ["Han", "Xintong", ""], ["Shechtman", "Eli", ""], ["Echevarria", "Jose", ""], ["Kalogerakis", "Evangelos", ""], ["Li", "Dingzeyu", ""]]}, {"id": "2004.13204", "submitter": "Ruizhen Hu", "authors": "Ruizhen Hu, Zeyu Huang, Yuhan Tang, Oliver van Kaick, Hao Zhang, Hui\n  Huang", "title": "Graph2Plan: Learning Floorplan Generation from Layout Graphs", "comments": null, "journal-ref": "ACM Transactions on Graphics 2020", "doi": "10.1145/3386569.3392391", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a learning framework for automated floorplan generation which\ncombines generative modeling using deep neural networks and user-in-the-loop\ndesigns to enable human users to provide sparse design constraints. Such\nconstraints are represented by a layout graph. The core component of our\nlearning framework is a deep neural network, Graph2Plan, which converts a\nlayout graph, along with a building boundary, into a floorplan that fulfills\nboth the layout and boundary constraints. Given an input building boundary, we\nallow a user to specify room counts and other layout constraints, which are\nused to retrieve a set of floorplans, with their associated layout graphs, from\na database. For each retrieved layout graph, along with the input boundary,\nGraph2Plan first generates a corresponding raster floorplan image, and then a\nrefined set of boxes representing the rooms. Graph2Plan is trained on RPLAN, a\nlarge-scale dataset consisting of 80K annotated floorplans. The network is\nmainly based on convolutional processing over both the layout graph, via a\ngraph neural network (GNN), and the input building boundary, as well as the\nraster floorplan images, via conventional image convolution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 23:17:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hu", "Ruizhen", ""], ["Huang", "Zeyu", ""], ["Tang", "Yuhan", ""], ["van Kaick", "Oliver", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2004.13254", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki", "title": "How the deprecation of Java applets affected online visualization\n  frameworks -- a case study", "comments": null, "journal-ref": "VisGap - The Gap between Visualization Research and Visualization\n  Software, The Eurographics Association, 2020", "doi": "10.2312/visgap.20201111", "report-no": "RIKEN-iTHEMS-Report-20", "categories": "cs.SE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The JavaView visualization framework was designed at the end of the 1990s as\na software that provides - among other services - easy, interactive geometry\nvisualizations on web pages. We discuss how this and other design goals were\nmet and present several applications to highlight the contemporary use-cases of\nthe framework. However, as JavaView's easy web exports was based on Java\nApplets, the deprecation of this technology disabled one main functionality of\nthe software. The remainder of the article uses JavaView as an example to\nhighlight the effects of changes in the underlying programming language on a\nvisualization toolkit. We discuss possible reactions of software to such\nchallenges, where the JavaView framework serves as an example to illustrate\ndevelopment decisions. These discussions are guided by the broader, underlying\nquestion as to how long it is sensible to maintain a software.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 02:51:55 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 07:02:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Skrodzki", "Martin", ""]]}, {"id": "2004.13497", "submitter": "Tim Kuipers", "authors": "Tim Kuipers, Eugeni L. Doubrovski, Jun Wu, Charlie C. L. Wang", "title": "A framework for adaptive width control of dense contour-parallel\n  toolpaths in fused deposition modeling", "comments": "16 pages, 20 figures, submitted to Computer-Aided Design journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D printing techniques such as Fused Deposition Modeling (FDM) have enabled\nthe fabrication of complex geometry quickly and cheaply. High stiffness parts\nare produced by filling the 2D polygons of consecutive layers with\ncontour-parallel extrusion toolpaths. Uniform width toolpaths consisting of\ninward offsets from the outline polygons produce over- and underfill regions in\nthe center of the shape, which are especially detrimental to the mechanical\nperformance of thin parts. In order to fill shapes with arbitrary diameter\ndensely the toolpaths require adaptive width. Existing approaches for\ngenerating toolpaths with adaptive width result in a large variation in widths,\nwhich for some hardware systems is difficult to realize accurately. In this\npaper we present a framework which supports multiple schemes to generate\ntoolpaths with adaptive width, by employing a function to decide the number of\nbeads and their widths. Furthermore, we propose a novel scheme which reduces\nextreme bead widths, while limiting the number of altered toolpaths. We\nstatistically validate the effectiveness of our framework and this novel scheme\non a data set of representative 3D models, and physically validate it by\ndeveloping a technique, called back pressure compensation, for off-the-shelf\nFDM systems to effectively realize adaptive width.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:30:18 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kuipers", "Tim", ""], ["Doubrovski", "Eugeni L.", ""], ["Wu", "Jun", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "2004.13630", "submitter": "Daniel Haehn", "authors": "Daniel Haehn, Loraine Franke, Fan Zhang, Suheyla Cetin Karayumak,\n  Steve Pieper, Lauren O'Donnell, Yogesh Rathi", "title": "TRAKO: Efficient Transmission of Tractography Data for Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fiber tracking produces large tractography datasets that are tens of\ngigabytes in size consisting of millions of streamlines. Such vast amounts of\ndata require formats that allow for efficient storage, transfer, and\nvisualization. We present TRAKO, a new data format based on the Graphics Layer\nTransmission Format (glTF) that enables immediate graphical and\nhardware-accelerated processing. We integrate a state-of-the-art compression\ntechnique for vertices, streamlines, and attached scalar and property data. We\nthen compare TRAKO to existing tractography storage methods and provide a\ndetailed evaluation on eight datasets. TRAKO can achieve data reductions of\nover 28x without loss of statistical significance when used to replicate\nanalysis from previously published studies.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 01:19:50 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Haehn", "Daniel", ""], ["Franke", "Loraine", ""], ["Zhang", "Fan", ""], ["Karayumak", "Suheyla Cetin", ""], ["Pieper", "Steve", ""], ["O'Donnell", "Lauren", ""], ["Rathi", "Yogesh", ""]]}, {"id": "2004.13859", "submitter": "Kun Wang", "authors": "Kun Wang, Mridul Aanjaneya, Kostas Bekris", "title": "A First Principles Approach for Data-Efficient System Identification of\n  Spring-Rod Systems via Differentiable Physics Engines", "comments": "accepted at 2020 Learning for Dynamics and Control (L4DC2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel differentiable physics engine for system identification of\ncomplex spring-rod assemblies. Unlike black-box data-driven methods for\nlearning the evolution of a dynamical system and its parameters, we modularize\nthe design of our engine using a discrete form of the governing equations of\nmotion, similar to a traditional physics engine. We further reduce the\ndimension from 3D to 1D for each module, which allows efficient learning of\nsystem parameters using linear regression. As a side benefit, the regression\nparameters correspond to physical quantities, such as spring stiffness or the\nmass of the rod, making the pipeline explainable. The approach significantly\nreduces the amount of training data required, and also avoids iterative\nidentification of data sampling and model training. We compare the performance\nof the proposed engine with previous solutions, and demonstrate its efficacy on\ntensegrity systems, such as NASA's icosahedron.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:37:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wang", "Kun", ""], ["Aanjaneya", "Mridul", ""], ["Bekris", "Kostas", ""]]}, {"id": "2004.13896", "submitter": "Fabian Bolte", "authors": "Fabian Bolte and Stefan Bruckner", "title": "Organic Narrative Charts", "comments": "Published as a short paper at the EuroGraphics2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storyline visualizations display the interactions of groups and entities and\ntheir development over time. Existing approaches have successfully adopted the\ngeneral layout from hand-drawn illustrations to automatically create similar\ndepictions. Ward Shelley is the author of several diagrammatic paintings that\nshow the timeline of art-related subjects, such as Downtown Body, a history of\nart scenes. His drawings include many stylistic elements that are not covered\nby existing storyline visualizations, like links between entities, splits and\nmerges of streams, and tags or labels to describe the individual elements. We\npresent a visualization method that provides a visual mapping for the complex\nrelationships in the data, creates a layout for their display, and adopts a\nsimilar styling of elements to imitate the artistic appeal of such\nillustrations. We compare our results to the original drawings and provide an\nopen-source authoring tool prototype.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 00:08:13 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Bolte", "Fabian", ""], ["Bruckner", "Stefan", ""]]}, {"id": "2004.14071", "submitter": "Noa Fish", "authors": "Noa Fish, Richard Zhang, Lilach Perry, Daniel Cohen-Or, Eli Shechtman,\n  Connelly Barnes", "title": "Image Morphing with Perceptual Constraints and STN Alignment", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.14027", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image morphing, a sequence of plausible frames are synthesized and\ncomposited together to form a smooth transformation between given instances.\nIntermediates must remain faithful to the input, stand on their own as members\nof the set, and maintain a well-paced visual transition from one to the next.\nIn this paper, we propose a conditional GAN morphing framework operating on a\npair of input images. The network is trained to synthesize frames corresponding\nto temporal samples along the transformation, and learns a proper shape prior\nthat enhances the plausibility of intermediate frames. While individual frame\nplausibility is boosted by the adversarial setup, a special training protocol\nproducing sequences of frames, combined with a perceptual similarity loss,\npromote smooth transformation over time. Explicit stating of correspondences is\nreplaced with a grid-based freeform deformation spatial transformer that\npredicts the geometric warp between the inputs, instituting the smooth\ngeometric effect by bringing the shapes into an initial alignment. We provide\ncomparisons to classic as well as latent space morphing techniques, and\ndemonstrate that, given a set of images for self-supervision, our network\nlearns to generate visually pleasing morphing effects featuring believable\nin-betweens, with robustness to changes in shape and texture, requiring no\ncorrespondence annotation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:49:10 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Fish", "Noa", ""], ["Zhang", "Richard", ""], ["Perry", "Lilach", ""], ["Cohen-Or", "Daniel", ""], ["Shechtman", "Eli", ""], ["Barnes", "Connelly", ""]]}, {"id": "2004.14107", "submitter": "He Wang", "authors": "Feixiang He, Yuanhang Xiang, Xi Zhao, He Wang", "title": "Informative Scene Decomposition for Crowd Analysis, Comparison and\n  Simulation Guidance", "comments": "accepted in SIGGRAPH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd simulation is a central topic in several fields including graphics. To\nachieve high-fidelity simulations, data has been increasingly relied upon for\nanalysis and simulation guidance. However, the information in real-world data\nis often noisy, mixed and unstructured, making it difficult for effective\nanalysis, therefore has not been fully utilized. With the fast-growing volume\nof crowd data, such a bottleneck needs to be addressed. In this paper, we\npropose a new framework which comprehensively tackles this problem. It centers\nat an unsupervised method for analysis. The method takes as input raw and noisy\ndata with highly mixed multi-dimensional (space, time and dynamics)\ninformation, and automatically structure it by learning the correlations among\nthese dimensions. The dimensions together with their correlations fully\ndescribe the scene semantics which consists of recurring activity patterns in a\nscene, manifested as space flows with temporal and dynamics profiles. The\neffectiveness and robustness of the analysis have been tested on datasets with\ngreat variations in volume, duration, environment and crowd dynamics. Based on\nthe analysis, new methods for data visualization, simulation evaluation and\nsimulation guidance are also proposed. Together, our framework establishes a\nhighly automated pipeline from raw data to crowd analysis, comparison and\nsimulation guidance. Extensive experiments and evaluations have been conducted\nto show the flexibility, versatility and intuitiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:03:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["He", "Feixiang", ""], ["Xiang", "Yuanhang", ""], ["Zhao", "Xi", ""], ["Wang", "He", ""]]}, {"id": "2004.14381", "submitter": "Kairong Jiang", "authors": "Kairong Jiang, Matthew Berger, Joshua A. Levine", "title": "Visualization of Unsteady Flow Using Heat Kernel Signatures", "comments": "Topic: Visualization, Topic: Heat Kernel, Topic: Flow Visualization,\n  Topic: Heat Kernel Signatures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new technique to visualize complex flowing phenomena by using\nconcepts from shape analysis. Our approach uses techniques that examine the\nintrinsic geometry of manifolds through their heat kernel, to obtain\nrepresentations of such manifolds that are isometry-invariant and multi-scale.\nThese representations permit us to compute heat kernel signatures of each point\non that manifold, and we can use these signatures as features for\nclassification and segmentation that identify points that have similar\nstructural properties.\n  Our approach adapts heat kernel signatures to unsteady flows by formulating a\nnotion of shape where pathlines are observations of a manifold living in a\nhigh-dimensional space.\n  We use this space to compute and visualize heat kernel signatures associated\nwith each pathline.\n  Besides being able to capture the structural features of a pathline, heat\nkernel signatures allow the comparison of pathlines from different flow\ndatasets through a shape matching pipeline. We demonstrate the analytic power\nof heat kernel signatures by comparing both (1) different timesteps from the\nsame unsteady flow as well as (2) flow datasets taken from ensemble simulations\nwith varying simulation parameters. Our analysis only requires the pathlines\nthemselves, and thus it does not utilize the underlying vector field directly.\nWe make minimal assumptions on the pathlines: while we assume they are sampled\nfrom a continuous, unsteady flow, our computations can tolerate pathlines that\nhave varying density and potential unknown boundaries. We evaluate our approach\nthrough visualizations of a variety of two-dimensional unsteady flows.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 23:27:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Jiang", "Kairong", ""], ["Berger", "Matthew", ""], ["Levine", "Joshua A.", ""]]}, {"id": "2004.14489", "submitter": "Daniel S\\'ykora", "authors": "Ond\\v{r}ej Texler, David Futschik, Michal Ku\\v{c}era, Ond\\v{r}ej\n  Jamri\\v{s}ka, \\v{S}\\'arka Sochorov\\'a, Menglei Chai, Sergey Tulyakov, and\n  Daniel S\\'ykora", "title": "Interactive Video Stylization Using Few-Shot Patch-Based Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a learning-based method to the keyframe-based video\nstylization that allows an artist to propagate the style from a few selected\nkeyframes to the rest of the sequence. Its key advantage is that the resulting\nstylization is semantically meaningful, i.e., specific parts of moving objects\nare stylized according to the artist's intention. In contrast to previous style\ntransfer techniques, our approach does not require any lengthy pre-training\nprocess nor a large training dataset. We demonstrate how to train an appearance\ntranslation network from scratch using only a few stylized exemplars while\nimplicitly preserving temporal consistency. This leads to a video stylization\nframework that supports real-time inference, parallel processing, and random\naccess to an arbitrary output frame. It can also merge the content from\nmultiple keyframes without the need to perform an explicit blending operation.\nWe demonstrate its practical utility in various interactive scenarios, where\nthe user paints over a selected keyframe and sees her style transferred to an\nexisting recorded sequence or a live video stream.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 21:33:28 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Texler", "Ond\u0159ej", ""], ["Futschik", "David", ""], ["Ku\u010dera", "Michal", ""], ["Jamri\u0161ka", "Ond\u0159ej", ""], ["Sochorov\u00e1", "\u0160\u00e1rka", ""], ["Chai", "Menglei", ""], ["Tulyakov", "Sergey", ""], ["S\u00fdkora", "Daniel", ""]]}, {"id": "2004.14753", "submitter": "Paolo Cignoni", "authors": "Andrea Maggiordomo, Federico Ponchio, Paolo Cignoni, Marco Tarini", "title": "Real-World Textured Things: a Repository of Textured Models Generated\n  with Modern Photo-Reconstruction Tools", "comments": null, "journal-ref": "Computer Aided Geometric Design Volume 83, November 2020, 101943", "doi": "10.1016/j.cagd.2020.101943", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We are witnessing a proliferation of textured 3D models captured from the\nreal world with automatic photo-reconstruction tools. Digital 3D models of this\nclass come with a unique set of characteristics and defects -- especially\nconcerning their parametrization -- setting them starkly apart from 3D models\noriginating from other, more traditional, sources. We study this class of 3D\nmodels by collecting a significant number of representatives and quantitatively\nevaluating their quality according to several metrics. These include a new\ninvariant metric we design to assess the fragmentation of the UV map, one of\nthe main weaknesses hindering the usability of these models. Our results back\nthe widely shared notion that such models are not fit for direct use in\ndownstream applications (such as videogames), and require challenging\nprocessing steps. Regrettably, existing automatic geometry processing tools are\nnot always up to the task: for example, we verify that available tools for UV\noptimization often fail due mesh inconsistencies, geometric and topological\nnoise, excessive resolution, or other factors; moreover, even when an output is\nproduced, it is rarely a significant improvement over the input (according to\nthe aforementioned measures). Therefore, we argue that further advancements are\nrequired specifically targeted at this class of models. Towards this goal, we\nshare the models we collected in the form of a new public repository,\nReal-World Textured Things (RWTT), a benchmark to systematic field-test and\ncompare algorithms. RWTT consists of 568 carefully selected textured 3D models\nrepresentative of all the main modern off-the-shelf photo-reconstruction tools.\nThe repository is available at http://texturedmesh.isti.cnr.it/ and is\nbrowsable by metadata collected during experiments, and comes with a tool,\nTexMetro, providing the same set of measures for generic UV mapped datasets.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:20:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Maggiordomo", "Andrea", ""], ["Ponchio", "Federico", ""], ["Cignoni", "Paolo", ""], ["Tarini", "Marco", ""]]}]