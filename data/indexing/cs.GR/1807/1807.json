[{"id": "1807.00399", "submitter": "Konstantinos Zampogiannis", "authors": "Konstantinos Zampogiannis, Cornelia Fermuller, Yiannis Aloimonos", "title": "cilantro: A Lean, Versatile, and Efficient Library for Point Cloud Data\n  Processing", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3243655", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce cilantro, an open-source C++ library for geometric and\ngeneral-purpose point cloud data processing. The library provides functionality\nthat covers low-level point cloud operations, spatial reasoning, various\nmethods for point cloud segmentation and generic data clustering, flexible\nalgorithms for robust or local geometric alignment, model fitting, as well as\npowerful visualization tools. To accommodate all kinds of workflows, cilantro\nis almost fully templated, and most of its generic algorithms operate in\narbitrary data dimension. At the same time, the library is easy to use and\nhighly expressive, promoting a clean and concise coding style. cilantro is\nhighly optimized, has a minimal set of external dependencies, and supports\nrapid development of performant point cloud processing software in a wide\nvariety of contexts.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 21:47:51 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 17:51:26 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 17:30:17 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zampogiannis", "Konstantinos", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1807.00410", "submitter": "Jamie Portsmouth", "authors": "David Koerner, Jamie Portsmouth and Wenzel Jakob", "title": "$P_N$-Method for Multiple Scattering in Participating Media", "comments": "Proceedings of Eurographics Symposium on Rendering 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering highly scattering participating media using brute force path\ntracing is a challenge. The diffusion approximation reduces the problem to\nsolving a simple linear partial differential equation. Flux-limited diffusion\nintroduces non-linearities to improve the accuracy of the solution, especially\nin low optical depth media, but introduces several ad-hoc assumptions. Both\nmethods are based on a spherical harmonics expansion of the radiance field that\nis truncated after the first order. In this paper, we investigate the open\nquestion of whether going to higher spherical harmonic orders provides a viable\nimprovement to these two approaches. Increasing the order introduces a set of\ncomplex coupled partial differential equations (the $P_N$-equations), whose\ngrowing number make them difficult to work with at higher orders. We thus use a\ncomputer algebra framework for representing and manipulating the underlying\nmathematical equations, and use it to derive the real-valued $P_N$-equations\nfor arbitrary orders. We further present a staggered-grid $P_N$-solver and\ngenerate its stencil code directly from the expression tree of the\n$P_N$-equations. Finally, we discuss how our method compares to prior work for\nvarious standard problems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 22:41:52 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Koerner", "David", ""], ["Portsmouth", "Jamie", ""], ["Jakob", "Wenzel", ""]]}, {"id": "1807.00687", "submitter": "Tom Kelly", "authors": "Tom Kelly, Niloy J. Mitra", "title": "Simplifying Urban Data Fusion with BigSUR", "comments": "Architecture_MPS under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Our ability to understand data has always lagged behind our ability to\ncollect it. This is particularly true in urban environments, where mass data\ncapture is particularly valuable, but the objects captured are more varied,\ndenser, and complex. To understand the structure and content of the\nenvironment, we must process the unstructured data to a structured form. BigSUR\nis an urban reconstruction algorithm which fuses GIS data, photogrammetric\nmeshes, and street level photography, to create clean representative,\nsemantically labelled, geometry. However, we have identified three problems\nwith the system i) the street level photography is often difficult to acquire;\nii) novel fa\\c{c}ade styles often frustrate the detection of windows and doors;\niii) the computational requirements of the system are large, processing a large\ncity block can take up to 15 hours. In this paper we describe the process of\nsimplifying and validating the BigSUR semantic reconstruction system. In\nparticular, the requirement for street level images is removed, and greedy\npost-process profile assignment is introduced to accelerate the system. We\naccomplish this by modifying the binary integer programming (BIP) optimization,\nand re-evaluating the effects of various parameters. The new variant of the\nsystem is evaluated over a variety of urban areas. We objectively measure mean\nsquared error (MSE) terms over the unstructured geometry, showing that BigSUR\nis able to accurately recover omissions from the input meshes. Further, we\nevaluate the ability of the system to label the walls and roofs of input\nmeshes, concluding that our new BigSUR variant achieves highly accurate\nsemantic labelling with shorter computational time and less input data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 14:14:57 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kelly", "Tom", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1807.00866", "submitter": "Alec Jacobson", "authors": "Silvia Sell\\'an and Herng Yi Cheng and Yuming Ma and Mitchell\n  Dembowski and Alec Jacobson", "title": "Solid Geometry Processing on Deconstructed Domains", "comments": "submitted to Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in geometry processing are modeled as variational problems solved\nnumerically using the finite element method. For solid shapes, this requires a\nvolumetric discretization, such as a boundary conforming tetrahedral mesh.\nUnfortunately, tetrahedral meshing remains an open challenge and existing\nmethods either struggle to conform to complex boundary surfaces or require\nmanual intervention to prevent failure. Rather than create a single volumetric\nmesh for the entire shape, we advocate for solid geometry processing on\ndeconstructed domains, where a large and complex shape is composed of\noverlapping solid subdomains. As each smaller and simpler part is now easier to\ntetrahedralize, the question becomes how to account for overlaps during problem\nmodeling and how to couple solutions on each subdomain together algebraically.\nWe explore how and why previous coupling methods fail, and propose a method\nthat couples solid domains only along their boundary surfaces. We demonstrate\nthe superiority of this method through empirical convergence tests and\nqualitative applications to solid geometry processing on a variety of popular\nsecond-order and fourth-order partial differential equations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 19:30:44 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Sell\u00e1n", "Silvia", ""], ["Cheng", "Herng Yi", ""], ["Ma", "Yuming", ""], ["Dembowski", "Mitchell", ""], ["Jacobson", "Alec", ""]]}, {"id": "1807.01519", "submitter": "Minhyuk Sung", "authors": "Minhyuk Sung, Anastasia Dubrovina, Vladimir G. Kim, Leonidas Guibas", "title": "Learning Fuzzy Set Representations of Partial Shapes on Dual Embedding\n  Spaces", "comments": "SGP 2018 (Symposium on Geometry Processing). 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling relations between components of 3D objects is essential for many\ngeometry editing tasks. Existing techniques commonly rely on labeled\ncomponents, which requires substantial annotation effort and limits components\nto a dictionary of predefined semantic parts. We propose a novel framework\nbased on neural networks that analyzes an uncurated collection of 3D models\nfrom the same category and learns two important types of semantic relations\namong full and partial shapes: complementarity and interchangeability. The\nformer helps to identify which two partial shapes make a complete plausible\nobject, and the latter indicates that interchanging two partial shapes from\ndifferent objects preserves the object plausibility. Our key idea is to jointly\nencode both relations by embedding partial shapes as fuzzy sets in dual\nembedding spaces. We model these two relations as fuzzy set operations\nperformed across the dual embedding spaces, and within each space,\nrespectively. We demonstrate the utility of our method for various retrieval\ntasks that are commonly needed in geometric modeling interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 11:10:38 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Sung", "Minhyuk", ""], ["Dubrovina", "Anastasia", ""], ["Kim", "Vladimir G.", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1807.02222", "submitter": "Li Chen", "authors": "Li Chen, David Coeurjolly", "title": "Digital Geometry, a Survey", "comments": "9 pages ; 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper provides an overview of modern digital geometry and topology\nthrough mathematical principles, algorithms, and measurements. It also covers\nrecent developments in the applications of digital geometry and topology\nincluding image processing, computer vision, and data science. Recent research\nstrongly showed that digital geometry has made considerable contributions to\nmodelings and algorithms in image segmentation, algorithmic analysis, and\nBigData analytics.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 02:20:16 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Chen", "Li", ""], ["Coeurjolly", "David", ""]]}, {"id": "1807.02284", "submitter": "Xiaopei Liu", "authors": "Wei Li, Kai Bai and Xiaopei Liu", "title": "Continuous-Scale Kinetic Fluid Simulation", "comments": "17 pages, 17 figures, accepted by IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinetic approaches, i.e., methods based on the lattice Boltzmann equations,\nhave long been recognized as an appealing alternative for solving\nincompressible Navier-Stokes equations in computational fluid dynamics.\nHowever, such approaches have not been widely adopted in graphics mainly due to\nthe underlying inaccuracy, instability and inflexibility. In this paper, we try\nto tackle these problems in order to make kinetic approaches practical for\ngraphical applications. To achieve more accurate and stable simulations, we\npropose to employ the non-orthogonal central-moment-relaxation model, where we\ndevelop a novel adaptive relaxation method to retain both stability and\naccuracy in turbulent flows. To achieve flexibility, we propose a novel\ncontinuous-scale formulation that enables samples at arbitrary resolutions to\neasily communicate with each other in a more continuous sense and with loose\ngeometrical constraints, which allows efficient and adaptive sample\nconstruction to better match the physical scale. Such a capability directly\nleads to an automatic sample construction which generates static and dynamic\nscales at initialization and during simulation, respectively. This effectively\nmakes our method suitable for simulating turbulent flows with arbitrary\ngeometrical boundaries. Our simulation results with applications to smoke\nanimations show the benefits of our method, with comparisons for justification\nand verification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 07:00:17 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Li", "Wei", ""], ["Bai", "Kai", ""], ["Liu", "Xiaopei", ""]]}, {"id": "1807.02451", "submitter": "Igor Grabec", "authors": "Lovrenc \\v{S}vegl, Igor Grabec", "title": "Evolution of natural patterns from random fields", "comments": "8th Conference on Information and Graphic Arts Technology, Ljubljana,\n  Slovenia, 7-8 June 2018, Abstracts, R. URBAS, N. PU\\v{S}NIK (eds.), Publ.:\n  Uni. Ljubljana, Faculty of Nat. Sci. and Eng., Dept., pp. 95-96", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the article a transition from pattern evolution equation of\nreaction-diffusion type to a cellular automaton (CA) is described. The\napplicability of CA is demonstrated by generating patterns of complex irregular\nstructure on a hexagonal and quadratic lattice. With this aim a random initial\nfield is transformed by a sequence of CA actions into a new pattern. On the\nhexagonal lattice this pattern resembles a lizard skin. The properties of CA\nare specified by the most simple majority rule that adapts selected cell state\nto the most frequent state of cells in its surrounding. The method could be of\ninterest for manufacturing of textiles as well as for modeling of patterns on\nskin of various animals.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 15:27:30 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["\u0160vegl", "Lovrenc", ""], ["Grabec", "Igor", ""]]}, {"id": "1807.02578", "submitter": "Ilke Demir", "authors": "Ilke Demir and Daniel G. Aliaga", "title": "Guided Proceduralization: Optimizing Geometry Processing and Grammar\n  Extraction for Architectural Models", "comments": null, "journal-ref": "Computers & Graphics, Volume 74, 2018, Pages 257-267, ISSN\n  0097-8493", "doi": "10.1016/j.cag.2018.05.013", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a guided proceduralization framework that optimizes geometry\nprocessing on architectural input models to extract target grammars. We aim to\nprovide efficient artistic workflows by creating procedural representations\nfrom existing 3D models, where the procedural expressiveness is controlled by\nthe user. Architectural reconstruction and modeling tasks have been handled as\neither time consuming manual processes or procedural generation with difficult\ncontrol and artistic influence. We bridge the gap between creation and\ngeneration by converting existing manually modeled architecture to procedurally\neditable parametrized models, and carrying the guidance to procedural domain by\nletting the user define the target procedural representation. Additionally, we\npropose various applications of such procedural representations, including\nguided completion of point cloud models, controllable 3D city modeling, and\nother benefits of procedural modeling.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 22:22:53 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Demir", "Ilke", ""], ["Aliaga", "Daniel G.", ""]]}, {"id": "1807.02921", "submitter": "Mustafa Hajij", "authors": "Paul Rosen, Mustafa Hajij, Junyi Tu, Tanvirul Arafin, Les Piegl", "title": "Inferring Quality in Point Cloud-based 3D Printed Objects using\n  Topological Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the quality of 3D printed models before they are printed remains a\nchalleng- ing problem, particularly when considering point cloud-based models.\nThis paper introduces an approach to quality assessment, which uses techniques\nfrom the field of Topological Data Analy- sis (TDA) to compute a topological\nabstraction of the eventual printed model. Two main tools of TDA, Mapper and\npersistent homology, are used to analyze both the printed space and empty space\ncreated by the model. This abstraction enables investigating certain qualities\nof the model, with respect to print quality, and identifies potential anomalies\nthat may appear in the final product.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 02:52:01 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Rosen", "Paul", ""], ["Hajij", "Mustafa", ""], ["Tu", "Junyi", ""], ["Arafin", "Tanvirul", ""], ["Piegl", "Les", ""]]}, {"id": "1807.03235", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar, Gerard Pons-Moll, Mario Fritz", "title": "Fashion is Taking Shape: Understanding Clothing Preference Based on Body\n  Shape From Online Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the correlation between clothing garments and body shape, we\ncollected a new dataset (Fashion Takes Shape), which includes images of users\nwith clothing category annotations. We employ our multi-photo approach to\nestimate body shapes of each user and build a conditional model of clothing\ncategories given body-shape. We demonstrate that in real-world data, clothing\ncategories and body-shapes are correlated and show that our multi-photo\napproach leads to a better predictive model for clothing categories compared to\nmodels based on single-view shape estimates or manually annotated body types.\nWe see our method as the first step towards the large-scale understanding of\nclothing preferences from body shape.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:35:13 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 11:27:46 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Pons-Moll", "Gerard", ""], ["Fritz", "Mario", ""]]}, {"id": "1807.03249", "submitter": "Daniel S\\'ykora", "authors": "Daniel S\\'ykora and Ond\\v{r}ej Jamri\\v{s}ka and Jingwan Lu and Eli\n  Shechtman", "title": "StyleBlit: Fast Example-Based Stylization with Local Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present StyleBlit---an efficient example-based style transfer algorithm\nthat can deliver high-quality stylized renderings in real-time on a single-core\nCPU. Our technique is especially suitable for style transfer applications that\nuse local guidance - descriptive guiding channels containing large spatial\nvariations. Local guidance encourages transfer of content from the source\nexemplar to the target image in a semantically meaningful way. Typical local\nguidance includes, e.g., normal values, texture coordinates or a displacement\nfield. Contrary to previous style transfer techniques, our approach does not\ninvolve any computationally expensive optimization. We demonstrate that when\nlocal guidance is used, optimization-based techniques converge to solutions\nthat can be well approximated by simple pixel-level operations. Inspired by\nthis observation, we designed an algorithm that produces results visually\nsimilar to, if not better than, the state-of-the-art, and is several orders of\nmagnitude faster. Our approach is suitable for scenarios with low computational\nbudget such as games and mobile applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:49:49 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["S\u00fdkora", "Daniel", ""], ["Jamri\u0161ka", "Ond\u0159ej", ""], ["Lu", "Jingwan", ""], ["Shechtman", "Eli", ""]]}, {"id": "1807.03350", "submitter": "Xiao Zhou", "authors": "Xiao Zhou, Desislava Hristova, Anastasios Noulas, and Cecilia Mascolo", "title": "Detecting Socio-Economic Impact of Cultural Investment Through\n  Geo-Social Network Analysis", "comments": "5 pages, 4 figures. The 11th International AAAI Conference on Web and\n  Social Media (ICWSM-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking advantage of nearly 4 million transition records for three years in\nLondon from a popular location-based social network service, Foursquare, we\nstudy how to track the impact and measure the effectiveness of cultural\ninvestment in small urban areas. We reveal the underlying relationships between\nsocio-economic status, local cultural expenditure, and network features\nextracted from user mobility trajectories. This research presents how\ngeo-social and mobile services more generally can be used as a proxy to track\nlocal changes as government financial effort is put in developing urban areas,\nand thus gives evidence and suggestions for further policy-making and\ninvestment optimization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 09:53:32 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zhou", "Xiao", ""], ["Hristova", "Desislava", ""], ["Noulas", "Anastasios", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "1807.03520", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Rui Wang and Subhransu Maji", "title": "Multiresolution Tree Networks for 3D Point Cloud Processing", "comments": "Accepted to ECCV 2018. 23 pages, including supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present multiresolution tree-structured networks to process point clouds\nfor 3D shape understanding and generation tasks. Our network represents a 3D\nshape as a set of locality-preserving 1D ordered list of points at multiple\nresolutions. This allows efficient feed-forward processing through 1D\nconvolutions, coarse-to-fine analysis through a multi-grid architecture, and it\nleads to faster convergence and small memory footprint during training. The\nproposed tree-structured encoders can be used to classify shapes and outperform\nexisting point-based architectures on shape classification benchmarks, while\ntree-structured decoders can be used for generating point clouds directly and\nthey outperform existing approaches for image-to-shape inference tasks learned\nusing the ShapeNet dataset. Our model also allows unsupervised learning of\npoint-cloud based shapes by using a variational autoencoder, leading to\nhigher-quality generated shapes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 08:28:01 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 20:19:30 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "1807.04184", "submitter": "Arnaud Mas", "authors": "Arnaud Mas (EDF R\\&D PERICLES), Idriss Isma\\\"el (EDF R\\&D PERICLES),\n  Nicolas Filliard (EDF R\\&D PERICLES)", "title": "Indy: a virtual reality multi-player game for navigation skills training", "comments": null, "journal-ref": "IEEE Fourth VR International Workshop on Collaborative Virtual\n  Environments (IEEEVR 2018), Mar 2018, Reutlingen, Germany", "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working in complex industrial facilities requires spatial navigation skills\nthat people build up with time and field experience. Training sessions\nconsisting in guided tours help discover places but they are insufficient to\nbecome intimately familiar with their layout. They imply passive learning\npostures, are time-limited and can be experienced only once because of\norganization constraints and potential interferences with ongoing activities in\nthe buildings. To overcome these limitations and improve the acquisition of\nnavigation skills, we developed Indy, a virtual reality system consisting in a\ncollaborative game of treasure hunting. It has several key advantages: it\nfocuses learners' attention on navigation tasks, implies their active\nengagement and provides them with feedbacks on their achievements. Virtual\nreality makes it possible to multiply the number and duration of situations\nthat learners can experience to better consolidate their skills. This paper\ndiscusses the main design principles and a typical usage scenario of Indy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:13:06 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Mas", "Arnaud", "", "EDF R\\&D PERICLES"], ["Isma\u00ebl", "Idriss", "", "EDF R\\&D PERICLES"], ["Filliard", "Nicolas", "", "EDF R\\&D PERICLES"]]}, {"id": "1807.06358", "submitter": "Huaibo Huang", "authors": "Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, Tieniu Tan", "title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel introspective variational autoencoder (IntroVAE) model for\nsynthesizing high-resolution photographic images. IntroVAE is capable of\nself-evaluating the quality of its generated samples and improving itself\naccordingly. Its inference and generator models are jointly trained in an\nintrospective way. On one hand, the generator is required to reconstruct the\ninput images from the noisy outputs of the inference model as normal VAEs. On\nthe other hand, the inference model is encouraged to classify between the\ngenerated and real samples while the generator tries to fool it as GANs. These\ntwo famous generative frameworks are integrated in a simple yet efficient\nsingle-stream architecture that can be trained in a single stage. IntroVAE\npreserves the advantages of VAEs, such as stable training and nice latent\nmanifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\nno extra discriminators, because the inference model itself serves as a\ndiscriminator to distinguish between the generated and real samples.\nExperiments demonstrate that our method produces high-resolution\nphoto-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\ncomparable to or better than the state-of-the-art GANs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:37:31 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 13:46:18 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Huang", "Huaibo", ""], ["Li", "Zhihang", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1807.06551", "submitter": "David George", "authors": "David George, Xianguha Xie, Yu-Kun Lai, Gary KL Tam", "title": "A Deep Learning Driven Active Framework for Segmentation of Large 3D\n  Shape Collections", "comments": "16 pages, 17 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level shape understanding and technique evaluation on large repositories\nof 3D shapes often benefit from additional information known about the shapes.\nOne example of such information is the semantic segmentation of a shape into\nfunctional or meaningful parts. Generating accurate segmentations with\nmeaningful segment boundaries is, however, a costly process, typically\nrequiring large amounts of user time to achieve high quality results. In this\npaper we present an active learning framework for large dataset segmentation,\nwhich iteratively provides the user with new predictions by training new models\nbased on already segmented shapes. Our proposed pipeline consists of three\nnovel components. First, we a propose a fast and relatively accurate\nfeature-based deep learning model to provide dataset-wide segmentation\npredictions. Second, we propose an information theory measure to estimate the\nprediction quality and for ordering subsequent fast and meaningful shape\nselection. Our experiments show that such suggestive ordering helps reduce\nusers time and effort, produce high quality predictions, and construct a model\nthat generalizes well. Finally, we provide effective segmentation refinement\nfeatures to help the user quickly correct any incorrect predictions. We show\nthat our framework is more accurate and in general more efficient than\nstate-of-the-art, for massive dataset segmentation with while also providing\nconsistent segment boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:58:06 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["George", "David", ""], ["Xie", "Xianguha", ""], ["Lai", "Yu-Kun", ""], ["Tam", "Gary KL", ""]]}, {"id": "1807.07449", "submitter": "Neng Shi", "authors": "Neng Shi and Yubo Tao", "title": "CNNs based Viewpoint Estimation for Volume Visualization", "comments": null, "journal-ref": "ACM Transactions on Intelligent Systems and Technology (TIST),\n  2019", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viewpoint estimation from 2D rendered images is helpful in understanding how\nusers select viewpoints for volume visualization and guiding users to select\nbetter viewpoints based on previous visualizations. In this paper, we propose a\nviewpoint estimation method based on Convolutional Neural Networks (CNNs) for\nvolume visualization. We first design an overfit-resistant image rendering\npipeline to generate the training images with accurate viewpoint annotations,\nand then train a category-specific viewpoint classification network to estimate\nthe viewpoint for the given rendered image. Our method can achieve good\nperformance on images rendered with different transfer functions and rendering\nparameters in several categories. We apply our model to recover the viewpoints\nof the rendered images in publications, and show how experts look at volumes.\nWe also introduce a CNN feature-based image similarity measure for similarity\nvoting based viewpoint selection, which can suggest semantically meaningful\noptimal viewpoints for different volumes and transfer functions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:08:45 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 03:44:50 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 15:58:55 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Shi", "Neng", ""], ["Tao", "Yubo", ""]]}, {"id": "1807.08304", "submitter": "Pascal Laube", "authors": "Pascal Laube, Matthias O. Franz, Georg Umlauf", "title": "Deep Learning Parametrization for B-Spline Curve Approximation", "comments": "Accepted at 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method using deep learning to compute\nparametrizations for B-spline curve approximation. Existing methods consider\nthe computation of parametric values and a knot vector as separate problems. We\npropose to train interdependent deep neural networks to predict parametric\nvalues and knots. We show that it is possible to include B-spline curve\napproximation directly into the neural network architecture. The resulting\nparametrizations yield tight approximations and are able to outperform\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 15:41:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Laube", "Pascal", ""], ["Franz", "Matthias O.", ""], ["Umlauf", "Georg", ""]]}, {"id": "1807.08474", "submitter": "Hui Zhao", "authors": "Hui Zhao and Na Lei and Xuan Li and Peng Zeng and Ke Xu and Xianfeng\n  Gu", "title": "Robust Edge-Preserved Surface Mesh Polycube Deformation", "comments": "9pages,15 figures ,conference or other essential info", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of polycube construction or deformation is an essential problem\nin computer graphics. In this paper, we present a robust, simple, efficient and\nautomatic algorithm to deform the meshes of arbitrary shapes into their\npolycube ones. We derive a clear relationship between a mesh and its\ncorresponding polycube shape. Our algorithm is edge-preserved, and works on\nsurface meshes with or without boundaries. Our algorithm outperforms previous\nones in speed, robustness, efficiency. Our method is simple to implement. To\ndemonstrate the robustness and effectiveness of our method, we apply it to\nhundreds of models of varying complexity and topology. We demonstrat that our\nmethod compares favorably to other state-of-the-art polycube deformation\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:21:36 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhao", "Hui", ""], ["Lei", "Na", ""], ["Li", "Xuan", ""], ["Zeng", "Peng", ""], ["Xu", "Ke", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1807.08486", "submitter": "Hui Zhao", "authors": "Hui Zhao and Xuan Li and Huabin Ge and Xianfeng Gu and Na Lei", "title": "Conformal Mesh Parameterization Using Discrete Calabi Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce discrete Calabi flow to the graphics research\ncommunity and present a novel conformal mesh parameterization algorithm. Calabi\nenergy has a succinct and explicit format. Its corresponding flow is conformal\nand convergent under certain conditions. Our method is based on the Calabi\nenergy and Calabi flow with solid theoretical and mathematical base. We\ndemonstrate our approach on dozens of models and compare it with other related\nflow based methods, such as the well-known Ricci flow and CETM. Our experiments\nshow that the performance of our algorithm is comparably the same with other\nmethods. The discrete Calabi flow in our method provides another perspective on\nconformal flow and conformal parameterization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:59:04 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhao", "Hui", ""], ["Li", "Xuan", ""], ["Ge", "Huabin", ""], ["Gu", "Xianfeng", ""], ["Lei", "Na", ""]]}, {"id": "1807.09064", "submitter": "Xiaoguang Han", "authors": "Xiaoguang Han, Kangcheng Hou, Dong Du, Yuda Qiu, Yizhou Yu, Kun Zhou,\n  Shuguang Cui", "title": "CaricatureShop: Personalized and Photorealistic Caricature Sketching", "comments": "12 pages,16 figures,submitted to IEEE TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first sketching system for interactively\npersonalized and photorealistic face caricaturing. Input an image of a human\nface, the users can create caricature photos by manipulating its facial feature\ncurves. Our system firstly performs exaggeration on the recovered 3D face model\naccording to the edited sketches, which is conducted by assigning the laplacian\nof each vertex a scaling factor. To construct the mapping between 2D sketches\nand a vertex-wise scaling field, a novel deep learning architecture is\ndeveloped. With the obtained 3D caricature model, two images are generated, one\nobtained by applying 2D warping guided by the underlying 3D mesh deformation\nand the other obtained by re-rendering the deformed 3D textured model. These\ntwo images are then seamlessly integrated to produce our final output. Due to\nthe severely stretching of meshes, the rendered texture is of blurry\nappearances. A deep learning approach is exploited to infer the missing details\nfor enhancing these blurry regions. Moreover, a relighting operation is\ninvented to further improve the photorealism of the result. Both quantitative\nand qualitative experiment results validated the efficiency of our sketching\nsystem and the superiority of our proposed techniques against existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:26:57 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Han", "Xiaoguang", ""], ["Hou", "Kangcheng", ""], ["Du", "Dong", ""], ["Qiu", "Yuda", ""], ["Yu", "Yizhou", ""], ["Zhou", "Kun", ""], ["Cui", "Shuguang", ""]]}, {"id": "1807.09193", "submitter": "Manyi Li", "authors": "Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan,\n  Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, Hao Zhang", "title": "GRAINS: Generative Recursive Autoencoders for INdoor Scenes", "comments": "21 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative neural network which enables us to generate plausible\n3D indoor scenes in large quantities and varieties, easily and highly\nefficiently. Our key observation is that indoor scene structures are inherently\nhierarchical. Hence, our network is not convolutional; it is a recursive neural\nnetwork or RvNN. Using a dataset of annotated scene hierarchies, we train a\nvariational recursive autoencoder, or RvNN-VAE, which performs scene object\ngrouping during its encoding phase and scene generation during decoding.\nSpecifically, a set of encoders are recursively applied to group 3D objects\nbased on support, surround, and co-occurrence relations in a scene, encoding\ninformation about object spatial properties, semantics, and their relative\npositioning with respect to other objects in the hierarchy. By training a\nvariational autoencoder (VAE), the resulting fixed-length codes roughly follow\na Gaussian distribution. A novel 3D scene can be generated hierarchically by\nthe decoder from a randomly sampled code from the learned distribution. We coin\nour method GRAINS, for Generative Recursive Autoencoders for INdoor Scenes. We\ndemonstrate the capability of GRAINS to generate plausible and diverse 3D\nindoor scenes and compare with existing methods for 3D scene synthesis. We show\napplications of GRAINS including 3D scene modeling from 2D layouts, scene\nediting, and semantic scene segmentation via PointNet whose performance is\nboosted by the large quantity and variety of 3D scenes generated by our method.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:46:50 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:07:23 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 16:45:02 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 07:53:31 GMT"}, {"version": "v5", "created": "Wed, 8 May 2019 11:58:03 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Li", "Manyi", ""], ["Patil", "Akshay Gadi", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Khan", "Owais", ""], ["Shamir", "Ariel", ""], ["Tu", "Changhe", ""], ["Chen", "Baoquan", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "1807.09202", "submitter": "Francesco Giannini", "authors": "Giuseppe Marra and Francesco Giannini and Michelangelo Diligenti and\n  Marco Gori", "title": "Constraint-Based Visual Generation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30508-6_45", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years the systematic adoption of deep learning to visual\ngeneration has produced impressive results that, amongst others, definitely\nbenefit from the massive exploration of convolutional architectures. In this\npaper, we propose a general approach to visual generation that combines\nlearning capabilities with logic descriptions of the target to be generated.\nThe process of generation is regarded as a constrained satisfaction problem,\nwhere the constraints describe a set of properties that characterize the\ntarget. Interestingly, the constraints can also involve logic variables, while\nall of them are converted into real-valued functions by means of the t-norm\ntheory. We use deep architectures to model the involved variables, and propose\na computational scheme where the learning process carries out a satisfaction of\nthe constraints. We propose some examples in which the theory can naturally be\nused, including the modeling of GAN and auto-encoders, and report promising\nresults in problems with the generation of handwritten characters and face\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 22:56:15 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 09:44:10 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 14:37:06 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Marra", "Giuseppe", ""], ["Giannini", "Francesco", ""], ["Diligenti", "Michelangelo", ""], ["Gori", "Marco", ""]]}, {"id": "1807.10517", "submitter": "Emanuele Rodol\\`a", "authors": "Riccardo Marin, Simone Melzi, Emanuele Rodol\\`a, Umberto Castellani", "title": "FARM: Functional Automatic Registration Method for 3D Human Bodies", "comments": "Under submission to CGF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for non-rigid registration of 3D human shapes. Our\nproposed pipeline builds upon a given parametric model of the human, and makes\nuse of the functional map representation for encoding and inferring shape maps\nthroughout the registration process. This combination endows our method with\nrobustness to a large variety of nuisances observed in practical settings,\nincluding non-isometric transformations, downsampling, topological noise, and\nocclusions; further, the pipeline can be applied invariably across different\nshape representations (e.g. meshes and point clouds), and in the presence of\n(even dramatic) missing parts such as those arising in real-world depth sensing\napplications. We showcase our method on a selection of challenging tasks,\ndemonstrating results in line with, or even surpassing, state-of-the-art\nmethods in the respective areas.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 09:53:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Marin", "Riccardo", ""], ["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""], ["Castellani", "Umberto", ""]]}, {"id": "1807.11079", "submitter": "Wayne Wu", "authors": "Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, Chen Change Loy", "title": "ReenactGAN: Learning to Reenact Faces via Boundary Transfer", "comments": "Accepted to ECCV 2018. Project page:\n  https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning-based framework for face reenactment. The\nproposed method, known as ReenactGAN, is capable of transferring facial\nmovements and expressions from monocular video input of an arbitrary person to\na target person. Instead of performing a direct transfer in the pixel space,\nwhich could result in structural artifacts, we first map the source face onto a\nboundary latent space. A transformer is subsequently used to adapt the boundary\nof source face to the boundary of target face. Finally, a target-specific\ndecoder is used to generate the reenacted target face. Thanks to the effective\nand reliable boundary-based transfer, our method can perform photo-realistic\nface reenactment. In addition, ReenactGAN is appealing in that the whole\nreenactment process is purely feed-forward, and thus the reenactment process\ncan run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model will be\npublicly available at\nhttps://wywu.github.io/projects/ReenactGAN/ReenactGAN.html\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 16:35:15 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wu", "Wayne", ""], ["Zhang", "Yunxuan", ""], ["Li", "Cheng", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1807.11212", "submitter": "Julien Tierny", "authors": "Guillaume Favelier, Noura Faraj, Brian Summa, Julien Tierny", "title": "Persistence Atlas for Critical Point Variability in Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": "2344815-v2", "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for the visualization and analysis of the\nspatial variability of features of interest represented by critical points in\nensemble data. Our framework, called Persistence Atlas, enables the\nvisualization of the dominant spatial patterns of critical points, along with\nstatistics regarding their occurrence in the ensemble. The persistence atlas\nrepresents in the geometrical domain each dominant pattern in the form of a\nconfidence map for the appearance of critical points. As a by-product, our\nmethod also provides 2-dimensional layouts of the entire ensemble, highlighting\nthe main trends at a global level. Our approach is based on the new notion of\nPersistence Map, a measure of the geometrical density in critical points which\nleverages the robustness to noise of topological persistence to better\nemphasize salient features. We show how to leverage spectral embedding to\nrepresent the ensemble members as points in a low-dimensional Euclidean space,\nwhere distances between points measure the dissimilarities between critical\npoint layouts and where statistical tasks, such as clustering, can be easily\ncarried out. Further, we show how the notion of mandatory critical point can be\nleveraged to evaluate for each cluster confidence regions for the appearance of\ncritical points. Most of the steps of this framework can be trivially\nparallelized and we show how to efficiently implement them. Extensive\nexperiments demonstrate the relevance of our approach. The accuracy of the\nconfidence regions provided by the persistence atlas is quantitatively\nevaluated and compared to a baseline strategy using an off-the-shelf clustering\napproach. We illustrate the importance of the persistence atlas in a variety of\nreal-life datasets, where clear trends in feature layouts are identified and\nanalyzed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 07:46:27 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Favelier", "Guillaume", ""], ["Faraj", "Noura", ""], ["Summa", "Brian", ""], ["Tierny", "Julien", ""]]}, {"id": "1807.11627", "submitter": "Zeyu Wang", "authors": "Zeyu Wang, Shiyu Qiu, Qingyang Chen, Alexander Ringlein, Julie Dorsey,\n  Holly Rushmeier", "title": "AniCode: Authoring Coded Artifacts for Network-Free Personalized\n  Animations", "comments": null, "journal-ref": "The Visual Computer 2019", "doi": "10.1007/s00371-019-01681-y", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-based media (videos, synthetic animations, and virtual reality\nexperiences) are used for communication, in applications such as manufacturers\nexplaining the operation of a new appliance to consumers and scientists\nillustrating the basis of a new conclusion. However, authoring time-based media\nthat are effective and personalized for the viewer remains a challenge. We\nintroduce AniCode, a novel framework for authoring and consuming time-based\nmedia. An author encodes a video animation in a printed code, and affixes the\ncode to an object. A consumer uses a mobile application to capture an image of\nthe object and code, and to generate a video presentation on the fly.\nImportantly, AniCode presents the video personalized in the consumer's visual\ncontext. Our system is designed to be low cost and easy to use. By not\nrequiring an internet connection, and through animations that decode correctly\nonly in the intended context, AniCode enhances privacy of communication using\ntime-based media. Animation schemes in the system include a series of 2D and 3D\ngeometric transformations, color transformation, and annotation. We demonstrate\nthe AniCode framework with sample applications from a wide range of domains,\nincluding product \"how to\" examples, cultural heritage, education, creative\nart, and design. We evaluate the ease of use and effectiveness of our system\nwith a user study.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 01:38:27 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wang", "Zeyu", ""], ["Qiu", "Shiyu", ""], ["Chen", "Qingyang", ""], ["Ringlein", "Alexander", ""], ["Dorsey", "Julie", ""], ["Rushmeier", "Holly", ""]]}, {"id": "1807.11760", "submitter": "Marta Ortin", "authors": "Yunjin Zhang, Marta Ortin, Victor Arellano, Rui Wang, Diego Gutierrez,\n  and Hujun Bao", "title": "On-the-Fly Power-Aware Rendering", "comments": null, "journal-ref": "Computer Graphics Forum, 2018", "doi": "10.1111/cgf.13483", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power saving is a prevailing concern in desktop computers and, especially, in\nbattery-powered devices such as mobile phones. This is generating a growing\ndemand for power-aware graphics applications that can extend battery life,\nwhile preserving good quality. In this paper, we address this issue by\npresenting a real-time power-efficient rendering framework, able to dynamically\nselect the rendering configuration with the best quality within a given power\nbudget. Different from the current state of the art, our method does not\nrequire precomputation of the whole camera-view space, nor Pareto curves to\nexplore the vast power-error space; as such, it can also handle dynamic scenes.\nOur algorithm is based on two key components: our novel power prediction model,\nand our runtime quality error estimation mechanism. These components allow us\nto search for the optimal rendering configuration at runtime, being transparent\nto the user. We demonstrate the performance of our framework on two different\nplatforms: a desktop computer, and a mobile device. In both cases, we produce\nresults close to the maximum quality, while achieving significant power\nsavings.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 11:13:23 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhang", "Yunjin", ""], ["Ortin", "Marta", ""], ["Arellano", "Victor", ""], ["Wang", "Rui", ""], ["Gutierrez", "Diego", ""], ["Bao", "Hujun", ""]]}, {"id": "1807.11847", "submitter": "Lei Li", "authors": "Lei Li, Hongbo Fu, Chiew-Lan Tai", "title": "Fast Sketch Segmentation and Labeling with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and efficient method based on deep learning to\nautomatically decompose sketched objects into semantically valid parts. We\ntrain a deep neural network to transfer existing segmentations and labelings\nfrom 3D models to freehand sketches without requiring numerous well-annotated\nsketches as training data. The network takes the binary image of a sketched\nobject as input and produces a corresponding segmentation map with per-pixel\nlabelings as output. A subsequent post-process procedure with multi-label graph\ncuts further refines the segmentation and labeling result. We validate our\nproposed method on two sketch datasets. Experiments show that our method\noutperforms the state-of-the-art method in terms of segmentation and labeling\naccuracy and is significantly faster, enabling further integration in\ninteractive drawing systems. We demonstrate the efficiency of our method in a\nsketch-based modeling application that automatically transforms input sketches\ninto 3D models by part assembly.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:56:02 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Li", "Lei", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-Lan", ""]]}]