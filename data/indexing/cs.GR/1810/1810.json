[{"id": "1810.00028", "submitter": "Aniket Bera", "authors": "Aniket Bera, Tanmay Randhavane, Emily Kubin, Husam Shaik, Kurt Gray,\n  Dinesh Manocha", "title": "Data-Driven Modeling of Group Entitativity in Virtual Environments", "comments": "Accepted at VRST 2018, November 28-December 1, 2018, Tokyo, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven algorithm to model and predict the socio-emotional\nimpact of groups on observers. Psychological research finds that highly\nentitative i.e. cohesive and uniform groups induce threat and unease in\nobservers. Our algorithm models realistic trajectory-level behaviors to\nclassify and map the motion-based entitativity of crowds. This mapping is based\non a statistical scheme that dynamically learns pedestrian behavior and\ncomputes the resultant entitativity induced emotion through group motion\ncharacteristics. We also present a novel interactive multi-agent simulation\nalgorithm to model entitative groups and conduct a VR user study to validate\nthe socio-emotional predictive power of our algorithm. We further show that\nmodel-generated high-entitativity groups do induce more negative emotions than\nlow-entitative groups.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 18:23:39 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Bera", "Aniket", ""], ["Randhavane", "Tanmay", ""], ["Kubin", "Emily", ""], ["Shaik", "Husam", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1810.00107", "submitter": "Xin Li", "authors": "Celong Liu and Xin Li", "title": "Superimposition-guided Facial Reconstruction from Skull", "comments": "14 pages; 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new algorithm to perform facial reconstruction from a given\nskull. This technique has forensic application in helping the identification of\nskeletal remains when other information is unavailable. Unlike most existing\nstrategies that directly reconstruct the face from the skull, we utilize a\ndatabase of portrait photos to create many face candidates, then perform a\nsuperimposition to get a well matched face, and then revise it according to the\nsuperimposition. To support this pipeline, we build an effective autoencoder\nfor image-based facial reconstruction, and a generative model for constrained\nface inpainting. Our experiments have demonstrated that the proposed pipeline\nis stable and accurate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 22:24:07 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Liu", "Celong", ""], ["Li", "Xin", ""]]}, {"id": "1810.00706", "submitter": "Rahul Arora", "authors": "Rahul Arora, Alec Jacobson, Timothy R. Langlois, Yijiang Huang,\n  Caitlin Mueller, Wojciech Matusik, Ariel Shamir, Karan Singh and David I.W.\n  Levin", "title": "Designing Volumetric Truss Structures", "comments": null, "journal-ref": null, "doi": "10.1145/3328939.3328999", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first algorithm for designing volumetric Michell Trusses. Our\nmethod uses a parametrization approach to generate trusses made of structural\nelements aligned with the primary direction of an object's stress field. Such\ntrusses exhibit high strength-to-weight ratios. We demonstrate the structural\nrobustness of our designs via a posteriori physical simulation. We believe our\nalgorithm serves as an important complement to existing structural optimization\ntools and as a novel standalone design tool itself.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 13:57:05 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 07:23:26 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 18:20:25 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Arora", "Rahul", ""], ["Jacobson", "Alec", ""], ["Langlois", "Timothy R.", ""], ["Huang", "Yijiang", ""], ["Mueller", "Caitlin", ""], ["Matusik", "Wojciech", ""], ["Shamir", "Ariel", ""], ["Singh", "Karan", ""], ["Levin", "David I. W.", ""]]}, {"id": "1810.01054", "submitter": "Yuanming Hu", "authors": "Yuanming Hu, Jiancheng Liu, Andrew Spielberg, Joshua B. Tenenbaum,\n  William T. Freeman, Jiajun Wu, Daniela Rus, Wojciech Matusik", "title": "ChainQueen: A Real-Time Differentiable Physical Simulator for Soft\n  Robotics", "comments": "In submission to ICRA 2019. Supplemental Video:\n  https://www.youtube.com/watch?v=4IWD4iGIsB4 Project Page:\n  https://github.com/yuanming-hu/ChainQueen", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical simulators have been widely used in robot planning and control.\nAmong them, differentiable simulators are particularly favored, as they can be\nincorporated into gradient-based optimization algorithms that are efficient in\nsolving inverse problems such as optimal control and motion planning.\nSimulating deformable objects is, however, more challenging compared to rigid\nbody dynamics. The underlying physical laws of deformable objects are more\ncomplex, and the resulting systems have orders of magnitude more degrees of\nfreedom and therefore they are significantly more computationally expensive to\nsimulate. Computing gradients with respect to physical design or controller\nparameters is typically even more computationally challenging. In this paper,\nwe propose a real-time, differentiable hybrid Lagrangian-Eulerian physical\nsimulator for deformable objects, ChainQueen, based on the Moving Least Squares\nMaterial Point Method (MLS-MPM). MLS-MPM can simulate deformable objects\nincluding contact and can be seamlessly incorporated into inference, control\nand co-design systems. We demonstrate that our simulator achieves high\nprecision in both forward simulation and backward gradient computation. We have\nsuccessfully employed it in a diverse set of control tasks for soft robots,\nincluding problems with nearly 3,000 decision variables.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 03:48:42 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Hu", "Yuanming", ""], ["Liu", "Jiancheng", ""], ["Spielberg", "Andrew", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""], ["Wu", "Jiajun", ""], ["Rus", "Daniela", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1810.01175", "submitter": "Pierre B\\'enard", "authors": "Pierre B\\'enard and Aaron Hertzmann", "title": "Line Drawings from 3D Models", "comments": "Submitted to Foundations and Trend in Computer Graphics and Vision", "journal-ref": "Foundations and Trends in Computer Graphics and Vision (2019).\n  Vol. 11: No. 1-2, pp 1-159", "doi": "10.1561/0600000075", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial describes the geometry and algorithms for generating line\ndrawings from 3D models, focusing on occluding contours.\n  The geometry of occluding contours on meshes and on smooth surfaces is\ndescribed in detail, together with algorithms for extracting contours,\ncomputing their visibility, and creating stylized renderings and animations.\nExact methods and hardware-accelerated fast methods are both described, and the\ntrade-offs between different methods are discussed. The tutorial brings\ntogether and organizes material that, at present, is scattered throughout the\nliterature. It also includes some novel explanations, and implementation tips.\n  A thorough survey of the field of non-photorealistic 3D rendering is also\nincluded, covering other kinds of line drawings and artistic shading.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 11:28:10 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 10:52:27 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["B\u00e9nard", "Pierre", ""], ["Hertzmann", "Aaron", ""]]}, {"id": "1810.01406", "submitter": "Ke Li", "authors": "Ke Li, Shichong Peng, Jitendra Malik", "title": "Super-Resolution via Conditional Implicit Maximum Likelihood Estimation", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution (SISR) is a canonical problem with diverse\napplications. Leading methods like SRGAN produce images that contain various\nartifacts, such as high-frequency noise, hallucinated colours and shape\ndistortions, which adversely affect the realism of the result. In this paper,\nwe propose an alternative approach based on an extension of the method of\nImplicit Maximum Likelihood Estimation (IMLE). We demonstrate greater\neffectiveness at noise reduction and preservation of the original colours and\nshapes, yielding more realistic super-resolved images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 17:58:02 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Li", "Ke", ""], ["Peng", "Shichong", ""], ["Malik", "Jitendra", ""]]}, {"id": "1810.01575", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Guandao Yang, Aditya Prakash, Qiuren Fang, Hanqing\n  Jiang, Bharath Hariharan, Serge Belongie", "title": "Deep Fundamental Matrix Estimation without Correspondences", "comments": "ECCV 2018, Geometry Meets Deep Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating fundamental matrices is a classic problem in computer vision.\nTraditional methods rely heavily on the correctness of estimated key-point\ncorrespondences, which can be noisy and unreliable. As a result, it is\ndifficult for these methods to handle image pairs with large occlusion or\nsignificantly different camera poses. In this paper, we propose novel neural\nnetwork architectures to estimate fundamental matrices in an end-to-end manner\nwithout relying on point correspondences. New modules and layers are introduced\nin order to preserve mathematical properties of the fundamental matrix as a\nhomogeneous rank-2 matrix with seven degrees of freedom. We analyze performance\nof the proposed models using various metrics on the KITTI dataset, and show\nthat they achieve competitive performance with traditional methods without the\nneed for extracting correspondences.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 03:59:15 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Poursaeed", "Omid", ""], ["Yang", "Guandao", ""], ["Prakash", "Aditya", ""], ["Fang", "Qiuren", ""], ["Jiang", "Hanqing", ""], ["Hariharan", "Bharath", ""], ["Belongie", "Serge", ""]]}, {"id": "1810.02042", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Lin Gao, Yu-Kun Lai, and Shihong Xia", "title": "Learning Bidirectional LSTM Networks for Synthesizing 3D Mesh Animation\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for learning to synthesize 3D mesh\nanimation sequences with long short-term memory (LSTM) blocks and mesh-based\nconvolutional neural networks (CNNs). Synthesizing realistic 3D mesh animation\nsequences is a challenging and important task in computer animation. To achieve\nthis, researchers have long been focusing on shape analysis to develop new\ninterpolation and extrapolation techniques. However, such techniques have\nlimited learning capabilities and therefore can produce unrealistic animation.\nDeep architectures that operate directly on mesh sequences remain unexplored,\ndue to the following major barriers: meshes with irregular triangles, sequences\ncontaining rich temporal information and flexible deformations. To address\nthese, we utilize convolutional neural networks defined on triangular meshes\nalong with a shape deformation representation to extract useful features,\nfollowed by LSTM cells that iteratively process the features. To allow\ncompletion of a missing mesh sequence from given endpoints, we propose a new\nweight-shared bidirectional structure. The bidirectional generation loss also\nhelps mitigate error accumulation over iterations. Benefiting from all these\ntechnical advances, our approach outperforms existing methods in sequence\nprediction and completion both qualitatively and quantitatively. Moreover, this\nnetwork can also generate follow-up frames conditioned on initial shapes and\nimprove the accuracy as more bootstrap models are provided, which other works\nin the geometry processing domain cannot achieve.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 03:43:16 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Gao", "Lin", ""], ["Lai", "Yu-Kun", ""], ["Xia", "Shihong", ""]]}, {"id": "1810.02303", "submitter": "Adrien Poulenard", "authors": "Adrien Poulenard, Maks Ovsjanikov", "title": "Multi-directional Geodesic Neural Networks via Equivariant Convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for performing convolution of signals on curved\nsurfaces and show its utility in a variety of geometric deep learning\napplications. Key to our construction is the notion of directional functions\ndefined on the surface, which extend the classic real-valued signals and which\ncan be naturally convolved with with real-valued template functions. As a\nresult, rather than trying to fix a canonical orientation or only keeping the\nmaximal response across all alignments of a 2D template at every point of the\nsurface, as done in previous works, we show how information across all\nrotations can be kept across different layers of the neural network. Our\nconstruction, which we call multi-directional geodesic convolution, or\ndirectional convolution for short, allows, in particular, to propagate and\nrelate directional information across layers and thus different regions on the\nshape. We first define directional convolution in the continuous setting, prove\nits key properties and then show how it can be implemented in practice, for\nshapes represented as triangle meshes. We evaluate directional convolution in a\nwide variety of learning scenarios ranging from classification of signals on\nsurfaces, to shape segmentation and shape matching, where we show a significant\nimprovement over several baselines.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:03:24 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Poulenard", "Adrien", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1810.02363", "submitter": "F\\'elix G. Harvey", "authors": "F\\'elix G. Harvey, Christopher Pal", "title": "Recurrent Transition Networks for Character Locomotion", "comments": "revision fixes: clarity issues in Section 4.4 (text and equations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually authoring transition animations for a complete locomotion system can\nbe a tedious and time-consuming task, especially for large games that allow\ncomplex and constrained locomotion movements, where the number of transitions\ngrows exponentially with the number of states. In this paper, we present a\nnovel approach, based on deep recurrent neural networks, to automatically\ngenerate such transitions given a past context of a few frames and a target\ncharacter state to reach. We present the Recurrent Transition Network (RTN),\nbased on a modified version of the Long-Short-Term-Memory (LSTM) network,\ndesigned specifically for transition generation and trained without any gait,\nphase, contact or action labels. We further propose a simple yet principled way\nto initialize the hidden states of the LSTM layer for a given sequence which\nimproves the performance and generalization to new motions. We both\nquantitatively and qualitatively evaluate our system and show that making the\nnetwork terrain-aware by adding a local terrain representation to the input\nyields better performance for rough-terrain navigation on long transitions. Our\nsystem produces realistic and fluid transitions that rival the quality of\nMotion Capture-based ground-truth motions, even before applying any\ninverse-kinematics postprocess. Direct benefits of our approach could be to\naccelerate the creation of transition variations for large coverage, or even to\nentirely replace transition nodes in an animation graph. We further explore\napplications of this model in a animation super-resolution setting where we\ntemporally decompress animations saved at 1 frame per second and show that the\nnetwork is able to reconstruct motions that are hard to distinguish from\nun-compressed locomotion sequences.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 15:12:13 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 14:40:10 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 19:44:41 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 21:23:44 GMT"}, {"version": "v5", "created": "Thu, 18 Mar 2021 20:00:10 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Harvey", "F\u00e9lix G.", ""], ["Pal", "Christopher", ""]]}, {"id": "1810.02460", "submitter": "Marcel Campen", "authors": "Marcel Campen, Hanxiao Shen, Jiaran Zhou, Denis Zorin", "title": "Seamless Parametrization with Arbitrarily Prescribed Cones", "comments": "17 pages, 21 figures", "journal-ref": "ACM Transactions on Graphics 39, 1, (2019)", "doi": "10.1145/3360511", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seamless global parametrization of surfaces is a key operation in geometry\nprocessing, e.g. for high-quality quad mesh generation. A common approach is to\nprescribe the parametric domain structure, in particular the locations of\nparametrization singularities (cones), and solve a non-convex optimization\nproblem minimizing a distortion measure, with local injectivity imposed through\neither constraints or barrier terms. In both cases, an initial valid\nparametrization is essential to serve as feasible starting point for obtaining\nan optimized solution. While convexified versions of the constraints eliminate\nthis initialization requirement, they narrow the range of solutions, causing\nsome problem instances that actually do have a solution to become infeasible.\nWe demonstrate that for arbitrary given sets of topologically admissible\nparametric cones with prescribed curvature, a global seamless parametrization\nalways exists (with the exception of one well-known case). Importantly, our\nproof is constructive and directly leads to a general algorithm for computing\nsuch parametrizations. Most distinctively, this algorithm is bootstrapped with\na convex optimization problem (solving for a conformal map), in tandem with a\nsimple linear equation system (determining a seamless modification of this\nmap). This initial map can then serve as valid starting point and be optimized\nwith respect to application specific distortion measures using existing\ninjectivity preserving methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 23:47:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Campen", "Marcel", ""], ["Shen", "Hanxiao", ""], ["Zhou", "Jiaran", ""], ["Zorin", "Denis", ""]]}, {"id": "1810.03599", "submitter": "Xue Bin Peng", "authors": "Xue Bin Peng, Angjoo Kanazawa, Jitendra Malik, Pieter Abbeel, Sergey\n  Levine", "title": "SFV: Reinforcement Learning of Physical Skills from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven character animation based on motion capture can produce highly\nnaturalistic behaviors and, when combined with physics simulation, can provide\nfor natural procedural responses to physical perturbations, environmental\nchanges, and morphological discrepancies. Motion capture remains the most\npopular source of motion data, but collecting mocap data typically requires\nheavily instrumented environments and actors. In this paper, we propose a\nmethod that enables physically simulated characters to learn skills from videos\n(SFV). Our approach, based on deep pose estimation and deep reinforcement\nlearning, allows data-driven animation to leverage the abundance of publicly\navailable video clips from the web, such as those from YouTube. This has the\npotential to enable fast and easy design of character controllers simply by\nquerying for video recordings of the desired behavior. The resulting\ncontrollers are robust to perturbations, can be adapted to new settings, can\nperform basic object interactions, and can be retargeted to new morphologies\nvia reinforcement learning. We further demonstrate that our method can predict\npotential human motions from still images, by forward simulation of learned\ncontrollers initialized from the observed pose. Our framework is able to learn\na broad range of dynamic skills, including locomotion, acrobatics, and martial\narts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 17:55:39 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 17:15:34 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Peng", "Xue Bin", ""], ["Kanazawa", "Angjoo", ""], ["Malik", "Jitendra", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1810.04703", "submitter": "Yinghao Huang", "authors": "Yinghao Huang, Manuel Kaufmann, Emre Aksan, Michael J. Black, Otmar\n  Hilliges, Gerard Pons-Moll", "title": "Deep Inertial Poser: Learning to Reconstruct Human Pose from Sparse\n  Inertial Measurements in Real Time", "comments": "SIGGRAPH Asia 2018. First two authors contributed equally to this\n  work. Project page: http://dip.is.tue.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a novel deep neural network capable of reconstructing human\nfull body pose in real-time from 6 Inertial Measurement Units (IMUs) worn on\nthe user's body. In doing so, we address several difficult challenges. First,\nthe problem is severely under-constrained as multiple pose parameters produce\nthe same IMU orientations. Second, capturing IMU data in conjunction with\nground-truth poses is expensive and difficult to do in many target application\nscenarios (e.g., outdoors). Third, modeling temporal dependencies through\nnon-linear optimization has proven effective in prior work but makes real-time\nprediction infeasible. To address this important limitation, we learn the\ntemporal pose priors using deep learning. To learn from sufficient data, we\nsynthesize IMU data from motion capture datasets. A bi-directional RNN\narchitecture leverages past and future information that is available at\ntraining time. At test time, we deploy the network in a sliding window fashion,\nretaining real time capabilities. To evaluate our method, we recorded DIP-IMU,\na dataset consisting of $10$ subjects wearing 17 IMUs for validation in $64$\nsequences with $330\\,000$ time instants; this constitutes the largest IMU\ndataset publicly available. We quantitatively evaluate our approach on multiple\ndatasets and show results from a real-time implementation. DIP-IMU and the code\nare available for research purposes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 18:45:55 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Huang", "Yinghao", ""], ["Kaufmann", "Manuel", ""], ["Aksan", "Emre", ""], ["Black", "Michael J.", ""], ["Hilliges", "Otmar", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1810.06514", "submitter": "Anpei Chen", "authors": "Anpei Chen, Minye Wu, Yingliang Zhang, Nianyi Li, Jie Lu, Shenghua\n  Gao, and Jingyi Yu", "title": "Deep Surface Light Fields", "comments": null, "journal-ref": null, "doi": "10.1145/3203192", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A surface light field represents the radiance of rays originating from any\npoints on the surface in any directions. Traditional approaches require\nultra-dense sampling to ensure the rendering quality. In this paper, we present\na novel neural network based technique called deep surface light field or DSLF\nto use only moderate sampling for high fidelity rendering. DSLF automatically\nfills in the missing data by leveraging different sampling patterns across the\nvertices and at the same time eliminates redundancies due to the network's\nprediction capability. For real data, we address the image registration problem\nas well as conduct texture-aware remeshing for aligning texture edges with\nvertices to avoid blurring. Comprehensive experiments show that DSLF can\nfurther achieve high data compression ratio while facilitating real-time\nrendering on the GPU.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 16:56:58 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chen", "Anpei", ""], ["Wu", "Minye", ""], ["Zhang", "Yingliang", ""], ["Li", "Nianyi", ""], ["Lu", "Jie", ""], ["Gao", "Shenghua", ""], ["Yu", "Jingyi", ""]]}, {"id": "1810.06884", "submitter": "Amir Vaxman", "authors": "Bram Custers and Amir Vaxman", "title": "Subdivision Directional Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel linear subdivision scheme for face-based tangent\ndirectional fields on triangle meshes. Our subdivision scheme is based on a\nnovel coordinate-free representation of directional fields as halfedge-based\nscalar quantities, bridging the finite-element representation with discrete\nexterior calculus. By commuting with differential operators, our subdivision is\nstructure-preserving: it reproduces curl-free fields precisely, and reproduces\ndivergence-free fields in the weak sense. Moreover, our subdivision scheme\ndirectly extends to directional fields with several vectors per face by working\non the branched covering space. Finally, we demonstrate how our scheme can be\napplied to directional-field design, advection, and robust earth mover's\ndistance computation, for efficient and robust computation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:57:41 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 10:07:02 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Custers", "Bram", ""], ["Vaxman", "Amir", ""]]}, {"id": "1810.07451", "submitter": "Georg Muntingh PhD", "authors": "Andrea Raffo, Oliver J.D. Barrowclough, Georg Muntingh", "title": "Reverse engineering of CAD models via clustering and approximate\n  implicitization", "comments": null, "journal-ref": null, "doi": "10.1016/j.cagd.2020.101876", "report-no": null, "categories": "math.NA cs.GR cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications like computer aided design, geometric models are often\nrepresented numerically as polynomial splines or NURBS, even when they\noriginate from primitive geometry. For purposes such as redesign and\nisogeometric analysis, it is of interest to extract information about the\nunderlying geometry through reverse engineering. In this work we develop a\nnovel method to determine these primitive shapes by combining clustering\nanalysis with approximate implicitization. The proposed method is automatic and\ncan recover algebraic hypersurfaces of any degree in any dimension. In exact\narithmetic, the algorithm returns exact results. All the required parameters,\nsuch as the implicit degree of the patches and the number of clusters of the\nmodel, are inferred using numerical approaches in order to obtain an algorithm\nthat requires as little manual input as possible. The effectiveness, efficiency\nand robustness of the method are shown both in a theoretical analysis and in\nnumerical examples implemented in Python.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 09:32:50 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:06:41 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 17:56:05 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Raffo", "Andrea", ""], ["Barrowclough", "Oliver J. D.", ""], ["Muntingh", "Georg", ""]]}, {"id": "1810.07882", "submitter": "Jian Chen", "authors": "Jian Chen and Guohao Zhang and Wesley Chiou and David H. Laidlaw and\n  Alexander P. Auchus", "title": "Measuring the Effects of Scalar and Spherical Colormaps on Ensembles of\n  DMRI Tubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report empirical study results on the color encoding of ensemble scalar\nand orientation to visualize diffusion magnetic resonance imaging (DMRI) tubes.\nThe experiment tested six scalar colormaps for average fractional anisotropy\n(FA) tasks (grayscale, blackbody, diverging, isoluminant-rainbow,\nextended-blackbody, and coolwarm) and four three-dimensional (3D) directional\nencodings for tract tracing tasks (uniform gray, absolute, eigenmap, and Boy's\nsurface embedding). We found that extended-blackbody, coolwarm, and blackbody\nremain the best three approaches for identifying ensemble average in 3D.\nIsoluminant-rainbow coloring led to the same ensemble mean accuracy as other\ncolormaps. However, more than 50% of the answers consistently had higher\nestimates of the ensemble average, independent of the mean values. Hue, not\nluminance, influences ensemble estimates of mean values. For ensemble\norientation-tracing tasks, we found that the Boy's surface embedding (greatest\nspatial resolution and contrast) and absolute color (lowest spatial resolution\nand contrast) schemes led to more accurate answers than the eigenmaps scheme\n(medium resolution and contrast), acting as the uncanny-valley phenomenon of\nvisualization design in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 02:46:20 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Chen", "Jian", ""], ["Zhang", "Guohao", ""], ["Chiou", "Wesley", ""], ["Laidlaw", "David H.", ""], ["Auchus", "Alexander P.", ""]]}, {"id": "1810.07982", "submitter": "Xiao Xiao", "authors": "Xiao Xiao, Malcolm Sabin, Fehmi Cirak", "title": "Interrogation of spline surfaces with application to isogeometric design\n  and analysis of lattice-skin structures", "comments": "23 pages, 15 figures", "journal-ref": "Computer Methods in Applied Mechanics and Engineering 351 (2019)\n  928-950", "doi": "10.1016/j.cma.2019.03.046", "report-no": null, "categories": "math.NA cs.GR cs.NA math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel surface interrogation technique is proposed to compute the\nintersection of curves with spline surfaces in isogeometric analysis. The\nintersection points are determined in one-shot without resorting to a\nNewton-Raphson iteration or successive refinement. Surface-curve intersection\nis required in a wide range of applications, including contact, immersed\nboundary methods and lattice-skin structures, and requires usually the solution\nof a system of nonlinear equations. It is assumed that the surface is given in\nform of a spline, such as a NURBS, T-spline or Catmull-Clark subdivision\nsurface, and is convertible into a collection of B\\'ezier patches. First, a\nhierarchical bounding volume tree is used to efficiently identify the B\\'ezier\npatches with a convex-hull intersecting the convex-hull of a given curve\nsegment. For ease of implementation convex-hulls are approximated with k-dops\n(discrete orientation polytopes). Subsequently, the intersections of the\nidentified B\\'ezier patches with the curve segment are determined with a\nmatrix-based implicit representation leading to the computation of a sequence\nof small singular value decompositions (SVDs). As an application of the\ndeveloped interrogation technique the isogeometric design and analysis of\nlattice-skin structures is investigated. The skin is a spline surface that is\nusually created in a computer-aided design (CAD) system and the periodic\nlattice to be fitted consists of unit cells, each containing a small number of\nstruts. The lattice-skin structure is generated by projecting selected lattice\nnodes onto the surface after determining the intersection of unit cell edges\nwith the surface. For mechanical analysis, the skin is modelled as a\nKirchhoff-Love thin-shell and the lattice as a pin-jointed truss. The two types\nof structures are coupled with a standard Lagrange multiplier approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 10:26:51 GMT"}, {"version": "v2", "created": "Sun, 12 May 2019 15:22:12 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Xiao", "Xiao", ""], ["Sabin", "Malcolm", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1810.08860", "submitter": "Ryan Overbeck", "authors": "Ryan S. Overbeck, Daniel Erickson, Daniel Evangelakos, Matt Pharr,\n  Paul Debevec", "title": "A System for Acquiring, Processing, and Rendering Panoramic Light Field\n  Stills for Virtual Reality", "comments": "15 pages, 14 figures, 2 tables, accepted by SIGGRAPH Asia 2018,\n  low-resolution version", "journal-ref": null, "doi": "10.1145/3272127.3275031", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for acquiring, processing, and rendering panoramic light\nfield still photography for display in Virtual Reality (VR). We acquire\nspherical light field datasets with two novel light field camera rigs designed\nfor portable and efficient light field acquisition. We introduce a novel\nreal-time light field reconstruction algorithm that uses a per-view geometry\nand a disk-based blending field. We also demonstrate how to use a light field\nprefiltering operation to project from a high-quality offline reconstruction\nmodel into our real-time model while suppressing artifacts. We introduce a\npractical approach for compressing light fields by modifying the VP9 video\ncodec to provide high quality compression with real-time, random access\ndecompression.\n  We combine these components into a complete light field system offering\nconvenient acquisition, compact file size, and high-quality rendering while\ngenerating stereo views at 90Hz on commodity VR hardware. Using our system, we\nbuilt a freely available light field experience application called Welcome to\nLight Fields featuring a library of panoramic light field stills for consumer\nVR which has been downloaded over 15,000 times.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 22:26:27 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Overbeck", "Ryan S.", ""], ["Erickson", "Daniel", ""], ["Evangelakos", "Daniel", ""], ["Pharr", "Matt", ""], ["Debevec", "Paul", ""]]}, {"id": "1810.09031", "submitter": "Saad Nadeem", "authors": "Saad Nadeem, Zhengyu Su, Wei Zeng, Arie Kaufman and Xianfeng Gu", "title": "Spherical Parameterization Balancing Angle and Area Distortions", "comments": "IEEE Transactions on Visualization and Computer Graphics,\n  23(6):1663-1676, 2017 (17 pages, 20 figures)", "journal-ref": "IEEE Trans. Vis. Comput. Graph., 23(6), pp.1663-1676, 2017", "doi": "10.1109/TVCG.2016.2542073", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel framework for spherical mesh parameterization. An\nefficient angle-preserving spherical parameterization algorithm is introduced,\nwhich is based on dynamic Yamabe flow and the conformal welding method with\nsolid theoretic foundation. An area-preserving spherical parameterization is\nalso discussed, which is based on discrete optimal mass transport theory.\nFurthermore, a spherical parameterization algorithm, which is based on the\npolar decomposition method, balancing angle distortion and area distortion is\npresented. The algorithms are tested on 3D geometric data and the experiments\ndemonstrate the efficiency and efficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 21:39:32 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Nadeem", "Saad", ""], ["Su", "Zhengyu", ""], ["Zeng", "Wei", ""], ["Kaufman", "Arie", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1810.09718", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis\n  and Adrien Bousseau", "title": "Single-Image SVBRDF Capture with a Rendering-Aware Deep Network", "comments": "15 pages, presented at Siggraph 2018", "journal-ref": "ACM Trans. Graph. 37, 4, Article 128 (August 2018), 15 pages", "doi": "10.1145/3197517.3201378", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture, highlights, and shading are some of many visual cues that allow\nhumans to perceive material appearance in single pictures. Yet, recovering\nspatially-varying bi-directional reflectance distribution functions (SVBRDFs)\nfrom a single image based on such cues has challenged researchers in computer\ngraphics for decades. We tackle lightweight appearance capture by training a\ndeep neural network to automatically extract and make sense of these visual\ncues. Once trained, our network is capable of recovering per-pixel normal,\ndiffuse albedo, specular albedo and specular roughness from a single picture of\na flat surface lit by a hand-held flash. We achieve this goal by introducing\nseveral innovations on training data acquisition and network design. For\ntraining, we leverage a large dataset of artist-created, procedural SVBRDFs\nwhich we sample and render under multiple lighting directions. We further\namplify the data by material mixing to cover a wide diversity of shading\neffects, which allows our network to work across many material classes.\nMotivated by the observation that distant regions of a material sample often\noffer complementary visual cues, we design a network that combines an\nencoder-decoder convolutional track for local feature extraction with a\nfully-connected track for global feature extraction and propagation. Many\nimportant material effects are view-dependent, and as such ambiguous when\nobserved in a single image. We tackle this challenge by defining the loss as a\ndifferentiable SVBRDF similarity metric that compares the renderings of the\npredicted maps against renderings of the ground truth from several lighting and\nviewing directions. Combined together, these novel ingredients bring clear\nimprovement over state of the art methods for single-shot capture of spatially\nvarying BRDFs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 08:30:34 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Aittala", "Miika", ""], ["Durand", "Fredo", ""], ["Drettakis", "George", ""], ["Bousseau", "Adrien", ""]]}, {"id": "1810.10206", "submitter": "Jean-Daniel Taupiac", "authors": "Jean-Daniel Taupiac (ICAR), Nancy Rodriguez (ICAR), Olivier Strauss\n  (ICAR)", "title": "Immercity: a curation content application in Virtual and Augmented\n  reality", "comments": null, "journal-ref": "10th International Conference, VAMR 2018, Held as Part of HCI\n  International 2018, II (2), Springer, 2018, Virtual, Augmented and Mixed\n  Reality: Applications in Health, Cultural Heritage, and Industry,\n  978-3-319-91583-8. https://link.springer.com/book/10.1007/978-3-319-91584-5", "doi": "10.1007/978-3-319-91584-5_18", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with emergent and appealing technologies as Virtual Reality,\nMixed Reality and Augmented Reality, the issue of definitions appear very\noften. Indeed, our experience with various publics allows us to notice that\ntechnology definitions pose ambiguity and representation problems for informed\nas well as novice users. In this paper we present Immercity, a content curation\nsystem designed in the context of a collaboration between the University of\nMontpellier and CapGemi-ni, to deliver a technology watch. It is also used as a\ntestbed for our experiences with Virtual, Mixed and Augmented reality to\nexplore new interaction techniques and devices, artificial intelligence\nintegration, visual affordances, performance , etc. But another, very\ninteresting goal appeared: use Immercity to communicate about Virtual, Mixed\nand Augmented Reality by using them as a support.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 06:23:46 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Taupiac", "Jean-Daniel", "", "ICAR"], ["Rodriguez", "Nancy", "", "ICAR"], ["Strauss", "Olivier", "", "ICAR"]]}, {"id": "1810.10933", "submitter": "Reed Williams", "authors": "Reed M. Williams, Horea T. Ilie\\c{s}", "title": "Practical Shape Analysis and Segmentation Methods for Point Cloud Models", "comments": "21 pages, 20 figures. To appear in The Journal of Computer Aided\n  Geometric Design's Special Issue on Heat Diffusion Equation and Optimal\n  Transport in Geometry Processing and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current point cloud processing algorithms do not have the capability to\nautomatically extract semantic information from the observed scenes, except in\nvery specialized cases. Furthermore, existing mesh analysis paradigms cannot be\ndirectly employed to automatically perform typical shape analysis tasks\ndirectly on point cloud models.\n  We present a potent framework for shape analysis, similarity, and\nsegmentation of noisy point cloud models for real objects of engineering\ninterest, models that may be incomplete. The proposed framework relies on\nspectral methods and the heat diffusion kernel to construct compact shape\nsignatures, and we show that the framework supports a variety of clustering\ntechniques that have traditionally been applied only on mesh models. We\ndeveloped and implemented one practical and convergent estimate of the\nLaplace-Beltrami operator for point clouds as well as a number of clustering\ntechniques adapted to work directly on point clouds to produce geometric\nfeatures of engineering interest. The key advantage of this framework is that\nit supports practical shape analysis capabilities that operate directly on\npoint cloud models of objects without requiring surface reconstruction or\nglobal meshing. We show that the proposed technique is robust against typical\nnoise present in possibly incomplete point clouds, and segment point clouds\nscanned by depth cameras (e.g. Kinect) into semantically-meaningful sub-shapes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 15:38:30 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Williams", "Reed M.", ""], ["Ilie\u015f", "Horea T.", ""]]}, {"id": "1810.11154", "submitter": "Erva Ulu", "authors": "Erva Ulu and James McCann and Levent Burak Kara", "title": "Lightweight Structure Design Under Force Location Uncertainty", "comments": "SIGGRAPH 2017", "journal-ref": "ACM Transactions on Graphics (TOG), 36(4), 158, 2017", "doi": "10.1145/3072959.3073626", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a lightweight structure optimization approach for problems in\nwhich there is uncertainty in the force locations. Such uncertainty may arise\ndue to force contact locations that change during use or are simply unknown a\npriori. Given an input 3D model, regions on its boundary where arbitrary normal\nforces may make contact, and a total force-magnitude budget, our algorithm\ngenerates a minimum weight 3D structure that withstands any force configuration\ncapped by the budget. Our approach works by repeatedly finding the most\ncritical force configuration and altering the internal structure accordingly. A\nkey issue, however, is that the critical force configuration changes as the\nstructure evolves, resulting in a significant computational challenge. To\naddress this, we propose an efficient critical instant analysis approach.\nCombined with a reduced order formulation, our method provides a practical\nsolution to the structural optimization problem. We demonstrate our method on a\nvariety of models and validate it with mechanical tests.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 00:42:34 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 17:22:34 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Ulu", "Erva", ""], ["McCann", "James", ""], ["Kara", "Levent Burak", ""]]}, {"id": "1810.11220", "submitter": "Yun Zhang", "authors": "Yun Zhang, Yu-Kun Lai, Fang-Lue Zhang", "title": "Content-Preserving Image Stitching with Regular Boundary Constraints", "comments": "12 figures, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to content-preserving stitching of images\nwith regular boundary constraints, which aims to stitch multiple images to\ngenerate a panoramic image with regular boundary. Existing methods treat image\nstitching and rectangling as two separate steps, which may result in suboptimal\nresults as the stitching process is not aware of the further warping needs for\nrectangling. We address these limitations by formulating image stitching with\nregular boundaries in a unified optimization. Starting from the initial\nstitching results produced by traditional warping-based optimization, we obtain\nthe irregular boundary from the warped meshes by polygon Boolean operations\nwhich robustly handle arbitrary mesh compositions, and by analyzing the\nirregular boundary construct a piecewise rectangular boundary. Based on this,\nwe further incorporate straight line preserving and regular boundary\nconstraints into the image stitching framework, and conduct iterative\noptimization to obtain an optimal piecewise rectangular boundary, thus can make\nthe panoramic boundary as close as possible to a rectangle, while reducing\nunwanted distortions. We further extend our method to panoramic videos and\nselfie photography, by integrating the temporal coherence and portrait\npreservation into the optimization. Experiments show that our method\nefficiently produces visually pleasing panoramas with regular boundaries and\nunnoticeable distortions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 08:09:53 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 11:40:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhang", "Yun", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Fang-Lue", ""]]}, {"id": "1810.11536", "submitter": "Zhihao Zhu", "authors": "Zhihao Zhu, Zhan Xue, Zejian Yuan", "title": "Automatic Graphics Program Generation using Attention-Based Hierarchical\n  Decoder", "comments": "Asian Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on deep learning has made it possible to automatically\ntransform the screenshot of Graphic User Interface (GUI) into code by using the\nencoder-decoder framework. While the commonly adopted image encoder (e.g., CNN\nnetwork), might be capable of extracting image features to the desired level,\ninterpreting these abstract image features into hundreds of tokens of code puts\na particular challenge on the decoding power of the RNN-based code generator.\nConsidering the code used for describing GUI is usually hierarchically\nstructured, we propose a new attention-based hierarchical code generation\nmodel, which can describe GUI images in a finer level of details, while also\nbeing able to generate hierarchically structured code in consistency with the\nhierarchical layout of the graphic elements in the GUI. Our model follows the\nencoder-decoder framework, all the components of which can be trained jointly\nin an end-to-end manner. The experimental results show that our method\noutperforms other current state-of-the-art methods on both a publicly available\nGUI-code dataset as well as a dataset established by our own.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 21:28:10 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Zhu", "Zhihao", ""], ["Xue", "Zhan", ""], ["Yuan", "Zejian", ""]]}, {"id": "1810.13251", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Koji Yatani, Mark D. Gross, Tom Yeh", "title": "Tabby: Explorable Design for 3D Printing Textures", "comments": "Pacific Graphics 2018. arXiv admin note: substantial text overlap\n  with arXiv:1703.05700", "journal-ref": null, "doi": "10.2312/pg.20181273", "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Tabby, an interactive and explorable design tool for 3D\nprinting textures. Tabby allows texture design with direct manipulation in the\nfollowing workflow: 1) select a target surface, 2) sketch and manipulate a\ntexture with 2D drawings, and then 3) generate 3D printing textures onto an\narbitrary curved surface. To enable efficient texture creation, Tabby leverages\nan auto-completion approach which automates the tedious, repetitive process of\napplying texture, while allowing flexible customization. Our user evaluation\nstudy with seven participants confirms that Tabby can effectively support the\ndesign exploration of different patterns for both novice and experienced users.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:40:06 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Suzuki", "Ryo", ""], ["Yatani", "Koji", ""], ["Gross", "Mark D.", ""], ["Yeh", "Tom", ""]]}]