[{"id": "2101.00631", "submitter": "Monan Wang", "authors": "Xin Wang, Su Gao, Monan Wang, Zhenghua Duan", "title": "A Marching Cube Algorithm Based on Edge Growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marching Cube algorithm is currently one of the most popular 3D\nreconstruction surface rendering algorithms. It forms cube voxels through the\ninput image, and then uses 15 basic topological configurations to extract the\niso-surfaces in the voxels. It processes each cube voxel in a traversal manner,\nbut it does not consider the relationship between iso-surfaces in adjacent\ncubes. Due to ambiguity, the final reconstructed model may have holes. We\npropose a Marching Cube algorithm based on edge growth. The algorithm first\nextracts seed triangles, then grows the seed triangles and reconstructs the\nentire 3D model. According to the position of the growth edge, we propose 17\ntopological configurations with iso-surfaces. From the reconstruction results,\nthe algorithm can reconstruct the 3D model well. When only the main contour of\nthe 3D model needs to be organized, the algorithm performs well. In addition,\nwhen there are multiple scattered parts in the data, the algorithm can extract\nonly the 3D contours of the parts connected to the seed by setting the region\nselected by the seed.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 14:34:21 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 13:14:52 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 04:08:12 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wang", "Xin", ""], ["Gao", "Su", ""], ["Wang", "Monan", ""], ["Duan", "Zhenghua", ""]]}, {"id": "2101.01036", "submitter": "Jian Chen", "authors": "Jian Chen and Meng Ling and Rui Li and Petra Isenberg and Tobias\n  Isenberg and Michael Sedlmair and Torsten M\\\"oller and Robert S. Laramee and\n  Han-Wei Shen and Katharina W\\\"unsche and Qiru Wang", "title": "VIS30K: A Collection of Figures and Tables from IEEE Visualization\n  Conference Publications", "comments": "12 pages", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 27\n  January 2021", "doi": "10.1109/TVCG.2021.3054916", "report-no": "33502982", "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the VIS30K dataset, a collection of 29,689 images that represents\n30 years of figures and tables from each track of the IEEE Visualization\nconference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage\nof the scientific literature in visualization not only reflects the progress of\nthe field but also enables researchers to study the evolution of the\nstate-of-the-art and to find relevant work based on graphical content. We\ndescribe the dataset and our semi-automatic collection process, which couples\nconvolutional neural networks (CNN) with curation. Extracting figures and\ntables semi-automatically allows us to verify that no images are overlooked or\nextracted erroneously. To improve quality further, we engaged in a peer-search\nprocess for high-quality figures from early IEEE Visualization papers. With the\nresulting data, we also contribute VISImageNavigator (VIN,\nvisimagenavigator.github.io), a web-based tool that facilitates searching and\nexploring VIS30K by author names, paper keywords, title and abstract, and\nyears.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:56:29 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 03:05:49 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 11:50:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Jian", ""], ["Ling", "Meng", ""], ["Li", "Rui", ""], ["Isenberg", "Petra", ""], ["Isenberg", "Tobias", ""], ["Sedlmair", "Michael", ""], ["M\u00f6ller", "Torsten", ""], ["Laramee", "Robert S.", ""], ["Shen", "Han-Wei", ""], ["W\u00fcnsche", "Katharina", ""], ["Wang", "Qiru", ""]]}, {"id": "2101.01602", "submitter": "Wentao Yuan", "authors": "Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, Steven Lovegrove", "title": "STaR: Self-supervised Tracking and Reconstruction of Rigid Objects in\n  Motion with Neural Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present STaR, a novel method that performs Self-supervised Tracking and\nReconstruction of dynamic scenes with rigid motion from multi-view RGB videos\nwithout any manual annotation. Recent work has shown that neural networks are\nsurprisingly effective at the task of compressing many views of a scene into a\nlearned function which maps from a viewing ray to an observed radiance value\nvia volume rendering. Unfortunately, these methods lose all their predictive\npower once any object in the scene has moved. In this work, we explicitly model\nrigid motion of objects in the context of neural representations of radiance\nfields. We show that without any additional human specified supervision, we can\nreconstruct a dynamic scene with a single rigid object in motion by\nsimultaneously decomposing it into its two constituent parts and encoding each\nwith its own neural representation. We achieve this by jointly optimizing the\nparameters of two neural radiance fields and a set of rigid poses which align\nthe two fields at each frame. On both synthetic and real world datasets, we\ndemonstrate that our method can render photorealistic novel views, where\nnovelty is measured on both spatial and temporal axes. Our factored\nrepresentation furthermore enables animation of unseen object motion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:45:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Yuan", "Wentao", ""], ["Lv", "Zhaoyang", ""], ["Schmidt", "Tanner", ""], ["Lovegrove", "Steven", ""]]}, {"id": "2101.01771", "submitter": "Efstratios Geronikolakis", "authors": "Efstratios Geronikolakis, George Papagiannakis", "title": "An XR rapid prototyping framework for interoperability across the\n  reality spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of the Extended Reality (XR) spectrum, a superset of Mixed,\nAugmented and Virtual Reality, are gaining prominence and can be employed in a\nvariety of areas, such as virtual museums. Examples can be found in the areas\nof education, cultural heritage, health/treatment, entertainment, marketing,\nand more. The majority of computer graphics applications nowadays are used to\noperate only in one of the above realities. The lack of applications across the\nXR spectrum is a real shortcoming. There are many advantages resulting from\nthis problem's solution. Firstly, releasing an application across the XR\nspectrum could contribute in discovering its most suitable reality. Moreover,\nan application could be more immersive within a particular reality, depending\non its context. Furthermore, its availability increases to a broader range of\nusers. For instance, if an application is released both in Virtual and\nAugmented Reality, it is accessible to users that may lack the possession of a\nVR headset, but not of a mobile AR device. The question that arises at this\npoint, would be \"Is it possible for a full s/w application stack to be\nconverted across XR without sacrificing UI/UX in a semi-automatic way?\". It may\nbe quite difficult, depending on the architecture and application\nimplementation. Most companies nowadays support only one reality, due to their\nlack of UI/UX software architecture or resources to support the complete XR\nspectrum. In this work, we present an \"automatic reality transition\" in the\ncontext of virtual museum applications. We propose a development framework,\nwhich will automatically allow this XR transition. This framework transforms\nany XR project into different realities such as Augmented or Virtual. It also\nreduces the development time while increasing the XR availability of 3D\napplications, encouraging developers to release applications across the XR\nspectrum.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:27:47 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 13:32:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Geronikolakis", "Efstratios", ""], ["Papagiannakis", "George", ""]]}, {"id": "2101.02285", "submitter": "Kathleen M Lewis", "authors": "Kathleen M Lewis, Srivatsan Varadharajan, Ira Kemelmacher-Shlizerman", "title": "TryOnGAN: Body-Aware Try-On via Layered Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of images-target person and garment on another person-we\nautomatically generate the target person in the given garment. Previous methods\nmostly focused on texture transfer via paired data training, while overlooking\nbody shape deformations, skin color, and seamless blending of garment with the\nperson. This work focuses on those three components, while also not requiring\npaired data training. We designed a pose conditioned StyleGAN2 architecture\nwith a clothing segmentation branch that is trained on images of people wearing\ngarments. Once trained, we propose a new layered latent space interpolation\nmethod that allows us to preserve and synthesize skin color and target body\nshape while transferring the garment from a different person. We demonstrate\nresults on high resolution 512x512 images, and extensively compare to state of\nthe art in try-on on both latent space generated and real images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 22:01:46 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 19:38:57 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lewis", "Kathleen M", ""], ["Varadharajan", "Srivatsan", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2101.02322", "submitter": "Zheng Liu", "authors": "Zheng Liu, YanLei Li, Weina Wang, Ligang Liu, and Renjie Chen", "title": "Mesh Total Generalized Variation for Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Total Generalized Variation (TGV) has recently been proven certainly\nsuccessful in image processing for preserving sharp features as well as smooth\ntransition variations. However, none of the existing works aims at numerically\ncalculating TGV over triangular meshes. In this paper, we develop a novel\nnumerical framework to discretize the second-order TGV over triangular meshes.\nFurther, we propose a TGV-based variational model to restore the face normal\nfield for mesh denoising. The TGV regularization in the proposed model is\nrepresented by a combination of a first- and second-order term, which can be\nautomatically balanced. This TGV regularization is able to locate sharp\nfeatures and preserve them via the first-order term, while recognize smoothly\ncurved regions and recover them via the second-order term. To solve the\noptimization problem, we introduce an efficient iterative algorithm based on\nvariable-splitting and augmented Lagrangian method. Extensive results and\ncomparisons on synthetic and real scanning data validate that the proposed\nmethod outperforms the state-of-the-art methods visually and numerically.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 01:38:09 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 08:44:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Liu", "Zheng", ""], ["Li", "YanLei", ""], ["Wang", "Weina", ""], ["Liu", "Ligang", ""], ["Chen", "Renjie", ""]]}, {"id": "2101.02496", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Ana Serrano, Diego Gutierrez, Belen Masia", "title": "The joint role of geometry and illumination on material recognition", "comments": "15 pages, 16 figures, Accepted to the Journal of Vision, 2021", "journal-ref": "Journal of Vision February 2021, Vol.21, 2", "doi": "10.1167/jov.21.2.2", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Observing and recognizing materials is a fundamental part of our daily life.\nUnder typical viewing conditions, we are capable of effortlessly identifying\nthe objects that surround us and recognizing the materials they are made of.\nNevertheless, understanding the underlying perceptual processes that take place\nto accurately discern the visual properties of an object is a long-standing\nproblem. In this work, we perform a comprehensive and systematic analysis of\nhow the interplay of geometry, illumination, and their spatial frequencies\naffects human performance on material recognition tasks. We carry out\nlarge-scale behavioral experiments where participants are asked to recognize\ndifferent reference materials among a pool of candidate samples. In the\ndifferent experiments, we carefully sample the information in the frequency\ndomain of the stimuli. From our analysis, we find significant first-order\ninteractions between the geometry and the illumination, of both the reference\nand the candidates. In addition, we observe that simple image statistics and\nhigher-order image histograms do not correlate with human performance.\nTherefore, we perform a high-level comparison of highly non-linear statistics\nby training a deep neural network on material recognition tasks. Our results\nshow that such models can accurately classify materials, which suggests that\nthey are capable of defining a meaningful representation of material appearance\nfrom labeled proximal image data. Last, we find preliminary evidence that these\nhighly non-linear models and humans may use similar high-level factors for\nmaterial recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 11:29:52 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 12:35:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Lagunas", "Manuel", ""], ["Serrano", "Ana", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "2101.02570", "submitter": "Anis Ur Rahman", "authors": "Sadia Tariq, Anis Ur Rahman, Tahir Azim, Rehman Gull Khan", "title": "Instanced model simplification using combined geometric and\n  appearance-related metric", "comments": "9 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Evolution of 3D graphics and graphical worlds has brought issues like content\noptimization, real-time processing, rendering, and shared storage limitation\nunder consideration. Generally, different simplification approaches are used to\nmake 3D meshes viable for rendering. However, many of these approaches ignore\nvertex attributes for instanced 3D meshes. In this paper, we implement and\nevaluate a simple and improved version to simplify instanced 3D textured\nmodels. The approach uses different vertex attributes in addition to geometry\nto simplify mesh instances. The resulting simplified models demonstrate\nefficient time-space requirements and better visual quality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:48:42 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Tariq", "Sadia", ""], ["Rahman", "Anis Ur", ""], ["Azim", "Tahir", ""], ["Khan", "Rehman Gull", ""]]}, {"id": "2101.02798", "submitter": "Serguei Kalentchouk", "authors": "Serguei Kalentchouk, Michael Hutchinson, Deepak Tolani", "title": "Enhanced Direct Delta Mush", "comments": null, "journal-ref": "SA '20 Posters: SIGGRAPH Asia 2020 Posters, December 2020, Article\n  No. 29", "doi": "10.1145/3415264.3425464", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct Delta Mush is a novel skinning deformation technique introduced by Le\nand Lewis (2019). It generalizes the iterative Delta Mush algorithm of\nMancewicz et al (2014), providing a direct solution with improved efficiency\nand control. Compared to Linear Blend Skinning, Direct Delta Mush offers better\nquality of deformations and ease of authoring at comparable performance.\nHowever, Direct Delta Mush does not handle non-rigid joint transformations\ncorrectly which limits its application for most production environments. This\npaper presents an extension to Direct Delta Mush that integrates the non-rigid\npart of joint transformations into the algorithm. In addition, the paper also\ndescribes practical considerations for computing the orthogonal component of\nthe transformation and stability issues observed during the implementation and\ntesting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 23:40:03 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Kalentchouk", "Serguei", ""], ["Hutchinson", "Michael", ""], ["Tolani", "Deepak", ""]]}, {"id": "2101.02847", "submitter": "Yunjin Zhang", "authors": "Yunjin Zhang, Rui Wang, Yifan (Evan) Peng, Wei Hua, Hujun Bao", "title": "Color Contrast Enhanced Rendering for Optical See-through Head-mounted\n  Displays", "comments": "13 pages, 22 figures, submitted to TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most commercially available optical see-through head-mounted displays\n(OST-HMDs) utilize optical combiners to simultaneously visualize the physical\nbackground and virtual objects. The displayed images perceived by users are a\nblend of rendered pixels and background colors. Enabling high fidelity color\nperception in mixed reality (MR) scenarios using OST-HMDs is an important but\nchallenging task. We propose a real-time rendering scheme to enhance the color\ncontrast between virtual objects and the surrounding background for OST-HMDs.\nInspired by the discovery of color perception in psychophysics, we first\nformulate the color contrast enhancement as a constrained optimization problem.\nWe then design an end-to-end algorithm to search the optimal complementary\nshift in both chromaticity and luminance of the displayed color. This aims at\nenhancing the contrast between virtual objects and the real background as well\nas keeping the consistency with the original color. We assess the performance\nof our approach using a simulated OST-HMD environment and an off-the-shelf\nOST-HMD. Experimental results from objective evaluations and subjective user\nstudies demonstrate that the proposed approach makes rendered virtual objects\nmore distinguishable from the surrounding background, thereby bringing a better\nvisual experience.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 04:42:39 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Yunjin", "", "Evan"], ["Wang", "Rui", "", "Evan"], ["Yifan", "", "", "Evan"], ["Peng", "", ""], ["Hua", "Wei", ""], ["Bao", "Hujun", ""]]}, {"id": "2101.02903", "submitter": "Shao-Kui Zhang", "authors": "Shao-Kui Zhang, Wei-Yu Xie, Song-Hai Zhang", "title": "Geometry-Based Layout Generation with Hyper-Relations AMONG Objects", "comments": "Accepted in the proceedings of Computational Visual Media Conference\n  2021. 10 Pages. 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies show increasing demands and interests in automatically\ngenerating layouts, while there is still much room for improving the\nplausibility and robustness. In this paper, we present a data-driven layout\nframework without model formulation and loss term optimization. We achieve and\norganize priors directly based on samples from datasets instead of sampling\nprobabilistic models. Therefore, our method enables expressing and generating\nmathematically inexpressible relations among three or more objects.\nSubsequently, a non-learning geometric algorithm attempts arranging objects\nplausibly considering constraints such as walls, windows, etc. Experiments\nwould show our generated layouts outperform the state-of-art and our framework\nis competitive to human designers.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 08:24:08 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Zhang", "Shao-Kui", ""], ["Xie", "Wei-Yu", ""], ["Zhang", "Song-Hai", ""]]}, {"id": "2101.03088", "submitter": "Dinesh Manocha", "authors": "Nannan Wu, Qianwen Chao, Yanzhen Chen, Weiwei Xu, Chen Liu, Dinesh\n  Manocha, Wenxin Sun, Yi Han, Xinran Yao, Xiaogang Jin", "title": "Example-based Real-time Clothing Synthesis for Virtual Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time cloth animation method for dressing virtual humans of\nvarious shapes and poses. Our approach formulates the clothing deformation as a\nhigh-dimensional function of body shape parameters and pose parameters. In\norder to accelerate the computation, our formulation factorizes the clothing\ndeformation into two independent components: the deformation introduced by body\npose variation (Clothing Pose Model) and the deformation from body shape\nvariation (Clothing Shape Model). Furthermore, we sample and cluster the poses\nspanning the entire pose space and use those clusters to efficiently calculate\nthe anchoring points. We also introduce a sensitivity-based distance\nmeasurement to both find nearby anchoring points and evaluate their\ncontributions to the final animation. Given a query shape and pose of the\nvirtual agent, we synthesize the resulting clothing deformation by blending the\nTaylor expansion results of nearby anchoring points. Compared to previous\nmethods, our approach is general and able to add the shape dimension to any\nclothing pose model. %and therefore it is more general. Furthermore, we can\nanimate clothing represented with tens of thousands of vertices at 50+ FPS on a\nCPU. Moreover, our example database is more representative and can be generated\nin parallel, and thereby saves the training time. We also conduct a user\nevaluation and show that our method can improve a user's perception of dressed\nvirtual agents in an immersive virtual environment compared to a conventional\nlinear blend skinning method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 16:32:50 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Wu", "Nannan", ""], ["Chao", "Qianwen", ""], ["Chen", "Yanzhen", ""], ["Xu", "Weiwei", ""], ["Liu", "Chen", ""], ["Manocha", "Dinesh", ""], ["Sun", "Wenxin", ""], ["Han", "Yi", ""], ["Yao", "Xinran", ""], ["Jin", "Xiaogang", ""]]}, {"id": "2101.03680", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Liwenhan Xie, Bongshin Lee, Yun Wang, Weiwei Cui, Huamin Qu", "title": "Learning to Automate Chart Layout Configurations Using Crowdsourced\n  Paired Comparison", "comments": "Accepted at ACM CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We contribute a method to automate parameter configurations for chart layouts\nby learning from human preferences. Existing charting tools usually determine\nthe layout parameters using predefined heuristics, producing sub-optimal\nlayouts. People can repeatedly adjust multiple parameters (e.g., chart size,\ngap) to achieve visually appealing layouts. However, this trial-and-error\nprocess is unsystematic and time-consuming, without a guarantee of improvement.\nTo address this issue, we develop Layout Quality Quantifier (LQ2), a machine\nlearning model that learns to score chart layouts from pairwise crowdsourcing\ndata. Combined with optimization techniques, LQ2 recommends layout parameters\nthat improve the charts' layout quality. We apply LQ2 on bar charts and conduct\nuser studies to evaluate its effectiveness by examining the quality of layouts\nit produces. Results show that LQ2 can generate more visually appealing layouts\nthan both laypeople and baselines. This work demonstrates the feasibility and\nusages of quantifying human preferences and aesthetics for chart layouts.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:49:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wu", "Aoyu", ""], ["Xie", "Liwenhan", ""], ["Lee", "Bongshin", ""], ["Wang", "Yun", ""], ["Cui", "Weiwei", ""], ["Qu", "Huamin", ""]]}, {"id": "2101.04560", "submitter": "David Breen", "authors": "Levi Kapllani, Chelsea Amanatides, Genevieve Dion, Vadim Shapiro,\n  David E. Breen", "title": "TopoKnit : A Process-Oriented Representation for Modeling the Topology\n  of Yarns in Weft-Knitted Textiles", "comments": "22 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine knitted textiles are complex multi-scale material structures\nincreasingly important in many industries, including consumer products,\narchitecture, composites, medical, and military. Computational modeling,\nsimulation, and design of industrial fabrics require efficient representations\nof the spatial, material, and physical properties of such structures. We\npropose a process-oriented representation, TopoKnit, that defines a\nfoundational data structure for representing the topology of weft-knitted\ntextiles at the yarn scale. Process space serves as an intermediary between the\nmachine and fabric spaces, and supports a concise, computationally efficient\nevaluation approach based on on-demand, near constant-time queries. In this\npaper, we define the properties of the process space, and design a data\nstructure to represent it and algorithms to evaluate it. We demonstrate the\neffectiveness of the representation scheme by providing results of evaluations\nof the data structure in support of common topological operations in the fabric\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 15:54:03 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Kapllani", "Levi", ""], ["Amanatides", "Chelsea", ""], ["Dion", "Genevieve", ""], ["Shapiro", "Vadim", ""], ["Breen", "David E.", ""]]}, {"id": "2101.04912", "submitter": "Niall Williams", "authors": "Niall L. Williams, Aniket Bera, Dinesh Manocha", "title": "ARC: Alignment-based Redirection Controller for Redirected Walking in\n  Complex Environments", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2021", "doi": "10.1109/TVCG.2021.3067781", "report-no": null, "categories": "cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel redirected walking controller based on alignment that\nallows the user to explore large and complex virtual environments, while\nminimizing the number of collisions with obstacles in the physical environment.\nOur alignment-based redirection controller, ARC, steers the user such that\ntheir proximity to obstacles in the physical environment matches the proximity\nto obstacles in the virtual environment as closely as possible. To quantify a\ncontroller's performance in complex environments, we introduce a new metric,\nComplexity Ratio (CR), to measure the relative environment complexity and\ncharacterize the difference in navigational complexity between the physical and\nvirtual environments. Through extensive simulation-based experiments, we show\nthat ARC significantly outperforms current state-of-the-art controllers in its\nability to steer the user on a collision-free path. We also show through\nquantitative and qualitative measures of performance that our controller is\nrobust in complex environments with many obstacles. Our method is applicable to\narbitrary environments and operates without any user input or parameter\ntweaking, aside from the layout of the environments. We have implemented our\nalgorithm on the Oculus Quest head-mounted display and evaluated its\nperformance in environments with varying complexity. Our project website is\navailable at https://gamma.umd.edu/arc/.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:19:42 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:11:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Williams", "Niall L.", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2101.05204", "submitter": "Yen-Chen Lin", "authors": "Frank Dellaert, Lin Yen-Chen", "title": "Neural Volume Rendering: NeRF And Beyond", "comments": "Blog: https://dellaert.github.io/NeRF/ Bibtex:\n  https://github.com/yenchenlin/awesome-NeRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the COVID-19 pandemic and political upheaval in the US, 2020 was also\nthe year in which neural volume rendering exploded onto the scene, triggered by\nthe impressive NeRF paper by Mildenhall et al. (2020). Both of us have tried to\ncapture this excitement, Frank on a blog post (Dellaert, 2020) and Yen-Chen in\na Github collection (Yen-Chen, 2020). This note is an annotated bibliography of\nthe relevant papers, and we posted the associated bibtex file on the\nrepository.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:37:25 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 21:29:40 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Dellaert", "Frank", ""], ["Yen-Chen", "Lin", ""]]}, {"id": "2101.05356", "submitter": "Gaurav Bharaj", "authors": "Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, C\\'edric Th\\'ebault,\n  Philippe-Henri Gosselin, Marco Romeo, Louis Chevallier", "title": "Practical Face Reconstruction via Differentiable Ray Tracing", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable ray-tracing based novel face reconstruction\napproach where scene attributes - 3D geometry, reflectance (diffuse, specular\nand roughness), pose, camera parameters, and scene illumination - are estimated\nfrom unconstrained monocular images. The proposed method models scene\nillumination via a novel, parameterized virtual light stage, which\nin-conjunction with differentiable ray-tracing, introduces a coarse-to-fine\noptimization formulation for face reconstruction. Our method can not only\nhandle unconstrained illumination and self-shadows conditions, but also\nestimates diffuse and specular albedos. To estimate the face attributes\nconsistently and with practical semantics, a two-stage optimization strategy\nsystematically uses a subset of parametric attributes, where subsequent\nattribute estimations factor those previously estimated. For example,\nself-shadows estimated during the first stage, later prevent its baking into\nthe personalized diffuse and specular albedos in the second stage. We show the\nefficacy of our approach in several real-world scenarios, where face attributes\ncan be estimated even under extreme illumination conditions. Ablation studies,\nanalyses and comparisons against several recent state-of-the-art methods show\nimproved accuracy and versatility of our approach. With consistent face\nattributes reconstruction, our method leads to several style -- illumination,\nalbedo, self-shadow -- edit and transfer applications, as discussed in the\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 21:36:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Dib", "Abdallah", ""], ["Bharaj", "Gaurav", ""], ["Ahn", "Junghyun", ""], ["Th\u00e9bault", "C\u00e9dric", ""], ["Gosselin", "Philippe-Henri", ""], ["Romeo", "Marco", ""], ["Chevallier", "Louis", ""]]}, {"id": "2101.05684", "submitter": "Gustav Eje Henter", "authors": "Simon Alexanderson, \\'Eva Sz\\'ekely, Gustav Eje Henter, Taras\n  Kucherenko, Jonas Beskow", "title": "Generating coherent spontaneous speech and gesture from text", "comments": "3 pages, 2 figures, published at the ACM International Conference on\n  Intelligent Virtual Agents (IVA) 2020", "journal-ref": "Proceedings of the 20th ACM International Conference on\n  Intelligent Virtual Agents (IVA '20), 2020, 3 pages", "doi": "10.1145/3383652.3423874", "report-no": null, "categories": "cs.LG cs.GR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied human communication encompasses both verbal (speech) and non-verbal\ninformation (e.g., gesture and head movements). Recent advances in machine\nlearning have substantially improved the technologies for generating synthetic\nversions of both of these types of data: On the speech side, text-to-speech\nsystems are now able to generate highly convincing, spontaneous-sounding speech\nusing unscripted speech audio as the source material. On the motion side,\nprobabilistic motion-generation methods can now synthesise vivid and lifelike\nspeech-driven 3D gesticulation. In this paper, we put these two\nstate-of-the-art technologies together in a coherent fashion for the first\ntime. Concretely, we demonstrate a proof-of-concept system trained on a\nsingle-speaker audio and motion-capture dataset, that is able to generate both\nspeech and full-body gestures together from text input. In contrast to previous\napproaches for joint speech-and-gesture generation, we generate full-body\ngestures from speech synthesis trained on recordings of spontaneous speech from\nthe same person as the motion-capture data. We illustrate our results by\nvisualising gesture spaces and text-speech-gesture alignments, and through a\ndemonstration video at https://simonalexanderson.github.io/IVA2020 .\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:02:21 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Alexanderson", "Simon", ""], ["Sz\u00e9kely", "\u00c9va", ""], ["Henter", "Gustav Eje", ""], ["Kucherenko", "Taras", ""], ["Beskow", "Jonas", ""]]}, {"id": "2101.05917", "submitter": "Pingchuan Ma", "authors": "Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, Andrew Spielberg, Daniela\n  Rus, Wojciech Matusik", "title": "DiffPD: Differentiable Projective Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, fast differentiable simulator for soft-body learning and\ncontrol applications. Existing differentiable soft-body simulators can be\nclassified into two categories based on their time integration methods:\nSimulators using explicit time-stepping scheme require tiny time steps to avoid\nnumerical instabilities in gradient computation, and simulators using implicit\ntime integration typically compute gradients by employing the adjoint method\nand solving the expensive linearized dynamics. Inspired by Projective Dynamics\n(PD), we present Differentiable Projective Dynamics (DiffPD), an efficient\ndifferentiable soft-body simulator based on PD with implicit time integration.\nThe key idea in DiffPD is to speed up backpropagation by exploiting the\nprefactorized Cholesky decomposition in forward PD simulation. In terms of\ncontact handling, DiffPD supports two types of contacts: a penalty-based model\ndescribing contact and friction forces and a complementarity-based model\nenforcing non-penetration conditions and static friction. We evaluate the\nperformance of DiffPD and observe it is 4-19 times faster compared to the\nstandard Newton's method in various applications including system\nidentification, inverse design problems, trajectory optimization, and\nclosed-loop control. We also apply DiffPD in a real-to-sim example with contact\nand collisions and show its capability of reconstructing a digital twin of\nreal-world scenes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 00:13:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 16:57:33 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Du", "Tao", ""], ["Wu", "Kui", ""], ["Ma", "Pingchuan", ""], ["Wah", "Sebastien", ""], ["Spielberg", "Andrew", ""], ["Rus", "Daniela", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2101.06322", "submitter": "Sarah Sch\\\"ottler", "authors": "Sarah Sch\\\"ottler, Yalong Yang, Hanspeter Pfister, Benjamin Bach", "title": "Visualizing and Interacting with Geospatial Networks: A Survey and\n  Design Space", "comments": "To be published in the Computer Graphics Forum (CGF) journal", "journal-ref": null, "doi": "10.1111/cgf.14198", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys visualization and interaction techniques for geospatial\nnetworks from a total of 95 papers. Geospatial networks are graphs where nodes\nand links can be associated with geographic locations. Examples can include\nsocial networks, trade and migration, as well as traffic and transport\nnetworks. Visualizing geospatial networks poses numerous challenges around the\nintegration of both network and geographical information as well as additional\ninformation such as node and link attributes, time, and uncertainty. Our\noverview analyzes existing techniques along four dimensions: i) the\nrepresentation of geographical information, ii) the representation of network\ninformation, iii) the visual integration of both, and iv) the use of\ninteraction. These four dimensions allow us to discuss techniques with respect\nto the trade-offs they make between showing information across all these\ndimensions and how they solve the problem of showing as much information as\nnecessary while maintaining readability of the visualization.\nhttps://geonetworks.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:12:15 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 10:49:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sch\u00f6ttler", "Sarah", ""], ["Yang", "Yalong", ""], ["Pfister", "Hanspeter", ""], ["Bach", "Benjamin", ""]]}, {"id": "2101.06543", "submitter": "Yun Chen", "authors": "Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan,\n  Sivabalan Manivasagam, Shangjie Xue, Ersin Yumer, Raquel Urtasun", "title": "GeoSim: Realistic Video Simulation via Geometry-Aware Composition for\n  Self-Driving", "comments": "Accepted by CVPR 2021 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable sensor simulation is an important yet challenging open problem for\nsafety-critical domains such as self-driving. Current works in image simulation\neither fail to be photorealistic or do not model the 3D environment and the\ndynamic objects within, losing high-level control and physical realism. In this\npaper, we present GeoSim, a geometry-aware image composition process which\nsynthesizes novel urban driving scenarios by augmenting existing images with\ndynamic objects extracted from other scenes and rendered at novel poses.\nTowards this goal, we first build a diverse bank of 3D objects with both\nrealistic geometry and appearance from sensor data. During simulation, we\nperform a novel geometry-aware simulation-by-composition procedure which 1)\nproposes plausible and realistic object placements into a given scene, 2)\nrender novel views of dynamic objects from the asset bank, and 3) composes and\nblends the rendered image segments. The resulting synthetic images are\nrealistic, traffic-aware, and geometrically consistent, allowing our approach\nto scale to complex use cases. We demonstrate two such important applications:\nlong-range realistic video simulation across multiple camera sensors, and\nsynthetic data generation for data augmentation on downstream segmentation\ntasks. Please check https://tmux.top/publication/geosim/ for high-resolution\nvideo results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:00:33 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 16:06:21 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 05:19:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chen", "Yun", ""], ["Rong", "Frieda", ""], ["Duggal", "Shivam", ""], ["Wang", "Shenlong", ""], ["Yan", "Xinchen", ""], ["Manivasagam", "Sivabalan", ""], ["Xue", "Shangjie", ""], ["Yumer", "Ersin", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2101.07579", "submitter": "Panagiotis Tigas", "authors": "Panagiotis Tigas and Tyson Hosmer", "title": "Spatial Assembly: Generative Architecture With Reinforcement Learning,\n  Self Play and Tree Search", "comments": "Workshop on Machine Learning for Creativity and Design at the 34rd\n  Conference on Neural Information Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With this work, we investigate the use of Reinforcement Learning (RL) for the\ngeneration of spatial assemblies, by combining ideas from Procedural Generation\nalgorithms (Wave Function Collapse algorithm (WFC)) and RL for Game Solving.\nWFC is a Generative Design algorithm, inspired by Constraint Solving. In WFC,\none defines a set of tiles/blocks and constraints and the algorithm generates\nan assembly that satisfies these constraints. Casting the problem of generation\nof spatial assemblies as a Markov Decision Process whose states transitions are\ndefined by WFC, we propose an algorithm that uses Reinforcement Learning and\nSelf-Play to learn a policy that generates assemblies that maximize objectives\nset by the designer. Finally, we demonstrate the use of our Spatial Assembly\nalgorithm in Architecture Design.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:57:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Tigas", "Panagiotis", ""], ["Hosmer", "Tyson", ""]]}, {"id": "2101.07906", "submitter": "Daniel Martin", "authors": "Daniel Martin, Sandra Malpica, Diego Gutierrez, Belen Masia and Ana\n  Serrano", "title": "Multimodality in VR: A survey", "comments": "35 pages (24 pages not including references), 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual reality (VR) is rapidly growing, with the potential to change the way\nwe create and consume content. In VR, users integrate multimodal sensory\ninformation they receive, to create a unified perception of the virtual world.\nIn this survey, we review the body of work addressing multimodality in VR, and\nits role and benefits in user experience, together with different applications\nthat leverage multimodality in many disciplines. These works thus encompass\nseveral fields of research, and demonstrate that multimodality plays a\nfundamental role in VR; enhancing the experience, improving overall\nperformance, and yielding unprecedented abilities in skill and knowledge\ntransfer.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:29:23 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 09:58:51 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Martin", "Daniel", ""], ["Malpica", "Sandra", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""], ["Serrano", "Ana", ""]]}, {"id": "2101.08779", "submitter": "Ruilong Li", "authors": "Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa", "title": "Learn to Dance with AIST++: Music Conditioned 3D Dance Generation", "comments": "Project page: https://google.github.io/aichoreographer/; Dataset\n  page: https://google.github.io/aistplusplus_dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a transformer-based learning framework for 3D dance\ngeneration conditioned on music. We carefully design our network architecture\nand empirically study the keys for obtaining qualitatively pleasing results.\nThe critical components include a deep cross-modal transformer, which well\nlearns the correlation between the music and dance motion; and the\nfull-attention with future-N supervision mechanism which is essential in\nproducing long-range non-freezing motion. In addition, we propose a new dataset\nof paired 3D motion and music called AIST++, which we reconstruct from the AIST\nmulti-view dance videos. This dataset contains 1.1M frames of 3D dance motion\nin 1408 sequences, covering 10 genres of dance choreographies and accompanied\nwith multi-view camera parameters. To our knowledge it is the largest dataset\nof this kind. Rich experiments on AIST++ demonstrate our method produces much\nbetter results than the state-of-the-art methods both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 18:59:22 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 05:23:59 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Li", "Ruilong", ""], ["Yang", "Shan", ""], ["Ross", "David A.", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2101.09376", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "The Role of Edges in Line Drawing Perception", "comments": "Accepted to _Perception_", "journal-ref": "Perception. 2021;50(3):266-275", "doi": "10.1177/0301006621994407", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has often been conjectured that the effectiveness of line drawings can be\nexplained by the similarity of edge images to line drawings. This paper\npresents several problems with explaining line drawing perception in terms of\nedges, and how the recently-proposed Realism Hypothesis of Hertzmann (2020)\nresolves these problems. There is nonetheless existing evidence that edges are\noften the best features for predicting where people draw lines; this paper\ndescribes how the Realism Hypothesis can explain this evidence.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:22:05 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "2101.09382", "submitter": "Krzysztof Szajowski", "authors": "Krzysztof J. Szajowski and Kinga W{\\l}odarczyk", "title": "A measure of the importance of roads based on topography and traffic\n  intensity", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GR math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of street traffic allowing assessment of the importance\nof their individual segments for the functionality of the street system is\nconsidering. Based on methods of cooperative games and the reliability theory\nthe suitable measure is constructed. The main goal is to analyze methods for\nassessing the importance (rank) of road fragments, including their functions. A\nrelevance of these elements for effective accessibility for the entire system\nwill be considered.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 23:53:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Szajowski", "Krzysztof J.", ""], ["W\u0142odarczyk", "Kinga", ""]]}, {"id": "2101.09866", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Yi Yang, Shih-En Wei, Xinshuo Weng, Yaser Sheikh, Shoou-I\n  Yu", "title": "Supervision by Registration and Triangulation for Landmark Detection", "comments": "Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2983935", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Supervision by Registration and Triangulation (SRT), an\nunsupervised approach that utilizes unlabeled multi-view video to improve the\naccuracy and precision of landmark detectors. Being able to utilize unlabeled\ndata enables our detectors to learn from massive amounts of unlabeled data\nfreely available and not be limited by the quality and quantity of manual human\nannotations. To utilize unlabeled data, there are two key observations: (1) the\ndetections of the same landmark in adjacent frames should be coherent with\nregistration, i.e., optical flow. (2) the detections of the same landmark in\nmultiple synchronized and geometrically calibrated views should correspond to a\nsingle 3D point, i.e., multi-view consistency. Registration and multi-view\nconsistency are sources of supervision that do not require manual labeling,\nthus it can be leveraged to augment existing training data during detector\ntraining. End-to-end training is made possible by differentiable registration\nand 3D triangulation modules. Experiments with 11 datasets and a newly proposed\nmetric to measure precision demonstrate accuracy and precision improvements in\nlandmark detection on both images and video. Code is available at\nhttps://github.com/D-X-Y/landmark-detection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 02:48:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yang", "Yi", ""], ["Wei", "Shih-En", ""], ["Weng", "Xinshuo", ""], ["Sheikh", "Yaser", ""], ["Yu", "Shoou-I", ""]]}, {"id": "2101.10463", "submitter": "An Zou", "authors": "An Zou, Jing Li, Christopher D. Gill, and Xuan Zhang", "title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with\n  Fine-Grain Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many emerging cyber-physical systems, such as autonomous vehicles and robots,\nrely heavily on artificial intelligence and machine learning algorithms to\nperform important system operations. Since these highly parallel applications\nare computationally intensive, they need to be accelerated by graphics\nprocessing units (GPUs) to meet stringent timing constraints. However, despite\nthe wide adoption of GPUs, efficiently scheduling multiple GPU applications\nwhile providing rigorous real-time guarantees remains a challenge. In this\npaper, we propose RTGPU, which can schedule the execution of multiple GPU\napplications in real-time to meet hard deadlines. Each GPU application can have\nmultiple CPU execution and memory copy segments, as well as GPU kernels. We\nstart with a model to explicitly account for the CPU and memory copy segments\nof these applications. We then consider the GPU architecture in the development\nof a precise timing model for the GPU kernels and leverage a technique known as\npersistent threads to implement fine-grained kernel scheduling with improved\nperformance through interleaved execution. Next, we propose a general method\nfor scheduling parallel GPU applications in real time. Finally, to schedule\nmultiple parallel GPU applications, we propose a practical real-time scheduling\nalgorithm based on federated scheduling and grid search (for GPU kernel\nsegments) with uniprocessor fixed priority scheduling (for multiple CPU and\nmemory copy segments). Our approach provides superior schedulability compared\nwith previous work, and gives real-time guarantees to meet hard deadlines for\nmultiple GPU applications according to comprehensive validation and evaluation\non a real NVIDIA GTX1080Ti GPU system.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 22:34:06 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 02:22:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Zou", "An", ""], ["Li", "Jing", ""], ["Gill", "Christopher D.", ""], ["Zhang", "Xuan", ""]]}, {"id": "2101.10503", "submitter": "Mathew Schwartz", "authors": "Mathew Schwartz", "title": "Human Centric Accessibility Graph For Environment Analysis", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": "10.1016/j.autcon.2021.103557", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding design decisions in relation to the future occupants of a\nbuilding is a crucial part of good design. However, limitations in tools and\nexpertise hinder meaningful human-centric decisions during the design process.\nIn this paper, a novel Spatial Human Accessibility graph for Planning and\nEnvironment Analysis (SHAPE) is introduced that brings together the technical\nchallenges of discrete representations of digital models, with human-based\nmetrics for evaluating the environment. SHAPE: does not need labeled geometry\nas input, works with multi-level buildings, captures surface variations (e.g.,\nslopes in a terrain), and can be used with existing graph theory (e.g.,\ngravity, centrality) techniques. SHAPE uses ray-casting to perform a search,\ngenerating a dense graph of all accessible locations within the environment and\nstoring the type of travel required in a graph (e.g., up a slope, down a step).\nThe ability to simultaneously evaluate and plan paths from multiple human\nfactors is shown to work on digital models across room, building, and\ntopography scales. The results enable designers and planners to evaluate\noptions of the built environment in new ways, and at higher fidelity, that will\nlead to more human-friendly and accessible environments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:59:47 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Schwartz", "Mathew", ""]]}, {"id": "2101.10994", "submitter": "Towaki Takikawa", "authors": "Towaki Takikawa, Joey Litalien, Kangxue Yin, Karsten Kreis, Charles\n  Loop, Derek Nowrouzezahrai, Alec Jacobson, Morgan McGuire, Sanja Fidler", "title": "Neural Geometric Level of Detail: Real-time Rendering with Implicit 3D\n  Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural signed distance functions (SDFs) are emerging as an effective\nrepresentation for 3D shapes. State-of-the-art methods typically encode the SDF\nwith a large, fixed-size neural network to approximate complex shapes with\nimplicit surfaces. Rendering with these large networks is, however,\ncomputationally expensive since it requires many forward passes through the\nnetwork for every pixel, making these representations impractical for real-time\ngraphics. We introduce an efficient neural representation that, for the first\ntime, enables real-time rendering of high-fidelity neural SDFs, while achieving\nstate-of-the-art geometry reconstruction quality. We represent implicit\nsurfaces using an octree-based feature volume which adaptively fits shapes with\nmultiple discrete levels of detail (LODs), and enables continuous LOD with SDF\ninterpolation. We further develop an efficient algorithm to directly render our\nnovel neural SDF representation in real-time by querying only the necessary\nLODs with sparse octree traversal. We show that our representation is 2-3\norders of magnitude more efficient in terms of rendering speed compared to\nprevious works. Furthermore, it produces state-of-the-art reconstruction\nquality for complex shapes under both 3D geometric and 2D image-space metrics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 18:50:22 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Takikawa", "Towaki", ""], ["Litalien", "Joey", ""], ["Yin", "Kangxue", ""], ["Kreis", "Karsten", ""], ["Loop", "Charles", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""], ["McGuire", "Morgan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2101.11101", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya and Nicholas Rewkowski and Abhishek Banerjee and\n  Pooja Guhan and Aniket Bera and Dinesh Manocha", "title": "Text2Gestures: A Transformer-Based Network for Generating Emotive Body\n  Gestures for Virtual Agents", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Text2Gestures, a transformer-based learning method to\ninteractively generate emotive full-body gestures for virtual agents aligned\nwith natural language text inputs. Our method generates emotionally expressive\ngestures by utilizing the relevant biomechanical features for body expressions,\nalso known as affective features. We also consider the intended task\ncorresponding to the text and the target virtual agents' intended gender and\nhandedness in our generation pipeline. We train and evaluate our network on the\nMPI Emotional Body Expressions Database and observe that our network produces\nstate-of-the-art performance in generating gestures for virtual agents aligned\nwith the text for narration or conversation. Our network can generate these\ngestures at interactive rates on a commodity GPU. We conduct a web-based user\nstudy and observe that around 91% of participants indicated our generated\ngestures to be at least plausible on a five-point Likert Scale. The emotions\nperceived by the participants from the gestures are also strongly positively\ncorrelated with the corresponding intended emotions, with a minimum Pearson\ncoefficient of 0.77 in the valence dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:07:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Rewkowski", "Nicholas", ""], ["Banerjee", "Abhishek", ""], ["Guhan", "Pooja", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2101.11529", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anirudh Thatipelli, Neel Trivedi, Ravi Kiran Sarvadevabhatla", "title": "NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions", "comments": "Code repository at https://github.com/skelemoa/ntu-x", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of fine-grained joints such as hand fingers is a fundamental\nperformance bottleneck for state of the art skeleton action recognition models\ntrained on the largest action recognition dataset, NTU-RGBD. To address this\nbottleneck, we introduce a new skeleton based human action dataset - NTU60-X.\nIn addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X\ndataset includes finger and facial joints, enabling a richer skeleton\nrepresentation. We appropriately modify the state of the art approaches to\nenable training using the introduced dataset. Our results demonstrate the\neffectiveness of NTU60-X in overcoming the aforementioned bottleneck and\nimprove state of the art performance, overall and on hitherto worst performing\naction categories.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:33:51 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:19:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Thatipelli", "Anirudh", ""], ["Trivedi", "Neel", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11530", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Divyanshu Sharma, Ravi Kiran Sarvadevabhatla", "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action\n  Recognition", "comments": "Accepted at ICIP-2021. Code and pretrained models available at\n  https://github.com/skelemoa/synse-zsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:34:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 23:59:56 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gupta", "Pranay", ""], ["Sharma", "Divyanshu", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11569", "submitter": "Marco Tarini", "authors": "Marco Tarini", "title": "Closed-form Quadrangulation of N-Sided Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We analyze the problem of quadrangulating a $n$-sided patch, each side at its\nboundary subdivided into a given number of edges, using a single irregular\nvertex (or none, when $n = 4$) that breaks the otherwise fully regular lattice.\nWe derive, in an analytical closed-form, (1) the necessary and sufficient\nconditions that a patch must meet to admit this quadrangulation, and (2) a full\ndescription of the resulting tessellation(s).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:54:11 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 13:42:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Tarini", "Marco", ""]]}, {"id": "2101.11856", "submitter": "Xiaopei Liu", "authors": "Yixin Chen, Wei Li, Rui Fan and Xiaopei Liu", "title": "GPU Optimization for High-Quality Kinetic Fluid Simulation", "comments": "16 pages, 25 figures, accepted by IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluid simulations are often performed using the incompressible Navier-Stokes\nequations (INSE), leading to sparse linear systems which are difficult to solve\nefficiently in parallel. Recently, kinetic methods based on the\nadaptive-central-moment multiple-relaxation-time (ACM-MRT) model have\ndemonstrated impressive capabilities to simulate both laminar and turbulent\nflows, with quality matching or surpassing that of state-of-the-art INSE\nsolvers. Furthermore, due to its local formulation, this method presents the\nopportunity for highly scalable implementations on parallel systems such as\nGPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a\nnumber of computational challenges, especially when dealing with complex solids\ninside the fluid domain. In this paper, we present multiple novel GPU\noptimization techniques to efficiently implement high-quality ACM-MRT-based\nkinetic fluid simulations in domains containing complex solids. Our techniques\ninclude a new communication-efficient data layout, a load-balanced\nimmersed-boundary method, a multi-kernel launch method using a simplified\nformulation of ACM-MRT calculations to enable greater parallelism, and the\nintegration of these techniques into a parametric cost model to enable\nautomated parameter search to achieve optimal execution performance. We also\nextended our method to multi-GPU systems to enable large-scale simulations. To\ndemonstrate the state-of-the-art performance and high visual quality of our\nsolver, we present extensive experimental results and comparisons to other\nsolvers.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:02:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Chen", "Yixin", ""], ["Li", "Wei", ""], ["Fan", "Rui", ""], ["Liu", "Xiaopei", ""]]}, {"id": "2101.12276", "submitter": "Shuaiying Hou", "authors": "Shuaiying Hou, Weiwei Xu, Jinxiang Chai, Congyi Wang, Wenlin Zhuang,\n  Yu Chen, Hujun Bao, Yangang Wang", "title": "A Causal Convolutional Neural Network for Motion Modeling and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel deep generative model based on causal convolutions for\nmulti-subject motion modeling and synthesis, which is inspired by the success\nof WaveNet in multi-subject speech synthesis. However, it is nontrivial to\nadapt WaveNet to handle high-dimensional and physically constrained motion\ndata. To this end, we add an encoder and a decoder to the WaveNet to translate\nthe motion data into features and back to the predicted motions. We also add 1D\nconvolution layers to take skeleton configuration as an input to model skeleton\nvariations across different subjects. As a result, our network can scale up\nwell to large-scale motion data sets across multiple subjects and support\nvarious applications, such as random and controllable motion synthesis, motion\ndenoising, and motion completion, in a unified way. Complex motions, such as\npunching, kicking and, kicking while punching, are also well handled. Moreover,\nour network can synthesize motions for novel skeletons not in the training\ndataset. After fine-tuning the network with a few motion data of the novel\nskeleton, it is able to capture the personalized style implied in the motion\nand generate high-quality motions for the skeleton. Thus, it has the potential\nto be used as a pre-trained network in few-shot learning for motion modeling\nand synthesis. Experimental results show that our model can effectively handle\nthe variation of skeleton configurations, and it runs fast to synthesize\ndifferent types of motions on-line. We also perform user studies to verify that\nthe quality of motions generated by our network is superior to the motions of\nstate-of-the-art human motion synthesis methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:01:43 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Hou", "Shuaiying", ""], ["Xu", "Weiwei", ""], ["Chai", "Jinxiang", ""], ["Wang", "Congyi", ""], ["Zhuang", "Wenlin", ""], ["Chen", "Yu", ""], ["Bao", "Hujun", ""], ["Wang", "Yangang", ""]]}, {"id": "2101.12408", "submitter": "Jingyu Chen", "authors": "Jingyu Chen, Victoria Kala, Alan Marquez-Razon, Elias Gueidon, David\n  A. B. Hyde, Joseph Teran", "title": "A Momentum-Conserving Implicit Material Point Method for Surface\n  Energies with Spatial Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Material Point Method (MPM) discretization of surface\ntension forces that arise from spatially varying surface energies. These\nvariations typically arise from surface energy dependence on temperature and/or\nconcentration. Furthermore, since the surface energy is an interfacial property\ndepending on the types of materials on either side of an interface, spatial\nvariation is required for modeling the contact angle at the triple junction\nbetween a liquid, solid and surrounding air. Our discretization is based on the\nsurface energy itself, rather than on the associated traction condition most\ncommonly used for discretization with particle methods. Our energy based\napproach automatically captures surface gradients without the explicit need to\nresolve them as in traction condition based approaches. We include an implicit\ndiscretization of thermomechanical material coupling with a novel\nparticle-based enforcement of Robin boundary conditions associated with\nconvective heating. Lastly, we design a particle resampling approach needed to\nachieve perfect conservation of linear and angular momentum with\nAffineParticle-In-Cell (APIC) [Jiang et al. 2015]. We show that our approach\nenables implicit time stepping for complex behaviors like the Marangoni effect\nand hydrophobicity/hydrophilicity. We demonstrate the robustness and utility of\nour method by simulating materials that exhibit highly diverse degrees of\nsurface tension and thermomechanical effects, such as water, wine and wax.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 05:18:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Chen", "Jingyu", ""], ["Kala", "Victoria", ""], ["Marquez-Razon", "Alan", ""], ["Gueidon", "Elias", ""], ["Hyde", "David A. B.", ""], ["Teran", "Joseph", ""]]}]