[{"id": "1512.03012", "submitter": "Manolis Savva", "authors": "Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan,\n  Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su,\n  Jianxiong Xiao, Li Yi, and Fisher Yu", "title": "ShapeNet: An Information-Rich 3D Model Repository", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2015 19:42:48 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""], ["Guibas", "Leonidas", ""], ["Hanrahan", "Pat", ""], ["Huang", "Qixing", ""], ["Li", "Zimo", ""], ["Savarese", "Silvio", ""], ["Savva", "Manolis", ""], ["Song", "Shuran", ""], ["Su", "Hao", ""], ["Xiao", "Jianxiong", ""], ["Yi", "Li", ""], ["Yu", "Fisher", ""]]}, {"id": "1512.04582", "submitter": "Jan Egger", "authors": "Jan Egger, Harald Busse, Philipp Brandmaier, Daniel Seider, Matthias\n  Gawlitza, Steffen Strocka, Philip Voglreiter, Mark Dokter, Michael Hofmann,\n  Bernhard Kainz, Alexander Hann, Xiaojun Chen, Tuomas Alhonnoro, Mika Pollari,\n  Dieter Schmalstieg, Michael Moche", "title": "Interactive Volumetry Of Liver Ablation Zones", "comments": "18 pages, 15 figures, 8 tables, 57 references", "journal-ref": "Sci. Rep. 5, 15373; doi: 10.1038/srep15373 (2015)", "doi": "10.1038/srep15373", "report-no": null, "categories": "cs.CV cs.GR cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique\nthat destroys cancer cells by heat. The heat results from focusing energy in\nthe radiofrequency spectrum through a needle. Amongst others, this can enable\nthe treatment of patients who are not eligible for an open surgery. However,\nthe possibility of recurrent liver cancer due to incomplete ablation of the\ntumor makes post-interventional monitoring via regular follow-up scans\nmandatory. These scans have to be carefully inspected for any conspicuousness.\nWithin this study, the RF ablation zones from twelve post-interventional CT\nacquisitions have been segmented semi-automatically to support the visual\ninspection. An interactive, graph-based contouring approach, which prefers\nspherically shaped regions, has been applied. For the quantitative and\nqualitative analysis of the algorithm's results, manual slice-by-slice\nsegmentations produced by clinical experts have been used as the gold standard\n(which have also been compared among each other). As evaluation metric for the\nstatistical validation, the Dice Similarity Coefficient (DSC) has been\ncalculated. The results show that the proposed tool provides lesion\nsegmentation with sufficient accuracy much faster than manual segmentation. The\nvisual feedback and interactivity make the proposed tool well suitable for the\nclinical workflow.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2015 08:14:32 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Egger", "Jan", ""], ["Busse", "Harald", ""], ["Brandmaier", "Philipp", ""], ["Seider", "Daniel", ""], ["Gawlitza", "Matthias", ""], ["Strocka", "Steffen", ""], ["Voglreiter", "Philip", ""], ["Dokter", "Mark", ""], ["Hofmann", "Michael", ""], ["Kainz", "Bernhard", ""], ["Hann", "Alexander", ""], ["Chen", "Xiaojun", ""], ["Alhonnoro", "Tuomas", ""], ["Pollari", "Mika", ""], ["Schmalstieg", "Dieter", ""], ["Moche", "Michael", ""]]}, {"id": "1512.08826", "submitter": "Manfred Lau", "authors": "Kapil Dev, Manfred Lau", "title": "Improving Style Similarity Metrics of 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of style similarity metrics has been recently developed for various\nmedia types such as 2D clip art and 3D shapes. We explore this style metric\nproblem and improve existing style similarity metrics of 3D shapes in four\nnovel ways. First, we consider the color and texture of 3D shapes which are\nimportant properties that have not been previously considered. Second, we\nexplore the effect of clustering a dataset of 3D models by comparing between\nstyle metrics for a single object type and style metrics that combine clusters\nof object types. Third, we explore the idea of user-guided learning for this\nproblem. Fourth, we introduce an iterative approach that can learn a metric\nfrom a general set of 3D models. We demonstrate these contributions with\nvarious classes of 3D shapes and with applications such as style-based\nsimilarity search and scene composition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2015 02:26:46 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Dev", "Kapil", ""], ["Lau", "Manfred", ""]]}]