[{"id": "1902.00650", "submitter": "Maodong Pan", "authors": "Maodong Pan, Falai Chen, Weihua Tong", "title": "Volumetric Spline Parameterization for Isogeometric Analysis", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2019.112769", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the spline representation of the boundary of a three dimensional\ndomain, constructing a volumetric spline parameterization of the domain (i.e.,\na map from a unit cube to the domain) with the given boundary is a fundamental\nproblem in isogeometric analysis. A good domain parameterization should satisfy\nthe following criteria: (1) the parameterization is a bijective map; and (2)\nthe map has lowest possible distortion. However, none of the state-of-the-art\nvolumetric parameterization methods has fully addressed the above issues. In\nthis paper, we propose a three-stage approach for constructing volumetric\nparameterization satisfying the above criteria. Firstly, a harmonic map is\ncomputed between a unit cube and the computational domain. Then a bijective map\nmodeled by a max-min optimization problem is computed in a coarse-to-fine way,\nand an algorithm based on divide and conquer strategy is proposed to solve the\noptimization problem efficiently. Finally, to ensure high quality of the\nparameterization, the MIPS (Most Isometric Parameterizations) method is adopted\nto reduce the conformal distortion of the bijective map. We provide several\nexamples to demonstrate the feasibility of our approach and to compare our\napproach with some state-of-the-art methods. The results show that our\nalgorithm produces bijective parameterization with high quality even for\ncomplex domains.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 06:26:22 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Pan", "Maodong", ""], ["Chen", "Falai", ""], ["Tong", "Weihua", ""]]}, {"id": "1902.00801", "submitter": "Minjae Lee Ph.D.", "authors": "Minjae Lee, David Hyde, Kevin Li, Ronald Fedkiw", "title": "A Robust Volume Conserving Method for Character-Water Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel volume conserving framework for character-water\ninteraction, using a novel volume-of-fluid solver on a skinned tetrahedral\nmesh, enabling the high degree of the spatial adaptivity in order to capture\nthin films and hair-water interactions. For efficiency, the bulk of the fluid\nvolume is simulated with a standard Eulerian solver which is two way coupled to\nour skinned arbitrary Lagrangian-Eulerian mesh using a fast, robust, and\nstraightforward to implement partitioned approach. This allows for a\nspecialized and efficient treatment of the volume-of-fluid solver, since it is\nonly required in a subset of the domain. The combination of conservation of\nfluid volume and a kinematically deforming skinned mesh allows us to robustly\nimplement interesting effects such as adhesion, and anisotropic porosity. We\nillustrate the efficacy of our method by simulating various water effects with\nsolid objects and animated characters.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 21:52:51 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Lee", "Minjae", ""], ["Hyde", "David", ""], ["Li", "Kevin", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1902.01192", "submitter": "Benjamin Marussig", "authors": "Benjamin Marussig", "title": "Advances in the Treatment of Trimmed CAD Models due to Isogeometric\n  Analysis", "comments": null, "journal-ref": "13th World Congress on Computational Mechanics (WCCM XIII) and 2nd\n  Pan American Congress on Computational Mechanics (PANACM II), July 22-27,\n  2018, New York City, NY, USA", "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trimming is a core technique in geometric modeling. Unfortunately, the\nresulting objects do not take the requirements of numerical simulations into\naccount and yield various problems. This paper outlines principal issues of\ntrimmed models and highlights different analysis-suitable strategies to address\nthem. It is discussed that these concepts not only provide important\ncomputational tools for isogeometric analysis, but can also improve the\ntreatment of trimmed models in a design context.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 12:55:30 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Marussig", "Benjamin", ""]]}, {"id": "1902.02371", "submitter": "Paul Yushkevich", "authors": "Paul A. Yushkevich, Ahmed Aly, Jiancong Wang, Long Xie, Robert C.\n  Gorman, Laurent Younes, Alison Pouch", "title": "Diffeomorphic Medial Modeling", "comments": "Accepted to the 26th International Conference on Information\n  Processing in Medical Imaging (IPMI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable shape modeling approaches that describe objects in terms of their\nmedial axis geometry (e.g., m-reps [Pizer et al., 2003]) yield rich geometrical\nfeatures that can be useful for analyzing the shape of sheet-like biological\nstructures, such as the myocardium. We present a novel shape analysis approach\nthat combines the benefits of medial shape modeling and diffeomorphometry. Our\nalgorithm is formulated as a problem of matching shapes using diffeomorphic\nflows under constraints that approximately preserve medial axis geometry during\ndeformation. As the result, correspondence between the medial axes of similar\nshapes is maintained. The approach is evaluated in the context of modeling the\nshape of the left ventricular wall from 3D echocardiography images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 19:29:51 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 20:02:42 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yushkevich", "Paul A.", ""], ["Aly", "Ahmed", ""], ["Wang", "Jiancong", ""], ["Xie", "Long", ""], ["Gorman", "Robert C.", ""], ["Younes", "Laurent", ""], ["Pouch", "Alison", ""]]}, {"id": "1902.02806", "submitter": "Lars Doyle", "authors": "Lars Doyle, Forest Anderson, Ehren Choy, and David Mould", "title": "Automated pebble mosaic stylization of images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital mosaics have usually used regular tiles, simulating the historical\n\"tessellated\" mosaics. In this paper, we present a method for synthesizing\npebble mosaics, a historical mosaic style in which the tiles are rounded\npebbles. We address both the tiling problem, where pebbles are distributed over\nthe image plane so as to approximate the input image content, and the problem\nof geometry, creating a smooth rounded shape for each pebble. We adapt SLIC,\nsimple linear iterative clustering, to obtain elongated tiles conforming to\nimage content, and smooth the resulting irregular shapes into shapes resembling\npebble cross-sections. Then, we create an interior and exterior contour for\neach pebble and solve a Laplace equation over the region between them to obtain\nheight-field geometry. The resulting pebble set approximates the input image\nwhile presenting full geometry that can be rendered and textured for a highly\ndetailed representation of a pebble mosaic.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 19:11:14 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Doyle", "Lars", ""], ["Anderson", "Forest", ""], ["Choy", "Ehren", ""], ["Mould", "David", ""]]}, {"id": "1902.02906", "submitter": "Felix Hamza-Lup", "authors": "Faith-Anne, L. Kocadag, Felix G. Hamza-Lup", "title": "X3D in Urban Planning - Savannah in 3D", "comments": "ACM-SE 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban planning often raises complex issues that are difficult to visualize\nand challenging to communicate. The increasing availability of 3D modeling\nstandards has provided the opportunity for many developers, engineers,\ndesigners, planners, investors, and government officials to effectively\ncollaborate to bring projects to fruition. Because of its real-time\ninteractivity and widespread web-based content players, X3D proves to be a good\nchoice for developing and visualizing 3D city content on the Web for planning\npurposes.\n  Passenger rail is a viable and cost-effective transportation solution in many\nareas, especially in view of rising energy costs. The Savannah in 3D (or S3D)\nproject is a multimedia tool for a feasibility study designed to bring\npassenger rail to Savannah; thereby opening up the historic, tourist-friendly\ncity to a wider audience. The paper outlines the development process of an\ninteractive 3D train model as it journeys from Atlanta to Savannah, Georgia -\nfocusing on user interactivity and scene immersion to supplement the city and\ntransportation planning agenda.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 01:19:54 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Faith-Anne", "", ""], ["Kocadag", "L.", ""], ["Hamza-Lup", "Felix G.", ""]]}, {"id": "1902.03438", "submitter": "Emil Saucan", "authors": "Emil Saucan", "title": "Metric Curvatures and their Applications 2: Metric Ricci Curvature and\n  Flow", "comments": "40 pages, 2 figures. Important correction made, figure added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this second part of our overview of the different metric curvatures and\ntheir various applications, we concentrate on the Ricci curvature and flow for\npolyhedral surfaces and higher dimensional manifolds, and we largely review our\nprevious studies on the subject, based upon Wald's curvature. In addition to\nour previous metric approaches to the discretization of Ricci curvature, we\nconsider yet another one, based on the Haantjes curvature, interpreted as a\ngeodesic curvature. We also try to understand the mathematical reasons behind\nthe recent proliferation of discretizations of Ricci curvature. Furthermore, we\npropose another approach to the metrization of Ricci curvature, based on\nForman's discretization, and in particular we propose on that uses our graph\nversion of Forman's Ricci curvature.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 16:14:10 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 19:35:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Saucan", "Emil", ""]]}, {"id": "1902.04285", "submitter": "Ohad Fried", "authors": "Ohad Fried, Maneesh Agrawala", "title": "Puppet Dubbing", "comments": null, "journal-ref": "Eurographics Symposium on Rendering, 2019", "doi": null, "report-no": null, "categories": "cs.GR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dubbing puppet videos to make the characters (e.g. Kermit the Frog)\nconvincingly speak a new speech track is a popular activity with many examples\nof well-known puppets speaking lines from films or singing rap songs. But\nmanually aligning puppet mouth movements to match a new speech track is tedious\nas each syllable of the speech must match a closed-open-closed segment of mouth\nmovement for the dub to be convincing. In this work, we present two methods to\nalign a new speech track with puppet video, one semi-automatic appearance-based\nand the other fully-automatic audio-based. The methods offer complementary\nadvantages and disadvantages. Our appearance-based approach directly identifies\nclosed-open-closed segments in the puppet video and is robust to low-quality\naudio as well as misalignments between the mouth movements and speech in the\noriginal performance, but requires some manual annotation. Our audio-based\napproach assumes the original performance matches a closed-open-closed mouth\nsegment to each syllable of the original speech. It is fully automatic, robust\nto visual occlusions and fast puppet movements, but does not handle\nmisalignments in the original performance. We compare the methods and show that\nboth improve the credibility of the resulting video over simple baseline\ntechniques, via quantitative evaluation and user ratings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 09:09:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fried", "Ohad", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1902.04805", "submitter": "Charles Gueunet", "authors": "Charles Gueunet (LIP6), P. Fortin (LLR), J Jomier, J Tierny", "title": "Task-based Augmented Contour Trees with Fibonacci Heaps", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, Institute\n  of Electrical and Electronics Engineers, In press", "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DM cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm for the fast, shared memory, multi-core\ncomputation of augmented contour trees on triangulations. In contrast to most\nexisting parallel algorithms our technique computes augmented trees, enabling\nthe full extent of contour tree based applications including data segmentation.\nOur approach completely revisits the traditional, sequential contour tree\nalgorithm to re-formulate all the steps of the computation as a set of\nindependent local tasks. This includes a new computation procedure based on\nFibonacci heaps for the join and split trees, two intermediate data structures\nused to compute the contour tree, whose constructions are efficiently carried\nout concurrently thanks to the dynamic scheduling of task parallelism. We also\nintroduce a new parallel algorithm for the combination of these two trees into\nthe output global contour tree. Overall, this results in superior time\nperformance in practice, both in sequential and in parallel thanks to the\nOpenMP task runtime. We report performance numbers that compare our approach to\nreference sequential and multi-threaded implementations for the computation of\naugmented merge and contour trees. These experiments demonstrate the run-time\nefficiency of our approach and its scalability on common workstations. We\ndemonstrate the utility of our approach in data segmentation applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:31:03 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Gueunet", "Charles", "", "LIP6"], ["Fortin", "P.", "", "LLR"], ["Jomier", "J", ""], ["Tierny", "J", ""]]}, {"id": "1902.04820", "submitter": "Nicolas Holliman Professor", "authors": "Nicolas S. Holliman, Manu Antony, James Charlton, Stephen Dowsland,\n  Philip James and Mark Turner", "title": "Petascale Cloud Supercomputing for Terapixel Visualization of a Digital\n  Twin", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Photo-realistic terapixel visualization is computationally\nintensive and to date there have been no such visualizations of urban digital\ntwins, the few terapixel visualizations that exist have looked towards space\nrather than earth. Objective: our aims are: creating a scalable cloud\nsupercomputer software architecture for visualization; a photo-realistic\nterapixel 3D visualization of urban IoT data supporting daily updates; a\nrigorous evaluation of cloud supercomputing for our application. Method: we\nmigrated the Blender Cycles path tracer to the public cloud within a new\nsoftware framework designed to scale to petaFLOP performance. Results: we\ndemonstrate we can compute a terapixel visualization in under one hour, the\nsystem scaling at 98% efficiency to use 1024 public cloud GPU nodes delivering\n14 petaFLOPS. The resulting terapixel image supports interactive browsing of\nthe city and its data at a wide range of sensing scales. Conclusion: The GPU\ncompute resource available in the cloud is greater than anything available on\nour national supercomputers providing access to globally competitive resources.\nThe direct financial cost of access, compared to procuring and running these\nsystems, was low. The indirect cost, in overcoming teething issues with cloud\nsoftware development, should reduce significantly over time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 09:54:14 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 11:35:02 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Holliman", "Nicolas S.", ""], ["Antony", "Manu", ""], ["Charlton", "James", ""], ["Dowsland", "Stephen", ""], ["James", "Philip", ""], ["Turner", "Mark", ""]]}, {"id": "1902.05027", "submitter": "Arun Lakshmanan", "authors": "Arun Lakshmanan, Andrew Patterson, Venanzio Cichella, Naira Hovakimyan", "title": "Proximity Queries for Absolutely Continuous Parametric Curves", "comments": "Proceedings of Robotics: Science and Systems", "journal-ref": null, "doi": "10.15607/RSS.2019.XV.042", "report-no": null, "categories": "cs.RO cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In motion planning problems for autonomous robots, such as self-driving cars,\nthe robot must ensure that its planned path is not in close proximity to\nobstacles in the environment. However, the problem of evaluating the proximity\nis generally non-convex and serves as a significant computational bottleneck\nfor motion planning algorithms. In this paper, we present methods for a general\nclass of absolutely continuous parametric curves to compute: (i) the minimum\nseparating distance, (ii) tolerance verification, and (iii) collision\ndetection. Our methods efficiently compute bounds on obstacle proximity by\nbounding the curve in a convex region. This bound is based on an upper bound on\nthe curve arc length that can be expressed in closed form for a useful class of\nparametric curves including curves with trigonometric or polynomial bases. We\ndemonstrate the computational efficiency and accuracy of our approach through\nnumerical simulations of several proximity problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:43:13 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 18:57:06 GMT"}, {"version": "v3", "created": "Sun, 5 May 2019 22:28:08 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 23:31:06 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2019 23:39:48 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Lakshmanan", "Arun", ""], ["Patterson", "Andrew", ""], ["Cichella", "Venanzio", ""], ["Hovakimyan", "Naira", ""]]}, {"id": "1902.05395", "submitter": "Wanming Huang", "authors": "Wanming Huang, Yida Xu, Ian Oppermann", "title": "Realistic Image Generation using Region-phrase Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generative Adversarial Network (GAN) has recently been applied to\ngenerate synthetic images from text. Despite significant advances, most current\nstate-of-the-art algorithms are regular-grid region based; when attention is\nused, it is mainly applied between individual regular-grid regions and a word.\nThese approaches are sufficient to generate images that contain a single object\nin its foreground, such as a \"bird\" or \"flower\". However, natural languages\noften involve complex foreground objects and the background may also constitute\na variable portion of the generated image. Therefore, the regular-grid based\nimage attention weights may not necessarily concentrate on the intended\nforeground region(s), which in turn, results in an unnatural looking image.\nAdditionally, individual words such as \"a\", \"blue\" and \"shirt\" do not\nnecessarily provide a full visual context unless they are applied together. For\nthis reason, in our paper, we proposed a novel method in which we introduced an\nadditional set of attentions between true-grid regions and word phrases. The\ntrue-grid region is derived using a set of auxiliary bounding boxes. These\nauxiliary bounding boxes serve as superior location indicators to where the\nalignment and attention should be drawn with the word phrases. Word phrases are\nderived from analysing Part-of-Speech (POS) results. We perform experiments on\nthis novel network architecture using the Microsoft Common Objects in Context\n(MSCOCO) dataset and the model generates $256 \\times 256$ conditioned on a\nshort sentence description. Our proposed approach is capable of generating more\nrealistic images compared with the current state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:23:00 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Huang", "Wanming", ""], ["Xu", "Yida", ""], ["Oppermann", "Ian", ""]]}, {"id": "1902.05672", "submitter": "Hao Zhu", "authors": "Hao Zhu, Mantang Guo, Hongdong Li, Qing Wang, Antonio Robles-Kelly", "title": "Breaking the Spatio-Angular Trade-off for Light Field Super-Resolution\n  via LSTM Modelling on Epipolar Plane Images", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light-field cameras (LFC) have received increasing attention due to their\nwide-spread applications. However, current LFCs suffer from the well-known\nspatio-angular trade-off, which is considered as an inherent and fundamental\nlimit for LFC designs. In this paper, by doing a detailed geometrical optical\nanalysis of the sampling process in an LFC, we show that the effective sampling\nresolution is generally higher than the number of micro-lenses. This\ncontribution makes it theoretically possible to break the resolution trade-off.\nOur second contribution is an epipolar plane image (EPI) based super-resolution\nmethod, which can super-resolve the spatial and angular dimensions\nsimultaneously. We prove that the light field is a 2D series, thus, a\nspecifically designed CNN-LSTM network is proposed to capture the continuity\nproperty of the EPI. Rather than leveraging semantic information, our network\nfocuses on extracting geometric continuity in the EPI. This gives our method an\nimproved generalization ability and makes it applicable to a wide range of\npreviously unseen scenes. Experiments on both synthetic and real light fields\ndemonstrate the improvements over state-of-the-art, especially in large\ndisparity areas.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 03:36:00 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Zhu", "Hao", ""], ["Guo", "Mantang", ""], ["Li", "Hongdong", ""], ["Wang", "Qing", ""], ["Robles-Kelly", "Antonio", ""]]}, {"id": "1902.05883", "submitter": "Amine Bohi", "authors": "Amine Bohi, Xiaoyu Wang, Mariam Al Harrach, Mickael Dinomais,\n  Fran\\c{c}ois Rousseau, Julien Lef\\`evre", "title": "Global Perturbation of Initial Geometry in a Biomechanical Model of\n  Cortical Morphogenesis", "comments": "4 pages 2 columns (IEEE style), 41st EMB Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.bio-ph cs.GR math.SP q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cortical folding pattern is a main characteristic of the geometry of the\nhuman brain which is formed by gyri (ridges) and sulci (grooves). Several\nbiological hypotheses have suggested different mechanisms that attempt to\nexplain the development of cortical folding and its abnormal evolutions. Based\non these hypotheses, biomechanical models of cortical folding have been\nproposed. In this work, we compare biomechanical simulations for several\ninitial conditions by using an adaptive spherical parameterization approach.\nOur approach allows us to study and explore one of the most potential sources\nof reproducible cortical folding pattern: the specification of initial geometry\nof the brain.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:16:28 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 13:41:27 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Bohi", "Amine", ""], ["Wang", "Xiaoyu", ""], ["Harrach", "Mariam Al", ""], ["Dinomais", "Mickael", ""], ["Rousseau", "Fran\u00e7ois", ""], ["Lef\u00e8vre", "Julien", ""]]}, {"id": "1902.05942", "submitter": "Alexander Keller", "authors": "Nikolaus Binder and Sascha Fricke and Alexander Keller", "title": "Massively Parallel Path Space Filtering", "comments": "submitted for the proceedings of MCQMC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricting path tracing to a small number of paths per pixel for performance\nreasons rarely achieves a satisfactory image quality for scenes of interest.\nHowever, path space filtering may dramatically improve the visual quality by\nsharing information across vertices of paths classified as proximate. Unlike\nscreen space-based approaches, these paths neither need to be present on the\nscreen, nor is filtering restricted to the first intersection with the scene.\nWhile searching proximate vertices had been more expensive than filtering in\nscreen space, we greatly improve over this performance penalty by storing,\nupdating, and looking up the required information in a hash table. The keys are\nconstructed from jittered and quantized information, such that only a single\nquery very likely replaces costly neighborhood searches. A massively parallel\nimplementation of the algorithm is demonstrated on a graphics processing unit\n(GPU).\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 18:50:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 12:30:49 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Binder", "Nikolaus", ""], ["Fricke", "Sascha", ""], ["Keller", "Alexander", ""]]}, {"id": "1902.06082", "submitter": "Christian Lessig", "authors": "Christian Lessig", "title": "Local Fourier Slice Photography", "comments": "images with reduced quality (please contact the author for a high\n  resolution version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras provide intriguing possibilities, such as post-capture\nrefocus or the ability to synthesize images from novel viewpoints. This comes,\nhowever, at the price of significant storage requirements. Compression\ntechniques can be used to reduce these but refocusing and reconstruction\nrequire so far again a dense pixel representation. To avoid this, we introduce\nlocal Fourier slice photography that allows for refocused image reconstruction\ndirectly from a sparse wavelet representation of a light field, either to\nobtain an image or a compressed representation of it. The result is made\npossible by wavelets that respect the \"slicing's\" intrinsic structure and\nenable us to derive exact reconstruction filters for the refocused image in\nclosed form. Image reconstruction then amounts to applying these filters to the\nlight field's wavelet coefficients, and hence no reconstruction of a dense\npixel representation is required. We demonstrate that this substantially\nreduces storage requirements and also computation times. We furthermore analyze\nthe computational complexity of our algorithm and show that it scales linearly\nwith the size of the reconstructed region and the non-negligible wavelet\ncoefficients, i.e. with the visual complexity.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:37:00 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 20:22:56 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Lessig", "Christian", ""]]}, {"id": "1902.08228", "submitter": "A. Cengiz Oztireli", "authors": "A. Cengiz \\\"Oztireli", "title": "A Comprehensive Theory and Variational Framework for Anti-aliasing\n  Sampling Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a comprehensive theory of anti-aliasing sampling\npatterns that explains and revises known results, and show how patterns as\npredicted by the theory can be generated via a variational optimization\nframework. We start by deriving the exact spectral expression for expected\nerror in reconstructing an image in terms of power spectra of sampling\npatterns, and analyzing how the shape of power spectra is related to\nanti-aliasing properties. Based on this analysis, we then formulate the problem\nof generating anti-aliasing sampling patterns as constrained variational\noptimization on power spectra. This allows us to not rely on any parametric\nform, and thus explore the whole space of realizable spectra. We show that the\nresulting optimized sampling patterns lead to reconstructions with less visible\naliasing artifacts, while keeping low frequencies as clean as possible.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 16:03:11 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["\u00d6ztireli", "A. Cengiz", ""]]}, {"id": "1902.08755", "submitter": "Stefan Eilemann", "authors": "Stefan Eilemann", "title": "Parallel Rendering and Large Data Visualization", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are living in the big data age: An ever increasing amount of data is being\nproduced through data acquisition and computer simulations. While large scale\nanalysis and simulations have received significant attention for cloud and\nhigh-performance computing, software to efficiently visualise large data sets\nis struggling to keep up.\n  Visualization has proven to be an efficient tool for understanding data, in\nparticular visual analysis is a powerful tool to gain intuitive insight into\nthe spatial structure and relations of 3D data sets. Large-scale visualization\nsetups are becoming ever more affordable, and high-resolution tiled display\nwalls are in reach even for small institutions. Virtual reality has arrived in\nthe consumer space, making it accessible to a large audience.\n  This thesis addresses these developments by advancing the field of parallel\nrendering. We formalise the design of system software for large data\nvisualization through parallel rendering, provide a reference implementation of\na parallel rendering framework, introduce novel algorithms to accelerate the\nrendering of large amounts of data, and validate this research and development\nwith new applications for large data visualization. Applications built using\nour framework enable domain scientists and large data engineers to better\nextract meaning from their data, making it feasible to explore more data and\nenabling the use of high-fidelity visualization installations to see more\ndetail of the data.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 08:44:59 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Eilemann", "Stefan", ""]]}, {"id": "1902.08767", "submitter": "Ahmed Abdelkader", "authors": "Ahmed Abdelkader, Chandrajit L. Bajaj, Mohamed S. Ebeida, Ahmed H.\n  Mahmoud, Scott A. Mitchell, John D. Owens, Ahmad A. Rushdi", "title": "VoroCrust: Voronoi Meshing Without Clipping", "comments": "18 pages (including appendix), 18 figures. Version without compressed\n  images available on https://www.dropbox.com/s/qc6sot1gaujundy/VoroCrust.pdf.\n  Supplemental materials available on\n  https://www.dropbox.com/s/6p72h1e2ivw6kj3/VoroCrust_supplemental_materials.pdf", "journal-ref": "ACM Transaction on Graphics, Vol. 39, No. 3, Article No. 23 (May\n  2020)", "doi": "10.1145/3337680", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyhedral meshes are increasingly becoming an attractive option with\nparticular advantages over traditional meshes for certain applications. What\nhas been missing is a robust polyhedral meshing algorithm that can handle broad\nclasses of domains exhibiting arbitrarily curved boundaries and sharp features.\nIn addition, the power of primal-dual mesh pairs, exemplified by\nVoronoi-Delaunay meshes, has been recognized as an important ingredient in\nnumerous formulations. The VoroCrust algorithm is the first provably-correct\nalgorithm for conforming polyhedral Voronoi meshing for non-convex and\nnon-manifold domains with guarantees on the quality of both surface and volume\nelements. A robust refinement process estimates a suitable sizing field that\nenables the careful placement of Voronoi seeds across the surface circumventing\nthe need for clipping and avoiding its many drawbacks. The algorithm has the\nflexibility of filling the interior by either structured or random samples,\nwhile preserving all sharp features in the output mesh. We demonstrate the\ncapabilities of the algorithm on a variety of models and compare against\nstate-of-the-art polyhedral meshing methods based on clipped Voronoi cells\nestablishing the clear advantage of VoroCrust output.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 09:48:34 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 17:20:36 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Abdelkader", "Ahmed", ""], ["Bajaj", "Chandrajit L.", ""], ["Ebeida", "Mohamed S.", ""], ["Mahmoud", "Ahmed H.", ""], ["Mitchell", "Scott A.", ""], ["Owens", "John D.", ""], ["Rushdi", "Ahmad A.", ""]]}, {"id": "1902.09396", "submitter": "Srihari Pratapa", "authors": "Srihar Pratapa, Dinesh Manocha", "title": "HMLFC: Hierarchical Motion-Compensated Light Field Compression for\n  Interactive Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new motion-compensated hierarchical compression scheme (HMLFC)\nfor encoding light field images (LFI) that is suitable for interactive\nrendering. Our method combines two different approaches, motion compensation\nschemes and hierarchical compression methods, to exploit redundancies in LFI.\nThe motion compensation schemes capture the redundancies in local regions of\nthe LFI efficiently (local coherence) and hierarchical schemes capture the\nredundancies present across the entire LFI (global coherence). Our hybrid\napproach combines the two schemes effectively capturing both local as well as\nglobal coherence to improve the overall compression rate. We compute a tree\nfrom LFI using a hierarchical scheme and use phase shifted motion compensation\ntechniques at each level of the hierarchy. Our representation provides random\naccess to the pixel values of the light field, which makes it suitable for\ninteractive rendering applications using a small run-time memory footprint. Our\napproach is GPU friendly and allows parallel decoding of LF pixel values. We\nhighlight the performance on the two-plane parameterized light fields and\nobtain a compression ratio of 30-800X with a PSNR of 40-45 dB. Overall, we\nobserve a 2-5X improvement in compression rates using HMLFC over prior light\nfield compression schemes that provide random access capability. In practice,\nour algorithm can render new views of resolution 512X512 on an NVIDIA GTX-980\nat ~200 fps.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 16:00:33 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:39:17 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Pratapa", "Srihar", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1902.10160", "submitter": "Scott Burns", "authors": "Scott A Burns", "title": "Chromatic Adaptation Transform by Spectral Reconstruction (Preprint)", "comments": "Ver 2 adds the abstract. Ver 3 gives attribution to Eq 1. Ver 4 adds\n  publication notice. Ver 5 corrects Table 4. Ver 6 adds email address, date,\n  and updates publication notice. Ver 7 adds link to full text of the final\n  published version at Color Res Appl. Ver 8 adds citation of final publication", "journal-ref": "Color Res Appl. 2019;44(5):682-693", "doi": "10.1002/col.22384", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A color appearance model (CAM) is an advanced colorimetric tool used to\npredict color appearance under a wide variety of viewing conditions. A\nchromatic adaptation transform (CAT) is an integral part of a CAM. Its role is\nto predict \"corresponding colors,\" that is, a pair of colors that have the same\ncolor appearance when viewed under different illuminants, after partial or full\nadaptation to each illuminant. Modern CATs perform well when applied to a\nlimited range of illuminant pairs and a limited range of source (test) colors.\nHowever, they can fail if operated outside these ranges. For imaging\napplications, it is important to have a CAT that can operate on any real color\nand illuminant pair without failure. This paper proposes a new CAT that does\nnot operate on the standard von Kries model of adaptation. Instead it relies on\nspectral reconstruction and how these reconstructions behave with respect to\ndifferent illuminants. It is demonstrated that the proposed CAT is immune to\nsome of the limitations of existing CATs (such as producing colors with\nnegative tristimulus values). The proposed CAT does not use established\nempirical corresponding-color datasets to optimize performance, as most modern\nCATs do, yet it performs as well as or better than the most recent CATs when\ntested against the corresponding-color datasets. This increase in robustness\ncomes at the expense of additional complexity and computational effort. If\nrobustness is of prime importance, then the proposed method may be justifiable.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 19:00:29 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 14:13:27 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 09:15:48 GMT"}, {"version": "v4", "created": "Thu, 18 Apr 2019 14:02:37 GMT"}, {"version": "v5", "created": "Tue, 23 Apr 2019 10:30:20 GMT"}, {"version": "v6", "created": "Tue, 21 May 2019 12:12:41 GMT"}, {"version": "v7", "created": "Thu, 4 Jul 2019 12:11:12 GMT"}, {"version": "v8", "created": "Sat, 28 Sep 2019 17:58:06 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Burns", "Scott A", ""]]}, {"id": "1902.11100", "submitter": "Sergey Belim", "authors": "S.V. Belim, D.E. Vilkhovskiy", "title": "Usage of analytic hierarchy process for steganographic inserts detection\n  in images", "comments": null, "journal-ref": "2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the method of steganography detection, which is formed\nby replacing the least significant bit (LSB). Detection is performed by\ndividing the image into layers and making an analysis of zero-layer of adjacent\nbits for every bit. First-layer and second-layer are analyzed too. Hierarchies\nanalysis method is used for making decision if current bit is changed.\nWeighting coefficients as part of the analytic hierarchy process are formed on\nthe values of bits. Then a matrix of corrupted pixels is generated.\nVisualization of matrix with corrupted pixels allows to determine size,\nlocation and presence of the embedded message. Computer experiment was\nperformed. Message was embedded in a bounded rectangular area of the image.\nThis method demonstrated efficiency even at low filling container, less than\n10\\%. Widespread statistical methods are unable to detect this steganographic\ninsert. The location and size of the embedded message can be determined with an\nerror which is not exceeding to five pixels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:34:21 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Belim", "S. V.", ""], ["Vilkhovskiy", "D. E.", ""]]}, {"id": "1902.11216", "submitter": "Bernd Huber", "authors": "Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver Wang,\n  Gautham J. Mysore", "title": "B-Script: Transcript-based B-roll Video Editing with Recommendations", "comments": "11 pages, 10 figures, CHI 2019", "journal-ref": null, "doi": "10.1145/3290605.3300311", "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video production, inserting B-roll is a widely used technique to enrich\nthe story and make a video more engaging. However, determining the right\ncontent and positions of B-roll and actually inserting it within the main\nfootage can be challenging, and novice producers often struggle to get both\ntiming and content right. We present B-Script, a system that supports B-roll\nvideo editing via interactive transcripts. B-Script has a built-in\nrecommendation system trained on expert-annotated data, recommending users\nB-roll position and content. To evaluate the system, we conducted a\nwithin-subject user study with 110 participants, and compared three interface\nvariations: a timeline-based editor, a transcript-based editor, and a\ntranscript-based editor with recommendations. Users found it easier and were\nfaster to insert B-roll using the transcript-based interface, and they created\nmore engaging videos when recommendations were provided.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 17:01:29 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Huber", "Bernd", ""], ["Shin", "Hijung Valentina", ""], ["Russell", "Bryan", ""], ["Wang", "Oliver", ""], ["Mysore", "Gautham J.", ""]]}]