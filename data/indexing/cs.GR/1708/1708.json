[{"id": "1708.00223", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Shengfeng He, Linchao Bao and Qingxiong\n  Yang", "title": "Learning to Hallucinate Face Images via Component Generation and\n  Enhancement", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sr/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage method for face hallucination. First, we generate\nfacial components of the input image using CNNs. These components represent the\nbasic facial structures. Second, we synthesize fine-grained facial structures\nfrom high resolution training images. The details of these structures are\ntransferred into facial components for enhancement. Therefore, we generate\nfacial components to approximate ground truth global appearance in the first\nstage and enhance them through recovering details in the second stage. The\nexperiments demonstrate that our method performs favorably against\nstate-of-the-art methods\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["He", "Shengfeng", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00224", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang", "title": "Fast Preprocessing for Robust Face Sketch Synthesis", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sketch/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based face sketch synthesis methods usually meet the challenging\nproblem that input photos are captured in different lighting conditions from\ntraining photos. The critical step causing the failure is the search of similar\npatch candidates for an input photo patch. Conventional illumination invariant\npatch distances are adopted rather than directly relying on pixel intensity\ndifference, but they will fail when local contrast within a patch changes. In\nthis paper, we propose a fast preprocessing method named Bidirectional\nLuminance Remapping (BLR), which interactively adjust the lighting of training\nand input photos. Our method can be directly integrated into state-of-the-art\nexemplar-based methods to improve their robustness with ignorable computational\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00636", "submitter": "Jaesung Park", "authors": "Jae Sung Park and Nam Ik Cho", "title": "Generation of High Dynamic Range Illumination from a Single Image for\n  the Enhancement of Undesirably Illuminated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm that enhances undesirably illuminated images\nby generating and fusing multi-level illuminations from a single image.The\ninput image is first decomposed into illumination and reflectance components by\nusing an edge-preserving smoothing filter. Then the reflectance component is\nscaled up to improve the image details in bright areas. The illumination\ncomponent is scaled up and down to generate several illumination images that\ncorrespond to certain camera exposure values different from the original. The\nvirtual multi-exposure illuminations are blended into an enhanced illumination,\nwhere we also propose a method to generate appropriate weight maps for the tone\nfusion. Finally, an enhanced image is obtained by multiplying the equalized\nillumination and enhanced reflectance. Experiments show that the proposed\nalgorithm produces visually pleasing output and also yields comparable\nobjective results to the conventional enhancement methods, while requiring\nmodest computational loads.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:14:18 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Park", "Jae Sung", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1708.01841", "submitter": "Minhyuk Sung", "authors": "Minhyuk Sung, Hao Su, Vladimir G. Kim, Siddhartha Chaudhuri, Leonidas\n  Guibas", "title": "ComplementMe: Weakly-Supervised Component Suggestions for 3D Modeling", "comments": "SIGGRAPH Asia 2017. 12 pages", "journal-ref": "ACM Trans. Graph. 36, 6, Article 226 (November 2017)", "doi": "10.1145/3130800.3130821", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assembly-based tools provide a powerful modeling paradigm for non-expert\nshape designers. However, choosing a component from a large shape repository\nand aligning it to a partial assembly can become a daunting task. In this paper\nwe describe novel neural network architectures for suggesting complementary\ncomponents and their placement for an incomplete 3D part assembly. Unlike most\nexisting techniques, our networks are trained on unlabeled data obtained from\npublic online repositories, and do not rely on consistent part segmentations or\nlabels. Absence of labels poses a challenge in indexing the database of parts\nfor the retrieval. We address it by jointly training embedding and retrieval\nnetworks, where the first indexes parts by mapping them to a low-dimensional\nfeature space, and the second maps partial assemblies to appropriate\ncomplements. The combinatorial nature of part arrangements poses another\nchallenge, since the retrieval network is not a function: several complements\ncan be appropriate for the same input. Thus, instead of predicting a single\noutput, we train our network to predict a probability distribution over the\nspace of part embeddings. This allows our method to deal with ambiguities and\nnaturally enables a UI that seamlessly integrates user preferences into the\ndesign process. We demonstrate that our method can be used to design complex\nshapes with minimal or no user input. To evaluate our approach we develop a\nnovel benchmark for component suggestion systems demonstrating significant\nimprovement over state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 04:10:26 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 17:44:58 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Sung", "Minhyuk", ""], ["Su", "Hao", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1708.01955", "submitter": "Morgan A. Schmitz", "authors": "Morgan A. Schmitz, Matthieu Heitz, Nicolas Bonneel, Fred Maurice\n  Ngol\\`e Mboula, David Coeurjolly, Marco Cuturi, Gabriel Peyr\\'e, Jean-Luc\n  Starck", "title": "Wasserstein Dictionary Learning: Optimal Transport-based unsupervised\n  non-linear dictionary learning", "comments": "Published in SIAM SIIMS. 46 pages, 24 figures", "journal-ref": "SIAM Journal on Imaging Sciences 11(1) (2018) 643-678", "doi": "10.1137/17M1140431", "report-no": null, "categories": "stat.ML cs.GR math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new nonlinear dictionary learning method for\nhistograms in the probability simplex. The method leverages optimal transport\ntheory, in the sense that our aim is to reconstruct histograms using so-called\ndisplacement interpolations (a.k.a. Wasserstein barycenters) between dictionary\natoms; such atoms are themselves synthetic histograms in the probability\nsimplex. Our method simultaneously estimates such atoms, and, for each\ndatapoint, the vector of weights that can optimally reconstruct it as an\noptimal transport barycenter of such atoms. Our method is computationally\ntractable thanks to the addition of an entropic regularization to the usual\noptimal transportation problem, leading to an approximation scheme that is\nefficient, parallel and simple to differentiate. Both atoms and weights are\nlearned using a gradient-based descent method. Gradients are obtained by\nautomatic differentiation of the generalized Sinkhorn iterations that yield\nbarycenters with entropic smoothing. Because of its formulation relying on\nWasserstein barycenters instead of the usual matrix product between dictionary\nand codes, our method allows for nonlinear relationships between atoms and the\nreconstruction of input data. We illustrate its application in several\ndifferent image processing settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 01:00:40 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 17:07:57 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 13:15:44 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Schmitz", "Morgan A.", ""], ["Heitz", "Matthieu", ""], ["Bonneel", "Nicolas", ""], ["Mboula", "Fred Maurice Ngol\u00e8", ""], ["Coeurjolly", "David", ""], ["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1708.02136", "submitter": "Weipeng Xu", "authors": "Weipeng Xu, Avishek Chatterjee, Michael Zollh\\\"ofer, Helge Rhodin,\n  Dushyant Mehta, Hans-Peter Seidel, Christian Theobalt", "title": "MonoPerfCap: Human Performance Capture from Monocular Video", "comments": "Accepted to ACM TOG 2018, to be presented on SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first marker-less approach for temporally coherent 3D\nperformance capture of a human with general clothing from monocular video. Our\napproach reconstructs articulated human skeleton motion as well as medium-scale\nnon-rigid surface deformations in general scenes. Human performance capture is\na challenging problem due to the large range of articulation, potentially fast\nmotion, and considerable non-rigid deformations, even from multi-view data.\nReconstruction from monocular video alone is drastically more challenging,\nsince strong occlusions and the inherent depth ambiguity lead to a highly\nill-posed reconstruction problem. We tackle these challenges by a novel\napproach that employs sparse 2D and 3D human pose detections from a\nconvolutional neural network using a batch-based pose estimation strategy.\nJoint recovery of per-batch motion allows to resolve the ambiguities of the\nmonocular reconstruction problem based on a low dimensional trajectory\nsubspace. In addition, we propose refinement of the surface geometry based on\nfully automatically extracted silhouettes to enable medium-scale non-rigid\nalignment. We demonstrate state-of-the-art performance capture results that\nenable exciting applications such as video editing and free viewpoint video,\npreviously infeasible from monocular video. Our qualitative and quantitative\nevaluation demonstrates that our approach significantly outperforms previous\nmonocular methods in terms of accuracy, robustness and scene complexity that\ncan be handled.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:43:57 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 12:40:25 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Xu", "Weipeng", ""], ["Chatterjee", "Avishek", ""], ["Zollh\u00f6fer", "Michael", ""], ["Rhodin", "Helge", ""], ["Mehta", "Dushyant", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1708.02731", "submitter": "Donghyeon Cho", "authors": "Donghyeon Cho, Jinsun Park, Tae-Hyun Oh, Yu-Wing Tai, In So Kweon", "title": "Weakly- and Self-Supervised Learning for Content-Aware Deep Image\n  Retargeting", "comments": "10 pages, 11 figures. To appear in ICCV 2017, Spotlight Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a weakly- and self-supervised deep convolutional neural\nnetwork (WSSDCNN) for content-aware image retargeting. Our network takes a\nsource image and a target aspect ratio, and then directly outputs a retargeted\nimage. Retargeting is performed through a shift map, which is a pixel-wise\nmapping from the source to the target grid. Our method implicitly learns an\nattention map, which leads to a content-aware shift map for image retargeting.\nAs a result, discriminative parts in an image are preserved, while background\nregions are adjusted seamlessly. In the training phase, pairs of an image and\nits image-level annotation are used to compute content and structure losses. We\ndemonstrate the effectiveness of our proposed method for a retargeting\napplication with insightful analyses.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:43:51 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Cho", "Donghyeon", ""], ["Park", "Jinsun", ""], ["Oh", "Tae-Hyun", ""], ["Tai", "Yu-Wing", ""], ["Kweon", "In So", ""]]}, {"id": "1708.02895", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li", "title": "Interacting with Acoustic Simulation and Fabrication", "comments": "ACM UIST 2017 Doctoral Symposium", "journal-ref": null, "doi": "10.1145/3131785.3131842", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating accurate physics-based simulation into interactive design tools\nis challenging. However, adding the physics accurately becomes crucial to\nseveral emerging technologies. For example, in virtual/augmented reality\n(VR/AR) videos, the faithful reproduction of surrounding audios is required to\nbring the immersion to the next level. Similarly, as personal fabrication is\nmade possible with accessible 3D printers, more intuitive tools that respect\nthe physical constraints can help artists to prototype designs. One main hurdle\nis the sheer amount of computation complexity to accurately reproduce the\nreal-world phenomena through physics-based simulation. In my thesis research, I\ndevelop interactive tools that implement efficient physics-based simulation\nalgorithms for automatic optimization and intuitive user interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:20:12 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:14:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Dingzeyu", ""]]}, {"id": "1708.02970", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Kyungdon Joo, Neel Joshi, Baoyuan Wang, In So Kweon, Sing\n  Bing Kang", "title": "Personalized Cinemagraphs using Semantic Understanding and Collaborative\n  Learning", "comments": "To appear in ICCV 2017. Total 17 pages including the supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In\nthese media, dynamic and still elements are juxtaposed to create an artistic\nand narrative experience. Creating a high-quality, aesthetically pleasing\ncinemagraph requires isolating objects in a semantically meaningful way and\nthen selecting good start times and looping periods for those objects to\nminimize visual artifacts (such a tearing). To achieve this, we present a new\ntechnique that uses object recognition and semantic segmentation as part of an\noptimization method to automatically create cinemagraphs from videos that are\nboth visually appealing and semantically meaningful. Given a scene with\nmultiple objects, there are many cinemagraphs one could create. Our method\nevaluates these multiple candidates and presents the best one, as determined by\na model trained to predict human preferences in a collaborative way. We\ndemonstrate the effectiveness of our approach with multiple results and a user\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:03:12 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Joo", "Kyungdon", ""], ["Joshi", "Neel", ""], ["Wang", "Baoyuan", ""], ["Kweon", "In So", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1708.03292", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi,\n  Ren Ng", "title": "Learning to Synthesize a 4D RGBD Light Field from a Single Image", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning algorithm that takes as input a 2D RGB image\nand synthesizes a 4D RGBD light field (color and depth of the scene in each ray\ndirection). For training, we introduce the largest public light field dataset,\nconsisting of over 3300 plenoptic camera light fields of scenes containing\nflowers and plants. Our synthesis pipeline consists of a convolutional neural\nnetwork (CNN) that estimates scene geometry, a stage that renders a Lambertian\nlight field using that geometry, and a second CNN that predicts occluded rays\nand non-Lambertian effects. Our algorithm builds on recent view synthesis\nmethods, but is unique in predicting RGBD for each light field ray and\nimproving unsupervised single image depth estimation by enforcing consistency\nof ray depths that should intersect the same scene point. Please see our\nsupplementary video at https://youtu.be/yLCvWoQLnms\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:50:29 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Wang", "Tongzhou", ""], ["Sreelal", "Ashwin", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""]]}, {"id": "1708.03462", "submitter": "Yanhong Wu", "authors": "Xun Zhao, Yanhong Wu, Weiwei Cui, Xinnan Du, Yuan Chen, Yong Wang, Dik\n  Lun Lee, Huamin Qu", "title": "SkyLens: Visual Analysis of Skyline on Multi-dimensional Data", "comments": "10 pages. Accepted for publication at IEEE VIS 2017 (in proceedings\n  of VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline queries have wide-ranging applications in fields that involve\nmulti-criteria decision making, including tourism, retail industry, and human\nresources. By automatically removing incompetent candidates, skyline queries\nallow users to focus on a subset of superior data items (i.e., the skyline),\nthus reducing the decision-making overhead. However, users are still required\nto interpret and compare these superior items manually before making a\nsuccessful choice. This task is challenging because of two issues. First,\npeople usually have fuzzy, unstable, and inconsistent preferences when\npresented with multiple candidates. Second, skyline queries do not reveal the\nreasons for the superiority of certain skyline points in a multi-dimensional\nspace. To address these issues, we propose SkyLens, a visual analytic system\naiming at revealing the superiority of skyline points from different\nperspectives and at different scales to aid users in their decision making. Two\nscenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of\nattributes. A qualitative study is also conducted to show that users can\nefficiently accomplish skyline understanding and comparison tasks with SkyLens.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 08:04:58 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 01:32:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhao", "Xun", ""], ["Wu", "Yanhong", ""], ["Cui", "Weiwei", ""], ["Du", "Xinnan", ""], ["Chen", "Yuan", ""], ["Wang", "Yong", ""], ["Lee", "Dik Lun", ""], ["Qu", "Huamin", ""]]}, {"id": "1708.03686", "submitter": "Matthew Berger", "authors": "Matthew Berger and Joshua A. Levine", "title": "Visualizing Time-Varying Particle Flows with Diffusion Geometry", "comments": "14 pages, 16 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tasks of identifying separation structures and clusters in flow data are\nfundamental to flow visualization. Significant work has been devoted to these\ntasks in flow represented by vector fields, but there are unique challenges in\naddressing these tasks for time-varying particle data. The unstructured nature\nof particle data, nonuniform and sparse sampling, and the inability to access\narbitrary particles in space-time make it difficult to define separation and\nclustering for particle data. We observe that weaker notions of separation and\nclustering through continuous measures of these structures are meaningful when\ncoupled with user exploration. We achieve this goal by defining a measure of\nparticle similarity between pairs of particles. More specifically, separation\noccurs when spatially-localized particles are dissimilar, while clustering is\ncharacterized by sets of particles that are similar to one another. To be\nrobust to imperfections in sampling we use diffusion geometry to compute\nparticle similarity. Diffusion geometry is parameterized by a scale that allows\na user to explore separation and clustering in a continuous manner. We\nillustrate the benefits of our technique on a variety of 2D and 3D flow\ndatasets, from particles integrated in fluid simulations based on time-varying\nvector fields, to particle-based simulations in astrophysics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 19:58:46 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Berger", "Matthew", ""], ["Levine", "Joshua A.", ""]]}, {"id": "1708.03748", "submitter": "Nazim Haouchine", "authors": "Nazim Haouchine, Frederick Roy, Hadrien Courtecuisse, Matthias\n  Nie{\\ss}ner and Stephane Cotin", "title": "Calipso: Physics-based Image and Video Editing through CAD Model Proxies", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Calipso, an interactive method for editing images and videos in a\nphysically-coherent manner. Our main idea is to realize physics-based\nmanipulations by running a full physics simulation on proxy geometries given by\nnon-rigidly aligned CAD models. Running these simulations allows us to apply\nnew, unseen forces to move or deform selected objects, change physical\nparameters such as mass or elasticity, or even add entire new objects that\ninteract with the rest of the underlying scene. In Calipso, the user makes\nedits directly in 3D; these edits are processed by the simulation and then\ntransfered to the target 2D content using shape-to-image correspondences in a\nphoto-realistic rendering process. To align the CAD models, we introduce an\nefficient CAD-to-image alignment procedure that jointly minimizes for rigid and\nnon-rigid alignment while preserving the high-level structure of the input\nshape. Moreover, the user can choose to exploit image flow to estimate scene\nmotion, producing coherent physical behavior with ambient dynamics. We\ndemonstrate Calipso's physics-based editing on a wide range of examples\nproducing myriad physical behavior while preserving geometric and visual\nconsistency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 07:40:39 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Haouchine", "Nazim", ""], ["Roy", "Frederick", ""], ["Courtecuisse", "Hadrien", ""], ["Nie\u00dfner", "Matthias", ""], ["Cotin", "Stephane", ""]]}, {"id": "1708.03760", "submitter": "Ming-Ze Yuan", "authors": "Ming-Ze Yuan, Lin Gao, Hongbo Fu, Shihong Xia", "title": "Temporal Upsampling of Depth Maps Using a Hybrid Camera", "comments": "IEEE Transactions on Visualization and Computer Graphics, 2018(13\n  pages, 16 figures)", "journal-ref": null, "doi": "10.1109/TVCG.2018.2812879", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, consumer-level depth cameras have been adopted for various\napplications. However, they often produce depth maps at only a moderately high\nframe rate (approximately 30 frames per second), preventing them from being\nused for applications such as digitizing human performance involving fast\nmotion. On the other hand, low-cost, high-frame-rate video cameras are\navailable. This motivates us to develop a hybrid camera that consists of a\nhigh-frame-rate video camera and a low-frame-rate depth camera and to allow\ntemporal interpolation of depth maps with the help of auxiliary color images.\nTo achieve this, we develop a novel algorithm that reconstructs intermediate\ndepth maps and estimates scene flow simultaneously. We test our algorithm on\nvarious examples involving fast, non-rigid motions of single or multiple\nobjects. Our experiments show that our scene flow estimation method is more\nprecise than a tracking-based method and the state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 09:52:16 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 12:04:56 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Yuan", "Ming-Ze", ""], ["Gao", "Lin", ""], ["Fu", "Hongbo", ""], ["Xia", "Shihong", ""]]}, {"id": "1708.04233", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Kyoji Matsushima, Takayuki Takahashi, Yuki\n  Nagahama, Satoki Hasegawa, Marie Sano, Ryuji Hirayama, Takashi Kakue,\n  Tomoyoshi Ito", "title": "Fast, large-scale hologram calculation in wavelet domain", "comments": null, "journal-ref": null, "doi": "10.1016/j.optcom.2017.11.066", "report-no": null, "categories": "cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a large-scale hologram calculation using WAvelet ShrinkAge-Based\nsuperpositIon (WASABI), a wavelet transform-based algorithm. An image-type\nhologram calculated using the WASABI method is printed on a glass substrate\nwith the resolution of $65,536 \\times 65,536$ pixels and a pixel pitch of $1\n\\mu$m. The hologram calculation time amounts to approximately 354 s on a\ncommercial CPU, which is approximately 30 times faster than conventional\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 23:02:34 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Matsushima", "Kyoji", ""], ["Takahashi", "Takayuki", ""], ["Nagahama", "Yuki", ""], ["Hasegawa", "Satoki", ""], ["Sano", "Marie", ""], ["Hirayama", "Ryuji", ""], ["Kakue", "Takashi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1708.04440", "submitter": "\\'Agoston R\\'oth", "authors": "\\'Agoston R\\'oth", "title": "An OpenGL and C++ based function library for curve and surface modeling\n  in a large class of extended Chebyshev spaces", "comments": "29 pages, 20 figures, 2 tables, additional references have been\n  included, some cross-references have been updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a platform-independent multi-threaded function library that\nprovides data structures to generate, differentiate and render both the\nordinary basis and the normalized B-basis of a user-specified extended\nChebyshev (EC) space that comprises the constants and can be identified with\nthe solution space of a constant-coefficient homogeneous linear differential\nequation defined on a sufficiently small interval. Using the obtained\nnormalized B-bases, our library can also generate, (partially) differentiate,\nmodify and visualize a large family of so-called B-curves and tensor product\nB-surfaces. Moreover, the library also implements methods that can be used to\nperform dimension elevation, to subdivide B-curves and B-surfaces by means of\nde Casteljau-like B-algorithms, and to generate basis transformations for the\nB-representation of arbitrary integral curves and surfaces that are described\nin traditional parametric form by means of the ordinary bases of the underlying\nEC spaces. Independently of the algebraic, exponential, trigonometric or mixed\ntype of the applied EC space, the proposed library is numerically stable and\nefficient up to a reasonable dimension number and may be useful for academics\nand engineers in the fields of Approximation Theory, Computer Aided Geometric\nDesign, Computer Graphics, Isogeometric and Numerical Analysis.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 09:09:48 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 11:51:36 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 08:14:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["R\u00f3th", "\u00c1goston", ""]]}, {"id": "1708.04672", "submitter": "Andrey Kurenkov", "authors": "Andrey Kurenkov, Jingwei Ji, Animesh Garg, Viraj Mehta, JunYoung Gwak,\n  Christopher Choy, Silvio Savarese", "title": "DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction\n  from a Single Image", "comments": "11 pages, 9 figures, NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from a single image is a key problem in multiple\napplications ranging from robotic manipulation to augmented reality. Prior\nmethods have tackled this problem through generative models which predict 3D\nreconstructions as voxels or point clouds. However, these methods can be\ncomputationally expensive and miss fine details. We introduce a new\ndifferentiable layer for 3D data deformation and use it in DeformNet to learn a\nmodel for 3D reconstruction-through-deformation. DeformNet takes an image\ninput, searches the nearest shape template from a database, and deforms the\ntemplate to match the query image. We evaluate our approach on the ShapeNet\ndataset and show that - (a) the Free-Form Deformation layer is a powerful new\nbuilding block for Deep Learning models that manipulate 3D data (b) DeformNet\nuses this FFD layer combined with shape retrieval for smooth and\ndetail-preserving 3D reconstruction of qualitatively plausible point clouds\nwith respect to a single query image (c) compared to other state-of-the-art 3D\nreconstruction methods, DeformNet quantitatively matches or outperforms their\nbenchmarks by significant margins. For more information, visit:\nhttps://deformnet-site.github.io/DeformNet-website/ .\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:43:19 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Kurenkov", "Andrey", ""], ["Ji", "Jingwei", ""], ["Garg", "Animesh", ""], ["Mehta", "Viraj", ""], ["Gwak", "JunYoung", ""], ["Choy", "Christopher", ""], ["Savarese", "Silvio", ""]]}, {"id": "1708.04820", "submitter": "Jocelyn Meyron", "authors": "Jocelyn Meyron, Quentin M\\'erigot and Boris Thibert", "title": "Light in Power: A General and Parameter-free Algorithm for Caustic\n  Design", "comments": null, "journal-ref": "ACM Transactions on Graphics, Association for Computing Machinery,\n  2018, 37 (6), pp.1-13", "doi": "10.1145/3272127.3275056", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a generic and parameter-free algorithm to\nefficiently build a wide variety of optical components, such as mirrors or\nlenses, that satisfy some light energy constraints. In all of our problems, one\nis given a collimated or point light source and a desired illumination after\nreflection or refraction and the goal is to design the geometry of a mirror or\nlens which transports exactly the light emitted by the source onto the target.\nWe first propose a general framework and show that eight different optical\ncomponent design problems amount to solving a light energy conservation\nequation that involves the computation of visibility diagrams. We then show\nthat these diagrams all have the same structure and can be obtained by\nintersecting a 3D Power diagram with a planar or spherical domain. This allows\nus to propose an efficient and fully generic algorithm capable to solve these\neight optical component design problems. The support of the prescribed target\nillumination can be a set of directions or a set of points located at a finite\ndistance. Our solutions satisfy design constraints such as convexity or\nconcavity. We show the effectiveness of our algorithm on simulated and\nfabricated examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 09:06:51 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 10:22:21 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Meyron", "Jocelyn", ""], ["M\u00e9rigot", "Quentin", ""], ["Thibert", "Boris", ""]]}, {"id": "1708.05349", "submitter": "Aayush Bansal", "authors": "Aayush Bansal and Yaser Sheikh and Deva Ramanan", "title": "PixelNN: Example-based Image Synthesis", "comments": "Project Page: http://www.cs.cmu.edu/~aayushb/pixelNN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple nearest-neighbor (NN) approach that synthesizes\nhigh-frequency photorealistic images from an \"incomplete\" signal such as a\nlow-resolution image, a surface normal map, or edges. Current state-of-the-art\ndeep generative models designed for such conditional image synthesis lack two\nimportant things: (1) they are unable to generate a large set of diverse\noutputs, due to the mode collapse problem. (2) they are not interpretable,\nmaking it difficult to control the synthesized output. We demonstrate that NN\napproaches potentially address such limitations, but suffer in accuracy on\nsmall datasets. We design a simple pipeline that combines the best of both\nworlds: the first stage uses a convolutional neural network (CNN) to maps the\ninput to a (overly-smoothed) image, and the second stage uses a pixel-wise\nnearest neighbor method to map the smoothed output to multiple high-quality,\nhigh-frequency outputs in a controllable manner. We demonstrate our approach\nfor various input modalities, and for various domains ranging from human faces\nto cats-and-dogs to shoes and handbags.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:13:42 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bansal", "Aayush", ""], ["Sheikh", "Yaser", ""], ["Ramanan", "Deva", ""]]}, {"id": "1708.06034", "submitter": "Qi Sun", "authors": "Qi Sun and Fu-Chung Huang and Li-Yi Wei and David Luebke and Arie\n  Kaufman and Joohwan Kim", "title": "Eccentricity Effects on Blur and Depth Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foveation and focus cue are the two most discussed topics on vision in\ndesigning near-eye displays. Foveation reduces rendering load by omitting\nspatial details in the content that the peripheral vision cannot appreciate;\nProviding richer focal cue can resolve vergence-accommodation conflict thereby\nlessening visual discomfort in using near-eye displays. We performed two\npsychophysical experiments to investigate the relationship between foveation\nand focus cue. The first study measured blur discrimination sensitivity as a\nfunction of visual eccentricity, where we found discrimination thresholds\nsignificantly lower than previously reported. The second study measured depth\ndiscrimination threshold where we found a clear dependency on visual\neccentricity. We discuss the results from the two studies and suggest further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 23:38:41 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 01:37:34 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Sun", "Qi", ""], ["Huang", "Fu-Chung", ""], ["Wei", "Li-Yi", ""], ["Luebke", "David", ""], ["Kaufman", "Arie", ""], ["Kim", "Joohwan", ""]]}, {"id": "1708.06673", "submitter": "Siddhartha Chaudhuri", "authors": "Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri", "title": "Tags2Parts: Discovering Semantic Regions from Shape Tags", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for discovering shape regions that strongly\ncorrelate with user-prescribed tags. For example, given a collection of chairs\ntagged as either \"has armrest\" or \"lacks armrest\", our system correctly\nhighlights the armrest regions as the main distinctive parts between the two\nchair types. To obtain point-wise predictions from shape-wise tags we develop a\nnovel neural network architecture that is trained with tag classification loss,\nbut is designed to rely on segmentation to predict the tag. Our network is\ninspired by U-Net, but we replicate shallow U structures several times with new\nskip connections and pooling layers, and call the resulting architecture\n\"WU-Net\". We test our method on segmentation benchmarks and show that even with\nweak supervision of whole shape tags, our method can infer meaningful semantic\nregions, without ever observing shape segmentations. Further, once trained, the\nmodel can process shapes for which the tag is entirely unknown. As a bonus, our\narchitecture is directly operational under full supervision and performs\nstrongly on standard benchmarks. We validate our method through experiments\nwith many variant architectures and prior baselines, and demonstrate several\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 15:26:00 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 11:26:29 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 20:29:41 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Muralikrishnan", "Sanjeev", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "1708.06684", "submitter": "Vaclav Skala", "authors": "Vaclav Skala", "title": "Fractions, Projective Representation, Duality, Linear Algebra and\n  Geometry", "comments": null, "journal-ref": "CZECH-SLOVAK CONFERENCE ON GEOMETRY AND GRAPHICS 2016, ISBN\n  978-80-7464-874-8", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution describes relationship between fractions, projective\nrepresentation, duality, linear algebra and geometry. Many problems lead to a\nsystem of linear equations. This paper presents equivalence of the\nCross-product operation and solution of a system of linear equations Ax=0 or\nAx=b using projective space representation and homogeneous coordinates. It\nleads to conclusion that division operation is not required for a solution of a\nsystem of linear equations, if the projective representation and homogeneous\ncoordinates are used. An efficient solution on CPU and GPU based architectures\nis presented with an application to barycentric coordinates computation as\nwell.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 10:52:52 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Skala", "Vaclav", ""]]}, {"id": "1708.06695", "submitter": "Vaclav Skala", "authors": "Rongjiang Pan, Vaclav Skala", "title": "A two-level approach to implicit surface modeling with compactly\n  supported radial basis functions", "comments": null, "journal-ref": "Engineering with Computers 2011 27:299-307", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a two-level method for computing a function whose zero-level set\nis the surface reconstructed from given points scattered over the surface and\nassociated with surface normal vectors. The function is defined as a linear\ncombination of compactly supported radial basis functions (CSRBFs). The method\npreserves the simplicity and efficiency of implicit surface interpolation with\nCSRBFs and the reconstructed implicit surface owns the attributes, which are\npreviously only associated with globally supported or globally regularized\nradial basis functions, such as exhibiting less extra zero-level sets, suitable\nfor inside and outside tests. First, in the coarse scale approximation, we\nchoose basis function centers on a grid that covers the enlarged bounding box\nof the given point set and compute their signed distances to the underlying\nsurface using local quadratic approximations of the nearest surface points.\nThen a fitting to the residual errors on the surface points and additional\noff-surface points is performed with fine scale basis functions. The final\nfunction is the sum of the two intermediate functions and is a good\napproximation of the signed distance field to the surface in the bounding box.\nExamples of surface reconstruction and set operations between shapes are\nprovided.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:33:02 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Pan", "Rongjiang", ""], ["Skala", "Vaclav", ""]]}, {"id": "1708.07391", "submitter": "Mei-Heng Yueh", "authors": "Mei-Heng Yueh, Wen-Wei Lin, Chin-Tien Wu, Shing-Tung Yau", "title": "A Novel Stretch Energy Minimization Algorithm for Equiareal\n  Parameterizations", "comments": "29 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface parameterizations have been widely applied to computer graphics and\ndigital geometry processing. In this paper, we propose a novel stretch energy\nminimization (SEM) algorithm for the computation of equiareal parameterizations\nof simply connected open surfaces with a very small area distortion and a\nhighly improved computational efficiency. In addition, the existence of\nnontrivial limit points of the SEM algorithm is guaranteed under some mild\nassumptions of the mesh quality. Numerical experiments indicate that the\nefficiency, accuracy, and robustness of the proposed SEM algorithm outperform\nother state-of-the-art algorithms. Applications of the SEM on surface remeshing\nand surface registration for simply connected open surfaces are demonstrated\nthereafter. Thanks to the SEM algorithm, the computations for these\napplications can be carried out efficiently and robustly.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 09:43:59 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Yueh", "Mei-Heng", ""], ["Lin", "Wen-Wei", ""], ["Wu", "Chin-Tien", ""], ["Yau", "Shing-Tung", ""]]}, {"id": "1708.07559", "submitter": "Jamie Portsmouth", "authors": "Jamie Portsmouth", "title": "Efficient barycentric point sampling on meshes", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an easy-to-implement and efficient analytical inversion algorithm\nfor the unbiased random sampling of a set of points on a triangle mesh whose\nsurface density is specified by barycentric interpolation of non-negative\nper-vertex weights. The correctness of the inversion algorithm is verified via\nstatistical tests, and we show that it is faster on average than rejection\nsampling.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 21:12:51 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Portsmouth", "Jamie", ""]]}, {"id": "1708.08188", "submitter": "Zherong Pan", "authors": "Zherong Pan, Dinesh Manocha", "title": "Active Animations of Reduced Deformable Models with Environment\n  Interactions", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient spacetime optimization method to automatically\ngenerate animations for a general volumetric, elastically deformable body. Our\napproach can model the interactions between the body and the environment and\nautomatically generate active animations. We model the frictional contact\nforces using contact invariant optimization and the fluid drag forces using a\nsimplified model. To handle complex objects, we use a reduced deformable model\nand present a novel hybrid optimizer to search for the local minima\nefficiently. This allows us to use long-horizon motion planning to\nautomatically generate animations such as walking, jumping, swimming, and\nrolling. We evaluate the approach on different shapes and animations, including\ndeformable body navigation and combining with an open-loop controller for\nrealtime forward simulation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 04:36:03 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 23:37:31 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Pan", "Zherong", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1708.08288", "submitter": "Yibing Song", "authors": "Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan\n  Yang", "title": "Stylizing Face Images via Multiple Exemplars", "comments": "In CVIU 2017. Project Page:\n  http://www.cs.cityu.edu.hk/~yibisong/cviu17/index.html", "journal-ref": null, "doi": "10.1016/j.cviu.2017.08.009", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of transferring the style of a headshot photo to face\nimages. Existing methods using a single exemplar lead to inaccurate results\nwhen the exemplar does not contain sufficient stylized facial components for a\ngiven photo. In this work, we propose an algorithm to stylize face images using\nmultiple exemplars containing different subjects in the same style. Patch\ncorrespondences between an input photo and multiple exemplars are established\nusing a Markov Random Field (MRF), which enables accurate local energy transfer\nvia Laplacian stacks. As image patches from multiple exemplars are used, the\nboundaries of facial components on the target image are inevitably\ninconsistent. The artifacts are removed by a post-processing step using an\nedge-preserving filter. Experimental results show that the proposed algorithm\nconsistently produces visually pleasing results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:36:33 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Song", "Yibing", ""], ["Bao", "Linchao", ""], ["He", "Shengfeng", ""], ["Yang", "Qingxiong", ""], ["Yang", "Ming-Hsuan", ""]]}]