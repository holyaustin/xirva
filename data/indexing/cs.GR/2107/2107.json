[{"id": "2107.00227", "submitter": "Wei Zeng", "authors": "Chi Zhang, Wei Zeng, Ligang Liu", "title": "UrbanVR: An immersive analytics system for context-aware urban design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban design is a highly visual discipline that requires visualization for\ninformed decision making. However, traditional urban design tools are mostly\nlimited to representations on 2D displays that lack intuitive awareness. The\npopularity of head-mounted displays (HMDs) promotes a promising alternative\nwith consumer-grade 3D displays. We introduce UrbanVR, an immersive analytics\nsystem with effective visualization and interaction techniques, to enable\narchitects to assess designs in a virtual reality (VR) environment.\nSpecifically, UrbanVR incorporates 1) a customized parallel coordinates plot\n(PCP) design to facilitate quantitative assessment of high-dimensional design\nmetrics, 2) a series of egocentric interactions, including gesture interactions\nand handle-bar metaphors, to facilitate user interactions, and 3) a viewpoint\noptimization algorithm to help users explore both the PCP for quantitative\nanalysis, and objects of interest for context awareness. Effectiveness and\nfeasibility of the system are validated through quantitative user studies and\nqualitative expert feedbacks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:49:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhang", "Chi", ""], ["Zeng", "Wei", ""], ["Liu", "Ligang", ""]]}, {"id": "2107.00397", "submitter": "Leon Victor", "authors": "L\\'eon Victor (LIRIS, INSA Lyon), Alexandre Meyer (LIRIS, UCBL),\n  Sa\\\"ida Bouakaz (LIRIS, UCBL)", "title": "Learning-based pose edition for efficient and interactive design", "comments": null, "journal-ref": "Computer Animation and Virtual Worlds, Wiley, 2021", "doi": "10.1002/cav.2013", "report-no": null, "categories": "cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authoring an appealing animation for a virtual character is a challenging\ntask. In computer-aided keyframe animation artists define the key poses of a\ncharacter by manipulating its underlying skeletons. To look plausible, a\ncharacter pose must respect many ill-defined constraints, and so the resulting\nrealism greatly depends on the animator's skill and knowledge. Animation\nsoftware provide tools to help in this matter, relying on various algorithms to\nautomatically enforce some of these constraints. The increasing availability of\nmotion capture data has raised interest in data-driven approaches to pose\ndesign, with the potential of shifting more of the task of assessing realism\nfrom the artist to the computer, and to provide easier access to nonexperts. In\nthis article, we propose such a method, relying on neural networks to\nautomatically learn the constraints from the data. We describe an efficient\ntool for pose design, allowing na{\\\"i}ve users to intuitively manipulate a pose\nto create character animations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:15:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Victor", "L\u00e9on", "", "LIRIS, INSA Lyon"], ["Meyer", "Alexandre", "", "LIRIS, UCBL"], ["Bouakaz", "Sa\u00efda", "", "LIRIS, UCBL"]]}, {"id": "2107.00480", "submitter": "Nadejda Roubtsova", "authors": "Nadejda Roubtsova, Martin Parsons, Nicola Binetti, Isabelle Mareschal,\n  Essi Viding and Darren Cosker", "title": "EmoGen: Quantifiable Emotion Generation and Analysis for Experimental\n  Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D facial modelling and animation in computer vision and graphics\ntraditionally require either digital artist's skill or complex pipelines with\nobjective-function-based solvers to fit models to motion capture. This\ninaccessibility of quality modelling to a non-expert is an impediment to\neffective quantitative study of facial stimuli in experimental psychology. The\nEmoGen methodology we present in this paper solves the issue democratising\nfacial modelling technology. EmoGen is a robust and configurable framework\nletting anyone author arbitrary quantifiable facial expressions in 3D through a\nuser-guided genetic algorithm search. Beyond sample generation, our methodology\nis made complete with techniques to analyse distributions of these expressions\nin a principled way. This paper covers the technical aspects of expression\ngeneration, specifically our production-quality facial blendshape model,\nautomatic corrective mechanisms of implausible facial configurations in the\nabsence of artist's supervision and the genetic algorithm implementation\nemployed in the model space search. Further, we provide a comparative\nevaluation of ways to quantify generated facial expressions in the blendshape\nand geometric domains and compare them theoretically and empirically. The\npurpose of this analysis is 1. to define a similarity cost function to simulate\nmodel space search for convergence and parameter dependence assessment of the\ngenetic algorithm and 2. to inform the best practices in the data distribution\nanalysis for experimental psychology.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:23:37 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Roubtsova", "Nadejda", ""], ["Parsons", "Martin", ""], ["Binetti", "Nicola", ""], ["Mareschal", "Isabelle", ""], ["Viding", "Essi", ""], ["Cosker", "Darren", ""]]}, {"id": "2107.00938", "submitter": "Mathias-Felipe De-Lima-Santos", "authors": "Mathias-Felipe de-Lima-Santos and Arwa Kooli", "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ\n  Labs Instagram Page", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.GR cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:51:49 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["de-Lima-Santos", "Mathias-Felipe", ""], ["Kooli", "Arwa", ""]]}, {"id": "2107.01664", "submitter": "Henrik Schumacher", "authors": "Chris Yu and Caleb Brakensiek and Henrik Schumacher and Keenan Crane", "title": "Repulsive Surfaces", "comments": "19 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NA math.DG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functionals that penalize bending or stretching of a surface play a key role\nin geometric and scientific computing, but to date have ignored a very basic\nrequirement: in many situations, surfaces must not pass through themselves or\neach other. This paper develops a numerical framework for optimization of\nsurface geometry while avoiding (self-)collision. The starting point is the\ntangent-point energy, which effectively pushes apart pairs of points that are\nclose in space but distant along the surface. We develop a discretization of\nthis energy for triangle meshes, and introduce a novel acceleration scheme\nbased on a fractional Sobolev inner product. In contrast to similar schemes\ndeveloped for curves, we avoid the complexity of building a multiresolution\nmesh hierarchy by decomposing our preconditioner into two ordinary Poisson\nequations, plus forward application of a fractional differential operator. We\nfurther accelerate this scheme via hierarchical approximation, and describe how\nto incorporate a variety of constraints (on area, volume, etc.). Finally, we\nexplore how this machinery might be applied to problems in mathematical\nvisualization, geometric modeling, and geometry processing.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 15:37:34 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Yu", "Chris", ""], ["Brakensiek", "Caleb", ""], ["Schumacher", "Henrik", ""], ["Crane", "Keenan", ""]]}, {"id": "2107.01887", "submitter": "Md. Khaledur Rahman", "authors": "Alexander Kiefer, Md. Khaledur Rahman", "title": "An Analytical Survey on Recent Trends in High Dimensional Data\n  Visualization", "comments": "17 pages, a survey on recent trends in high dimensional data\n  visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data visualization is the process by which data of any size or dimensionality\nis processed to produce an understandable set of data in a lower\ndimensionality, allowing it to be manipulated and understood more easily by\npeople. The goal of our paper is to survey the performance of current\nhigh-dimensional data visualization techniques and quantify their strengths and\nweaknesses through relevant quantitative measures, including runtime, memory\nusage, clustering quality, separation quality, global structure preservation,\nand local structure preservation. To perform the analysis, we select a subset\nof state-of-the-art methods. Our work shows how the selected algorithms produce\nembeddings with unique qualities that lend themselves towards certain tasks,\nand how each of these algorithms are constrained by compute resources.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:22:44 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kiefer", "Alexander", ""], ["Rahman", "Md. Khaledur", ""]]}, {"id": "2107.02041", "submitter": "Zicheng Zhang", "authors": "Zicheng Zhang, Wei Sun, Xiongkuo Min, Tao Wang, Wei Lu, and Guangtao\n  Zhai", "title": "No-Reference Quality Assessment for Colored Point Cloud and Mesh Based\n  on Natural Scene Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the viewer's quality of experience and optimize processing systems\nin computer graphics applications, the 3D quality assessment (3D-QA) has become\nan important task in the multimedia area. Point cloud and mesh are the two most\nwidely used electronic representation formats of 3D models, the quality of\nwhich is quite sensitive to operations like simplification and compression.\nTherefore, many studies concerning point cloud quality assessment (PCQA) and\nmesh quality assessment (MQA) have been carried out to measure the visual\nquality degradations caused by lossy operations. However, a large part of\nprevious studies utilizes full-reference (FR) metrics, which means they may\nfail to predict the accurate quality level of 3D models when the reference 3D\nmodel is not available. Furthermore, limited numbers of 3D-QA metrics are\ncarried out to take color features into consideration, which significantly\nrestricts the effectiveness and scope of application. In many quality\nassessment studies, natural scene statistics (NSS) have shown a good ability to\nquantify the distortion of natural scenes to statistical parameters. Therefore,\nwe propose an NSS-based no-reference quality assessment metric for colored 3D\nmodels. In this paper, quality-aware features are extracted from the aspects of\ncolor and geometry directly from the 3D models. Then the statistic parameters\nare estimated using different distribution models to describe the\ncharacteristic of the 3D models. Our method is mainly validated on the colored\npoint cloud quality assessment database (SJTU-PCQA) and the colored mesh\nquality assessment database (CMDM). The experimental results show that the\nproposed method outperforms all the state-of-art NR 3D-QA metrics and obtains\nan acceptable gap with the state-of-art FR 3D-QA metrics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 14:03:15 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 05:21:16 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 14:44:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zhang", "Zicheng", ""], ["Sun", "Wei", ""], ["Min", "Xiongkuo", ""], ["Wang", "Tao", ""], ["Lu", "Wei", ""], ["Zhai", "Guangtao", ""]]}, {"id": "2107.02191", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Justus Thies, Angela Dai,\n  Matthias Nie{\\ss}ner", "title": "TransformerFusion: Monocular RGB Scene Reconstruction using Transformers", "comments": "Video: https://youtu.be/LIpTKYfKSqw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TransformerFusion, a transformer-based 3D scene reconstruction\napproach. From an input monocular RGB video, the video frames are processed by\na transformer network that fuses the observations into a volumetric feature\ngrid representing the scene; this feature grid is then decoded into an implicit\n3D scene representation. Key to our approach is the transformer architecture\nthat enables the network to learn to attend to the most relevant image frames\nfor each 3D location in the scene, supervised only by the scene reconstruction\ntask. Features are fused in a coarse-to-fine fashion, storing fine-level\nfeatures only where needed, requiring lower memory storage and enabling fusion\nat interactive rates. The feature grid is then decoded to a higher-resolution\nscene reconstruction, using an MLP-based surface occupancy prediction from\ninterpolated coarse-to-fine 3D features. Our approach results in an accurate\nsurface reconstruction, outperforming state-of-the-art multi-view stereo depth\nestimation methods, fully-convolutional 3D reconstruction approaches, and\napproaches using LSTM- or GRU-based recurrent networks for video sequence\nfusion.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:00:11 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Thies", "Justus", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2107.02270", "submitter": "Matthew Petroff", "authors": "Matthew A. Petroff", "title": "Accessible Color Cycles for Data Visualization", "comments": "12 pages, 5 figures, 3 tables; comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color cycles, ordered sets of colors for data visualization, that balance\naesthetics with accessibility considerations are presented. In order to model\naesthetic preference, data were collected with an online survey, and the\nresults were used to train a machine-learning model. To ensure accessibility,\nthis model was combined with minimum-perceptual-distance constraints, including\nfor simulated color-vision deficiencies, as well as with\nminimum-lightness-distance constraints for grayscale printing,\nmaximum-lightness constraints for maintaining contrast with a white background,\nand scores from a color-saliency model for ease of use of the colors in verbal\nand written descriptions. Optimal color cycles containing six, eight, and ten\ncolors were generated using the data-driven aesthetic-preference model and\naccessibility constraints. Due to the balance of aesthetics and accessibility\nconsiderations, the resulting color cycles can serve as reasonable defaults in\ndata-plotting codes.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 21:10:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Petroff", "Matthew A.", ""]]}, {"id": "2107.02708", "submitter": "Hanqi Guo", "authors": "Hanqi Guo and Tom Peterka", "title": "Exact Analytical Parallel Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9507-0621", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates that parallel vector curves are piecewise cubic\nrational curves in 3D piecewise linear vector fields. Parallel vector curves --\nloci of points where two vector fields are parallel -- have been widely used to\nextract features including ridges, valleys, and vortex core lines in scientific\ndata. We define the term \\emph{generalized and underdetermined eigensystem} in\nthe form of\n$\\mathbf{A}\\mathbf{x}+\\mathbf{a}=\\lambda(\\mathbf{B}\\mathbf{x}+\\mathbf{b})$ in\norder to derive the piecewise rational representation of 3D parallel vector\ncurves. We discuss how singularities of the rationals lead to different types\nof intersections with tetrahedral cells.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 16:13:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Guo", "Hanqi", ""], ["Peterka", "Tom", ""]]}, {"id": "2107.02791", "submitter": "Kangle Deng", "authors": "Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan", "title": "Depth-supervised NeRF: Fewer Views and Faster Training for Free", "comments": "Project page: http://www.cs.cmu.edu/~dsnerf/ GitHub:\n  https://github.com/dunbar12138/DSNeRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One common failure mode of Neural Radiance Field (NeRF) models is fitting\nincorrect geometries when given an insufficient number of input views. We\npropose DS-NeRF (Depth-supervised Neural Radiance Fields), a loss for learning\nneural radiance fields that takes advantage of readily-available depth\nsupervision. Our key insight is that sparse depth supervision can be used to\nregularize the learned geometry, a crucial component for effectively rendering\nnovel views using NeRF. We exploit the fact that current NeRF pipelines require\nimages with known camera poses that are typically estimated by running\nstructure-from-motion (SFM). Crucially, SFM also produces sparse 3D points that\ncan be used as ``free\" depth supervision during training: we simply add a loss\nto ensure that depth rendered along rays that intersect these 3D points is\nclose to the observed depth. We find that DS-NeRF can render more accurate\nimages given fewer training views while training 2-6x faster. With only two\ntraining views on real-world images, DS-NeRF significantly outperforms NeRF as\nwell as other sparse-view variants. We show that our loss is compatible with\nthese NeRF models, demonstrating that depth is a cheap and easily digestible\nsupervisory signal. Finally, we show that DS-NeRF supports other types of depth\nsupervision such as scanned depth sensors and RGBD reconstruction outputs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:58:35 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Deng", "Kangle", ""], ["Liu", "Andrew", ""], ["Zhu", "Jun-Yan", ""], ["Ramanan", "Deva", ""]]}, {"id": "2107.02909", "submitter": "Shota Hattori", "authors": "Shota Hattori, Tatsuya Yatagawa, Yutaka Ohtake, Hiromasa Suzuki", "title": "Deep Mesh Prior: Unsupervised Mesh Restoration using Graph Convolutional\n  Networks", "comments": "10 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses mesh restoration problems, i.e., denoising and\ncompletion, by learning self-similarity in an unsupervised manner. For this\npurpose, the proposed method, which we refer to as Deep Mesh Prior, uses a\ngraph convolutional network on meshes to learn the self-similarity. The network\ntakes a single incomplete mesh as input data and directly outputs the\nreconstructed mesh without being trained using large-scale datasets. Our method\ndoes not use any intermediate representations such as an implicit field because\nthe whole process works on a mesh. We demonstrate that our unsupervised method\nperforms equally well or even better than the state-of-the-art methods using\nlarge-scale datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 07:21:10 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hattori", "Shota", ""], ["Yatagawa", "Tatsuya", ""], ["Ohtake", "Yutaka", ""], ["Suzuki", "Hiromasa", ""]]}, {"id": "2107.03030", "submitter": "Hu Chen Dr.", "authors": "Hu Chen, Hong Li, Bifu Hu, Kenan Ma, Yuchun Sun", "title": "A convolutional neural network for teeth margin detection on\n  3-dimensional dental meshes", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a convolutional neural network for vertex classification on\n3-dimensional dental meshes, and used it to detect teeth margins. An expanding\nlayer was constructed to collect statistic values of neighbor vertex features\nand compute new features for each vertex with convolutional neural networks. An\nend-to-end neural network was proposed to take vertex features, including\ncoordinates, curvatures and distance, as input and output each vertex\nclassification label. Several network structures with different parameters of\nexpanding layers and a base line network without expanding layers were designed\nand trained by 1156 dental meshes. The accuracy, recall and precision were\nvalidated on 145 dental meshes to rate the best network structures, which were\nfinally tested on another 144 dental meshes. All networks with our expanding\nlayers performed better than baseline, and the best one achieved an accuracy of\n0.877 both on validation dataset and test dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 06:16:17 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Chen", "Hu", ""], ["Li", "Hong", ""], ["Hu", "Bifu", ""], ["Ma", "Kenan", ""], ["Sun", "Yuchun", ""]]}, {"id": "2107.03106", "submitter": "Mohamed Elgharib", "authors": "Ye Yu, Abhimitra Meka, Mohamed Elgharib, Hans-Peter Seidel, Christian\n  Theobalt, William A. P. Smith", "title": "Self-supervised Outdoor Scene Relighting", "comments": "Published in ECCV '20,\n  http://gvv.mpi-inf.mpg.de/projects/SelfRelight/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor scene relighting is a challenging problem that requires good\nunderstanding of the scene geometry, illumination and albedo. Current\ntechniques are completely supervised, requiring high quality synthetic\nrenderings to train a solution. Such renderings are synthesized using priors\nlearned from limited data. In contrast, we propose a self-supervised approach\nfor relighting. Our approach is trained only on corpora of images collected\nfrom the internet without any user-supervision. This virtually endless source\nof training data allows training a general relighting solution. Our approach\nfirst decomposes an image into its albedo, geometry and illumination. A novel\nrelighting is then produced by modifying the illumination parameters. Our\nsolution capture shadow using a dedicated shadow prediction map, and does not\nrely on accurate geometry estimation. We evaluate our technique subjectively\nand objectively using a new dataset with ground-truth relighting. Results show\nthe ability of our technique to produce photo-realistic and physically\nplausible results, that generalizes to unseen scenes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:46:19 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Yu", "Ye", ""], ["Meka", "Abhimitra", ""], ["Elgharib", "Mohamed", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""], ["Smith", "William A. P.", ""]]}, {"id": "2107.03109", "submitter": "Mohamed Elgharib", "authors": "Mohamed Elgharib, Mohit Mendiratta, Justus Thies, Matthias\n  Nie{\\ss}ner, Hans-Peter Seidel, Ayush Tewari, Vladislav Golyanik, Christian\n  Theobalt", "title": "Egocentric Videoconferencing", "comments": "Mohamed Elgharib and Mohit Mendiratta contributed equally to this\n  work. http://gvv.mpi-inf.mpg.de/projects/EgoChat/", "journal-ref": "ACM Transactions on Graphics, volume = 39, number = 6, articleno =\n  268, year = 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for egocentric videoconferencing that enables\nhands-free video calls, for instance by people wearing smart glasses or other\nmixed-reality devices. Videoconferencing portrays valuable non-verbal\ncommunication and face expression cues, but usually requires a front-facing\ncamera. Using a frontal camera in a hands-free setting when a person is on the\nmove is impractical. Even holding a mobile phone camera in the front of the\nface while sitting for a long duration is not convenient. To overcome these\nissues, we propose a low-cost wearable egocentric camera setup that can be\nintegrated into smart glasses. Our goal is to mimic a classical video call, and\ntherefore, we transform the egocentric perspective of this camera into a front\nfacing video. To this end, we employ a conditional generative adversarial\nneural network that learns a transition from the highly distorted egocentric\nviews to frontal views common in videoconferencing. Our approach learns to\ntransfer expression details directly from the egocentric view without using a\ncomplex intermediate parametric expressions model, as it is used by related\nface reenactment methods. We successfully handle subtle expressions, not easily\ncaptured by parametric blendshape-based solutions, e.g., tongue movement, eye\nmovements, eye blinking, strong expressions and depth varying movements. To get\ncontrol over the rigid head movements in the target view, we condition the\ngenerator on synthetic renderings of a moving neutral face. This allows us to\nsynthesis results at different head poses. Our technique produces temporally\nsmooth video-realistic renderings in real-time using a video-to-video\ntranslation network in conjunction with a temporal discriminator. We\ndemonstrate the improved capabilities of our technique by comparing against\nrelated state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 09:49:39 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Elgharib", "Mohamed", ""], ["Mendiratta", "Mohit", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["Seidel", "Hans-Peter", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "2107.04240", "submitter": "Sharon Xiaolei Huang", "authors": "Yuan Xue, Yuan-Chen Guo, Han Zhang, Tao Xu, Song-Hai Zhang, Xiaolei\n  Huang", "title": "Deep Image Synthesis from Intuitive User Input: A Review and\n  Perspectives", "comments": "26 pages, 7 figures, 1 table", "journal-ref": "Computational Visual Media 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many applications of computer graphics, art and design, it is desirable\nfor a user to provide intuitive non-image input, such as text, sketch, stroke,\ngraph or layout, and have a computer system automatically generate\nphoto-realistic images that adhere to the input content. While classic works\nthat allow such automatic image content generation have followed a framework of\nimage retrieval and composition, recent advances in deep generative models such\nas generative adversarial networks (GANs), variational autoencoders (VAEs), and\nflow-based methods have enabled more powerful and versatile image generation\ntasks. This paper reviews recent works for image synthesis given intuitive user\ninput, covering advances in input versatility, image generation methodology,\nbenchmark datasets, and evaluation metrics. This motivates new perspectives on\ninput representation and interactivity, cross pollination between major image\ngeneration paradigms, and evaluation and comparison of generation methods.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 06:31:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Xue", "Yuan", ""], ["Guo", "Yuan-Chen", ""], ["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Zhang", "Song-Hai", ""], ["Huang", "Xiaolei", ""]]}, {"id": "2107.04286", "submitter": "Yilin Liu", "authors": "Yilin Liu and Fuyou Xue and Hui Huang", "title": "UrbanScene3D: A Large Scale Urban Scene Dataset and Simulator", "comments": "Project page: https://vcc.tech/UrbanScene3D/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perceive the environments in different ways is essential to\nrobotic research. This involves the analysis of both 2D and 3D data sources. We\npresent a large scale urban scene dataset associated with a handy simulator\nbased on Unreal Engine 4 and AirSim, which consists of both man-made and\nreal-world reconstruction scenes in different scales, referred to as\nUrbanScene3D. Unlike previous works that purely based on 2D information or\nman-made 3D CAD models, UrbanScene3D contains both compact man-made models and\ndetailed real-world models reconstructed by aerial images. Each building has\nbeen manually extracted from the entire scene model and then has been assigned\nwith a unique label, forming an instance segmentation map. The provided 3D\nground-truth textured models with instance segmentation labels in UrbanScene3D\nallow users to obtain all kinds of data they would like to have: instance\nsegmentation map, depth map in arbitrary resolution, 3D point cloud/mesh in\nboth visible and invisible places, etc. In addition, with the help of AirSim,\nusers can also simulate the robots (cars/drones)to test a variety of autonomous\ntasks in the proposed city environment. Please refer to our paper and\nwebsite(https://vcc.tech/UrbanScene3D/) for further details and applications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:56:46 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Yilin", ""], ["Xue", "Fuyou", ""], ["Huang", "Hui", ""]]}, {"id": "2107.04331", "submitter": "Yucheol Jung", "authors": "Wonjong Jang, Gwangjin Ju, Yucheol Jung, Jiaolong Yang, Xin Tong,\n  Seungyong Lee", "title": "StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation", "comments": "Accepted to SIGGRAPH 2021. For supplementary material, see\n  http://cg.postech.ac.kr/papers/2021_StyleCariGAN_supp.zip", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 116. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459860", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a caricature generation framework based on shape and style\nmanipulation using StyleGAN. Our framework, dubbed StyleCariGAN, automatically\ncreates a realistic and detailed caricature from an input photo with optional\ncontrols on shape exaggeration degree and color stylization type. The key\ncomponent of our method is shape exaggeration blocks that are used for\nmodulating coarse layer feature maps of StyleGAN to produce desirable\ncaricature shape exaggerations. We first build a layer-mixed StyleGAN for\nphoto-to-caricature style conversion by swapping fine layers of the StyleGAN\nfor photos to the corresponding layers of the StyleGAN trained to generate\ncaricatures. Given an input photo, the layer-mixed model produces detailed\ncolor stylization for a caricature but without shape exaggerations. We then\nappend shape exaggeration blocks to the coarse layers of the layer-mixed model\nand train the blocks to create shape exaggerations while preserving the\ncharacteristic appearances of the input. Experimental results show that our\nStyleCariGAN generates realistic and detailed caricatures compared to the\ncurrent state-of-the-art methods. We demonstrate StyleCariGAN also supports\nother StyleGAN-based image manipulations, such as facial expression control.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:49:31 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jang", "Wonjong", ""], ["Ju", "Gwangjin", ""], ["Jung", "Yucheol", ""], ["Yang", "Jiaolong", ""], ["Tong", "Xin", ""], ["Lee", "Seungyong", ""]]}, {"id": "2107.04510", "submitter": "Anastasia Zvezdakova", "authors": "Maksim Siniukov, Anastasia Antsiferova, Dmitriy Kulikov, Dmitriy\n  Vatolin", "title": "Hacking VMAF and VMAF NEG: metrics vulnerability to different\n  preprocessing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality measurement plays a critical role in the development of video\nprocessing applications. In this paper, we show how popular quality metrics\nVMAF and its tuning-resistant version VMAF NEG can be artificially increased by\nvideo preprocessing. We propose a pipeline for tuning parameters of processing\nalgorithms that allows increasing VMAF by up to 218.8%. A subjective comparison\nof preprocessed videos showed that with the majority of methods visual quality\ndrops down or stays unchanged. We show that VMAF NEG scores can also be\nincreased by some preprocessing methods by up to 23.6%.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:53:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Siniukov", "Maksim", ""], ["Antsiferova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "2107.04875", "submitter": "Manos Kamarianakis", "authors": "Manos Kamarianakis, Nick Lydatakis and George Papagiannakis", "title": "Never 'Drop the Ball' in the Operating Room: An efficient hand-based VR\n  HMD controller interpolation algorithm, for collaborative, networked virtual\n  environments", "comments": "11 pages, 11 figures, Initial paper submitted and accepted to CGI2021\n  - ENGAGE Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose two algorithms that can be applied in the context of\na networked virtual environment to efficiently handle the interpolation of\ndisplacement data for hand-based VR HMDs. Our algorithms, based on the use of\ndual-quaternions and multivectors respectively, impact the network consumption\nrate and are highly effective in scenarios involving multiple users. We\nillustrate convincing results in a modern game engine and a medical VR\ncollaborative training scenario.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 16:48:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kamarianakis", "Manos", ""], ["Lydatakis", "Nick", ""], ["Papagiannakis", "George", ""]]}, {"id": "2107.04974", "submitter": "Boris Kovalerchuk", "authors": "Rose McDonald, Boris Kovalerchuk", "title": "Non-linear Visual Knowledge Discovery with Elliptic Paired Coordinates", "comments": "29 pages, 29 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging for humans to enable visual knowledge discovery in data\nwith more than 2-3 dimensions with a naked eye. This chapter explores the\nefficiency of discovering predictive machine learning models interactively\nusing new Elliptic Paired coordinates (EPC) visualizations. It is shown that\nEPC are capable to visualize multidimensional data and support visual machine\nlearning with preservation of multidimensional information in 2-D. Relative to\nparallel and radial coordinates, EPC visualization requires only a half of the\nvisual elements for each n-D point. An interactive software system EllipseVis,\nwhich is developed in this work, processes high-dimensional datasets, creates\nEPC visualizations, and produces predictive classification models by\ndiscovering dominance rules in EPC. By using interactive and automatic\nprocesses it discovers zones in EPC with a high dominance of a single class.\nThe EPC methodology has been successful in discovering non-linear predictive\nmodels with high coverage and precision in the computational experiments. This\ncan benefit multiple domains by producing visually appealing dominance rules.\nThis chapter presents results of successful testing the EPC non-linear\nmethodology in experiments using real and simulated data, EPC generalized to\nthe Dynamic Elliptic Paired Coordinates (DEPC), incorporation of the weights of\ncoordinates to optimize the visual discovery, introduction of an alternative\nEPC design and introduction of the concept of incompact machine learning\nmethodology based on EPC/DEPC.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 05:53:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["McDonald", "Rose", ""], ["Kovalerchuk", "Boris", ""]]}, {"id": "2107.05043", "submitter": "Daisuke Iwai", "authors": "Kenta Yamamoto, Daisuke Iwai, Kosuke Sato", "title": "A Projector-Camera System Using Hybrid Pixels with Projection and\n  Capturing Capabilities", "comments": "Author's version of a paper published at IDW (International Display\n  Workshops) 2020", "journal-ref": "In Proceedings of the International Display Workshops, pp.\n  655-658, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel projector-camera system (ProCams) in which each pixel has\nboth projection and capturing capabilities. Our proposed ProCams solves the\ndifficulty of obtaining precise pixel correspondence between the projector and\nthe camera. We implemented a proof-of-concept ProCams prototype and\ndemonstrated its applicability to a dynamic projection mapping.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 13:27:25 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Yamamoto", "Kenta", ""], ["Iwai", "Daisuke", ""], ["Sato", "Kosuke", ""]]}, {"id": "2107.05200", "submitter": "Oded Stein", "authors": "Oded Stein, Jiajin Li, Justin Solomon", "title": "A Splitting Scheme for Flip-Free Distortion Energies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a robust optimization method for flip-free distortion energies\nused, for example, in parametrization, deformation, and volume correspondence.\nThis method can minimize a variety of distortion energies, such as the\nsymmetric Dirichlet energy and our new symmetric gradient energy. We identify\nand exploit the special structure of distortion energies to employ an operator\nsplitting technique, leading us to propose a novel Alternating Direction Method\nof Multipliers (ADMM) algorithm to deal with the non-convex, non-smooth nature\nof distortion energies. The scheme results in an efficient method where the\nglobal step involves a single matrix multiplication and the local steps are\nclosed-form per-triangle/per-tetrahedron expressions that are highly\nparallelizable. The resulting general-purpose optimization algorithm exhibits\nrobustness to flipped triangles and tetrahedra in initial data as well as\nduring the optimization. We establish the convergence of our proposed algorithm\nunder certain conditions and demonstrate applications to parametrization,\ndeformation, and volume correspondence.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:27:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Stein", "Oded", ""], ["Li", "Jiajin", ""], ["Solomon", "Justin", ""]]}, {"id": "2107.05284", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Nenglun Chen, Duygu Ceylan, Christian Theobalt, Wenping\n  Wang, Niloy J. Mitra", "title": "CurveFusion: Reconstructing Thin Structures from RGBD Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CurveFusion, the first approach for high quality scanning of\nthin structures at interactive rates using a handheld RGBD camera. Thin\nfilament-like structures are mathematically just 1D curves embedded in R^3, and\nintegration-based reconstruction works best when depth sequences (from the thin\nstructure parts) are fused using the object's (unknown) curve skeleton. Thus,\nusing the complementary but noisy color and depth channels, CurveFusion first\nautomatically identifies point samples on potential thin structures and groups\nthem into bundles, each being a group of a fixed number of aligned consecutive\nframes. Then, the algorithm extracts per-bundle skeleton curves using L1 axes,\nand aligns and iteratively merges the L1 segments from all the bundles to form\nthe final complete curve skeleton. Thus, unlike previous methods,\nreconstruction happens via integration along a data-dependent fusion primitive,\ni.e., the extracted curve skeleton. We extensively evaluate CurveFusion on a\nrange of challenging examples, different scanner and calibration settings, and\npresent high fidelity thin structure reconstructions previously just not\npossible from raw RGBD sequences.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 09:38:27 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liu", "Lingjie", ""], ["Chen", "Nenglun", ""], ["Ceylan", "Duygu", ""], ["Theobalt", "Christian", ""], ["Wang", "Wenping", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2107.05775", "submitter": "Pengsheng Guo", "authors": "Pengsheng Guo, Miguel Angel Bautista, Alex Colburn, Liang Yang, Daniel\n  Ulbricht, Joshua M. Susskind, Qi Shan", "title": "Fast and Explicit Neural View Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of novel view synthesis of a scene comprised of 3D\nobjects. We propose a simple yet effective approach that is neither continuous\nnor implicit, challenging recent trends on view synthesis. We demonstrate that\nalthough continuous radiance field representations have gained a lot of\nattention due to their expressive power, our simple approach obtains comparable\nor even better novel view reconstruction quality comparing with\nstate-of-the-art baselines while increasing rendering speed by over 400x. Our\nmodel is trained in a category-agnostic manner and does not require\nscene-specific optimization. Therefore, it is able to generalize novel view\nsynthesis to object categories not seen during training. In addition, we show\nthat with our simple formulation, we can use view synthesis as a\nself-supervision signal for efficient learning of 3D geometry without explicit\n3D supervision.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 23:24:53 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Guo", "Pengsheng", ""], ["Bautista", "Miguel Angel", ""], ["Colburn", "Alex", ""], ["Yang", "Liang", ""], ["Ulbricht", "Daniel", ""], ["Susskind", "Joshua M.", ""], ["Shan", "Qi", ""]]}, {"id": "2107.05962", "submitter": "Sumit Shekhar", "authors": "Ulrike Bath, Sumit Shekhar, J\\\"urgen D\\\"ollner, Matthias Trapp", "title": "COLiER: Collaborative Editing of Raster Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various web-based image-editing tools and web-based collaborative tools exist\nin isolation. Research focusing to bridge the gap between these two domains is\nsparse. We respond to the above and develop prototype groupware for real-time\ncollaborative editing of raster images in a web browser. To better understand\nthe requirements, we conduct a preliminary user study and establish\ncommunication and synchronization as key elements. The existing groupware for\ntext documents, presentations, and vector graphics handles the above through\nwell-established techniques. However, those cannot be extended as it is for\nraster graphics manipulation. To this end, we develop a document model that is\nmaintained by a server and is delivered and synchronized to multiple clients.\nOur prototypical implementation is based on a scalable client-server\narchitecture: using WebGL for interactive browser-based rendering and WebSocket\nconnections to maintain synchronization. We evaluate our work qualitatively\nthrough a post-deployment user study for three different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 10:09:13 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 10:53:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bath", "Ulrike", ""], ["Shekhar", "Sumit", ""], ["D\u00f6llner", "J\u00fcrgen", ""], ["Trapp", "Matthias", ""]]}, {"id": "2107.06165", "submitter": "Albert Matveev", "authors": "Albert Matveev, Alexey Artemov, Denis Zorin and Evgeny Burnaev", "title": "3D Parametric Wireframe Extraction Based on Distance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a pipeline for parametric wireframe extraction from densely\nsampled point clouds. Our approach processes a scalar distance field that\nrepresents proximity to the nearest sharp feature curve. In intermediate\nstages, it detects corners, constructs curve segmentation, and builds a\ntopological graph fitted to the wireframe. As an output, we produce parametric\nspline curves that can be edited and sampled arbitrarily. We evaluate our\nmethod on 50 complex 3D shapes and compare it to the novel deep learning-based\ntechnique, demonstrating superior quality.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 15:25:14 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2107.06212", "submitter": "Bharadwaj Manda", "authors": "Bharadwaj Manda, Shubham Dhayarkar, Sai Mitheran, V.K. Viekash,\n  Ramanathan Muthuganapathy", "title": "'CADSketchNet' -- An Annotated Sketch dataset for 3D CAD Model Retrieval\n  with Deep Neural Networks", "comments": "Computers & Graphics Journal, Special Section on 3DOR 2021", "journal-ref": "Computers & Graphics, Volume 99, 2021, Pages 100-113, ISSN\n  0097-8493", "doi": "10.1016/j.cag.2021.07.001", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ongoing advancements in the fields of 3D modelling and digital archiving have\nled to an outburst in the amount of data stored digitally. Consequently,\nseveral retrieval systems have been developed depending on the type of data\nstored in these databases. However, unlike text data or images, performing a\nsearch for 3D models is non-trivial. Among 3D models, retrieving 3D\nEngineering/CAD models or mechanical components is even more challenging due to\nthe presence of holes, volumetric features, presence of sharp edges etc., which\nmake CAD a domain unto itself. The research work presented in this paper aims\nat developing a dataset suitable for building a retrieval system for 3D CAD\nmodels based on deep learning. 3D CAD models from the available CAD databases\nare collected, and a dataset of computer-generated sketch data, termed\n'CADSketchNet', has been prepared. Additionally, hand-drawn sketches of the\ncomponents are also added to CADSketchNet. Using the sketch images from this\ndataset, the paper also aims at evaluating the performance of various retrieval\nsystem or a search engine for 3D CAD models that accepts a sketch image as the\ninput query. Many experimental models are constructed and tested on\nCADSketchNet. These experiments, along with the model architecture, choice of\nsimilarity metrics are reported along with the search results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:10:16 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 06:26:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Manda", "Bharadwaj", ""], ["Dhayarkar", "Shubham", ""], ["Mitheran", "Sai", ""], ["Viekash", "V. K.", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2107.06239", "submitter": "Srikrishna Karanam", "authors": "Ren Li and Meng Zheng and Srikrishna Karanam and Terrence Chen and\n  Ziyan Wu", "title": "Everybody Is Unique: Towards Unbiased Human Mesh Recovery", "comments": "10 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of obese human mesh recovery, i.e., fitting a\nparametric human mesh to images of obese people. Despite obese person mesh\nfitting being an important problem with numerous applications (e.g.,\nhealthcare), much recent progress in mesh recovery has been restricted to\nimages of non-obese people. In this work, we identify this crucial gap in the\ncurrent literature by presenting and discussing limitations of existing\nalgorithms. Next, we present a simple baseline to address this problem that is\nscalable and can be easily used in conjunction with existing algorithms to\nimprove their performance. Finally, we present a generalized human mesh\noptimization algorithm that substantially improves the performance of existing\nmethods on both obese person images as well as community-standard benchmark\ndatasets. A key innovation of this technique is that it does not rely on\nsupervision from expensive-to-create mesh parameters. Instead, starting from\nwidely and cheaply available 2D keypoints annotations, our method automatically\ngenerates mesh parameters that can in turn be used to re-train and fine-tune\nany existing mesh estimation algorithm. This way, we show our method acts as a\ndrop-in to improve the performance of a wide variety of contemporary mesh\nestimation methods. We conduct extensive experiments on multiple datasets\ncomprising both standard and obese person images and demonstrate the efficacy\nof our proposed techniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:52:55 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Ren", ""], ["Zheng", "Meng", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Wu", "Ziyan", ""]]}, {"id": "2107.06262", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li, Adam Bargteil", "title": "Learning Aesthetic Layouts via Visual Guidance", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We explore computational approaches for visual guidance to aid in creating\naesthetically pleasing art and graphic design. Our work complements and builds\non previous work that developed models for how humans look at images. Our\napproach comprises three steps. First, we collected a dataset of art\nmasterpieces and labeled the visual fixations with state-of-art vision models.\nSecond, we clustered the visual guidance templates of the art masterpieces with\nunsupervised learning. Third, we developed a pipeline using generative\nadversarial networks to learn the principles of visual guidance and that can\nproduce aesthetically pleasing layouts. We show that the aesthetic visual\nguidance principles can be learned and integrated into a high-dimensional model\nand can be queried by the features of graphic elements. We evaluate our\napproach by generating layouts on various drawings and graphic designs.\nMoreover, our model considers the color and structure of graphic elements when\ngenerating layouts. Consequently, we believe our tool, which generates multiple\naesthetic layout options in seconds, can help artists create beautiful art and\ngraphic designs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:46:42 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2107.06481", "submitter": "Bharadwaj Manda", "authors": "Bharadwaj Manda, Pranjal Bhaskare, Ramanathan Muthuganapathy", "title": "A Convolutional Neural Network Approach to the Classification of\n  Engineering Models", "comments": null, "journal-ref": "in IEEE Access, vol. 9, pp. 22711-22723, 2021", "doi": "10.1109/ACCESS.2021.3055826", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a deep learning approach for the classification of\nEngineering (CAD) models using Convolutional Neural Networks (CNNs). Owing to\nthe availability of large annotated datasets and also enough computational\npower in the form of GPUs, many deep learning-based solutions for object\nclassification have been proposed of late, especially in the domain of images\nand graphical models. Nevertheless, very few solutions have been proposed for\nthe task of functional classification of CAD models. Hence, for this research,\nCAD models have been collected from Engineering Shape Benchmark (ESB), National\nDesign Repository (NDR) and augmented with newer models created using a\nmodelling software to form a dataset - 'CADNET'. It is proposed to use a\nresidual network architecture for CADNET, inspired by the popular ResNet. A\nweighted Light Field Descriptor (LFD) scheme is chosen as the method of feature\nextraction, and the generated images are fed as inputs to the CNN. The problem\nof class imbalance in the dataset is addressed using a class weights approach.\nExperiments have been conducted with other signatures such as geodesic distance\netc. using deep networks as well as other network architectures on the CADNET.\nThe LFD-based CNN approach using the proposed network architecture, along with\ngradient boosting yielded the best classification accuracy on CADNET.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 04:33:50 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Manda", "Bharadwaj", ""], ["Bhaskare", "Pranjal", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2107.06921", "submitter": "Efstratios Kakaletsis", "authors": "Efstratios Kakaletsis, Nikos Nikolaidis", "title": "Potential UAV Landing Sites Detection through Digital Elevation Models\n  Analysis", "comments": "Proceedings of the 2019 27th European Signal Processing Conference\n  (EUSIPCO) satellite workshop \"Signal Processing Computer vision and Deep\n  Learning for Autonomous Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a simple technique for Unmanned Aerial Vehicles (UAVs)\npotential landing site detection using terrain information through\nidentification of flat areas, is presented. The algorithm utilizes digital\nelevation models (DEM) that represent the height distribution of an area. Flat\nareas which constitute appropriate landing zones for UAVs in normal or\nemergency situations result by thresholding the image gradient magnitude of the\ndigital surface model (DSM). The proposed technique also uses connected\ncomponents evaluation on the thresholded gradient image in order to discover\nconnected regions of sufficient size for landing. Moreover, man-made structures\nand vegetation areas are detected and excluded from the potential landing\nsites. Quantitative performance evaluation of the proposed landing site\ndetection algorithm in a number of areas on real world and synthetic datasets,\naccompanied by a comparison with a state-of-the-art algorithm, proves its\nefficiency and superiority.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:13:35 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Kakaletsis", "Efstratios", ""], ["Nikolaidis", "Nikos", ""]]}, {"id": "2107.07058", "submitter": "Pingping Zhang Dr", "authors": "Wei Liu and Pingping Zhang and Yinjie Lei and Xiaolin Huang and Jie\n  Yang and Michael Ng", "title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing", "comments": "This work is accepted by TPAMI. The code is available at\n  https://github.com/wliusjtu/Generalized-Smoothing-Framework. arXiv admin\n  note: substantial text overlap with arXiv:1907.09642", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, we first\nintroduce the truncated Huber penalty function which shows strong flexibility\nunder different parameter settings. A generalized framework is then proposed\nwith the introduced truncated Huber penalty function. When combined with its\nstrong flexibility, our framework is able to achieve diverse smoothing natures\nwhere contradictive smoothing behaviors can even be achieved. It can also yield\nthe smoothing behavior that can seldom be achieved by previous methods, and\nsuperior performance is thus achieved in challenging cases. These together\nenable our framework capable of a range of applications and able to outperform\nthe state-of-the-art approaches in several tasks, such as image detail\nenhancement, clip-art compression artifacts removal, guided depth map\nrestoration, image texture removal, etc. In addition, an efficient numerical\nsolution is provided and its convergence is theoretically guaranteed even the\noptimization framework is non-convex and non-smooth. A simple yet effective\napproach is further proposed to reduce the computational cost of our method\nwhile maintaining its performance. The effectiveness and superior performance\nof our approach are validated through comprehensive experiments in a range of\napplications. Our code is available at\nhttps://github.com/wliusjtu/Generalized-Smoothing-Framework.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 00:55:27 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 03:30:38 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:43:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Lei", "Yinjie", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Ng", "Michael", ""]]}, {"id": "2107.07259", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Xin Sun, Jimei Yang, Ruben Villegas, Jianming Zhang,\n  Zhixin Shu, Belen Masia, and Diego Gutierrez", "title": "Single-image Full-body Human Relighting", "comments": "11 pages, 12 figures", "journal-ref": "Eurographics Symposium on Rendering (EGSR), 2021", "doi": "10.2312/sr.20211300", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a single-image data-driven method to automatically relight images\nwith full-body humans in them. Our framework is based on a realistic scene\ndecomposition leveraging precomputed radiance transfer (PRT) and spherical\nharmonics (SH) lighting. In contrast to previous work, we lift the assumptions\non Lambertian materials and explicitly model diffuse and specular reflectance\nin our data. Moreover, we introduce an additional light-dependent residual term\nthat accounts for errors in the PRT-based image reconstruction. We propose a\nnew deep learning architecture, tailored to the decomposition performed in PRT,\nthat is trained using a combination of L1, logarithmic, and rendering losses.\nOur model outperforms the state of the art for full-body human relighting both\nwith synthetic images and photographs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 11:34:03 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lagunas", "Manuel", ""], ["Sun", "Xin", ""], ["Yang", "Jimei", ""], ["Villegas", "Ruben", ""], ["Zhang", "Jianming", ""], ["Shu", "Zhixin", ""], ["Masia", "Belen", ""], ["Gutierrez", "Diego", ""]]}, {"id": "2107.07477", "submitter": "Paul Rosen", "authors": "Ghulam Jilani Quadri and Paul Rosen", "title": "A Survey of Perception-Based Visualization Studies by Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge of human perception has long been incorporated into visualizations\nto enhance their quality and effectiveness. The last decade, in particular, has\nshown an increase in perception-based visualization research studies. With all\nof this recent progress, the visualization community lacks a comprehensive\nguide to contextualize their results. In this report, we provide a systematic\nand comprehensive review of research studies on perception related to\nvisualization. This survey reviews perception-focused visualization studies\nsince 1980 and summarizes their research developments focusing on low-level\ntasks, further breaking techniques down by visual encoding and visualization\ntype. In particular, we focus on how perception is used to evaluate the\neffectiveness of visualizations, to help readers understand and apply the\nprinciples of perception of their visualization designs through a\ntask-optimized approach. We concluded our report with a summary of the\nweaknesses and open research questions in the area.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:28:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Quadri", "Ghulam Jilani", ""], ["Rosen", "Paul", ""]]}, {"id": "2107.07501", "submitter": "Jie Xu", "authors": "Jie Xu, Tao Chen, Lara Zlokapa, Michael Foshey, Wojciech Matusik,\n  Shinjiro Sueda, Pulkit Agrawal", "title": "An End-to-End Differentiable Framework for Contact-Aware Robot Design", "comments": "Robotics: Science and Systems", "journal-ref": null, "doi": "10.15607/RSS.2021.XVII.008", "report-no": null, "categories": "cs.RO cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current dominant paradigm for robotic manipulation involves two separate\nstages: manipulator design and control. Because the robot's morphology and how\nit can be controlled are intimately linked, joint optimization of design and\ncontrol can significantly improve performance. Existing methods for\nco-optimization are limited and fail to explore a rich space of designs. The\nprimary reason is the trade-off between the complexity of designs that is\nnecessary for contact-rich tasks against the practical constraints of\nmanufacturing, optimization, contact handling, etc. We overcome several of\nthese challenges by building an end-to-end differentiable framework for\ncontact-aware robot design. The two key components of this framework are: a\nnovel deformation-based parameterization that allows for the design of\narticulated rigid robots with arbitrary, complex geometry, and a differentiable\nrigid body simulator that can handle contact-rich scenarios and computes\nanalytical gradients for a full spectrum of kinematic and dynamic parameters.\nOn multiple manipulation tasks, our framework outperforms existing methods that\neither only optimize for control or for design using alternate representations\nor co-optimize using gradient-free methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:53:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xu", "Jie", ""], ["Chen", "Tao", ""], ["Zlokapa", "Lara", ""], ["Foshey", "Michael", ""], ["Matusik", "Wojciech", ""], ["Sueda", "Shinjiro", ""], ["Agrawal", "Pulkit", ""]]}, {"id": "2107.07737", "submitter": "Dunjie Zhang", "authors": "Jinyin Chen, Dunjie Zhang, Zhaoyan Ming, Mingwei Jia, and Yi Liu", "title": "EGC2: Enhanced Graph Classification with Easy Graph Compression", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph classification plays a significant role in network analysis. It also\nfaces potential security threat like adversarial attacks. Some defense methods\nmay sacrifice algorithm complexity for robustness like adversarial training,\nwhile others may sacrifice the clean example performance such as\nsmoothing-based defense. Most of them are suffered from high-complexity or less\ntransferability. To address this problem, we proposed EGC$^2$, an enhanced\ngraph classification model with easy graph compression. EGC$^2$ captures the\nrelationship between features of different nodes by constructing feature graphs\nand improving aggregate node-level representation. To achieve lower complexity\ndefense applied to various graph classification models, EGC$^2$ utilizes a\ncentrality-based edge importance index to compress graphs, filtering out\ntrivial structures and even adversarial perturbations of the input graphs, thus\nimproves its robustness. Experiments on seven benchmark datasets demonstrate\nthat the proposed feature read-out and graph compression mechanisms enhance the\nrobustness of various basic models, thus achieving the state-of-the-art\nperformance of accuracy and robustness in the threat of different adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 07:17:29 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chen", "Jinyin", ""], ["Zhang", "Dunjie", ""], ["Ming", "Zhaoyan", ""], ["Jia", "Mingwei", ""], ["Liu", "Yi", ""]]}, {"id": "2107.07789", "submitter": "Julien Tierny", "authors": "Mathieu Pont, Jules Vidal, Julie Delon and Julien Tierny", "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:27:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Pont", "Mathieu", ""], ["Vidal", "Jules", ""], ["Delon", "Julie", ""], ["Tierny", "Julien", ""]]}, {"id": "2107.08093", "submitter": "Joshua Levine", "authors": "Nghia Truong, Cem Yuksel, Chakrit Watcharopas, Joshua A. Levine,\n  Robert M. Kirby", "title": "Particle Merging-and-Splitting", "comments": "IEEE Trans. Vis. Comput. Graph", "journal-ref": null, "doi": "10.1109/TVCG.2021.3093776", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustly handling collisions between individual particles in a large\nparticle-based simulation has been a challenging problem. We introduce particle\nmerging-and-splitting, a simple scheme for robustly handling collisions between\nparticles that prevents inter-penetrations of separate objects without\nintroducing numerical instabilities. This scheme merges colliding particles at\nthe beginning of the time-step and then splits them at the end of the\ntime-step. Thus, collisions last for the duration of a time-step, allowing\nneighboring particles of the colliding particles to influence each other. We\nshow that our merging-and-splitting method is effective in robustly handling\ncollisions and avoiding penetrations in particle-based simulations. We also\nshow how our merging-and-splitting approach can be used for coupling different\nsimulation systems using different and otherwise incompatible integrators. We\npresent simulation tests involving complex solid-fluid interactions, including\nsolid fractures generated by fluid interactions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 19:46:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Truong", "Nghia", ""], ["Yuksel", "Cem", ""], ["Watcharopas", "Chakrit", ""], ["Levine", "Joshua A.", ""], ["Kirby", "Robert M.", ""]]}, {"id": "2107.08737", "submitter": "Minyoung Kim", "authors": "Minyoung Kim and Young J. Kim", "title": "Synthesizing Human Faces using Latent Space Factorization and Local\n  Weights (Extended Version)", "comments": "Extended version of the paper to will be published in Computer\n  Graphics International 2021 (LNCS Proceeding Papers)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a 3D face generative model with local weights to increase the\nmodel's variations and expressiveness. The proposed model allows partial\nmanipulation of the face while still learning the whole face mesh. For this\npurpose, we address an effective way to extract local facial features from the\nentire data and explore a way to manipulate them during a holistic generation.\nFirst, we factorize the latent space of the whole face to the subspace\nindicating different parts of the face. In addition, local weights generated by\nnon-negative matrix factorization are applied to the factorized latent space so\nthat the decomposed part space is semantically meaningful. We experiment with\nour model and observe that effective facial part manipulation is possible and\nthat the model's expressiveness is improved.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 10:17:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kim", "Minyoung", ""], ["Kim", "Young J.", ""]]}, {"id": "2107.09024", "submitter": "Pablo Antolin", "authors": "Pablo Antolin and Thibaut Hirschler", "title": "Quadrature-free Immersed Isogeometric Analysis", "comments": "24 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for solving partial differential equations\non three-dimensional CAD geometries by means of immersed isogeometric\ndiscretizations that do not require quadrature schemes. It relies on a new\ndeveloped technique for the evaluation of polynomial integrals over spline\nboundary representations that is exclusively based on analytical computations.\nFirst, through a consistent polynomial approximation step, the finite element\noperators of the Galerkin method are transformed into integrals involving only\npolynomial integrands. Then, by successive applications of the divergence\ntheorem, those integrals over B-Reps are transformed into first surface and\nthen line integrals with polynomials integrands. Eventually these line\nintegrals are evaluated analytically with machine precision accuracy. The\nperformance of the proposed method is demonstrated by means of numerical\nexperiments in the context of 2D and 3D elliptic problems, retrieving optimal\nerror convergence order in all cases. Finally, the methodology is illustrated\nfor 3D CAD models with an industrial level of complexity.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:10:37 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Antolin", "Pablo", ""], ["Hirschler", "Thibaut", ""]]}, {"id": "2107.09568", "submitter": "Thibaut Hirschler", "authors": "Thibaut Hirschler and Pablo Antolin and Annalisa Buffa", "title": "Fast and Multiscale Formation of Isogeometric matrices of\n  Microstructured Geometric Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix formation associated to high-order discretizations is known to be\nnumerically demanding. Based on the existing procedure of interpolation and\nlookup, we design a multiscale assembly procedure to reduce the exorbitant\nassembly time in the context of isogeometric linear elasticity of complex\nmicrostructured geometries modeled via spline compositions. The developed\nisogeometric approach involves a polynomial approximation occurring at the\nmacro-scale and the use of lookup tables with pre-computed integrals\nincorporating the micro-scale information. We provide theoretical insights and\nnumerical examples to investigate the performance of the procedure. The\nstrategy turns out to be of great interest not only to form finite element\noperators but also to compute other quantities in a fast manner as for instance\nsensitivity analyses commonly used in design optimization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:30:07 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Hirschler", "Thibaut", ""], ["Antolin", "Pablo", ""], ["Buffa", "Annalisa", ""]]}, {"id": "2107.09965", "submitter": "James Noeckel", "authors": "James Noeckel, Haisen Zhao, Brian Curless, Adriana Schulz", "title": "Fabrication-Aware Reverse Engineering for Carpentry", "comments": "24 pages, plus 6 pages of supplemental material. 14 figures. To be\n  published in Eurographics Symposium on Geometry Processing, Volume 40 (2021),\n  Number 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to generate fabrication blueprints from images of\ncarpentered items. While 3D reconstruction from images is a well-studied\nproblem, typical approaches produce representations that are ill-suited for\ncomputer-aided design and fabrication applications. Our key insight is that\nfabrication processes define and constrain the design space for carpentered\nobjects, and can be leveraged to develop novel reconstruction methods. Our\nmethod makes use of domain-specific constraints to recover not just valid\ngeometry, but a semantically valid assembly of parts, using a combination of\nimage-based and geometric optimization techniques.\n  We demonstrate our method on a variety of wooden objects and furniture, and\nshow that we can automatically obtain designs that are both easy to edit and\naccurate recreations of the ground truth. We further illustrate how our method\ncan be used to fabricate a physical replica of the captured object as well as a\ncustomized version, which can be produced by directly editing the reconstructed\nmodel in CAD software.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:25:15 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Noeckel", "James", ""], ["Zhao", "Haisen", ""], ["Curless", "Brian", ""], ["Schulz", "Adriana", ""]]}, {"id": "2107.10607", "submitter": "Moritz Ibing", "authors": "Moritz Ibing, Isaak Lim, Leif Kobbelt", "title": "3D Shape Generation with Grid-based Implicit Functions", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous approaches to generate shapes in a 3D setting train a GAN on the\nlatent space of an autoencoder (AE). Even though this produces convincing\nresults, it has two major shortcomings. As the GAN is limited to reproduce the\ndataset the AE was trained on, we cannot reuse a trained AE for novel data.\nFurthermore, it is difficult to add spatial supervision into the generation\nprocess, as the AE only gives us a global representation. To remedy these\nissues, we propose to train the GAN on grids (i.e. each cell covers a part of a\nshape). In this representation each cell is equipped with a latent vector\nprovided by an AE. This localized representation enables more expressiveness\n(since the cell-based latent vectors can be combined in novel ways) as well as\nspatial control of the generation process (e.g. via bounding boxes). Our method\noutperforms the current state of the art on all established evaluation\nmeasures, proposed for quantitatively evaluating the generative capabilities of\nGANs. We show limitations of these measures and propose the adaptation of a\nrobust criterion from statistical analysis as an alternative.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 12:23:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ibing", "Moritz", ""], ["Lim", "Isaak", ""], ["Kobbelt", "Leif", ""]]}, {"id": "2107.11008", "submitter": "Mehdi Mousavi", "authors": "Mehdi Mousavi, Rolando Estrada", "title": "SuperCaustics: Real-time, open-source simulation of transparent objects\n  for deep learning applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transparent objects are a very challenging problem in computer vision. They\nare hard to segment or classify due to their lack of precise boundaries, and\nthere is limited data available for training deep neural networks. As such,\ncurrent solutions for this problem employ rigid synthetic datasets, which lack\nflexibility and lead to severe performance degradation when deployed on\nreal-world scenarios. In particular, these synthetic datasets omit features\nsuch as refraction, dispersion and caustics due to limitations in the rendering\npipeline. To address this issue, we present SuperCaustics, a real-time,\nopen-source simulation of transparent objects designed for deep learning\napplications. SuperCaustics features extensive modules for stochastic\nenvironment creation; uses hardware ray-tracing to support caustics,\ndispersion, and refraction; and enables generating massive datasets with\nmulti-modal, pixel-perfect ground truth annotations. To validate our proposed\nsystem, we trained a deep neural network from scratch to segment transparent\nobjects in difficult lighting scenarios. Our neural network achieved\nperformance comparable to the state-of-the-art on a real-world dataset using\nonly 10% of the training data and in a fraction of the training time. Further\nexperiments show that a model trained with SuperCaustics can segment different\ntypes of caustics, even in images with multiple overlapping transparent\nobjects. To the best of our knowledge, this is the first such result for a\nmodel trained on synthetic data. Both our open-source code and experimental\ndata are freely available online.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 03:11:47 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Mousavi", "Mehdi", ""], ["Estrada", "Rolando", ""]]}, {"id": "2107.11186", "submitter": "Yotam Nitzan", "authors": "Yotam Nitzan, Rinon Gal, Ofir Brenner, Daniel Cohen-Or", "title": "LARGE: Latent-Based Regression through GAN Semantics", "comments": "Code at https://github.com/YotamNitzan/LARGE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for solving regression tasks using few-shot or weak\nsupervision. At the core of our method is the fundamental observation that GANs\nare incredibly successful at encoding semantic information within their latent\nspace, even in a completely unsupervised setting. For modern generative\nframeworks, this semantic encoding manifests as smooth, linear directions which\naffect image attributes in a disentangled manner. These directions have been\nwidely used in GAN-based image editing. We show that such directions are not\nonly linear, but that the magnitude of change induced on the respective\nattribute is approximately linear with respect to the distance traveled along\nthem. By leveraging this observation, our method turns a pre-trained GAN into a\nregression model, using as few as two labeled samples. This enables solving\nregression tasks on datasets and attributes which are difficult to produce\nquality supervision for. Additionally, we show that the same latent-distances\ncan be used to sort collections of images by the strength of given attributes,\neven in the absence of explicit supervision. Extensive experimental evaluations\ndemonstrate that our method can be applied across a wide range of domains,\nleverage multiple latent direction discovery frameworks, and achieve\nstate-of-the-art results in few-shot and low-supervision settings, even when\ncompared to methods designed to tackle a single task.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:55:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nitzan", "Yotam", ""], ["Gal", "Rinon", ""], ["Brenner", "Ofir", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2107.11505", "submitter": "Rachel Brown", "authors": "Rachel Brown, Vasha DuTell, Bruce Walter, Ruth Rosenholtz, Peter\n  Shirley, Morgan McGuire, David Luebke", "title": "Efficient Dataflow Modeling of Peripheral Encoding in the Human Visual\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer graphics seeks to deliver compelling images, generated within a\ncomputing budget, targeted at a specific display device, and ultimately viewed\nby an individual user. The foveated nature of human vision offers an\nopportunity to efficiently allocate computation and compression to appropriate\nareas of the viewer's visual field, especially with the rise of high resolution\nand wide field-of-view display devices. However, while the ongoing study of\nfoveal vision is advanced, much less is known about how humans process imagery\nin the periphery of their vision -- which comprises, at any given moment, the\nvast majority of the pixels in the image. We advance computational models for\nperipheral vision aimed toward their eventual use in computer graphics. In\nparticular, we present a dataflow computational model of peripheral encoding\nthat is more efficient than prior pooling - based methods and more compact than\ncontrast sensitivity-based methods. Further, we account for the explicit\nencoding of \"end stopped\" features in the image, which was missing from\nprevious methods. Finally, we evaluate our model in the context of perception\nof textures in the periphery. Our improved peripheral encoding may simplify\ndevelopment and testing of more sophisticated, complete models in more robust\nand realistic settings relevant to computer graphics.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 01:41:12 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Brown", "Rachel", ""], ["DuTell", "Vasha", ""], ["Walter", "Bruce", ""], ["Rosenholtz", "Ruth", ""], ["Shirley", "Peter", ""], ["McGuire", "Morgan", ""], ["Luebke", "David", ""]]}, {"id": "2107.11548", "submitter": "Nikunj Raghuvanshi", "authors": "Nikunj Raghuvanshi", "title": "Dynamic Portal Occlusion for Precomputed Interactive Sound Propagation", "comments": "6 pages, 5 figures, planning to submit to IEEE TVCG Short papers at a\n  future date", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An immersive audio-visual experience in games and virtual reality requires\nfast calculation of diffraction-based acoustic effects. To maintain\nplausibility, the effects must retain spatial smoothness on source and listener\nmotion within geometrically complex scenes. Precomputed wave-based techniques\ncan render such results at low runtime CPU cost, but remain limited to static\nscenes. Modeling the occlusion effect of dynamic portals such as doors present\nan unresolved challenge to maintain audio-visual consistency. We present a fast\nsolution implementable as a drop-in extension to existing precomputed systems.\nKey is a novel portal-search method that leverages precomputed propagation\ndelay and direction data to find portals intervening the diffracted shortest\npath connecting dynamic source and listener at runtime. The method scales\nlinearly with number of portals in worst case, far cheaper than explicit global\npath search that scales with scene area. We discuss culling techniques to\naccelerate further. The search algorithm is combined with geometric-acoustic\napproximations to model the additional direct and indirect energy loss from\nintervening portals depending on their dynamic closure state. We demonstrate\nplausible audio-visual animations within our system integrated with Unreal\nEngine 4 (TM) and AudioKinetic Wwise (TM).\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 07:04:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 03:28:53 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Raghuvanshi", "Nikunj", ""]]}, {"id": "2107.12265", "submitter": "Haisen Zhao", "authors": "Haisen Zhao, Max Willsey, Amy Zhu, Chandrakana Nandi, Zachary Tatlock,\n  Justin Solomon, Adriana Schulz", "title": "Co-Optimization of Design and Fabrication Plans for Carpentry", "comments": "14 pages, 13 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Past work on optimizing fabrication plans given a carpentry design can\nprovide Pareto-optimal plans trading off between material waste, fabrication\ntime, precision, and other considerations. However, when developing fabrication\nplans, experts rarely restrict to a single design, instead considering families\nof design variations, sometimes adjusting designs to simplify fabrication.\nJointly exploring the design and fabrication plan spaces for each design is\nintractable using current techniques. We present a new approach to jointly\noptimize design and fabrication plans for carpentered objects. To make this\nbi-level optimization tractable, we adapt recent work from program synthesis\nbased on equality graphs (e-graphs), which encode sets of equivalent programs.\nOur insight is that subproblems within our bi-level problem share significant\nsubstructures. By representing both designs and fabrication plans in a new bag\nof parts(BOP) e-graph, we amortize the cost of optimizing design components\nshared among multiple candidates. Even using BOP e-graphs, the optimization\nspace grows quickly in practice. Hence, we also show how a feedback-guided\nsearch strategy dubbed Iterative Contraction and Expansion on E-graphs(ICEE)\ncan keep the size of the e-graph manage-able and direct the search toward\npromising candidates. We illustrate the advantages of our pipeline through\nexamples from the carpentry domain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:16:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhao", "Haisen", ""], ["Willsey", "Max", ""], ["Zhu", "Amy", ""], ["Nandi", "Chandrakana", ""], ["Tatlock", "Zachary", ""], ["Solomon", "Justin", ""], ["Schulz", "Adriana", ""]]}, {"id": "2107.12351", "submitter": "Kai-En Lin", "authors": "Tiancheng Sun, Kai-En Lin, Sai Bi, Zexiang Xu, Ravi Ramamoorthi", "title": "NeLF: Neural Light-transport Field for Portrait View Synthesis and\n  Relighting", "comments": "Published at EGSR 2021. Project page with video and code:\n  http://cseweb.ucsd.edu/~viscomp/projects/EGSR21NeLF/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human portraits exhibit various appearances when observed from different\nviews under different lighting conditions. We can easily imagine how the face\nwill look like in another setup, but computer algorithms still fail on this\nproblem given limited observations. To this end, we present a system for\nportrait view synthesis and relighting: given multiple portraits, we use a\nneural network to predict the light-transport field in 3D space, and from the\npredicted Neural Light-transport Field (NeLF) produce a portrait from a new\ncamera view under a new environmental lighting. Our system is trained on a\nlarge number of synthetic models, and can generalize to different synthetic and\nreal portraits under various lighting conditions. Our method achieves\nsimultaneous view synthesis and relighting given multi-view portraits as the\ninput, and achieves state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:44:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Sun", "Tiancheng", ""], ["Lin", "Kai-En", ""], ["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2107.12672", "submitter": "Sebastian Weiss", "authors": "Sebastian Weiss, R\\\"udiger Westermann", "title": "Differentiable Direct Volume Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a differentiable volume rendering solution that provides\ndifferentiability of all continuous parameters of the volume rendering process.\nThis differentiable renderer is used to steer the parameters towards a setting\nwith an optimal solution of a problem-specific objective function. We have\ntailored the approach to volume rendering by enforcing a constant memory\nfootprint via analytic inversion of the blending functions. This makes it\nindependent of the number of sampling steps through the volume and facilitates\nthe consideration of small-scale changes. The approach forms the basis for\nautomatic optimizations regarding external parameters of the rendering process\nand the volumetric density field itself. We demonstrate its use for automatic\nviewpoint selection using differentiable entropy as objective, and for\noptimizing a transfer function from rendered images of a given volume.\nOptimization of per-voxel densities is addressed in two different ways: First,\nwe mimic inverse tomography and optimize a 3D density field from images using\nan absorption model. This simplification enables comparisons with algebraic\nreconstruction techniques and state-of-the-art differentiable path tracers.\nSecond, we introduce a novel approach for tomographic reconstruction from\nimages using an emission-absorption model with post-shading via an arbitrary\ntransfer function.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 08:38:20 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Weiss", "Sebastian", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2107.12900", "submitter": "Daisuke Iwai", "authors": "Haruka Terai, Daisuke Iwai, Kosuke Sato", "title": "Projector Pixel Redirection Using Phase-Only Spatial Light Modulator", "comments": "Author's version of a paper published at IDW (International Display\n  Workshops) 2020", "journal-ref": "In Proceedings of the International Display Workshops, pp.\n  663-665, 2020", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In projection mapping from a projector to a non-planar surface, the pixel\ndensity on the surface becomes uneven. This causes the critical problem of\nlocal spatial resolution degradation. We confirmed that the pixel density\nuniformity on the surface was improved by redirecting projected rays using a\nphase-only spatial light modulator.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 13:24:57 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Terai", "Haruka", ""], ["Iwai", "Daisuke", ""], ["Sato", "Kosuke", ""]]}, {"id": "2107.13421", "submitter": "Yuan Liu", "authors": "Yuan Liu and Sida Peng and Lingjie Liu and Qianqian Wang and Peng Wang\n  and Christian Theobalt and Xiaowei Zhou and Wenping Wang", "title": "Neural Rays for Occlusion-aware Image-based Rendering", "comments": "16 pages and 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new neural representation, called Neural Ray (NeuRay), for the\nnovel view synthesis (NVS) task with multi-view images as input. Existing\nneural scene representations for solving the NVS problem, such as NeRF, cannot\ngeneralize to new scenes and take an excessively long time on training on each\nnew scene from scratch. The other subsequent neural rendering methods based on\nstereo matching, such as PixelNeRF, SRF and IBRNet are designed to generalize\nto unseen scenes but suffer from view inconsistency in complex scenes with\nself-occlusions. To address these issues, our NeuRay method represents every\nscene by encoding the visibility of rays associated with the input views. This\nneural representation can efficiently be initialized from depths estimated by\nexternal MVS methods, which is able to generalize to new scenes and achieves\nsatisfactory rendering images without any training on the scene. Then, the\ninitialized NeuRay can be further optimized on every scene with little training\ntiming to enforce spatial coherence to ensure view consistency in the presence\nof severe self-occlusion. Experiments demonstrate that NeuRay can quickly\ngenerate high-quality novel view images of unseen scenes with little finetuning\nand can handle complex scenes with severe self-occlusions which previous\nmethods struggle with.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:09:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Yuan", ""], ["Peng", "Sida", ""], ["Liu", "Lingjie", ""], ["Wang", "Qianqian", ""], ["Wang", "Peng", ""], ["Theobalt", "Christian", ""], ["Zhou", "Xiaowei", ""], ["Wang", "Wenping", ""]]}, {"id": "2107.13452", "submitter": "Qing Guo", "authors": "Qing Guo and Zhijie Wang and Felix Juefei-Xu and Di Lin and Lei Ma and\n  Wei Feng and Yang Liu", "title": "CarveNet: Carving Point-Block for Complex 3D Shape Completion", "comments": "10 pages and 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D point cloud completion is very challenging because it heavily relies on\nthe accurate understanding of the complex 3D shapes (e.g., high-curvature,\nconcave/convex, and hollowed-out 3D shapes) and the unknown & diverse patterns\nof the partially available point clouds. In this paper, we propose a novel\nsolution,i.e., Point-block Carving (PC), for completing the complex 3D point\ncloud completion. Given the partial point cloud as the guidance, we carve a3D\nblock that contains the uniformly distributed 3D points, yielding the entire\npoint cloud. To achieve PC, we propose a new network architecture, i.e.,\nCarveNet. This network conducts the exclusive convolution on each point of the\nblock, where the convolutional kernels are trained on the 3D shape data.\nCarveNet determines which point should be carved, for effectively recovering\nthe details of the complete shapes. Furthermore, we propose a sensor-aware\nmethod for data augmentation,i.e., SensorAug, for training CarveNet on richer\npatterns of partial point clouds, thus enhancing the completion power of the\nnetwork. The extensive evaluations on the ShapeNet and KITTI datasets\ndemonstrate the generality of our approach on the partial point clouds with\ndiverse patterns. On these datasets, CarveNet successfully outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:07:20 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Guo", "Qing", ""], ["Wang", "Zhijie", ""], ["Juefei-Xu", "Felix", ""], ["Lin", "Di", ""], ["Ma", "Lei", ""], ["Feng", "Wei", ""], ["Liu", "Yang", ""]]}, {"id": "2107.14024", "submitter": "Hongwei Lin", "authors": "Chenkai Xu, Yaqi He, Hui Hu, Hongwei Lin", "title": "Stochastic Geometric Iterative Method for Loop Subdivision Surface\n  Fitting", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a stochastic geometric iterative method to\napproximate the high-resolution 3D models by finite Loop subdivision surfaces.\nGiven an input mesh as the fitting target, the initial control mesh is\ngenerated using the mesh simplification algorithm. Then, our method adjusts the\ncontrol mesh iteratively to make its finite Loop subdivision surface\napproximates the input mesh. In each geometric iteration, we randomly select\npart of points on the subdivision surface to calculate the difference vectors\nand distribute the vectors to the control points. Finally, the control points\nare updated by adding the weighted average of these difference vectors. We\nprove the convergence of our method and verify it by demonstrating error curves\nin the experiment. In addition, compared with an existing geometric iterative\nmethod, our method has a faster fitting speed and higher fitting precision.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:35:16 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Xu", "Chenkai", ""], ["He", "Yaqi", ""], ["Hu", "Hui", ""], ["Lin", "Hongwei", ""]]}, {"id": "2107.14230", "submitter": "Dongdong Chen", "authors": "Shuquan Ye and Dongdong Chen and Songfang Han and Jing Liao", "title": "Learning with Noisy Labels for Robust Point Cloud Segmentation", "comments": "ICCV 2021 Oral, Relabeled ScanNetV2 and code are available at\n  https://shuquanye.com/PNAL_website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud segmentation is a fundamental task in 3D. Despite recent progress\non point cloud segmentation with the power of deep networks, current deep\nlearning methods based on the clean label assumptions may fail with noisy\nlabels. Yet, object class labels are often mislabeled in real-world point cloud\ndatasets. In this work, we take the lead in solving this issue by proposing a\nnovel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing\nnoise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with\nthe spatially variant noise rate problem specific to point clouds.\nSpecifically, we propose a novel point-wise confidence selection to obtain\nreliable labels based on the historical predictions of each point. A novel\ncluster-wise label correction is proposed with a voting strategy to generate\nthe best possible label taking the neighbor point correlations into\nconsideration. We conduct extensive experiments to demonstrate the\neffectiveness of PNAL on both synthetic and real-world noisy datasets. In\nparticular, even with $60\\%$ symmetric noisy labels, our proposed method\nproduces much better results than its baseline counterpart without PNAL and is\ncomparable to the ideal upper bound trained on a completely clean dataset.\nMoreover, we fully re-labeled the test set of a popular but noisy real-world\nscene dataset ScanNetV2 to make it clean, for rigorous experiment and future\nresearch. Our code and data will be available at\n\\url{https://shuquanye.com/PNAL_website/}.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:59:54 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ye", "Shuquan", ""], ["Chen", "Dongdong", ""], ["Han", "Songfang", ""], ["Liao", "Jing", ""]]}]