[{"id": "2011.00510", "submitter": "Jeffrey R. Weeks", "authors": "Jeff Weeks", "title": "Body coherence in curved-space virtual reality games", "comments": "20 pages, 28 figures", "journal-ref": "Computers & Graphics 97 (2021) 28-41", "doi": "10.1016/j.cag.2021.04.002", "report-no": null, "categories": "physics.ed-ph cs.GR math.HO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Virtual-reality simulations of curved space are most effective and most fun\nwhen presented as a game (for example, curved-space billiards), so the user not\nonly has something to see in the curved space, but also has something fun to do\nthere. However, such simulations encounter a geometrical problem: they must\ntrack the player's hands as well as her head, and in curved space the effects\nof holonomy would quickly lead to violations of \"body coherence\". That is, what\nthe player sees with her eyes would disagree with what she feels with her\nhands. This article presents a solution to the body coherence problem, as well\nas several other questions that arise in interactive VR simulations in curved\nspace (radians vs. meters, visualization of the projection transformation,\nnative-inhabitant view vs. tourist view, and mental models of curved space).\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 14:20:11 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 20:41:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Weeks", "Jeff", ""]]}, {"id": "2011.01446", "submitter": "Mingdong Zhang", "authors": "Mingdong Zhang, Li Chen, Xiaoru Yuan, Renpei Huang, Shuang Liu, and\n  Junhai Yong", "title": "Visualization of Technical and Tactical Characteristics in Fencing", "comments": null, "journal-ref": "Journal of Visualization, 2019, 22(1): 109-124", "doi": "10.1007/s12650-018-0521-3", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fencing is a sport that relies heavily on the use of tactics. However, most\nexisting methods for analyzing fencing data are based on statistical models in\nwhich hidden patterns are difficult to discover. Unlike sequential games, such\nas tennis and table tennis, fencing is a type of simultaneous game. Thus, the\nexisting methods on the sports visualization do not operate well for fencing\nmatches. In this study, we cooperated with experts to analyze the technical and\ntactical characteristics of fencing competitions. To meet the requirements of\nthe fencing experts, we designed and implemented FencingVis, an interactive\nvisualization system for fencing competition data.The action sequences in the\nbout are first visualized by modified bar charts to reveal the actions of\nfootworks and bladeworks of both fencers. Then an interactive technique is\nprovided for exploring the patterns of behavior of fencers. The different\ncombinations of tactical behavior patterns are further mapped to the graph\nmodel and visualized by a tactical flow graph. This graph can reveal the\ndifferent strategies adopted by both fencers and their mutual influence in one\nbout. We also provided a number of well-coordinated views to supplement the\ntactical flow graph and display the information of the fencing competition from\ndifferent perspectives. The well-coordinated views are meant to organically\nintegrate with the tactical flow graph through consistent visual style and view\ncoordination. We demonstrated the usability and effectiveness of the proposed\nsystem with three case studies. On the basis of expert feedback, FencingVis can\nhelp analysts find not only the tactical patterns hidden in fencing bouts, but\nalso the technical and tactical characteristics of the contestant.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:26:10 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Mingdong", ""], ["Chen", "Li", ""], ["Yuan", "Xiaoru", ""], ["Huang", "Renpei", ""], ["Liu", "Shuang", ""], ["Yong", "Junhai", ""]]}, {"id": "2011.01497", "submitter": "Mingdong Zhang", "authors": "Mingdong Zhang, Li Chen, Quan Li, Xiaoru Yuan, Junhai Yong", "title": "Uncertainty-Oriented Ensemble Data Visualization and Exploration using\n  Variable Spatial Spreading", "comments": "in IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030377", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important method of handling potential uncertainties in numerical\nsimulations, ensemble simulation has been widely applied in many disciplines.\nVisualization is a promising and powerful ensemble simulation analysis method.\nHowever, conventional visualization methods mainly aim at data simplification\nand highlighting important information based on domain expertise instead of\nproviding a flexible data exploration and intervention mechanism.\nTrial-and-error procedures have to be repeatedly conducted by such approaches.\nTo resolve this issue, we propose a new perspective of ensemble data analysis\nusing the attribute variable dimension as the primary analysis dimension.\nParticularly, we propose a variable uncertainty calculation method based on\nvariable spatial spreading. Based on this method, we design an interactive\nensemble analysis framework that provides a flexible interactive exploration of\nthe ensemble data. Particularly, the proposed spreading curve view, the region\nstability heat map view, and the temporal analysis view, together with the\ncommonly used 2D map view, jointly support uncertainty distribution perception,\nregion selection, and temporal analysis, as well as other analysis\nrequirements. We verify our approach by analyzing a real-world ensemble\nsimulation dataset. Feedback collected from domain experts confirms the\nefficacy of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 06:09:46 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zhang", "Mingdong", ""], ["Chen", "Li", ""], ["Li", "Quan", ""], ["Yuan", "Xiaoru", ""], ["Yong", "Junhai", ""]]}, {"id": "2011.01630", "submitter": "Nicolas Mellado", "authors": "Chems-Eddine Himeur, Thibault Lejemble, Thomas Pellegrini, Mathias\n  Paulin, Loic Barthe, Nicolas Mellado", "title": "PCEDNet : A Lightweight Neural Network for Fast and Interactive Edge\n  Detection in 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNN) have proven to be\nefficient analysis tools for processing point clouds, e.g., for reconstruction,\nsegmentation and classification. In this paper, we focus on the classification\nof edges in point clouds, where both edges and their surrounding are described.\nWe propose a new parameterization adding to each point a set of differential\ninformation on its surrounding shape reconstructed at different scales. These\nparameters, stored in a Scale-Space Matrix (SSM), provide a well suited\ninformation from which an adequate neural network can learn the description of\nedges and use it to efficiently detect them in acquired point clouds. After\nsuccessfully applying a multi-scale CNN on SSMs for the efficient\nclassification of edges and their neighborhood, we propose a new lightweight\nneural network architecture outperforming the CNN in learning time, processing\ntime and classification capabilities. Our architecture is compact, requires\nsmall learning sets, is very fast to train and classifies millions of points in\nseconds.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 11:16:13 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:05:29 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Himeur", "Chems-Eddine", ""], ["Lejemble", "Thibault", ""], ["Pellegrini", "Thomas", ""], ["Paulin", "Mathias", ""], ["Barthe", "Loic", ""], ["Mellado", "Nicolas", ""]]}, {"id": "2011.01934", "submitter": "Chihiro Noguchi", "authors": "Chihiro Noguchi and Tatsuro Kawamoto", "title": "Palette diagram: A Python package for visualization of collective\n  categorical data", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical data, wherein a numerical quantity is assigned to each category\n(nominal variable), are ubiquitous in data science. A palette diagram is a\nvisualization tool for a large number of categorical datasets, each comprising\nseveral categories.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 10:50:05 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Noguchi", "Chihiro", ""], ["Kawamoto", "Tatsuro", ""]]}, {"id": "2011.02368", "submitter": "Nilanjan Goswami", "authors": "Nilanjan Goswami, Amer Qouneh, Chao Li, Tao Li", "title": "An Empirical-cum-Statistical Approach to Power-Performance\n  Characterization of Concurrent GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing deployment of power and energy efficient throughput accelerators\n(GPU) in data centers demands enhancement of power-performance co-optimization\ncapabilities of GPUs. Realization of exascale computing using accelerators\nrequires further improvements in power efficiency. With hardwired kernel\nconcurrency enablement in accelerators, inter- and intra-workload simultaneous\nkernels computation predicts increased throughput at lower energy budget. To\nimprove Performance-per-Watt metric of the architectures, a systematic\nempirical study of real-world throughput workloads (with concurrent kernel\nexecution) is required. To this end, we propose a multi-kernel throughput\nworkload generation framework that will facilitate aggressive energy and\nperformance management of exascale data centers and will stimulate synergistic\npower-performance co-optimization of throughput architectures. Also, we\ndemonstrate a multi-kernel throughput benchmark suite based on the framework\nthat encapsulates symmetric, asymmetric and co-existing (often appears\ntogether) kernel based workloads. On average, our analysis reveals that spatial\nand temporal concurrency within kernel execution in throughput architectures\nsaves energy consumption by 32%, 26% and 33% in GTX470, Tesla M2050 and Tesla\nK20 across 12 benchmarks. Concurrency and enhanced utilization are often\ncorrelated but do not imply significant deviation in power dissipation.\nDiversity analysis of proposed multi-kernels confirms characteristic variation\nand power-profile diversity within the suite. Besides, we explain several\nfindings regarding power-performance co-optimization of concurrent throughput\nworkloads.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:58:54 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 03:25:55 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Goswami", "Nilanjan", ""], ["Qouneh", "Amer", ""], ["Li", "Chao", ""], ["Li", "Tao", ""]]}, {"id": "2011.02523", "submitter": "Mike Roberts", "authors": "Mike Roberts, Nathan Paczan", "title": "Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene\n  Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many fundamental scene understanding tasks, it is difficult or impossible\nto obtain per-pixel ground truth labels from real images. We address this\nchallenge by introducing Hypersim, a photorealistic synthetic dataset for\nholistic indoor scene understanding. To create our dataset, we leverage a large\nrepository of synthetic scenes created by professional artists, and we generate\n77,400 images of 461 indoor scenes with detailed per-pixel labels and\ncorresponding ground truth geometry. Our dataset: (1) relies exclusively on\npublicly available 3D assets; (2) includes complete scene geometry, material\ninformation, and lighting information for every scene; (3) includes dense\nper-pixel semantic instance segmentations for every image; and (4) factors\nevery image into diffuse reflectance, diffuse illumination, and a non-diffuse\nresidual term that captures view-dependent lighting effects. Together, these\nfeatures make our dataset well-suited for geometric learning problems that\nrequire direct 3D supervision, multi-task learning problems that require\nreasoning jointly over multiple input and output modalities, and inverse\nrendering problems. We analyze our dataset at the level of scenes, objects, and\npixels, and we analyze costs in terms of money, annotation effort, and\ncomputation time. Remarkably, we find that it is possible to generate our\nentire dataset from scratch, for roughly half the cost of training a\nstate-of-the-art natural language processing model. All the code we used to\ngenerate our dataset is available online.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 20:12:07 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 20:43:43 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 08:05:56 GMT"}, {"version": "v4", "created": "Tue, 17 Nov 2020 02:54:24 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Roberts", "Mike", ""], ["Paczan", "Nathan", ""]]}, {"id": "2011.03082", "submitter": "Ludwig Leonard", "authors": "Ludwig Leonard, Kevin Hoehlein and Ruediger Westermann", "title": "Learning Multiple-Scattering Solutions for Sphere-Tracing of Volumetric\n  Subsurface Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate subsurface scattering solutions require the integration of optical\nmaterial properties along many complicated light paths. We present a method\nthat learns a simple geometric approximation of random paths in a homogeneous\nvolume of translucent material. The generated representation allows determining\nthe absorption along the path as well as a direct lighting contribution, which\nis representative of all scattering events along the path. A sequence of\nconditional variational auto-encoders (CVAEs) is trained to model the\nstatistical distribution of the photon paths inside a spherical region in\npresence of multiple scattering events. A first CVAE learns to sample the\nnumber of scattering events, occurring on a ray path inside the sphere, which\neffectively determines the probability of the ray being absorbed. Conditioned\non this, a second model predicts the exit position and direction of the light\nparticle. Finally, a third model generates a representative sample of photon\nposition and direction along the path, which is used to approximate the\ncontribution of direct illumination due to in-scattering. To accelerate the\ntracing of the light path through the volumetric medium toward the solid\nboundary, we employ a sphere-tracing strategy that considers the light\nabsorption and is able to perform statistically accurate next-event estimation.\nWe demonstrate efficient learning using shallow networks of only three layers\nand no more than 16 nodes. In combination with a GPU shader that evaluates the\nCVAEs' predictions, performance gains can be demonstrated for a variety of\ndifferent scenarios. A quality evaluation analyzes the approximation error that\nis introduced by the data-driven scattering simulation and sheds light on the\nmajor sources of error in the accelerated path tracing process.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:15:22 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Leonard", "Ludwig", ""], ["Hoehlein", "Kevin", ""], ["Westermann", "Ruediger", ""]]}, {"id": "2011.03209", "submitter": "Youjia Zhou", "authors": "Youjia Zhou, Nithin Chalapathi, Archit Rathore, Yaodong Zhao, Bei Wang", "title": "Mapper Interactive: A Scalable, Extendable, and Interactive Toolbox for\n  the Visual Exploration of High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mapper algorithm is a popular tool from topological data analysis for\nextracting topological summaries of high-dimensional datasets. In this paper,\nwe present Mapper Interactive, a web-based framework for the interactive\nanalysis and visualization of high-dimensional point cloud data. It implements\nthe mapper algorithm in an interactive, scalable, and easily extendable way,\nthus supporting practical data analysis. In particular, its command-line API\ncan compute mapper graphs for 1 million points of 256 dimensions in about 3\nminutes (4 times faster than the vanilla implementation). Its visual interface\nallows on-the-fly computation and manipulation of the mapper graph based on\nuser-specified parameters and supports the addition of new analysis modules\nwith a few lines of code. Mapper Interactive makes the mapper algorithm\naccessible to nonspecialists and accelerates topological analytics workflows.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:55:59 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 06:03:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhou", "Youjia", ""], ["Chalapathi", "Nithin", ""], ["Rathore", "Archit", ""], ["Zhao", "Yaodong", ""], ["Wang", "Bei", ""]]}, {"id": "2011.03277", "submitter": "Samuli Laine", "authors": "Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko\n  Lehtinen, Timo Aila", "title": "Modular Primitives for High-Performance Differentiable Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modular differentiable renderer design that yields performance\nsuperior to previous methods by leveraging existing, highly optimized hardware\ngraphics pipelines. Our design supports all crucial operations in a modern\ngraphics pipeline: rasterizing large numbers of triangles, attribute\ninterpolation, filtered texture lookups, as well as user-programmable shading\nand geometry processing, all in high resolutions. Our modular primitives allow\ncustom, high-performance graphics pipelines to be built directly within\nautomatic differentiation frameworks such as PyTorch or TensorFlow. As a\nmotivating application, we formulate facial performance capture as an inverse\nrendering problem and show that it can be solved efficiently using our tools.\nOur results indicate that this simple and straightforward approach achieves\nexcellent geometric correspondence between rendered results and reference\nimagery.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:48:43 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Laine", "Samuli", ""], ["Hellsten", "Janne", ""], ["Karras", "Tero", ""], ["Seol", "Yeongho", ""], ["Lehtinen", "Jaakko", ""], ["Aila", "Timo", ""]]}, {"id": "2011.03596", "submitter": "Iulia-Cristina Stanica", "authors": "Iulia-Cristina Stanica, Florica Moldoveanu, Giovanni-Paul Portelli,\n  Maria-Iuliana Dascalu, Alin Moldoveanu and Mariana Georgiana Ristea", "title": "Flexible Virtual Reality System for Neurorehabilitation and Quality of\n  Life Improvement", "comments": "47 pages, 20 figures, 17 tables (including annexes), part of the MDPI\n  Sesnsors \"Special Issue Smart Sensors and Measurements Methods for Quality of\n  Life and Ambient Assisted Living\"", "journal-ref": "Sensors 2020 Volume 20 Issue 21 10.3390/s20216045 Sensors Volume\n  20 Issue 21 10.3390/s20216045 MDPI Sensors, 20(21)", "doi": "10.3390/s20216045", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As life expectancy is mostly increasing, the incidence of many neurological\ndisorders is also constantly growing. For improving the physical functions\naffected by a neurological disorder, rehabilitation procedures are mandatory,\nand they must be performed regularly. Unfortunately, neurorehabilitation\nprocedures have disadvantages in terms of costs, accessibility and a lack of\ntherapists. This paper presents Immersive Neurorehabilitation Exercises Using\nVirtual Reality (INREX-VR), our innovative immersive neurorehabilitation system\nusing virtual reality. The system is based on a thorough research methodology\nand is able to capture real-time user movements and evaluate joint mobility for\nboth upper and lower limbs, record training sessions and save electromyography\ndata. The use of the first-person perspective increases immersion, and the\njoint range of motion is calculated with the help of both the HTC Vive system\nand inverse kinematics principles applied on skeleton rigs. Tutorial exercises\nare demonstrated by a virtual therapist, as they were recorded with real-life\nphysicians, and sessions can be monitored and configured through tele-medicine.\nComplex movements are practiced in gamified settings, encouraging\nself-improvement and competition. Finally, we proposed a training plan and\npreliminary tests which show promising results in terms of accuracy and user\nfeedback. As future developments, we plan to improve the system's accuracy and\ninvestigate a wireless alternative based on neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:04:00 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Stanica", "Iulia-Cristina", ""], ["Moldoveanu", "Florica", ""], ["Portelli", "Giovanni-Paul", ""], ["Dascalu", "Maria-Iuliana", ""], ["Moldoveanu", "Alin", ""], ["Ristea", "Mariana Georgiana", ""]]}, {"id": "2011.03618", "submitter": "Maksim Sorokin", "authors": "Maks Sorokin, Wenhao Yu, Sehoon Ha, C. Karen Liu", "title": "Learning Human Search Behavior from Egocentric Visual Inputs", "comments": "(preprint) submitted to EUROGRAPHICS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Looking for things\" is a mundane but critical task we repeatedly carry on in\nour daily life. We introduce a method to develop a human character capable of\nsearching for a randomly located target object in a detailed 3D scene using its\nlocomotion capability and egocentric vision perception represented as RGBD\nimages. By depriving the privileged 3D information from the human character, it\nis forced to move and look around simultaneously to account for the restricted\nsensing capability, resulting in natural navigation and search behaviors. Our\nmethod consists of two components: 1) a search control policy based on an\nabstract character model, and 2) an online replanning control module for\nsynthesizing detailed kinematic motion based on the trajectories planned by the\nsearch policy. We demonstrate that the combined techniques enable the character\nto effectively find often occluded household items in indoor environments. The\nsame search policy can be applied to different full-body characters without the\nneed for retraining. We evaluate our method quantitatively by testing it on\nrandomly generated scenarios. Our work is a first step toward creating\nintelligent virtual agents with humanlike behaviors driven by onboard sensors,\npaving the road toward future robotic applications.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:31:20 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Sorokin", "Maks", ""], ["Yu", "Wenhao", ""], ["Ha", "Sehoon", ""], ["Liu", "C. Karen", ""]]}, {"id": "2011.03630", "submitter": "Philipp Ladwig", "authors": "Philipp Ladwig, Alexander Pech, Ralf D\\\"orner and Christian Geiger", "title": "Unmasking Communication Partners: A Low-Cost AI Solution for Digitally\n  Removing Head-Mounted Displays in VR-Based Telepresence", "comments": "9 pages, IEEE 3rd International Conference on Artificial Intelligence\n  & Virtual Reality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face-to-face conversation in Virtual Reality (VR) is a challenge when\nparticipants wear head-mounted displays (HMD). A significant portion of a\nparticipant's face is hidden and facial expressions are difficult to perceive.\nPast research has shown that high-fidelity face reconstruction with personal\navatars in VR is possible under laboratory conditions with high-cost hardware.\nIn this paper, we propose one of the first low-cost systems for this task which\nuses only open source, free software and affordable hardware. Our approach is\nto track the user's face underneath the HMD utilizing a Convolutional Neural\nNetwork (CNN) and generate corresponding expressions with Generative\nAdversarial Networks (GAN) for producing RGBD images of the person's face. We\nuse commodity hardware with low-cost extensions such as 3D-printed mounts and\nminiature cameras. Our approach learns end-to-end without manual intervention,\nruns in real time, and can be trained and executed on an ordinary gaming\ncomputer. We report evaluation results showing that our low-cost system does\nnot achieve the same fidelity of research prototypes using high-end hardware\nand closed source software, but it is capable of creating individual facial\navatars with person-specific characteristics in movements and expressions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:17:12 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ladwig", "Philipp", ""], ["Pech", "Alexander", ""], ["D\u00f6rner", "Ralf", ""], ["Geiger", "Christian", ""]]}, {"id": "2011.03632", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Zherong Pan, Dinesh Manocha", "title": "LCollision: Fast Generation of Collision-Free Human Poses using Learned\n  Non-Penetration Constraints", "comments": "AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LCollision, a learning-based method that synthesizes\ncollision-free 3D human poses. At the crux of our approach is a novel deep\narchitecture that simultaneously decodes new human poses from the latent space\nand predicts colliding body parts. These two components of our architecture are\nused as the objective function and surrogate hard constraints in a constrained\noptimization for collision-free human pose generation. A novel aspect of our\napproach is the use of a bilevel autoencoder that decomposes whole-body\ncollisions into groups of collisions between localized body parts. By solving\nthe constrained optimizations, we show that a significant amount of collision\nartifacts can be resolved. Furthermore, in a large test set of $2.5\\times 10^6$\nrandomized poses from SCAPE, our architecture achieves a collision-prediction\naccuracy of $94.1\\%$ with $80\\times$ speedup over exact collision detection\nalgorithms. To the best of our knowledge, LCollision is the first approach that\naccelerates collision detection and resolves penetrations using a neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 23:32:00 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 21:00:33 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 22:19:20 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 06:21:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Tan", "Qingyang", ""], ["Pan", "Zherong", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2011.03675", "submitter": "Dusanka Boskovic", "authors": "Selma Rizvic, Dusanka Boskovic, Vensada Okanovic, Sanda Sljivo, Merima\n  Zukic", "title": "Interactive digital storytelling: bringing cultural heritage in a\n  classroom", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Interactive digital storytelling becomes a popular choice for information\npresentation in many fields. Its application spans from media industry and\nbusiness information visualization, through digital cultural heritage, serious\ngames, education, to contemporary theater and visual arts. The benefits of this\nform of multimedia presentation in education are generally recognized, and\nseveral studies exploring and supporting the opinion are conducted. In addition\nto discussing the benefits, we wanted to address the challenges in introducing\ninteractive digital storytelling and serious games in the classroom. The\nchallenge of inherent ambiguity of edutainment, due to opposing features of\neducation and entertainment is augmented with different viewpoints of\nmultidisciplinary team members. We specifically address the opposing views on\nartistic liberty, at one side, and technical constraints and historic facts, on\nthe other side. In this paper we present the first findings related to these\nquestions and to initiate furthering discussions in this area.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 19:28:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Rizvic", "Selma", ""], ["Boskovic", "Dusanka", ""], ["Okanovic", "Vensada", ""], ["Sljivo", "Sanda", ""], ["Zukic", "Merima", ""]]}, {"id": "2011.04055", "submitter": "Giuseppe Patan\\`e", "authors": "Giuseppe Patan\\`e", "title": "Fourier-based and Rational Graph Filters for Spectral Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data are represented as graphs in a wide range of applications, such as\nComputer Vision (e.g., images) and Graphics (e.g., 3D meshes), network analysis\n(e.g., social networks), and bio-informatics (e.g., molecules). In this\ncontext, our overall goal is the definition of novel Fourier-based and graph\nfilters induced by rational polynomials for graph processing, which generalise\npolynomial filters and the Fourier transform to non-Euclidean domains. For the\nefficient evaluation of discrete spectral Fourier-based and wavelet operators,\nwe introduce a spectrum-free approach, which requires the solution of a small\nset of sparse, symmetric, well-conditioned linear systems and is oblivious of\nthe evaluation of the Laplacian or kernel spectrum. Approximating arbitrary\ngraph filters with rational polynomials provides a more accurate and\nnumerically stable alternative with respect to polynomials. To achieve these\ngoals, we also study the link between spectral operators, wavelets, and\nfiltered convolution with integral operators induced by spectral kernels.\nAccording to our tests, main advantages of the proposed approach are (i) its\ngenerality with respect to the input data (e.g., graphs, 3D shapes),\napplications (e.g., signal reconstruction and smoothing, shape correspondence),\nand filters (e.g., polynomial, rational polynomial), and (ii) a spectrum-free\ncomputation with a generally low computational cost and storage overhead.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 19:02:52 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 21:24:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Patan\u00e8", "Giuseppe", ""]]}, {"id": "2011.04612", "submitter": "Miaojing Shi", "authors": "Yanlin Qian and Miaojing Shi and Joni-Kristian K\\\"am\\\"ar\\\"ainen and\n  Jiri Matas", "title": "Fast Fourier Intrinsic Network", "comments": "WACV 2021 - camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of decomposing an image into albedo and shading. We\npropose the Fast Fourier Intrinsic Network, FFI-Net in short, that operates in\nthe spectral domain, splitting the input into several spectral bands. Weights\nin FFI-Net are optimized in the spectral domain, allowing faster convergence to\na lower error. FFI-Net is lightweight and does not need auxiliary networks for\ntraining. The network is trained end-to-end with a novel spectral loss which\nmeasures the global distance between the network prediction and corresponding\nground truth. FFI-Net achieves state-of-the-art performance on MPI-Sintel, MIT\nIntrinsic, and IIW datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 18:14:39 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Qian", "Yanlin", ""], ["Shi", "Miaojing", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Matas", "Jiri", ""]]}, {"id": "2011.04728", "submitter": "Dishant Parikh", "authors": "Dishant Parikh, Shambhavi Aggarwal", "title": "Similarity-Based Clustering for Enhancing Image Classification\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are at the center of best in class computer vision\napplications for a wide assortment of undertakings. Since 2014, profound amount\nof work began to make better convolutional architectures, yielding generous\nadditions in different benchmarks. Albeit expanded model size and computational\ncost will, in general, mean prompt quality increases for most undertakings but,\nthe architectures now need to have some additional information to increase the\nperformance. We show empirical evidence that with the amalgamation of\ncontent-based image similarity and deep learning models, we can provide the\nflow of information which can be used in making clustered learning possible. We\nshow how parallel training of sub-dataset clusters not only reduces the cost of\ncomputation but also increases the benchmark accuracies by 5-11 percent.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:03:28 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Parikh", "Dishant", ""], ["Aggarwal", "Shambhavi", ""]]}, {"id": "2011.04910", "submitter": "Kun Wang", "authors": "Kun Wang, Mridul Aanjaneya and Kostas Bekris", "title": "Spring-Rod System Identification via Differentiable Physics Engine", "comments": "Workshop on Differentiable Vision, Graphics, and Physics in Machine\n  Learning at NeurIPS 2020. arXiv admin note: substantial text overlap with\n  arXiv:2004.13859", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel differentiable physics engine for system identification of\ncomplex spring-rod assemblies. Unlike black-box data-driven methods for\nlearning the evolution of a dynamical system \\emph{and} its parameters, we\nmodularize the design of our engine using a discrete form of the governing\nequations of motion, similar to a traditional physics engine. We further reduce\nthe dimension from 3D to 1D for each module, which allows efficient learning of\nsystem parameters using linear regression. The regression parameters correspond\nto physical quantities, such as spring stiffness or the mass of the rod, making\nthe pipeline explainable. The approach significantly reduces the amount of\ntraining data required, and also avoids iterative identification of data\nsampling and model training. We compare the performance of the proposed engine\nwith previous solutions, and demonstrate its efficacy on tensegrity systems,\nsuch as NASA's icosahedron.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 04:36:22 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Wang", "Kun", ""], ["Aanjaneya", "Mridul", ""], ["Bekris", "Kostas", ""]]}, {"id": "2011.04929", "submitter": "Kun Wang", "authors": "Kun Wang, Mridul Aanjaneya and Kostas Bekris", "title": "Sim2Sim Evaluation of a Novel Data-Efficient Differentiable Physics\n  Engine for Tensegrity Robots", "comments": "Accepted to IROS2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning policies in simulation is promising for reducing human effort when\ntraining robot controllers. This is especially true for soft robots that are\nmore adaptive and safe but also more difficult to accurately model and control.\nThe sim2real gap is the main barrier to successfully transfer policies from\nsimulation to a real robot. System identification can be applied to reduce this\ngap but traditional identification methods require a lot of manual tuning.\nData-driven alternatives can tune dynamical models directly from data but are\noften data hungry, which also incorporates human effort in collecting data.\nThis work proposes a data-driven, end-to-end differentiable simulator focused\non the exciting but challenging domain of tensegrity robots. To the best of the\nauthors' knowledge, this is the first differentiable physics engine for\ntensegrity robots that supports cable, contact, and actuation modeling. The aim\nis to develop a reasonably simplified, data-driven simulation, which can learn\napproximate dynamics with limited ground truth data. The dynamics must be\naccurate enough to generate policies that can be transferred back to the\nground-truth system. As a first step in this direction, the current work\ndemonstrates sim2sim transfer, where the unknown physical model of MuJoCo acts\nas a ground truth system. Two different tensegrity robots are used for\nevaluation and learning of locomotion policies, a 6-bar and a 3-bar tensegrity.\nThe results indicate that only 0.25\\% of ground truth data are needed to train\na policy that works on the ground truth system when the differentiable engine\nis used for training against training the policy directly on the ground truth\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:19:54 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 23:08:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Kun", ""], ["Aanjaneya", "Mridul", ""], ["Bekris", "Kostas", ""]]}, {"id": "2011.05276", "submitter": "Piotr Beben", "authors": "Piotr Beben", "title": "Topology of Frame Field Design for Hex Meshing", "comments": "More citations added; last section updated; new material added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade frame fields have emerged as a promising approach for\ngenerating hexahedral meshes for CFD and CAE applications. One important\nproblem asks for construction of a boundary-aligned frame field with prescribed\nsingularity constraints over a volume that corresponds to a valid hexahedral\nmesh. We give a necessary and sufficient condition in terms of solutions to a\nsystem of monomial equations with variables in the binary octahedral group when\na boundary frame field and singularity graph have been fixed. This is phrased\nwith respect to general $CW$-decompositions of the volume, which allows some\nflexibility in simplifying these systems. Along the way we look at frame field\ndesign from an algebraic topological perspective, proving various results, some\nknown, some new.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:51:40 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 18:57:25 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 14:55:14 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 17:58:36 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 18:51:57 GMT"}, {"version": "v6", "created": "Thu, 3 Dec 2020 17:56:24 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Beben", "Piotr", ""]]}, {"id": "2011.05538", "submitter": "Shiguang Liu Prof.", "authors": "Shiguang Liu, Dinesh Manocha", "title": "Sound Synthesis, Propagation, and Rendering: A Survey", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sound, as a crucial sensory channel, plays a vital role in improving the\nreality and immersiveness of a virtual environment, following only vision in\nimportance. Sound can provide important clues such as sound directionality and\nspatial size. This paper gives a broad overview of research works on sound\nsimulation in virtual reality, games, multimedia, computer-aided design. We\nfirst survey various sound synthesis methods, including harmonic synthesis,\ntexture synthesis, spectral analysis, and physics-based synthesis. Then, we\nsummarize popular sound propagation techniques, namely wave-based methods,\ngeometric-based methods, and hybrid methods. Next, the sound rendering methods\nare reviewed. We further demonstrate the latest deep learning based sound\nsimulation approaches. Finally, we point to some future directions of this\nfield. To the best of our knowledge, this is the first attempt to provide a\ncomprehensive summary of sound research in the field of computer graphics.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 04:08:38 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 14:04:36 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 10:25:51 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 09:27:55 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 03:11:49 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Shiguang", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2011.05550", "submitter": "Abhishek Madan", "authors": "Abhishek Madan, Alec Jacobson, David I.W. Levin", "title": "Diffusion Structures for Architectural Stripe Pattern Generation", "comments": "10 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Diffusion Structures, a family of resilient shell structures from\nthe eigenfunctions of a pair of novel diffusion operators. This approach is\nbased on Michell's theorem but avoids expensive non-linear optimization with\ncomputation that amounts to constructing and solving two generalized eigenvalue\nproblems to generate two sets of stripe patterns. This structure family can be\ngenerated quickly, and navigated in real-time using a small number of tuneable\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 05:18:53 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Madan", "Abhishek", ""], ["Jacobson", "Alec", ""], ["Levin", "David I. W.", ""]]}, {"id": "2011.05689", "submitter": "Renata Georgia Raidou", "authors": "Renata G. Raidou, M. Eduard Gr\\\"oller, Hsiang-Yun Wu", "title": "Slice and Dice: A Physicalization Workflow for Anatomical Edutainment", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.14173", "report-no": null, "categories": "cs.GR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  During the last decades, anatomy has become an interesting topic in\neducation---even for laymen or schoolchildren. As medical imaging techniques\nbecome increasingly sophisticated, virtual anatomical education applications\nhave emerged. Still, anatomical models are often preferred, as they facilitate\n3D localization of anatomical structures. Recently, data physicalizations\n(i.e., physical visualizations) have proven to be effective and\nengaging---sometimes, even more than their virtual counterparts. So far,\nmedical data physicalizations involve mainly 3D printing, which is still\nexpensive and cumbersome. We investigate alternative forms of physicalizations,\nwhich use readily available technologies (home printers) and inexpensive\nmaterials (paper or semi-transparent films) to generate crafts for anatomical\nedutainment. To the best of our knowledge, this is the first computer-generated\ncrafting approach within an anatomical edutainment context. Our approach\nfollows a cost-effective, simple, and easy-to-employ workflow, resulting in\nassemblable data sculptures (i.e., semi-transparent sliceforms). It primarily\nsupports volumetric data (such as CT or MRI), but mesh data can also be\nimported. An octree slices the imported volume and an optimization step\nsimplifies the slice configuration, proposing the optimal order for easy\nassembly. A packing algorithm places the resulting slices with their labels,\nannotations, and assembly instructions on a paper or transparent film of\nuser-selected size, to be printed, assembled into a sliceform, and explored. We\nconducted two user studies to assess our approach, demonstrating that it is an\ninitial positive step towards the successful creation of interactive and\nengaging anatomical physicalizations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 10:51:55 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Raidou", "Renata G.", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Wu", "Hsiang-Yun", ""]]}, {"id": "2011.06822", "submitter": "Raghav Brahmadesam Venkataramaiyer", "authors": "Raghav B. Venkataramaiyer, Abhishek Joshi, Saisha Narang and Vinay P.\n  Namboodiri", "title": "SHAD3S: A model to Sketch, Shade and Shadow", "comments": "10 pages, 11 figures, 2 tables Accepted to WACV 2021. Project Page:\n  https://bvraghav.com/shad3s/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hatching is a common method used by artists to accentuate the third dimension\nof a sketch, and to illuminate the scene. Our system SHAD3S attempts to compete\nwith a human at hatching generic three-dimensional (3D) shapes, and also tries\nto assist her in a form exploration exercise. The novelty of our approach lies\nin the fact that we make no assumptions about the input other than that it\nrepresents a 3D shape, and yet, given a contextual information of illumination\nand texture, we synthesise an accurate hatch pattern over the sketch, without\naccess to 3D or pseudo 3D. In the process, we contribute towards a) a cheap yet\neffective method to synthesise a sufficiently large high fidelity dataset,\npertinent to task; b) creating a pipeline with conditional generative\nadversarial network (CGAN); and c) creating an interactive utility with GIMP,\nthat is a tool for artists to engage with automated hatching or a\nform-exploration exercise. User evaluation of the tool suggests that the model\nperformance does generalise satisfactorily over diverse input, both in terms of\nstyle as well as shape. A simple comparison of inception scores suggest that\nthe generated distribution is as diverse as the ground truth.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:25:46 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 10:41:46 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Venkataramaiyer", "Raghav B.", ""], ["Joshi", "Abhishek", ""], ["Narang", "Saisha", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "2011.07374", "submitter": "Jixuan Zhi", "authors": "Jixuan Zhi, Lap-Fai Yu and Jyh-Ming Lien", "title": "Designing Human-Robot Coexistence Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When the human-robot interactions become ubiquitous, the environment\nsurrounding these interactions will have significant impact on the safety and\ncomfort of the human and the effectiveness and efficiency of the robot.\nAlthough most robots are designed to work in the spaces created for humans,\nmany environments, such as living rooms and offices, can be and should be\nredesigned to enhance and improve human-robot collaboration and interactions.\nThis work uses autonomous wheelchair as an example and investigates the\ncomputational design in the human-robot coexistence spaces. Given the room size\nand the objects $O$ in the room, the proposed framework computes the optimal\nlayouts of $O$ that satisfy both human preferences and navigation constraints\nof the wheelchair. The key enabling technique is a motion planner that can\nefficiently evaluate hundreds of similar motion planning problems. Our\nimplementation shows that the proposed framework can produce a design around\nthree to five minutes on average comparing to 10 to 20 minutes without the\nproposed motion planner. Our results also show that the proposed method\nproduces reasonable designs even for tight spaces and for users with different\npreferences.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 19:32:11 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhi", "Jixuan", ""], ["Yu", "Lap-Fai", ""], ["Lien", "Jyh-Ming", ""]]}, {"id": "2011.07532", "submitter": "Michael Aupetit", "authors": "Michael Aupetit", "title": "Aquanims -- Area-Preserving Animated Transitions based on a Hydraulic\n  Metaphor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose \"Aquanims\" as new design metaphors for animated transitions that\npreserve displayed areas during the transformation. As liquids are\nincompressible fluids, we use a hydraulic metaphor to convey the sense of area\npreservation during animated transitions. We study the design space of Aquanims\nfor rectangle-based charts.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 14:00:27 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Aupetit", "Michael", ""]]}, {"id": "2011.08232", "submitter": "Soroosh Tayebi Arasteh", "authors": "Soroosh Tayebi Arasteh, Adam Kalisz", "title": "Conversion Between Cubic Bezier Curves and Catmull-Rom Splines", "comments": "9 pages, 4 figures, submitted to SN Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.AG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Splines are one of the main methods of mathematically representing\ncomplicated shapes, which have become the primary technique in the fields of\nComputer Graphics (CG) and Computer-Aided Geometric Design (CAGD) for modeling\ncomplex surfaces. Among all, B\\'ezier and Catmull-Rom splines are the most\ncommon in the sub-fields of engineering. In this paper, we focus on conversion\nbetween cubic B\\'ezier and Catmull-Rom curve segments, rather than going\nthrough their properties. By deriving the conversion equations, we aim at\nconverting the original set of the control points of either of the Catmull-Rom\nor B\\'ezier cubic curves to a new set of control points, which corresponds to\napproximately the same shape as the original curve, when considered as the set\nof the control points of the other curve. Due to providing simple linear\ntransformations of control points, the method is very simple, efficient, and\neasy to implement, which is further validated in this paper using some\nnumerical and visual examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 19:22:16 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 20:01:30 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Arasteh", "Soroosh Tayebi", ""], ["Kalisz", "Adam", ""]]}, {"id": "2011.08559", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Normalized Weighting Schemes for Image Interpolation Algorithms", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents and evaluates four weighting schemes for image\ninterpolation algorithms. The first scheme is based on the normalized area of\nthe circle, whose diameter is equal to the minimum side of a tetragon. The\nsecond scheme is based on the normalized area of the circle, whose radius is\nequal to the hypotenuse. The third scheme is based on the normalized area of\nthe triangle, whose base and height are equal to the hypotenuse and virtual\npixel length, respectively. The fourth weighting scheme is based on the\nnormalized area of the circle, whose radius is equal to the virtual pixel\nlength-based hypotenuse. Experiments demonstrated debatable algorithm\nperformances and the need for further research.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:47:50 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 17:57:40 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2011.08676", "submitter": "Talha Bin Masood", "authors": "Wito Engelke, Talha Bin Masood, Jakob Beran, Rodrigo Caballero and\n  Ingrid Hotz", "title": "Topology-Based Feature Design and Tracking for Multi-Center Cyclones", "comments": "13 pages, 9 figures, 8th workshop on Topological Methods in Data\n  Analysis and Visualization (TopoInVis 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a concept to design, track, and compare\napplication-specific feature definitions expressed as sets of critical points.\nOur work has been inspired by the observation that in many applications a large\nvariety of different feature definitions for the same concept are used. Often,\nthese definitions compete with each other and it is unclear which definition\nshould be used in which context. A prominent example is the definition of\ncyclones in climate research. Despite the differences, frequently these feature\ndefinitions can be related to topological concepts.\n  In our approach, we provide a cyclone tracking framework that supports\ninteractive feature definition and comparison based on a precomputed tracking\ngraph that stores all extremal points as well as their temporal correspondents.\nThe framework combines a set of independent building blocks: critical point\nextraction, critical point tracking, feature definition, and track exploration.\nOne of the major advantages of such an approach is the flexibility it provides,\nthat is, each block is exchangeable. Moreover, it also enables us to perform\nthe most expensive analysis, the construction of a full tracking graph, as a\nprepossessing step, while keeping the feature definition interactive. Different\nfeature definitions can be explored and compared interactively based on this\ntracking graph. Features are specified by rules for grouping critical points,\nwhile feature tracking corresponds to filtering and querying the full tracking\ngraph by specific requests. We demonstrate this method for cyclone\nidentification and tracking in the context of climate research.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 17:39:27 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Engelke", "Wito", ""], ["Masood", "Talha Bin", ""], ["Beran", "Jakob", ""], ["Caballero", "Rodrigo", ""], ["Hotz", "Ingrid", ""]]}, {"id": "2011.08697", "submitter": "Hanqi Guo", "authors": "Hanqi Guo, David Lenz, Jiayi Xu, Xin Liang, Wenbin He, Iulian R.\n  Grindeanu, Han-Wei Shen, Tom Peterka, Todd Munson, Ian Foster", "title": "FTK: A Simplicial Spacetime Meshing Framework for Robust and Scalable\n  Feature Tracking", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2021.3073399", "report-no": "ANL/MCS-P9423-1120", "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Feature Tracking Kit (FTK), a framework that simplifies,\nscales, and delivers various feature-tracking algorithms for scientific data.\nThe key of FTK is our high-dimensional simplicial meshing scheme that\ngeneralizes both regular and unstructured spatial meshes to spacetime while\ntessellating spacetime mesh elements into simplices. The benefits of using\nsimplicial spacetime meshes include (1) reducing ambiguity cases for feature\nextraction and tracking, (2) simplifying the handling of degeneracies using\nsymbolic perturbations, and (3) enabling scalable and parallel processing. The\nuse of simplicial spacetime meshing simplifies and improves the implementation\nof several feature-tracking algorithms for critical points, quantum vortices,\nand isosurfaces. As a software framework, FTK provides end users with\nVTK/ParaView filters, Python bindings, a command line interface, and\nprogramming interfaces for feature-tracking applications. We demonstrate use\ncases as well as scalability studies through both synthetic data and scientific\napplications including Tokamak, fluid dynamics, and superconductivity\nsimulations. We also conduct end-to-end performance studies on the Summit\nsupercomputer. FTK is open-sourced under the MIT license:\nhttps://github.com/hguo/ftk\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:24:17 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 03:41:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Guo", "Hanqi", ""], ["Lenz", "David", ""], ["Xu", "Jiayi", ""], ["Liang", "Xin", ""], ["He", "Wenbin", ""], ["Grindeanu", "Iulian R.", ""], ["Shen", "Han-Wei", ""], ["Peterka", "Tom", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""]]}, {"id": "2011.08826", "submitter": "Udaranga Wickramasinghe", "authors": "Udaranga Wickramasinghe and Graham Knott and Pascal Fua", "title": "Deep Active Surface Models", "comments": "11 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Surface Models have a long history of being useful to model complex 3D\nsurfaces but only Active Contours have been used in conjunction with deep\nnetworks, and then only to produce the data term as well as meta-parameter maps\ncontrolling them. In this paper, we advocate a much tighter integration. We\nintroduce layers that implement them that can be integrated seamlessly into\nGraph Convolutional Networks to enforce sophisticated smoothness priors at an\nacceptable computational cost. We will show that the resulting Deep Active\nSurface Models outperform equivalent architectures that use traditional\nregularization loss terms to impose smoothness priors for 3D surface\nreconstruction from 2D images and for 3D volume segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 18:48:28 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 18:25:58 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 23:19:36 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:31:20 GMT"}, {"version": "v5", "created": "Mon, 7 Jun 2021 22:19:00 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wickramasinghe", "Udaranga", ""], ["Knott", "Graham", ""], ["Fua", "Pascal", ""]]}, {"id": "2011.09201", "submitter": "Martin Pichlmair", "authors": "Martin Pichlmair and Mads Johansen", "title": "Designing Game Feel. A Survey", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TG.2021.3072241", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game feel design is the intentional design of the affective impact of\nmoment-to-moment interaction with games. In this paper we survey academic\nresearch and publications by practitioners to give a complete overview of the\nstate of research concerning this aspect of game design, including context from\nrelated areas. We analysed over 200 sources and categorised their content\naccording to the design purpose presented. This resulted in three different\ndomains of intended player experiences: physicality, amplification, and\nsupport. In these domains, the act of polishing that determines game feel,\ntakes the shape of tuning, juicing, and streamlining respectively. Tuning the\nphysicality of game objects creates cohesion, predictability, and the resulting\nmovement informs many other design aspects. Juicing is the act of polishing\namplification and it results in empowerment and provides clarity of feedback by\ncommunicating the importance of game events. Streamlining allows a game to act\non the intention of the player, supporting the execution of actions in the\ngame. These three design intents are the main means through which designers\ncontrol minute details of interactivity and inform the player's reaction. This\nframework and its nuanced vocabulary can lead to an understanding of game feel\nthat is shared between practitioners and researchers as highlighted in the\nconcluding future research section.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:39:18 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pichlmair", "Martin", ""], ["Johansen", "Mads", ""]]}, {"id": "2011.09657", "submitter": "Marcelo Kallmann Prof", "authors": "Yusuke Niiro and Marcelo Kallmann", "title": "Assembling a Pipeline for 3D Face Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a pipeline built with open source tools for\ninterpolating 3D facial expressions taken from images. The presented approach\nallows anyone to create 3D face animations from 2 input photos: one from the\nstart face expression, and the other from the final face expression. Given the\ninput photos, corresponding 3D face models are constructed and texture-mapped\nusing the photos as textures aligned with facial features. Animations are then\ngenerated by morphing the models by interpolation of the geometries and\ntextures of the models. This work was performed as a MS project at the\nUniversity of California, Merced.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 05:19:25 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Niiro", "Yusuke", ""], ["Kallmann", "Marcelo", ""]]}, {"id": "2011.10118", "submitter": "Rogerio Bonatti", "authors": "Rogerio Bonatti, Arthur Bucker, Sebastian Scherer, Mustafa Mukadam and\n  Jessica Hodgins", "title": "Batteries, camera, action! Learning a semantic control space for\n  expressive robot cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aerial vehicles are revolutionizing the way film-makers can capture shots of\nactors by composing novel aerial and dynamic viewpoints. However, despite great\nadvancements in autonomous flight technology, generating expressive camera\nbehaviors is still a challenge and requires non-technical users to edit a large\nnumber of unintuitive control parameters. In this work, we develop a\ndata-driven framework that enables editing of these complex camera positioning\nparameters in a semantic space (e.g. calm, enjoyable, establishing). First, we\ngenerate a database of video clips with a diverse range of shots in a\nphoto-realistic simulator, and use hundreds of participants in a crowd-sourcing\nframework to obtain scores for a set of semantic descriptors for each clip.\nNext, we analyze correlations between descriptors and build a semantic control\nspace based on cinematography guidelines and human perception studies. Finally,\nwe learn a generative model that can map a set of desired semantic video\ndescriptors into low-level camera trajectory parameters. We evaluate our system\nby demonstrating that our model successfully generates shots that are rated by\nparticipants as having the expected degrees of expression for each descriptor.\nWe also show that our models generalize to different scenes in both simulation\nand real-world experiments. Data and video found at:\nhttps://sites.google.com/view/robotcam.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:56:53 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:21 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bonatti", "Rogerio", ""], ["Bucker", "Arthur", ""], ["Scherer", "Sebastian", ""], ["Mukadam", "Mustafa", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2011.10232", "submitter": "Yusuke Monno", "authors": "Takeru Suda, Masayuki Tanaka, Yusuke Monno, Masatoshi Okutomi", "title": "Deep Snapshot HDR Imaging Using Multi-Exposure Color Filter Array", "comments": "Accepted at ACCV2020 (Oral). Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/DSHDR/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a deep snapshot high dynamic range (HDR) imaging\nframework that can effectively reconstruct an HDR image from the RAW data\ncaptured using a multi-exposure color filter array (ME-CFA), which consists of\na mosaic pattern of RGB filters with different exposure levels. To effectively\nlearn the HDR image reconstruction network, we introduce the idea of luminance\nnormalization that simultaneously enables effective loss computation and input\ndata normalization by considering relative local contrasts in the\n\"normalized-by-luminance\" HDR domain. This idea makes it possible to equally\nhandle the errors in both bright and dark areas regardless of absolute\nluminance levels, which significantly improves the visual image quality in a\ntone-mapped domain. Experimental results using two public HDR image datasets\ndemonstrate that our framework outperforms other snapshot methods and produces\nhigh-quality HDR images with fewer visual artifacts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 06:31:37 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Suda", "Takeru", ""], ["Tanaka", "Masayuki", ""], ["Monno", "Yusuke", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "2011.10257", "submitter": "Nils Thuerey", "authors": "Kiwon Um, Xiangyu Hu, Nils Thuerey", "title": "Perceptual Evaluation of Liquid Simulation Methods", "comments": "Details at: http://ge.in.tum.de/publications/2017-sig-um/", "journal-ref": null, "doi": "10.1145/3072959.3073633", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel framework to evaluate fluid simulation methods\nbased on crowd-sourced user studies in order to robustly gather large numbers\nof opinions. The key idea for a robust and reliable evaluation is to use a\nreference video from a carefully selected real-world setup in the user study.\nBy conducting a series of controlled user studies and comparing their\nevaluation results, we observe various factors that affect the perceptual\nevaluation. Our data show that the availability of a reference video makes the\nevaluation consistent. We introduce this approach for computing scores of\nsimulation methods as visual accuracy metric. As an application of the proposed\nframework, a variety of popular simulation methods are evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:08:00 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Um", "Kiwon", ""], ["Hu", "Xiangyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "2011.10284", "submitter": "Nils Thuerey", "authors": "Marie-Lena Eckert, Kiwon Um, Nils Thuerey", "title": "ScalarFlow: A Large-Scale Volumetric Data Set of Real-world Scalar\n  Transport Flows for Computer Animation and Machine Learning", "comments": "Details and data at:\n  https://ge.in.tum.de/publications/2019-scalarflow-eckert/", "journal-ref": null, "doi": "10.1145/3355089.3356545", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ScalarFlow, a first large-scale data set of\nreconstructions of real-world smoke plumes. We additionally propose a framework\nfor accurate physics-based reconstructions from a small number of video\nstreams. Central components of our algorithm are a novel estimation of unseen\ninflow regions and an efficient regularization scheme. Our data set includes a\nlarge number of complex and natural buoyancy-driven flows. The flows transition\nto turbulent flows and contain observable scalar transport processes. As such,\nthe ScalarFlow data set is tailored towards computer graphics, vision, and\nlearning applications. The published data set will contain volumetric\nreconstructions of velocity and density, input image sequences, together with\ncalibration data, code, and instructions how to recreate the commodity hardware\ncapture setup. We further demonstrate one of the many potential application\nareas: a first perceptual evaluation study, which reveals that the complexity\nof the captured flows requires a huge simulation resolution for regular solvers\nin order to recreate at least parts of the natural complexity contained in the\ncaptured data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 08:55:00 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Eckert", "Marie-Lena", ""], ["Um", "Kiwon", ""], ["Thuerey", "Nils", ""]]}, {"id": "2011.10348", "submitter": "Heajung Min", "authors": "Heajung Min, Kyung Min Han and Young J. Kim", "title": "Accelerating Probabilistic Volumetric Mapping using Ray-Tracing Graphics\n  Hardware", "comments": "Submitted IEEE International Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic volumetric mapping (PVM) represents a 3D environmental map for\nan autonomous robotic navigational task. A popular implementation such as\nOctomap is widely used in the robotics community for such a purpose. The\nOctomap relies on octree to represent a PVM and its main bottleneck lies in\nmassive ray-shooting to determine the occupancy of the underlying volumetric\nvoxel grids. In this paper, we propose GPU-based ray shooting to drastically\nimprove the ray shooting performance in Octomap. Our main idea is based on the\nuse of recent ray-tracing RTX GPU, mainly designed for real-time\nphoto-realistic computer graphics and the accompanying graphics API, known as\nDXR. Our ray-shooting first maps leaf-level voxels in the given octree to a set\nof axis-aligned bounding boxes (AABBs) and employ massively parallel ray\nshooting on them using GPUs to find free and occupied voxels. These are fed\nback into CPU to update the voxel occupancy and restructure the octree. In our\nexperiments, we have observed more than three-orders-of-magnitude performance\nimprovement in terms of ray shooting using ray-tracing RTX GPU over a\nstate-of-the-art Octomap CPU implementation, where the benchmarking\nenvironments consist of more than 77K points and 25K~34K voxel grids.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 11:25:07 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:30:40 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Min", "Heajung", ""], ["Han", "Kyung Min", ""], ["Kim", "Young J.", ""]]}, {"id": "2011.10379", "submitter": "Julian Ost", "authors": "Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, Felix Heide", "title": "Neural Scene Graphs for Dynamic Scenes", "comments": "Updated Project Page http://light.princeton.edu/neural-scene-graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent implicit neural rendering methods have demonstrated that it is\npossible to learn accurate view synthesis for complex scenes by predicting\ntheir volumetric density and color supervised solely by a set of RGB images.\nHowever, existing methods are restricted to learning efficient representations\nof static scenes that encode all scene objects into a single neural network,\nand lack the ability to represent dynamic scenes and decompositions into\nindividual scene objects. In this work, we present the first neural rendering\nmethod that decomposes dynamic scenes into scene graphs. We propose a learned\nscene graph representation, which encodes object transformation and radiance,\nto efficiently render novel arrangements and views of the scene. To this end,\nwe learn implicitly encoded scenes, combined with a jointly learned latent\nrepresentation to describe objects with a single implicit function. We assess\nthe proposed method on synthetic and real automotive data, validating that our\napproach learns dynamic scenes -- only by observing a video of this scene --\nand allows for rendering novel photo-realistic views of novel scene\ncompositions with unseen sets of objects at unseen poses.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:37:10 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 11:20:52 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:21:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ost", "Julian", ""], ["Mannan", "Fahim", ""], ["Thuerey", "Nils", ""], ["Knodt", "Julian", ""], ["Heide", "Felix", ""]]}, {"id": "2011.10687", "submitter": "Gowri Somanath", "authors": "Gowri Somanath and Daniel Kurz", "title": "HDR Environment Map Estimation for Real-Time Augmented Reality", "comments": "Supplementary video at\n  https://docs-assets.developer.apple.com/ml-research/papers/hdr-environment-map.mp4\n  Code at https://github.com/apple/ml-envmapnet Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate an HDR environment map from a narrow\nfield-of-view LDR camera image in real-time. This enables perceptually\nappealing reflections and shading on virtual objects of any material finish,\nfrom mirror to diffuse, rendered into a real physical environment using\naugmented reality. Our method is based on our efficient convolutional neural\nnetwork architecture, EnvMapNet, trained end-to-end with two novel losses,\nProjectionLoss for the generated image, and ClusterLoss for adversarial\ntraining. Through qualitative and quantitative comparison to state-of-the-art\nmethods, we demonstrate that our algorithm reduces the directional error of\nestimated light sources by more than 50%, and achieves 3.7 times lower Frechet\nInception Distance (FID). We further showcase a mobile application that is able\nto run our neural network model in under 9 ms on an iPhone XS, and render in\nreal-time, visually coherent virtual objects in previously unseen real-world\nenvironments.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:01:53 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 19:32:32 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:32:49 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 22:15:31 GMT"}, {"version": "v5", "created": "Tue, 27 Jul 2021 20:48:22 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Somanath", "Gowri", ""], ["Kurz", "Daniel", ""]]}, {"id": "2011.10688", "submitter": "Xinwei Yao", "authors": "Xinwei Yao, Ohad Fried, Kayvon Fatahalian, Maneesh Agrawala", "title": "Iterative Text-based Editing of Talking-heads Using Neural Retargeting", "comments": "Project Website is https://davidyao.me/projects/text2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a text-based tool for editing talking-head video that enables an\niterative editing workflow. On each iteration users can edit the wording of the\nspeech, further refine mouth motions if necessary to reduce artifacts and\nmanipulate non-verbal aspects of the performance by inserting mouth gestures\n(e.g. a smile) or changing the overall performance style (e.g. energetic,\nmumble). Our tool requires only 2-3 minutes of the target actor video and it\nsynthesizes the video for each iteration in about 40 seconds, allowing users to\nquickly explore many editing possibilities as they iterate. Our approach is\nbased on two key ideas. (1) We develop a fast phoneme search algorithm that can\nquickly identify phoneme-level subsequences of the source repository video that\nbest match a desired edit. This enables our fast iteration loop. (2) We\nleverage a large repository of video of a source actor and develop a new\nself-supervised neural retargeting technique for transferring the mouth motions\nof the source actor to the target actor. This allows us to work with relatively\nshort target actor videos, making our approach applicable in many real-world\nediting scenarios. Finally, our refinement and performance controls give users\nthe ability to further fine-tune the synthesized results.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 01:05:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yao", "Xinwei", ""], ["Fried", "Ohad", ""], ["Fatahalian", "Kayvon", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2011.11134", "submitter": "Yuanxin Zhong", "authors": "Yuanxin Zhong", "title": "Differentiable Computational Geometry for 2D and 3D machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the growth of machine learning algorithms with geometry primitives, a\nhigh-efficiency library with differentiable geometric operators are desired. We\npresent an optimized Differentiable Geometry Algorithm Library (DGAL) loaded\nwith implementations of differentiable operators for geometric primitives like\nlines and polygons. The library is a header-only templated C++ library with GPU\nsupport. We discuss the internal design of the library and benchmark its\nperformance on some tasks with other implementations.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 23:22:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhong", "Yuanxin", ""]]}, {"id": "2011.11704", "submitter": "Xiumin Shang", "authors": "Xiumin Shang, Ahmed Sabbir Arif, Marcelo Kallmann", "title": "Evaluating Feedback Strategies for Virtual Human Trainers", "comments": "A video demonstrating our work is available at:\n  https://drive.google.com/file/d/103iz2sUeLRX4KY457wucLZnhXhfiQZNv/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address feedback strategies for an autonomous virtual\ntrainer. First, a pilot study was conducted to identify and specify feedback\nstrategies for assisting participants in performing a given task. The task\ninvolved sorting virtual cubes according to areas of countries displayed on\nthem. Two feedback strategies were specified. The first provides correctness\nfeedback by fully correcting user responses at each stage of the task, and the\nsecond provides suggestive feedback by only notifying if and how a response can\nbe corrected. Both strategies were implemented in a virtual training system and\nempirically evaluated. The correctness feedback strategy was preferred by the\nparticipants, was more effective time-wise, and was more effective in improving\ntask performance skills. The overall system was also rated comparable to\nhypothetically performing the same task with real interactions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:13:34 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Shang", "Xiumin", ""], ["Arif", "Ahmed Sabbir", ""], ["Kallmann", "Marcelo", ""]]}, {"id": "2011.12104", "submitter": "Wanquan Feng", "authors": "Wanquan Feng, Juyong Zhang, Hongrui Cai, Haofei Xu, Junhui Hou and\n  Hujun Bao", "title": "Recurrent Multi-view Alignment Network for Unsupervised Surface\n  Registration", "comments": "Accepted to CVPR2021. The source codes are available at\n  https://github.com/WanquanF/RMA-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning non-rigid registration in an end-to-end manner is challenging due to\nthe inherent high degrees of freedom and the lack of labeled training data. In\nthis paper, we resolve these two challenges simultaneously. First, we propose\nto represent the non-rigid transformation with a point-wise combination of\nseveral rigid transformations. This representation not only makes the solution\nspace well-constrained but also enables our method to be solved iteratively\nwith a recurrent framework, which greatly reduces the difficulty of learning.\nSecond, we introduce a differentiable loss function that measures the 3D shape\nsimilarity on the projected multi-view 2D depth images so that our full\nframework can be trained end-to-end without ground truth supervision. Extensive\nexperiments on several different datasets demonstrate that our proposed method\noutperforms the previous state-of-the-art by a large margin. The source codes\nare available at https://github.com/WanquanF/RMA-Net.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:22:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 08:07:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Feng", "Wanquan", ""], ["Zhang", "Juyong", ""], ["Cai", "Hongrui", ""], ["Xu", "Haofei", ""], ["Hou", "Junhui", ""], ["Bao", "Hujun", ""]]}, {"id": "2011.12125", "submitter": "Sara Johansson Fernstad", "authors": "Sara Johansson Fernstad and Jimmy Johansson", "title": "To Explore What Isn't There -- Glyph-based Visualization for Analysis of\n  Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a novel visualization method, Missingness Glyph, for\nanalysis and exploration of missing values in data. Missing values are a common\nchallenge in most data generating domains and may cause a range of analysis\nissues. Missingness in data may indicate potential problems in data collection\nand pre-processing, or highlight important data characteristics. While the\ndevelopment and improvement of statistical methods for dealing with missing\ndata is a research area in its own right, mainly focussing on replacing missing\nvalues with estimated values, considerably less focus has been put on\nvisualization of missing values. Nonetheless, visualization and explorative\nanalysis has great potential to support understanding of missingness in data,\nand to enable gaining of novel insights into patterns of missingness in a way\nthat statistical methods are unable to. The Missingness Glyph supports\nidentification of relevant missingness patterns in data, and is evaluated and\ncompared to two other visualization methods in context of the missingness\npatterns. The results are promising and confirms that the Missingness Glyph in\nseveral cases perform better than the alternative visualization methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 14:32:26 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Fernstad", "Sara Johansson", ""], ["Johansson", "Jimmy", ""]]}, {"id": "2011.12440", "submitter": "Bernhard Egger", "authors": "Skylar Sutherland and Bernhard Egger and Joshua Tenenbaum", "title": "Building 3D Morphable Models from a Single Scan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for constructing generative models of 3D objects from a\nsingle 3D mesh. Our method produces a 3D morphable model that represents shape\nand albedo in terms of Gaussian processes. We define the shape deformations in\nphysical (3D) space and the albedo deformations as a combination of\nphysical-space and color-space deformations. Whereas previous approaches have\ntypically built 3D morphable models from multiple high-quality 3D scans through\nprincipal component analysis, we build 3D morphable models from a single scan\nor template. We demonstrate the utility of these models in the domain of face\nmodeling through inverse rendering and registration tasks. Specifically, we\nshow that our approach can be used to perform face recognition using only a\nsingle 3D scan (one scan total, not one per person), and further demonstrate\nhow multiple scans can be incorporated to improve performance without requiring\ndense correspondence. Our approach enables the synthesis of 3D morphable models\nfor 3D object categories where dense correspondence between multiple scans is\nunavailable. We demonstrate this by constructing additional 3D morphable models\nfor fish and birds and use them to perform simple inverse rendering tasks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 23:08:14 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Sutherland", "Skylar", ""], ["Egger", "Bernhard", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "2011.12490", "submitter": "Daniel Rebain", "authors": "Daniel Rebain, Wei Jiang, Soroosh Yazdani, Ke Li, Kwang Moo Yi, Andrea\n  Tagliasacchi", "title": "DeRF: Decomposed Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of Neural Radiance Fields (NeRF), neural networks can now\nrender novel views of a 3D scene with quality that fools the human eye. Yet,\ngenerating these images is very computationally intensive, limiting their\napplicability in practical scenarios. In this paper, we propose a technique\nbased on spatial decomposition capable of mitigating this issue. Our key\nobservation is that there are diminishing returns in employing larger (deeper\nand/or wider) networks. Hence, we propose to spatially decompose a scene and\ndedicate smaller networks for each decomposed part. When working together,\nthese networks can render the whole scene. This allows us near-constant\ninference time regardless of the number of decomposed parts. Moreover, we show\nthat a Voronoi spatial decomposition is preferable for this purpose, as it is\nprovably compatible with the Painter's Algorithm for efficient and GPU-friendly\nrendering. Our experiments show that for real-world scenes, our method provides\nup to 3x more efficient inference than NeRF (with the same rendering quality),\nor an improvement of up to 1.0~dB in PSNR (for the same inference cost).\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 02:47:16 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rebain", "Daniel", ""], ["Jiang", "Wei", ""], ["Yazdani", "Soroosh", ""], ["Li", "Ke", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2011.12799", "submitter": "Dani Lischinski", "authors": "Zongze Wu, Dani Lischinski, Eli Shechtman", "title": "StyleSpace Analysis: Disentangled Controls for StyleGAN Image Generation", "comments": "25 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore and analyze the latent style space of StyleGAN2, a\nstate-of-the-art architecture for image generation, using models pretrained on\nseveral different datasets. We first show that StyleSpace, the space of\nchannel-wise style parameters, is significantly more disentangled than the\nother intermediate latent spaces explored by previous works. Next, we describe\na method for discovering a large collection of style channels, each of which is\nshown to control a distinct visual attribute in a highly localized and\ndisentangled manner. Third, we propose a simple method for identifying style\nchannels that control a specific attribute, using a pretrained classifier or a\nsmall number of example images. Manipulation of visual attributes via these\nStyleSpace controls is shown to be better disentangled than via those proposed\nin previous works. To show this, we make use of a newly proposed Attribute\nDependency metric. Finally, we demonstrate the applicability of StyleSpace\ncontrols to the manipulation of real images. Our findings pave the way to\nsemantically meaningful and well-disentangled image manipulations via simple\nand intuitive interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:00:33 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 17:30:00 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wu", "Zongze", ""], ["Lischinski", "Dani", ""], ["Shechtman", "Eli", ""]]}, {"id": "2011.12833", "submitter": "Wonwoong Cho", "authors": "Wonwoong Cho, Inyeop Lee, David Inouye", "title": "Enhanced 3DMM Attribute Control via Synthetic Dataset Creation Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While facial attribute manipulation of 2D images via Generative Adversarial\nNetworks (GANs) has become common in computer vision and graphics due to its\nmany practical uses, research on 3D attribute manipulation is relatively\nundeveloped. Existing 3D attribute manipulation methods are limited because the\nsame semantic changes are applied to every 3D face. The key challenge for\ndeveloping better 3D attribute control methods is the lack of paired training\ndata in which one attribute is changed while other attributes are held fixed --\ne.g., a pair of 3D faces where one is male and the other is female but all\nother attributes, such as race and expression, are the same. To overcome this\nchallenge, we design a novel pipeline for generating paired 3D faces by\nharnessing the power of GANs. On top of this pipeline, we then propose an\nenhanced non-linear 3D conditional attribute controller that increases the\nprecision and diversity of 3D attribute control compared to existing methods.\nWe demonstrate the validity of our dataset creation pipeline and the superior\nperformance of our conditional attribute controller via quantitative and\nqualitative evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 15:43:24 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 04:47:46 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cho", "Wonwoong", ""], ["Lee", "Inyeop", ""], ["Inouye", "David", ""]]}, {"id": "2011.12893", "submitter": "Wonwoong Cho", "authors": "Myunggi Lee, Wonwoong Cho, Moonheum Kim, David Inouye, Nojun Kwak", "title": "StyleUV: Diverse and High-fidelity UV Map Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing 3D human faces in the wild with the 3D Morphable Model (3DMM)\nhas become popular in recent years. While most prior work focuses on estimating\nmore robust and accurate geometry, relatively little attention has been paid to\nimproving the quality of the texture model. Meanwhile, with the advent of\nGenerative Adversarial Networks (GANs), there has been great progress in\nreconstructing realistic 2D images. Recent work demonstrates that GANs trained\nwith abundant high-quality UV maps can produce high-fidelity textures superior\nto those produced by existing methods. However, acquiring such high-quality UV\nmaps is difficult because they are expensive to acquire, requiring laborious\nprocesses to refine. In this work, we present a novel UV map generative model\nthat learns to generate diverse and realistic synthetic UV maps without\nrequiring high-quality UV maps for training. Our proposed framework can be\ntrained solely with in-the-wild images (i.e., UV maps are not required) by\nleveraging a combination of GANs and a differentiable renderer. Both\nquantitative and qualitative evaluations demonstrate that our proposed texture\nmodel produces more diverse and higher fidelity textures compared to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 17:19:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lee", "Myunggi", ""], ["Cho", "Wonwoong", ""], ["Kim", "Moonheum", ""], ["Inouye", "David", ""], ["Kwak", "Nojun", ""]]}, {"id": "2011.12948", "submitter": "Keunhong Park", "authors": "Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien Bouaziz, Dan\n  B Goldman, Steven M. Seitz, Ricardo Martin-Brualla", "title": "Nerfies: Deformable Neural Radiance Fields", "comments": "Project page with videos: https://nerfies.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method capable of photorealistically reconstructing\ndeformable scenes using photos/videos captured casually from mobile phones. Our\napproach augments neural radiance fields (NeRF) by optimizing an additional\ncontinuous volumetric deformation field that warps each observed point into a\ncanonical 5D NeRF. We observe that these NeRF-like deformation fields are prone\nto local minima, and propose a coarse-to-fine optimization method for\ncoordinate-based models that allows for more robust optimization. By adapting\nprinciples from geometry processing and physical simulation to NeRF-like\nmodels, we propose an elastic regularization of the deformation field that\nfurther improves robustness. We show that our method can turn casually captured\nselfie photos/videos into deformable NeRF models that allow for photorealistic\nrenderings of the subject from arbitrary viewpoints, which we dub \"nerfies.\" We\nevaluate our method by collecting time-synchronized data using a rig with two\nmobile phones, yielding train/validation images of the same pose at different\nviewpoints. We show that our method faithfully reconstructs non-rigidly\ndeforming scenes and reproduces unseen views with high fidelity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 18:55:04 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 01:52:45 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 04:42:44 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 00:30:29 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Park", "Keunhong", ""], ["Sinha", "Utkarsh", ""], ["Barron", "Jonathan T.", ""], ["Bouaziz", "Sofien", ""], ["Goldman", "Dan B", ""], ["Seitz", "Steven M.", ""], ["Martin-Brualla", "Ricardo", ""]]}, {"id": "2011.12999", "submitter": "Jo\\~ao Pedro Moreira Ferreira", "authors": "Jo\\~ao P. Ferreira, Thiago M. Coutinho, Thiago L. Gomes, Jos\\'e F.\n  Neto, Rafael Azevedo, Renato Martins, Erickson R. Nascimento", "title": "Learning to dance: A graph convolutional adversarial network to generate\n  realistic dance motions from audio", "comments": "Accepted at the Elsevier Computers & Graphics (C&G) 2020", "journal-ref": null, "doi": "10.1016/j.cag.2020.09.009", "report-no": null, "categories": "cs.GR cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing human motion through learning techniques is becoming an\nincreasingly popular approach to alleviating the requirement of new data\ncapture to produce animations. Learning to move naturally from music, i.e., to\ndance, is one of the more complex motions humans often perform effortlessly.\nEach dance movement is unique, yet such movements maintain the core\ncharacteristics of the dance style. Most approaches addressing this problem\nwith classical convolutional and recursive neural models undergo training and\nvariability issues due to the non-Euclidean geometry of the motion manifold\nstructure.In this paper, we design a novel method based on graph convolutional\nnetworks to tackle the problem of automatic dance generation from audio\ninformation. Our method uses an adversarial learning scheme conditioned on the\ninput music audios to create natural motions preserving the key movements of\ndifferent music styles. We evaluate our method with three quantitative metrics\nof generative methods and a user study. The results suggest that the proposed\nGCN model outperforms the state-of-the-art dance generation method conditioned\non music in different experiments. Moreover, our graph-convolutional approach\nis simpler, easier to be trained, and capable of generating more realistic\nmotion styles regarding qualitative and different quantitative metrics. It also\npresented a visual movement perceptual quality comparable to real motion data.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:53:53 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 17:59:15 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ferreira", "Jo\u00e3o P.", ""], ["Coutinho", "Thiago M.", ""], ["Gomes", "Thiago L.", ""], ["Neto", "Jos\u00e9 F.", ""], ["Azevedo", "Rafael", ""], ["Martins", "Renato", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "2011.13045", "submitter": "Homer Walke", "authors": "Homer Walke, R. Kenny Jones, Daniel Ritchie", "title": "Learning to Infer Shape Programs Using Latent Execution Self Training", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring programs which generate 2D and 3D shapes is important for reverse\nengineering, enabling shape editing, and more. Supervised learning is hard to\napply to this problem, as paired (program, shape) data rarely exists. Recent\napproaches use supervised pre-training with randomly-generated programs and\nthen refine using self-supervised learning. But self-supervised learning either\nrequires that the program execution process be differentiable or relies on\nreinforcement learning, which is unstable and slow to converge. In this paper,\nwe present a new approach for learning to infer shape programs, which we call\nlatent execution self training (LEST). As with recent prior work, LEST starts\nby training on randomly-generated (program, shape) pairs. As its name implies,\nit is based on the idea of self-training: running a model on unlabeled input\nshapes, treating the predicted programs as ground truth latent labels, and\ntraining again. Self-training is known to be susceptible to local minima. LEST\ncircumvents this problem by leveraging the fact that predicted latent programs\nare executable: for a given shape $\\mathbf{x}^* \\in S^*$ and its predicted\nprogram $\\mathbf{z} \\in P$, we execute $\\mathbf{z}$ to obtain a shape\n$\\mathbf{x} \\in S$ and train on $(\\mathbf{z} \\in P, \\mathbf{x} \\in S)$ pairs,\nrather than $(\\mathbf{z} \\in P, \\mathbf{x}^* \\in S^*)$ pairs. Experiments show\nthat the distribution of executed shapes $S$ converges toward the distribution\nof real shapes $S^*$. We establish connections between LEST and algorithms for\nlearning generative models, including variational Bayes, wake sleep, and\nexpectation maximization. For constructive solid geometry and assembly-based\nmodeling, LEST's inferred programs converge to high reconstruction accuracy\nsignificantly faster than those of reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 22:10:32 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Walke", "Homer", ""], ["Jones", "R. Kenny", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2011.13076", "submitter": "Or Litany", "authors": "Or Litany, Emanuele Rodol\\`a, Alex Bronstein, Michael Bronstein,\n  Daniel Cremers", "title": "Non-Rigid Puzzles", "comments": null, "journal-ref": "Computer Graphics Forum, Volume 35, Issue 5, August 2016", "doi": "10.1111/cgf.12970", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape correspondence is a fundamental problem in computer graphics and\nvision, with applications in various problems including animation, texture\nmapping, robotic vision, medical imaging, archaeology and many more. In\nsettings where the shapes are allowed to undergo non-rigid deformations and\nonly partial views are available, the problem becomes very challenging. To this\nend, we present a non-rigid multi-part shape matching algorithm. We assume to\nbe given a reference shape and its multiple parts undergoing a non-rigid\ndeformation. Each of these query parts can be additionally contaminated by\nclutter, may overlap with other parts, and there might be missing parts or\nredundant ones. Our method simultaneously solves for the segmentation of the\nreference model, and for a dense correspondence to (subsets of) the parts.\nExperimental results on synthetic as well as real scans demonstrate the\neffectiveness of our method in dealing with this challenging matching scenario.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 00:32:30 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Litany", "Or", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Alex", ""], ["Bronstein", "Michael", ""], ["Cremers", "Daniel", ""]]}, {"id": "2011.13202", "submitter": "Soroosh Poorgholi", "authors": "Soroosh Poorgholi, Osman Semih Kayhan and Jan C. van Gemert", "title": "t-EVA: Time-Efficient t-SNE Video Annotation", "comments": "ICPR 2020 (HCAU)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video understanding has received more attention in the past few years due to\nthe availability of several large-scale video datasets. However, annotating\nlarge-scale video datasets are cost-intensive. In this work, we propose a\ntime-efficient video annotation method using spatio-temporal feature similarity\nand t-SNE dimensionality reduction to speed up the annotation process\nmassively. Placing the same actions from different videos near each other in\nthe two-dimensional space based on feature similarity helps the annotator to\ngroup-label video clips. We evaluate our method on two subsets of the\nActivityNet (v1.3) and a subset of the Sports-1M dataset. We show that t-EVA\ncan outperform other video annotation tools while maintaining test accuracy on\nvideo classification.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 09:56:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Poorgholi", "Soroosh", ""], ["Kayhan", "Osman Semih", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2011.13417", "submitter": "Paul Guerrero", "authors": "Wamiq Para, Paul Guerrero, Tom Kelly, Leonidas Guibas, Peter Wonka", "title": "Generative Layout Modeling using Constraint Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new generative model for layout generation. We generate layouts\nin three steps. First, we generate the layout elements as nodes in a layout\ngraph. Second, we compute constraints between layout elements as edges in the\nlayout graph. Third, we solve for the final layout using constrained\noptimization. For the first two steps, we build on recent transformer\narchitectures. The layout optimization implements the constraints efficiently.\nWe show three practical contributions compared to the state of the art: our\nwork requires no user input, produces higher quality layouts, and enables many\nnovel capabilities for conditional layout generation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 18:18:37 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Para", "Wamiq", ""], ["Guerrero", "Paul", ""], ["Kelly", "Tom", ""], ["Guibas", "Leonidas", ""], ["Wonka", "Peter", ""]]}, {"id": "2011.13740", "submitter": "Aditya Jyoti Paul", "authors": "Aditya Jyoti Paul", "title": "Recent Advances in Selective Image Encryption and its Indispensability\n  due to COVID-19", "comments": "6 pages, Published in IEEE RAICS 2020, see https://raics.in", "journal-ref": "2020 IEEE Recent Advances in Intelligent Computational Systems\n  (RAICS), 2020, pp. 201-206", "doi": "10.1109/RAICS51191.2020.9332513", "report-no": null, "categories": "cs.CR cs.CY cs.GR cs.IT cs.MM math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic serves as a grim reminder of the unexpected nature of\nthese outbreaks and gives rise to a unique set of research challenges in a\nvariety of fields. As people all over the world adjust to this new 'normal',\nwith most workplaces, from companies to educational institutions shifting\nonline, enormous surges in the transmission of images and videos have been\nobserved, creating record-breaking stresses on the internet backbone. At the\nsame time, maintaining the privacy and security of the users' data is of\nimmense importance, this is where fast and efficient image encryption\nalgorithms play a vital role. This paper discusses the calamitous effects of\nthe pandemic on the world population and how their changes in multimedia\nconsumption have led to an urgent need for the development and deployment of\nsecure and fast image encryption, especially selective image encryption\ntechniques. It carefully surveys the most recent advances in this field,\ndiscusses their real-world effects and finally explores some future research\navenues, to provide swift relief and recover from the disastrous effects of the\npandemic.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:02:54 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 02:11:48 GMT"}, {"version": "v3", "created": "Sat, 13 Feb 2021 10:21:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Paul", "Aditya Jyoti", ""]]}, {"id": "2011.13748", "submitter": "Fatemeh Teimury", "authors": "Fatemeh Teimury, Bruno Roy, Juan Sebasti\\'an Casallas, David\n  MacDonald, Mark Coates", "title": "GraphSeam: Supervised Graph Learning Framework for Semantic UV Mapping", "comments": "13 pages including the supplementary material, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently there has been a significant effort to automate UV mapping, the\nprocess of mapping 3D-dimensional surfaces to the UV space while minimizing\ndistortion and seam length. Although state-of-the-art methods, Autocuts and\nOptCuts, addressed this task via energy-minimization approaches, they fail to\nproduce semantic seam styles, an essential factor for professional artists. The\nrecent emergence of Graph Neural Networks (GNNs), and the fact that a mesh can\nbe represented as a particular form of a graph, has opened a new bridge to\nnovel graph learning-based solutions in the computer graphics domain. In this\nwork, we use the power of supervised GNNs for the first time to propose a fully\nautomated UV mapping framework that enables users to replicate their desired\nseam styles while reducing distortion and seam length. To this end, we provide\naugmentation and decimation tools to enable artists to create their dataset and\ntrain the network to produce their desired seam style. We provide a\ncomplementary post-processing approach for reducing the distortion based on\ngraph algorithms to refine low-confidence seam predictions and reduce seam\nlength (or the number of shells in our supervised case) using a skeletonization\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:26:46 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 16:04:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Teimury", "Fatemeh", ""], ["Roy", "Bruno", ""], ["Casallas", "Juan Sebasti\u00e1n", ""], ["MacDonald", "David", ""], ["Coates", "Mark", ""]]}, {"id": "2011.14035", "submitter": "Constantinos Chamzas", "authors": "Dimitrios Chamzas, Constantinos Chamzas and Konstantinos Moustakas", "title": "cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex\n  Polytope", "comments": "Accepted in GRAPP 2021", "journal-ref": "GRAPP, 2021, 229-236", "doi": "10.5220/0010259002290236", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, the emerging field of Augmented & Virtual Reality\n(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop\nlow cost high-quality AR systems where computing poweris in demand. Feature\npoints are extensively used in these real-time frame-rate and 3D applications,\nthereforeefficient high-speed feature detectors are necessary. Corners are such\nspecial features and often are used as thefirst step in the marker alignment in\nAugmented Reality (AR). Corners are also used in image registration\nandrecognition, tracking, SLAM, robot path finding and 2D or 3D object\ndetection and retrieval. Therefore thereis a large number of corner detection\nalgorithms but most of them are too computationally intensive for use\ninreal-time applications of any complexity. Many times the border of the image\nis a convex polygon. For thisspecial, but quite common case, we have developed\na specific algorithm, cMinMax. The proposed algorithmis faster, approximately\nby a factor of 5 compared to the widely used Harris Corner Detection algorithm.\nInaddition is highly parallelizable. The algorithm is suitable for the fast\nregistration of markers in augmentedreality systems and in applications where a\ncomputationally efficient real time feature detector is necessary.The algorithm\ncan also be extended to N-dimensional polyhedrons.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 00:32:11 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 15:11:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chamzas", "Dimitrios", ""], ["Chamzas", "Constantinos", ""], ["Moustakas", "Konstantinos", ""]]}, {"id": "2011.14143", "submitter": "Tarun Yenamandra", "authors": "Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-Peter Seidel,\n  Mohamed Elgharib, Daniel Cremers, Christian Theobalt", "title": "i3DMM: Deep Implicit 3D Morphable Model of Human Heads", "comments": "Project page: http://gvv.mpi-inf.mpg.de/projects/i3DMM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first deep implicit 3D morphable model (i3DMM) of full heads.\nUnlike earlier morphable face models it not only captures identity-specific\ngeometry, texture, and expressions of the frontal face, but also models the\nentire head, including hair. We collect a new dataset consisting of 64 people\nwith different expressions and hairstyles to train i3DMM. Our approach has the\nfollowing favorable properties: (i) It is the first full head morphable model\nthat includes hair. (ii) In contrast to mesh-based models it can be trained on\nmerely rigidly aligned scans, without requiring difficult non-rigid\nregistration. (iii) We design a novel architecture to decouple the shape model\ninto an implicit reference shape and a deformation of this reference shape.\nWith that, dense correspondences between shapes can be learned implicitly. (iv)\nThis architecture allows us to semantically disentangle the geometry and color\ncomponents, as color is learned in the reference space. Geometry is further\ndisentangled as identity, expressions, and hairstyle, while color is\ndisentangled as identity and hairstyle components. We show the merits of i3DMM\nusing ablation studies, comparisons to state-of-the-art models, and\napplications such as semantic head editing and texture transfer. We will make\nour model publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 15:01:53 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Yenamandra", "Tarun", ""], ["Tewari", "Ayush", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Cremers", "Daniel", ""], ["Theobalt", "Christian", ""]]}, {"id": "2011.14285", "submitter": "Haoxi Ran", "authors": "Haoxi Ran, Li Lu", "title": "Deeper or Wider Networks of Point Clouds with Self-attention?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevalence of deeper networks driven by self-attention is in stark contrast\nto underexplored point-based methods. In this paper, we propose groupwise\nself-attention as the basic block to construct our network: SepNet. Our\nproposed module can effectively capture both local and global dependencies.\nThis module computes the features of a group based on the summation of the\nweighted features of any point within the group. For convenience, we generalize\ngroupwise operations to assemble this module. To further facilitate our\nnetworks, we deepen and widen SepNet on the tasks of segmentation and\nclassification respectively, and verify its practicality. Specifically, SepNet\nachieves state-of-the-art for the tasks of classification and segmentation on\nmost of the datasets. We show empirical evidence that SepNet can obtain extra\naccuracy in classification or segmentation from increased width or depth,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 05:03:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Ran", "Haoxi", ""], ["Lu", "Li", ""]]}, {"id": "2011.14398", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen, Animesh Karnewar, Lam Huynh, Esa Rahtu, Jiri Matas,\n  Janne Heikkila", "title": "RGBD-Net: Predicting color and depth images for novel views synthesis", "comments": "19 pages, 15 figures. Code will be available at:\n  https://github.com/phongnhhn92/RGBDNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new cascaded architecture for novel view synthesis, called\nRGBD-Net, which consists of two core components: a hierarchical depth\nregression network and a depth-aware generator network. The former one predicts\ndepth maps of the target views by using adaptive depth scaling, while the\nlatter one leverages the predicted depths and renders spatially and temporally\nconsistent target images. In the experimental evaluation on standard datasets,\nRGBD-Net not only outperforms the state-of-the-art by a clear margin, but it\nalso generalizes well to new scenes without per-scene optimization. Moreover,\nwe show that RGBD-Net can be optionally trained without depth supervision while\nstill retaining high-quality rendering. Thanks to the depth regression network,\nRGBD-Net can be also used for creating dense 3D point clouds that are more\naccurate than those produced by some state-of-the-art multi-view stereo\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 16:42:53 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 15:06:24 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Nguyen", "Phong", ""], ["Karnewar", "Animesh", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Matas", "Jiri", ""], ["Heikkila", "Janne", ""]]}, {"id": "2011.14535", "submitter": "Sarah Radway", "authors": "Sarah Radway, Anthony Luo, Carmine Elvezio, Jenny Cha, Sophia Kolak,\n  Elijah Zulu, Sad Adib", "title": "Beyond LunAR: An augmented reality UI for deep-space exploration\n  missions", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As space exploration efforts shift to deep space missions, new challenges\nemerge regarding astronaut communication and task completion. While the round\ntrip propagation delay for lunar communications is 2.6 seconds, the time delay\nincreases to nearly 22 minutes for Mars missions. This creates a need for\nastronaut independence from earth-based assistance, and places greater\nsignificance upon the limited communications that are able to be delivered. To\naddress this issue, we prototyped an augmented reality user interface for the\nnew xEMU spacesuit, intended for use on planetary surface missions. This user\ninterface assists with functions that would usually be completed by flight\ncontrollers in Mission Control, or are currently completed in manners that are\nunnecessarily difficult. We accomplish this through features such as AR\nmodel-based task instruction, sampling task assistance, note taking, and\ntelemetry monitoring and display.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 04:16:19 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Radway", "Sarah", ""], ["Luo", "Anthony", ""], ["Elvezio", "Carmine", ""], ["Cha", "Jenny", ""], ["Kolak", "Sophia", ""], ["Zulu", "Elijah", ""], ["Adib", "Sad", ""]]}, {"id": "2011.15119", "submitter": "Tingwu Wang", "authors": "Tingwu Wang, Yunrong Guo, Maria Shugrina, Sanja Fidler", "title": "UniCon: Universal Neural Controller For Physics-based Character Motion", "comments": "15 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of physics-based animation is gaining importance due to the\nincreasing demand for realism in video games and films, and has recently seen\nwide adoption of data-driven techniques, such as deep reinforcement learning\n(RL), which learn control from (human) demonstrations. While RL has shown\nimpressive results at reproducing individual motions and interactive\nlocomotion, existing methods are limited in their ability to generalize to new\nmotions and their ability to compose a complex motion sequence interactively.\nIn this paper, we propose a physics-based universal neural controller (UniCon)\nthat learns to master thousands of motions with different styles by learning on\nlarge-scale motion datasets. UniCon is a two-level framework that consists of a\nhigh-level motion scheduler and an RL-powered low-level motion executor, which\nis our key innovation. By systematically analyzing existing multi-motion RL\nframeworks, we introduce a novel objective function and training techniques\nwhich make a significant leap in performance. Once trained, our motion executor\ncan be combined with different high-level schedulers without the need for\nretraining, enabling a variety of real-time interactive applications. We show\nthat UniCon can support keyboard-driven control, compose motion sequences drawn\nfrom a large pool of locomotion and acrobatics skills and teleport a person\ncaptured on video to a physics-based virtual avatar. Numerical and qualitative\nresults demonstrate a significant improvement in efficiency, robustness and\ngeneralizability of UniCon over prior state-of-the-art, showcasing\ntransferability to unseen motions, unseen humanoid models and unseen\nperturbation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:51:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Wang", "Tingwu", ""], ["Guo", "Yunrong", ""], ["Shugrina", "Maria", ""], ["Fidler", "Sanja", ""]]}, {"id": "2011.15128", "submitter": "Aleksander Holynski", "authors": "Aleksander Holynski, Brian Curless, Steven M. Seitz, Richard Szeliski", "title": "Animating Pictures with Eulerian Motion Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate a fully automatic method for converting a still\nimage into a realistic animated looping video. We target scenes with continuous\nfluid motion, such as flowing water and billowing smoke. Our method relies on\nthe observation that this type of natural motion can be convincingly reproduced\nfrom a static Eulerian motion description, i.e. a single, temporally constant\nflow field that defines the immediate motion of a particle at a given 2D\nlocation. We use an image-to-image translation network to encode motion priors\nof natural scenes collected from online videos, so that for a new photo, we can\nsynthesize a corresponding motion field. The image is then animated using the\ngenerated motion through a deep warping technique: pixels are encoded as deep\nfeatures, those features are warped via Eulerian motion, and the resulting\nwarped feature maps are decoded as images. In order to produce continuous,\nseamlessly looping video textures, we propose a novel video looping technique\nthat flows features both forward and backward in time and then blends the\nresults. We demonstrate the effectiveness and robustness of our method by\napplying it to a large collection of examples including beaches, waterfalls,\nand flowing rivers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 18:59:06 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Holynski", "Aleksander", ""], ["Curless", "Brian", ""], ["Seitz", "Steven M.", ""], ["Szeliski", "Richard", ""]]}]