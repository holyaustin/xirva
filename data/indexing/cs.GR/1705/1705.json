[{"id": "1705.00274", "submitter": "Sibel Tari", "authors": "Asli Genctav, Yusuf Sahillioglu, and Sibel Tari", "title": "Topologically Robust 3D Shape Matching via Gradual Deflation and\n  Inflation", "comments": "Section 2 replaced", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being vastly ignored in the literature, coping with topological noise\nis an issue of increasing importance, especially as a consequence of the\nincreasing number and diversity of 3D polygonal models that are captured by\ndevices of different qualities or synthesized by algorithms of different\nstabilities. One approach for matching 3D shapes under topological noise is to\nreplace the topology-sensitive geodesic distance with distances that are less\nsensitive to topological changes. We propose an alternative approach utilising\ngradual deflation (or inflation) of the shape volume, of which purpose is to\nbring the pair of shapes to be matched to a \\emph{comparable} topology before\nthe search for correspondences. Illustrative experiments using different\ndatasets demonstrate that as the level of topological noise increases, our\napproach outperforms the other methods in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 06:40:18 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 21:48:29 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Genctav", "Asli", ""], ["Sahillioglu", "Yusuf", ""], ["Tari", "Sibel", ""]]}, {"id": "1705.00949", "submitter": "Christian Mostegel", "authors": "Christian Mostegel and Rudolf Prettenthaler and Friedrich Fraundorfer\n  and Horst Bischof", "title": "Scalable Surface Reconstruction from Point Clouds with Extreme Scale and\n  Density Diversity", "comments": "This paper was accepted to the IEEE Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2017. The copyright was transfered to IEEE\n  (ieee.org). The official version of the paper will be made available on IEEE\n  Xplore (R) (ieeexplore.ieee.org). This version of the paper also contains the\n  supplementary material, which will not appear IEEE Xplore (R)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a scalable approach for robustly computing a 3D\nsurface mesh from multi-scale multi-view stereo point clouds that can handle\nextreme jumps of point density (in our experiments three orders of magnitude).\nThe backbone of our approach is a combination of octree data partitioning,\nlocal Delaunay tetrahedralization and graph cut optimization. Graph cut\noptimization is used twice, once to extract surface hypotheses from local\nDelaunay tetrahedralizations and once to merge overlapping surface hypotheses\neven when the local tetrahedralizations do not share the same topology.This\nformulation allows us to obtain a constant memory consumption per sub-problem\nwhile at the same time retaining the density independent interpolation\nproperties of the Delaunay-based optimization. On multiple public datasets, we\ndemonstrate that our approach is highly competitive with the state-of-the-art\nin terms of accuracy, completeness and outlier resilience. Further, we\ndemonstrate the multi-scale potential of our approach by processing a newly\nrecorded dataset with 2 billion points and a point density variation of more\nthan four orders of magnitude - requiring less than 9GB of RAM per process.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 13:13:47 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Mostegel", "Christian", ""], ["Prettenthaler", "Rudolf", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1705.01156", "submitter": "Balazs Kovacs", "authors": "Balazs Kovacs, Sean Bell, Noah Snavely, Kavita Bala", "title": "Shading Annotations in the Wild", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding shading effects in images is critical for a variety of vision\nand graphics problems, including intrinsic image decomposition, shadow removal,\nimage relighting, and inverse rendering. As is the case with other vision\ntasks, machine learning is a promising approach to understanding shading - but\nthere is little ground truth shading data available for real-world images. We\nintroduce Shading Annotations in the Wild (SAW), a new large-scale, public\ndataset of shading annotations in indoor scenes, comprised of multiple forms of\nshading judgments obtained via crowdsourcing, along with shading annotations\nautomatically generated from RGB-D imagery. We use this data to train a\nconvolutional neural network to predict per-pixel shading information in an\nimage. We demonstrate the value of our data and network in an application to\nintrinsic images, where we can reduce decomposition artifacts produced by\nexisting algorithms. Our database is available at\nhttp://opensurfaces.cs.cornell.edu/saw/.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 19:54:31 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kovacs", "Balazs", ""], ["Bell", "Sean", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""]]}, {"id": "1705.01263", "submitter": "Alexander Keller", "authors": "Alexander Keller, Carsten W\\\"achter, Matthias Raab, Daniel Seibert,\n  Dietger van Antwerpen, Johann Kornd\\\"orfer and Lutz Kettner", "title": "The Iray Light Transport Simulation and Rendering System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While ray tracing has become increasingly common and path tracing is well\nunderstood by now, a major challenge lies in crafting an easy-to-use and\nefficient system implementing these technologies. Following a purely\nphysically-based paradigm while still allowing for artistic workflows, the Iray\nlight transport simulation and rendering system allows for rendering complex\nscenes by the push of a button and thus makes accurate light transport\nsimulation widely available. In this document we discuss the challenges and\nimplementation choices that follow from our primary design decisions,\ndemonstrating that such a rendering system can be made a practical, scalable,\nand efficient real-world application that has been adopted by various companies\nacross many fields and is in use by many industry professionals today.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 06:03:08 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Keller", "Alexander", ""], ["W\u00e4chter", "Carsten", ""], ["Raab", "Matthias", ""], ["Seibert", "Daniel", ""], ["van Antwerpen", "Dietger", ""], ["Kornd\u00f6rfer", "Johann", ""], ["Kettner", "Lutz", ""]]}, {"id": "1705.01425", "submitter": "Mengyu Chu", "authors": "Mengyu Chu and Nils Thuerey", "title": "Data-Driven Synthesis of Smoke Flows with CNN-based Feature Descriptors", "comments": "14 pages, 17 figures, to appear at SIGGRAPH 2017, v2 only fixes small\n  typos", "journal-ref": "ACM Trans. Graph.36, 4 (2017), 69:1-69:13", "doi": "10.1145/3072959.3073643", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel data-driven algorithm to synthesize high-resolution flow\nsimulations with reusable repositories of space-time flow data. In our work, we\nemploy a descriptor learning approach to encode the similarity between fluid\nregions with differences in resolution and numerical viscosity. We use\nconvolutional neural networks to generate the descriptors from fluid data such\nas smoke density and flow velocity. At the same time, we present a deformation\nlimiting patch advection method which allows us to robustly track deformable\nfluid regions. With the help of this patch advection, we generate stable\nspace-time data sets from detailed fluids for our repositories. We can then use\nour learned descriptors to quickly localize a suitable data set when running a\nnew simulation. This makes our approach very efficient, and resolution\nindependent. We will demonstrate with several examples that our method yields\nvolumes with very high effective resolutions, and non-dissipative small scale\ndetails that naturally integrate into the motions of the underlying flow.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:41:49 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 14:35:49 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Chu", "Mengyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1705.01583", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Srinath Sridhar, Oleksandr Sotnychenko, Helge Rhodin,\n  Mohammad Shafiei, Hans-Peter Seidel, Weipeng Xu, Dan Casas, Christian\n  Theobalt", "title": "VNect: Real-time 3D Human Pose Estimation with a Single RGB Camera", "comments": "Accepted to SIGGRAPH 2017", "journal-ref": null, "doi": "10.1145/3072959.3073596", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first real-time method to capture the full global 3D skeletal\npose of a human in a stable, temporally consistent manner using a single RGB\ncamera. Our method combines a new convolutional neural network (CNN) based pose\nregressor with kinematic skeleton fitting. Our novel fully-convolutional pose\nformulation regresses 2D and 3D joint positions jointly in real time and does\nnot require tightly cropped input frames. A real-time kinematic skeleton\nfitting method uses the CNN output to yield temporally stable 3D global pose\nreconstructions on the basis of a coherent kinematic skeleton. This makes our\napproach the first monocular RGB method usable in real-time applications such\nas 3D character control---thus far, the only monocular methods for such\napplications employed specialized RGB-D cameras. Our method's accuracy is\nquantitatively on par with the best offline 3D monocular RGB pose estimation\nmethods. Our results are qualitatively comparable to, and sometimes better\nthan, results from monocular RGB-D approaches, such as the Kinect. However, we\nshow that our approach is more broadly applicable than RGB-D solutions, i.e. it\nworks for outdoor scenes, community videos, and low quality commodity RGB\ncameras.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 19:13:23 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mehta", "Dushyant", ""], ["Sridhar", "Srinath", ""], ["Sotnychenko", "Oleksandr", ""], ["Rhodin", "Helge", ""], ["Shafiei", "Mohammad", ""], ["Seidel", "Hans-Peter", ""], ["Xu", "Weipeng", ""], ["Casas", "Dan", ""], ["Theobalt", "Christian", ""]]}, {"id": "1705.01661", "submitter": "Li Yi", "authors": "Li Yi, Leonidas Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su,\n  Ersin Yumer", "title": "Learning Hierarchical Shape Segmentation and Labeling from Online\n  Repositories", "comments": null, "journal-ref": null, "doi": "10.1145/3072959.3073652", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for converting geometric shapes into hierarchically\nsegmented parts with part labels. Our key idea is to train category-specific\nmodels from the scene graphs and part names that accompany 3D shapes in public\nrepositories. These freely-available annotations represent an enormous,\nuntapped source of information on geometry. However, because the models and\ncorresponding scene graphs are created by a wide range of modelers with\ndifferent levels of expertise, modeling tools, and objectives, these models\nhave very inconsistent segmentations and hierarchies with sparse and noisy\ntextual tags. Our method involves two analysis steps. First, we perform a joint\noptimization to simultaneously cluster and label parts in the database while\nalso inferring a canonical tag dictionary and part hierarchy. We then use this\nlabeled data to train a method for hierarchical segmentation and labeling of\nnew 3D shapes. We demonstrate that our method can mine complex information,\ndetecting hierarchies in man-made objects and their constituent parts,\nobtaining finer scale details than existing alternatives. We also show that, by\nperforming domain transfer using a few supervised examples, our technique\noutperforms fully-supervised techniques that require hundreds of\nmanually-labeled models.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 00:11:16 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Yi", "Li", ""], ["Guibas", "Leonidas", ""], ["Hertzmann", "Aaron", ""], ["Kim", "Vladimir G.", ""], ["Su", "Hao", ""], ["Yumer", "Ersin", ""]]}, {"id": "1705.01674", "submitter": "Wei Liu", "authors": "Wei Liu, Xiaogang Chen, Chuanhua Shen, Zhi Liu and Jie Yang", "title": "Semi-Global Weighted Least Squares in Image Filtering", "comments": "Appearing in Proc. Int. Conf.Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving the global method of Weighted Least Squares (WLS) model in image\nfiltering is both time- and memory-consuming. In this paper, we present an\nalternative approximation in a time- and memory- efficient manner which is\ndenoted as Semi-Global Weighed Least Squares (SG-WLS). Instead of solving a\nlarge linear system, we propose to iteratively solve a sequence of subsystems\nwhich are one-dimensional WLS models. Although each subsystem is\none-dimensional, it can take two-dimensional neighborhood information into\naccount due to the proposed special neighborhood construction. We show such a\ndesirable property makes our SG-WLS achieve close performance to the original\ntwo-dimensional WLS model but with much less time and memory cost. While\nprevious related methods mainly focus on the 4-connected/8-connected\nneighborhood system, our SG-WLS can handle a more general and larger\nneighborhood system thanks to the proposed fast solution. We show such a\ngeneralization can achieve better performance than the 4-connected/8-connected\nneighborhood system in some applications. Our SG-WLS is $\\sim20$ times faster\nthan the WLS model. For an image of $M\\times N$, the memory cost of SG-WLS is\nat most at the magnitude of $max\\{\\frac{1}{M}, \\frac{1}{N}\\}$ of that of the\nWLS model. We show the effectiveness and efficiency of our SG-WLS in a range of\napplications. The code is publicly available at:\nhttps://github.com/wliusjtu/Semi-Global-Weighted-Least-Squares-in-Image-Filtering.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 02:04:37 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 10:44:45 GMT"}, {"version": "v3", "created": "Thu, 20 Jul 2017 01:40:48 GMT"}, {"version": "v4", "created": "Tue, 22 Sep 2020 10:52:45 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Xiaogang", ""], ["Shen", "Chuanhua", ""], ["Liu", "Zhi", ""], ["Yang", "Jie", ""]]}, {"id": "1705.01759", "submitter": "Hou-Ning Hu", "authors": "Hou-Ning Hu, Yen-Chen Lin, Ming-Yu Liu, Hsien-Tzu Cheng, Yung-Ju\n  Chang, Min Sun", "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360{\\deg}\n  Sports Video", "comments": "13 pages, 8 figures, To appear in CVPR 2017 as an Oral paper. The\n  first two authors contributed equally to this work.\n  https://aliensunmin.github.io/project/360video/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching a 360{\\deg} sports video requires a viewer to continuously select a\nviewing angle, either through a sequence of mouse clicks or head movements. To\nrelieve the viewer from this \"360 piloting\" task, we propose \"deep 360 pilot\"\n-- a deep learning-based agent for piloting through 360{\\deg} sports videos\nautomatically. At each frame, the agent observes a panoramic image and has the\nknowledge of previously selected viewing angles. The task of the agent is to\nshift the current viewing angle (i.e. action) to the next preferred one (i.e.,\ngoal). We propose to directly learn an online policy of the agent from data. We\nuse the policy gradient technique to jointly train our pipeline: by minimizing\n(1) a regression loss measuring the distance between the selected and ground\ntruth viewing angles, (2) a smoothness loss encouraging smooth transition in\nviewing angle, and (3) maximizing an expected reward of focusing on a\nforeground object. To evaluate our method, we build a new 360-Sports video\ndataset consisting of five sports domains. We train domain-specific agents and\nachieve the best performance on viewing angle selection accuracy and transition\nsmoothness compared to [51] and other baselines.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 09:26:58 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Hu", "Hou-Ning", ""], ["Lin", "Yen-Chen", ""], ["Liu", "Ming-Yu", ""], ["Cheng", "Hsien-Tzu", ""], ["Chang", "Yung-Ju", ""], ["Sun", "Min", ""]]}, {"id": "1705.02090", "submitter": "Kai Xu", "authors": "Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao Zhang, Leonidas\n  Guibas", "title": "GRASS: Generative Recursive Autoencoders for Shape Structures", "comments": "Corresponding author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2017) 36, 4, Article 52", "doi": "10.1145/3072959.3073613", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel neural network architecture for encoding and synthesis\nof 3D shapes, particularly their structures. Our key insight is that 3D shapes\nare effectively characterized by their hierarchical organization of parts,\nwhich reflects fundamental intra-shape relationships such as adjacency and\nsymmetry. We develop a recursive neural net (RvNN) based autoencoder to map a\nflat, unlabeled, arbitrary part layout to a compact code. The code effectively\ncaptures hierarchical structures of man-made 3D objects of varying structural\ncomplexities despite being fixed-dimensional: an associated decoder maps a code\nback to a full hierarchy. The learned bidirectional mapping is further tuned\nusing an adversarial setup to yield a generative model of plausible structures,\nfrom which novel structures can be sampled. Finally, our structure synthesis\nframework is augmented by a second trained module that produces fine-grained\npart geometry, conditioned on global and local structural context, leading to a\nfull generative pipeline for 3D shapes. We demonstrate that without\nsupervision, our network learns meaningful structural hierarchies adhering to\nperceptual grouping principles, produces compact codes which enable\napplications such as shape classification and partial matching, and supports\nshape synthesis and interpolation with significant variations in topology and\ngeometry.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:45:10 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 04:49:23 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Li", "Jun", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yumer", "Ersin", ""], ["Zhang", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1705.02422", "submitter": "Marcel Campen", "authors": "Marcel Campen, Denis Zorin", "title": "On Discrete Conformal Seamless Similarity Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for the computation of global discrete conformal\nparametrizations with prescribed global holonomy signatures for triangle meshes\nwas recently described in [Campen and Zorin 2017]. In this paper we provide a\ndetailed analysis of convergence and correctness of this algorithm. We\ngeneralize and extend ideas of [Springborn et al. 2008] to show a connection of\nthe algorithm to Newton's algorithm applied to solving the system of\nconstraints on angles in the parametric domain, and demonstrate that this\nsystem can be obtained as a gradient of a convex energy.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 00:25:31 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Campen", "Marcel", ""], ["Zorin", "Denis", ""]]}, {"id": "1705.02997", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Jun-Yan Zhu, Nima Khademi Kalantari, Alexei A. Efros,\n  Ravi Ramamoorthi", "title": "Light Field Video Capture Using a Learning-Based Hybrid Imaging System", "comments": "ACM Transactions on Graphics (Proceedings of SIGGRAPH 2017)", "journal-ref": null, "doi": "10.1145/3072959.3073614", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field cameras have many advantages over traditional cameras, as they\nallow the user to change various camera settings after capture. However,\ncapturing light fields requires a huge bandwidth to record the data: a modern\nlight field camera can only take three images per second. This prevents current\nconsumer light field cameras from capturing light field videos. Temporal\ninterpolation at such extreme scale (10x, from 3 fps to 30 fps) is infeasible\nas too much information will be entirely missing between adjacent frames.\nInstead, we develop a hybrid imaging system, adding another standard video\ncamera to capture the temporal information. Given a 3 fps light field sequence\nand a standard 30 fps 2D video, our system can then generate a full light field\nvideo at 30 fps. We adopt a learning-based approach, which can be decomposed\ninto two steps: spatio-temporal flow estimation and appearance estimation. The\nflow estimation propagates the angular information from the light field\nsequence to the 2D video, so we can warp input images to the target view. The\nappearance estimation then combines these warped images to output the final\npixels. The whole process is trained end-to-end using convolutional neural\nnetworks. Experimental results demonstrate that our algorithm outperforms\ncurrent video interpolation methods, enabling consumer light field videography,\nand making applications such as refocusing and parallax view generation\nachievable on videos for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:56:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Zhu", "Jun-Yan", ""], ["Kalantari", "Nima Khademi", ""], ["Efros", "Alexei A.", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1705.02999", "submitter": "Richard Zhang", "authors": "Richard Zhang, Jun-Yan Zhu, Phillip Isola, Xinyang Geng, Angela S.\n  Lin, Tianhe Yu, Alexei A. Efros", "title": "Real-Time User-Guided Image Colorization with Learned Deep Priors", "comments": "Accepted to SIGGRAPH 2017. Project page:\n  https://richzhang.github.io/ideepcolor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning approach for user-guided image colorization. The\nsystem directly maps a grayscale image, along with sparse, local user \"hints\"\nto an output colorization with a Convolutional Neural Network (CNN). Rather\nthan using hand-defined rules, the network propagates user edits by fusing\nlow-level cues along with high-level semantic information, learned from\nlarge-scale data. We train on a million images, with simulated user inputs. To\nguide the user towards efficient input selection, the system recommends likely\ncolors based on the input image and current user inputs. The colorization is\nperformed in a single feed-forward pass, enabling real-time use. Even with\nrandomly simulated user inputs, we show that the proposed system helps novice\nusers quickly create realistic colorizations, and offers large improvements in\ncolorization quality with just a minute of use. In addition, we demonstrate\nthat the framework can incorporate other user \"hints\" to the desired\ncolorization, showing an application to color histogram transfer. Our code and\nmodels are available at https://richzhang.github.io/ideepcolor.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 17:58:11 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Isola", "Phillip", ""], ["Geng", "Xinyang", ""], ["Lin", "Angela S.", ""], ["Yu", "Tianhe", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1705.03493", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev, Alexander Malyshev", "title": "Signal reconstruction via operator guiding", "comments": "5 pages, 8 figures. To appear in Proceedings of SampTA 2017: Sampling\n  Theory and Applications, 12th International Conference, July 3-7, 2017,\n  Tallinn, Estonia", "journal-ref": "IEEE Xplore: 2017 International Conference on Sampling Theory and\n  Applications (SampTA), Tallin, Estonia, 2017, pp. 630-634", "doi": "10.1109/SAMPTA.2017.8024424", "report-no": "MERL TR2017-087", "categories": "cs.CV cs.GR cs.IT cs.NA math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal reconstruction from a sample using an orthogonal projector onto a\nguiding subspace is theoretically well justified, but may be difficult to\npractically implement. We propose more general guiding operators, which\nincrease signal components in the guiding subspace relative to those in a\ncomplementary subspace, e.g., iterative low-pass edge-preserving filters for\nsuper-resolution of images. Two examples of super-resolution illustrate our\ntechnology: a no-flash RGB photo guided using a high resolution flash RGB\nphoto, and a depth image guided using a high resolution RGB photo.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 19:06:16 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Knyazev", "Andrew", ""], ["Malyshev", "Alexander", ""]]}, {"id": "1705.03531", "submitter": "David Barina", "authors": "Stanislav Svoboda, David Barina", "title": "New Transforms for JPEG Format", "comments": "preprint submitted to SCCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-dimensional discrete cosine transform (DCT) can be found in the heart\nof many image compression algorithms. Specifically, the JPEG format uses a\nlossy form of compression based on that transform. Since the standardization of\nthe JPEG, many other transforms become practical in lossy data compression.\nThis article aims to analyze the use of these transforms as the DCT replacement\nin the JPEG compression chain. Each transform is examined for different image\ndatasets and subsequently compared to other transforms using the peak\nsignal-to-noise ratio (PSNR). Our experiments show that an overlapping\nvariation of the DCT, the local cosine transform (LCT), overcame the original\nblock-wise transform at low bitrates. At high bitrates, the discrete wavelet\ntransform employing the Cohen-Daubechies-Feauveau 9/7 wavelet offers about the\nsame compression performance as the DCT.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:34:44 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Svoboda", "Stanislav", ""], ["Barina", "David", ""]]}, {"id": "1705.03811", "submitter": "Marco Livesu", "authors": "Marco Livesu and Stefano Ellero and Jon\\'as Mart\\`inez and Sylvain\n  Lefebvre and Marco Attene", "title": "From 3D Models to 3D Prints: an Overview of the Processing Pipeline", "comments": "European Union (EU); Horizon 2020; H2020-FoF-2015; RIA - Research and\n  Innovation action; Grant agreement N. 680448", "journal-ref": null, "doi": "10.1111/cgf.13147", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide diffusion of 3D printing technologies, geometric algorithms\nfor Additive Manufacturing are being invented at an impressive speed. Each\nsingle step, in particular along the Process Planning pipeline, can now count\non dozens of methods that prepare the 3D model for fabrication, while analysing\nand optimizing geometry and machine instructions for various objectives. This\nreport provides a classification of this huge state of the art, and elicits the\nrelation between each single algorithm and a list of desirable objectives\nduring Process Planning. The objectives themselves are listed and discussed,\nalong with possible needs for tradeoffs. Additive Manufacturing technologies\nare broadly categorized to explicitly relate classes of devices and supported\nfeatures. Finally, this report offers an analysis of the state of the art while\ndiscussing open and challenging problems from both an academic and an\nindustrial perspective.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:12:14 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 09:35:30 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Livesu", "Marco", ""], ["Ellero", "Stefano", ""], ["Mart\u00ecnez", "Jon\u00e1s", ""], ["Lefebvre", "Sylvain", ""], ["Attene", "Marco", ""]]}, {"id": "1705.04932", "submitter": "Shuchang Zhou", "authors": "Shuchang Zhou, Taihong Xiao, Yi Yang, Dieqiao Feng, Qinyao He, Weiran\n  He", "title": "GeneGAN: Learning Object Transfiguration and Attribute Subspace from\n  Unpaired Data", "comments": "Github: https://github.com/Prinsphield/GeneGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Transfiguration replaces an object in an image with another object\nfrom a second image. For example it can perform tasks like \"putting exactly\nthose eyeglasses from image A on the nose of the person in image B\". Usage of\nexemplar images allows more precise specification of desired modifications and\nimproves the diversity of conditional image generation. However, previous\nmethods that rely on feature space operations, require paired data and/or\nappearance models for training or disentangling objects from background. In\nthis work, we propose a model that can learn object transfiguration from two\nunpaired sets of images: one set containing images that \"have\" that kind of\nobject, and the other set being the opposite, with the mild constraint that the\nobjects be located approximately at the same place. For example, the training\ndata can be one set of reference face images that have eyeglasses, and another\nset of images that have not, both of which spatially aligned by face landmarks.\nDespite the weak 0/1 labels, our model can learn an \"eyeglasses\" subspace that\ncontain multiple representatives of different types of glasses. Consequently,\nwe can perform fine-grained control of generated images, like swapping the\nglasses in two images by swapping the projected components in the \"eyeglasses\"\nsubspace, to create novel images of people wearing eyeglasses.\n  Overall, our deterministic generative model learns disentangled attribute\nsubspaces from weakly labeled data by adversarial training. Experiments on\nCelebA and Multi-PIE datasets validate the effectiveness of the proposed model\non real world data, in generating images with specified eyeglasses, smiling,\nhair styles, and lighting conditions etc. The code is available online.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 08:59:36 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhou", "Shuchang", ""], ["Xiao", "Taihong", ""], ["Yang", "Yi", ""], ["Feng", "Dieqiao", ""], ["He", "Qinyao", ""], ["He", "Weiran", ""]]}, {"id": "1705.05016", "submitter": "Yong Khoo", "authors": "Yong Khoo", "title": "A Correspondence Relaxation Approach for 3D Shape Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for 3D shape reconstruction based on two\nexisting methods. A 3D reconstruction from a single photograph is introduced by\nboth papers: the first one uses a photograph and a set of existing 3D model to\ngenerate the 3D object in the photograph, while the second one uses a\nphotograph and a selected similar model to create the 3D object in the\nphotograph. According to their difference, we propose a relaxation based method\nfor more accurate correspondence establishment and shape recovery. The\nexperiment demonstrates promising results compared to the state-of-the-art work\non 3D shape estimation.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 19:02:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Khoo", "Yong", ""]]}, {"id": "1705.05138", "submitter": "Sebastian Boblest", "authors": "Grzegorz K. Karch, Filip Sadlo, Sebastian Boblest, Moritz Ertl,\n  Bernhard Weigand, Kelly Gaither, Thomas Ertl", "title": "Visualization of Feature Separation in Advected Scalar Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalar features in time-dependent fluid flow are traditionally visualized\nusing 3D representation, and their topology changes over time are often\nconveyed with abstract graphs. Using such techniques, however, the structural\ndetails of feature separation and the temporal evolution of features undergoing\ntopological changes are difficult to analyze. In this paper, we propose a novel\napproach for the spatio-temporal visualization of feature separation that\nsegments feature volumes into regions with respect to their contribution to\ndistinct features after separation. To this end, we employ particle-based\nfeature tracking to find volumetric correspondences between features at two\ndifferent instants of time. We visualize this segmentation by constructing mesh\nboundaries around each volume segment of a feature at the initial time that\ncorrespond to the separated features at the later time. To convey temporal\nevolution of the partitioning within the investigated time interval, we\ncomplement our approach with spatio-temporal separation surfaces. For the\napplication of our approach to multiphase flow, we additionally present a\nfeature-based corrector method to ensure phase-consistent particle\ntrajectories. The utility of our technique is demonstrated by application to\ntwo-phase (liquid-gas) and multi-component (liquid-liquid) flows where the\nscalar field represents the fraction of one of the phases.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 09:47:21 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 08:44:02 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Karch", "Grzegorz K.", ""], ["Sadlo", "Filip", ""], ["Boblest", "Sebastian", ""], ["Ertl", "Moritz", ""], ["Weigand", "Bernhard", ""], ["Gaither", "Kelly", ""], ["Ertl", "Thomas", ""]]}, {"id": "1705.05508", "submitter": "Yong Khoo", "authors": "Yong Khoo, Sang Chung", "title": "Automated Body Structure Extraction from Arbitrary 3D Mesh", "comments": null, "journal-ref": "Imaging and Graphics, 2017", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automated method for 3D character skeleton extraction\nthat can be applied for generic 3D shapes. Our work is motivated by the\nskeleton-based prior work on automatic rigging focused on skeleton extraction\nand can automatically aligns the extracted structure to fit the 3D shape of the\ngiven 3D mesh. The body mesh can be subsequently skinned based on the extracted\nskeleton and thus enables rigging process. In the experiment, we apply public\ndataset to drive the estimated skeleton from different body shapes, as well as\nthe real data obtained from 3D scanning systems. Satisfactory results are\nobtained compared to the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 02:58:44 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Khoo", "Yong", ""], ["Chung", "Sang", ""]]}, {"id": "1705.05893", "submitter": "Indrasen Bhattacharya", "authors": "Brett Kelly, Indrasen Bhattacharya, Maxim Shusteff, Robert M. Panas,\n  Hayden K. Taylor, Christopher M. Spadaccini", "title": "Computed Axial Lithography (CAL): Toward Single Step 3D Printing of\n  Arbitrary Geometries", "comments": "10 pages, 17 figure, ACM SIGGRAPH format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most additive manufacturing processes today operate by printing voxels (3D\npixels) serially point-by-point to build up a 3D part. In some more\nrecently-developed techniques, for example optical printing methods such as\nprojection stereolithography [Zheng et al. 2012], [Tumbleston et al. 2015],\nparts are printed layer-by-layer by curing full 2d (very thin in one dimension)\nlayers of the 3d part in each print step. There does not yet exist a technique\nwhich is able to print arbitrarily-defined 3D geometries in a single print\nstep. If such a technique existed, it could be used to expand the range of\nprintable geometries in additive manufacturing and relax constraints on factors\nsuch as overhangs in topology optimization. It could also vastly increase print\nspeed for 3D parts. In this work, we develop the principles for an approach for\nsingle exposure 3D printing of arbitrarily defined geometries. The approach,\ntermed Computed Axial Lithgography (CAL), is based on tomographic\nreconstruction, with mathematical optimization to generate a set of projections\nto optically define an arbitrary dose distribution within a target volume. We\ndemonstrate the potential ability of the technique to print 3D parts using a\nprototype CAL system based on sequential illumination from many angles. We also\npropose new hardware designs which will help us to realize true single-shot\narbitrary-geometry 3D CAL.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:56:58 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Kelly", "Brett", ""], ["Bhattacharya", "Indrasen", ""], ["Shusteff", "Maxim", ""], ["Panas", "Robert M.", ""], ["Taylor", "Hayden K.", ""], ["Spadaccini", "Christopher M.", ""]]}, {"id": "1705.06086", "submitter": "Sebastian Werner", "authors": "Sebastian Werner, Zdravko Velinov, Wenzel Jakob, Matthias B. Hullin", "title": "Scratch iridescence: Wave-optical rendering of diffractive surface\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surface of metal, glass and plastic objects is often characterized by\nmicroscopic scratches caused by manufacturing and/or wear. A closer look onto\nsuch scratches reveals iridescent colors with a complex dependency on viewing\nand lighting conditions. The physics behind this phenomenon is well understood;\nit is caused by diffraction of the incident light by surface features on the\norder of the optical wavelength. Existing analytic models are able to reproduce\nspatially unresolved microstructure such as the iridescent appearance of\ncompact disks and similar materials. Spatially resolved scratches, on the other\nhand, have proven elusive due to the highly complex wave-optical light\ntransport simulations needed to account for their appearance. In this paper, we\npropose a wave-optical shading model based on non-paraxial scalar diffraction\ntheory to render this class of effects. Our model expresses surface roughness\nas a collection of line segments. To shade a point on the surface, the\nindividual diffraction patterns for contributing scratch segments are computed\nanalytically and superimposed coherently. This provides natural transitions\nfrom localized glint-like iridescence to smooth BRDFs representing the\nsuperposition of many reflections at large viewing distances. We demonstrate\nthat our model is capable of recreating the overall appearance as well as\ncharacteristic detail effects observed on real-world examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 10:59:29 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Werner", "Sebastian", ""], ["Velinov", "Zdravko", ""], ["Jakob", "Wenzel", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1705.06148", "submitter": "Nadav Dym", "authors": "Nadav Dym, Haggai Maron and Yaron Lipman", "title": "DS++: A flexible, scalable and provably tight relaxation for matching\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence problems are often modelled as quadratic optimization problems\nover permutations. Common scalable methods for approximating solutions of these\nNP-hard problems are the spectral relaxation for non-convex energies and the\ndoubly stochastic (DS) relaxation for convex energies. Lately, it has been\ndemonstrated that semidefinite programming relaxations can have considerably\nimproved accuracy at the price of a much higher computational cost. We present\na convex quadratic programming relaxation which is provably stronger than both\nDS and spectral relaxations, with the same scalability as the DS relaxation.\nThe derivation of the relaxation also naturally suggests a projection method\nfor achieving meaningful integer solutions which improves upon the standard\nclosest-permutation projection. Our method can be easily extended to\noptimization over doubly stochastic matrices, partial or injective matching,\nand problems with additional linear constraints. We employ recent advances in\noptimization of linear-assignment type problems to achieve an efficient\nalgorithm for solving the convex relaxation.\n  We present experiments indicating that our method is more accurate than local\nminimization or competing relaxations for non-convex problems. We successfully\napply our algorithm to shape matching and to the problem of ordering images in\na grid, obtaining results which compare favorably with state of the art\nmethods. We believe our results indicate that our method should be considered\nthe method of choice for quadratic optimization over permutations.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 13:33:41 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Dym", "Nadav", ""], ["Maron", "Haggai", ""], ["Lipman", "Yaron", ""]]}, {"id": "1705.06250", "submitter": "Majid Masoumi", "authors": "Majid Masoumi and A. Ben Hamza", "title": "Shape Classification using Spectral Graph Wavelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral shape descriptors have been used extensively in a broad spectrum of\ngeometry processing applications ranging from shape retrieval and segmentation\nto classification. In this pa- per, we propose a spectral graph wavelet\napproach for 3D shape classification using the bag-of-features paradigm. In an\neffort to capture both the local and global geometry of a 3D shape, we present\na three-step feature description framework. First, local descriptors are\nextracted via the spectral graph wavelet transform having the Mexican hat\nwavelet as a generating ker- nel. Second, mid-level features are obtained by\nembedding lo- cal descriptors into the visual vocabulary space using the soft-\nassignment coding step of the bag-of-features model. Third, a global descriptor\nis constructed by aggregating mid-level fea- tures weighted by a geodesic\nexponential kernel, resulting in a matrix representation that describes the\nfrequency of appearance of nearby codewords in the vocabulary. Experimental\nresults on two standard 3D shape benchmarks demonstrate the effective- ness of\nthe proposed classification approach in comparison with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 01:23:55 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Masoumi", "Majid", ""], ["Hamza", "A. Ben", ""]]}, {"id": "1705.07108", "submitter": "Clara Callenberg", "authors": "Clara Callenberg, Felix Heide, Gordon Wetzstein, Matthias Hullin", "title": "Snapshot Difference Imaging using Time-of-Flight Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational photography encompasses a diversity of imaging techniques, but\none of the core operations performed by many of them is to compute image\ndifferences. An intuitive approach to computing such differences is to capture\nseveral images sequentially and then process them jointly. Usually, this\napproach leads to artifacts when recording dynamic scenes. In this paper, we\nintroduce a snapshot difference imaging approach that is directly implemented\nin the sensor hardware of emerging time-of-flight cameras. With a variety of\nexamples, we demonstrate that the proposed snapshot difference imaging\ntechnique is useful for direct-global illumination separation, for direct\nimaging of spatial and temporal image gradients, for direct depth edge imaging,\nand more.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:39:43 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Callenberg", "Clara", ""], ["Heide", "Felix", ""], ["Wetzstein", "Gordon", ""], ["Hullin", "Matthias", ""]]}, {"id": "1705.07118", "submitter": "Andre Mastmeyer", "authors": "Andre Mastmeyer, Dirk Fortmeier, Heinz Handels", "title": "Evaluation of Direct Haptic 4D Volume Rendering of Partially Segmented\n  Data for Liver Puncture Simulation", "comments": "15 pages, 16 figures, 1 tables, 11 equations, 39 references", "journal-ref": "Nature - Scientific Reports, Nature Publishing Group (NPG),\n  7(671), 2017", "doi": "10.1038/s41598-017-00746-z", "report-no": null, "categories": "cs.GR cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an evaluation study using a force feedback evaluation\nframework for a novel direct needle force volume rendering concept in the\ncontext of liver puncture simulation. PTC/PTCD puncture interventions targeting\nthe bile ducts have been selected to illustrate this concept. The haptic\nalgorithms of the simulator system are based on (1) partially segmented patient\nimage data and (2) a non-linear spring model effective at organ borders. The\nprimary aim is to quantitatively evaluate force errors caused by our patient\nmodeling approach, in comparison to haptic force output obtained from using\ngold-standard, completely manually-segmented data. The evaluation of the force\nalgorithms compared to a force output from fully manually segmented\ngold-standard patient models, yields a low mean of 0.12 N root mean squared\nforce error and up to 1.6 N for systematic maximum absolute errors. Force\nerrors were evaluated on 31,222 preplanned test paths from 10 patients. Only\ntwelve percent of the emitted forces along these paths were affected by errors.\nThis is the first study evaluating haptic algorithms with deformable virtual\npatients in silico. We prove haptic rendering plausibility on a very high\nnumber of test paths. Important errors are below just noticeable differences\nfor the hand-arm system.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:13:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Mastmeyer", "Andre", ""], ["Fortmeier", "Dirk", ""], ["Handels", "Heinz", ""]]}, {"id": "1705.07640", "submitter": "Leonid Keselman", "authors": "Stan Melax, Leonid Keselman, Sterling Orsten", "title": "Dynamics Based 3D Skeletal Hand Tracking", "comments": "Published in Graphics Interface 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the full skeletal pose of the hands and fingers is a challenging\nproblem that has a plethora of applications for user interaction. Existing\ntechniques either require wearable hardware, add restrictions to user pose, or\nrequire significant computation resources. This research explores a new\napproach to tracking hands, or any articulated model, by using an augmented\nrigid body simulation. This allows us to phrase 3D object tracking as a linear\ncomplementarity problem with a well-defined solution. Based on a depth sensor's\nsamples, the system generates constraints that limit motion orthogonal to the\nrigid body model's surface. These constraints, along with prior motion,\ncollision/contact constraints, and joint mechanics, are resolved with a\nprojected Gauss-Seidel solver. Due to camera noise properties and attachment\nerrors, the numerous surface constraints are impulse capped to avoid\noverpowering mechanical constraints. To improve tracking accuracy, multiple\nsimulations are spawned at each frame and fed a variety of heuristics,\nconstraints and poses. A 3D error metric selects the best-fit simulation,\nhelping the system handle challenging hand motions. Such an approach enables\nreal-time, robust, and accurate 3D skeletal tracking of a user's hand on a\nvariety of depth cameras, while only utilizing a single x86 CPU core for\nprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:01:39 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Melax", "Stan", ""], ["Keselman", "Leonid", ""], ["Orsten", "Sterling", ""]]}, {"id": "1705.10041", "submitter": "Arturo Deza", "authors": "Arturo Deza, Aditya Jonnalagadda, Miguel Eckstein", "title": "Towards Metamerism via Foveated Style Transfer", "comments": "Published at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of $\\textit{visual metamerism}$ is defined as finding a family of\nperceptually indistinguishable, yet physically different images. In this paper,\nwe propose our NeuroFovea metamer model, a foveated generative model that is\nbased on a mixture of peripheral representations and style transfer\nforward-pass algorithms. Our gradient-descent free model is parametrized by a\nfoveated VGG19 encoder-decoder which allows us to encode images in high\ndimensional space and interpolate between the content and texture information\nwith adaptive instance normalization anywhere in the visual field. Our\ncontributions include: 1) A framework for computing metamers that resembles a\nnoisy communication system via a foveated feed-forward encoder-decoder network\n-- We observe that metamerism arises as a byproduct of noisy perturbations that\npartially lie in the perceptual null space; 2) A perceptual optimization scheme\nas a solution to the hyperparametric nature of our metamer model that requires\ntuning of the image-texture tradeoff coefficients everywhere in the visual\nfield which are a consequence of internal noise; 3) An ABX psychophysical\nevaluation of our metamers where we also find that the rate of growth of the\nreceptive fields in our model match V1 for reference metamers and V2 between\nsynthesized samples. Our model also renders metamers at roughly a second,\npresenting a $\\times1000$ speed-up compared to the previous work, which allows\nfor tractable data-driven metamer experiments.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 05:38:20 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 04:42:43 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 22:37:48 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Deza", "Arturo", ""], ["Jonnalagadda", "Aditya", ""], ["Eckstein", "Miguel", ""]]}, {"id": "1705.10819", "submitter": "Joan Bruna", "authors": "Ilya Kostrikov, Zhongshi Jiang, Daniele Panozzo, Denis Zorin, Joan\n  Bruna", "title": "Surface Networks", "comments": null, "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study data-driven representations for three-dimensional triangle meshes,\nwhich are one of the prevalent objects used to represent 3D geometry. Recent\nworks have developed models that exploit the intrinsic geometry of manifolds\nand graphs, namely the Graph Neural Networks (GNNs) and its spectral variants,\nwhich learn from the local metric tensor via the Laplacian operator. Despite\noffering excellent sample complexity and built-in invariances, intrinsic\ngeometry alone is invariant to isometric deformations, making it unsuitable for\nmany applications. To overcome this limitation, we propose several upgrades to\nGNNs to leverage extrinsic differential geometry properties of\nthree-dimensional surfaces, increasing its modeling power.\n  In particular, we propose to exploit the Dirac operator, whose spectrum\ndetects principal curvature directions --- this is in stark contrast with the\nclassical Laplace operator, which directly measures mean curvature. We coin the\nresulting models \\emph{Surface Networks (SN)}. We prove that these models\ndefine shape representations that are stable to deformation and to\ndiscretization, and we demonstrate the efficiency and versatility of SNs on two\nchallenging tasks: temporal prediction of mesh deformations under non-linear\ndynamics and generative models using a variational autoencoder framework with\nencoders/decoders given by SNs.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 18:40:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:42:28 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Kostrikov", "Ilya", ""], ["Jiang", "Zhongshi", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""]]}, {"id": "1705.11050", "submitter": "David George", "authors": "David George, Xianghua Xie and Gary KL Tam", "title": "3D Mesh Segmentation via Multi-branch 1D Convolutional Neural Networks", "comments": "14 pages, 9 figures, 5 tables", "journal-ref": "Graphical Models 96 (2018) 1-10", "doi": "10.1016/j.gmod.2018.01.001", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in applying deep learning to 3D mesh\nsegmentation. We observe that 1) existing feature-based techniques are often\nslow or sensitive to feature resizing, 2) there are minimal comparative studies\nand 3) techniques often suffer from reproducibility issue. This study\ncontributes in two ways. First, we propose a novel convolutional neural network\n(CNN) for mesh segmentation. It uses 1D data, filters and a multi-branch\narchitecture for separate training of multi-scale features. Together with a\nnovel way of computing conformal factor (CF), our technique clearly\nout-performs existing work. Secondly, we publicly provide implementations of\nseveral deep learning techniques, namely, neural networks (NNs), autoencoders\n(AEs) and CNNs, whose architectures are at least two layers deep. The\nsignificance of this study is that it proposes a robust form of CF, offers a\nnovel and accurate CNN technique, and a comprehensive study of several deep\nlearning techniques for baseline comparison.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 12:10:32 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 13:29:19 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["George", "David", ""], ["Xie", "Xianghua", ""], ["Tam", "Gary KL", ""]]}]