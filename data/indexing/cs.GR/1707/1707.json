[{"id": "1707.00648", "submitter": "Fabien Pierre", "authors": "Johannes Persch, Fabien Pierre, Gabriele Steidl", "title": "Examplar-Based Face Colorization Using Image Morphing", "comments": "13 pages, 6 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorization of gray-scale images relies on prior color information.\nExamplar-based methods use a color image as source of such information. Then\nthe colors of the source image are transferred to the gray-scale image. In the\nliterature, this transfer is mainly guided by texture descriptors. Face images\nusually contain few texture so that the common approaches frequently fail. In\nthis paper we propose a new method based on image morphing. This technique is\nable to compute a correspondence map between images with similar shapes. It is\nbased on the geometric structure of the images rather than textures which is\nmore reliable for faces. Our numerical experiments show that our morphing based\napproach clearly outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 11:51:37 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Persch", "Johannes", ""], ["Pierre", "Fabien", ""], ["Steidl", "Gabriele", ""]]}, {"id": "1707.01170", "submitter": "Franz Georg Fuchs", "authors": "Franz G. Fuchs, Oliver J. D. Barrowclough, Jon M. Hjelmervik, Heidi E.\n  I. Dahl", "title": "Direct interactive visualization of locally refined spline volumes for\n  scalar and vector fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach enabling interactive visualization of volumetric\nLocally Refined B-splines (LR-splines). To this end we propose a highly\nefficient algorithm for direct visualization of scalar and vector fields given\nby an LR-spline. In both cases, our main contribution to achieve interactive\nframe rates is an acceleration structure for fast element look-up and a change\nof basis for efficient evaluation. To further improve the efficiency, we\npresent a heuristic for adaptive sampling distance for the numerical\nintegration. A comparison with existing adaptive approaches is performed. The\nalgorithms are designed to fully utilize modern graphics processing unit (GPU)\ncapabilities. Important applications where LR-spline volumes emerge are given\nfor instance by approximation of large-scale simulation and sensor data, and\nIsogeometric Analysis (IGA). We showcase interactive rendering achieved by our\napproach on different representative use cases, stemming from simulations of\nwind flow around a telescope, Magnetic Resonance (MR) imaging of a human brain,\nand simulations of a fluidized bed used for mixing and coating particles in\nindustrial processes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 22:47:40 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 11:33:32 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Fuchs", "Franz G.", ""], ["Barrowclough", "Oliver J. D.", ""], ["Hjelmervik", "Jon M.", ""], ["Dahl", "Heidi E. I.", ""]]}, {"id": "1707.01732", "submitter": "Ran Dong", "authors": "Ran Dong, Dongsheng Cai, Nobuyoshi Asai", "title": "Nonlinear dance motion analysis and motion editing using Hilbert-Huang\n  transform", "comments": "6 pages, 10 figures, Computer Graphics International 2017, Conference\n  short paper", "journal-ref": null, "doi": "10.1145/3095140.3095175", "report-no": "Article No. 35", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motions (especially dance motions) are very noisy, and it is hard to\nanalyze and edit the motions. To resolve this problem, we propose a new method\nto decompose and modify the motions using the Hilbert-Huang transform (HHT).\nFirst, HHT decomposes a chromatic signal into \"monochromatic\" signals that are\nthe so-called Intrinsic Mode Functions (IMFs) using an Empirical Mode\nDecomposition (EMD) [6]. After applying the Hilbert Transform to each IMF, the\ninstantaneous frequencies of the \"monochromatic\" signals can be obtained. The\nHHT has the advantage to analyze non-stationary and nonlinear signals such as\nhuman-joint-motions over FFT or Wavelet transform.\n  In the present paper, we propose a new framework to analyze and extract some\nnew features from a famous Japanese threesome pop singer group called\n\"Perfume\", and compare it with Waltz and Salsa dance. Using the EMD, their\ndance motions can be decomposed into motion (choreographic) primitives or IMFs.\nTherefore we can scale, combine, subtract, exchange, and modify those IMFs, and\ncan blend them into new dance motions self-consistently. Our analysis and\nframework can lead to a motion editing and blending method to create a new\ndance motion from different dance motions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 11:18:42 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Dong", "Ran", ""], ["Cai", "Dongsheng", ""], ["Asai", "Nobuyoshi", ""]]}, {"id": "1707.02596", "submitter": "Simone Melzi", "authors": "Simone Melzi, Emanuele Rodol\\`a, Umberto Castellani, Michael M.\n  Bronstein", "title": "Localized Manifold Harmonics for Spectral Shape Analysis", "comments": "Accepted to Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Laplacian eigenfunctions is ubiquitous in a wide range of computer\ngraphics and geometry processing applications. In particular, Laplacian\neigenbases allow generalizing the classical Fourier analysis to manifolds. A\nkey drawback of such bases is their inherently global nature, as the Laplacian\neigenfunctions carry geometric and topological structure of the entire\nmanifold. In this paper, we introduce a new framework for local spectral shape\nanalysis. We show how to efficiently construct localized orthogonal bases by\nsolving an optimization problem that in turn can be posed as the\neigendecomposition of a new operator obtained by a modification of the standard\nLaplacian. We study the theoretical and computational aspects of the proposed\nframework and showcase our new construction on the classical problems of shape\napproximation and correspondence. We obtain significant improvement compared to\nclassical Laplacian eigenbases as well as other alternatives for constructing\nlocalized bases.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 16:03:30 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 09:18:05 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""], ["Castellani", "Umberto", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1707.02880", "submitter": "Michael Gharbi", "authors": "Micha\\\"el Gharbi, Jiawen Chen, Jonathan T. Barron, Samuel W. Hasinoff,\n  and Fr\\'edo Durand", "title": "Deep Bilateral Learning for Real-Time Image Enhancement", "comments": "12 pages, 14 figures, Siggraph 2017", "journal-ref": "ACM Trans. Graph. 36, 4, Article 118 (2017)", "doi": "10.1145/3072959.3073592", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance is a critical challenge in mobile image processing. Given a\nreference imaging pipeline, or even human-adjusted pairs of images, we seek to\nreproduce the enhancements and enable real-time evaluation. For this, we\nintroduce a new neural network architecture inspired by bilateral grid\nprocessing and local affine color transforms. Using pairs of input/output\nimages, we train a convolutional neural network to predict the coefficients of\na locally-affine model in bilateral space. Our architecture learns to make\nlocal, global, and content-dependent decisions to approximate the desired image\ntransformation. At runtime, the neural network consumes a low-resolution\nversion of the input image, produces a set of affine transformations in\nbilateral space, upsamples those transformations in an edge-preserving fashion\nusing a new slicing node, and then applies those upsampled transformations to\nthe full-resolution image. Our algorithm processes high-resolution images on a\nsmartphone in milliseconds, provides a real-time viewfinder at 1080p\nresolution, and matches the quality of state-of-the-art approximation\ntechniques on a large class of image operators. Unlike previous work, our model\nis trained off-line from data and therefore does not require access to the\noriginal operator at runtime. This allows our model to learn complex,\nscene-dependent transformations for which no reference implementation is\navailable, such as the photographic edits of a human retoucher.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 14:34:06 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 19:26:08 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Gharbi", "Micha\u00ebl", ""], ["Chen", "Jiawen", ""], ["Barron", "Jonathan T.", ""], ["Hasinoff", "Samuel W.", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1707.04348", "submitter": "Alec Jacobson", "authors": "Oded Stein and Eitan Grinspun and Max Wardetzky and Alec Jacobson", "title": "Natural Boundary Conditions for Smoothing in Geometry Processing", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geometry processing, smoothness energies are commonly used to model\nscattered data interpolation, dense data denoising, and regularization during\nshape optimization. The squared Laplacian energy is a popular choice of energy\nand has a corresponding standard implementation: squaring the discrete\nLaplacian matrix. For compact domains, when values along the boundary are not\nknown in advance, this construction bakes in low-order boundary conditions.\nThis causes the geometric shape of the boundary to strongly bias the solution.\nFor many applications, this is undesirable. Instead, we propose using the\nsquared Frobenious norm of the Hessian as a smoothness energy. Unlike the\nsquared Laplacian energy, this energy's natural boundary conditions (those that\nbest minimize the energy) correspond to meaningful high-order boundary\nconditions. These boundary conditions model free boundaries where the shape of\nthe boundary should not bias the solution locally. Our analysis begins in the\nsmooth setting and concludes with discretizations using finite-differences on\n2D grids or mixed finite elements for triangle meshes. We demonstrate the core\nbehavior of the squared Hessian as a smoothness energy for various tasks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 22:56:46 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Stein", "Oded", ""], ["Grinspun", "Eitan", ""], ["Wardetzky", "Max", ""], ["Jacobson", "Alec", ""]]}, {"id": "1707.04805", "submitter": "Takayuki Itoh", "authors": "Shiho Furuya, Takayuki Itoh", "title": "A Streamline Selection Technique Overlaying with Isosurfaces", "comments": "TopoInVis2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of scalar and vector visualization has been an interesting topic.\nThis paper presents a technique to appropriately select and display multiple\nstreamlines while overlaying with isosurfaces, aiming an integrated scalar and\nvector field visualization. The technique visualizes a scalar field by multiple\nsemitransparent isosurfaces, and a vector field by multiple streamlines, while\nthe technique adequately selects the streamlines considering reduction of\ncluttering among the isosurfaces and streamlines. The technique first selects\nand renders isosurfaces, and then generates large number of streamlines from\nrandomly selected seed points. The technique evaluates each of the streamlines\naccording to their shapes on a 2D display space, distances to critical points\nof the given vector fields, and occlusion by isosurfaces. It then selects the\nspecified number of highly evaluated streamlines. As a result, we can visualize\nboth scalar and vector fields as a set of view-independently selected\nisosurfaces and view-dependently selected streamlines.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 01:42:41 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Furuya", "Shiho", ""], ["Itoh", "Takayuki", ""]]}, {"id": "1707.05754", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li, Avinash S. Nair, Shree K. Nayar, Changxi Zheng", "title": "AirCode: Unobtrusive Physical Tags for Digital Fabrication", "comments": "ACM UIST 2017 Technical Papers", "journal-ref": null, "doi": "10.1145/3126594.3126635", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AirCode, a technique that allows the user to tag physically\nfabricated objects with given information. An AirCode tag consists of a group\nof carefully designed air pockets placed beneath the object surface. These air\npockets are easily produced during the fabrication process of the object,\nwithout any additional material or postprocessing. Meanwhile, the air pockets\naffect only the scattering light transport under the surface, and thus are hard\nto notice to our naked eyes. But, by using a computational imaging method, the\ntags become detectable. We present a tool that automates the design of air\npockets for the user to encode information. AirCode system also allows the user\nto retrieve the information from captured images via a robust decoding\nalgorithm. We demonstrate our tagging technique with applications for metadata\nembedding, robotic grasping, as well as conveying object affordances.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:27:16 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 22:34:53 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Dingzeyu", ""], ["Nair", "Avinash S.", ""], ["Nayar", "Shree K.", ""], ["Zheng", "Changxi", ""]]}, {"id": "1707.05882", "submitter": "Sumanta Pattanaik", "authors": "Charly Collin, Sumanta Pattanaik", "title": "GPU accelerated computation of Polarized Subsurface BRDF for Flat\n  Particulate Layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BRDF of most real world materials has two components, the surface BRDF due to\nthe light reflecting at the surface of the material and the subsurface BRDF due\nto the light entering and going through many scattering events inside the\nmaterial. Each of these events modifies light's path, power, polarization\nstate. Computing polarized subsurface BRDF of a material requires simulating\nthe light transport inside the material. The transport of polarized light is\nmodeled by the Vector Radiative Transfer Equation (VRTE), an\nintegro-differential equation. Computing solution to that equation is\nexpensive. The Discrete Ordinate Method (DOM) is a common approach to solving\nthe VRTE. Such solvers are very time consuming for complex uses such as BRDF\ncomputation, where one must solve VRTE for surface radiance distribution due to\nlight incident from every direction of the hemisphere above the surface. In\nthis paper, we present a GPU based DOM solution of the VRTE to expedite the\nsubsurface BRDF computation. As in other DOM based solutions, our solution is\nbased on Fourier expansions of the phase function and the radiance function.\nThis allows us to independently solve the VRTE for each order of expansion. We\ntake advantage of those repetitions and of the repetitions in each of the\nsub-steps of the solution process. Our solver is implemented to run mainly on\ngraphics hardware using the OpenCL library and runs up to seven times faster\nthan its CPU equivalent, allowing the computation of subsurface BRDF in a\nmatter of minutes. We compute and present the subsurface BRDF lobes due to\npowders and paints of a few materials. We also show the rendering of objects\nwith the computed BRDF. The solver is available for public use through the\nauthors' web site.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 22:20:12 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Collin", "Charly", ""], ["Pattanaik", "Sumanta", ""]]}, {"id": "1707.06267", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Subhransu Maji, Rui Wang", "title": "Shape Generation using Spatially Partitioned Point Clouds", "comments": "To appear at BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate 3D shapes using point clouds. Given a\npoint-cloud representation of a 3D shape, our method builds a kd-tree to\nspatially partition the points. This orders them consistently across all\nshapes, resulting in reasonably good correspondences across all shapes. We then\nuse PCA analysis to derive a linear shape basis across the spatially\npartitioned points, and optimize the point ordering by iteratively minimizing\nthe PCA reconstruction error. Even with the spatial sorting, the point clouds\nare inherently noisy and the resulting distribution over the shape coefficients\ncan be highly multi-modal. We propose to use the expressive power of neural\nnetworks to learn a distribution over the shape coefficients in a\ngenerative-adversarial framework. Compared to 3D shape generative models\ntrained on voxel-representations, our point-based method is considerably more\nlight-weight and scalable, with little loss of quality. It also outperforms\nsimpler linear factor models such as Probabilistic PCA, both qualitatively and\nquantitatively, on a number of categories from the ShapeNet dataset.\nFurthermore, our method can easily incorporate other point attributes such as\nnormal and color information, an additional advantage over voxel-based\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 19:32:47 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Gadelha", "Matheus", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1707.06375", "submitter": "Zhaoliang Lun", "authors": "Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji,\n  Rui Wang", "title": "3D Shape Reconstruction from Sketches via Multi-view Convolutional\n  Networks", "comments": "3DV 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for reconstructing 3D shapes from 2D sketches in the form\nof line drawings. Our method takes as input a single sketch, or multiple\nsketches, and outputs a dense point cloud representing a 3D reconstruction of\nthe input sketch(es). The point cloud is then converted into a polygon mesh. At\nthe heart of our method lies a deep, encoder-decoder network. The encoder\nconverts the sketch into a compact representation encoding shape information.\nThe decoder converts this representation into depth and normal maps capturing\nthe underlying surface from several output viewpoints. The multi-view maps are\nthen consolidated into a 3D point cloud by solving an optimization problem that\nfuses depth and normals across all viewpoints. Based on our experiments,\ncompared to other methods, such as volumetric networks, our architecture offers\nseveral advantages, including more faithful reconstruction, higher output\nsurface resolution, better preservation of topology and shape structure.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 05:05:26 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 18:38:22 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 05:18:30 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Lun", "Zhaoliang", ""], ["Gadelha", "Matheus", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1707.06683", "submitter": "Paul Rosen", "authors": "Mustafa Hajij, Bei Wang, Carlos Scheidegger, Paul Rosen", "title": "Visual Detection of Structural Changes in Time-Varying Graphs Using\n  Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis is an emerging area in exploratory data analysis\nand data mining. Its main tool, persistent homology, has become a popular\ntechnique to study the structure of complex, high-dimensional data. In this\npaper, we propose a novel method using persistent homology to quantify\nstructural changes in time-varying graphs. Specifically, we transform each\ninstance of the time-varying graph into metric spaces, extract topological\nfeatures using persistent homology, and compare those features over time. We\nprovide a visualization that assists in time-varying graph exploration and\nhelps to identify patterns of behavior within the data. To validate our\napproach, we conduct several case studies on real world data sets and show how\nour method can find cyclic patterns, deviations from those patterns, and\none-time events in time-varying graphs. We also examine whether\npersistence-based similarity measure as a graph metric satisfies a set of\nwell-established, desirable properties for graph metrics.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:13:55 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 03:30:17 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hajij", "Mustafa", ""], ["Wang", "Bei", ""], ["Scheidegger", "Carlos", ""], ["Rosen", "Paul", ""]]}, {"id": "1707.07070", "submitter": "Yu Wang", "authors": "Yu Wang, Mirela Ben-Chen, Iosif Polterovich, Justin Solomon", "title": "Steklov Spectral Geometry for Extrinsic Shape Analysis", "comments": "Additional experiments added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using the Dirichlet-to-Neumann operator as an extrinsic\nalternative to the Laplacian for spectral geometry processing and shape\nanalysis. Intrinsic approaches, usually based on the Laplace-Beltrami operator,\ncannot capture the spatial embedding of a shape up to rigid motion, and many\nprevious extrinsic methods lack theoretical justification. Instead, we consider\nthe Steklov eigenvalue problem, computing the spectrum of the\nDirichlet-to-Neumann operator of a surface bounding a volume. A remarkable\nproperty of this operator is that it completely encodes volumetric geometry. We\nuse the boundary element method (BEM) to discretize the operator, accelerated\nby hierarchical numerical schemes and preconditioning; this pipeline allows us\nto solve eigenvalue and linear problems on large-scale meshes despite the\ndensity of the Dirichlet-to-Neumann discretization. We further demonstrate that\nour operators naturally fit into existing frameworks for geometry processing,\nmaking a shift from intrinsic to extrinsic geometry as simple as substituting\nthe Laplace-Beltrami operator with the Dirichlet-to-Neumann operator.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 23:28:53 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 20:18:25 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Wang", "Yu", ""], ["Ben-Chen", "Mirela", ""], ["Polterovich", "Iosif", ""], ["Solomon", "Justin", ""]]}, {"id": "1707.08323", "submitter": "Jianchao Tan", "authors": "Jianchao Tan, Stephen DiVerdi, Jingwan Lu, Yotam Gingold", "title": "Pigmento: Pigment-Based Image Analysis and Editing", "comments": "add copyright to images; add acknowledgements, is accepted by IEEE\n  Transactions on Visualization and Computer Graphics (IEEE TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The colorful appearance of a physical painting is determined by the\ndistribution of paint pigments across the canvas, which we model as a per-pixel\nmixture of a small number of pigments with multispectral absorption and\nscattering coefficients. We present an algorithm to efficiently recover this\nstructure from an RGB image, yielding a plausible set of pigments and a low RGB\nreconstruction error. We show that under certain circumstances we are able to\nrecover pigments that are close to ground truth, while in all cases our results\nare always plausible. Using our decomposition, we repose standard digital image\nediting operations as operations in pigment space rather than RGB, with\ninterestingly novel results. We demonstrate tonal adjustments, selection\nmasking, cut-copy-paste, recoloring, palette summarization, and edge\nenhancement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 08:50:14 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 21:11:36 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 22:42:54 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Tan", "Jianchao", ""], ["DiVerdi", "Stephen", ""], ["Lu", "Jingwan", ""], ["Gingold", "Yotam", ""]]}, {"id": "1707.08358", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni, Eric Heitz", "title": "Notes on optimal approximations for importance sampling", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we derive optimal conditions for building function\napproximations that minimize variance when used as importance sampling\nestimators for Monte Carlo integration problems. Particularly, we study the\nproblem of finding the optimal projection $g$ of an integrand $f$ onto certain\nclasses of piecewise constant functions, in order to minimize the variance of\nthe unbiased importance sampling estimator $E_g[f/g]$, as well as the related\nproblem of finding optimal mixture weights to approximate and importance sample\na target mixture distribution $f = \\sum_i \\alpha_i f_i$ with components $f_i$\nin a family $\\mathcal{F}$, through a corresponding mixture of importance\nsampling densities $g_i$ that are only approximately proportional to $f_i$. We\nfurther show that in both cases the optimal projection is different from the\ncommonly used $\\ell_1$ projection, and provide an intuitive explanation for the\ndifference.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:26:08 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 10:57:27 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Pantaleoni", "Jacopo", ""], ["Heitz", "Eric", ""]]}, {"id": "1707.08360", "submitter": "Michael Rabinovich", "authors": "Michael Rabinovich, Tim Hoffmann, Olga Sorkine-Hornung", "title": "Discrete Geodesic Nets for Modeling Developable Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a discrete theory for modeling developable surfaces as\nquadrilateral meshes satisfying simple angle constraints. The basis of our\nmodel is a lesser known characterization of developable surfaces as manifolds\nthat can be parameterized through orthogonal geodesics. Our model is simple,\nlocal, and, unlike previous works, it does not directly encode the surface\nrulings. This allows us to model continuous deformations of discrete\ndevelopable surfaces independently of their decomposition into torsal and\nplanar patches or the surface topology. We prove and experimentally demonstrate\nstrong ties to smooth developable surfaces, including a theorem stating that\nevery sampling of the smooth counterpart satisfies our constraints up to second\norder. We further present an extension of our model that enables a local\ndefinition of discrete isometry. We demonstrate the effectiveness of our\ndiscrete model in a developable surface editing system, as well as computation\nof an isometric interpolation between isometric discrete developable shapes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:30:34 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Rabinovich", "Michael", ""], ["Hoffmann", "Tim", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1707.08390", "submitter": "Johanna Delanoy", "authors": "Johanna Delanoy, Mathieu Aubry, Phillip Isola, Alexei A. Efros, Adrien\n  Bousseau", "title": "3D Sketching using Multi-View Deep Volumetric Prediction", "comments": "See our accompanying video on https://youtu.be/DGIYzmlm2pQ, networks\n  and databases available at https://ns.inria.fr/d3/3DSketching/. To appear in\n  PACMCGIT", "journal-ref": null, "doi": "10.1145/3203197", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch-based modeling strives to bring the ease and immediacy of drawing to\nthe 3D world. However, while drawings are easy for humans to create, they are\nvery challenging for computers to interpret due to their sparsity and\nambiguity. We propose a data-driven approach that tackles this challenge by\nlearning to reconstruct 3D shapes from one or more drawings. At the core of our\napproach is a deep convolutional neural network (CNN) that predicts occupancy\nof a voxel grid from a line drawing. This CNN provides us with an initial 3D\nreconstruction as soon as the user completes a single drawing of the desired\nshape. We complement this single-view network with an updater CNN that refines\nan existing prediction given a new drawing of the shape created from a novel\nviewpoint. A key advantage of our approach is that we can apply the updater\niteratively to fuse information from an arbitrary number of viewpoints, without\nrequiring explicit stroke correspondences between the drawings. We train both\nCNNs by rendering synthetic contour drawings from hand-modeled shape\ncollections as well as from procedurally-generated abstract shapes. Finally, we\nintegrate our CNNs in a minimal modeling interface that allows users to\nseamlessly draw an object, rotate it to see its 3D reconstruction, and refine\nit by re-drawing from another vantage point using the 3D reconstruction as\nguidance. The main strengths of our approach are its robustness to freehand\nbitmap drawings, its ability to adapt to different object categories, and the\ncontinuum it offers between single-view and multi-view sketch-based modeling.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:49:48 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 06:24:49 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 20:56:46 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 12:24:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Delanoy", "Johanna", ""], ["Aubry", "Mathieu", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""], ["Bousseau", "Adrien", ""]]}, {"id": "1707.09123", "submitter": "Yong Wang", "authors": "Yong Wang, Huai-yu Wu", "title": "Research on Shape Mapping of 3D Mesh Models based on Hidden Markov\n  Random Field and EM Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to establish the matching (or corresponding) between two different 3D\nshapes is a classical problem. This paper focused on the research on shape\nmapping of 3D mesh models, and proposed a shape mapping algorithm based on\nHidden Markov Random Field and EM algorithm, as introducing a hidden state\nrandom variable associated with the adjacent blocks of shape matching when\nestablishing HMRF. This algorithm provides a new theory and method to ensure\nthe consistency of the edge data of adjacent blocks, and the experimental\nresults show that the algorithm in this paper has a great improvement on the\nshape mapping of 3D mesh models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 07:03:29 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Wang", "Yong", ""], ["Wu", "Huai-yu", ""]]}, {"id": "1707.09366", "submitter": "Vaclav Skala", "authors": "Rongjiang Pan, Vaclav Skala", "title": "Continuous Global Optimization in Surface Reconstruction from an\n  Oriented Point Cloud", "comments": null, "journal-ref": "Computer Aided Design, Vol.43, No.8, pp.896-901, Elsevier, ISSN\n  0010-4485, 2011", "doi": "10.1016/j.cad.2011.03.005", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous global optimization method to the field of surface\nreconstruction from discrete noisy cloud of points with weak information on\norientation. The proposed method uses an energy functional combining flux-based\ndata-fit measures and a regularization term. A continuous convex relaxation\nscheme assures the global minima of the geometric surface functional. The\nreconstructed surface is implicitly represented by the binary segmentation of\nvertices of a 3D uniform grid and a triangulated surface can be obtained by\nextracting an appropriate isosurface. Unlike the discrete graph-cut solution,\nthe continuous global optimization entails advantages like memory requirements,\nreduction of metrication errors for geometric quantities, allowing globally\noptimal surface reconstruction at higher grid resolutions. We demonstrate the\nperformance of the proposed method on several oriented point clouds captured by\nlaser scanners. Experimental results confirm that our approach is robust to\nnoise, large holes and non-uniform sampling density under the condition of very\ncoarse orientation information.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 08:45:37 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Pan", "Rongjiang", ""], ["Skala", "Vaclav", ""]]}, {"id": "1707.09405", "submitter": "Qifeng Chen", "authors": "Qifeng Chen and Vladlen Koltun", "title": "Photographic Image Synthesis with Cascaded Refinement Networks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to synthesizing photographic images conditioned on\nsemantic layouts. Given a semantic label map, our approach produces an image\nwith photographic appearance that conforms to the input layout. The approach\nthus functions as a rendering engine that takes a two-dimensional semantic\nspecification of the scene and produces a corresponding photographic image.\nUnlike recent and contemporaneous work, our approach does not rely on\nadversarial training. We show that photographic images can be synthesized from\nsemantic layouts by a single feedforward network with appropriate structure,\ntrained end-to-end with a direct regression objective. The presented approach\nscales seamlessly to high resolutions; we demonstrate this by synthesizing\nphotographic images at 2-megapixel resolution, the full resolution of our\ntraining data. Extensive perceptual experiments on datasets of outdoor and\nindoor scenes demonstrate that images synthesized by the presented approach are\nconsiderably more realistic than alternative approaches. The results are shown\nin the supplementary video at https://youtu.be/0fhUJT21-bs\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 20:24:44 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Chen", "Qifeng", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1707.09432", "submitter": "Jo\\~ao Miguel Cunha", "authors": "Jo\\~ao Miguel Cunha, Pedro Martins, Am\\'ilcar Cardoso, Penousal\n  Machado", "title": "Generation of concept-representative symbols", "comments": "cite as \"Cunha, J. M., Martins, P., Cardoso, A., & Machado, P.\n  (2015). Generation of Concept-Representative Symbols. In ICCBR (Workshops).\",\n  Computational creativity, Computational generation, Concept representation,\n  Visual representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual representation of concepts or ideas through the use of simple\nshapes has always been explored in the history of Humanity, and it is believed\nto be the origin of writing. We focus on computational generation of visual\nsymbols to represent concepts. We aim to develop a system that uses background\nknowledge about the world to find connections among concepts, with the goal of\ngenerating symbols for a given concept. We are also interested in exploring the\nsystem as an approach to visual dissociation and visual conceptual blending.\nThis has a great potential in the area of Graphic Design as a tool to both\nstimulate creativity and aid in brainstorming in projects such as logo,\npictogram or signage design.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 22:33:01 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Cunha", "Jo\u00e3o Miguel", ""], ["Martins", "Pedro", ""], ["Cardoso", "Am\u00edlcar", ""], ["Machado", "Penousal", ""]]}, {"id": "1707.09629", "submitter": "Christos Mousas", "authors": "Christos Ouzounis, Alex Kilias, Christos Mousas", "title": "Kernel Projection of Latent Structures Regression for Facial Animation\n  Retargeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by kernel methods that have been used extensively in achieving\nefficient facial animation retargeting, this paper presents a solution to\nretargeting facial animation in virtual character's face model based on the\nkernel projection of latent structure (KPLS) regression between semantically\nsimilar facial expressions. Specifically, a given number of corresponding\nsemantically similar facial expressions are projected into the latent space. By\nusing the Nonlinear Iterative Partial Least Square method, decomposition of the\nlatent variables is achieved. Finally, the KPLS is achieved by solving a\nkernalized version of the eigenvalue problem. By evaluating our methodology\nwith other kernel-based solutions, the efficiency of the presented methodology\nin transferring facial animation to face models with different morphological\nvariations is demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:12:52 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Ouzounis", "Christos", ""], ["Kilias", "Alex", ""], ["Mousas", "Christos", ""]]}, {"id": "1707.09713", "submitter": "Laird Robert Hocking", "authors": "L. Robert Hocking, Thomas Holding, and Carola-Bibiane Schoenlieb", "title": "Numerical analysis of shell-based geometric image inpainting algorithms\n  and their semi-implicit extension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a class of fast geometric image inpainting methods\nbased on the idea of filling the inpainting domain in successive shells from\nits boundary inwards. Image pixels are filled by assigning them a color equal\nto a weighted average of their already filled neighbors. However, there is\nflexibility in terms of the order in which pixels are filled, the weights used\nfor averaging, and the neighborhood that is averaged over. Varying these\ndegrees of freedom leads to different algorithms, and indeed the literature\ncontains several methods falling into this general class. All of them are very\nfast, but at the same time all of them leave undesirable artifacts such as\n\"kinking\" (bending) or blurring of extrapolated isophotes. Our objective in\nthis paper is to build a theoretical model, based on a continuum limit and a\nconnection to stopped random walks, in order to understand why these artifacts\noccur and what, if anything, can be done about them. At the same time, we\nconsider a semi-implicit extension in which pixels in a given shell are solved\nfor simultaneously by solving a linear system. We prove (within the continuum\nlimit) that this extension is able to completely eliminate kinking artifacts,\nwhich we also prove must always be present in the direct method. Although our\nanalysis makes the strong assumption of a square inpainting domain, it makes\nweak smoothness assumptions and is thus applicable to the low regularity\ninherent in images.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 03:57:12 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hocking", "L. Robert", ""], ["Holding", "Thomas", ""], ["Schoenlieb", "Carola-Bibiane", ""]]}, {"id": "1707.09839", "submitter": "Alexandre B", "authors": "Alexandre Bali", "title": "Superposition de calques monochromes d'opacit\\'es variables", "comments": "Poorly written article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a monochrome layer $x$ of opacity $0\\le o_x\\le1 $ placed on another\nmonochrome layer of opacity 1, the result given by the standard formula is\n$$\\small\\Pi\\left({\\bf\nC}_\\varphi\\right)=1+\\sum_{n=1}^2\\left(2-n-(-1)^no_{\\chi(\\varphi+1)}\\right)\\left(\\chi(n+\\varphi-1)-o_{\\chi(n+\\varphi-1)}\\right),$$\nthe formula being of course explained in detail in this paper. We will\neventually deduce a very simple theorem, generalize it and then see its\nvalidity with alternative formulas to this standard containing the same main\nproperties here exposed.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:16:06 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 13:08:46 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 11:14:09 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Bali", "Alexandre", ""]]}]