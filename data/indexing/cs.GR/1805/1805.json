[{"id": "1805.01375", "submitter": "Tim Kuipers", "authors": "Tim Kuipers, Willemijn Elkhuizen, Jouke Verlinden, Eugeni Doubrovski", "title": "Hatching for 3D prints: line-based halftoning for dual extrusion fused\n  deposition modeling", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.cag.2018.04.006", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents a halftoning technique to manufacture 3D objects with the\nappearance of continuous grayscale imagery for Fused Deposition Modeling (FDM)\nprinters. While droplet-based dithering is a common halftoning technique, this\nis not applicable to FDM printing, since FDM builds up objects by extruding\nmaterial in semi-continuous paths. The line-based halftoning principle called\n'hatching' is applied to the line patterns naturally occuring in FDM prints,\nwhich are built up in a layer-by-layer fashion. The proposed halftoning\ntechnique isn't limited by the challenges existing techniques face; existing\nFDM coloring techniques greatly influence the surface geometry and deteriorate\nwith surface slopes deviating from vertical or greatly influence the basic\nparameters of the printing process and thereby the structural properties of the\nresulting product. Furthermore, the proposed technique has little effect on\nprinting time. Experiments on a dual-nozzle FDM printer show promising results.\nFuture work is required to calibrate the perceived tone.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:28:50 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 08:52:05 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Kuipers", "Tim", ""], ["Elkhuizen", "Willemijn", ""], ["Verlinden", "Jouke", ""], ["Doubrovski", "Eugeni", ""]]}, {"id": "1805.01934", "submitter": "Qifeng Chen", "authors": "Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun", "title": "Learning to See in the Dark", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging in low light is challenging due to low photon count and low SNR.\nShort-exposure images suffer from noise, while long exposure can induce blur\nand is often impractical. A variety of denoising, deblurring, and enhancement\ntechniques have been proposed, but their effectiveness is limited in extreme\nconditions, such as video-rate imaging at night. To support the development of\nlearning-based pipelines for low-light image processing, we introduce a dataset\nof raw short-exposure low-light images, with corresponding long-exposure\nreference images. Using the presented dataset, we develop a pipeline for\nprocessing low-light images, based on end-to-end training of a\nfully-convolutional network. The network operates directly on raw sensor data\nand replaces much of the traditional image processing pipeline, which tends to\nperform poorly on such data. We report promising results on the new dataset,\nanalyze factors that affect performance, and highlight opportunities for future\nwork. The results are shown in the supplementary video at\nhttps://youtu.be/qWKUFK7MWvg\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 21:03:12 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chen", "Chen", ""], ["Chen", "Qifeng", ""], ["Xu", "Jia", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1805.02493", "submitter": "Casper Van Leeuwen", "authors": "Casper van Leeuwen", "title": "GeneVis - An interactive visualization tool for combining\n  cross-discipline datasets within genetics", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR q-bio.GN", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  GeneVis is a web-based tool to visualize complementary data sets of different\ndisciplines within the field of genetics. It overlays gene-cluster information,\ngene-interaction data and gene-disease association data by means of web-based\ninteractive graph visualizations. This allows an intuitive and quick assessment\nof possible relations between the different datasets. By starting from a\nhigh-level graph abstraction based on gene clusters, which can be selected for\ndetailed inspection at the gene-interaction level in a separate window, GeneVis\ncircumvents the common visual clutter problem when using gene datasets with a\nhigh number of gene entries.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 13:09:21 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["van Leeuwen", "Casper", ""]]}, {"id": "1805.02651", "submitter": "Adrian Jarabo", "authors": "Adrian Jarabo, Carlos Aliaga, Diego Gutierrez", "title": "A Radiative Transfer Framework for Spatially-Correlated Materials", "comments": null, "journal-ref": "ACM Trans. Graph. 37, 4, Article 83 (August 2018), 13 pages (2018)", "doi": "10.1145/3197517.3201282", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a non-exponential radiative framework that takes into account\nthe local spatial correlation of scattering particles in a medium. Most\nprevious works in graphics have ignored this, assuming uncorrelated media with\na uniform, random local distribution of particles. However, positive and\nnegative correlation lead to slower- and faster-than-exponential attenuation\nrespectively, which cannot be predicted by the Beer-Lambert law. As our results\nshow, this has a major effect on extinction, and thus appearance. From recent\nadvances in neutron transport, we first introduce our Extended Generalized\nBoltzmann Equation, and develop a general framework for light transport in\ncorrelated media. We lift the limitations of the original formulation,\nincluding an analysis of the boundary conditions, and present a model suitable\nfor computer graphics, based on optical properties of the media and statistical\ndistributions of scatterers. In addition, we present an analytic expression for\ntransmittance in the case of positive correlation, and show how to incorporate\nit efficiently into a Monte Carlo renderer. We show results with a wide range\nof both positive and negative correlation, and demonstrate the differences\ncompared to classic light transport.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 17:59:34 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Jarabo", "Adrian", ""], ["Aliaga", "Carlos", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1805.03482", "submitter": "Bojian Wu", "authors": "Bojian Wu, Yang Zhou, Yiming Qian, Minglun Gong, Hui Huang", "title": "Full 3D Reconstruction of Transparent Objects", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous techniques have been proposed for reconstructing 3D models for\nopaque objects in past decades. However, none of them can be directly applied\nto transparent objects. This paper presents a fully automatic approach for\nreconstructing complete 3D shapes of transparent objects. Through positioning\nan object on a turntable, its silhouettes and light refraction paths under\ndifferent viewing directions are captured. Then, starting from an initial rough\nmodel generated from space carving, our algorithm progressively optimizes the\nmodel under three constraints: surface and refraction normal consistency,\nsurface projection and silhouette consistency, and surface smoothness.\nExperimental results on both synthetic and real objects demonstrate that our\nmethod can successfully recover the complex shapes of transparent objects and\nfaithfully reproduce their light refraction properties.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 12:39:06 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 13:40:00 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Wu", "Bojian", ""], ["Zhou", "Yang", ""], ["Qian", "Yiming", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1805.03874", "submitter": "Hsien-Yu Meng", "authors": "Hsien-Yu Meng, Tzu-heng Lin, Xiubao Jiang, Yao Lu, Jiangtao Wen", "title": "LSTM-Based Facial Performance Capture Using Embedding Between\n  Expressions", "comments": "Novelty of this paper is very limited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel end-to-end framework for facial performance capture given\na monocular video of an actor's face. Our framework are comprised of 2 parts.\nFirst, to extract the information in the frames, we optimize a triplet loss to\nlearn the embedding space which ensures the semantically closer facial\nexpressions are closer in the embedding space and the model can be transferred\nto distinguish the expressions that are not presented in the training dataset.\nSecond, the embeddings are fed into an LSTM network to learn the deformation\nbetween frames. In the experiments, we demonstrated that compared to other\nmethods, our method can distinguish the delicate motion around lips and\nsignificantly reduce jitters between the tracked meshes.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 08:22:12 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 11:32:14 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 09:09:51 GMT"}, {"version": "v4", "created": "Sat, 22 Dec 2018 14:59:10 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Meng", "Hsien-Yu", ""], ["Lin", "Tzu-heng", ""], ["Jiang", "Xiubao", ""], ["Lu", "Yao", ""], ["Wen", "Jiangtao", ""]]}, {"id": "1805.04487", "submitter": "Yang Zhou", "authors": "Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, Hui\n  Huang", "title": "Non-Stationary Texture Synthesis by Adversarial Expansion", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real world exhibits an abundance of non-stationary textures. Examples\ninclude textures with large-scale structures, as well as spatially variant and\ninhomogeneous textures. While existing example-based texture synthesis methods\ncan cope well with stationary textures, non-stationary textures still pose a\nconsiderable challenge, which remains unresolved. In this paper, we propose a\nnew approach for example-based non-stationary texture synthesis. Our approach\nuses a generative adversarial network (GAN), trained to double the spatial\nextent of texture blocks extracted from a specific texture exemplar. Once\ntrained, the fully convolutional generator is able to expand the size of the\nentire exemplar, as well as of any of its sub-blocks. We demonstrate that this\nconceptually simple approach is highly effective for capturing large-scale\nstructures, as well as other non-stationary attributes of the input exemplar.\nAs a result, it can cope with challenging textures, which, to our knowledge, no\nother existing method can handle.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 16:46:52 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Zhou", "Yang", ""], ["Zhu", "Zhen", ""], ["Bai", "Xiang", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "1805.04605", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu", "title": "Unsupervised Learning for Fast Probabilistic Diffeomorphic Registration", "comments": "MICCAI 2018 (Oral Presentation). Proceedings: LNCS 11070, pp 729-738", "journal-ref": "LNCS 11070, pp 729-738, Springer. 2018", "doi": "10.1007/978-3-030-00928-1_82", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates. In this paper, we present a probabilistic generative\nmodel and derive an unsupervised learning-based inference algorithm that makes\nuse of recent developments in convolutional neural networks (CNNs). We\ndemonstrate our method on a 3D brain registration task, and provide an\nempirical analysis of the algorithm. Our approach results in state of the art\naccuracy and very fast runtimes, while providing diffeomorphic guarantees and\nuncertainty estimates. Our implementation is available online at\nhttp://voxelmorph.csail.mit.edu .\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 22:12:01 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 13:28:36 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Balakrishnan", "Guha", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1805.04792", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li and Timothy R. Langlois and Changxi Zheng", "title": "Scene-Aware Audio for 360\\textdegree{} Videos", "comments": "SIGGRAPH 2018, Technical Papers, 12 pages, 17 figures,\n  http://www.cs.columbia.edu/cg/360audio/", "journal-ref": null, "doi": "10.1145/3197517.3201391", "report-no": null, "categories": "cs.GR cs.CV cs.ET cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although 360\\textdegree{} cameras ease the capture of panoramic footage, it\nremains challenging to add realistic 360\\textdegree{} audio that blends into\nthe captured scene and is synchronized with the camera motion. We present a\nmethod for adding scene-aware spatial audio to 360\\textdegree{} videos in\ntypical indoor scenes, using only a conventional mono-channel microphone and a\nspeaker. We observe that the late reverberation of a room's impulse response is\nusually diffuse spatially and directionally. Exploiting this fact, we propose a\nmethod that synthesizes the directional impulse response between any source and\nlistening locations by combining a synthesized early reverberation part and a\nmeasured late reverberation tail. The early reverberation is simulated using a\ngeometric acoustic simulation and then enhanced using a frequency modulation\nmethod to capture room resonances. The late reverberation is extracted from a\nrecorded impulse response, with a carefully chosen time duration that separates\nout the late reverberation from the early reverberation. In our validations, we\nshow that our synthesized spatial audio matches closely with recordings using\nambisonic microphones. Lastly, we demonstrate the strength of our method in\nseveral applications.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 22:06:04 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Li", "Dingzeyu", ""], ["Langlois", "Timothy R.", ""], ["Zheng", "Changxi", ""]]}, {"id": "1805.05715", "submitter": "Bailin Deng", "authors": "Yue Peng, Bailin Deng, Juyong Zhang, Fanyu Geng, Wenjie Qin, Ligang\n  Liu", "title": "Anderson Acceleration for Geometry Optimization and Physics Simulation", "comments": "SIGGRAPH 2018 Technical Paper", "journal-ref": null, "doi": "10.1145/3197517.3201290", "report-no": null, "categories": "cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer graphics problems require computing geometric shapes subject to\ncertain constraints. This often results in non-linear and non-convex\noptimization problems with globally coupled variables, which pose great\nchallenge for interactive applications. Local-global solvers developed in\nrecent years can quickly compute an approximate solution to such problems,\nmaking them an attractive choice for applications that prioritize efficiency\nover accuracy. However, these solvers suffer from lower convergence rate, and\nmay take a long time to compute an accurate result. In this paper, we propose a\nsimple and effective technique to accelerate the convergence of such solvers.\nBy treating each local-global step as a fixed-point iteration, we apply\nAnderson acceleration, a well-established technique for fixed-point solvers, to\nspeed up the convergence of a local-global solver. To address the stability\nissue of classical Anderson acceleration, we propose a simple strategy to\nguarantee the decrease of target energy and ensure its global convergence. In\naddition, we analyze the connection between Anderson acceleration and\nquasi-Newton methods, and show that the canonical choice of its mixing\nparameter is suitable for accelerating local-global solvers. Moreover, our\ntechnique is effective beyond classical local-global solvers, and can be\napplied to iterative methods with a common structure. We evaluate the\nperformance of our technique on a variety of geometry optimization and physics\nsimulation problems. Our approach significantly reduces the number of\niterations required to compute an accurate result, with only a slight increase\nof computational cost per iteration. Its simplicity and effectiveness makes it\na promising tool for accelerating existing algorithms as well as designing\nefficient new algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 11:43:29 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Peng", "Yue", ""], ["Deng", "Bailin", ""], ["Zhang", "Juyong", ""], ["Geng", "Fanyu", ""], ["Qin", "Wenjie", ""], ["Liu", "Ligang", ""]]}, {"id": "1805.05903", "submitter": "Wenwu Yang", "authors": "Wenwu Yang and Nathan Marshak and Daniel S\\'ykora and Srikumar\n  Ramalingam and Ladislav Kavan", "title": "Building Anatomically Realistic Jaw Kinematics Model from Data", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a different aspect of anatomical face modeling:\nkinematic modeling of the jaw, i.e., the Temporo-Mandibular Joint (TMJ).\nPrevious work often relies on simple models of jaw kinematics, even though the\nactual physiological behavior of the TMJ is quite complex, allowing not only\nfor mouth opening, but also for some amount of sideways (lateral) and\nfront-to-back (protrusion) motions. Fortuitously, the TMJ is the only joint\nwhose kinematics can be accurately measured with optical methods, because the\nbones of the lower and upper jaw are rigidly connected to the lower and upper\nteeth. We construct a person-specific jaw kinematic model by asking an actor to\nexercise the entire range of motion of the jaw while keeping the lips open so\nthat the teeth are at least partially visible. This performance is recorded\nwith three calibrated cameras. We obtain highly accurate 3D models of the teeth\nwith a standard dental scanner and use these models to reconstruct the rigid\nbody trajectories of the teeth from the videos (markerless tracking). The\nrelative rigid transformations samples between the lower and upper teeth are\nmapped to the Lie algebra of rigid body motions in order to linearize the\nrotational motion. Our main contribution is to fit these samples with a\nthree-dimensional nonlinear model parameterizing the entire range of motion of\nthe TMJ. We show that standard Principal Component Analysis (PCA) fails to\ncapture the nonlinear trajectories of the moving mandible. However, we found\nthese nonlinearities can be captured with a special modification of autoencoder\nneural networks known as Nonlinear PCA. By mapping back to the Lie group of\nrigid transformations, we obtain parameterization of the jaw kinematics which\nprovides an intuitive interface allowing the animators to explore realistic jaw\nmotions in a user-friendly way.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 16:53:20 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yang", "Wenwu", ""], ["Marshak", "Nathan", ""], ["S\u00fdkora", "Daniel", ""], ["Ramalingam", "Srikumar", ""], ["Kavan", "Ladislav", ""]]}, {"id": "1805.06019", "submitter": "Srihari Pratapa", "authors": "Srihari Pratapa, Dinesh Manocha", "title": "RLFC: Random Access Light Field Compression using Key Views and Bounded\n  Integer Encoding", "comments": "Accepted for publication at Symposium on Interactive 3D Graphics and\n  Games (I3D '19)", "journal-ref": null, "doi": "10.1145/3306131.3317018", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new hierarchical compression scheme for encoding light field\nimages (LFI) that is suitable for interactive rendering. Our method (RLFC)\nexploits redundancies in the light field images by constructing a tree\nstructure. The top level (root) of the tree captures the common high-level\ndetails across the LFI, and other levels (children) of the tree capture\nspecific low-level details of the LFI. Our decompressing algorithm corresponds\nto tree traversal operations and gathers the values stored at different levels\nof the tree. Furthermore, we use bounded integer sequence encoding which\nprovides random access and fast hardware decoding for compressing the blocks of\nchildren of the tree. We have evaluated our method for 4D two-plane\nparameterized light fields. The compression rates vary from 0.08 - 2.5 bits per\npixel (bpp), resulting in compression ratios of around 200:1 to 20:1 for a PSNR\nquality of 40 to 50 dB. The decompression times for decoding the blocks of LFI\nare 1 - 3 microseconds per channel on an NVIDIA GTX-960 and we can render new\nviews with a resolution of 512X512 at 200 fps. Our overall scheme is simple to\nimplement and involves only bit manipulations and integer arithmetic\noperations.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 20:12:17 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 06:25:13 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 20:47:51 GMT"}, {"version": "v4", "created": "Thu, 12 Jul 2018 23:46:54 GMT"}, {"version": "v5", "created": "Sat, 13 Oct 2018 00:37:29 GMT"}, {"version": "v6", "created": "Fri, 25 Jan 2019 19:54:45 GMT"}, {"version": "v7", "created": "Thu, 28 Feb 2019 15:53:56 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Pratapa", "Srihari", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1805.06021", "submitter": "Christopher Tralie", "authors": "Christopher Tralie, Matthew Berger", "title": "Topological Eulerian Synthesis of Slow Motion Periodic Videos", "comments": "9 pages, 5 Figures. IEEE International Conference on Image\n  Processing, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of taking a video that is comprised of multiple\nperiods of repetitive motion, and reordering the frames of the video into a\nsingle period, producing a detailed, single cycle video of motion. This problem\nis challenging, as such videos often contain noise, drift due to camera motion\nand from cycle to cycle, and irrelevant background motion/occlusions, and these\nfactors can confound the relevant periodic motion we seek in the video. To\naddress these issues in a simple and efficient manner, we introduce a tracking\nfree Eulerian approach for synthesizing a single cycle of motion. Our approach\nis geometric: we treat each frame as a point in high-dimensional Euclidean\nspace, and analyze the sliding window embedding formed by this sequence of\npoints, which yields samples along a topological loop regardless of the type of\nperiodic motion. We combine tools from topological data analysis and spectral\ngeometric analysis to estimate the phase of each window, and we exploit the\nsliding window structure to robustly reorder frames. We show quantitative\nresults that highlight the robustness of our technique to camera shake, noise,\nand occlusions, and qualitative results of single-cycle motion synthesis across\na variety of scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 20:19:46 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Tralie", "Christopher", ""], ["Berger", "Matthew", ""]]}, {"id": "1805.07035", "submitter": "Morad Behandish", "authors": "Morad Behandish, Saigopal Nelaturi, and Johan de Kleer", "title": "Automated Process Planning for Hybrid Manufacturing", "comments": "Special Issue on symposium on Solid and Physical Modeling (SPM'2018)", "journal-ref": "Journal of Computer-Aided Design, 2018", "doi": "10.1016/j.cad.2018.04.022", "report-no": null, "categories": "cs.CG cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid manufacturing (HM) technologies combine additive and subtractive\nmanufacturing (AM/SM) capabilities, leveraging AM's strengths in fabricating\ncomplex geometries and SM's precision and quality to produce finished parts. We\npresent a systematic approach to automated computer-aided process planning\n(CAPP) for HM that can identify non-trivial, qualitatively distinct, and\ncost-optimal combinations of AM/SM modalities. A multimodal HM process plan is\nrepresented by a finite Boolean expression of AM and SM manufacturing\nprimitives, such that the expression evaluates to an 'as-manufactured'\nartifact. We show that primitives that respect spatial constraints such as\naccessibility and collision avoidance may be constructed by solving inverse\nconfiguration space problems on the 'as-designed' artifact and manufacturing\ninstruments. The primitives generate a finite Boolean algebra (FBA) that\nenumerates the entire search space for planning. The FBA's canonical\nintersection terms (i.e., 'atoms') provide the complete domain decomposition to\nreframe manufacturability analysis and process planning into purely symbolic\nreasoning, once a subcollection of atoms is found to be interchangeable with\nthe design target. The approach subsumes unimodal (all-AM or all-SM) process\nplanning as special cases. We demonstrate the practical potency of our\nframework and its computational efficiency when applied to process planning of\ncomplex 3D parts with dramatically different AM and SM instruments.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:27:35 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Behandish", "Morad", ""], ["Nelaturi", "Saigopal", ""], ["de Kleer", "Johan", ""]]}, {"id": "1805.07339", "submitter": "Alex Poms", "authors": "Alex Poms, Will Crichton, Pat Hanrahan, Kayvon Fatahalian", "title": "Scanner: Efficient Video Analysis at Scale", "comments": "14 pages, 14 figuers", "journal-ref": null, "doi": "10.1145/3197517.3201394", "report-no": null, "categories": "cs.CV cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of visual computing applications depend on the analysis of\nlarge video collections. The challenge is that scaling applications to operate\non these datasets requires efficient systems for pixel data access and parallel\nprocessing across large numbers of machines. Few programmers have the\ncapability to operate efficiently at these scales, limiting the field's ability\nto explore new applications that leverage big video data. In response, we have\ncreated Scanner, a system for productive and efficient video analysis at scale.\nScanner organizes video collections as tables in a data store optimized for\nsampling frames from compressed video, and executes pixel processing\ncomputations, expressed as dataflow graphs, on these frames. Scanner schedules\nvideo analysis applications expressed using these abstractions onto\nheterogeneous throughput computing hardware, such as multi-core CPUs, GPUs, and\nmedia processing ASICs, for high-throughput pixel processing. We demonstrate\nthe productivity of Scanner by authoring a variety of video processing\napplications including the synthesis of stereo VR video streams from\nmulti-camera rigs, markerless 3D human pose reconstruction from video, and\ndata-mining big video datasets such as hundreds of feature-length films or over\n70,000 hours of TV news. These applications achieve near-expert performance on\na single machine and scale efficiently to hundreds of machines, enabling\nformerly long-running big video data analysis tasks to be carried out in\nminutes to hours.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 17:43:55 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Poms", "Alex", ""], ["Crichton", "Will", ""], ["Hanrahan", "Pat", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "1805.07794", "submitter": "Xi Xia", "authors": "Ligang Liu, Xi Xia, Han Sun, Qi Shen, Juzhan Xu, Bin Chen, Hui Huang,\n  Kai Xu", "title": "Object-Aware Guidance for Autonomous Scene Reconstruction", "comments": "12 pages, 17 figures", "journal-ref": "ACM Transactions on Graphics 37(4). August 2018", "doi": "10.1145/3197517.3201295", "report-no": null, "categories": "cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To carry out autonomous 3D scanning and online reconstruction of unknown\nindoor scenes, one has to find a balance between global exploration of the\nentire scene and local scanning of the objects within it. In this work, we\npropose a novel approach, which provides object-aware guidance for\nautoscanning, for exploring, reconstructing, and understanding an unknown scene\nwithin one navigation pass. Our approach interleaves between object analysis to\nidentify the next best object (NBO) for global exploration, and object-aware\ninformation gain analysis to plan the next best view (NBV) for local scanning.\nFirst, an objectness-based segmentation method is introduced to extract\nsemantic objects from the current scene surface via a multi-class graph cuts\nminimization. Then, an object of interest (OOI) is identified as the NBO which\nthe robot aims to visit and scan. The robot then conducts fine scanning on the\nOOI with views determined by the NBV strategy. When the OOI is recognized as a\nfull object, it can be replaced by its most similar 3D model in a shape\ndatabase. The algorithm iterates until all of the objects are recognized and\nreconstructed in the scene. Various experiments and comparisons have shown the\nfeasibility of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 16:44:20 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Liu", "Ligang", ""], ["Xia", "Xi", ""], ["Sun", "Han", ""], ["Shen", "Qi", ""], ["Xu", "Juzhan", ""], ["Chen", "Bin", ""], ["Huang", "Hui", ""], ["Xu", "Kai", ""]]}, {"id": "1805.08500", "submitter": "Renato Farias", "authors": "Renato Farias, Marcelo Kallmann", "title": "Improved Shortest Path Maps with GPU Shaders", "comments": "Work being submitted for peer review, 9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper several improvements for computing shortest path\nmaps using OpenGL shaders. The approach explores GPU rasterization as a way to\npropagate optimal costs on a polygonal 2D environment, producing shortest path\nmaps which can efficiently be queried at run-time. Our improved method relies\non Compute Shaders for improved performance, does not require any CPU\npre-computation, and handles shortest path maps both with source points and\nwith line segment sources. The produced path maps partition the input\nenvironment into regions sharing a same parent point along the shortest path to\nthe closest source point or segment source. Our method produces paths with\nglobal optimality, a characteristic which has been mostly neglected in animated\nvirtual environments. The proposed approach is particularly suitable for the\nanimation of multiple agents moving toward the entrances or exits of a virtual\nenvironment, a situation which is efficiently represented with the proposed\npath maps.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:03:30 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Farias", "Renato", ""], ["Kallmann", "Marcelo", ""]]}, {"id": "1805.08729", "submitter": "Marta Ortin", "authors": "Marta Ortin, Adrian Jarabo, Belen Masia, Diego Gutierrez", "title": "Analyzing Interfaces and Workflows for Light Field Editing", "comments": "10 papers, 14 figures", "journal-ref": "Marta Ortin, Adrian Jarabo, Belen Masia, and Diego Gutierrez.\n  Analyzing Interfaces and Workflows for Light Field Editing. IEEE Journal of\n  Selected Topics in Signal Processing, vol. 11, no. 7, pp. 1162-1172, Oct.\n  2017", "doi": "10.1109/JSTSP.2017.2746263", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of available consumer light field cameras, such as\nLytro TM, Raytrix TM, or Pelican Imaging TM, this new form of photography is\nprogressively becoming more common. However, there are still very few tools for\nlight field editing, and the interfaces to create those edits remain largely\nunexplored. Given the extended dimensionality of light field data, it is not\nclear what the most intuitive interfaces and optimal workflows are, in contrast\nwith well-studied 2D image manipulation software. In this work we provide a\ndetailed description of subjects' performance and preferences for a number of\nsimple editing tasks, which form the basis for more complex operations. We\nperform a detailed state sequence analysis and hidden Markov chain analysis\nbased on the sequence of tools and interaction paradigms users employ while\nediting light fields. These insights can aid researchers and designers in\ncreating new light field editing tools and interfaces, thus helping to close\nthe gap between 4D and 2D image editing.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:39:12 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ortin", "Marta", ""], ["Jarabo", "Adrian", ""], ["Masia", "Belen", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1805.08831", "submitter": "C\\'elestin Marot", "authors": "C\\'elestin Marot, Jeanne Pellerin, Jean-Fran\\c{c}ois Remacle", "title": "One machine, one minute, three billion tetrahedra", "comments": null, "journal-ref": null, "doi": "10.1002/nme.5987", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new scalable parallelization scheme to generate the 3D\nDelaunay triangulation of a given set of points. Our first contribution is an\nefficient serial implementation of the incremental Delaunay insertion\nalgorithm. A simple dedicated data structure, an efficient sorting of the\npoints and the optimization of the insertion algorithm have permitted to\naccelerate reference implementations by a factor three. Our second contribution\nis a multi-threaded version of the Delaunay kernel that is able to concurrently\ninsert vertices. Moore curve coordinates are used to partition the point set,\navoiding heavy synchronization overheads. Conflicts are managed by modifying\nthe partitions with a simple rescaling of the space-filling curve. The\nperformances of our implementation have been measured on three different\nprocessors, an Intel core-i7, an Intel Xeon Phi and an AMD EPYC, on which we\nhave been able to compute 3 billion tetrahedra in 53 seconds. This corresponds\nto a generation rate of over 55 million tetrahedra per second. We finally show\nhow this very efficient parallel Delaunay triangulation can be integrated in a\nDelaunay refinement mesh generator which takes as input the triangulated\nsurface boundary of the volume to mesh.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:40:50 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 16:10:21 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 13:34:33 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Marot", "C\u00e9lestin", ""], ["Pellerin", "Jeanne", ""], ["Remacle", "Jean-Fran\u00e7ois", ""]]}, {"id": "1805.08893", "submitter": "Michael Kenzel", "authors": "Michael Kenzel, Bernhard Kerbl, Wolfgang Tatzgern, Elena Ivanchenko,\n  Dieter Schmalstieg, Markus Steinberger", "title": "On-the-fly Vertex Reuse for Massively-Parallel Software Geometry\n  Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute-mode rendering is becoming more and more attractive for non-standard\nrendering applications, due to the high flexibility of compute-mode execution.\nThese newly designed pipelines often include streaming vertex and geometry\nprocessing stages. In typical triangle meshes, the same transformed vertex is\non average required six times during rendering. To avoid redundant computation,\na post-transform cache is traditionally suggested to enable reuse of vertex\nprocessing results. However, traditional caching neither scales well as the\nhardware becomes more parallel, nor can be efficiently implemented in a\nsoftware design. We investigate alternative strategies to reusing vertex\nshading results on-the-fly for massively parallel software geometry processing.\nForming static and dynamic batching on the data input stream, we analyze the\neffectiveness of identifying potential local reuse based on sorting, hashing,\nand efficient intra-thread-group communication. Altogether, we present four\nvertex reuse strategies, tailored to modern parallel architectures. Our\nsimulations showcase that our batch-based strategies significantly outperform\nparallel caches in terms of reuse. On actual GPU hardware, our evaluation shows\nthat our strategies not only lead to good reuse of processing results, but also\nboost performance by $2-3\\times$ compared to na\\\"ively ignoring reuse in a\nvariety of practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 22:40:07 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kenzel", "Michael", ""], ["Kerbl", "Bernhard", ""], ["Tatzgern", "Wolfgang", ""], ["Ivanchenko", "Elena", ""], ["Schmalstieg", "Dieter", ""], ["Steinberger", "Markus", ""]]}, {"id": "1805.09048", "submitter": "Ibon Guillen", "authors": "Ib\\'on Guill\\'en, Carlos Ure\\~na, Alan King, Marcos Fajardo, Iliyan\n  Georgiev, Jorge L\\'opez-Moreno, Adrian Jarabo", "title": "Area-preserving parameterizations for spherical ellipses", "comments": null, "journal-ref": "Computer Graphics Forum 36 (2017) 179-187", "doi": "10.1111/cgf.13234", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methods for uniformly sampling the solid angle subtended by a\ndisk. To achieve this, we devise two novel area-preserving mappings from the\nunit square $[0,1]^2$ to a spherical ellipse (i.e. the projection of the disk\nonto the unit sphere). These mappings allow for low-variance stratified\nsampling of direct illumination from disk-shaped light sources. We discuss how\nto efficiently incorporate our methods into a production renderer and\ndemonstrate the quality of our maps, showing significantly lower variance than\nprevious work.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 10:51:02 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Guill\u00e9n", "Ib\u00f3n", ""], ["Ure\u00f1a", "Carlos", ""], ["King", "Alan", ""], ["Fajardo", "Marcos", ""], ["Georgiev", "Iliyan", ""], ["L\u00f3pez-Moreno", "Jorge", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1805.09110", "submitter": "Joshua Levine", "authors": "Julien Tierny, Guillaume Favelier, Joshua A. Levine, Charles Gueunet,\n  and Michael Michaux", "title": "The Topology ToolKit", "comments": null, "journal-ref": "IEEE Trans. Vis. Comput. Graph. 24(1) (2018) 832-842", "doi": "10.1109/TVCG.2017.2743938", "report-no": null, "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This system paper presents the Topology ToolKit (TTK), a software platform\ndesigned for topological data analysis in scientific visualization. TTK\nprovides a unified, generic, efficient, and robust implementation of key\nalgorithms for the topological analysis of scalar data, including: critical\npoints, integral lines, persistence diagrams, persistence curves, merge trees,\ncontour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots,\nJacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due\nto a tight integration with ParaView. It is also easily accessible to\ndevelopers through a variety of bindings (Python, VTK/C++) for fast prototyping\nor through direct, dependence-free, C++, to ease integration into pre-existing\ncomplex systems. While developing TTK, we faced several algorithmic and\nsoftware engineering challenges, which we document in this paper. In\nparticular, we present an algorithm for the construction of a discrete gradient\nthat complies to the critical points extracted in the piecewise-linear setting.\nThis algorithm guarantees a combinatorial consistency across the topological\nabstractions supported by TTK, and importantly, a unified implementation of\ntopological data simplification for multi-scale exploration and analysis. We\nalso present a cached triangulation data structure, that supports time\nefficient and generic traversals, which self-adjusts its memory usage on demand\nfor input simplicial meshes and which implicitly emulates a triangulation for\nregular grids with no memory overhead. Finally, we describe an original\nsoftware architecture, which guarantees memory efficient and direct accesses to\nTTK features, while still allowing for researchers powerful and easy bindings\nand extensions. TTK is open source (BSD license) and its code, online\ndocumentation and video tutorials are available on TTK's website.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:27:24 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:53:53 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Tierny", "Julien", ""], ["Favelier", "Guillaume", ""], ["Levine", "Joshua A.", ""], ["Gueunet", "Charles", ""], ["Michaux", "Michael", ""]]}, {"id": "1805.09170", "submitter": "Nicholas Sharp", "authors": "Nicholas Sharp, Yousuf Soliman, Keenan Crane", "title": "The Vector Heat Method", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for efficiently computing parallel transport of\ntangent vectors on curved surfaces, or more generally, any vector-valued data\non a curved manifold. More precisely, it extends a vector field defined over\nany region to the rest of the domain via parallel transport along shortest\ngeodesics. This basic operation enables fast, robust algorithms for\nextrapolating level set velocities, inverting the exponential map, computing\ngeometric medians and Karcher/Fr\\'{e}chet means of arbitrary distributions,\nconstructing centroidal Voronoi diagrams, and finding consistently ordered\nlandmarks. Rather than evaluate parallel transport by explicitly tracing\ngeodesics, we show that it can be computed via a short-time heat flow involving\nthe connection Laplacian. As a result, transport can be achieved by solving\nthree prefactored linear systems, each akin to a standard Poisson problem. To\nimplement the method we need only a discrete connection Laplacian, which we\ndescribe for a variety of geometric data structures (point clouds, polygon\nmeshes, etc). We also study the numerical behavior of our method, showing\nempirically that it converges under refinement, and augment the construction of\nintrinsic Delaunay triangulations (iDT) so that they can be used in the context\nof tangent vector field processing.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:47:51 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 15:02:16 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 15:54:28 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Sharp", "Nicholas", ""], ["Soliman", "Yousuf", ""], ["Crane", "Keenan", ""]]}, {"id": "1805.09258", "submitter": "Julio Marco", "authors": "Julio Marco, Adrian Jarabo, Wojciech Jarosz, Diego Gutierrez", "title": "Second-Order Occlusion-Aware Volumetric Radiance Caching", "comments": null, "journal-ref": "ACM Transactions on Graphics, 37, 2, Article 20 (February 2018)", "doi": "10.1145/3185225", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a second-order gradient analysis of light transport in\nparticipating media and use this to develop an improved radiance caching\nalgorithm for volumetric light transport. We adaptively sample and interpolate\nradiance from sparse points in the medium using a second-order Hessian-based\nerror metric to determine when interpolation is appropriate. We derive our\nmetric from each point's incoming light field, computed by using a proxy\ntriangulation-based representation of the radiance reflected by the surrounding\nmedium and geometry. We use this representation to efficiently compute the\nfirst- and second-order derivatives of the radiance at the cache points while\naccounting for occlusion changes.\n  We also propose a self-contained two-dimensional model for light transport in\nmedia and use it to validate and analyze our approach, demonstrating that our\nmethod outperforms previous radiance caching algorithms both in terms of\naccurate derivative estimates and final radiance extrapolation. We generalize\nthese findings to practical three-dimensional scenarios, where we show improved\nresults while reducing computation time by up to 30\\% compared to previous\nwork.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 16:08:43 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Marco", "Julio", ""], ["Jarabo", "Adrian", ""], ["Jarosz", "Wojciech", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1805.09305", "submitter": "Julio Marco", "authors": "Julio Marco, Quercus Hernandez, Adolfo Mu\\~noz, Yue Dong, Adrian\n  Jarabo, Min H. Kim, Xin Tong, Diego Gutierrez", "title": "DeepToF: Off-the-Shelf Real-Time Correction of Multipath Interference in\n  Time-of-Flight Imaging", "comments": null, "journal-ref": "ACM Transactions on Graphics 36, 6, Article 219 (November 2017)", "doi": "10.1145/3130800.3130884", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-of-flight (ToF) imaging has become a widespread technique for depth\nestimation, allowing affordable off-the-shelf cameras to provide depth maps in\nreal time. However, multipath interference (MPI) resulting from indirect\nillumination significantly degrades the captured depth. Most previous works\nhave tried to solve this problem by means of complex hardware modifications or\ncostly computations. In this work we avoid these approaches, and propose a new\ntechnique that corrects errors in depth caused by MPI that requires no camera\nmodifications, and corrects depth in just 10 milliseconds per frame. By\nobserving that most MPI information can be expressed as a function of the\ncaptured depth, we pose MPI removal as a convolutional approach, and model it\nusing a convolutional neural network. In particular, given that the input and\noutput data present similar structure, we base our network in an autoencoder,\nwhich we train in two stages: first, we use the encoder (convolution filters)\nto learn a suitable basis to represent corrupted range images; then, we train\nthe decoder (deconvolution filters) to correct depth from the learned basis\nfrom synthetically generated scenes. This approach allows us to tackle the lack\nof reference data, by using a large-scale captured training set with corrupted\ndepth to train the encoder, and a smaller synthetic training set with ground\ntruth depth to train the corrector stage of the network, which we generate by\nusing a physically-based, time-resolved rendering. We demonstrate and validate\nour method on both synthetic and real complex scenarios, using an off-the-shelf\nToF camera, and with only the captured incorrect depth as input.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 17:43:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Marco", "Julio", ""], ["Hernandez", "Quercus", ""], ["Mu\u00f1oz", "Adolfo", ""], ["Dong", "Yue", ""], ["Jarabo", "Adrian", ""], ["Kim", "Min H.", ""], ["Tong", "Xin", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1805.09488", "submitter": "Yang Zhou", "authors": "Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu\n  Maji, Karan Singh", "title": "VisemeNet: Audio-Driven Animator-Centric Speech Animation", "comments": "10 pages, 5 figures, to appear in SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep-learning based approach to producing animator-centric\nspeech motion curves that drive a JALI or standard FACS-based production\nface-rig, directly from input audio. Our three-stage Long Short-Term Memory\n(LSTM) network architecture is motivated by psycho-linguistic insights:\nsegmenting speech audio into a stream of phonetic-groups is sufficient for\nviseme construction; speech styles like mumbling or shouting are strongly\nco-related to the motion of facial landmarks; and animator style is encoded in\nviseme motion curve profiles. Our contribution is an automatic real-time\nlip-synchronization from audio solution that integrates seamlessly into\nexisting animation pipelines. We evaluate our results by: cross-validation to\nground-truth data; animator critique and edits; visual comparison to recent\ndeep-learning lip-synchronization solutions; and showing our approach to be\nresilient to diversity in speaker and language.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 02:34:42 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhou", "Yang", ""], ["Xu", "Zhan", ""], ["Landreth", "Chris", ""], ["Kalogerakis", "Evangelos", ""], ["Maji", "Subhransu", ""], ["Singh", "Karan", ""]]}, {"id": "1805.09562", "submitter": "Julio Marco", "authors": "Julio Marco, Ib\\'on Guill\\'en, Wojciech Jarosz, Diego Gutierrez,\n  Adrian Jarabo", "title": "Progressive Transient Photon Beams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a novel algorithm for transient rendering in\nparticipating media. Our method is consistent, robust, and is able to generate\nanimations of time-resolved light transport featuring complex caustic light\npaths in media. We base our method on the observation that the spatial\ncontinuity provides an increased coverage of the temporal domain, and\ngeneralize photon beams to transient-state. We extend the beam steady-state\nradiance estimates to include the temporal domain. Then, we develop a\nprogressive version of spatio-temporal density estimations, that converges to\nthe correct solution with finite memory requirements by iteratively averaging\nseveral realizations of independent renders with a progressively reduced kernel\nbandwidth. We derive the optimal convergence rates accounting for space and\ntime kernels, and demonstrate our method against previous consistent transient\nrendering methods for participating media.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:15:15 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Marco", "Julio", ""], ["Guill\u00e9n", "Ib\u00f3n", ""], ["Jarosz", "Wojciech", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1805.09817", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, Noah Snavely", "title": "Stereo Magnification: Learning View Synthesis using Multiplane Images", "comments": "Accepted to SIGGRAPH 2018. Project webpage:\n  https://people.eecs.berkeley.edu/~tinghuiz/projects/mpi/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The view synthesis problem--generating novel views of a scene from known\nimagery--has garnered recent attention due in part to compelling applications\nin virtual and augmented reality. In this paper, we explore an intriguing\nscenario for view synthesis: extrapolating views from imagery captured by\nnarrow-baseline stereo cameras, including VR cameras and now-widespread\ndual-lens camera phones. We call this problem stereo magnification, and propose\na learning framework that leverages a new layered representation that we call\nmultiplane images (MPIs). Our method also uses a massive new data source for\nlearning view extrapolation: online videos on YouTube. Using data mined from\nsuch videos, we train a deep network that predicts an MPI from an input stereo\nimage pair. This inferred MPI can then be used to synthesize a range of novel\nviews of the scene, including views that extrapolate significantly beyond the\ninput baseline. We show that our method compares favorably with several recent\nview synthesis methods, and demonstrate applications in magnifying\nnarrow-baseline stereo images.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:58:02 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Zhou", "Tinghui", ""], ["Tucker", "Richard", ""], ["Flynn", "John", ""], ["Fyffe", "Graham", ""], ["Snavely", "Noah", ""]]}, {"id": "1805.11714", "submitter": "Michael Zollh\\\"ofer", "authors": "Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies,\n  Matthias Nie{\\ss}ner, Patrick P\\'erez, Christian Richardt, Michael\n  Zollh\\\"ofer, Christian Theobalt", "title": "Deep Video Portraits", "comments": "SIGGRAPH 2018, Video: https://www.youtube.com/watch?v=qc5P2bvfl44", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach that enables photo-realistic re-animation of\nportrait videos using only an input video. In contrast to existing approaches\nthat are restricted to manipulations of facial expressions only, we are the\nfirst to transfer the full 3D head position, head rotation, face expression,\neye gaze, and eye blinking from a source actor to a portrait video of a target\nactor. The core of our approach is a generative neural network with a novel\nspace-time architecture. The network takes as input synthetic renderings of a\nparametric face model, based on which it predicts photo-realistic video frames\nfor a given target actor. The realism in this rendering-to-video transfer is\nachieved by careful adversarial training, and as a result, we can create\nmodified target videos that mimic the behavior of the synthetically-created\ninput. In order to enable source-to-target video re-animation, we render a\nsynthetic target video with the reconstructed head animation parameters from a\nsource video, and feed it into the trained network -- thus taking full control\nof the target. With the ability to freely recombine source and target\nparameters, we are able to demonstrate a large variety of video rewrite\napplications without explicitly modeling hair, body or background. For\ninstance, we can reenact the full head using interactive user-controlled\nediting, and realize high-fidelity visual dubbing. To demonstrate the high\nquality of our output, we conduct an extensive series of experiments and\nevaluations, where for instance a user study shows that our video edits are\nhard to detect.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 21:31:14 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Garrido", "Pablo", ""], ["Tewari", "Ayush", ""], ["Xu", "Weipeng", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["P\u00e9rez", "Patrick", ""], ["Richardt", "Christian", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "1805.11729", "submitter": "Justus Thies", "authors": "Justus Thies, Michael Zollh\\\"ofer, Christian Theobalt, Marc\n  Stamminger, Matthias Nie{\\ss}ner", "title": "HeadOn: Real-time Reenactment of Human Portrait Videos", "comments": "Video: https://www.youtube.com/watch?v=7Dg49wv2c_g Presented at\n  Siggraph'18", "journal-ref": null, "doi": "10.1145/3197517.3201350", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HeadOn, the first real-time source-to-target reenactment approach\nfor complete human portrait videos that enables transfer of torso and head\nmotion, face expression, and eye gaze. Given a short RGB-D video of the target\nactor, we automatically construct a personalized geometry proxy that embeds a\nparametric head, eye, and kinematic torso model. A novel real-time reenactment\nalgorithm employs this proxy to photo-realistically map the captured motion\nfrom the source actor to the target actor. On top of the coarse geometric\nproxy, we propose a video-based rendering technique that composites the\nmodified target portrait video via view- and pose-dependent texturing, and\ncreates photo-realistic imagery of the target actor under novel torso and head\nposes, facial expressions, and gaze directions. To this end, we propose a\nrobust tracking of the face and torso of the source actor. We extensively\nevaluate our approach and show significant improvements in enabling much\ngreater flexibility in creating realistic reenacted output videos.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 22:24:13 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""], ["Stamminger", "Marc", ""], ["Nie\u00dfner", "Matthias", ""]]}]