[{"id": "2103.00239", "submitter": "Floor Verhoeven", "authors": "Floor Verhoeven, Amir Vaxman, Tim Hoffmann, Olga Sorkine-Hornung", "title": "Dev2PQ: Planar Quadrilateral Strip Remeshing of Developable Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce an algorithm to remesh triangle meshes representing developable\nsurfaces to planar quad dominant meshes. The output of our algorithm consists\nof planar quadrilateral (PQ) strips that are aligned to principal curvature\ndirections and closely approximate the curved parts of the input developable,\nand planar polygons representing the flat parts of the input. Developable\nPQ-strip meshes are useful in many areas of shape modeling, thanks to the\nsimplicity of fabrication from flat sheet material. Unfortunately, they are\ndifficult to model due to their restrictive combinatorics and locking issues.\nOther representations of developable surfaces, such as arbitrary triangle or\nquad meshes, are more suitable for interactive freeform modeling, but generally\nhave non-planar faces or are not aligned to principal curvatures. Our method\nleverages the modeling flexibility of non-ruling based representations of\ndevelopable surfaces, while still obtaining developable, curvature aligned\nPQ-strip meshes. Our algorithm optimizes for a scalar function on the input\nmesh, such that its level sets are extrinsically straight and align well to the\nlocally estimated ruling directions. The condition that guarantees straight\nlevel sets is nonlinear of high order and numerically difficult to enforce in a\nstraightforward manner. We devise an alternating optimization method that makes\nour problem tractable and practical to compute. Our method works automatically\non any developable input, including multiple patches and curved folds, without\nexplicit domain decomposition. We demonstrate the effectiveness of our approach\non a variety of developable surfaces and show how our remeshing can be used\nalongside handle based interactive freeform modeling of developable shapes.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:21:52 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 15:15:12 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Verhoeven", "Floor", ""], ["Vaxman", "Amir", ""], ["Hoffmann", "Tim", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2103.00262", "submitter": "Claudio Mura", "authors": "Claudio Mura, Renato Pajarola, Konrad Schindler, Niloy Mitra", "title": "Walk2Map: Extracting Floor Plans from Indoor Walk Trajectories", "comments": "To be published in Computer Graphics Forum (Proc. Eurographics 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a proliferation of new digital products for the\nefficient management of indoor spaces, with important applications like\nemergency management, virtual property showcasing and interior design. These\nproducts rely on accurate 3D models of the environments considered, including\ninformation on both architectural and non-permanent elements. These models must\nbe created from measured data such as RGB-D images or 3D point clouds, whose\ncapture and consolidation involves lengthy data workflows. This strongly limits\nthe rate at which 3D models can be produced, preventing the adoption of many\ndigital services for indoor space management. We provide an alternative to such\ndata-intensive procedures by presenting Walk2Map, a data-driven approach to\ngenerate floor plans only from trajectories of a person walking inside the\nrooms. Thanks to recent advances in data-driven inertial odometry, such\nminimalistic input data can be acquired from the IMU readings of consumer-level\nsmartphones, which allows for an effortless and scalable mapping of real-world\nindoor spaces. Our work is based on learning the latent relation between an\nindoor walk trajectory and the information represented in a floor plan:\ninterior space footprint, portals, and furniture. We distinguish between\nrecovering area-related (interior footprint, furniture) and wall-related\n(doors) information and use two different neural architectures for the two\ntasks: an image-based Encoder-Decoder and a Graph Convolutional Network,\nrespectively. We train our networks using scanned 3D indoor models and apply\nthem in a cascaded fashion on an indoor walk trajectory at inference time. We\nperform a qualitative and quantitative evaluation using both simulated and\nmeasured, real-world trajectories, and compare against a baseline method for\nimage-to-image translation. The experiments confirm the feasibility of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:29:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mura", "Claudio", ""], ["Pajarola", "Renato", ""], ["Schindler", "Konrad", ""], ["Mitra", "Niloy", ""]]}, {"id": "2103.00776", "submitter": "Tianyang Shi", "authors": "Yinglin Duan (1), Tianyang Shi (1), Zhengxia Zou (2), Yenan Lin (3),\n  Zhehui Qian (3), Bohan Zhang (3), Yi Yuan (1) ((1) NetEase Fuxi AI Lab, (2)\n  University of Michigan, (3) NetEase)", "title": "Single-Shot Motion Completion with Transformer", "comments": "10 pages, 6 figures. Project page: https://github.com/FuxiCV/SSMCT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion completion is a challenging and long-discussed problem, which is of\ngreat significance in film and game applications. For different motion\ncompletion scenarios (in-betweening, in-filling, and blending), most previous\nmethods deal with the completion problems with case-by-case designs. In this\nwork, we propose a simple but effective method to solve multiple motion\ncompletion problems under a unified framework and achieves a new state of the\nart accuracy under multiple evaluation settings. Inspired by the recent great\nsuccess of attention-based models, we consider the completion as a sequence to\nsequence prediction problem. Our method consists of two modules - a standard\ntransformer encoder with self-attention that learns long-range dependencies of\ninput motions, and a trainable mixture embedding module that models temporal\ninformation and discriminates key-frames. Our method can run in a\nnon-autoregressive manner and predict multiple missing frames within a single\nforward propagation in real time. We finally show the effectiveness of our\nmethod in music-dance applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:00:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Duan", "Yinglin", ""], ["Shi", "Tianyang", ""], ["Zou", "Zhengxia", ""], ["Lin", "Yenan", ""], ["Qian", "Zhehui", ""], ["Zhang", "Bohan", ""], ["Yuan", "Yi", ""]]}, {"id": "2103.01152", "submitter": "Adam McCaughan", "authors": "A. N. McCaughan, A. M. Tait, S. M. Buckley, D. M. Oh, J. T. Chiles, J.\n  M. Shainline, S. W. Nam", "title": "PHIDL: Python CAD layout and geometry creation for nanolithography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.supr-con cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided design (CAD) has become a critical element in the creation of\nnanopatterned structures and devices. In particular, with the increased\nadoption of easy-to-learn programming languages like Python there has been a\nsignificant rise in the amount of lithographic geometries generated through\nscripting and programming. However, there are currently unaddressed gaps in\nusability for open-source CAD tools -- especially those in the GDSII design\nspace -- that prevent wider adoption by scientists and students who might\notherwise benefit from scripted design. For example, constructing relations\nbetween adjacent geometries is often much more difficult than necessary --\nspacing a resonator structure a few micrometers from a readout structure often\nrequires manually-coding the placement arithmetic. While inconveniences like\nthis can be overcome by writing custom functions, they are often significant\nbarriers to entry for new users or those less familiar with programming. To\nhelp streamline the design process and reduce barrier to entry for scripting\ndesigns, we have developed PHIDL, an open-source GDSII-based CAD tool for\nPython 2 and 3.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:44:03 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["McCaughan", "A. N.", ""], ["Tait", "A. M.", ""], ["Buckley", "S. M.", ""], ["Oh", "D. M.", ""], ["Chiles", "J. T.", ""], ["Shainline", "J. M.", ""], ["Nam", "S. W.", ""]]}, {"id": "2103.01261", "submitter": "Mianlun Zheng", "authors": "Mianlun Zheng, Yi Zhou, Duygu Ceylan, Jernej Barbi\\v{c}", "title": "A Deep Emulator for Secondary Motion of 3D Characters", "comments": "Accepted at CVPR 2021, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and light-weight methods for animating 3D characters are desirable in\nvarious applications such as computer games. We present a learning-based\napproach to enhance skinning-based animations of 3D characters with vivid\nsecondary motion effects. We design a neural network that encodes each local\npatch of a character simulation mesh where the edges implicitly encode the\ninternal forces between the neighboring vertices. The network emulates the\nordinary differential equations of the character dynamics, predicting new\nvertex positions from the current accelerations, velocities and positions.\nBeing a local method, our network is independent of the mesh topology and\ngeneralizes to arbitrarily shaped 3D character meshes at test time. We further\nrepresent per-vertex constraints and material properties such as stiffness,\nenabling us to easily adjust the dynamics in different parts of the mesh. We\nevaluate our method on various character meshes and complex motion sequences.\nOur method can be over 30 times more efficient than ground-truth physically\nbased simulation, and outperforms alternative solutions that provide fast\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:13:35 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:35:43 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 18:49:21 GMT"}, {"version": "v4", "created": "Sun, 11 Apr 2021 18:26:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zheng", "Mianlun", ""], ["Zhou", "Yi", ""], ["Ceylan", "Duygu", ""], ["Barbi\u010d", "Jernej", ""]]}, {"id": "2103.01500", "submitter": "Dongseok Yang", "authors": "Dongseok Yang, Doyeon Kim, Sung-Hee Lee", "title": "LoBSTr: Real-time Lower-body Pose Prediction from Sparse Upper-body\n  Tracking Signals", "comments": null, "journal-ref": "Computer Graphics Forum, 40: 265-275 (2021)", "doi": "10.1111/cgf.142631", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the popularization of game and VR/AR devices, there is a growing need\nfor capturing human motion with a sparse set of tracking data. In this paper,\nwe introduce a deep neural-network (DNN) based method for real-time prediction\nof the lower-body pose only from the tracking signals of the upper-body joints.\nSpecifically, our Gated Recurrent Unit (GRU)-based recurrent architecture\npredicts the lower-body pose and feet contact probability from past sequence of\ntracking signals of the head, hands and pelvis. A major feature of our method\nis that the input signal is represented with the velocity of tracking signals.\nWe show that the velocity representation better models the correlation between\nthe upper-body and lower-body motions and increase the robustness against the\ndiverse scales and proportions of the user body than position-orientation\nrepresentations. In addition, to remove foot-skating and floating artifacts,\nour network predicts feet contact state, which is used to post-process the\nlower-body pose with inverse kinematics to preserve the contact. Our network is\nlightweight so as to run in real-time applications. We show the effectiveness\nof our method through several quantitative evaluations against other\narchitectures and input representations, with respect to wild tracking data\nobtained from commercial VR devices.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:36:36 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 10:53:20 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Yang", "Dongseok", ""], ["Kim", "Doyeon", ""], ["Lee", "Sung-Hee", ""]]}, {"id": "2103.01618", "submitter": "Eugene d'Eon", "authors": "Eugene d'Eon", "title": "An analytic BRDF for materials with spherical Lambertian scatterers", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new analytic BRDF for porous materials comprised of spherical\nLambertian scatterers. The BRDF has a single parameter: the albedo of the\nLambertian particles. The resulting appearance exhibits strong back scattering\nand saturation effects that height-field-based models such as Oren-Nayar cannot\nreproduce.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:19:17 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["d'Eon", "Eugene", ""]]}, {"id": "2103.01694", "submitter": "Yu-Jie Yuan", "authors": "Yu-Jie Yuan, Yu-Kun Lai, Tong Wu, Lin Gao and Ligang Liu", "title": "A Revisit of Shape Editing Techniques: from the Geometric to the Neural\n  Viewpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape editing is widely used in a range of applications such as movie\nproduction, computer games and computer aided design. It is also a popular\nresearch topic in computer graphics and computer vision. In past decades,\nresearchers have developed a series of editing methods to make the editing\nprocess faster, more robust, and more reliable. Traditionally, the deformed\nshape is determined by the optimal transformation and weights for an energy\nterm. With increasing availability of 3D shapes on the Internet, data-driven\nmethods were proposed to improve the editing results. More recently as the deep\nneural networks became popular, many deep learning based editing methods have\nbeen developed in this field, which is naturally data-driven. We mainly survey\nrecent research works from the geometric viewpoint to those emerging neural\ndeformation techniques and categorize them into organic shape editing methods\nand man-made model editing methods. Both traditional methods and recent neural\nnetwork based methods are reviewed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:56:24 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Yuan", "Yu-Jie", ""], ["Lai", "Yu-Kun", ""], ["Wu", "Tong", ""], ["Gao", "Lin", ""], ["Liu", "Ligang", ""]]}, {"id": "2103.01891", "submitter": "Uri Ascher", "authors": "Uri M. Ascher and Egor Larionov and Seung Heon Sheen and Dinesh K. Pai", "title": "Simulating deformable objects for computer animation: a numerical\n  perspective", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine a variety of numerical methods that arise when considering\ndynamical systems in the context of physics-based simulations of deformable\nobjects. Such problems arise in various applications, including animation,\nrobotics, control and fabrication. The goals and merits of suitable numerical\nalgorithms for these applications are different from those of typical numerical\nanalysis research in dynamical systems. Here the mathematical model is not\nfixed a priori but must be adjusted as necessary to capture the desired\nbehaviour, with an emphasis on effectively producing lively animations of\nobjects with complex geometries. Results are often judged by how realistic they\nappear to observers (by the \"eye-norm\") as well as by the efficacy of the\nnumerical procedures employed. And yet, we show that with an adjusted view\nnumerical analysis and applied mathematics can contribute significantly to the\ndevelopment of appropriate methods and their analysis in a variety of areas\nincluding finite element methods, stiff and highly oscillatory ODEs, model\nreduction, and constrained optimization.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:44:40 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ascher", "Uri M.", ""], ["Larionov", "Egor", ""], ["Sheen", "Seung Heon", ""], ["Pai", "Dinesh K.", ""]]}, {"id": "2103.01954", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer,\n  Yaser Sheikh, Jason Saragih", "title": "Mixture of Volumetric Primitives for Efficient Neural Rendering", "comments": "13 pages; SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time rendering and animation of humans is a core function in games,\nmovies, and telepresence applications. Existing methods have a number of\ndrawbacks we aim to address with our work. Triangle meshes have difficulty\nmodeling thin structures like hair, volumetric representations like Neural\nVolumes are too low-resolution given a reasonable memory budget, and\nhigh-resolution implicit representations like Neural Radiance Fields are too\nslow for use in real-time applications. We present Mixture of Volumetric\nPrimitives (MVP), a representation for rendering dynamic 3D content that\ncombines the completeness of volumetric representations with the efficiency of\nprimitive-based rendering, e.g., point-based or mesh-based methods. Our\napproach achieves this by leveraging spatially shared computation with a\ndeconvolutional architecture and by minimizing computation in empty regions of\nspace with volumetric primitives that can move to cover only occupied regions.\nOur parameterization supports the integration of correspondence and tracking\nconstraints, while being robust to areas where classical tracking fails, such\nas around thin or translucent structures and areas with large topological\nvariability. MVP is a hybrid that generalizes both volumetric and\nprimitive-based representations. Through a series of extensive experiments we\ndemonstrate that it inherits the strengths of each, while avoiding many of\ntheir limitations. We also compare our approach to several state-of-the-art\nmethods and demonstrate that MVP produces superior results in terms of quality\nand runtime performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:59:42 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:40:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Schwartz", "Gabriel", ""], ["Zollhoefer", "Michael", ""], ["Sheikh", "Yaser", ""], ["Saragih", "Jason", ""]]}, {"id": "2103.02114", "submitter": "Chang Liu", "authors": "Chang Liu, Wenzhong Yan, Pehuen Moure, Cody Fan, and Ankur Mehta", "title": "A Computational Design and Evaluation Tool for 3D Structures with Planar\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Three dimensional (3D) structures composed of planar surfaces can be build\nout of accessible materials using easier fabrication technique with shorter\nfabrication time. To better design 3D structures with planar surfaces,\nrealistic models are required to understand and evaluate mechanical behaviors.\nExisting design tools are either effort-consuming (e.g. finite element\nanalysis) or bounded by assumptions (e.g. numerical solutions). In this\nproject, We have built a computational design tool that is (1) capable of\nrapidly and inexpensively evaluating planar surfaces in 3D structures, with\nsufficient computational efficiency and accuracy; (2) applicable to complex\nboundary conditions and loading conditions, both isotropic materials and\northotropic materials; and (3) suitable for rapid accommodation when design\nparameters need to be adjusted. We demonstrate the efficiency and necessity of\nthis design tool by evaluating a glass table as well as a wood bookcase, and\niteratively designing an origami gripper to satisfy performance requirements.\nThis design tool gives non-expert users as well as engineers a simple and\neffective modus operandi in structural design.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:15:21 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Chang", ""], ["Yan", "Wenzhong", ""], ["Moure", "Pehuen", ""], ["Fan", "Cody", ""], ["Mehta", "Ankur", ""]]}, {"id": "2103.02309", "submitter": "Ugur Gudukbay", "authors": "Aytek Aman, Serkan Demirci, U\\u{g}ur G\\\"ud\\\"ukbay", "title": "Compact Tetrahedralization-based Acceleration Structure for Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a compact and efficient tetrahedral mesh representation to improve\nthe ray-tracing performance. We reorder tetrahedral mesh data using a\nspace-filling curve to improve cache locality. Most importantly, we propose an\nefficient ray traversal algorithm. We provide details of common ray tracing\noperations on tetrahedral meshes and give the GPU implementation of our\ntraversal method. We demonstrate our findings through a set of comprehensive\nexperiments. Our method outperforms existing tetrahedral mesh-based traversal\nmethods and yields comparable results to the traversal methods based on the\nstate of the art acceleration structures such as k-dimensional (k-d) trees and\nBounding Volume Hierarchies (BVHs).\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:39:03 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Aman", "Aytek", ""], ["Demirci", "Serkan", ""], ["G\u00fcd\u00fckbay", "U\u011fur", ""]]}, {"id": "2103.02380", "submitter": "Bin Chen", "authors": "Ruizhen Hu, Bin Chen, Juzhan Xu, Oliver van Kaick, Oliver Deussen, Hui\n  Huang", "title": "Shape-driven Coordinate Ordering for Star Glyph Sets via Reinforcement\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2021", "doi": "10.1109/TVCG.2021.3052167", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural optimization model trained with reinforcement learning to\nsolve the coordinate ordering problem for sets of star glyphs. Given a set of\nstar glyphs associated to multiple class labels, we propose to use shape\ncontext descriptors to measure the perceptual distance between pairs of glyphs,\nand use the derived silhouette coefficient to measure the perception of class\nseparability within the entire set. To find the optimal coordinate order for\nthe given set, we train a neural network using reinforcement learning to reward\norderings with high silhouette coefficients. The network consists of an encoder\nand a decoder with an attention mechanism. The encoder employs a recurrent\nneural network (RNN) to encode input shape and class information, while the\ndecoder together with the attention mechanism employs another RNN to output a\nsequence with the new coordinate order. In addition, we introduce a neural\nnetwork to efficiently estimate the similarity between shape context\ndescriptors, which allows to speed up the computation of silhouette\ncoefficients and thus the training of the axis ordering network. Two user\nstudies demonstrate that the orders provided by our method are preferred by\nusers for perceiving class separation. We tested our model on different\nsettings to show its robustness and generalization abilities and demonstrate\nthat it allows to order input sets with unseen data size, data dimension, or\nnumber of classes. We also demonstrate that our model can be adapted to\ncoordinate ordering of other types of plots such as RadViz by replacing the\nproposed shape-aware silhouette coefficient with the corresponding quality\nmetric to guide network training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:05:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hu", "Ruizhen", ""], ["Chen", "Bin", ""], ["Xu", "Juzhan", ""], ["van Kaick", "Oliver", ""], ["Deussen", "Oliver", ""], ["Huang", "Hui", ""]]}, {"id": "2103.02486", "submitter": "Julius Nehring-Wirxel", "authors": "Julius Nehring-Wirxel and Philip Trettner and Leif Kobbelt", "title": "Fast Exact Booleans for Iterated CSG using Octree-Embedded BSPs", "comments": null, "journal-ref": null, "doi": "10.1016/j.cad.2021.103015", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present octree-embedded BSPs, a volumetric mesh data structure suited for\nperforming a sequence of Boolean operations (iterated CSG) efficiently. At its\ncore, our data structure leverages a plane-based geometry representation and\ninteger arithmetics to guarantee unconditionally robust operations. These\ntypically present considerable performance challenges which we overcome by\nusing custom-tailored fixed-precision operations and an efficient algorithm for\ncutting a convex mesh against a plane. Consequently, BSP Booleans and mesh\nextraction are formulated in terms of mesh cutting. The octree is used as a\nglobal acceleration structure to keep modifications local and bound the BSP\ncomplexity. With our optimizations, we can perform up to 2.5 million mesh-plane\ncuts per second on a single core, which creates roughly 40-50 million output\nBSP nodes for CSG. We demonstrate our system in two iterated CSG settings:\nsweep volumes and a milling simulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:54:50 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Nehring-Wirxel", "Julius", ""], ["Trettner", "Philip", ""], ["Kobbelt", "Leif", ""]]}, {"id": "2103.02502", "submitter": "Min Chen", "authors": "Min Chen, Alfie Abdul-Rahman, Deborah Silver, and Mateu Sbert", "title": "A Bounded Measure for Estimating the Benefit of Visualization: Case\n  Studies and Empirical Evaluation", "comments": "Following the SciVis 2020 reviewers' request for more explanation and\n  clarification, the origianl article, \"A Bounded Measure for Estimating the\n  Benefit of Visualization, arxiv:2002.05282\", has been split into two\n  articles, on \"Theoretical Discourse and Conceptual Evaluation\" and \"Case\n  Studies and Empirical Evaluation\" respectively. This is the second article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many visual representations, such as volume-rendered images and metro maps,\nfeature a noticeable amount of information loss. At a glance, there seem to be\nnumerous opportunities for viewers to misinterpret the data being visualized,\nhence undermining the benefits of these visual representations. In practice,\nthere is little doubt that these visual representations are useful. The\nrecently-proposed information-theoretic measure for analyzing the cost-benefit\nratio of visualization processes can explain such usefulness experienced in\npractice, and postulate that the viewers' knowledge can reduce the potential\ndistortion (e.g., misinterpretation) due to information loss. This suggests\nthat viewers' knowledge can be estimated by comparing the potential distortion\nwithout any knowledge and the actual distortion with some knowledge. In this\npaper, we describe several case studies for collecting instances that can (i)\nsupport the evaluation of several candidate measures for estimating the\npotential distortion distortion in visualization, and (ii) demonstrate their\napplicability in practical scenarios. Because the theoretical discourse on\nchoosing an appropriate bounded measure for estimating the potential distortion\nis yet conclusive, it is the real world data about visualization further\ninforms the selection of a bounded measure, providing practical evidence to aid\na theoretical conclusion. Meanwhile, once we can measure the potential\ndistortion in a bounded manner, we can interpret the numerical values\ncharacterizing the benefit of visualization more intuitively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:12:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Min", ""], ["Abdul-Rahman", "Alfie", ""], ["Silver", "Deborah", ""], ["Sbert", "Mateu", ""]]}, {"id": "2103.02505", "submitter": "Min Chen", "authors": "Min Chen and Mateu Sbert", "title": "A Bounded Measure for Estimating the Benefit of Visualization:\n  Theoretical Discourse and Conceptual Evaluation", "comments": "Following the SciVis 2020 reviewers' request for more explanation and\n  clarification, the origianl article, \"A Bounded Measure for Estimating the\n  Benefit of Visualization, arxiv:2002.05282\", was split into two articles, on\n  \"Theoretical Discourse and Conceptual Evaluation\" and \"Case Studies and\n  Empirical Evaluation\" respectively. This is the first article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.GR cs.HC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory can be used to analyze the cost-benefit of visualization\nprocesses. However, the current measure of benefit contains an unbounded term\nthat is neither easy to estimate nor intuitive to interpret. In this work, we\npropose to revise the existing cost-benefit measure by replacing the unbounded\nterm with a bounded one. We examine a number of bounded measures that include\nthe Jenson-Shannon divergence and a new divergence measure formulated as part\nof this work. We describe the rationale for proposing a new divergence measure.\nAs the first part of comparative evaluation, we use visual analysis to support\nthe multi-criteria comparison, narrowing the search down to several options\nwith better mathematical properties. The theoretical discourse and conceptual\nevaluation in this paper provide the basis for further comparative evaluation\nthrough synthetic and experimental case studies, which are to be reported in a\nseparate paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:14:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chen", "Min", ""], ["Sbert", "Mateu", ""]]}, {"id": "2103.02533", "submitter": "Yunbo Zhang", "authors": "Yunbo Zhang, Wenhao Yu, C. Karen Liu, Charles C. Kemp, Greg Turk", "title": "Learning to Manipulate Amorphous Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of training character manipulation of amorphous materials\nsuch as those often used in cooking. Common examples of amorphous materials\ninclude granular materials (salt, uncooked rice), fluids (honey), and\nvisco-plastic materials (sticky rice, softened butter). A typical task is to\nspread a given material out across a flat surface using a tool such as a\nscraper or knife. We use reinforcement learning to train our controllers to\nmanipulate materials in various ways. The training is performed in a physics\nsimulator that uses position-based dynamics of particles to simulate the\nmaterials to be manipulated. The neural network control policy is given\nobservations of the material (e.g. a low-resolution density map), and the\npolicy outputs actions such as rotating and translating the knife. We\ndemonstrate policies that have been successfully trained to carry out the\nfollowing tasks: spreading, gathering, and flipping. We produce a final\nanimation by using inverse kinematics to guide a character's arm and hand to\nmatch the motion of the manipulation tool such as a knife or a frying pan.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:17:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhang", "Yunbo", ""], ["Yu", "Wenhao", ""], ["Liu", "C. Karen", ""], ["Kemp", "Charles C.", ""], ["Turk", "Greg", ""]]}, {"id": "2103.02597", "submitter": "Zhaoyang Lv", "authors": "Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green, Christoph\n  Lassner, Changil Kim, Tanner Schmidt, Steven Lovegrove, Michael Goesele,\n  Zhaoyang Lv", "title": "Neural 3D Video Synthesis", "comments": "Project website: https://neural-3d-video.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach for 3D video synthesis that is able to represent\nmulti-view video recordings of a dynamic real-world scene in a compact, yet\nexpressive representation that enables high-quality view synthesis and motion\ninterpolation. Our approach takes the high quality and compactness of static\nneural radiance fields in a new direction: to a model-free, dynamic setting. At\nthe core of our approach is a novel time-conditioned neural radiance fields\nthat represents scene dynamics using a set of compact latent codes. To exploit\nthe fact that changes between adjacent frames of a video are typically small\nand locally consistent, we propose two novel strategies for efficient training\nof our neural network: 1) An efficient hierarchical training scheme, and 2) an\nimportance sampling strategy that selects the next rays for training based on\nthe temporal variation of the input videos. In combination, these two\nstrategies significantly boost the training speed, lead to fast convergence of\nthe training process, and enable high quality results. Our learned\nrepresentation is highly compact and able to represent a 10 second 30 FPS\nmulti-view video recording by 18 cameras with a model size of just 28MB. We\ndemonstrate that our method can render high-fidelity wide-angle novel views at\nover 1K resolution, even for highly complex and dynamic scenes. We perform an\nextensive qualitative and quantitative evaluation that shows that our approach\noutperforms the current state of the art. We include additional video and\ninformation at: https://neural-3d-video.github.io/\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:47:40 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Tianye", ""], ["Slavcheva", "Mira", ""], ["Zollhoefer", "Michael", ""], ["Green", "Simon", ""], ["Lassner", "Christoph", ""], ["Kim", "Changil", ""], ["Schmidt", "Tanner", ""], ["Lovegrove", "Steven", ""], ["Goesele", "Michael", ""], ["Lv", "Zhaoyang", ""]]}, {"id": "2103.02992", "submitter": "Min Lu", "authors": "Or Malkai, Min Lu, Daniel Cohen-Or", "title": "Clusterplot: High-dimensional Cluster Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Clusterplot, a multi-class high-dimensional data visualization\ntool designed to visualize cluster-level information offering an intuitive\nunderstanding of the cluster inter-relations. Our unique plots leverage 2D\nblobs devised to convey the geometrical and topological characteristics of\nclusters within the high-dimensional data, and their pairwise relations, such\nthat general inter-cluster behavior is easily interpretable in the plot. Class\nidentity supervision is utilized to drive the measuring of relations among\nclusters in high-dimension, particularly, proximity and overlap, which are then\nreflected spatially through the 2D blobs. We demonstrate the strength of our\nclusterplots and their ability to deliver a clear and intuitive informative\nexploration experience for high-dimensional clusters characterized by complex\nstructure and significant overlap.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:34:14 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Malkai", "Or", ""], ["Lu", "Min", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2103.03231", "submitter": "Thomas Neff", "authors": "Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas Kurz, Joerg H.\n  Mueller, Chakravarty R. Alla Chaitanya, Anton Kaplanyan, Markus Steinberger", "title": "DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields\n  using Depth Oracle Networks", "comments": "Accepted to EGSR 2021 in the CGF track; Project website:\n  https://depthoraclenerf.github.io/", "journal-ref": "Computer Graphics Forum Volume 40, Issue 4, 2021", "doi": "10.1111/cgf.14340", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent research explosion around implicit neural representations, such as\nNeRF, shows that there is immense potential for implicitly storing high-quality\nscene and lighting information in compact neural networks. However, one major\nlimitation preventing the use of NeRF in real-time rendering applications is\nthe prohibitive computational cost of excessive network evaluations along each\nview ray, requiring dozens of petaFLOPS. In this work, we bring compact neural\nrepresentations closer to practical rendering of synthetic content in real-time\napplications, such as games and virtual reality. We show that the number of\nsamples required for each view ray can be significantly reduced when samples\nare placed around surfaces in the scene without compromising image quality. To\nthis end, we propose a depth oracle network that predicts ray sample locations\nfor each view ray with a single network evaluation. We show that using a\nclassification network around logarithmically discretized and spherically\nwarped depth values is essential to encode surface locations rather than\ndirectly estimating depth. The combination of these techniques leads to DONeRF,\nour compact dual network design with a depth oracle network as its first step\nand a locally sampled shading network for ray accumulation. With DONeRF, we\nreduce the inference costs by up to 48x compared to NeRF when conditioning on\navailable ground truth depth information. Compared to concurrent acceleration\nmethods for raymarching-based neural representations, DONeRF does not require\nadditional memory for explicit caching or acceleration structures, and can\nrender interactively (20 frames per second) on a single GPU.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:09 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:57:56 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 09:56:38 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 09:05:10 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Neff", "Thomas", ""], ["Stadlbauer", "Pascal", ""], ["Parger", "Mathias", ""], ["Kurz", "Andreas", ""], ["Mueller", "Joerg H.", ""], ["Chaitanya", "Chakravarty R. Alla", ""], ["Kaplanyan", "Anton", ""], ["Steinberger", "Markus", ""]]}, {"id": "2103.04033", "submitter": "Mohammad Idrisi Dr.", "authors": "Dr. M. Javed Idrisi and Aayesha Ashraf", "title": "An Effective Approach to Minimize Error in Midpoint Ellipse Drawing\n  Algorithm", "comments": "12 pages, 7 tables and 3 figures", "journal-ref": "Internaational Journal of Engineering Research and Technology,\n  Vol. 10, Issue 2, p. 621-625, 2021", "doi": null, "report-no": "IJERTV10IS020274", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper deals with the generalization of Midpoint Ellipse Drawing\nAlgorithm (MPEDA) to minimize the error in the existing MPEDA in cartesian\nform. In this method, we consider three different values of h, i.e., 1, 0.5 and\n0.1. For h = 1, all the results of MPEDA have been verified. For other values\nof h it is observed that as the value of h decreases, the number of iteration\nincreases but the error between the points generated and the original ellipse\npoints decreases and vice-versa.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:26:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Idrisi", "Dr. M. Javed", ""], ["Ashraf", "Aayesha", ""]]}, {"id": "2103.04183", "submitter": "Yuxuan Yu", "authors": "Yuxuan Yu, Jialei Ginny Liu and Yongjie Jessica Zhang", "title": "HexDom: Polycube-Based Hexahedral-Dominant Mesh Generation", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.14213", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend our earlier polycube-based all-hexahedral mesh\ngeneration method to hexahedral-dominant mesh generation, and present the\nHexDom software package. Given the boundary representation of a solid model,\nHexDom creates a hex-dominant mesh by using a semi-automated polycube-based\nmesh generation method. The resulting hexahedral dominant mesh includes\nhexahedra, tetrahedra, and triangular prisms. By adding non-hexahedral\nelements, we are able to generate better quality hexahedral elements than in\nall-hexahedral meshes. We explain the underlying algorithms in four modules\nincluding segmentation, polycube construction, hex-dominant mesh generation and\nquality improvement, and use a rockerarm model to explain how to run the\nsoftware. We also apply our software to a number of other complex models to\ntest their robustness. The software package and all tested models are availabe\nin github (https://github.com/CMU-CBML/HexDom).\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:51:07 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 15:19:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yu", "Yuxuan", ""], ["Liu", "Jialei Ginny", ""], ["Zhang", "Yongjie Jessica", ""]]}, {"id": "2103.05606", "submitter": "Pakkapon Phongthawee", "authors": "Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai,\n  Supasorn Suwajanakorn", "title": "NeX: Real-time View Synthesis with Neural Basis Expansion", "comments": "CVPR 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NeX, a new approach to novel view synthesis based on enhancements\nof multiplane image (MPI) that can reproduce next-level view-dependent effects\n-- in real time. Unlike traditional MPI that uses a set of simple RGB$\\alpha$\nplanes, our technique models view-dependent effects by instead parameterizing\neach pixel as a linear combination of basis functions learned from a neural\nnetwork. Moreover, we propose a hybrid implicit-explicit modeling strategy that\nimproves upon fine detail and produces state-of-the-art results. Our method is\nevaluated on benchmark forward-facing datasets as well as our newly-introduced\ndataset designed to test the limit of view-dependent modeling with\nsignificantly more challenging effects such as rainbow reflections on a CD. Our\nmethod achieves the best overall scores across all major metrics on these\ndatasets with more than 1000$\\times$ faster rendering time than the state of\nthe art. For real-time demos, visit https://nex-mpi.github.io/\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:27:27 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 09:40:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wizadwongsa", "Suttisak", ""], ["Phongthawee", "Pakkapon", ""], ["Yenphraphai", "Jiraphon", ""], ["Suwajanakorn", "Supasorn", ""]]}, {"id": "2103.05640", "submitter": "Arun Srinivasa", "authors": "Zhujiang Wang, Arun R. Srinivasa, J.N. Reddy, Adam Dubrowski", "title": "FlowMesher: An automatic unstructured mesh generation algorithm with\n  applications from finite element analysis to medical simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an automatic mesh generation algorithm, FlowMesher,\nwhich can be used to generate unstructured meshes for mesh domains in any shape\nwith minimum (or even no) user intervention. The approach can generate\nhigh-quality simplex meshes directly from scanned images in OBJ format in 2D\nand 3D or just from a line drawing in 2-D. Mesh grading can be easily\ncontrolled also. The FlowMesher is robust and easy to be implemented and is\nuseful for a variety of applications including surgical simulators.\n  The core idea of the FlowMesher is that a mesh domain is considered as an\n\"airtight container\" into which fluid particles are \"injected\" at one or\nmultiple selected interior points. The particles repel each other and occupy\nthe whole domain somewhat like blowing up a balloon. When the container is full\nof fluid particles and the flow is stopped, a Delaunay triangulation algorithm\nis employed to link the fluid particles together to generate an unstructured\nmesh (which is then optimized using a combination of automated mesh smoothing\nand element removal in 3D). The performance of the FlowMesher is demonstrated\nby generating meshes for several 2D and 3D mesh domains including a scanned\nimage of a bone.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:49:45 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Zhujiang", ""], ["Srinivasa", "Arun R.", ""], ["Reddy", "J. N.", ""], ["Dubrowski", "Adam", ""]]}, {"id": "2103.05678", "submitter": "Wilson Marc\\'ilio-Jr", "authors": "Wilson Est\\'ecio Marc\\'ilio J\\'unior and Danilo Medeiros Eler", "title": "Explaining dimensionality reduction results using Shapley values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) techniques have been consistently supporting\nhigh-dimensional data analysis in various applications. Besides the patterns\nuncovered by these techniques, the interpretation of DR results based on each\nfeature's contribution to the low-dimensional representation supports new finds\nthrough exploratory analysis. Current literature approaches designed to\ninterpret DR techniques do not explain the features' contributions well since\nthey focus only on the low-dimensional representation or do not consider the\nrelationship among features. This paper presents ClusterShapley to address\nthese problems, using Shapley values to generate explanations of dimensionality\nreduction techniques and interpret these algorithms using a cluster-oriented\nanalysis. ClusterShapley explains the formation of clusters and the meaning of\ntheir relationship, which is useful for exploratory data analysis in various\ndomains. We propose novel visualization techniques to guide the interpretation\nof features' contributions on clustering formation and validate our methodology\nthrough case studies of publicly available datasets. The results demonstrate\nour approach's interpretability and analysis power to generate insights about\npathologies and patients in different conditions using DR results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:28:10 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["J\u00fanior", "Wilson Est\u00e9cio Marc\u00edlio", ""], ["Eler", "Danilo Medeiros", ""]]}, {"id": "2103.05875", "submitter": "Michael Stengel", "authors": "Michael Stengel, Zander Majercik, Benjamin Boudaoud, Morgan McGuire", "title": "A Distributed, Decoupled System for Losslessly Streaming Dynamic Light\n  Probes to Thin Clients", "comments": "12 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present a networked, high performance graphics system that combines\ndynamic, high quality, ray traced global illumination computed on a server with\ndirect illumination and primary visibility computed on a client. This approach\nprovides many of the image quality benefits of real-time ray tracing on\nlow-power and legacy hardware, while maintaining a low latency response and\nmobile form factor. Our system distributes the graphic pipeline over a network\nby computing diffuse global illumination on a remote machine. Global\nillumination is computed using a recent irradiance volume representation\ncombined with a novel, lossless, HEVC-based, hardware-accelerated encoding, and\na perceptually-motivated update scheme. Our experimental implementation streams\nthousands of irradiance probes per second and requires less than 50 Mbps of\nthroughput, reducing the consumed bandwidth by 99.4% when streaming at 60 Hz\ncompared to traditional lossless texture compression. This bandwidth reduction\nallows higher quality and lower latency graphics than state-of-the-art remote\nrendering via video streaming. In addition, our split-rendering solution\ndecouples remote computation from local rendering and so does not limit local\ndisplay update rate or resolution.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 05:21:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Stengel", "Michael", ""], ["Majercik", "Zander", ""], ["Boudaoud", "Benjamin", ""], ["McGuire", "Morgan", ""]]}, {"id": "2103.06084", "submitter": "Loann Giovannangeli", "authors": "Loann Giovannangeli, Romain Giot, David Auber and Romain Bourqui", "title": "Impacts of the Numbers of Colors and Shapes on Outlier Detection: from\n  Automated to User Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of efficient representations is well established as a fruitful way\nto explore and analyze complex or large data. In these representations, data\nare encoded with various visual attributes depending on the needs of the\nrepresentation itself. To make coherent design choices about visual attributes,\nthe visual search field proposes guidelines based on the human brain perception\nof features. However, information visualization representations frequently need\nto depict more data than the amount these guidelines have been validated on.\nSince, the information visualization community has extended these guidelines to\na wider parameter space.\n  This paper contributes to this theme by extending visual search theories to\nan information visualization context. We consider a visual search task where\nsubjects are asked to find an unknown outlier in a grid of randomly laid out\ndistractor. Stimuli are defined by color and shape features for the purpose of\nvisually encoding categorical data. The experimental protocol is made of a\nparameters space reduction step (i.e., sub-sampling) based on a machine\nlearning model, and a user evaluation to measure capacity limits and validate\nhypotheses. The results show that the major difficulty factor is the number of\nvisual attributes that are used to encode the outlier. When redundantly\nencoded, the display heterogeneity has no effect on the task. When encoded with\none attribute, the difficulty depends on that attribute heterogeneity until its\ncapacity limit (7 for color, 5 for shape) is reached. Finally, when encoded\nwith two attributes simultaneously, performances drop drastically even with\nminor heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:35:53 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Giovannangeli", "Loann", ""], ["Giot", "Romain", ""], ["Auber", "David", ""], ["Bourqui", "Romain", ""]]}, {"id": "2103.06139", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Pierre-Alain Fayolle", "title": "On the Complexity of the CSG Tree Extraction Problem", "comments": "Add references for the programming language based approaches and the\n  construction of the intersection graph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we discuss the complexity of the search space for the\nproblem of finding a CSG expression (or CSG tree) corresponding to an input\npoint-cloud and a list of fitted solid primitives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:52:51 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 04:41:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Friedrich", "Markus", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2103.06878", "submitter": "Dongdong Chen", "authors": "Zhentao Tan and Menglei Chai and Dongdong Chen and Jing Liao and Qi\n  Chu and Bin Liu and Gang Hua and Nenghai Yu", "title": "Diverse Semantic Image Synthesis via Probability Distribution Modeling", "comments": "Accepted By CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image synthesis, translating semantic layouts to photo-realistic\nimages, is a one-to-many mapping problem. Though impressive progress has been\nrecently made, diverse semantic synthesis that can efficiently produce\nsemantic-level multimodal results, still remains a challenge. In this paper, we\npropose a novel diverse semantic image synthesis framework from the perspective\nof semantic class distributions, which naturally supports diverse generation at\nsemantic or even instance level. We achieve this by modeling class-level\nconditional modulation parameters as continuous probability distributions\ninstead of discrete values, and sampling per-instance modulation parameters\nthrough instance-adaptive stochastic sampling that is consistent across the\nnetwork. Moreover, we propose prior noise remapping, through linear\nperturbation parameters encoded from paired references, to facilitate\nsupervised training and exemplar-based instance style control at test time.\nExtensive experiments on multiple datasets show that our method can achieve\nsuperior diversity and comparable quality compared to state-of-the-art methods.\nCode will be available at \\url{https://github.com/tzt101/INADE.git}\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:59:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Tan", "Zhentao", ""], ["Chai", "Menglei", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Chu", "Qi", ""], ["Liu", "Bin", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2103.07013", "submitter": "Brennan Shacklett", "authors": "Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva,\n  Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian", "title": "Large Batch Simulation for Deep Reinforcement Learning", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We accelerate deep reinforcement learning-based training in visually complex\n3D environments by two orders of magnitude over prior work, realizing\nend-to-end training speeds of over 19,000 frames of experience per second on a\nsingle GPU and up to 72,000 frames per second on a single eight-GPU machine.\nThe key idea of our approach is to design a 3D renderer and embodied navigation\nsimulator around the principle of \"batch simulation\": accepting and executing\nlarge batches of requests simultaneously. Beyond exposing large amounts of work\nat once, batch simulation allows implementations to amortize in-memory storage\nof scene assets, rendering work, data loading, and synchronization costs across\nmany simulation requests, dramatically improving the number of simulated agents\nper GPU and overall simulation throughput. To balance DNN inference and\ntraining costs with faster simulation, we also build a computationally\nefficient policy DNN that maintains high task performance, and modify training\nalgorithms to maintain sample efficiency when training with large mini-batches.\nBy combining batch simulation and DNN performance optimizations, we demonstrate\nthat PointGoal navigation agents can be trained in complex 3D environments on a\nsingle GPU in 1.5 days to 97% of the accuracy of agents trained on a prior\nstate-of-the-art system using a 64-GPU cluster over three days. We provide\nopen-source reference implementations of our batch 3D renderer and simulator to\nfacilitate incorporation of these ideas into RL systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:22:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shacklett", "Brennan", ""], ["Wijmans", "Erik", ""], ["Petrenko", "Aleksei", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""], ["Koltun", "Vladlen", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2103.07658", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R, Ayush Tewari, Abdallah Dib, Tim Weyrich, Bernd\n  Bickel, Hans-Peter Seidel, Hanspeter Pfister, Wojciech Matusik, Louis\n  Chevallier, Mohamed Elgharib, Christian Theobalt", "title": "PhotoApp: Photorealistic Appearance Editing of Head Portraits", "comments": "http://gvv.mpi-inf.mpg.de/projects/PhotoApp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic editing of portraits is a challenging task as humans are very\nsensitive to inconsistencies in faces. We present an approach for high-quality\nintuitive editing of the camera viewpoint and scene illumination in a portrait\nimage. This requires our method to capture and control the full reflectance\nfield of the person in the image. Most editing approaches rely on supervised\nlearning using training data captured with setups such as light and camera\nstages. Such datasets are expensive to acquire, not readily available and do\nnot capture all the rich variations of in-the-wild portrait images. In\naddition, most supervised approaches only focus on relighting, and do not allow\ncamera viewpoint editing. Thus, they only capture and control a subset of the\nreflectance field. Recently, portrait editing has been demonstrated by\noperating in the generative model space of StyleGAN. While such approaches do\nnot require direct supervision, there is a significant loss of quality when\ncompared to the supervised approaches. In this paper, we present a method which\nlearns from limited supervised training data. The training images only include\npeople in a fixed neutral expression with eyes closed, without much hair or\nbackground variations. Each person is captured under 150 one-light-at-a-time\nconditions and under 8 camera poses. Instead of training directly in the image\nspace, we design a supervised problem which learns transformations in the\nlatent space of StyleGAN. This combines the best of supervised learning and\ngenerative adversarial modeling. We show that the StyleGAN prior allows for\ngeneralisation to different expressions, hairstyles and backgrounds. This\nproduces high-quality photorealistic results for in-the-wild images and\nsignificantly outperforms existing methods. Our approach can edit the\nillumination and pose simultaneously, and runs at interactive rates.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 08:59:49 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 17:59:43 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Dib", "Abdallah", ""], ["Weyrich", "Tim", ""], ["Bickel", "Bernd", ""], ["Seidel", "Hans-Peter", ""], ["Pfister", "Hanspeter", ""], ["Matusik", "Wojciech", ""], ["Chevallier", "Louis", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2103.07745", "submitter": "Marco Livesu", "authors": "Marco Livesu, Luca Pitzalis, Gianmarco Cherchi", "title": "Optimal Dual Schemes for Adaptive Grid Based Hexmeshing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hexahedral meshes are an ubiquitous domain for the numerical resolution of\npartial differential equations. Computing a pure hexahedral mesh from an\nadaptively refined grid is a prominent approach to automatic hexmeshing, and\nrequires the ability to restore the all hex property around the hanging nodes\nthat arise at the interface between cells having different size. The most\nadvanced tools to accomplish this task are based on mesh dualization. These\napproaches use topological schemes to regularize the valence of inner vertices\nand edges, such that dualizing the grid yields a pure hexahedral mesh. In this\npaper we study in detail the dual approach, and propose four main contributions\nto it: (i) we enumerate all the possible transitions that dual methods must be\nable to handle, showing that prior schemes do not natively cover all of them;\n(ii) we show that schemes are internally asymmetric, therefore not only their\nimplementation is ambiguous, but different implementation choices lead to\nhexahedral meshes with different singular structure; (iii) we explore the\ncombinatorial space of dual schemes, selecting the minimum set that covers all\nthe possible configurations and also yields the simplest singular structure in\nthe output hexmesh; (iv) we enlarge the class of adaptive grids that can be\ntransformed into pure hexahedral meshes, relaxing one of the tight requirements\nimposed by previous approaches, and ultimately permitting to obtain much\ncoarser meshes for same geometric accuracy. Last but not least, for the first\ntime we make grid-based hexmeshing truly reproducible, releasing our code and\nalso revealing a conspicuous amount of technical details that were always\noverlooked in previous literature, creating an entry barrier that was hard to\novercome for practitioners in the field.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 16:30:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Livesu", "Marco", ""], ["Pitzalis", "Luca", ""], ["Cherchi", "Gianmarco", ""]]}, {"id": "2103.07987", "submitter": "Daniel McDuff", "authors": "Daniel McDuff and Ewa Nowara", "title": "\"Warm Bodies\": A Post-Processing Technique for Animating Dynamic Blood\n  Flow on Photos and Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What breathes life into an embodied agent or avatar? While body motions such\nas facial expressions, speech and gestures have been well studied, relatively\nlittle attention has been applied to subtle changes due to underlying\nphysiology. We argue that subtle pulse signals are important for creating more\nlifelike and less disconcerting avatars. We propose a method for animating\nblood flow patterns, based on a data-driven physiological model that can be\nused to directly augment the appearance of synthetic avatars and\nphoto-realistic faces. While the changes are difficult for participants to\n\"see\", they significantly more frequently select faces with blood flow as more\nanthropomorphic and animated than faces without blood flow. Furthermore, by\nmanipulating the frequency of the heart rate in the underlying signal we can\nchange the perceived arousal of the character.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:09:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["McDuff", "Daniel", ""], ["Nowara", "Ewa", ""]]}, {"id": "2103.07992", "submitter": "Vasilis Toulatzis", "authors": "Vasilis Toulatzis, Ioannis Fudos", "title": "Deep Tiling: Texture Tile Synthesis Using a Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texturing is a fundamental process in computer graphics. Texture is leveraged\nto enhance the visualization outcome for a 3D scene. In many cases a texture\nimage cannot cover a large 3D model surface because of its small resolution.\nConventional techniques like repeating, mirror repeating or clamp to edge do\nnot yield visually acceptable results. Deep learning based texture synthesis\nhas proven to be very effective in such cases. All deep texture synthesis\nmethods trying to create larger resolution textures are limited in terms of GPU\nmemory resources. In this paper, we propose a novel approach to example-based\ntexture synthesis by using a robust deep learning process for creating tiles of\narbitrary resolutions that resemble the structural components of an input\ntexture. In this manner, our method is firstly much less memory limited owing\nto the fact that a new texture tile of small size is synthesized and merged\nwith the original texture and secondly can easily produce missing parts of a\nlarge texture.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:17:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Toulatzis", "Vasilis", ""], ["Fudos", "Ioannis", ""]]}, {"id": "2103.08141", "submitter": "Yufeng Zhu", "authors": "Yufeng Zhu", "title": "Eigen Space of Mesh Distortion Energy Hessian", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mesh distortion optimization is a popular research topic and has wide range\nof applications in computer graphics, including geometry modeling, variational\nshape interpolation, UV parameterization, elastoplastic simulation, etc. In\nrecent years, many solvers have been proposed to solve this nonlinear\noptimization efficiently, among which projected Newton has been shown to have\nbest convergence rate and work well in both 2D and 3D applications. Traditional\nNewton approach suffers from ill conditioning and indefiniteness of local\nenergy approximation. A crucial step in projected Newton is to fix this issue\nby projecting energy Hessian onto symmetric positive definite (SPD) cone so as\nto guarantee the search direction always pointing to decrease the energy\nlocally. Such step relies on time consuming eigen decomposition of element\nHessian, which has been addressed by several work before on how to obtain a\nconjugacy that is as diagonal as possible. In this report, we demonstrate an\nanalytic form of Hessian eigen system for distortion energy defined using\nprincipal stretches, which is the most general representation. Compared with\nexisting projected Newton diagonalization approaches, our formulation is more\ngeneral as it doesn't require the energy to be representable by tensor\ninvariants. In this report, we will only show the derivation for 3D and the\nextension to 2D case is straightforward.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:15:11 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 02:34:24 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhu", "Yufeng", ""]]}, {"id": "2103.08275", "submitter": "Yue Wu", "authors": "Hua Wang, Yue Wu, Xu Han, Mingliang Xu, Weizhe Chen, Guoliang Chen", "title": "Automatic Generation of Large-scale 3D Road Networks based on GIS Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How to automatically generate a realistic large-scale 3D road network is a\nkey point for immersive and credible traffic simulations. Existing methods\ncannot automatically generate various kinds of intersections in 3D space based\non GIS data. In this paper, we propose a method to generate complex and\nlarge-scale 3D road networks automatically with the open source GIS data,\nincluding satellite imagery, elevation data and two-dimensional(2D) road center\naxis data, as input. We first introduce a semantic structure of road network to\nobtain high-detailed and well-formed networks in a 3D scene. We then generate\n2D shapes and topological data of the road network according to the semantic\nstructure and 2D road center axis data. At last, we segment the elevation data\nand generate the surface of the 3D road network according to the 2D semantic\ndata and satellite imagery data. Results show that our method does well in the\ngeneration of various types of intersections and the high-detailed features of\nroads. The traffic semantic structure, which must be provided in traffic\nsimulation, can also be generated automatically according to our method.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 11:02:36 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Hua", ""], ["Wu", "Yue", ""], ["Han", "Xu", ""], ["Xu", "Mingliang", ""], ["Chen", "Weizhe", ""], ["Chen", "Guoliang", ""]]}, {"id": "2103.09448", "submitter": "Mazen Abdelfattah Mr", "authors": "Mazen Abdelfattah, Kaiwen Yuan, Z. Jane Wang, and Rabab Ward", "title": "Adversarial Attacks on Camera-LiDAR Models for 3D Car Detection", "comments": "arXiv admin note: text overlap with arXiv:2101.10747", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most autonomous vehicles (AVs) rely on LiDAR and RGB camera sensors for\nperception. Using these point cloud and image data, perception models based on\ndeep neural nets (DNNs) have achieved state-of-the-art performance in 3D\ndetection. The vulnerability of DNNs to adversarial attacks have been heavily\ninvestigated in the RGB image domain and more recently in the point cloud\ndomain, but rarely in both domains simultaneously. Multi-modal perception\nsystems used in AVs can be divided into two broad types: cascaded models which\nuse each modality independently, and fusion models which learn from different\nmodalities simultaneously. We propose a universal and physically realizable\nadversarial attack for each type, and study and contrast their respective\nvulnerabilities to attacks. We place a single adversarial object with specific\nshape and texture on top of a car with the objective of making this car evade\ndetection. Evaluating on the popular KITTI benchmark, our adversarial object\nmade the host vehicle escape detection by each model type nearly 50% of the\ntime. The dense RGB input contributed more to the success of the adversarial\nattacks on both cascaded and fusion models. We found that the fusion model was\nrelatively more robust to adversarial attacks than the cascaded model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 05:24:48 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abdelfattah", "Mazen", ""], ["Yuan", "Kaiwen", ""], ["Wang", "Z. Jane", ""], ["Ward", "Rabab", ""]]}, {"id": "2103.09583", "submitter": "Stefan Ohrhallinger", "authors": "Stefan Ohrhallinger and Jiju Peethambaran and Amal D. Parakkat and\n  Tamal K. Dey and Ramanathan Muthuganapathy", "title": "2D Points Curve Reconstruction Survey and Benchmark", "comments": "24 pages, 22 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Curve reconstruction from unstructured points in a plane is a fundamental\nproblem with many applications that has generated research interest for\ndecades. Involved aspects like handling open, sharp, multiple and non-manifold\noutlines, run-time and provability as well as potential extension to 3D for\nsurface reconstruction have led to many different algorithms. We survey the\nliterature on 2D curve reconstruction and then present an open-sourced\nbenchmark for the experimental study. Our unprecedented evaluation on a\nselected set of planar curve reconstruction algorithms aims to give an overview\nof both quantitative analysis and qualitative aspects for helping users to\nselect the right algorithm for specific problems in the field. Our benchmark\nframework is available online to permit reproducing the results, and easy\nintegration of new algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:55:43 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ohrhallinger", "Stefan", ""], ["Peethambaran", "Jiju", ""], ["Parakkat", "Amal D.", ""], ["Dey", "Tamal K.", ""], ["Muthuganapathy", "Ramanathan", ""]]}, {"id": "2103.09988", "submitter": "Yue Wu", "authors": "Mingliang Xu, Hua Wang, Zhigang Deng, Yue Wu", "title": "Human-Oriented Autonomous Traffic Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an innovative framework, Crowdsourcing Autonomous Traffic\nSimulation (CATS) framework, in order to safely implement and realize orderly\ntraffic flows. We firstly provide a semantic description of the CATS framework\nusing theories of economics to construct coupling constraints among drivers, in\nwhich drivers monitor each other by making use of transportation resources and\ndriving credit. We then introduce an emotion-based traffic simulation, which\nutilizes the Weber-Fechner law to integrate economic factors into drivers'\nbehaviors. Simulation results show that the CATS framework can significantly\nreduce traffic accidents and improve urban traffic conditions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:25:18 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Xu", "Mingliang", ""], ["Wang", "Hua", ""], ["Deng", "Zhigang", ""], ["Wu", "Yue", ""]]}, {"id": "2103.10390", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Challenges of 3D Surface Reconstruction in Capsule Endoscopy", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are currently many challenges related to three-dimensional (3D) surface\nreconstruction using capsule endoscopy (CE) images. There are also challenges\nrelated to viewing the content of reconstructed 3D surfaces. In this\npreliminary investigation, the author focuses on the latter and evaluates their\neffects on the content of reconstructed 3D surfaces using CE images. The\nevaluation of such challenges is preliminarily conducted into two parts. The\nfirst part focuses on the comparison of the content of 3D surfaces\nreconstructed using both preprocessed and non-preprocessed CE images. The\nsecond part focuses on the comparison of the content of 3D surfaces viewed at\nthe same azimuth angles and different elevation angles of the line-of-sight.\nThe experiments demonstrated the need for generalizable line-of-sight and\nadvanced CE image preprocessing means as well as further research in 3D surface\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:18:48 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2103.10428", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I\n  Chang, Yan Xu", "title": "Large Scale Image Completion via Co-Modulated Generative Adversarial\n  Networks", "comments": "ICLR 2021 (Spotlight). Code: https://github.com/zsyzzsoft/co-mod-gan\n  Demo: https://comodgan.ml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous task-specific variants of conditional generative adversarial\nnetworks have been developed for image completion. Yet, a serious limitation\nremains that all existing algorithms tend to fail when handling large-scale\nmissing regions. To overcome this challenge, we propose a generic new approach\nthat bridges the gap between image-conditional and recent modulated\nunconditional generative architectures via co-modulation of both conditional\nand stochastic style representations. Also, due to the lack of good\nquantitative metrics for image completion, we propose the new Paired/Unpaired\nInception Discriminative Score (P-IDS/U-IDS), which robustly measures the\nperceptual fidelity of inpainted images compared to real images via linear\nseparability in a feature space. Experiments demonstrate superior performance\nin terms of both quality and diversity over state-of-the-art methods in\nfree-form image completion and easy generalization to image-to-image\ntranslation. Code is available at https://github.com/zsyzzsoft/co-mod-gan.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:59:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zhao", "Shengyu", ""], ["Cui", "Jonathan", ""], ["Sheng", "Yilun", ""], ["Dong", "Yue", ""], ["Liang", "Xiao", ""], ["Chang", "Eric I", ""], ["Xu", "Yan", ""]]}, {"id": "2103.10602", "submitter": "Xiaoyu Pan", "authors": "Xiaoyu Pan, Jiancong Huang, Jiaming Mai, He Wang, Honglin Li, Tongkui\n  Su, Wenjun Wang and Xiaogang Jin", "title": "HeterSkinNet: A Heterogeneous Network for Skin Weights Prediction", "comments": "I3D 2021", "journal-ref": null, "doi": "10.1145/3451262", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Character rigging is universally needed in computer graphics but notoriously\nlaborious. We present a new method, HeterSkinNet, aiming to fully automate such\nprocesses and significantly boost productivity. Given a character mesh and\nskeleton as input, our method builds a heterogeneous graph that treats the mesh\nvertices and the skeletal bones as nodes of different types and uses graph\nconvolutions to learn their relationships. To tackle the graph heterogeneity,\nwe propose a new graph network convolution operator that transfers information\nbetween heterogeneous nodes. The convolution is based on a new distance\nHollowDist that quantifies the relations between mesh vertices and bones. We\nshow that HeterSkinNet is robust for production characters by providing the\nability to incorporate meshes and skeletons with arbitrary topologies and\nmorphologies (e.g., out-of-body bones, disconnected mesh components, etc.).\nThrough exhaustive comparisons, we show that HeterSkinNet outperforms\nstate-of-the-art methods by large margins in terms of rigging accuracy and\nnaturalness. HeterSkinNet provides a solution for effective and robust\ncharacter rigging.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:53:00 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Pan", "Xiaoyu", ""], ["Huang", "Jiancong", ""], ["Mai", "Jiaming", ""], ["Wang", "He", ""], ["Li", "Honglin", ""], ["Su", "Tongkui", ""], ["Wang", "Wenjun", ""], ["Jin", "Xiaogang", ""]]}, {"id": "2103.10951", "submitter": "David Bau iii", "authors": "David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian,\n  Aude Oliva, Antonio Torralba", "title": "Paint by Word", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of zero-shot semantic image painting. Instead of\npainting modifications into an image using only concrete colors or a finite set\nof semantic concepts, we ask how to create semantic paint based on open\nfull-text descriptions: our goal is to be able to point to a location in a\nsynthesized image and apply an arbitrary new concept such as \"rustic\" or\n\"opulent\" or \"happy dog.\" To do this, our method combines a state-of-the art\ngenerative model of realistic images with a state-of-the-art text-image\nsemantic similarity network. We find that, to make large changes, it is\nimportant to use non-gradient methods to explore latent space, and it is\nimportant to relax the computations of the GAN to target changes to a specific\nregion. We conduct user studies to compare our methods to several baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:59:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 05:46:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bau", "David", ""], ["Andonian", "Alex", ""], ["Cui", "Audrey", ""], ["Park", "YeonHwan", ""], ["Jahanian", "Ali", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "2103.11310", "submitter": "Bolun Wang", "authors": "Bolun Wang, Xin Jiang, Guanying Huo, Danlei Ye, Cheng Su, Zehong Lu,\n  Dongming Yan, Zhiming Zheng", "title": "KPI Method for Dynamic Surface Reconstruction With B-splines based on\n  Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional B-splines are catching tremendous attentions in fields of\nIso-geometry Analysis, dynamic surface reconstruction and so on. However, the\nactual measured data are usually sparse and nonuniform, which might not meet\nthe requirement of traditional B-spline algorithms. In this paper, we present a\nnovel dynamic surface reconstruction approach, which is a 3-dimensional key\npoints interpolation method (KPI) based on B-spline, aimed at dealing with\nsparse distributed data. This method includes two stages: a data set generation\nalgorithm based on Kriging and a control point solving method based on key\npoints interpolation. The data set generation method is designed to construct a\ngrided dataset which can meet the requirement of B-spline interpolation, while\npromisingly catching the trend of sparse data, and it also includes a parameter\nreduction method which can significantly reduce the number of control points of\nthe result surface. The control points solving method ensures the 3-dimensional\nB-spline function to interpolate the sparse data points precisely while\napproximating the data points generated by Kriging. We apply the method into a\ntemperature data interpolation problem. It is shown that the generated dynamic\nsurface accurately interpolates the sparsely distributed temperature data,\npreserves the dynamic characteristics, with fewer control points than those of\ntraditional B-spline surface interpolation algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:31:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Bolun", ""], ["Jiang", "Xin", ""], ["Huo", "Guanying", ""], ["Ye", "Danlei", ""], ["Su", "Cheng", ""], ["Lu", "Zehong", ""], ["Yan", "Dongming", ""], ["Zheng", "Zhiming", ""]]}, {"id": "2103.11571", "submitter": "Petr Kellnhofer", "authors": "Petr Kellnhofer, Lars Jebe, Andrew Jones, Ryan Spicer, Kari Pulli and\n  Gordon Wetzstein", "title": "Neural Lumigraph Rendering", "comments": "Project website:\n  http://www.computationalimaging.org/publications/nlr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis is a challenging and ill-posed inverse rendering\nproblem. Neural rendering techniques have recently achieved photorealistic\nimage quality for this task. State-of-the-art (SOTA) neural volume rendering\napproaches, however, are slow to train and require minutes of inference (i.e.,\nrendering) time for high image resolutions. We adopt high-capacity neural scene\nrepresentations with periodic activations for jointly optimizing an implicit\nsurface and a radiance field of a scene supervised exclusively with posed 2D\nimages. Our neural rendering pipeline accelerates SOTA neural volume rendering\nby about two orders of magnitude and our implicit surface representation is\nunique in allowing us to export a mesh with view-dependent texture information.\nThus, like other implicit surface representations, ours is compatible with\ntraditional graphics pipelines, enabling real-time rendering rates, while\nachieving unprecedented image quality compared to other surface methods. We\nassess the quality of our approach using existing datasets as well as\nhigh-quality 3D face data captured with a custom multi-camera rig.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:46:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kellnhofer", "Petr", ""], ["Jebe", "Lars", ""], ["Jones", "Andrew", ""], ["Spicer", "Ryan", ""], ["Pulli", "Kari", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2103.11930", "submitter": "Andrew Willis", "authors": "Andrew Willis, Prashant Ganesh, Kyle Volle, Jincheng Zhang, Kevin\n  Brink", "title": "Volumetric Procedural Models for Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a volumetric approach for procedural shape modeling\nand a new Procedural Shape Modeling Language (PSML) that facilitates the\nspecification of these models. PSML provides programmers the ability to\ndescribe shapes in terms of their 3D elements where each element may be a\nsemantic group of 3D objects, e.g., a brick wall, or an indivisible object,\ne.g., an individual brick. Modeling shapes in this manner facilitates the\ncreation of models that more closely approximate the organization and structure\nof their real-world counterparts. As such, users may query these models for\nvolumetric information such as the number, position, orientation and volume of\n3D elements which cannot be provided using surface based model-building\ntechniques. PSML also provides a number of new language-specific capabilities\nthat allow for a rich variety of context-sensitive behaviors and\npost-processing functions. These capabilities include an object-oriented\napproach for model design, methods for querying the model for component-based\ninformation and the ability to access model elements and components to perform\nBoolean operations on the model parts. PSML is open-source and includes freely\navailable tutorial videos, demonstration code and an integrated development\nenvironment to support writing PSML programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:17:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Willis", "Andrew", ""], ["Ganesh", "Prashant", ""], ["Volle", "Kyle", ""], ["Zhang", "Jincheng", ""], ["Brink", "Kevin", ""]]}, {"id": "2103.12266", "submitter": "Peng-Shuai Wang", "authors": "Shi-Lin Liu, Hao-Xiang Guo, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang\n  Liu", "title": "Deep Implicit Moving Least-Squares Functions for 3D Reconstruction", "comments": "Accepted by CVPR 2021, Code: https://github.com/Andy97/DeepMLS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point set is a flexible and lightweight representation widely used for 3D\ndeep learning. However, their discrete nature prevents them from representing\ncontinuous and fine geometry, posing a major issue for learning-based shape\ngeneration. In this work, we turn the discrete point sets into smooth surfaces\nby introducing the well-known implicit moving least-squares (IMLS) surface\nformulation, which naturally defines locally implicit functions on point sets.\nWe incorporate IMLS surface generation into deep neural networks for inheriting\nboth the flexibility of point sets and the high quality of implicit surfaces.\nOur IMLSNet predicts an octree structure as a scaffold for generating MLS\npoints where needed and characterizes shape geometry with learned local priors.\nFurthermore, our implicit function evaluation is independent of the neural\nnetwork once the MLS points are predicted, thus enabling fast runtime\nevaluation. Our experiments on 3D object reconstruction demonstrate that\nIMLSNets outperform state-of-the-art learning-based methods in terms of\nreconstruction quality and computational efficiency. Extensive ablation tests\nalso validate our network design and loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:26:07 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:14:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Liu", "Shi-Lin", ""], ["Guo", "Hao-Xiang", ""], ["Pan", "Hao", ""], ["Wang", "Peng-Shuai", ""], ["Tong", "Xin", ""], ["Liu", "Yang", ""]]}, {"id": "2103.13415", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman,\n  Ricardo Martin-Brualla, Pratul P. Srinivasan", "title": "Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rendering procedure used by neural radiance fields (NeRF) samples a scene\nwith a single ray per pixel and may therefore produce renderings that are\nexcessively blurred or aliased when training or testing images observe scene\ncontent at different resolutions. The straightforward solution of supersampling\nby rendering with multiple rays per pixel is impractical for NeRF, because\nrendering each ray requires querying a multilayer perceptron hundreds of times.\nOur solution, which we call \"mip-NeRF\" (a la \"mipmap\"), extends NeRF to\nrepresent the scene at a continuously-valued scale. By efficiently rendering\nanti-aliased conical frustums instead of rays, mip-NeRF reduces objectionable\naliasing artifacts and significantly improves NeRF's ability to represent fine\ndetails, while also being 7% faster than NeRF and half the size. Compared to\nNeRF, mip-NeRF reduces average error rates by 17% on the dataset presented with\nNeRF and by 60% on a challenging multiscale variant of that dataset that we\npresent. Mip-NeRF is also able to match the accuracy of a brute-force\nsupersampled NeRF on our multiscale dataset while being 22x faster.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:02:11 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 22:17:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Barron", "Jonathan T.", ""], ["Mildenhall", "Ben", ""], ["Tancik", "Matthew", ""], ["Hedman", "Peter", ""], ["Martin-Brualla", "Ricardo", ""], ["Srinivasan", "Pratul P.", ""]]}, {"id": "2103.13922", "submitter": "Daniel Martin", "authors": "Daniel Martin, Ana Serrano, Alexander W. Bergman, Gordon Wetzstein,\n  Belen Masia", "title": "ScanGAN360: A Generative Model of Realistic Scanpaths for 360$^{\\circ}$\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding and modeling the dynamics of human gaze behavior in 360$^\\circ$\nenvironments is a key challenge in computer vision and virtual reality.\nGenerative adversarial approaches could alleviate this challenge by generating\na large number of possible scanpaths for unseen images. Existing methods for\nscanpath generation, however, do not adequately predict realistic scanpaths for\n360$^\\circ$ images. We present ScanGAN360, a new generative adversarial\napproach to address this challenging problem. Our network generator is tailored\nto the specifics of 360$^\\circ$ images representing immersive environments.\nSpecifically, we accomplish this by leveraging the use of a spherical\nadaptation of dynamic-time warping as a loss function and proposing a novel\nparameterization of 360$^\\circ$ scanpaths. The quality of our scanpaths\noutperforms competing approaches by a large margin and is almost on par with\nthe human baseline. ScanGAN360 thus allows fast simulation of large numbers of\nvirtual observers, whose behavior mimics real users, enabling a better\nunderstanding of gaze behavior and novel applications in virtual scene design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:34:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Martin", "Daniel", ""], ["Serrano", "Ana", ""], ["Bergman", "Alexander W.", ""], ["Wetzstein", "Gordon", ""], ["Masia", "Belen", ""]]}, {"id": "2103.14024", "submitter": "Alex Yu", "authors": "Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, Angjoo Kanazawa", "title": "PlenOctrees for Real-time Rendering of Neural Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a method to render Neural Radiance Fields (NeRFs) in real time\nusing PlenOctrees, an octree-based 3D representation which supports\nview-dependent effects. Our method can render 800x800 images at more than 150\nFPS, which is over 3000 times faster than conventional NeRFs. We do so without\nsacrificing quality while preserving the ability of NeRFs to perform\nfree-viewpoint rendering of scenes with arbitrary geometry and view-dependent\neffects. Real-time performance is achieved by pre-tabulating the NeRF into a\nPlenOctree. In order to preserve view-dependent effects such as specularities,\nwe factorize the appearance via closed-form spherical basis functions.\nSpecifically, we show that it is possible to train NeRFs to predict a spherical\nharmonic representation of radiance, removing the viewing direction as an input\nto the neural network. Furthermore, we show that PlenOctrees can be directly\noptimized to further minimize the reconstruction loss, which leads to equal or\nbetter quality compared to competing methods. Moreover, this octree\noptimization step can be used to reduce the training time, as we no longer need\nto wait for the NeRF training to converge fully. Our real-time neural rendering\napproach may potentially enable new applications such as 6-DOF industrial and\nproduct visualizations, as well as next generation AR/VR systems. PlenOctrees\nare amenable to in-browser rendering as well; please visit the project page for\nthe interactive online demo, as well as video and code:\nhttps://alexyu.net/plenoctrees\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:06 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yu", "Alex", ""], ["Li", "Ruilong", ""], ["Tancik", "Matthew", ""], ["Li", "Hao", ""], ["Ng", "Ren", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2103.14031", "submitter": "Dongdong Chen", "authors": "Ziyu Wan and Jingbo Zhang and Dongdong Chen and Jing Liao", "title": "High-Fidelity Pluralistic Image Completion with Transformers", "comments": "Project Page: http://raywzy.com/ICT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image completion has made tremendous progress with convolutional neural\nnetworks (CNNs), because of their powerful texture modeling capacity. However,\ndue to some inherent properties (e.g., local inductive prior, spatial-invariant\nkernels), CNNs do not perform well in understanding global structures or\nnaturally support pluralistic completion. Recently, transformers demonstrate\ntheir power in modeling the long-term relationship and generating diverse\nresults, but their computation complexity is quadratic to input length, thus\nhampering the application in processing high-resolution images. This paper\nbrings the best of both worlds to pluralistic image completion: appearance\nprior reconstruction with transformer and texture replenishment with CNN. The\nformer transformer recovers pluralistic coherent structures together with some\ncoarse textures, while the latter CNN enhances the local texture details of\ncoarse priors guided by the high-resolution masked images. The proposed method\nvastly outperforms state-of-the-art methods in terms of three aspects: 1) large\nperformance boost on image fidelity even compared to deterministic completion\nmethods; 2) better diversity and higher fidelity for pluralistic completion; 3)\nexceptional generalization ability on large masks and generic dataset, like\nImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Jingbo", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""]]}, {"id": "2103.14274", "submitter": "Hung Yu Ling", "authors": "Hung Yu Ling and Fabio Zinno and George Cheng and Michiel van de Panne", "title": "Character Controllers Using Motion VAEs", "comments": "Project page: https://www.cs.ubc.ca/~hyuling/projects/mvae/ ; Code:\n  https://github.com/electronicarts/character-motion-vaes", "journal-ref": null, "doi": "10.1145/3386569.3392422", "report-no": null, "categories": "cs.LG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A fundamental problem in computer animation is that of realizing purposeful\nand realistic human movement given a sufficiently-rich set of motion capture\nclips. We learn data-driven generative models of human movement using\nautoregressive conditional variational autoencoders, or Motion VAEs. The latent\nvariables of the learned autoencoder define the action space for the movement\nand thereby govern its evolution over time. Planning or control algorithms can\nthen use this action space to generate desired motions. In particular, we use\ndeep reinforcement learning to learn controllers that achieve goal-directed\nmovements. We demonstrate the effectiveness of the approach on multiple tasks.\nWe further evaluate system-design choices and describe the current limitations\nof Motion VAEs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 05:51:41 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Ling", "Hung Yu", ""], ["Zinno", "Fabio", ""], ["Cheng", "George", ""], ["van de Panne", "Michiel", ""]]}, {"id": "2103.14338", "submitter": "Zhichao Huang", "authors": "Zhichao Huang, Xintong Han, Jia Xu, Tong Zhang", "title": "Few-Shot Human Motion Transfer by Personalized Geometry and Texture\n  Modeling", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for few-shot human motion transfer that achieves\nrealistic human image generation with only a small number of appearance inputs.\nDespite recent advances in single person motion transfer, prior methods often\nrequire a large number of training images and take long training time. One\npromising direction is to perform few-shot human motion transfer, which only\nneeds a few of source images for appearance transfer. However, it is\nparticularly challenging to obtain satisfactory transfer results. In this\npaper, we address this issue by rendering a human texture map to a surface\ngeometry (represented as a UV map), which is personalized to the source person.\nOur geometry generator combines the shape information from source images, and\nthe pose information from 2D keypoints to synthesize the personalized UV map. A\ntexture generator then generates the texture map conditioned on the texture of\nsource images to fill out invisible parts. Furthermore, we may fine-tune the\ntexture map on the manifold of the texture generator from a few source images\nat the test time, which improves the quality of the texture map without\nover-fitting or artifacts. Extensive experiments show the proposed method\noutperforms state-of-the-art methods both qualitatively and quantitatively. Our\ncode is available at https://github.com/HuangZhiChao95/FewShotMotionTransfer.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:01:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Zhichao", ""], ["Han", "Xintong", ""], ["Xu", "Jia", ""], ["Zhang", "Tong", ""]]}, {"id": "2103.14507", "submitter": "Jordi Sanchez-Riera", "authors": "Jordi Sanchez-Riera, Aniol Civit, Marta Altarriba and Francesc\n  Moreno-Noguer", "title": "AVATAR: Blender add-on for fast creation of 3D human models", "comments": "7 pages, 2 figures, software description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Create an articulated and realistic human 3D model is a complicated task, not\nonly get a model with the right body proportions but also to the whole process\nof rigging the model with correct articulation points and vertices weights.\nHaving a tool that can create such a model with just a few clicks will be very\nadvantageous for amateurs developers to use in their projects, researchers to\neasily generate datasets to train neural networks and industry for game\ndevelopment. We present a software that is integrated in Blender in form of\nadd-on that allows us to design and animate a dressed 3D human models based on\nMakehuman with just a few clicks. Moreover, as it is already integrated in\nBlender, python scripts can be created to animate, render and further customize\nthe current available options.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:05:06 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sanchez-Riera", "Jordi", ""], ["Civit", "Aniol", ""], ["Altarriba", "Marta", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "2103.14627", "submitter": "Marco Cavallo", "authors": "Marco Cavallo", "title": "Higher Dimensional Graphics: Conceiving Worlds in Four Spatial\n  Dimensions and Beyond", "comments": "Eurographics 2021 / Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the interpretation of high-dimensional datasets has become a necessity\nin most industries, and is supported by continuous advances in data science and\nmachine learning, the spatial visualization of higher-dimensional geometry has\nmostly remained a niche research topic for mathematicians and physicists.\nIntermittent contributions to this field date back more than a century, and\nhave had a non-negligible influence on contemporary art and philosophy.\nHowever, most contributions have focused on the understanding of specific\nmathematical shapes, with few concrete applications. In this work, we attempt\nto revive the community's interest in visualizing higher dimensional geometry\nby shifting the focus from the visualization of abstract shapes to the design\nof a broader hyper-universe concept, wherein 3D and 4D objects can coexist and\ninteract with each other. Specifically, we discuss the content definition,\nauthoring patterns, and technical implementations associated with the process\nof extending standard 3D applications as to support 4D mechanics. We\noperationalize our ideas through the introduction of a new hybrid 3D/4D\nvideogame called Across Dimensions, which we developed in Unity3D through the\nintegration of our own 4D plugin.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:41:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cavallo", "Marco", ""]]}, {"id": "2103.14645", "submitter": "Ben Mildenhall", "authors": "Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T.\n  Barron, Paul Debevec", "title": "Baking Neural Radiance Fields for Real-Time View Synthesis", "comments": "Project page: https://nerf.live", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural volumetric representations such as Neural Radiance Fields (NeRF) have\nemerged as a compelling technique for learning to represent 3D scenes from\nimages with the goal of rendering photorealistic images of the scene from\nunobserved viewpoints. However, NeRF's computational requirements are\nprohibitive for real-time applications: rendering views from a trained NeRF\nrequires querying a multilayer perceptron (MLP) hundreds of times per ray. We\npresent a method to train a NeRF, then precompute and store (i.e. \"bake\") it as\na novel representation called a Sparse Neural Radiance Grid (SNeRG) that\nenables real-time rendering on commodity hardware. To achieve this, we\nintroduce 1) a reformulation of NeRF's architecture, and 2) a sparse voxel grid\nrepresentation with learned feature vectors. The resulting scene representation\nretains NeRF's ability to render fine geometric details and view-dependent\nappearance, is compact (averaging less than 90 MB per scene), and can be\nrendered in real-time (higher than 30 frames per second on a laptop GPU).\nActual screen captures are shown in our video.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:59:52 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hedman", "Peter", ""], ["Srinivasan", "Pratul P.", ""], ["Mildenhall", "Ben", ""], ["Barron", "Jonathan T.", ""], ["Debevec", "Paul", ""]]}, {"id": "2103.14696", "submitter": "Vedavyas Mallela", "authors": "Vedvayas Mallela, Polina Golland and Razvan V. Marinescu", "title": "BrainPainter v2: Mouse Brain Visualization Software", "comments": "9 pages, 4 figures, 2 movies (see ancillary files)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  BrainPainter is a software for the 3D visualization of human brain\nstructures; it generates colored brain images using user-defined biomarker data\nfor each brain region. However, BrainPainter is only able to generate human\nbrain images. In this paper, we present updates to the existing BrainPainter\nsoftware which enables the generation of mouse brain images. We load meshes for\neach mouse brain region, based on the Allen Mouse Brain Atlas, into Blender, a\npowerful 3D computer graphics engine. We then use Blender to color each region\nand generate images of subcortical, outer-cortical, inner-cortical, top and\nbottom view renders. In addition to those views, we add new render angles and\nseparate visualization settings for the left and right hemispheres. While\nBrainPainter traditionally ran from the browser (\nhttps://brainpainter.csail.mit.edu ), we also created a graphical user\ninterface that launches image-generation requests in a user-friendly way, by\nconnecting to the Blender backend via a Docker API. We illustrate a use case of\nBrainPainter for modeling the progression of tau protein accumulation in a\nmouse study. Our contributions can help neuroscientists visualize brains in\nmouse studies and show disease progression. In addition, integration into\nBlender can subsequently enable the generation of complex animations using a\nmoving camera, generation of complex mesh deformations that simulate tumors and\nother pathologies, as well as visualization of toxic proteins using Blender's\nparticle system.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:06:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mallela", "Vedvayas", ""], ["Golland", "Polina", ""], ["Marinescu", "Razvan V.", ""]]}, {"id": "2103.14794", "submitter": "Kaizhang Kang", "authors": "Kaizhang Kang, Cihui Xie, Ruisheng Zhu, Xiaohe Ma, Ping Tan, Hongzhi\n  Wu and Kun Zhou", "title": "Learning Efficient Photometric Feature Transform for Multi-view Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework to learn to convert the perpixel photometric\ninformation at each view into spatially distinctive and view-invariant\nlow-level features, which can be plugged into existing multi-view stereo\npipeline for enhanced 3D reconstruction. Both the illumination conditions\nduring acquisition and the subsequent per-pixel feature transform can be\njointly optimized in a differentiable fashion. Our framework automatically\nadapts to and makes efficient use of the geometric information available in\ndifferent forms of input data. High-quality 3D reconstructions of a variety of\nchallenging objects are demonstrated on the data captured with an illumination\nmultiplexing device, as well as a point light. Our results compare favorably\nwith state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 02:53:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kang", "Kaizhang", ""], ["Xie", "Cihui", ""], ["Zhu", "Ruisheng", ""], ["Ma", "Xiaohe", ""], ["Tan", "Ping", ""], ["Wu", "Hongzhi", ""], ["Zhou", "Kun", ""]]}, {"id": "2103.14870", "submitter": "Parag Chaudhuri", "authors": "Avirup Mandal, Parag Chaudhuri, Subhasis Chaudhuri", "title": "Remeshing-Free Graph-Based Finite Element Method for Ductile and Brittle\n  Fracture", "comments": "13 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a remeshing-free, graph-based finite element method (FEM)\nfor simulating ductile and brittle fracture. Since fracture produces new mesh\nfragments and introduces additional degrees of freedom in the system dynamics,\nexisting FEM based methods suffer from an explosion in computational cost as\nthe system matrix size increases. Our model develops an algorithm for modelling\nfracture on the graph induced in a volumetric mesh with tetrahedral elements,\nby its vertices and edges. In order to initialize and propagate fracture, We\nsimply relabel the edges of the graph using a damage variable, and control the\ndiffusion of fracture inside the object by varying the support of a damage\nkernel. We present the reformulated system dynamics for this relabeled graph\nthat allows us to simulate fracture in a FEM setting, without changing the size\nof system dynamics matrix of the mesh. This makes our computational method\nremeshing-free and scalable to high-resolution meshes. The fracture surface has\nto be reconstructed explicitly for visualization purposes only. We evaluate our\nalgorithm extensively on a variety of brittle and ductile materials and compare\nits features with other state of the art methods. We simulate standard\nlaboratory experiments from structural mechanics and compare the results\nvisually and quantitatively with real-world experiments performed on physical\nmaterial samples. We also present results evaluating the performance of our\nmethod, and show that our techniques offer stability and speed that is\nunmatched in current literature.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 10:04:36 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mandal", "Avirup", ""], ["Chaudhuri", "Parag", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2103.14877", "submitter": "Yuki Endo", "authors": "Yuki Endo and Yoshihiro Kanamori", "title": "Few-shot Semantic Image Synthesis Using StyleGAN Prior", "comments": "The source codes are available at\n  https://github.com/endo-yuki-t/Fewshot-SMIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles a challenging problem of generating photorealistic images\nfrom semantic layouts in few-shot scenarios where annotated training pairs are\nhardly available but pixel-wise annotation is quite costly. We present a\ntraining strategy that performs pseudo labeling of semantic masks using the\nStyleGAN prior. Our key idea is to construct a simple mapping between the\nStyleGAN feature and each semantic class from a few examples of semantic masks.\nWith such mappings, we can generate an unlimited number of pseudo semantic\nmasks from random noise to train an encoder for controlling a pre-trained\nStyleGAN generator. Although the pseudo semantic masks might be too coarse for\nprevious approaches that require pixel-aligned masks, our framework can\nsynthesize high-quality images from not only dense semantic masks but also\nsparse inputs such as landmarks and scribbles. Qualitative and quantitative\nresults with various datasets demonstrate improvement over previous approaches\nwith respect to layout fidelity and visual quality in as few as one- or\nfive-shot settings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 11:04:22 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 09:37:56 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""]]}, {"id": "2103.14910", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, Gim Hee\n  Lee", "title": "NeMI: Unifying Neural Radiance Fields with Multiplane Images for Novel\n  View Synthesis", "comments": "Main paper and supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose an approach to perform novel view synthesis and\ndepth estimation via dense 3D reconstruction from a single image. Our NeMI\nunifies Neural radiance fields (NeRF) with Multiplane Images (MPI).\nSpecifically, our NeMI is a general two-dimensional and image-conditioned\nextension of NeRF, and a continuous depth generalization of MPI. Given a single\nimage as input, our method predicts a 4-channel image (RGB and volume density)\nat arbitrary depth values to jointly reconstruct the camera frustum and fill in\noccluded contents. The reconstructed and inpainted frustum can then be easily\nrendered into novel RGB or depth views using differentiable rendering.\nExtensive experiments on RealEstate10K, KITTI and Flowers Light Fields show\nthat our NeMI outperforms state-of-the-art by a large margin in novel view\nsynthesis. We also achieve competitive results in depth estimation on iBims-1\nand NYU-v2 without annotated depth supervision. Project page available at\nhttps://vincentfung13.github.io/projects/nemi/\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:41:00 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 02:28:33 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Jiaxin", ""], ["Feng", "Zijian", ""], ["She", "Qi", ""], ["Ding", "Henghui", ""], ["Wang", "Changhu", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2103.14984", "submitter": "Zihao Jian", "authors": "Zihao Jian, Minshan Xie", "title": "Realistic face animation generation from videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  3D face reconstruction and face alignment are two fundamental and highly\nrelated topics in computer vision. Recently, some works start to use deep\nlearning models to estimate the 3DMM coefficients to reconstruct 3D face\ngeometry. However, the performance is restricted due to the limitation of the\npre-defined face templates. To address this problem, some end-to-end methods,\nwhich can completely bypass the calculation of 3DMM coefficients, are proposed\nand attract much attention. In this report, we introduce and analyse three\nstate-of-the-art methods in 3D face reconstruction and face alignment. Some\npotential improvement on PRN are proposed to further enhance its accuracy and\nspeed.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:18:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jian", "Zihao", ""], ["Xie", "Minshan", ""]]}, {"id": "2103.15163", "submitter": "Theodore Kim", "authors": "Theodore Kim and Holly Rushmeier and Julie Dorsey and Derek\n  Nowrouzezahrai and Raqi Syed and Wojciech Jarosz and A.M. Darke", "title": "Countering Racial Bias in Computer Graphics Research", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current computer graphics research practices contain racial biases that have\nresulted in investigations into \"skin\" and \"hair\" that focus on the hegemonic\nvisual features of Europeans and East Asians. To broaden our research horizons\nto encompass all of humanity, we propose a variety of improvements to\nquantitative measures and qualitative practices, and pose novel, open research\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 16:15:39 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 11:48:39 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kim", "Theodore", ""], ["Rushmeier", "Holly", ""], ["Dorsey", "Julie", ""], ["Nowrouzezahrai", "Derek", ""], ["Syed", "Raqi", ""], ["Jarosz", "Wojciech", ""], ["Darke", "A. M.", ""]]}, {"id": "2103.15369", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Flaviano Christian Reyes, Ritika Shrivastava,\n  Oladapo Afolabi, Luisa Caldas, Allen Y. Yang", "title": "Contextual Scene Augmentation and Synthesis via GSACNet", "comments": "arXiv admin note: text overlap with arXiv:2009.12395 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene augmentation has become an emerging topic in the field of\ncomputer vision and graphics with applications in augmented and virtual\nreality. However, current state-of-the-art systems using deep neural networks\nrequire large datasets for training. In this paper we introduce GSACNet, a\ncontextual scene augmentation system that can be trained with limited scene\npriors. GSACNet utilizes a novel parametric data augmentation method combined\nwith a Graph Attention and Siamese network architecture followed by an\nAutoencoder network to facilitate training with small datasets. We show the\neffectiveness of our proposed system by conducting ablation and comparative\nstudies with alternative systems on the Matterport3D dataset. Our results\nindicate that our scene augmentation outperforms prior art in scene synthesis\nwith limited scene priors available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:47:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Reyes", "Flaviano Christian", ""], ["Shrivastava", "Ritika", ""], ["Afolabi", "Oladapo", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""]]}, {"id": "2103.15472", "submitter": "Tsukasa Fukusato", "authors": "Tsukasa Fukusato, Akinobu Maejima", "title": "View-Dependent Formulation of 2.5D Cartoon Models", "comments": "11 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2.5D cartoon models are methods to simulate three-dimensional (3D)-like\nmovements, such as out-of-plane rotation, from two-dimensional (2D) shapes in\ndifferent views. However, cartoon objects and characters have several distorted\nparts which do not correspond to any real 3D positions (e.g., Mickey Mouse's\nears), that implies that existing systems are not suitable for designing such\nrepresentations. Hence, we formulate it as a view-dependent deformation (VDD)\nproblem, which has been proposed in the field of 3D character animation. The\ndistortions in an arbitrary viewpoint are automatically obtained by blending\nthe user-specified 2D shapes of key views. This model is simple enough to\neasily implement in an existing animation system. Several examples demonstrate\nthe robustness of our method over previous methods. In addition, we conduct a\nuser study and confirm that the proposed system is effective for animating\nclassic cartoon characters.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:07:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fukusato", "Tsukasa", ""], ["Maejima", "Akinobu", ""]]}, {"id": "2103.15627", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Jonas Kohler, Thomas Hofmann, Aurelien Lucchi", "title": "Learning Generative Models of Textured 3D Meshes from Real-World Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in differentiable rendering have sparked an interest in\nlearning generative models of textured 3D meshes from image collections. These\nmodels natively disentangle pose and appearance, enable downstream applications\nin computer graphics, and improve the ability of generative models to\nunderstand the concept of image formation. Although there has been prior work\non learning such models from collections of 2D images, these approaches require\na delicate pose estimation step that exploits annotated keypoints, thereby\nrestricting their applicability to a few specific datasets. In this work, we\npropose a GAN framework for generating textured triangle meshes without relying\non such annotations. We show that the performance of our approach is on par\nwith prior work that relies on ground-truth keypoints, and more importantly, we\ndemonstrate the generality of our method by setting new baselines on a larger\nset of categories from ImageNet - for which keypoints are not available -\nwithout any class-specific hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:07:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pavllo", "Dario", ""], ["Kohler", "Jonas", ""], ["Hofmann", "Thomas", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "2103.16020", "submitter": "Eisa Hedayati", "authors": "Eisa Hedayati, Timothy C. Havens, Jeremy P. Bos", "title": "Machine learning method for light field refocusing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging introduced the capability to refocus an image after\ncapturing. Currently there are two popular methods for refocusing,\nshift-and-sum and Fourier slice methods. Neither of these two methods can\nrefocus the light field in real-time without any pre-processing. In this paper\nwe introduce a machine learning based refocusing technique that is capable of\nextracting 16 refocused images with refocusing parameters of\n\\alpha=0.125,0.250,0.375,...,2.0 in real-time. We have trained our network,\nwhich is called RefNet, in two experiments. Once using the Fourier slice method\nas the training -- i.e., \"ground truth\" -- data and another using the\nshift-and-sum method as the training data. We showed that in both cases, not\nonly is the RefNet method at least 134x faster than previous approaches, but\nalso the color prediction of RefNet is superior to both Fourier slice and\nshift-and-sum methods while having similar depth of field and focus distance\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:46:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hedayati", "Eisa", ""], ["Havens", "Timothy C.", ""], ["Bos", "Jeremy P.", ""]]}, {"id": "2103.16060", "submitter": "Scott Davidoff", "authors": "Connie Ye, Lukas Hermann, Nur Yildirim, Shravya Bhat, Dominik Moritz,\n  Scott Davidoff", "title": "PIXLISE-C: Exploring The Data Analysis Needs of NASA Scientists for\n  Mineral Identification", "comments": "5 pages, 6 figures, ACM Conference on Human Factors in Computing\n  Systems Workshop on Human-Computer Interaction for Space Exploration\n  (SpaceCHI 2021), https://spacechi.media.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR physics.comp-ph physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NASA JPL scientists working on the micro x-ray fluorescence (microXRF)\nspectroscopy data collected from Mars surface perform data analysis to look for\nsigns of past microbial life on Mars. Their data analysis workflow mainly\ninvolves identifying mineral compounds through the element abundance in\nspatially distributed data points. Working with the NASA JPL team, we\nidentified pain points and needs to further develop their existing data\nvisualization and analysis tool. Specifically, the team desired improvements\nfor the process of creating and interpreting mineral composition groups. To\naddress this problem, we developed an interactive tool that enables scientists\nto (1) cluster the data using either manual lasso-tool selection or through\nvarious machine learning clustering algorithms, and (2) compare the clusters\nand individual data points to make informed decisions about mineral\ncompositions. Our preliminary tool supports a hybrid data analysis workflow\nwhere the user can manually refine the machine-generated clusters.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:11:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ye", "Connie", ""], ["Hermann", "Lukas", ""], ["Yildirim", "Nur", ""], ["Bhat", "Shravya", ""], ["Moritz", "Dominik", ""], ["Davidoff", "Scott", ""]]}, {"id": "2103.16066", "submitter": "Jun Zhou", "authors": "Jun Zhou, Wei Jin, Mingjie Wang, Xiuping Liu, Zhiyang Li and Zhaobin\n  Liu", "title": "Fast and Accurate Normal Estimation for Point Cloud via Patch Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an effective normal estimation method adopting\nmulti-patch stitching for an unstructured point cloud. The majority of\nlearning-based approaches encode a local patch around each point of a whole\nmodel and estimate the normals in a point-by-point manner. In contrast, we\nsuggest a more efficient pipeline, in which we introduce a patch-level normal\nestimation architecture to process a series of overlapping patches.\nAdditionally, a multi-normal selection method based on weights, dubbed as\nmulti-patch stitching, integrates the normals from the overlapping patches. To\nreduce the adverse effects of sharp corners or noise in a patch, we introduce\nan adaptive local feature aggregation layer to focus on an anisotropic\nneighborhood. We then utilize a multi-branch planar experts module to break the\nmutual influence between underlying piecewise surfaces in a patch. At the\nstitching stage, we use the learned weights of multi-branch planar experts and\ndistance weights between points to select the best normal from the overlapping\nparts. Furthermore, we put forward constructing a sparse matrix representation\nto reduce large-scale retrieval overheads for the loop iterations dramatically.\nExtensive experiments demonstrate that our method achieves SOTA results with\nthe advantage of lower computational costs and higher robustness to noise over\nmost of the existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:30:35 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 05:18:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zhou", "Jun", ""], ["Jin", "Wei", ""], ["Wang", "Mingjie", ""], ["Liu", "Xiuping", ""], ["Li", "Zhiyang", ""], ["Liu", "Zhaobin", ""]]}, {"id": "2103.16115", "submitter": "Tiange Xiang", "authors": "Tiange Xiang, Hongliang Yuan, Haozhi Huang, Yujin Shi", "title": "Two-Stage Monte Carlo Denoising with Adaptive Sampling and Kernel Pool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monte Carlo path tracer renders noisy image sequences at low sampling counts.\nAlthough great progress has been made on denoising such sequences, existing\nmethods still suffer from spatial and temporary artifacts. In this paper, we\ntackle the problems in Monte Carlo rendering by proposing a two-stage denoiser\nbased on the adaptive sampling strategy. In the first stage, concurrent to\nadjusting samples per pixel (spp) on-the-fly, we reuse the computations to\ngenerate extra denoising kernels applying on the adaptively rendered image.\nRather than a direct prediction of pixel-wise kernels, we save the overhead\ncomplexity by interpolating such kernels from a public kernel pool, which can\nbe dynamically updated to fit input signals. In the second stage, we design the\nposition-aware pooling and semantic alignment operators to improve\nspatial-temporal stability. Our method was first benchmarked on 10 synthesized\nscenes rendered from the Mitsuba renderer and then validated on 3 additional\nscenes rendered from our self-built RTX-based renderer. Our method outperforms\nstate-of-the-art counterparts in terms of both numerical error and visual\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:05:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xiang", "Tiange", ""], ["Yuan", "Hongliang", ""], ["Huang", "Haozhi", ""], ["Shi", "Yujin", ""]]}, {"id": "2103.16352", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Iasonas Kokkinos", "title": "Learning monocular 3D reconstruction of articulated categories from\n  motion", "comments": "Accepted to CVPR2021. For project website see\n  https://fkokkinos.github.io/video_3d_reconstruction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular 3D reconstruction of articulated object categories is challenging\ndue to the lack of training data and the inherent ill-posedness of the problem.\nIn this work we use video self-supervision, forcing the consistency of\nconsecutive 3D reconstructions by a motion-based cycle loss. This largely\nimproves both optimization-based and learning-based 3D mesh reconstruction. We\nfurther introduce an interpretable model of 3D template deformations that\ncontrols a 3D surface through the displacement of a small number of local,\nlearnable handles. We formulate this operation as a structured layer relying on\nmesh-laplacian regularization and show that it can be trained in an end-to-end\nmanner. We finally introduce a per-sample numerical optimisation approach that\njointly optimises over mesh displacements and cameras within a video, boosting\naccuracy both for training and also as test time post-processing. While relying\nexclusively on a small set of videos collected per category for supervision, we\nobtain state-of-the-art reconstructions with diverse shapes, viewpoints and\ntextures for multiple articulated object categories.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:50:27 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:14:18 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "2103.16365", "submitter": "Qi Sun", "authors": "Nianchen Deng and Zhenyi He and Jiannan Ye and Praneeth Chakravarthula\n  and Xubo Yang and Qi Sun", "title": "Foveated Neural Radiance Fields for Real-Time and Egocentric Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional high-quality 3D graphics requires large volumes of fine-detailed\nscene data for rendering. This demand compromises computational efficiency and\nlocal storage resources. Specifically, it becomes more concerning for future\nwearable and portable virtual and augmented reality (VR/AR) displays. Recent\napproaches to combat this problem include remote rendering/streaming and neural\nrepresentations of 3D assets. These approaches have redefined the traditional\nlocal storage-rendering pipeline by distributed computing or compression of\nlarge data. However, these methods typically suffer from high latency or low\nquality for practical visualization of large immersive virtual scenes, notably\nwith extra high resolution and refresh rate requirements for VR applications\nsuch as gaming and design.\n  Tailored for the future portable, low-storage, and energy-efficient VR\nplatforms, we present the first gaze-contingent 3D neural representation and\nview synthesis method. We incorporate the human psychophysics of visual- and\nstereo-acuity into an egocentric neural representation of 3D scenery.\nFurthermore, we jointly optimize the latency/performance and visual quality,\nwhile mutually bridging human perception and neural scene synthesis, to achieve\nperceptually high-quality immersive interaction. Both objective analysis and\nsubjective study demonstrate the effectiveness of our approach in significantly\nreducing local storage volume and synthesis latency (up to 99% reduction in\nboth data size and computational time), while simultaneously presenting\nhigh-fidelity rendering, with perceptual quality identical to that of fully\nlocally stored and rendered high-quality imagery.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:05:47 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Deng", "Nianchen", ""], ["He", "Zhenyi", ""], ["Ye", "Jiannan", ""], ["Chakravarthula", "Praneeth", ""], ["Yang", "Xubo", ""], ["Sun", "Qi", ""]]}, {"id": "2103.16510", "submitter": "Cagatay Basdogan", "authors": "Senem Ezgi Emgin, Amirreza Aghakhani, T. Metin Sezgin, and Cagatay\n  Basdogan", "title": "HapTable: An Interactive Tabletop Providing Online Haptic Feedback for\n  Touch Gestures", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019,\n  Vol. 25, No. 9, pp. 2749-2762", "doi": "10.1109/TVCG.2018.2855154", "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HapTable; a multimodal interactive tabletop that allows users to\ninteract with digital images and objects through natural touch gestures, and\nreceive visual and haptic feedback accordingly. In our system, hand pose is\nregistered by an infrared camera and hand gestures are classified using a\nSupport Vector Machine (SVM) classifier. To display a rich set of haptic\neffects for both static and dynamic gestures, we integrated electromechanical\nand electrostatic actuation techniques effectively on tabletop surface of\nHapTable, which is a surface capacitive touch screen. We attached four piezo\npatches to the edges of tabletop to display vibrotactile feedback for static\ngestures. For this purpose, the vibration response of the tabletop, in the form\nof frequency response functions (FRFs), was obtained by a laser Doppler\nvibrometer for 84 grid points on its surface. Using these FRFs, it is possible\nto display localized vibrotactile feedback on the surface for static gestures.\nFor dynamic gestures, we utilize the electrostatic actuation technique to\nmodulate the frictional forces between finger skin and tabletop surface by\napplying voltage to its conductive layer. Here, we present two examples of such\napplications, one for static and one for dynamic gestures, along with detailed\nuser studies. In the first one, user detects the direction of a virtual flow,\nsuch as that of wind or water, by putting their hand on the tabletop surface\nand feeling a vibrotactile stimulus traveling underneath it. In the second\nexample, user rotates a virtual knob on the tabletop surface to select an item\nfrom a menu while feeling the knob's detents and resistance to rotation in the\nform of frictional haptic feedback.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:12:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Emgin", "Senem Ezgi", ""], ["Aghakhani", "Amirreza", ""], ["Sezgin", "T. Metin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2103.16563", "submitter": "Benjamin Planche", "authors": "Benjamin Planche, Rajat Vikram Singh", "title": "Physics-based Differentiable Depth Sensor Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based algorithms are crucial to modern computer-vision and graphics\napplications, enabling learning-based optimization and inverse problems. For\nexample, photorealistic differentiable rendering pipelines for color images\nhave been proven highly valuable to applications aiming to map 2D and 3D\ndomains. However, to the best of our knowledge, no effort has been made so far\ntowards extending these gradient-based methods to the generation of depth\n(2.5D) images, as simulating structured-light depth sensors implies solving\ncomplex light transport and stereo-matching problems. In this paper, we\nintroduce a novel end-to-end differentiable simulation pipeline for the\ngeneration of realistic 2.5D scans, built on physics-based 3D rendering and\ncustom block-matching algorithms. Each module can be differentiated w.r.t\nsensor and scene parameters; e.g., to automatically tune the simulation for new\ndevices over some provided scans or to leverage the pipeline as a 3D-to-2.5D\ntransformer within larger computer-vision applications. Applied to the training\nof deep-learning methods for various depth-based recognition tasks\n(classification, pose estimation, semantic segmentation), our simulation\ngreatly improves the performance of the resulting models on real scans, thereby\ndemonstrating the fidelity and value of its synthetic depth data compared to\nprevious static simulations and learning-based domain adaptation schemes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Planche", "Benjamin", ""], ["Singh", "Rajat Vikram", ""]]}, {"id": "2103.16748", "submitter": "Ning Yu", "authors": "Ning Yu, Guilin Liu, Aysegul Dundar, Andrew Tao, Bryan Catanzaro,\n  Larry Davis, Mario Fritz", "title": "Dual Contrastive Loss and Attention for GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) produce impressive results on\nunconditional image generation when powered with large-scale image datasets.\nYet generated images are still easy to spot especially on datasets with high\nvariance (e.g. bedroom, church). In this paper, we propose various improvements\nto further push the boundaries in image generation. Specifically, we propose a\nnovel dual contrastive loss and show that, with this loss, discriminator learns\nmore generalized and distinguishable representations to incentivize generation.\nIn addition, we revisit attention and extensively experiment with different\nattention blocks in the generator. We find attention to be still an important\nmodule for successful image generation even though it was not used in the\nrecent state-of-the-art models. Lastly, we study different attention\narchitectures in the discriminator, and propose a reference attention\nmechanism. By combining the strengths of these remedies, we improve the\ncompelling state-of-the-art Fr\\'{e}chet Inception Distance (FID) by at least\n17.5% on several benchmark datasets. We obtain even more significant\nimprovements on compositional synthetic scenes (up to 47.5% in FID).\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:10:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Liu", "Guilin", ""], ["Dundar", "Aysegul", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "2103.16807", "submitter": "Li-Ke Ma", "authors": "Li-Ke Ma, Zeshi Yang, Xin Tong, Baining Guo, KangKang Yin", "title": "Learning and Exploring Motor Skills with Spacetime Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Equipping characters with diverse motor skills is the current bottleneck of\nphysics-based character animation. We propose a Deep Reinforcement Learning\n(DRL) framework that enables physics-based characters to learn and explore\nmotor skills from reference motions. The key insight is to use loose space-time\nconstraints, termed spacetime bounds, to limit the search space in an early\ntermination fashion. As we only rely on the reference to specify loose\nspacetime bounds, our learning is more robust with respect to low quality\nreferences. Moreover, spacetime bounds are hard constraints that improve\nlearning of challenging motion segments, which can be ignored by imitation-only\nlearning. We compare our method with state-of-the-art tracking-based DRL\nmethods. We also show how to guide style exploration within the proposed\nframework\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:48:59 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ma", "Li-Ke", ""], ["Yang", "Zeshi", ""], ["Tong", "Xin", ""], ["Guo", "Baining", ""], ["Yin", "KangKang", ""]]}, {"id": "2103.16942", "submitter": "Luca Morreale", "authors": "Luca Morreale, Noam Aigerman, Vladimir Kim, Niloy J. Mitra", "title": "Neural Surface Maps", "comments": "project page: http://geometry.cs.ucl.ac.uk/projects/2021/neuralmaps/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maps are arguably one of the most fundamental concepts used to define and\noperate on manifold surfaces in differentiable geometry. Accordingly, in\ngeometry processing, maps are ubiquitous and are used in many core\napplications, such as paramterization, shape analysis, remeshing, and\ndeformation. Unfortunately, most computational representations of surface maps\ndo not lend themselves to manipulation and optimization, usually entailing\nhard, discrete problems. While algorithms exist to solve these problems, they\nare problem-specific, and a general framework for surface maps is still in\nneed. In this paper, we advocate considering neural networks as encoding\nsurface maps. Since neural networks can be composed on one another and are\ndifferentiable, we show it is easy to use them to define surfaces via atlases,\ncompose them for surface-to-surface mappings, and optimize differentiable\nobjectives relating to them, such as any notion of distortion, in a trivial\nmanner. In our experiments, we represent surfaces by generating a neural map\nthat approximates a UV parameterization of a 3D model. Then, we compose this\nmap with other neural maps which we optimize with respect to distortion\nmeasures. We show that our formulation enables trivial optimization of rather\nelusive mapping tasks, such as maps between a collection of surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:48:26 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Morreale", "Luca", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2103.17185", "submitter": "Dmytro Kotovenko", "authors": "Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Bj\\\"orn Ommer", "title": "Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been many successful implementations of neural style transfer in\nrecent years. In most of these works, the stylization process is confined to\nthe pixel domain. However, we argue that this representation is unnatural\nbecause paintings usually consist of brushstrokes rather than pixels. We\npropose a method to stylize images by optimizing parameterized brushstrokes\ninstead of pixels and further introduce a simple differentiable rendering\nmechanism. Our approach significantly improves visual quality and enables\nadditional control over the stylization process such as controlling the flow of\nbrushstrokes through user input. We provide qualitative and quantitative\nevaluations that show the efficacy of the proposed parameterized\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:15:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kotovenko", "Dmytro", ""], ["Wright", "Matthias", ""], ["Heimbrecht", "Arthur", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2103.17249", "submitter": "Dani Lischinski", "authors": "Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, Dani\n  Lischinski", "title": "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery", "comments": "18 pages, 24 figures, code and video may be found here:\n  https://github.com/orpatashnik/StyleCLIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the ability of StyleGAN to generate highly realistic images in a\nvariety of domains, much recent work has focused on understanding how to use\nthe latent spaces of StyleGAN to manipulate generated and real images. However,\ndiscovering semantically meaningful latent manipulations typically involves\npainstaking human examination of the many degrees of freedom, or an annotated\ncollection of images for each desired manipulation. In this work, we explore\nleveraging the power of recently introduced Contrastive Language-Image\nPre-training (CLIP) models in order to develop a text-based interface for\nStyleGAN image manipulation that does not require such manual effort. We first\nintroduce an optimization scheme that utilizes a CLIP-based loss to modify an\ninput latent vector in response to a user-provided text prompt. Next, we\ndescribe a latent mapper that infers a text-guided latent manipulation step for\na given input image, allowing faster and more stable text-based manipulation.\nFinally, we present a method for mapping a text prompts to input-agnostic\ndirections in StyleGAN's style space, enabling interactive text-driven image\nmanipulation. Extensive results and comparisons demonstrate the effectiveness\nof our approaches.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:51:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Patashnik", "Or", ""], ["Wu", "Zongze", ""], ["Shechtman", "Eli", ""], ["Cohen-Or", "Daniel", ""], ["Lischinski", "Dani", ""]]}, {"id": "2103.17261", "submitter": "Aayush Bansal", "authors": "Kevin Wang and Deva Ramanan and Aayush Bansal", "title": "Video Exploration via Video-Specific Autoencoders", "comments": "Project Page: https://www.cs.cmu.edu/~aayushb/Video-ViSA/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present simple video-specific autoencoders that enables human-controllable\nvideo exploration. This includes a wide variety of analytic tasks such as (but\nnot limited to) spatial and temporal super-resolution, spatial and temporal\nediting, object removal, video textures, average video exploration, and\ncorrespondence estimation within and across videos. Prior work has\nindependently looked at each of these problems and proposed different\nformulations. In this work, we observe that a simple autoencoder trained (from\nscratch) on multiple frames of a specific video enables one to perform a large\nvariety of video processing and editing tasks. Our tasks are enabled by two key\nobservations: (1) latent codes learned by the autoencoder capture spatial and\ntemporal properties of that video and (2) autoencoders can project\nout-of-sample inputs onto the video-specific manifold. For e.g. (1)\ninterpolating latent codes enables temporal super-resolution and\nuser-controllable video textures; (2) manifold reprojection enables spatial\nsuper-resolution, object removal, and denoising without training for any of the\ntasks. Importantly, a two-dimensional visualization of latent codes via\nprincipal component analysis acts as a tool for users to both visualize and\nintuitively control video edits. Finally, we quantitatively contrast our\napproach with the prior art and found that without any supervision and\ntask-specific knowledge, our approach can perform comparably to supervised\napproaches specifically trained for a task.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:56:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Kevin", ""], ["Ramanan", "Deva", ""], ["Bansal", "Aayush", ""]]}]