[{"id": "1803.00073", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat, E. A. Bashkov, S. A. Zori", "title": "Resolution Improvement of the Common Method for Presentating Arbitrary\n  Space Curves Voxel", "comments": "5 pages, 2 figures, 4 tables", "journal-ref": "International Journal of Electronics Communication and Computer\n  Engineering, 2016", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of voxel resolution for a space curve in video memory of 3D display\nis set. Furthermore, an approach solution of voxel resolution of arbitrary\nspace curve, given in parametric form, is studied. Numerous numbers of\nintensive experiments are conducted and interesting results with significant\nrecommendations are presented.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 20:39:59 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Bashkov", "E. A.", ""], ["Zori", "S. A.", ""]]}, {"id": "1803.00430", "submitter": "Dinesh Manocha", "authors": "Carl Schissler and Dinesh Manocha", "title": "Interactive Sound Rendering on Mobile Devices using Ray-Parameterized\n  Reverberation Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new sound rendering pipeline that is able to generate plausible\nsound propagation effects for interactive dynamic scenes. Our approach combines\nray-tracing-based sound propagation with reverberation filters using robust\nautomatic reverb parameter estimation that is driven by impulse responses\ncomputed at a low sampling rate.We propose a unified spherical harmonic\nrepresentation of directional sound in both the propagation and auralization\nmodules and use this formulation to perform a constant number of convolution\noperations for any number of sound sources while rendering spatial audio. In\ncomparison to previous geometric acoustic methods, we achieve a speedup of over\nan order of magnitude while delivering similar audio to high-quality\nconvolution rendering algorithms. As a result, our approach is the first\ncapable of rendering plausible dynamic sound propagation effects on commodity\nsmartphones.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:09:12 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Schissler", "Carl", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1803.00940", "submitter": "Aaditya Prakash", "authors": "Aaditya Prakash, Nick Moran, Solomon Garber, Antonella DiLillo, James\n  Storer", "title": "Protecting JPEG Images Against Adversarial Attacks", "comments": "Accepted to IEEE Data Compression Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks (DNNs) have been integrated into critical systems,\nseveral methods to attack these systems have been developed. These adversarial\nattacks make imperceptible modifications to an image that fool DNN classifiers.\nWe present an adaptive JPEG encoder which defends against many of these\nattacks. Experimentally, we show that our method produces images with high\nvisual quality while greatly reducing the potency of state-of-the-art attacks.\nOur algorithm requires only a modest increase in encoding time, produces a\ncompressed image which can be decompressed by an off-the-shelf JPEG decoder,\nand classified by an unmodified classifier\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 16:35:44 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Prakash", "Aaditya", ""], ["Moran", "Nick", ""], ["Garber", "Solomon", ""], ["DiLillo", "Antonella", ""], ["Storer", "James", ""]]}, {"id": "1803.02266", "submitter": "Demetris Marnerides", "authors": "Demetris Marnerides, Thomas Bashford-Rogers, Jonathan Hatchett and\n  Kurt Debattista", "title": "ExpandNet: A Deep Convolutional Neural Network for High Dynamic Range\n  Expansion from Low Dynamic Range Content", "comments": "Eurographics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dynamic range (HDR) imaging provides the capability of handling real\nworld lighting as opposed to the traditional low dynamic range (LDR) which\nstruggles to accurately represent images with higher dynamic range. However,\nmost imaging content is still available only in LDR. This paper presents a\nmethod for generating HDR content from LDR content based on deep Convolutional\nNeural Networks (CNNs) termed ExpandNet. ExpandNet accepts LDR images as input\nand generates images with an expanded range in an end-to-end fashion. The model\nattempts to reconstruct missing information that was lost from the original\nsignal due to quantization, clipping, tone mapping or gamma correction. The\nadded information is reconstructed from learned features, as the network is\ntrained in a supervised fashion using a dataset of HDR images. The approach is\nfully automatic and data driven; it does not require any heuristics or human\nexpertise. ExpandNet uses a multiscale architecture which avoids the use of\nupsampling layers to improve image quality. The method performs well compared\nto expansion/inverse tone mapping operators quantitatively on multiple metrics,\neven for badly exposed inputs.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 15:56:51 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 08:10:25 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Marnerides", "Demetris", ""], ["Bashford-Rogers", "Thomas", ""], ["Hatchett", "Jonathan", ""], ["Debattista", "Kurt", ""]]}, {"id": "1803.03259", "submitter": "Eugene d'Eon", "authors": "Eugene d'Eon", "title": "A reciprocal formulation of non-exponential radiative transfer. 1:\n  Sketch and motivation", "comments": "16 pages, addressed reviewer comments", "journal-ref": null, "doi": "10.1080/23324309.2018.1481433", "report-no": null, "categories": "physics.comp-ph cs.GR nucl-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous proposals to permit non-exponential free-path statistics in\nradiative transfer have not included support for volume and boundary sources\nthat are spatially uncorrelated from the scattering events in the medium.\nBirth-collision free paths are treated identically to collision-collision free\npaths and application of this to general, bounded scenes with inclusions leads\nto non-reciprocal transport. Beginning with reciprocity as a desired property,\nwe propose a new way to integrate non-exponential transport theory into general\nscenes. We distinguish between the free-path-length statistics between\ncorrelated medium particles and the free-path-length statistics beginning at\nlocations not correlated to medium particles, such as boundary surfaces,\ninclusions and uncorrelated sources. Reciprocity requires that the uncorrelated\nfree-path distributions are simply the normalized transmittance of the\ncorrelated free-path distributions. The combination leads to an equilibrium\nimbedding of a previously derived generalized transport equation into bounded\ndomains. We compare predictions of this approach to Monte Carlo simulation of\nmultiple scattering from negatively-correlated suspensions of monodispersive\nhard spheres in bounded two-dimensional domains and demonstrate improved\nperformance relative to previous work. We also derive new, exact, reciprocal,\nsingle-scattering solutions for plane-parallel half-spaces over a variety of\nnon-exponential media types.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 12:19:16 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 12:04:46 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 09:58:53 GMT"}, {"version": "v4", "created": "Tue, 20 Mar 2018 03:39:31 GMT"}, {"version": "v5", "created": "Thu, 22 Mar 2018 08:50:18 GMT"}, {"version": "v6", "created": "Fri, 18 May 2018 03:27:24 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["d'Eon", "Eugene", ""]]}, {"id": "1803.03949", "submitter": "Wei Dong", "authors": "Wei Dong, Jieqi Shi, Weijie Tang, Xin Wang, Hongbin Zha", "title": "An Efficient Volumetric Mesh Representation for Real-time Scene\n  Reconstruction using Spatial Hashing", "comments": "Accepted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh plays an indispensable role in dense real-time reconstruction essential\nin robotics. Efforts have been made to maintain flexible data structures for 3D\ndata fusion, yet an efficient incremental framework specifically designed for\nonline mesh storage and manipulation is missing. We propose a novel framework\nto compactly generate, update, and refine mesh for scene reconstruction upon a\nvolumetric representation. Maintaining a spatial-hashed field of cubes, we\ndistribute vertices with continuous value on discrete edges that support O(1)\nvertex accessing and forbid memory redundancy. By introducing Hamming distance\nin mesh refinement, we further improve the mesh quality regarding the triangle\ntype consistency with a low cost. Lock-based and lock-free operations were\napplied to avoid thread conflicts in GPU parallel computation. Experiments\ndemonstrate that the mesh memory consumption is significantly reduced while the\nrunning speed is kept in the online reconstruction process.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 11:55:55 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Dong", "Wei", ""], ["Shi", "Jieqi", ""], ["Tang", "Weijie", ""], ["Wang", "Xin", ""], ["Zha", "Hongbin", ""]]}, {"id": "1803.04305", "submitter": "Yiheng Zhang", "authors": "Qi Liu, Yiheng Zhang and Lizhuang Ma", "title": "Light Transport Simulation via Generalized Multiple Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple importance sampling (MIS) is employed to reduce variance of\nestimators, but when sampling and weighting are not suitable to the integrand,\nthe estimators would have extra variance. Therefore, robust light transport\nsimulation algorithms based on Monte Carlo sampling for different types of\nscenes are still uncompleted. In this paper, we address this problem by present\na general method, named generalized multiple importance sampling (GMIS), to\nenhance the robustness of light transport simulation based on MIS. GMIS\ncombines different sampling techniques and weighting functions, extending MIS\nto a more generalized framework. Meanwhile, we implement the GMIS in common\nrenderers and illustrate how it increase the robustness of light transport\nsimulation. Experiments show that, by applying GMIS, we obtain better\nconvergence performance and lower variance, and increase the rendering of\nambient light and specular shadow effects apparently.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 15:20:06 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 08:28:47 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Liu", "Qi", ""], ["Zhang", "Yiheng", ""], ["Ma", "Lizhuang", ""]]}, {"id": "1803.04612", "submitter": "Ricardo Barros Duarte D'Oliveira", "authors": "Ricardo B. D. d'Oliveira, Antonio L. Apolin\\'ario Jr", "title": "Procedural Planetary Multi-resolution Terrain Generation for Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Terrains are the main part of an electronic game. To reduce human effort on\ngame development, procedural techniques are used to generate synthetic\nterrains. However rendering a terrain is not a trivial task. Their rendering\ntechniques must be optimal for gaming. Specially planetary terrains, which must\naccount for precision and scale conversion. Multi-resolution models are best\nfit to planetary terrains. An observer can change his point of view without\nnoticing any decrease in visual quality. There are several proposals regarding\nreal-time terrain rendering with multi-resolution models, and there are game\nengines capable of generating large scale terrains with fixed resolution.\nHowever for the best of our knowledge, it was noticed that there are no\ntechniques which combine both aspects. In this paper we present a new technique\ncapable of generating large-scale multi-resolution terrains, whichcan be\nrendered and viewed at different scales. Rendering large scale models with high\ndefinition and low scale areas with finer details added with the aid of\nprocedural content generation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 04:29:54 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["d'Oliveira", "Ricardo B. D.", ""], ["Apolin\u00e1rio", "Antonio L.", "Jr"]]}, {"id": "1803.04656", "submitter": "Amin Banitalebi-Dehkordi", "authors": "Amin Banitalebi-Dehkordi, Mahsa T. Pourazad, and Panos Nasiopoulos", "title": "Effect of Eye Dominance on the Perception of Stereoscopic 3D Video", "comments": null, "journal-ref": "ICIP, 2014", "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetric schemes have widespread applications in the 3D video transmission\npipeline. The significance of eye dominance becomes a concern when designing\nsuch schemes. In this paper, in order to investigate the effect of eye\ndominance on the perceptual 3D video quality, a database of representative\nasymmetric stereoscopic sequences is prepared and the overall 3D quality of\nthese sequences is evaluated through subjective experiments. Experiment results\nshowed that viewers find an asymmetric video more pleasant when the view with\nhigher quality is projected to their dominant eye. Moreover, the eye dominance\nchanges the mean opinion quality score by 16 % at most, a result caused by\nslight asymmetric video compression. For all other representative types of\nasymmetry, the statistical difference is much lower and in some cases even\nnegligible.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 06:53:20 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Banitalebi-Dehkordi", "Amin", ""], ["Pourazad", "Mahsa T.", ""], ["Nasiopoulos", "Panos", ""]]}, {"id": "1803.05788", "submitter": "Tao Liu", "authors": "Zihao Liu, Tao Liu, Wujie Wen, Lei Jiang, Jie Xu, Yanzhi Wang, Gang\n  Quan", "title": "DeepN-JPEG: A Deep Neural Network Favorable JPEG-based Image Compression\n  Framework", "comments": "55th Design Automation Conference (DAC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of most fascinating machine learning techniques, deep neural network\n(DNN) has demonstrated excellent performance in various intelligent tasks such\nas image classification. DNN achieves such performance, to a large extent, by\nperforming expensive training over huge volumes of training data. To reduce the\ndata storage and transfer overhead in smart resource-limited Internet-of-Thing\n(IoT) systems, effective data compression is a \"must-have\" feature before\ntransferring real-time produced dataset for training or classification. While\nthere have been many well-known image compression approaches (such as JPEG), we\nfor the first time find that a human-visual based image compression approach\nsuch as JPEG compression is not an optimized solution for DNN systems,\nespecially with high compression ratios. To this end, we develop an image\ncompression framework tailored for DNN applications, named \"DeepN-JPEG\", to\nembrace the nature of deep cascaded information process mechanism of DNN\narchitecture. Extensive experiments, based on \"ImageNet\" dataset with various\nstate-of-the-art DNNs, show that \"DeepN-JPEG\" can achieve ~3.5x higher\ncompression rate over the popular JPEG solution while maintaining the same\naccuracy level for image recognition, demonstrating its great potential of\nstorage and power efficiency in DNN-based smart IoT system design.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 02:18:55 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Liu", "Zihao", ""], ["Liu", "Tao", ""], ["Wen", "Wujie", ""], ["Jiang", "Lei", ""], ["Xu", "Jie", ""], ["Wang", "Yanzhi", ""], ["Quan", "Gang", ""]]}, {"id": "1803.06542", "submitter": "Yuhang Wu", "authors": "Yuhang Wu, Le Anh Vu Ha, Xiang Xu, Ioannis A. Kakadiaris", "title": "Convolutional Point-set Representation: A Convolutional Bridge Between a\n  Densely Annotated Image and 3D Face Alignment", "comments": "Preprint Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust method for estimating the facial pose and shape\ninformation from a densely annotated facial image. The method relies on\nConvolutional Point-set Representation (CPR), a carefully designed matrix\nrepresentation to summarize different layers of information encoded in the set\nof detected points in the annotated image. The CPR disentangles the\ndependencies of shape and different pose parameters and enables updating\ndifferent parameters in a sequential manner via convolutional neural networks\nand recurrent layers. When updating the pose parameters, we sample reprojection\nerrors along with a predicted direction and update the parameters based on the\npattern of reprojection errors. This technique boosts the model's capability in\nsearching a local minimum under challenging scenarios. We also demonstrate that\nannotation from different sources can be merged under the framework of CPR and\ncontributes to outperforming the current state-of-the-art solutions for 3D face\nalignment. Experiments indicate the proposed CPRFA (CPR-based Face Alignment)\nsignificantly improves 3D alignment accuracy when the densely annotated image\ncontains noise and missing values, which is common under \"in-the-wild\"\nacquisition scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 17:20:25 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 21:49:54 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wu", "Yuhang", ""], ["Ha", "Le Anh Vu", ""], ["Xu", "Xiang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1803.06783", "submitter": "Xuequan Lu", "authors": "Xuequan Lu, Scott Schaefer, Jun Luo, Lizhuang Ma and Ying He", "title": "Low Rank Matrix Approximation for Geometry Filtering", "comments": "Updated at early August, 2018, Corresponding author: Xuequan Lu\n  (xuequanlu@ntu.edu.sg)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust normal estimation method for both point clouds and meshes\nusing a low rank matrix approximation algorithm. First, we compute a local\nfeature descriptor for each point and find similar, non-local neighbors that we\norganize into a matrix. We then show that a low rank matrix approximation\nalgorithm can robustly estimate normals for both point clouds and meshes.\nFurthermore, we provide a new filtering method for point cloud data to smooth\nthe position data to fit the estimated normals. We show applications of our\nmethod to point cloud filtering, point set upsampling, surface reconstruction,\nmesh denoising, and geometric texture removal. Our experiments show that our\nmethod outperforms current methods in both visual quality and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:12:05 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 06:30:27 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Lu", "Xuequan", ""], ["Schaefer", "Scott", ""], ["Luo", "Jun", ""], ["Ma", "Lizhuang", ""], ["He", "Ying", ""]]}, {"id": "1803.06843", "submitter": "Pawe{\\l} Wo\\'zny", "authors": "Filip Chudy, Pawe{\\l} Wo\\'zny", "title": "Linear-time geometric algorithm for evaluating B\\'ezier curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm for computing a point on a polynomial or rational curve in\nB\\'{e}zier form is proposed. The method has a geometric interpretation and uses\nonly convex combinations of control points. The new algorithm's computational\ncomplexity is linear with respect to the number of control points and its\nmemory complexity is $O(1)$. Some remarks on similar methods for surfaces in\nrectangular and triangular B\\'{e}zier form are also given.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 09:36:36 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 10:54:29 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 11:48:36 GMT"}, {"version": "v4", "created": "Wed, 19 Jun 2019 11:23:45 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Chudy", "Filip", ""], ["Wo\u017any", "Pawe\u0142", ""]]}, {"id": "1803.07835", "submitter": "Yao Feng", "authors": "Yao Feng, Fan Wu, Xiaohu Shao, Yanfeng Wang, Xi Zhou", "title": "Joint 3D Face Reconstruction and Dense Alignment with Position Map\n  Regression Network", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a straightforward method that simultaneously reconstructs the 3D\nfacial structure and provides dense alignment. To achieve this, we design a 2D\nrepresentation called UV position map which records the 3D shape of a complete\nface in UV space, then train a simple Convolutional Neural Network to regress\nit from a single 2D image. We also integrate a weight mask into the loss\nfunction during training to improve the performance of the network. Our method\ndoes not rely on any prior face model, and can reconstruct full facial geometry\nalong with semantic meaning. Meanwhile, our network is very light-weighted and\nspends only 9.8ms to process an image, which is extremely faster than previous\nworks. Experiments on multiple challenging datasets show that our method\nsurpasses other state-of-the-art methods on both reconstruction and alignment\ntasks by a large margin.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 10:27:04 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Feng", "Yao", ""], ["Wu", "Fan", ""], ["Shao", "Xiaohu", ""], ["Wang", "Yanfeng", ""], ["Zhou", "Xi", ""]]}, {"id": "1803.08495", "submitter": "Kevin Chen", "authors": "Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X. Chang, Thomas\n  Funkhouser, Silvio Savarese", "title": "Text2Shape: Generating Shapes from Natural Language by Learning Joint\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for generating colored 3D shapes from natural language.\nTo this end, we first learn joint embeddings of freeform text descriptions and\ncolored 3D shapes. Our model combines and extends learning by association and\nmetric learning approaches to learn implicit cross-modal connections, and\nproduces a joint representation that captures the many-to-many relations\nbetween language and physical properties of 3D shapes such as color and shape.\nTo evaluate our approach, we collect a large dataset of natural language\ndescriptions for physical 3D objects in the ShapeNet dataset. With this learned\njoint embedding we demonstrate text-to-shape retrieval that outperforms\nbaseline approaches. Using our embeddings with a novel conditional Wasserstein\nGAN framework, we generate colored 3D shapes from text. Our method is the first\nto connect natural language text with realistic 3D objects exhibiting rich\nvariations in color, texture, and shape detail. See video at\nhttps://youtu.be/zraPvRdl13Q\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:57:47 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Chen", "Kevin", ""], ["Choy", "Christopher B.", ""], ["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Funkhouser", "Thomas", ""], ["Savarese", "Silvio", ""]]}, {"id": "1803.09109", "submitter": "Yin Yang", "authors": "Ran Luo, Tianjia Shao, Huamin Wang, Weiwei Xu, Kun Zhou, Yin Yang", "title": "DeepWarp: DNN-based Nonlinear Deformation", "comments": "13 papges", "journal-ref": null, "doi": "10.1109/TVCG.2018.2881451", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepWarp is an efficient and highly re-usable deep neural network (DNN) based\nnonlinear deformable simulation framework. Unlike other deep learning\napplications such as image recognition, where different inputs have a uniform\nand consistent format (e.g. an array of all the pixels in an image), the input\nfor deformable simulation is quite variable, high-dimensional, and\nparametrization-unfriendly. Consequently, even though DNN is known for its rich\nexpressivity of nonlinear functions, directly using DNN to reconstruct the\nforce-displacement relation for general deformable simulation is nearly\nimpossible. DeepWarp obviates this difficulty by partially restoring the\nforce-displacement relation via warping the nodal displacement simulated using\na simplistic constitutive model -- the linear elasticity. In other words,\nDeepWarp yields an incremental displacement fix based on a simplified\n(therefore incorrect) simulation result other than returning the unknown\ndisplacement directly. We contrive a compact yet effective feature vector\nincluding geodesic, potential and digression to sort training pairs of per-node\nlinear and nonlinear displacement. DeepWarp is robust under different model\nshapes and tessellations. With the assistance of deformation substructuring,\none DNN training is able to handle a wide range of 3D models of various\ngeometries including most examples shown in the paper. Thanks to the linear\nelasticity and its constant system matrix, the underlying simulator only needs\nto perform one pre-factorized matrix solve at each time step, and DeepWarp is\nable to simulate large models in real time.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 13:35:38 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Luo", "Ran", ""], ["Shao", "Tianjia", ""], ["Wang", "Huamin", ""], ["Xu", "Weiwei", ""], ["Zhou", "Kun", ""], ["Yang", "Yin", ""]]}, {"id": "1803.09263", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Hui Huang, Daniel Cohen-Or, Hao Zhang", "title": "P2P-NET: Bidirectional Point Displacement Net for Shape Transform", "comments": "siggraph revision is done. 13 pages", "journal-ref": "ACM Transactions on Graphics(Proc. of SIGGRAPH), 37(4),\n  152:1-152:13, 2018", "doi": "10.1145/3197517.3201288", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce P2P-NET, a general-purpose deep neural network which learns\ngeometric transformations between point-based shape representations from two\ndomains, e.g., meso-skeletons and surfaces, partial and complete scans, etc.\nThe architecture of the P2P-NET is that of a bi-directional point displacement\nnetwork, which transforms a source point set to a target point set with the\nsame cardinality, and vice versa, by applying point-wise displacement vectors\nlearned from data. P2P-NET is trained on paired shapes from the source and\ntarget domains, but without relying on point-to-point correspondences between\nthe source and target point sets. The training loss combines two\nuni-directional geometric losses, each enforcing a shape-wise similarity\nbetween the predicted and the target point sets, and a cross-regularization\nterm to encourage consistency between displacement vectors going in opposite\ndirections. We develop and present several different applications enabled by\nour general-purpose bidirectional P2P-NET to highlight the effectiveness,\nversatility, and potential of our network in solving a variety of point-based\nshape transformation problems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 14:30:51 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 07:52:48 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 10:12:56 GMT"}, {"version": "v4", "created": "Tue, 15 May 2018 08:14:30 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yin", "Kangxue", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "1803.10048", "submitter": "Salman Faraji", "authors": "Salman Faraji, Auke Jan Ijspeert", "title": "Scalable closed-form trajectories for periodic and non-periodic\n  human-like walking", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework to generate human-like lower-limb trajectories in\nperiodic and non-periodic walking conditions. In our method, walking dynamics\nis encoded in 3LP, a linear simplified model composed of three pendulums to\nmodel falling, swing and torso balancing dynamics. To stabilize the motion, we\nuse an optimal time-projecting controller which suggests new footstep\nlocations. On top of gait generation and stabilization in the simplified space,\nwe introduce a kinematic conversion method that synthesizes more human-like\ntrajectories by combining geometric variables of the 3LP model adaptively.\nWithout any tuning, numerical optimization or off-line data, our walking gaits\nare scalable with respect to body properties and gait parameters. We can change\nvarious parameters such as body mass and height, walking direction, speed,\nfrequency, double support time, torso style, ground clearance and terrain\ninclination. We can also simulate the effect of constant external dragging\nforces or momentary perturbations. The proposed framework offers closed-form\nsolutions in all the three stages which enable simulation speeds orders of\nmagnitude faster than real time. This can be used for video games and\nanimations on portable electronic devices with a limited power. It also gives\ninsights for generation of more human-like walking gaits with humanoid robots.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 12:51:26 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Faraji", "Salman", ""], ["Ijspeert", "Auke Jan", ""]]}, {"id": "1803.11385", "submitter": "Tianjia Shao", "authors": "Tianjia Shao, Yin Yang, Yanlin Weng, Qiming Hou, Kun Zhou", "title": "H-CNN: Spatial Hashing Based CNN for 3D Shape Analysis", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TVCG.2018.2887262", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel spatial hashing based data structure to facilitate 3D\nshape analysis using convolutional neural networks (CNNs). Our method well\nutilizes the sparse occupancy of 3D shape boundary and builds hierarchical hash\ntables for an input model under different resolutions. Based on this data\nstructure, we design two efficient GPU algorithms namely hash2col and col2hash\nso that the CNN operations like convolution and pooling can be efficiently\nparallelized. The spatial hashing is nearly minimal, and our data structure is\nalmost of the same size as the raw input. Compared with state-of-the-art\noctree-based methods, our data structure significantly reduces the memory\nfootprint during the CNN training. As the input geometry features are more\ncompactly packed, CNN operations also run faster with our data structure. The\nexperiment shows that, under the same network structure, our method yields\ncomparable or better benchmarks compared to the state-of-the-art while it has\nonly one-third memory consumption. Such superior memory performance allows the\nCNN to handle high-resolution shape analysis.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 08:34:26 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Shao", "Tianjia", ""], ["Yang", "Yin", ""], ["Weng", "Yanlin", ""], ["Hou", "Qiming", ""], ["Zhou", "Kun", ""]]}]