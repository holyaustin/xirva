[{"id": "2104.00337", "submitter": "Yinlin Hu", "authors": "Yinlin Hu, Sebastien Speierer, Wenzel Jakob, Pascal Fua, Mathieu\n  Salzmann", "title": "Wide-Depth-Range 6D Object Pose Estimation in Space", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  6D pose estimation in space poses unique challenges that are not commonly\nencountered in the terrestrial setting. One of the most striking differences is\nthe lack of atmospheric scattering, allowing objects to be visible from a great\ndistance while complicating illumination conditions. Currently available\nbenchmark datasets do not place a sufficient emphasis on this aspect and mostly\ndepict the target in close proximity.\n  Prior work tackling pose estimation under large scale variations relies on a\ntwo-stage approach to first estimate scale, followed by pose estimation on a\nresized image patch. We instead propose a single-stage hierarchical end-to-end\ntrainable network that is more robust to scale variations. We demonstrate that\nit outperforms existing approaches not only on images synthesized to resemble\nimages taken in space but also on standard benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:39:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Hu", "Yinlin", ""], ["Speierer", "Sebastien", ""], ["Jakob", "Wenzel", ""], ["Fua", "Pascal", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2104.00484", "submitter": "Longwen Zhang", "authors": "Longwen Zhang, Qixuan Zhang, Minye Wu, Jingyi Yu, Lan Xu", "title": "Neural Video Portrait Relighting in Real-time via Consistency Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video portraits relighting is critical in user-facing human photography,\nespecially for immersive VR/AR experience. Recent advances still fail to\nrecover consistent relit result under dynamic illuminations from monocular RGB\nstream, suffering from the lack of video consistency supervision. In this\npaper, we propose a neural approach for real-time, high-quality and coherent\nvideo portrait relighting, which jointly models the semantic, temporal and\nlighting consistency using a new dynamic OLAT dataset. We propose a hybrid\nstructure and lighting disentanglement in an encoder-decoder architecture,\nwhich combines a multi-task and adversarial training strategy for\nsemantic-aware consistency modeling. We adopt a temporal modeling scheme via\nflow-based supervision to encode the conjugated temporal consistency in a cross\nmanner. We also propose a lighting sampling strategy to model the illumination\nconsistency and mutation for natural portrait light manipulation in real-world.\nExtensive experiments demonstrate the effectiveness of our approach for\nconsistent video portrait light-editing and relighting, even using mobile\ncomputing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:13:28 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Longwen", ""], ["Zhang", "Qixuan", ""], ["Wu", "Minye", ""], ["Yu", "Jingyi", ""], ["Xu", "Lan", ""]]}, {"id": "2104.00514", "submitter": "Luc Moschella", "authors": "Luca Moschella, Simone Melzi, Luca Cosmo, Filippo Maggioli, Or Litany,\n  Maks Ovsjanikov, Leonidas Guibas, Emanuele Rodol\\`a", "title": "Spectral Unions of Partial Deformable 3D Shapes", "comments": "17 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral geometric methods have brought revolutionary changes to the field of\ngeometry processing -- however, when the data to be processed exhibits severe\npartiality, such methods fail to generalize. As a result, there exists a big\nperformance gap between methods dealing with complete shapes, and methods that\naddress missing geometry. In this paper, we propose a possible way to fill this\ngap. We introduce the first method to compute compositions of non-rigidly\ndeforming shapes, without requiring to solve first for a dense correspondence\nbetween the given partial shapes. We do so by operating in a purely spectral\ndomain, where we define a union operation between short sequences of\neigenvalues. Working with eigenvalues allows to deal with unknown\ncorrespondence, different sampling, and different discretization (point clouds\nand meshes alike), making this operation especially robust and general. Our\napproach is data-driven, and can generalize to isometric and non-isometric\ndeformations of the surface, as long as these stay within the same semantic\nclass (e.g., human bodies), as well as to partiality artifacts not seen at\ntraining time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:19:18 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Moschella", "Luca", ""], ["Melzi", "Simone", ""], ["Cosmo", "Luca", ""], ["Maggioli", "Filippo", ""], ["Litany", "Or", ""], ["Ovsjanikov", "Maks", ""], ["Guibas", "Leonidas", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2104.00618", "submitter": "Benjamin Kahl", "authors": "Benjamin Kahl", "title": "Real-Time Global Illumination Using OpenGL And Voxel Cone Tracing", "comments": "75 pages, 50 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building systems capable of replicating global illumination models with\ninteractive frame-rates has long been one of the toughest conundrums facing\ncomputer graphics researchers. Voxel Cone Tracing, as proposed by Cyril Crassin\net al. in 2011, makes use of mipmapped 3D textures containing a voxelized\nrepresentation of an environments direct light component to trace diffuse,\nspecular and occlusion cones in linear time to extrapolate a surface fragments\nindirect light emitted towards a given photo-receptor. Seemingly providing a\nwell-disposed balance between performance and physical fidelity, this thesis\nexamines the algorithms theoretical side on the basis of the rendering equation\nas well as its practical side in the context of a self-implemented,\nOpenGL-based variant. Whether if it can compete with long standing alternatives\nsuch as radiosity and raytracing will be determined in the subsequent\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:49:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kahl", "Benjamin", ""]]}, {"id": "2104.00637", "submitter": "Jianfeng Wu", "authors": "Min Zhang, Dongsheng An, Na Lei, Jianfeng Wu, Tong Zhao, Xiaoyin Xu,\n  Yalin Wang, Xianfeng Gu", "title": "Cortical Morphometry Analysis based on Worst Transportation Theory", "comments": "IPMI 2021, 13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Biomarkers play an important role in early detection and intervention in\nAlzheimer's disease (AD). However, obtaining effective biomarkers for AD is\nstill a big challenge. In this work, we propose to use the worst transportation\ncost as a univariate biomarker to index cortical morphometry for tracking AD\nprogression. The worst transportation (WT) aims to find the least economical\nway to transport one measure to the other, which contrasts to the optimal\ntransportation (OT) that finds the most economical way between measures. To\ncompute the WT cost, we generalize the Brenier theorem for the OT map to the WT\nmap, and show that the WT map is the gradient of a concave function satisfying\nthe Monge-Ampere equation. We also develop an efficient algorithm to compute\nthe WT map based on computational geometry. We apply the algorithm to analyze\ncortical shape difference between dementia due to AD and normal aging\nindividuals. The experimental results reveal the effectiveness of our proposed\nmethod which yields better statistical performance than other competiting\nmethods including the OT.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:35:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Min", ""], ["An", "Dongsheng", ""], ["Lei", "Na", ""], ["Wu", "Jianfeng", ""], ["Zhao", "Tong", ""], ["Xu", "Xiaoyin", ""], ["Wang", "Yalin", ""], ["Gu", "Xianfeng", ""]]}, {"id": "2104.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, Noah Snavely", "title": "PhySG: Inverse Rendering with Spherical Gaussians for Physics-based\n  Material Editing and Relighting", "comments": "Accepted to CVPR 2021; Project page:\n  https://kai-46.github.io/PhySG-website/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PhySG, an end-to-end inverse rendering pipeline that includes a\nfully differentiable renderer and can reconstruct geometry, materials, and\nillumination from scratch from a set of RGB input images. Our framework\nrepresents specular BRDFs and environmental illumination using mixtures of\nspherical Gaussians, and represents geometry as a signed distance function\nparameterized as a Multi-Layer Perceptron. The use of spherical Gaussians\nallows us to efficiently solve for approximate light transport, and our method\nworks on scenes with challenging non-Lambertian reflectance captured under\nnatural, static illumination. We demonstrate, with both synthetic and real\ndata, that our reconstructions not only enable rendering of novel viewpoints,\nbut also physics-based appearance editing of materials and illumination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:02 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Kai", ""], ["Luan", "Fujun", ""], ["Wang", "Qianqian", ""], ["Bala", "Kavita", ""], ["Snavely", "Noah", ""]]}, {"id": "2104.00677", "submitter": "Ajay Jain", "authors": "Ajay Jain and Matthew Tancik and Pieter Abbeel", "title": "Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis", "comments": "Project website: https://www.ajayj.com/dietnerf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present DietNeRF, a 3D neural scene representation estimated from a few\nimages. Neural Radiance Fields (NeRF) learn a continuous volumetric\nrepresentation of a scene through multi-view consistency, and can be rendered\nfrom novel viewpoints by ray casting. While NeRF has an impressive ability to\nreconstruct geometry and fine details given many images, up to 100 for\nchallenging 360{\\deg} scenes, it often finds a degenerate solution to its image\nreconstruction objective when only a few input views are available. To improve\nfew-shot quality, we propose DietNeRF. We introduce an auxiliary semantic\nconsistency loss that encourages realistic renderings at novel poses. DietNeRF\nis trained on individual scenes to (1) correctly render given input views from\nthe same pose, and (2) match high-level semantic attributes across different,\nrandom poses. Our semantic loss allows us to supervise DietNeRF from arbitrary\nposes. We extract these semantics using a pre-trained visual encoder such as\nCLIP, a Vision Transformer trained on hundreds of millions of diverse\nsingle-view, 2D photographs mined from the web with natural language\nsupervision. In experiments, DietNeRF improves the perceptual quality of\nfew-shot view synthesis when learned from scratch, can render novel views with\nas few as one observed image when pre-trained on a multi-view dataset, and\nproduces plausible completions of completely unobserved regions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jain", "Ajay", ""], ["Tancik", "Matthew", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2104.00702", "submitter": "Pablo Palafox", "authors": "Pablo Palafox, Alja\\v{z} Bo\\v{z}i\\v{c}, Justus Thies, Matthias\n  Nie{\\ss}ner, Angela Dai", "title": "NPMs: Neural Parametric Models for 3D Deformable Shapes", "comments": "Video: https://youtu.be/muZXXgkkMPY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric 3D models have enabled a wide variety of tasks in computer\ngraphics and vision, such as modeling human bodies, faces, and hands. However,\nthe construction of these parametric models is often tedious, as it requires\nheavy manual tweaking, and they struggle to represent additional complexity and\ndetails such as wrinkles or clothing. To this end, we propose Neural Parametric\nModels (NPMs), a novel, learned alternative to traditional, parametric 3D\nmodels, which does not require hand-crafted, object-specific constraints. In\nparticular, we learn to disentangle 4D dynamics into latent-space\nrepresentations of shape and pose, leveraging the flexibility of recent\ndevelopments in learned implicit functions. Crucially, once learned, our neural\nparametric models of shape and pose enable optimization over the learned spaces\nto fit to new observations, similar to the fitting of a traditional parametric\nmodel, e.g., SMPL. This enables NPMs to achieve a significantly more accurate\nand detailed representation of observed deformable sequences. We show that NPMs\nimprove notably over both parametric and non-parametric state of the art in\nreconstruction and tracking of monocular depth sequences of clothed humans and\nhands. Latent-space interpolation as well as shape / pose transfer experiments\nfurther demonstrate the usefulness of NPMs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:14:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Palafox", "Pablo", ""], ["Bo\u017ei\u010d", "Alja\u017e", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""], ["Dai", "Angela", ""]]}, {"id": "2104.00837", "submitter": "Pingchuan Ma", "authors": "Pingchuan Ma, Tao Du, John Z. Zhang, Kui Wu, Andrew Spielberg, Robert\n  K. Katzschmann, Wojciech Matusik", "title": "DiffAqua: A Differentiable Computational Design Pipeline for Soft\n  Underwater Swimmers with Shape Interpolation", "comments": "ACM SIGGRAPH 2021. Homepage: http://diffaqua.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3450626.3459832", "report-no": null, "categories": "cs.LG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational design of soft underwater swimmers is challenging because\nof the high degrees of freedom in soft-body modeling. In this paper, we present\na differentiable pipeline for co-designing a soft swimmer's geometry and\ncontroller. Our pipeline unlocks gradient-based algorithms for discovering\nnovel swimmer designs more efficiently than traditional gradient-free\nsolutions. We propose Wasserstein barycenters as a basis for the geometric\ndesign of soft underwater swimmers since it is differentiable and can naturally\ninterpolate between bio-inspired base shapes via optimal transport. By\ncombining this design space with differentiable simulation and control, we can\nefficiently optimize a soft underwater swimmer's performance with fewer\nsimulations than baseline methods. We demonstrate the efficacy of our method on\nvarious design problems such as fast, stable, and energy-efficient swimming and\ndemonstrate applicability to multi-objective design.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:18:15 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 18:58:41 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ma", "Pingchuan", ""], ["Du", "Tao", ""], ["Zhang", "John Z.", ""], ["Wu", "Kui", ""], ["Spielberg", "Andrew", ""], ["Katzschmann", "Robert K.", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2104.00867", "submitter": "Jumyung Chang", "authors": "Jumyung Chang, Vinicius C. Azevedo, Christopher Batty", "title": "Curl-Flow: Pointwise Incompressible Velocity Interpolation forGrid-Based\n  Fluids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology to enhance grid-based fluid animation with\npointwise divergence-free velocity interpolation. Our method takes as input a\ndiscretely divergence-free staggered grid velocity field generated by a\nstandard pressure projection, and first recovers a consistent corresponding\nedge-based discrete vector potential in 3D (or node-based stream function in\n2D). We interpolate these values to form a pointwise potential, and apply the\ncontinuous curl operator to recover a pointwise flow field that is perfectly\nincompressible. Our method supports irregular geometry through the use of level\nset-based cut-cells. To recover a smooth and velocity-consistent discrete\nvector potential in 3D, we employ a sweeping approach followed by a gauge\ncorrection that requires a single scalar Poisson solve, rather than a vector\nPoisson problem. In both 2D and 3D, we show how modified interpolation\nstrategies can be applied to better account for the presence of irregular\ncut-cell boundaries. Our results demonstrate that our overall proposed\nCurl-Flow framework produces significantly better particle trajectories that\nsuffer from far fewer spurious sources or sinks, respect irregular obstacles,\nand better preserve particle distributions over time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:12:04 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chang", "Jumyung", ""], ["Azevedo", "Vinicius C.", ""], ["Batty", "Christopher", ""]]}, {"id": "2104.00912", "submitter": "Frederic Le Mouel", "authors": "Michael Puentes (UIS), Diana Novoa, John Delgado Nivia (UTS), Carlos\n  Barrios Hern\\'andez (UIS), Oscar Carrillo (DYNAMID, CPE), Fr\\'ed\\'eric Le\n  Mou\\\"el (DYNAMID)", "title": "Datacentric analysis to reduce pedestrians accidents: A case study in\n  Colombia", "comments": null, "journal-ref": "International Conference on Sustainable Smart Cities and\n  Territories (SSCt2021), Apr 2021, Doha, Qatar", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2012, in a case-study in Bucaramanga-Colombia, 179 pedestrians died in\ncar accidents, and another 2873 pedestrians were injured. Each day, at least\none passerby is involved in a tragedy. Knowing the causes to decrease accidents\nis crucial, and using system-dynamics to reproduce the collisions' events is\ncritical to prevent further accidents. This work implements simulations to save\nlives by reducing the city's accidental rate and suggesting new safety policies\nto implement. Simulation's inputs are video recordings in some areas of the\ncity. Deep Learning analysis of the images results in the segmentation of the\ndifferent objects in the scene, and an interaction model identifies the primary\nreasons which prevail in the pedestrians or vehicles' behaviours. The first and\nmost efficient safety policy to implement-validated by our simulations-would be\nto build speed bumps in specific places before the crossings reducing the\naccident rate by 80%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:59:50 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Puentes", "Michael", "", "UIS"], ["Novoa", "Diana", "", "UTS"], ["Nivia", "John Delgado", "", "UTS"], ["Hern\u00e1ndez", "Carlos Barrios", "", "UIS"], ["Carrillo", "Oscar", "", "DYNAMID, CPE"], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", "", "DYNAMID"]]}, {"id": "2104.01019", "submitter": "Susanne M Hoffmann", "authors": "Georg Zotti, Susanne M Hoffmann, Alexander Wolf, Fabien Ch\\'ereau,\n  Guillaume Ch\\'ereau", "title": "The Simulated Sky: Stellarium for Cultural Astronomy Research", "comments": "https://journal.equinoxpub.com/JSA/issue/view/1762", "journal-ref": "Journal of Skyscape Archaeology JSA 6.2 (2020) 221-258", "doi": "10.1558/jsa.17822", "report-no": null, "categories": "astro-ph.IM cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For centuries, the rich nocturnal environment of the starry sky could be\nmodelled only by analogue tools such as paper planispheres, atlases, globes and\nnumerical tables. The immersive sky simulator of the twentieth century, the\noptomechanical planetarium, provided new ways for representing and teaching\nabout the sky, but the high construction and running costs meant that they have\nnot become common. However, in recent decades, \"desktop planetarium programs\"\nrunning on personal computers have gained wide attention. Modern incarnations\nare immensely versatile tools, mostly targeted towards the community of amateur\nastronomers and for knowledge transfer in transdisciplinary research. Cultural\nastronomers also value the possibilities they give of simulating the skies of\npast times or other cultures. With this paper, we provide an extended\npresentation of the open-source project Stellarium, which in the last few years\nhas been enriched with capabilities for cultural astronomy research not found\nin similar, commercial alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:28:25 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zotti", "Georg", ""], ["Hoffmann", "Susanne M", ""], ["Wolf", "Alexander", ""], ["Ch\u00e9reau", "Fabien", ""], ["Ch\u00e9reau", "Guillaume", ""]]}, {"id": "2104.01381", "submitter": "Tsubasa Murate", "authors": "Tsubasa Murate, Takashi Watanabe, Masaki Yamada", "title": "Learning Mobile CNN Feature Extraction Toward Fast Computation of Visual\n  Object Tracking", "comments": "9 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct a lightweight, high-precision and high-speed\nobject tracking using a trained CNN. Conventional methods with trained CNNs use\nVGG16 network which requires powerful computational resources. Therefore, there\nis a problem that it is difficult to apply in low computation resources\nenvironments. To solve this problem, we use MobileNetV3, which is a CNN for\nmobile terminals.Based on Feature Map Selection Tracking, we propose a new\narchitecture that extracts effective features of MobileNet for object tracking.\nThe architecture requires no online learning but only offline learning. In\naddition, by using features of objects other than tracking target, the features\nof tracking target are extracted more efficiently. We measure the tracking\naccuracy with Visual Tracker Benchmark and confirm that the proposed method can\nperform high-precision and high-speed calculation even in low computation\nresource environments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 11:49:54 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Murate", "Tsubasa", ""], ["Watanabe", "Takashi", ""], ["Yamada", "Masaki", ""]]}, {"id": "2104.01762", "submitter": "Yanhong Zeng", "authors": "Yanhong Zeng, Jianlong Fu, Hongyang Chao", "title": "3D Human Body Reshaping with Anthropometric Modeling", "comments": "ICIMCS 2017(oral). The final publication is available at Springer via\n  https://doi.org/10.1007/978-981-10-8530-7_10", "journal-ref": "In International Conference on Internet Multimedia Computing and\n  Service (pp. 96-107). Springer, Singapore (2017)", "doi": "10.1007/978-981-10-8530-7_10", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Reshaping accurate and realistic 3D human bodies from anthropometric\nparameters (e.g., height, chest size, etc.) poses a fundamental challenge for\nperson identification, online shopping and virtual reality. Existing approaches\nfor creating such 3D shapes often suffer from complex measurement by range\ncameras or high-end scanners, which either involve heavy expense cost or result\nin low quality. However, these high-quality equipments limit existing\napproaches in real applications, because the equipments are not easily\naccessible for common users. In this paper, we have designed a 3D human body\nreshaping system by proposing a novel feature-selection-based local mapping\ntechnique, which enables automatic anthropometric parameter modeling for each\nbody facet. Note that the proposed approach can leverage limited anthropometric\nparameters (i.e., 3-5 measurements) as input, which avoids complex measurement,\nand thus better user-friendly experience can be achieved in real scenarios.\nSpecifically, the proposed reshaping model consists of three steps. First, we\ncalculate full-body anthropometric parameters from limited user inputs by\nimputation technique, and thus essential anthropometric parameters for 3D body\nreshaping can be obtained. Second, we select the most relevant anthropometric\nparameters for each facet by adopting relevance masks, which are learned\noffline by the proposed local mapping technique. Third, we generate the 3D body\nmeshes by mapping matrices, which are learned by linear regression from the\nselected parameters to mesh-based body representation. We conduct experiments\nby anthropomorphic evaluation and a user study from 68 volunteers. Experiments\nshow the superior results of the proposed system in terms of mean\nreconstruction error against the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 04:09:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zeng", "Yanhong", ""], ["Fu", "Jianlong", ""], ["Chao", "Hongyang", ""]]}, {"id": "2104.02052", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Krishna Kumar Singh, Yong Jae Lee", "title": "Generating Furry Cars: Disentangling Object Shape & Appearance across\n  Multiple Domains", "comments": "Camera ready version for ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the novel task of learning disentangled representations of object\nshape and appearance across multiple domains (e.g., dogs and cars). The goal is\nto learn a generative model that learns an intermediate distribution, which\nborrows a subset of properties from each domain, enabling the generation of\nimages that did not exist in any domain exclusively. This challenging problem\nrequires an accurate disentanglement of object shape, appearance, and\nbackground from each domain, so that the appearance and shape factors from the\ntwo domains can be interchanged. We augment an existing approach that can\ndisentangle factors within a single domain but struggles to do so across\ndomains. Our key technical contribution is to represent object appearance with\na differentiable histogram of visual features, and to optimize the generator so\nthat two images with the same latent appearance factor but different latent\nshape factors produce similar histograms. On multiple multi-domain datasets, we\ndemonstrate our method leads to accurate and consistent appearance and shape\ntransfer across domains.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:59:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Singh", "Krishna Kumar", ""], ["Lee", "Yong Jae", ""]]}, {"id": "2104.02180", "submitter": "Xue Bin Peng", "authors": "Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, Angjoo Kanazawa", "title": "AMP: Adversarial Motion Priors for Stylized Physics-Based Character\n  Control", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459670", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synthesizing graceful and life-like behaviors for physically simulated\ncharacters has been a fundamental challenge in computer animation. Data-driven\nmethods that leverage motion tracking are a prominent class of techniques for\nproducing high fidelity motions for a wide range of behaviors. However, the\neffectiveness of these tracking-based methods often hinges on carefully\ndesigned objective functions, and when applied to large and diverse motion\ndatasets, these methods require significant additional machinery to select the\nappropriate motion for the character to track in a given scenario. In this\nwork, we propose to obviate the need to manually design imitation objectives\nand mechanisms for motion selection by utilizing a fully automated approach\nbased on adversarial imitation learning. High-level task objectives that the\ncharacter should perform can be specified by relatively simple reward\nfunctions, while the low-level style of the character's behaviors can be\nspecified by a dataset of unstructured motion clips, without any explicit clip\nselection or sequencing. These motion clips are used to train an adversarial\nmotion prior, which specifies style-rewards for training the character through\nreinforcement learning (RL). The adversarial RL procedure automatically selects\nwhich motion to perform, dynamically interpolating and generalizing from the\ndataset. Our system produces high-quality motions that are comparable to those\nachieved by state-of-the-art tracking-based techniques, while also being able\nto easily accommodate large datasets of unstructured motion clips. Composition\nof disparate skills emerges automatically from the motion prior, without\nrequiring a high-level motion planner or other task-specific annotations of the\nmotion clips. We demonstrate the effectiveness of our framework on a diverse\ncast of complex simulated characters and a challenging suite of motor control\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 22:43:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Peng", "Xue Bin", ""], ["Ma", "Ze", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2104.02654", "submitter": "Pepe Eulzer", "authors": "Pepe Eulzer, Monique Meuschke, Carsten Klingner, Kai Lawonn", "title": "Visualizing Carotid Blood Flow Simulations for Stroke Prevention", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.GR physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate how concepts from medical flow visualization can\nbe applied to enhance stroke prevention diagnostics. Our focus lies on carotid\nstenoses, i.e., local narrowings of the major brain-supplying arteries, which\nare a frequent cause of stroke. Carotid surgery can reduce the stroke risk\nassociated with stenoses, however, the procedure entails risks itself.\nTherefore, a thorough assessment of each case is necessary. In routine\ndiagnostics, the morphology and hemodynamics of an afflicted vessel are\nseparately analyzed using angiography and sonography, respectively. Blood flow\nsimulations based on computational fluid dynamics could enable the visual\nintegration of hemodynamic and morphological information and provide a higher\nresolution on relevant parameters. We identify and abstract the tasks involved\nin the assessment of stenoses and investigate how clinicians could derive\nrelevant insights from carotid blood flow simulations. We adapt and refine a\ncombination of techniques to facilitate this purpose, integrating\nspatiotemporal navigation, dimensional reduction, and contextual embedding. We\nevaluated and discussed our approach with an interdisciplinary group of medical\npractitioners, fluid simulation and flow visualization researchers. Our initial\nfindings indicate that visualization techniques could promote usage of carotid\nblood flow simulations in practice.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:39:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Eulzer", "Pepe", ""], ["Meuschke", "Monique", ""], ["Klingner", "Carsten", ""], ["Lawonn", "Kai", ""]]}, {"id": "2104.02656", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi, Vipul Bajaj, Badri N Patro, K S Venkatesh, Vinay P\n  Namboodiri, Preethi Jyothi", "title": "Collaborative Learning to Generate Audio-Video Jointly", "comments": "ICASSP 2021 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There have been a number of techniques that have demonstrated the generation\nof multimedia data for one modality at a time using GANs, such as the ability\nto generate images, videos, and audio. However, so far, the task of multi-modal\ngeneration of data, specifically for audio and videos both, has not been\nsufficiently well-explored. Towards this, we propose a method that demonstrates\nthat we are able to generate naturalistic samples of video and audio data by\nthe joint correlated generation of audio and video modalities. The proposed\nmethod uses multiple discriminators to ensure that the audio, video, and the\njoint output are also indistinguishable from real-world samples. We present a\ndataset for this task and show that we are able to generate realistic samples.\nThis method is validated using various standard metrics such as Inception\nScore, Frechet Inception Distance (FID) and through human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:00:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Bajaj", "Vipul", ""], ["Patro", "Badri N", ""], ["Venkatesh", "K S", ""], ["Namboodiri", "Vinay P", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2104.02789", "submitter": "Alexandr Kuznetsov", "authors": "Alexandr Kuznetsov, Krishna Mullia, Zexiang Xu, Milo\\v{s} Ha\\v{s}an\n  and Ravi Ramamoorthi", "title": "NeuMIP: Multi-Resolution Neural Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose NeuMIP, a neural method for representing and rendering a variety\nof material appearances at different scales. Classical prefiltering\n(mipmapping) methods work well on simple material properties such as diffuse\ncolor, but fail to generalize to normals, self-shadowing, fibers or more\ncomplex microstructures and reflectances. In this work, we generalize\ntraditional mipmap pyramids to pyramids of neural textures, combined with a\nfully connected network. We also introduce neural offsets, a novel method which\nallows rendering materials with intricate parallax effects without any\ntessellation. This generalizes classical parallax mapping, but is trained\nwithout supervision by any explicit heightfield. Neural materials within our\nsystem support a 7-dimensional query, including position, incoming and outgoing\ndirection, and the desired filter kernel size. The materials have small storage\n(on the order of standard mipmapping except with more texture channels), and\ncan be integrated within common Monte-Carlo path tracing systems. We\ndemonstrate our method on a variety of materials, resulting in complex\nappearance across levels of detail, with accurate parallax, self-shadowing, and\nother effects.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:22:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Kuznetsov", "Alexandr", ""], ["Mullia", "Krishna", ""], ["Xu", "Zexiang", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2104.03311", "submitter": "Chuang Gan", "authors": "Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B.\n  Tenenbaum, Chuang Gan", "title": "PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable\n  Physics", "comments": "Accepted to ICLR 2021 as a spotlight presentation. Project page:\n  http://plasticinelab.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulated virtual environments serve as one of the main driving forces behind\ndeveloping and evaluating skill learning algorithms. However, existing\nenvironments typically only simulate rigid body physics. Additionally, the\nsimulation process usually does not provide gradients that might be useful for\nplanning and control optimizations. We introduce a new differentiable physics\nbenchmark called PasticineLab, which includes a diverse collection of soft body\nmanipulation tasks. In each task, the agent uses manipulators to deform the\nplasticine into the desired configuration. The underlying physics engine\nsupports differentiable elastic and plastic deformation using the DiffTaichi\nsystem, posing many under-explored challenges to robotic agents. We evaluate\nseveral existing reinforcement learning (RL) methods and gradient-based methods\non this benchmark. Experimental results suggest that 1) RL-based approaches\nstruggle to solve most of the tasks efficiently; 2) gradient-based approaches,\nby optimizing open-loop control sequences with the built-in differentiable\nphysics engine, can rapidly find a solution within tens of iterations, but\nstill fall short on multi-stage tasks that require long-term planning. We\nexpect that PlasticineLab will encourage the development of novel algorithms\nthat combine differentiable physics and RL for more complex physics-based skill\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Huang", "Zhiao", ""], ["Hu", "Yuanming", ""], ["Du", "Tao", ""], ["Zhou", "Siyuan", ""], ["Su", "Hao", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2104.03493", "submitter": "Ziqian Bai", "authors": "Ziqian Bai, Zhaopeng Cui, Xiaoming Liu, Ping Tan", "title": "Riggable 3D Face Reconstruction via In-Network Optimization", "comments": "CVPR2021. Code: https://github.com/zqbai-jeremy/INORig Camera Ready\n  Paper: https://zqbai-jeremy.github.io/files/INORig.pdf Camera Ready Supp:\n  https://zqbai-jeremy.github.io/files/INORig_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for riggable 3D face reconstruction from\nmonocular images, which jointly estimates a personalized face rig and per-image\nparameters including expressions, poses, and illuminations. To achieve this\ngoal, we design an end-to-end trainable network embedded with a differentiable\nin-network optimization. The network first parameterizes the face rig as a\ncompact latent code with a neural decoder, and then estimates the latent code\nas well as per-image parameters via a learnable optimization. By estimating a\npersonalized face rig, our method goes beyond static reconstructions and\nenables downstream applications such as video retargeting. In-network\noptimization explicitly enforces constraints derived from the first principles,\nthus introduces additional priors than regression-based methods. Finally,\ndata-driven priors from deep learning are utilized to constrain the ill-posed\nmonocular setting and ease the optimization difficulty. Experiments demonstrate\nthat our method achieves SOTA reconstruction accuracy, reasonable robustness\nand generalization ability, and supports standard face rig applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:53:20 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bai", "Ziqian", ""], ["Cui", "Zhaopeng", ""], ["Liu", "Xiaoming", ""], ["Tan", "Ping", ""]]}, {"id": "2104.03515", "submitter": "Diqiong Jiang", "authors": "Diqiong Jiang, Yiwei Jin, Risheng Deng, Ruofeng Tong, Fanglue Zhang,\n  Yukun Yai, Ming Tang", "title": "Reconstructing Recognizable 3D Face Shapes based on 3D Morphable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many recent works have reconstructed distinctive 3D face shapes by\naggregating shape parameters of the same identity and separating those of\ndifferent people based on parametric models (e.g., 3D morphable models\n(3DMMs)). However, despite the high accuracy in the face recognition task using\nthese shape parameters, the visual discrimination of face shapes reconstructed\nfrom those parameters is unsatisfactory. The following research question has\nnot been answered in previous works: Do discriminative shape parameters\nguarantee visual discrimination in represented 3D face shapes? This paper\nanalyzes the relationship between shape parameters and reconstructed shape\ngeometry and proposes a novel shape identity-aware regularization(SIR) loss for\nshape parameters, aiming at increasing discriminability in both the shape\nparameter and shape geometry domains. Moreover, to cope with the lack of\ntraining data containing both landmark and identity annotations, we propose a\nnetwork structure and an associated training strategy to leverage mixed data\ncontaining either identity or landmark labels. We compare our method with\nexisting methods in terms of the reconstruction error, visual\ndistinguishability, and face recognition accuracy of the shape parameters.\nExperimental results show that our method outperforms the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 05:11:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jiang", "Diqiong", ""], ["Jin", "Yiwei", ""], ["Deng", "Risheng", ""], ["Tong", "Ruofeng", ""], ["Zhang", "Fanglue", ""], ["Yai", "Yukun", ""], ["Tang", "Ming", ""]]}, {"id": "2104.03800", "submitter": "Yuta Itoh", "authors": "Yuta Itoh, Takumi Kaminokado, Kaan Aksit", "title": "Beaming Displays", "comments": "10 pages. This is a preprint of a publication at IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2021. Presented at and nominated\n  for best journal papers at IEEE Virtual Reality (VR) 2021\n  (https://ieeevr.org/2021/awards/conference-awards/)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2021.3067764", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing near-eye display designs struggle to balance between multiple\ntrade-offs such as form factor, weight, computational requirements, and battery\nlife. These design trade-offs are major obstacles on the path towards an\nall-day usable near-eye display. In this work, we address these trade-offs by,\nparadoxically, \\textit{removing the display} from near-eye displays. We present\nthe beaming displays, a new type of near-eye display system that uses a\nprojector and an all passive wearable headset. We modify an off-the-shelf\nprojector with additional lenses. We install such a projector to the\nenvironment to beam images from a distance to a passive wearable headset. The\nbeaming projection system tracks the current position of a wearable headset to\nproject distortion-free images with correct perspectives. In our system, a\nwearable headset guides the beamed images to a user's retina, which are then\nperceived as an augmented scene within a user's field of view. In addition to\nproviding the system design of the beaming display, we provide a physical\nprototype and show that the beaming display can provide resolutions as high as\nconsumer-level near-eye displays. We also discuss the different aspects of the\ndesign space for our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:24:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Itoh", "Yuta", ""], ["Kaminokado", "Takumi", ""], ["Aksit", "Kaan", ""]]}, {"id": "2104.03900", "submitter": "Xianghao Xu", "authors": "Xianghao Xu, Wenzhe Peng, Chin-Yi Cheng, Karl D.D. Willis, Daniel\n  Ritchie", "title": "Inferring CAD Modeling Sequences Using Zone Graphs", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In computer-aided design (CAD), the ability to \"reverse engineer\" the\nmodeling steps used to create 3D shapes is a long-sought-after goal. This\nprocess can be decomposed into two sub-problems: converting an input mesh or\npoint cloud into a boundary representation (or B-rep), and then inferring\nmodeling operations which construct this B-rep. In this paper, we present a new\nsystem for solving the second sub-problem. Central to our approach is a new\ngeometric representation: the zone graph. Zones are the set of solid regions\nformed by extending all B-Rep faces and partitioning space with them; a zone\ngraph has these zones as its nodes, with edges denoting geometric adjacencies\nbetween them. Zone graphs allow us to tractably work with industry-standard CAD\noperations, unlike prior work using CSG with parametric primitives. We focus on\nCAD programs consisting of sketch + extrude + Boolean operations, which are\ncommon in CAD practice. We phrase our problem as search in the space of such\nextrusions permitted by the zone graph, and we train a graph neural network to\nscore potential extrusions in order to accelerate the search. We show that our\napproach outperforms an existing CSG inference baseline in terms of geometric\nreconstruction accuracy and reconstruction time, while also creating more\nplausible modeling sequences.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:55:59 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 19:24:39 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xu", "Xianghao", ""], ["Peng", "Wenzhe", ""], ["Cheng", "Chin-Yi", ""], ["Willis", "Karl D. D.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2104.03916", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Vladimir G. Kim, Michael Kazhdan", "title": "Field Convolutions for Surface CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel surface convolution operator acting on vector fields that\nis based on a simple observation: instead of combining neighboring features\nwith respect to a single coordinate parameterization defined at a given point,\nwe have every neighbor describe the position of the point within its own\ncoordinate frame. This formulation combines intrinsic spatial convolution with\nparallel transport in a scattering operation while placing no constraints on\nthe filters themselves, providing a definition of convolution that commutes\nwith the action of isometries, has increased descriptive potential, and is\nrobust to noise and other nuisance factors. The result is a rich notion of\nconvolution which we call field convolution, well-suited for CNNs on surfaces.\nField convolutions are flexible and straight-forward to implement, and their\nhighly discriminating nature has cascading effects throughout the learning\npipeline. Using simple networks constructed from residual field convolution\nblocks, we achieve state-of-the-art results on standard benchmarks in\nfundamental geometry processing tasks, such as shape classification,\nsegmentation, correspondence, and sparse matching.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:11:14 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Kim", "Vladimir G.", ""], ["Kazhdan", "Michael", ""]]}, {"id": "2104.03954", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu and Ameesh Makadia and Jiajun Wu and Noah Snavely and\n  Richard Tucker and Angjoo Kanazawa", "title": "De-rendering the World's Revolutionary Artefacts", "comments": "CVPR 2021. Project page: https://sorderender.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works have shown exciting results in unsupervised image de-rendering\n-- learning to decompose 3D shape, appearance, and lighting from single-image\ncollections without explicit supervision. However, many of these assume\nsimplistic material and lighting models. We propose a method, termed RADAR,\nthat can recover environment illumination and surface materials from real\nsingle-image collections, relying neither on explicit 3D supervision, nor on\nmulti-view or multi-light images. Specifically, we focus on rotationally\nsymmetric artefacts that exhibit challenging surface properties including\nspecular reflections, such as vases. We introduce a novel self-supervised\nalbedo discriminator, which allows the model to recover plausible albedo\nwithout requiring any ground-truth during training. In conjunction with a shape\nreconstruction module exploiting rotational symmetry, we present an end-to-end\nlearning framework that is able to de-render the world's revolutionary\nartefacts. We conduct experiments on a real vase dataset and demonstrate\ncompelling decomposition results, allowing for applications including\nfree-viewpoint rendering and relighting.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:56:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wu", "Shangzhe", ""], ["Makadia", "Ameesh", ""], ["Wu", "Jiajun", ""], ["Snavely", "Noah", ""], ["Tucker", "Richard", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2104.03960", "submitter": "Ishit Mehta", "authors": "Ishit Mehta, Micha\\\"el Gharbi, Connelly Barnes, Eli Shechtman, Ravi\n  Ramamoorthi, Manmohan Chandraker", "title": "Modulated Periodic Activations for Generalizable Local Functional\n  Representations", "comments": "Project Page at https://ishit.github.io/modsine/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-Layer Perceptrons (MLPs) make powerful functional representations for\nsampling and reconstruction problems involving low-dimensional signals like\nimages,shapes and light fields. Recent works have significantly improved their\nability to represent high-frequency content by using periodic activations or\npositional encodings. This often came at the expense of generalization: modern\nmethods are typically optimized for a single signal. We present a new\nrepresentation that generalizes to multiple instances and achieves\nstate-of-the-art fidelity. We use a dual-MLP architecture to encode the\nsignals. A synthesis network creates a functional mapping from a\nlow-dimensional input (e.g. pixel-position) to the output domain (e.g. RGB\ncolor). A modulation network maps a latent code corresponding to the target\nsignal to parameters that modulate the periodic activations of the synthesis\nnetwork. We also propose a local-functional representation which enables\ngeneralization. The signal's domain is partitioned into a regular grid,with\neach tile represented by a latent code. At test time, the signal is encoded\nwith high-fidelity by inferring (or directly optimizing) the latent code-book.\nOur approach produces generalizable functional representations of images,\nvideos and shapes, and achieves higher reconstruction quality than prior works\nthat are optimized for a single signal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:59:04 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mehta", "Ishit", ""], ["Gharbi", "Micha\u00ebl", ""], ["Barnes", "Connelly", ""], ["Shechtman", "Eli", ""], ["Ramamoorthi", "Ravi", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "2104.03978", "submitter": "Andrei Burov", "authors": "Andrei Burov and Matthias Nie{\\ss}ner and Justus Thies", "title": "Dynamic Surface Function Networks for Clothed Human Bodies", "comments": "Video: https://youtu.be/4wbSi9Sqdm4 | Project page:\n  https://github.com/andreiburov/DSFN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for temporal coherent reconstruction and tracking\nof clothed humans. Given a monocular RGB-D sequence, we learn a person-specific\nbody model which is based on a dynamic surface function network. To this end,\nwe explicitly model the surface of the person using a multi-layer perceptron\n(MLP) which is embedded into the canonical space of the SMPL body model. With\nclassical forward rendering, the represented surface can be rasterized using\nthe topology of a template mesh. For each surface point of the template mesh,\nthe MLP is evaluated to predict the actual surface location. To handle\npose-dependent deformations, the MLP is conditioned on the SMPL pose\nparameters. We show that this surface representation as well as the pose\nparameters can be learned in a self-supervised fashion using the principle of\nanalysis-by-synthesis and differentiable rasterization. As a result, we are\nable to reconstruct a temporally coherent mesh sequence from the input data.\nThe underlying surface representation can be used to synthesize new animations\nof the reconstructed person including pose-dependent deformations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Burov", "Andrei", ""], ["Nie\u00dfner", "Matthias", ""], ["Thies", "Justus", ""]]}, {"id": "2104.03989", "submitter": "Jon Hasselgren", "authors": "Jon Hasselgren, Jacob Munkberg, Jaakko Lehtinen, Miika Aittala, Samuli\n  Laine", "title": "Appearance-Driven Automatic 3D Model Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a suite of techniques for jointly optimizing triangle meshes and\nshading models to match the appearance of reference scenes. This capability has\na number of uses, including appearance-preserving simplification of extremely\ncomplex assets, conversion between rendering systems, and even conversion\nbetween geometric scene representations.\n  We follow and extend the classic analysis-by-synthesis family of techniques:\nenabled by a highly efficient differentiable renderer and modern nonlinear\noptimization algorithms, our results are driven to minimize the image-space\ndifference to the target scene when rendered in similar viewing and lighting\nconditions. As the only signals driving the optimization are differences in\nrendered images, the approach is highly general and versatile: it easily\nsupports many different forward rendering models such as normal mapping,\nspatially-varying BRDFs, displacement mapping, etc. Supervision through images\nonly is also key to the ability to easily convert between rendering systems and\nscene representations.\n  We output triangle meshes with textured materials to ensure that the models\nrender efficiently on modern graphics hardware and benefit from, e.g.,\nhardware-accelerated rasterization, ray tracing, and filtered texture lookups.\nOur system is integrated in a small Python code base, and can be applied at\nhigh resolutions and on large models. We describe several use cases, including\nmesh decimation, level of detail generation, seamless mesh filtering and\napproximations of aggregate geometry.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hasselgren", "Jon", ""], ["Munkberg", "Jacob", ""], ["Lehtinen", "Jaakko", ""], ["Aittala", "Miika", ""], ["Laine", "Samuli", ""]]}, {"id": "2104.04055", "submitter": "Gaurav Bharaj", "authors": "David Ferman, Gaurav Bharaj", "title": "Generative Landmarks", "comments": "2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose approach to detect landmarks with improved\ntemporal consistency, and personalization. Most sparse landmark detection\nmethods rely on laborious, manually labelled landmarks, where inconsistency in\nannotations over a temporal volume leads to sub-optimal landmark learning.\nFurther, high-quality landmarks with personalization is often hard to achieve.\nWe pose landmark detection as an image translation problem. We capture two sets\nof unpaired marked (with paint) and unmarked videos. We then use a generative\nadversarial network and cyclic consistency to predict deformations of landmark\ntemplates that simulate markers on unmarked images until these images are\nindistinguishable from ground-truth marked images. Our novel method does not\nrely on manually labelled priors, is temporally consistent, and image class\nagnostic -- face, and hand landmarks detection examples are shown.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:59:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ferman", "David", ""], ["Bharaj", "Gaurav", ""]]}, {"id": "2104.04387", "submitter": "Cagatay Basdogan", "authors": "Igor Peterlik, Mert Sedef, Cagatay Basdogan, Ludek Matyska", "title": "Real-time visio-haptic interaction with static soft tissue model shaving\n  geometric and material nonlinearity", "comments": null, "journal-ref": "Computers and Graphics, 2010, Vol. 34, No.1, pp. 43-54", "doi": "10.1016/j.cag.2009.10.005", "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach allowing visio-haptic interaction with a FE model\nof a human liver having both non-linear geometric and material properties. The\nmaterial properties used in the model are extracted from the experimental data\nof pig liver to make the simulations more realistic. Our computational approach\nconsists of two main steps: a pre-computation of the configuration space of all\npossible deformation states of the model, followed by the interpolation of the\nprecomputed data for the calculation of the reaction forces displayed to the\nuser through a haptic device during the real-time interactions. No a priori\nassumptions or modeling simplifications about the mathematical complexity of\nthe underlying soft tissue model, size and irregularity of the FE mesh are\nnecessary. We show that deformation and force response of the liver in\nsimulations are heavily influenced by the material model, boundary conditions,\npath of the loading and the type of function used for the interpolation of the\npre-computed data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:21:06 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Peterlik", "Igor", ""], ["Sedef", "Mert", ""], ["Basdogan", "Cagatay", ""], ["Matyska", "Ludek", ""]]}, {"id": "2104.04523", "submitter": "Matthew Berger", "authors": "Yuzhe Lu, Kairong Jiang, Joshua A. Levine, and Matthew Berger", "title": "Compressive Neural Representations of Volumetric Scalar Fields", "comments": "EuroVis 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach for compressing volumetric scalar fields using\nimplicit neural representations. Our approach represents a scalar field as a\nlearned function, wherein a neural network maps a point in the domain to an\noutput scalar value. By setting the number of weights of the neural network to\nbe smaller than the input size, we achieve compressed representations of scalar\nfields, thus framing compression as a type of function approximation. Combined\nwith carefully quantizing network weights, we show that this approach yields\nhighly compact representations that outperform state-of-the-art volume\ncompression approaches. The conceptual simplicity of our approach enables a\nnumber of benefits, such as support for time-varying scalar fields, optimizing\nto preserve spatial gradients, and random-access field evaluation. We study the\nimpact of network design choices on compression performance, highlighting how\nsimple network architectures are effective for a broad range of volumes.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 15:24:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lu", "Yuzhe", ""], ["Jiang", "Kairong", ""], ["Levine", "Joshua A.", ""], ["Berger", "Matthew", ""]]}, {"id": "2104.04614", "submitter": "Marcel Campen", "authors": "Marcel Campen, Ryan Capouellez, Hanxiao Shen, Leyi Zhu, Daniele\n  Panozzo, Denis Zorin", "title": "Efficient and Robust Discrete Conformal Equivalence with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an efficient algorithm to compute a conformally equivalent metric\nfor a discrete surface, possibly with boundary, exhibiting prescribed Gaussian\ncurvature at all interior vertices and prescribed geodesic curvature along the\nboundary. Our construction is based on the theory developed in [Gu et al. 2018;\nSpringborn 2020], and in particular relies on results on hyperbolic Delaunay\ntriangulations. Generality is achieved by considering the surface's intrinsic\ntriangulation as a degree of freedom, and particular attention is paid to the\nproper treatment of surface boundaries. While via a double cover approach the\nboundary case can be reduced to the closed case quite naturally, the implied\nsymmetry of the setting causes additional challenges related to stable\nDelaunay-critical configurations that we address explicitly in this work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:32:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Campen", "Marcel", ""], ["Capouellez", "Ryan", ""], ["Shen", "Hanxiao", ""], ["Zhu", "Leyi", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""]]}, {"id": "2104.04934", "submitter": "Damien Rohmer", "authors": "Damien Rohmer, Marco Tarini, Niranjan Kalyanasundaram, Faezeh\n  Moshfeghifar, Marie-Paule Cani, Victor Zordan", "title": "Velocity Skinning for Real-time Stylized Skeletal Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secondary animation effects are essential for liveliness. We propose a\nsimple, real-time solution for adding them on top of standard skinning,\nenabling artist-driven stylization of skeletal motion. Our method takes a\nstandard skeleton animation as input, along with a skin mesh and rig weights.\nIt then derives per-vertex deformations from the different linear and angular\nvelocities along the skeletal hierarchy. We highlight two specific applications\nof this general framework, namely the cartoonlike \"squashy\" and \"floppy\"\neffects, achieved from specific combinations of velocity terms. As our results\nshow, combining these effects enables to mimic, enhance and stylize\nphysical-looking behaviours within a standard animation pipeline, for arbitrary\nskinned characters. Interactive on CPU, our method allows for GPU\nimplementation, yielding real-time performances even on large meshes. Animator\ncontrol is supported through a simple interface toolkit, enabling to refine the\ndesired type and magnitude of deformation at relevant vertices by simply\npainting weights. The resulting rigged character automatically responds to new\nskeletal animation, without further input.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 06:49:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rohmer", "Damien", ""], ["Tarini", "Marco", ""], ["Kalyanasundaram", "Niranjan", ""], ["Moshfeghifar", "Faezeh", ""], ["Cani", "Marie-Paule", ""], ["Zordan", "Victor", ""]]}, {"id": "2104.05052", "submitter": "Wenzhong Yan", "authors": "Wenzhong Yan, Dawei Zhao, Ankur Mehta", "title": "Fabrication-aware Design for Furniture with Planar Pieces", "comments": "21 pages, 14 figures, submitted to Robotica", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.RO physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational design tool to enable casual end-users to easily\ndesign, fabricate, and assemble flat-pack furniture with guaranteed\nmanufacturability. Using our system, users select parameterized components from\na library and constrain their dimensions. Then they abstractly specify\nconnections among components to define the furniture. Once fabrication\nspecifications (e.g. materials) designated, the mechanical implementation of\nthe furniture is automatically handled by leveraging encoded domain expertise.\nAfterwards, the system outputs 3D models for visualization and mechanical\ndrawings for fabrication. We demonstrate the validity of our approach by\ndesigning, fabricating, and assembling a variety of flat-pack (scaled)\nfurniture on demand.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:57:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yan", "Wenzhong", ""], ["Zhao", "Dawei", ""], ["Mehta", "Ankur", ""]]}, {"id": "2104.05281", "submitter": "Marco Attene PhD", "authors": "Marco Attene", "title": "Shapes In A Box -- Disassembling 3D objects for efficient packing and\n  fabrication", "comments": null, "journal-ref": "Computer Graphics Forum, 2015", "doi": "10.1111/cgf.12608", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern 3D printing technologies and the upcoming mass-customization paradigm\ncall for efficient methods to produce and distribute arbitrarily-shaped 3D\nobjects. This paper introduces an original algorithm to split a 3D model in\nparts that can be efficiently packed within a box, with the objective of\nreassembling them after delivery. The first step consists in the creation of a\nhierarchy of possible parts that can be tightly packed within their minimum\nbounding boxes. In a second step, the hierarchy is exploited to extract the\n(single) segmentation whose parts can be most tightly packed. The fact that\nshape packing is an NP-complete problem justifies the use of heuristics and\napproximated solutions whose efficacy and efficiency must be assessed.\nExtensive experimentation demonstrates that our algorithm produces satisfactory\nresults for arbitrarily-shaped objects while being comparable to ad-hoc methods\nwhen specific shapes are considered.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:23:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Attene", "Marco", ""]]}, {"id": "2104.05652", "submitter": "Fenggen Yu", "authors": "Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani, Ali\n  Mahdavi-Amiri and Hao Zhang", "title": "CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CAPRI-Net, a neural network for learning compact and\ninterpretable implicit representations of 3D computer-aided design (CAD)\nmodels, in the form of adaptive primitive assemblies. Our network takes an\ninput 3D shape that can be provided as a point cloud or voxel grids, and\nreconstructs it by a compact assembly of quadric surface primitives via\nconstructive solid geometry (CSG) operations. The network is self-supervised\nwith a reconstruction loss, leading to faithful 3D reconstructions with sharp\nedges and plausible CSG trees, without any ground-truth shape assemblies. While\nthe parametric nature of CAD models does make them more predictable locally, at\nthe shape level, there is a great deal of structural and topological\nvariations, which present a significant generalizability challenge to\nstate-of-the-art neural models for 3D shapes. Our network addresses this\nchallenge by adaptive training with respect to each test shape, with which we\nfine-tune the network that was pre-trained on a model collection. We evaluate\nour learning framework on both ShapeNet and ABC, the largest and most diverse\nCAD dataset to date, in terms of reconstruction quality, shape edges,\ncompactness, and interpretability, to demonstrate superiority over current\nalternatives suitable for neural CAD reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:21:19 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yu", "Fenggen", ""], ["Chen", "Zhiqin", ""], ["Li", "Manyi", ""], ["Sanghi", "Aditya", ""], ["Shayani", "Hooman", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "2104.05988", "submitter": "Marcel B\\\"uhler", "authors": "Marcel C. B\\\"uhler (1), Abhimitra Meka (2), Gengyan Li (1 and 2),\n  Thabo Beeler (2), Otmar Hilliges (1) ((1) ETH Zurich, (2) Google)", "title": "VariTex: Variational Neural Face Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:24:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["B\u00fchler", "Marcel C.", "", "ETH Zurich"], ["Meka", "Abhimitra", "", "Google"], ["Li", "Gengyan", "", "1 and 2"], ["Beeler", "Thabo", "", "Google"], ["Hilliges", "Otmar", "", "ETH Zurich"]]}, {"id": "2104.06316", "submitter": "Tarik A. Rashid", "authors": "Arjina Maharjan, Abeer Alsadoon, P.W.C. Prasad, Nada AlSallami, Tarik\n  A. Rashid, Ahmad Alrubaie, Sami Haddad", "title": "A Novel Solution of Using Mixed Reality in Bowel and Oral and\n  Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm", "comments": "27 pages", "journal-ref": "International Journal of Medical Robotics and Computer Assisted\n  Surgery,2020", "doi": "10.1002/rcs.2161", "report-no": null, "categories": "physics.med-ph cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background and aim: Most of the Mixed Reality models used in the surgical\ntelepresence are suffering from discrepancies in the boundary area and\nspatial-temporal inconsistency due to the illumination variation in the video\nframes. The aim behind this work is to propose a new solution that helps\nproduce the composite video by merging the augmented video of the surgery site\nand the virtual hand of the remote expertise surgeon. The purpose of the\nproposed solution is to decrease the processing time and enhance the accuracy\nof merged video by decreasing the overlay and visualization error and removing\nocclusion and artefacts. Methodology: The proposed system enhanced the mean\nvalue cloning algorithm that helps to maintain the spatial-temporal consistency\nof the final composite video. The enhanced algorithm includes the 3D mean value\ncoordinates and improvised mean value interpolant in the image cloning process,\nwhich helps to reduce the sawtooth, smudging and discolouration artefacts\naround the blending region. Results: As compared to the state of the art\nsolution, the accuracy in terms of overlay error of the proposed solution is\nimproved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization\nerror is improved from 98.8% to 99.4%. The processing time is reduced to 0.173\nseconds from 0.211 seconds. Conclusion: Our solution helps make the object of\ninterest consistent with the light intensity of the target image by adding the\nspace distance that helps maintain the spatial consistency in the final merged\nvideo.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:01:06 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maharjan", "Arjina", ""], ["Alsadoon", "Abeer", ""], ["Prasad", "P. W. C.", ""], ["AlSallami", "Nada", ""], ["Rashid", "Tarik A.", ""], ["Alrubaie", "Ahmad", ""], ["Haddad", "Sami", ""]]}, {"id": "2104.06392", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones, David Charatan, Paul Guerrero, Niloy J. Mitra, Daniel\n  Ritchie", "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs", "comments": "SIGGRAPH 2021. Project Page: https://rkjones4.github.io/shapeMOD.html", "journal-ref": null, "doi": "10.1145/3450626.3459821", "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:54:03 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:56:17 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jones", "R. Kenny", ""], ["Charatan", "David", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2104.06405", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, Simon Lucey", "title": "BARF: Bundle-Adjusting Neural Radiance Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural Radiance Fields (NeRF) have recently gained a surge of interest within\nthe computer vision community for its power to synthesize photorealistic novel\nviews of real-world scenes. One limitation of NeRF, however, is its requirement\nof accurate camera poses to learn the scene representations. In this paper, we\npropose Bundle-Adjusting Neural Radiance Fields (BARF) for training NeRF from\nimperfect (or even unknown) camera poses -- the joint problem of learning\nneural 3D representations and registering camera frames. We establish a\ntheoretical connection to classical image alignment and show that\ncoarse-to-fine registration is also applicable to NeRF. Furthermore, we show\nthat na\\\"ively applying positional encoding in NeRF has a negative impact on\nregistration with a synthesis-based objective. Experiments on synthetic and\nreal-world data show that BARF can effectively optimize the neural scene\nrepresentations and resolve large camera pose misalignment at the same time.\nThis enables view synthesis and localization of video sequences from unknown\ncamera poses, opening up new avenues for visual localization systems (e.g.\nSLAM) and potential applications for dense 3D mapping and reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Ma", "Wei-Chiu", ""], ["Torralba", "Antonio", ""], ["Lucey", "Simon", ""]]}, {"id": "2104.06820", "submitter": "Utkarsh Ojha", "authors": "Utkarsh Ojha, Yijun Li, Jingwan Lu, Alexei A. Efros, Yong Jae Lee, Eli\n  Shechtman, Richard Zhang", "title": "Few-shot Image Generation via Cross-domain Correspondence", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative models, such as GANs, on a target domain containing\nlimited examples (e.g., 10) can easily result in overfitting. In this work, we\nseek to utilize a large source domain for pretraining and transfer the\ndiversity information from source to target. We propose to preserve the\nrelative similarities and differences between instances in the source via a\nnovel cross-domain distance consistency loss. To further reduce overfitting, we\npresent an anchor-based strategy to encourage different levels of realism over\ndifferent regions in the latent space. With extensive results in both\nphotorealistic and non-photorealistic domains, we demonstrate qualitatively and\nquantitatively that our few-shot model automatically discovers correspondences\nbetween source and target domains and generates more diverse and realistic\nimages than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:59:35 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ojha", "Utkarsh", ""], ["Li", "Yijun", ""], ["Lu", "Jingwan", ""], ["Efros", "Alexei A.", ""], ["Lee", "Yong Jae", ""], ["Shechtman", "Eli", ""], ["Zhang", "Richard", ""]]}, {"id": "2104.07526", "submitter": "Markus Sch\\\"utz", "authors": "Markus Sch\\\"utz, Bernhard Kerbl, Michael Wimmer", "title": "Rendering Point Clouds with Compute Shaders and Vertex Order\n  Optimization", "comments": "13 pages content, 5 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While commodity GPUs provide a continuously growing range of features and\nsophisticated methods for accelerating compute jobs, many state-of-the-art\nsolutions for point cloud rendering still rely on the provided point primitives\n(GL_POINTS, POINTLIST, ...) of graphics APIs for image synthesis. In this\npaper, we present several compute-based point cloud rendering approaches that\noutperform the hardware pipeline by up to an order of magnitude and achieve\nsignificantly better frame times than previous compute-based methods. Beyond\nbasic closest-point rendering, we also introduce a fast, high-quality variant\nto reduce aliasing. We present and evaluate several variants of our proposed\nmethods with different flavors of optimization, in order to ensure their\napplicability and achieve optimal performance on a range of platforms and\narchitectures with varying support for novel GPU hardware features. During our\nexperiments, the observed peak performance was reached rendering 796 million\npoints (12.7GB) at rates of 62 to 64 frames per second (50 billion points per\nsecond, 802GB/s) on an RTX 3090 without the use of level-of-detail structures.\n  We further introduce an optimized vertex order for point clouds to boost the\nefficiency of GL_POINTS by a factor of 5x in cases where hardware rendering is\ncompulsory. We compare different orderings and show that Morton sorted buffers\nare faster for some viewpoints, while shuffled vertex buffers are faster in\nothers. In contrast, combining both approaches by first sorting according to\nMorton-code and shuffling the resulting sequence in batches of 128 points leads\nto a vertex buffer layout with high rendering performance and low sensitivity\nto viewpoint changes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:26:07 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sch\u00fctz", "Markus", ""], ["Kerbl", "Bernhard", ""], ["Wimmer", "Michael", ""]]}, {"id": "2104.07660", "submitter": "Qianli Ma", "authors": "Qianli Ma, Shunsuke Saito, Jinlong Yang, Siyu Tang, Michael J. Black", "title": "SCALE: Modeling Clothed Humans with a Surface Codec of Articulated Local\n  Elements", "comments": "In CVPR 2021. Project page: https://qianlim.github.io/SCALE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to model and reconstruct humans in clothing is challenging due to\narticulation, non-rigid deformation, and varying clothing types and topologies.\nTo enable learning, the choice of representation is the key. Recent work uses\nneural networks to parameterize local surface elements. This approach captures\nlocally coherent geometry and non-planar details, can deal with varying\ntopology, and does not require registered training data. However, naively using\nsuch methods to model 3D clothed humans fails to capture fine-grained local\ndeformations and generalizes poorly. To address this, we present three key\ninnovations: First, we deform surface elements based on a human body model such\nthat large-scale deformations caused by articulation are explicitly separated\nfrom topological changes and local clothing deformations. Second, we address\nthe limitations of existing neural surface elements by regressing local\ngeometry from local features, significantly improving the expressiveness.\nThird, we learn a pose embedding on a 2D parameterization space that encodes\nposed body geometry, improving generalization to unseen poses by reducing\nnon-local spurious correlations. We demonstrate the efficacy of our surface\nrepresentation by learning models of complex clothing from point clouds. The\nclothing can change topology and deviate from the topology of the body. Once\nlearned, we can animate previously unseen motions, producing high-quality point\nclouds, from which we generate realistic images with neural rendering. We\nassess the importance of each technical contribution and show that our approach\noutperforms the state-of-the-art methods in terms of reconstruction accuracy\nand inference time. The code is available for research purposes at\nhttps://qianlim.github.io/SCALE .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:39 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ma", "Qianli", ""], ["Saito", "Shunsuke", ""], ["Yang", "Jinlong", ""], ["Tang", "Siyu", ""], ["Black", "Michael J.", ""]]}, {"id": "2104.07661", "submitter": "Dongdong Chen", "authors": "Tianyi Wei and Dongdong Chen and Wenbo Zhou and Jing Liao and Weiming\n  Zhang and Lu Yuan and Gang Hua and Nenghai Yu", "title": "A Simple Baseline for StyleGAN Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of StyleGAN inversion, which plays an\nessential role in enabling the pretrained StyleGAN to be used for real facial\nimage editing tasks. This problem has the high demand for quality and\nefficiency. Existing optimization-based methods can produce high quality\nresults, but the optimization often takes a long time. On the contrary,\nforward-based methods are usually faster but the quality of their results is\ninferior. In this paper, we present a new feed-forward network for StyleGAN\ninversion, with significant improvement in terms of efficiency and quality. In\nour inversion network, we introduce: 1) a shallower backbone with multiple\nefficient heads across scales; 2) multi-layer identity loss and multi-layer\nface parsing loss to the loss function; and 3) multi-stage refinement.\nCombining these designs together forms a simple and efficient baseline method\nwhich exploits all benefits of optimization-based and forward-based methods.\nQuantitative and qualitative results show that our method performs better than\nexisting forward-based methods and comparably to state-of-the-art\noptimization-based methods, while maintaining the high efficiency as well as\nforward-based methods. Moreover, a number of real image editing applications\ndemonstrate the efficacy of our method. Our project page is\n~\\url{https://wty-ustc.github.io/inversion}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:49 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wei", "Tianyi", ""], ["Chen", "Dongdong", ""], ["Zhou", "Wenbo", ""], ["Liao", "Jing", ""], ["Zhang", "Weiming", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2104.08016", "submitter": "Stuart Lee", "authors": "Stuart Lee, Dianne Cook, Natalia da Silva, Ursula Laa, Earo Wang, Nick\n  Spyrison, H. Sherry Zhang", "title": "A Review of the State-of-the-Art on Tours for Dynamic Visualization of\n  High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article discusses a high-dimensional visualization technique called the\ntour, which can be used to view data in more than three dimensions. We review\nthe theory and history behind the technique, as well as modern software\ndevelopments and applications of the tour that are being found across the\nsciences and machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:24:04 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 03:14:51 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lee", "Stuart", ""], ["Cook", "Dianne", ""], ["da Silva", "Natalia", ""], ["Laa", "Ursula", ""], ["Wang", "Earo", ""], ["Spyrison", "Nick", ""], ["Zhang", "H. Sherry", ""]]}, {"id": "2104.08057", "submitter": "Pierre-Alain Fayolle", "authors": "Pierre-Alain Fayolle", "title": "Signed Distance Function Computation from an Implicit Surface", "comments": "Fix typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe in this short note a technique to convert an implicit surface\ninto a Signed Distance Function (SDF) while exactly preserving the zero\nlevel-set of the implicit. The proposed approach relies on embedding the input\nimplicit in the final layer of a neural network, which is trained to minimize a\nloss function characterizing the SDF.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:06:53 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 00:00:58 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fayolle", "Pierre-Alain", ""]]}, {"id": "2104.09125", "submitter": "Amir Hertz", "authors": "Amir Hertz, Or Perel, Raja Giryes, Olga Sorkine-Hornung and Daniel\n  Cohen-Or", "title": "SAPE: Spatially-Adaptive Progressive Encoding for Neural Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multilayer-perceptrons (MLP) are known to struggle with learning functions of\nhigh-frequencies, and in particular cases with wide frequency bands. We present\na spatially adaptive progressive encoding (SAPE) scheme for input signals of\nMLP networks, which enables them to better fit a wide range of frequencies\nwithout sacrificing training stability or requiring any domain specific\npreprocessing. SAPE gradually unmasks signal components with increasing\nfrequencies as a function of time and space. The progressive exposure of\nfrequencies is monitored by a feedback loop throughout the neural optimization\nprocess, allowing changes to propagate at different rates among local spatial\nportions of the signal space. We demonstrate the advantage of SAPE on a variety\nof domains and applications, including regression of low dimensional signals\nand images, representation learning of occupancy networks, and a geometric task\nof mesh transfer between 3D shapes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:55 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 15:46:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Hertz", "Amir", ""], ["Perel", "Or", ""], ["Giryes", "Raja", ""], ["Sorkine-Hornung", "Olga", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2104.09621", "submitter": "Karl Willis", "authors": "Karl D.D. Willis, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Hang\n  Chu, Yewen Pu", "title": "Engineering Sketch Generation for Computer-Aided Design", "comments": "The 1st Workshop on Sketch-Oriented Deep Learning @ CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering sketches form the 2D basis of parametric Computer-Aided Design\n(CAD), the foremost modeling paradigm for manufactured objects. In this paper\nwe tackle the problem of learning based engineering sketch generation as a\nfirst step towards synthesis and composition of parametric CAD models. We\npropose two generative models, CurveGen and TurtleGen, for engineering sketch\ngeneration. Both models generate curve primitives without the need for a sketch\nconstraint solver and explicitly consider topology for downstream use with\nconstraints and 3D CAD modeling operations. We find in our perceptual\nevaluation using human subjects that both CurveGen and TurtleGen produce more\nrealistic engineering sketches when compared with the current state-of-the-art\nfor engineering sketch generation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:38:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Willis", "Karl D. D.", ""], ["Jayaraman", "Pradeep Kumar", ""], ["Lambourne", "Joseph G.", ""], ["Chu", "Hang", ""], ["Pu", "Yewen", ""]]}, {"id": "2104.10203", "submitter": "Hang Zhou", "authors": "Hang Zhou, Weiming Zhang, Kejiang Chen, Weixiang Li and Nenghai Yu", "title": "Three-Dimensional Mesh Steganography and Steganalysis: A Review", "comments": "Accepted to TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Three-dimensional (3-D) meshes are commonly used to represent virtual\nsurfaces and volumes. Over the past decade, 3-D meshes have emerged in\nindustrial, medical, and entertainment applications, being of large practical\nsignificance for 3-D mesh steganography and steganalysis. In this article, we\nprovide a systematic survey of the literature on 3-D mesh steganography and\nsteganalysis. Compared with an earlier survey [1], we propose a new taxonomy of\nsteganographic algorithms with four categories: 1) two-state domain, 2) LSB\ndomain, 3) permutation domain, and 4) transform domain. Regarding steganalysis\nalgorithms, we divide them into two categories: 1) universal steganalysis and\n2) specific steganalysis. For each category, the history of technical\ndevelopments and the current technological level are introduced and discussed.\nFinally, we highlight some promising future research directions and challenges\nin improving the performance of 3-D mesh steganography and steganalysis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:46:56 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Hang", ""], ["Zhang", "Weiming", ""], ["Chen", "Kejiang", ""], ["Li", "Weixiang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2104.10299", "submitter": "Cho-Ying Wu", "authors": "Cho-Ying Wu, Ke Xu, Chin-Cheng Hsu, Ulrich Neumann", "title": "Voice2Mesh: Cross-Modal 3D Face Model Generation from Voices", "comments": "Project page: https://choyingw.github.io/works/Voice2Mesh/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the analysis that whether 3D face models can be learned\nfrom only the speech inputs of speakers. Previous works for cross-modal face\nsynthesis study image generation from voices. However, image synthesis includes\nvariations such as hairstyles, backgrounds, and facial textures, that are\narguably irrelevant to voice or without direct studies to show correlations. We\ninstead investigate the ability to reconstruct 3D faces to concentrate on only\ngeometry, which is more physiologically grounded. We propose both the\nsupervised learning and unsupervised learning frameworks. Especially we\ndemonstrate how unsupervised learning is possible in the absence of a direct\nvoice-to-3D-face dataset under limited availability of 3D face scans when the\nmodel is equipped with knowledge distillation. To evaluate the performance, we\nalso propose several metrics to measure the geometric fitness of two 3D faces\nbased on points, lines, and regions. We find that 3D face shapes can be\nreconstructed from voices. Experimental results suggest that 3D faces can be\nreconstructed from voices, and our method can improve the performance over the\nbaseline. The best performance gains (15% - 20%) on ear-to-ear distance ratio\nmetric (ER) coincides with the intuition that one can roughly envision whether\na speaker's face is overall wider or thinner only from a person's voice. See\nour project page for codes and data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 01:14:50 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Xu", "Ke", ""], ["Hsu", "Chin-Cheng", ""], ["Neumann", "Ulrich", ""]]}, {"id": "2104.10369", "submitter": "Jun Zhou", "authors": "Jun Zhou, Wei Jin, Mingjie Wang, Xiuping Liu, Zhiyang Li, Zhaobin Liu", "title": "Improvement of Normal Estimation for PointClouds via Simplifying Surface\n  Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the burst development of neural networks in recent years, the task of\nnormal estimation has once again become a concern. By introducing the neural\nnetworks to classic methods based on problem-specific knowledge, the\nadaptability of the normal estimation algorithm to noise and scale has been\ngreatly improved. However, the compatibility between neural networks and the\ntraditional methods has not been considered. Similar to the principle of\nOccam's razor, that is, the simpler is better. We observe that a more\nsimplified process of surface fitting can significantly improve the accuracy of\nthe normal estimation. In this paper, two simple-yet-effective strategies are\nproposed to address the compatibility between the neural networks and surface\nfitting process to improve normal estimation. Firstly, a dynamic top-k\nselection strategy is introduced to better focus on the most critical points of\na given patch, and the points selected by our learning method tend to fit a\nsurface by way of a simple tangent plane, which can dramatically improve the\nnormal estimation results of patches with sharp corners or complex patterns.\nThen, we propose a point update strategy before local surface fitting, which\nsmooths the sharp boundary of the patch to simplify the surface fitting\nprocess, significantly reducing the fitting distortion and improving the\naccuracy of the predicted point normal. The experiments analyze the\neffectiveness of our proposed strategies and demonstrate that our method\nachieves SOTA results with the advantage of higher estimation accuracy over\nmost existed approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:13:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Jun", ""], ["Jin", "Wei", ""], ["Wang", "Mingjie", ""], ["Liu", "Xiuping", ""], ["Li", "Zhiyang", ""], ["Liu", "Zhaobin", ""]]}, {"id": "2104.10622", "submitter": "Chenlei Lv", "authors": "Chenlei Lv, Weisi Lin, Baoquan Zhao", "title": "Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3073265", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh reconstruction from a 3D point cloud is an important topic in the fields\nof computer graphic, computer vision, and multimedia analysis. In this paper,\nwe propose a voxel structure-based mesh reconstruction framework. It provides\nthe intrinsic metric to improve the accuracy of local region detection. Based\non the detected local regions, an initial reconstructed mesh can be obtained.\nWith the mesh optimization in our framework, the initial reconstructed mesh is\noptimized into an isotropic one with the important geometric features such as\nexternal and internal edges. The experimental results indicate that our\nframework shows great advantages over peer ones in terms of mesh quality,\ngeometric feature keeping, and processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:31:49 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 12:50:41 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 16:55:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lv", "Chenlei", ""], ["Lin", "Weisi", ""], ["Zhao", "Baoquan", ""]]}, {"id": "2104.10898", "submitter": "Eduardo Alvarado", "authors": "Chlo\\'e Paliard (LTCI), Eduardo Alvarado (LIX), Damien Rohmer (LIX),\n  Marie-Paule Cani (LIX)", "title": "Soft Walks: Real-Time, Two-Ways Interaction between a Character and\n  Loose Grounds", "comments": null, "journal-ref": "Eurographics (Short), May 2021, Vienna, Austria", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When walking on loose terrains, possibly covered with vegetation, the ground\nand grass should deform, but the character's gait should also change\naccordingly. We propose a method for modeling such two-ways interactions in\nreal-time. We first complement a layered character model by a high-level\ncontroller, which uses position and angular velocity inputs to improve dynamic\noscillations when walking on various slopes. Secondly, at a refined level, the\nfeet are set to locally deform the ground and surrounding vegetation using\nefficient procedural functions, while the character's response to such\ndeformations is computed through adapted inverse kinematics. While simple to\nset up, our method is generic enough to adapt to any character morphology.\nMoreover, its ability to generate in real time, consistent gaits on a variety\nof loose grounds of arbitrary slope, possibly covered with grass, makes it an\ninteresting solution to enhance films and games.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:07:52 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 08:13:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Paliard", "Chlo\u00e9", "", "LTCI"], ["Alvarado", "Eduardo", "", "LIX"], ["Rohmer", "Damien", "", "LIX"], ["Cani", "Marie-Paule", "", "LIX"]]}, {"id": "2104.11222", "submitter": "Gaurav Parmar", "authors": "Gaurav Parmar, Richard Zhang, Jun-Yan Zhu", "title": "On Buggy Resizing Libraries and Surprising Subtleties in FID Calculation", "comments": "GitHub: https://www.github.com/GaParmar/clean-fid Website:\n  https://www.cs.cmu.edu/~clean-fid/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sensitivity of the Fr\\'echet Inception Distance (FID)\nscore to inconsistent and often incorrect implementations across different\nimage processing libraries. FID score is widely used to evaluate generative\nmodels, but each FID implementation uses a different low-level image processing\nprocess. Image resizing functions in commonly-used deep learning libraries\noften introduce aliasing artifacts. We observe that numerous subtle choices\nneed to be made for FID calculation and a lack of consistencies in these\nchoices can lead to vastly different FID scores. In particular, we show that\nthe following choices are significant: (1) selecting what image resizing\nlibrary to use, (2) choosing what interpolation kernel to use, (3) what\nencoding to use when representing images. We additionally outline numerous\ncommon pitfalls that should be avoided and provide recommendations for\ncomputing the FID score accurately. We provide an easy-to-use optimized\nimplementation of our proposed recommendations in the accompanying code.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:58:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Parmar", "Gaurav", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "2104.11224", "submitter": "Tomas Jakab", "authors": "Tomas Jakab, Richard Tucker, Ameesh Makadia, Jiajun Wu, Noah Snavely,\n  Angjoo Kanazawa", "title": "KeypointDeformer: Unsupervised 3D Keypoint Discovery for Shape Control", "comments": "CVPR 2021 (oral). Project page:\n  http://tomasjakab.github.io/KeypointDeformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce KeypointDeformer, a novel unsupervised method for shape control\nthrough automatically discovered 3D keypoints. We cast this as the problem of\naligning a source 3D object to a target 3D object from the same object\ncategory. Our method analyzes the difference between the shapes of the two\nobjects by comparing their latent representations. This latent representation\nis in the form of 3D keypoints that are learned in an unsupervised way. The\ndifference between the 3D keypoints of the source and the target objects then\ninforms the shape deformation algorithm that deforms the source object into the\ntarget object. The whole model is learned end-to-end and simultaneously\ndiscovers 3D keypoints while learning to use them for deforming object shapes.\nOur approach produces intuitive and semantically consistent control of shape\ndeformations. Moreover, our discovered 3D keypoints are consistent across\nobject category instances despite large shape variations. As our method is\nunsupervised, it can be readily deployed to new object categories without\nrequiring annotations for 3D keypoints and deformations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jakab", "Tomas", ""], ["Tucker", "Richard", ""], ["Makadia", "Ameesh", ""], ["Wu", "Jiajun", ""], ["Snavely", "Noah", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2104.11228", "submitter": "Dongdong Chen", "authors": "Can Wang and Menglei Chai and Mingming He and Dongdong Chen and Jing\n  Liao", "title": "Cross-Domain and Disentangled Face Manipulation with 3D Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face image manipulation via three-dimensional guidance has been widely\napplied in various interactive scenarios due to its semantically-meaningful\nunderstanding and user-friendly controllability. However, existing\n3D-morphable-model-based manipulation methods are not directly applicable to\nout-of-domain faces, such as non-photorealistic paintings, cartoon portraits,\nor even animals, mainly due to the formidable difficulties in building the\nmodel for each specific face domain. To overcome this challenge, we propose, as\nfar as we know, the first method to manipulate faces in arbitrary domains using\nhuman 3DMM. This is achieved through two major steps: 1) disentangled mapping\nfrom 3DMM parameters to the latent space embedding of a pre-trained StyleGAN2\nthat guarantees disentangled and precise controls for each semantic attribute;\nand 2) cross-domain adaptation that bridges domain discrepancies and makes\nhuman 3DMM applicable to out-of-domain faces by enforcing a consistent latent\nspace embedding. Experiments and comparisons demonstrate the superiority of our\nhigh-quality semantic manipulation method on a variety of face domains with all\nmajor 3D facial attributes controllable: pose, expression, shape, albedo, and\nillumination. Moreover, we develop an intuitive editing interface to support\nuser-friendly control and instant feedback. Our project page is\nhttps://cassiepython.github.io/sigasia/cddfm3d.html.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Can", ""], ["Chai", "Menglei", ""], ["He", "Mingming", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""]]}, {"id": "2104.11554", "submitter": "Haoran Xie", "authors": "Yi He, Haoran Xie, Chao Zhang, Xi Yang, Kazunori Miyata", "title": "Sketch-based Normal Map Generation with Geometric Sampling", "comments": "accepted in International Workshop on Advanced Image Technology 2021,\n  5 pages, 2 figures", "journal-ref": null, "doi": "10.1117/12.2590760", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normal map is an important and efficient way to represent complex 3D models.\nA designer may benefit from the auto-generation of high quality and accurate\nnormal maps from freehand sketches in 3D content creation. This paper proposes\na deep generative model for generating normal maps from users sketch with\ngeometric sampling. Our generative model is based on Conditional Generative\nAdversarial Network with the curvature-sensitive points sampling of conditional\nmasks. This sampling process can help eliminate the ambiguity of generation\nresults as network input. In addition, we adopted a U-Net structure\ndiscriminator to help the generator be better trained. It is verified that the\nproposed framework can generate more accurate normal maps.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:30:22 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["He", "Yi", ""], ["Xie", "Haoran", ""], ["Zhang", "Chao", ""], ["Yang", "Xi", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2104.11776", "submitter": "Pablo Martinez-Gonzalez", "authors": "Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas,\n  Alberto Garcia-Garcia, Sergio Orts-Escolano, Jose Garcia-Rodriguez and Markus\n  Vincze", "title": "UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual\n  3D Environments", "comments": "Accepted at International Joint Conference on Neural Networks (IJCNN)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synthetic data generation has become essential in last years for feeding\ndata-driven algorithms, which surpassed traditional techniques performance in\nalmost every computer vision problem. Gathering and labelling the amount of\ndata needed for these data-hungry models in the real world may become\nunfeasible and error-prone, while synthetic data give us the possibility of\ngenerating huge amounts of data with pixel-perfect annotations. However, most\nsynthetic datasets lack from enough realism in their rendered images. In that\ncontext UnrealROX generation tool was presented in 2019, allowing to generate\nhighly realistic data, at high resolutions and framerates, with an efficient\npipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX\nenabled robotic vision researchers to generate realistic and visually plausible\ndata with full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation. Nevertheless, its workflow was very tied to generate\nimage sequences from a robotic on-board camera, making hard to generate data\nfor other purposes. In this work, we present UnrealROX+, an improved version of\nUnrealROX where its decoupled and easy-to-use data acquisition system allows to\nquickly design and generate data in a much more flexible and customizable way.\nMoreover, it is packaged as an Unreal plug-in, which makes it more comfortable\nto use with already existing Unreal projects, and it also includes new features\nsuch as generating albedo or a Python API for interacting with the virtual\nenvironment from Deep Learning frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:45:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Castro-Vargas", "John Alejandro", ""], ["Garcia-Garcia", "Alberto", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""], ["Vincze", "Markus", ""]]}, {"id": "2104.11867", "submitter": "Peng Xie", "authors": "Peng Xie, Wenyuan Tao, Jie Li, Wentao Huang, Siming Chen", "title": "Exploring Multi-dimensional Data via Subset Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional data exploration is a classic research topic in\nvisualization. Most existing approaches are designed for identifying record\npatterns in dimensional space or subspace. In this paper, we propose a visual\nanalytics approach to exploring subset patterns. The core of the approach is a\nsubset embedding network (SEN) that represents a group of subsets as\nuniformly-formatted embeddings. We implement the SEN as multiple subnets with\nseparate loss functions. The design enables to handle arbitrary subsets and\ncapture the similarity of subsets on single features, thus achieving accurate\npattern exploration, which in most cases is searching for subsets having\nsimilar values on few features. Moreover, each subnet is a fully-connected\nneural network with one hidden layer. The simple structure brings high training\nefficiency. We integrate the SEN into a visualization system that achieves a\n3-step workflow. Specifically, analysts (1) partition the given dataset into\nsubsets, (2) select portions in a projected latent space created using the SEN,\nand (3) determine the existence of patterns within selected subsets. Generally,\nthe system combines visualizations, interactions, automatic methods, and\nquantitative measures to balance the exploration flexibility and operation\nefficiency, and improve the interpretability and faithfulness of the identified\npatterns. Case studies and quantitative experiments on multiple open datasets\ndemonstrate the general applicability and effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 03:08:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Xie", "Peng", ""], ["Tao", "Wenyuan", ""], ["Li", "Jie", ""], ["Huang", "Wentao", ""], ["Chen", "Siming", ""]]}, {"id": "2104.11993", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu and Alec Jacobson", "title": "Normal-Driven Spherical Shape Analogies", "comments": "Eurographics Symposium on Geometry Processing", "journal-ref": "Computer Graphics Forum 2021", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method to stylize 3D geometry. The key\nobservation is that the surface normal is an effective instrument to capture\ndifferent geometric styles. Centered around this observation, we cast\nstylization as a shape analogy problem, where the analogy relationship is\ndefined on the surface normal. This formulation can deform a 3D shape into\ndifferent styles within a single framework. One can plug-and-play different\ntarget styles by providing an exemplar shape or an energy-based style\ndescription (e.g., developable surfaces). Our surface stylization methodology\nenables Normal Captures as a geometric counterpart to material captures\n(MatCaps) used in rendering, and the prototypical concept of Spherical Shape\nAnalogies as a geometric counterpart to image analogies in image processing.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 18:05:11 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 00:33:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "2104.12051", "submitter": "Wang Qianyun", "authors": "Qianyun Wang, Zhenfeng Fan, Shihong Xia", "title": "3D-TalkEmo: Learning to Synthesize 3D Emotional Talking Head", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive progress has been made in audio-driven 3D facial animation\nrecently, but synthesizing 3D talking-head with rich emotion is still unsolved.\nThis is due to the lack of 3D generative models and available 3D emotional\ndataset with synchronized audios. To address this, we introduce 3D-TalkEmo, a\ndeep neural network that generates 3D talking head animation with various\nemotions. We also create a large 3D dataset with synchronized audios and\nvideos, rich corpus, as well as various emotion states of different persons\nwith the sophisticated 3D face reconstruction methods. In the emotion\ngeneration network, we propose a novel 3D face representation structure -\ngeometry map by classical multi-dimensional scaling analysis. It maps the\ncoordinates of vertices on a 3D face to a canonical image plane, while\npreserving the vertex-to-vertex geodesic distance metric in a least-square\nsense. This maintains the adjacency relationship of each vertex and holds the\neffective convolutional structure for the 3D facial surface. Taking a neutral\n3D mesh and a speech signal as inputs, the 3D-TalkEmo is able to generate vivid\nfacial animations. Moreover, it provides access to change the emotion state of\nthe animated speaker.\n  We present extensive quantitative and qualitative evaluation of our method,\nin addition to user studies, demonstrating the generated talking-heads of\nsignificantly higher quality compared to previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 02:48:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wang", "Qianyun", ""], ["Fan", "Zhenfeng", ""], ["Xia", "Shihong", ""]]}, {"id": "2104.12297", "submitter": "Haoran Xie", "authors": "Zhengyu Huang, Yichen Peng, Tomohiro Hibino, Chunqi Zhao, Haoran Xie,\n  Tsukasa Fukusato, Kazunori Miyata", "title": "dualFace:Two-Stage Drawing Guidance for Freehand Portrait Sketching", "comments": "Accepted in the Journal of Computational Visual Media Conference\n  2021. 13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose dualFace, a portrait drawing interface to assist\nusers with different levels of drawing skills to complete recognizable and\nauthentic face sketches. dualFace consists of two-stage drawing assistance to\nprovide global and local visual guidance: global guidance, which helps users\ndraw contour lines of portraits (i.e., geometric structure), and local\nguidance, which helps users draws details of facial parts (which conform to\nuser-drawn contour lines), inspired by traditional artist workflows in portrait\ndrawing. In the stage of global guidance, the user draws several contour lines,\nand dualFace then searches several relevant images from an internal database\nand displays the suggested face contour lines over the background of the\ncanvas. In the stage of local guidance, we synthesize detailed portrait images\nwith a deep generative model from user-drawn contour lines, but use the\nsynthesized results as detailed drawing guidance. We conducted a user study to\nverify the effectiveness of dualFace, and we confirmed that dualFace\nsignificantly helps achieve a detailed portrait sketch. see\nhttp://www.jaist.ac.jp/~xie/dualface.html\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 00:56:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Zhengyu", ""], ["Peng", "Yichen", ""], ["Hibino", "Tomohiro", ""], ["Zhao", "Chunqi", ""], ["Xie", "Haoran", ""], ["Fukusato", "Tsukasa", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2104.12365", "submitter": "Zeshi Yang", "authors": "Zeshi Yang and Zhiqi Yin", "title": "Efficient Hyperparameter Optimization for Physics-based Character\n  Animation", "comments": "published in ACM SIGGRAPH Symposium on Interactive 3D Graphics and\n  Games 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physics-based character animation has seen significant advances in recent\nyears with the adoption of Deep Reinforcement Learning (DRL). However,\nDRL-based learning methods are usually computationally expensive and their\nperformance crucially depends on the choice of hyperparameters. Tuning\nhyperparameters for these methods often requires repetitive training of control\npolicies, which is even more computationally prohibitive. In this work, we\npropose a novel Curriculum-based Multi-Fidelity Bayesian Optimization framework\n(CMFBO) for efficient hyperparameter optimization of DRL-based character\ncontrol systems. Using curriculum-based task difficulty as fidelity criterion,\nour method improves searching efficiency by gradually pruning search space\nthrough evaluation on easier motor skill tasks. We evaluate our method on two\nphysics-based character control tasks: character morphology optimization and\nhyperparameter tuning of DeepMimic. Our algorithm significantly outperforms\nstate-of-the-art hyperparameter optimization methods applicable for\nphysics-based character animation. In particular, we show that hyperparameters\noptimized through our algorithm result in at least 5x efficiency gain comparing\nto author-released settings in DeepMimic.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 06:46:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yang", "Zeshi", ""], ["Yin", "Zhiqi", ""]]}, {"id": "2104.12826", "submitter": "Dmitriy Smirnov", "authors": "Dmitriy Smirnov and Justin Solomon", "title": "HodgeNet: Learning Spectral Geometry on Triangle Meshes", "comments": "Accepted to SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constrained by the limitations of learning toolkits engineered for other\napplications, such as those in image processing, many mesh-based learning\nalgorithms employ data flows that would be atypical from the perspective of\nconventional geometry processing. As an alternative, we present a technique for\nlearning from meshes built from standard geometry processing modules and\noperations. We show that low-order eigenvalue/eigenvector computation from\noperators parameterized using discrete exterior calculus is amenable to\nefficient approximate backpropagation, yielding spectral per-element or\nper-mesh features with similar formulas to classical descriptors like the\nheat/wave kernel signatures. Our model uses few parameters, generalizes to\nhigh-resolution meshes, and exhibits performance and time complexity on par\nwith past work.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:00:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Smirnov", "Dmitriy", ""], ["Solomon", "Justin", ""]]}, {"id": "2104.13514", "submitter": "Brooke Krajancich", "authors": "Brooke Krajancich, Petr Kellnhofer, Gordon Wetzstein", "title": "A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker\n  Fusion and its Applications to Foveated Graphics", "comments": null, "journal-ref": "ACM Trans. Graph. 40, 4, Article 47 (August 2021), 11 pages", "doi": "10.1145/3450626.3459784", "report-no": null, "categories": "cs.HC cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual and augmented reality (VR/AR) displays strive to provide a\nresolution, framerate and field of view that matches the perceptual\ncapabilities of the human visual system, all while constrained by limited\ncompute budgets and transmission bandwidths of wearable computing systems.\nFoveated graphics techniques have emerged that could achieve these goals by\nexploiting the falloff of spatial acuity in the periphery of the visual field.\nHowever, considerably less attention has been given to temporal aspects of\nhuman vision, which also vary across the retina. This is in part due to\nlimitations of current eccentricity-dependent models of the visual system. We\nintroduce a new model, experimentally measuring and computationally fitting\neccentricity-dependent critical flicker fusion thresholds jointly for both\nspace and time. In this way, our model is unique in enabling the prediction of\ntemporal information that is imperceptible for a certain spatial frequency,\neccentricity, and range of luminance levels. We validate our model with an\nimage quality user study, and use it to predict potential bandwidth savings 7x\nhigher than those afforded by current spatial-only foveated models. As such,\nthis work forms the enabling foundation for new temporally foveated graphics\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 00:51:14 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 08:12:08 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 14:31:16 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 07:15:21 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2021 03:53:06 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Krajancich", "Brooke", ""], ["Kellnhofer", "Petr", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2104.13755", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Jiayi Eris Zhang, Mirela Ben-Chen, Alec Jacobson", "title": "Surface Multigrid via Intrinsic Prolongation", "comments": "13 pages, 27 figures, SIGGRAPH 2021", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 80. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459768", "report-no": null, "categories": "cs.GR cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel geometric multigrid solver for unstructured\ncurved surfaces. Multigrid methods are highly efficient iterative methods for\nsolving systems of linear equations. Despite the success in solving problems\ndefined on structured domains, generalizing multigrid to unstructured curved\ndomains remains a challenging problem. The critical missing ingredient is a\nprolongation operator to transfer functions across different multigrid levels.\nWe propose a novel method for computing the prolongation for triangulated\nsurfaces based on intrinsic geometry, enabling an efficient geometric multigrid\nsolver for curved surfaces. Our surface multigrid solver achieves better\nconvergence than existing multigrid methods. Compared to direct solvers, our\nsolver is orders of magnitude faster. We evaluate our method on many geometry\nprocessing applications and a wide variety of complex shapes with and without\nboundaries. By simply replacing the direct solver, we upgrade existing\nalgorithms to interactive frame rates, and shift the computational bottleneck\naway from solving linear systems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:35:32 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:24:44 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Zhang", "Jiayi Eris", ""], ["Ben-Chen", "Mirela", ""], ["Jacobson", "Alec", ""]]}, {"id": "2104.14559", "submitter": "Fangzhou Han", "authors": "Fangzhou Han, Shuquan Ye, Mingming He, Menglei Chai and Jing Liao", "title": "Exemplar-Based 3D Portrait Stylization", "comments": "Project page: https://halfjoe.github.io/projs/3DPS/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based portrait stylization is widely attractive and highly desired.\nDespite recent successes, it remains challenging, especially when considering\nboth texture and geometric styles. In this paper, we present the first\nframework for one-shot 3D portrait style transfer, which can generate 3D face\nmodels with both the geometry exaggerated and the texture stylized while\npreserving the identity from the original content. It requires only one\narbitrary style image instead of a large set of training examples for a\nparticular style, provides geometry and texture outputs that are fully\nparameterized and disentangled, and enables further graphics applications with\nthe 3D representations. The framework consists of two stages. In the first\ngeometric style transfer stage, we use facial landmark translation to capture\nthe coarse geometry style and guide the deformation of the dense 3D face\ngeometry. In the second texture style transfer stage, we focus on performing\nstyle transfer on the canonical texture by adopting a differentiable renderer\nto optimize the texture in a multi-view framework. Experiments show that our\nmethod achieves robustly good results on different artistic styles and\noutperforms existing methods. We also demonstrate the advantages of our method\nvia various 2D and 3D graphics applications. Project page is\nhttps://halfjoe.github.io/projs/3DPS/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:54 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Han", "Fangzhou", ""], ["Ye", "Shuquan", ""], ["He", "Mingming", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""]]}, {"id": "2104.14667", "submitter": "T.J. Jankun-Kelly", "authors": "Donald W. Johnson and T. J. Jankun-Kelly", "title": "Efficacy of Images Versus Data Buffers: Optimizing Interactive\n  Applications Utilizing OpenCL for Scientific Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSU-140331-01", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines an algorithm using dual OpenCL image buffers to optimize\ndata streaming for ensemble processing and visualization. Image buffers were\nutilized because they allow cached memory access, unlike simple data buffers,\nwhich are more commonly used. OpenCL image object performance was improved by\nallowing upload and mapping into one buffer to occur concurrently with mapping\nand/or processing of data in another buffer. This technique was applied in an\ninteractive application allowing multiple flood extent maps to be combined into\na single image, and allowing users to vary input image sets in real time. The\nefficiency of this technique was tested by varying both dimensions of input\nimages and number of iterations; computation scaled linearly with number of\ninput images, with best results achieved using ~4k images. Tests were performed\nto determine the rate at which data could be moved from data buffers to image\nbuffers, examining a large range of possible image buffer dimensions.\nAdditional tests examined kernel runtimes with different image and buffer\nvariants. Limitations of the algorithm and possible applications are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 21:28:40 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Johnson", "Donald W.", ""], ["Jankun-Kelly", "T. J.", ""]]}, {"id": "2104.14786", "submitter": "Jiakai Zhang", "authors": "Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun Zhang,\n  Minye Wu, Yingliang Zhang, Lan Xu, Jingyi Yu", "title": "Editable Free-viewpoint Video Using a Layered Neural Representation", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459756", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating free-viewpoint videos is critical for immersive VR/AR experience\nbut recent neural advances still lack the editing ability to manipulate the\nvisual perception for large dynamic scenes. To fill this gap, in this paper we\npropose the first approach for editable photo-realistic free-viewpoint video\ngeneration for large-scale dynamic scenes using only sparse 16 cameras. The\ncore of our approach is a new layered neural representation, where each dynamic\nentity including the environment itself is formulated into a space-time\ncoherent neural layered radiance representation called ST-NeRF. Such layered\nrepresentation supports fully perception and realistic manipulation of the\ndynamic scene whilst still supporting a free viewing experience in a wide\nrange. In our ST-NeRF, the dynamic entity/layer is represented as continuous\nfunctions, which achieves the disentanglement of location, deformation as well\nas the appearance of the dynamic entity in a continuous and self-supervised\nmanner. We propose a scene parsing 4D label map tracking to disentangle the\nspatial information explicitly, and a continuous deform module to disentangle\nthe temporal motion implicitly. An object-aware volume rendering scheme is\nfurther introduced for the re-assembling of all the neural layers. We adopt a\nnovel layered loss and motion-aware ray sampling strategy to enable efficient\ntraining for a large dynamic scene with multiple performers, Our framework\nfurther enables a variety of editing functions, i.e., manipulating the scale\nand location, duplicating or retiming individual neural layers to create\nnumerous visual effects while preserving high realism. Extensive experiments\ndemonstrate the effectiveness of our approach to achieve high-quality,\nphoto-realistic, and editable free-viewpoint video generation for dynamic\nscenes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 06:50:45 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Jiakai", ""], ["Liu", "Xinhang", ""], ["Ye", "Xinyi", ""], ["Zhao", "Fuqiang", ""], ["Zhang", "Yanshun", ""], ["Wu", "Minye", ""], ["Zhang", "Yingliang", ""], ["Xu", "Lan", ""], ["Yu", "Jingyi", ""]]}, {"id": "2104.14802", "submitter": "Xinjian Zhang", "authors": "Xinjian Zhang, Yi Xu, Su Yang, Longwen Gao, Huyang Sun", "title": "Dance Generation with Style Embedding: Learning and Transferring Latent\n  Representations of Dance Styles", "comments": "Submit to IJCAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreography refers to creation of dance steps and motions for dances\naccording to the latent knowledge in human mind, where the created dance\nmotions are in general style-specific and consistent. So far, such latent\nstyle-specific knowledge about dance styles cannot be represented explicitly in\nhuman language and has not yet been learned in previous works on music-to-dance\ngeneration tasks. In this paper, we propose a novel music-to-dance synthesis\nframework with controllable style embeddings. These embeddings are learned\nrepresentations of style-consistent kinematic abstraction of reference dance\nclips, which act as controllable factors to impose style constraints on dance\ngeneration in a latent manner. Thus, the dance styles can be transferred to\ndance motions by merely modifying the style embeddings. To support this study,\nwe build a large music-to-dance dataset. The qualitative and quantitative\nevaluations demonstrate the advantage of our proposed framework, as well as the\nability of synthesizing diverse styles of dances from identical music via style\nembeddings.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:36:49 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Xinjian", ""], ["Xu", "Yi", ""], ["Yang", "Su", ""], ["Gao", "Longwen", ""], ["Sun", "Huyang", ""]]}, {"id": "2104.15116", "submitter": "David Lowry-Duda", "authors": "David Lowry-Duda, Adam Sakareassen", "title": "Towards Flying through Modular Forms", "comments": "4 pages, 4 figures, for Bridges", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NT cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modular forms are highly self-symmetric functions studied in number theory,\nwith connections to several areas of mathematics. But they are rarely\nvisualized. We discuss ongoing work to compute and visualize modular forms as\n3D surfaces and to use these techniques to make videos flying around the peaks\nand canyons of these \"modular terrains.\" Our goal is to make beautiful\nvisualizations exposing the symmetries of these functions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:57:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lowry-Duda", "David", ""], ["Sakareassen", "Adam", ""]]}]