[{"id": "1711.00765", "submitter": "Barak Sober", "authors": "Barak Sober, Yariv Aizenbud, David Levin", "title": "Approximation of Functions over Manifolds: A Moving Least-Squares\n  Approach", "comments": "arXiv admin note: text overlap with arXiv:1606.07104", "journal-ref": null, "doi": "10.1016/j.cam.2020.113140", "report-no": null, "categories": "stat.ML cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for approximating a function defined over a\n$d$-dimensional manifold utilizing only noisy function values at locations\nsampled from the manifold with noise. To produce the approximation we do not\nrequire any knowledge regarding the manifold other than its dimension $d$. We\nuse the Manifold Moving Least-Squares approach of (Sober and Levin 2016) to\nreconstruct the atlas of charts and the approximation is built on-top of those\ncharts. The resulting approximant is shown to be a function defined over a\nneighborhood of a manifold, approximating the originally sampled manifold. In\nother words, given a new point, located near the manifold, the approximation\ncan be evaluated directly on that point. We prove that our construction yields\na smooth function, and in case of noiseless samples the approximation order is\n$\\mathcal{O}(h^{m+1})$, where $h$ is a local density of sample parameter (i.e.,\nthe fill distance) and $m$ is the degree of a local polynomial approximation,\nused in our algorithm. In addition, the proposed algorithm has linear time\ncomplexity with respect to the ambient-space's dimension. Thus, we are able to\navoid the computational complexity, commonly encountered in high dimensional\napproximations, without having to perform non-linear dimension reduction, which\ninevitably introduces distortions to the geometry of the data. Additionaly, we\nshow numerical experiments that the proposed approach compares favorably to\nstatistical approaches for regression over manifolds and show its potential.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:37:04 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 07:49:10 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 00:05:59 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 02:15:41 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Sober", "Barak", ""], ["Aizenbud", "Yariv", ""], ["Levin", "David", ""]]}, {"id": "1711.00967", "submitter": "Angus Forbes", "authors": "Angus G. Forbes, Andrew Burks, Kristine Lee, Xing Li, Pierre\n  Boutillier, Jean Krivine, Walter Fontana", "title": "Dynamic Influence Networks for Rule-based Models", "comments": "Accepted to TVCG, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.SI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Dynamic Influence Network (DIN), a novel visual analytics\ntechnique for representing and analyzing rule-based models of protein-protein\ninteraction networks. Rule-based modeling has proved instrumental in developing\nbiological models that are concise, comprehensible, easily extensible, and that\nmitigate the combinatorial complexity of multi-state and multi-component\nbiological molecules. Our technique visualizes the dynamics of these rules as\nthey evolve over time. Using the data produced by KaSim, an open source\nstochastic simulator of rule-based models written in the Kappa language, DINs\nprovide a node-link diagram that represents the influence that each rule has on\nthe other rules. That is, rather than representing individual biological\ncomponents or types, we instead represent the rules about them (as nodes) and\nthe current influence of these rules (as links). Using our interactive DIN-Viz\nsoftware tool, researchers are able to query this dynamic network to find\nmeaningful patterns about biological processes, and to identify salient aspects\nof complex rule-based models. To evaluate the effectiveness of our approach, we\ninvestigate a simulation of a circadian clock model that illustrates the\noscillatory behavior of the KaiC protein phosphorylation cycle.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 22:59:23 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Forbes", "Angus G.", ""], ["Burks", "Andrew", ""], ["Lee", "Kristine", ""], ["Li", "Xing", ""], ["Boutillier", "Pierre", ""], ["Krivine", "Jean", ""], ["Fontana", "Walter", ""]]}, {"id": "1711.02793", "submitter": "Matthias Lee", "authors": "Matthias Lee, Tamas Budavari, Richard White and Charles Gulian", "title": "Robust Statistics for Image Deconvolution", "comments": null, "journal-ref": null, "doi": "10.1016/j.ascom.2017.09.002", "report-no": null, "categories": "astro-ph.IM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a blind multiframe image-deconvolution method based on robust\nstatistics. The usual shortcomings of iterative optimization of the likelihood\nfunction are alleviated by minimizing the M-scale of the residuals, which\nachieves more uniform convergence across the image. We focus on the\ndeconvolution of astronomical images, which are among the most challenging due\nto their huge dynamic ranges and the frequent presence of large noise-dominated\nregions in the images. We show that high-quality image reconstruction is\npossible even in super-resolution and without the use of traditional\nregularization terms. Using a robust \\r{ho}-function is straightforward to\nimplement in a streaming setting and, hence our method is applicable to the\nlarge volumes of astronomy images. The power of our method is demonstrated on\nobservations from the Sloan Digital Sky Survey (Stripe 82) and we briefly\ndiscuss the feasibility of a pipeline based on Graphical Processing Units for\nthe next generation of telescope surveys.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 01:26:59 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Lee", "Matthias", ""], ["Budavari", "Tamas", ""], ["White", "Richard", ""], ["Gulian", "Charles", ""]]}, {"id": "1711.03065", "submitter": "Saturnino Luz", "authors": "Saturnino Luz and Masood Masoodian", "title": "An Application of Mosaic Diagrams to the Visualization of Set\n  Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an application of mosaic diagrams to the visualisation of set\nrelations. Venn and Euler diagrams are the best known visual representations of\nsets and their relationships (intersections, containment or subsets, exclusion\nor disjointness). In recent years, alternative forms of visualisation have been\nproposed. Among them, linear diagrams have been shown to compare favourably to\nVenn and Euler diagrams, in supporting non-interactive assessment of set\nrelationships. Recent studies that compared several variants of linear diagrams\nhave demonstrated that users perform best at tasks involving identification of\nintersections, disjointness and subsets when using a horizontally drawn linear\ndiagram with thin lines representing sets, and employing vertical lines as\nguide lines. The essential visual task the user needs to perform in order to\ninterpret this kind of diagram is vertical alignment of parallel lines and\ndetection of overlaps. Space-filling mosaic diagrams which support this same\nvisual task have been used in other applications, such as the visualization of\nschedules of activities, where they have been shown to be superior to linear\nGantt charts. In this paper, we present an application of mosaic diagrams for\nvisualization of set relationships, and compare it to linear diagrams in terms\nof accuracy, time-to-answer, and subjective ratings of perceived task\ndifficulty. The study participants exhibited similar performance on both\nvisualisations, suggesting that mosaic diagrams are a good alternative to Venn\nand Euler diagrams, and that the choice between linear diagrams and mosaics may\nbe solely guided by visual design considerations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 17:45:09 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Luz", "Saturnino", ""], ["Masoodian", "Masood", ""]]}, {"id": "1711.03538", "submitter": "Lukas Cavigelli", "authors": "Manuel Eggimann, Christelle Gloor, Florian Scheidegger, Lukas\n  Cavigelli, Michael Schaffner, Aljosa Smolic, Luca Benini", "title": "Hydra: An Accelerator for Real-Time Edge-Aware Permeability Filtering in\n  65nm CMOS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AR cs.GR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern video processing pipelines rely on edge-aware (EA) filtering\nmethods. However, recent high-quality methods are challenging to run in\nreal-time on embedded hardware due to their computational load. To this end, we\npropose an area-efficient and real-time capable hardware implementation of a\nhigh quality EA method. In particular, we focus on the recently proposed\npermeability filter (PF) that delivers promising quality and performance in the\ndomains of HDR tone mapping, disparity and optical flow estimation. We present\nan efficient hardware accelerator that implements a tiled variant of the PF\nwith low on-chip memory requirements and a significantly reduced external\nmemory bandwidth (6.4x w.r.t. the non-tiled PF). The design has been taped out\nin 65 nm CMOS technology, is able to filter 720p grayscale video at 24.8 Hz and\nachieves a high compute density of 6.7 GFLOPS/mm2 (12x higher than embedded\nGPUs when scaled to the same technology node). The low area and bandwidth\nrequirements make the accelerator highly suitable for integration into SoCs\nwhere silicon area budget is constrained and external memory is typically a\nheavily contended resource.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 22:34:33 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Eggimann", "Manuel", ""], ["Gloor", "Christelle", ""], ["Scheidegger", "Florian", ""], ["Cavigelli", "Lukas", ""], ["Schaffner", "Michael", ""], ["Smolic", "Aljosa", ""], ["Benini", "Luca", ""]]}, {"id": "1711.03678", "submitter": "Michael Janner", "authors": "Michael Janner, Jiajun Wu, Tejas D. Kulkarni, Ilker Yildirim, Joshua\n  B. Tenenbaum", "title": "Self-Supervised Intrinsic Image Decomposition", "comments": "NIPS 2017 camera-ready version, project page:\n  http://rin.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic decomposition from a single image is a highly challenging task, due\nto its inherent ambiguity and the scarcity of training data. In contrast to\ntraditional fully supervised learning approaches, in this paper we propose\nlearning intrinsic image decomposition by explaining the input image. Our\nmodel, the Rendered Intrinsics Network (RIN), joins together an image\ndecomposition pipeline, which predicts reflectance, shape, and lighting\nconditions given a single image, with a recombination function, a learned\nshading model used to recompose the original input based off of intrinsic image\npredictions. Our network can then use unsupervised reconstruction error as an\nadditional signal to improve its intermediate representations. This allows\nlarge-scale unlabeled data to be useful during training, and also enables\ntransferring learned knowledge to images of unseen object categories, lighting\nconditions, and shapes. Extensive experiments demonstrate that our method\nperforms well on both intrinsic image decomposition and knowledge transfer.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 03:31:27 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 22:52:18 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Janner", "Michael", ""], ["Wu", "Jiajun", ""], ["Kulkarni", "Tejas D.", ""], ["Yildirim", "Ilker", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1711.04118", "submitter": "Alexander Khoperskov V.", "authors": "A.V. Khoperskov, M.E. Kovalev, A.S. Astakhov, V.V. Novochadov, A.A.\n  Terpilovskiy, K.P. Tiras, D.A. Malanin", "title": "Software for full-color 3D reconstruction of the biological tissues\n  internal structure", "comments": "11 pages, 8 figures", "journal-ref": "Lecture Notes in Computer Science, 2017, v.10594, p.1-10", "doi": "10.1007/978-3-319-69182-4_1", "report-no": null, "categories": "physics.med-ph cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A software for processing sets of full-color images of biological tissue\nhistological sections is developed. We used histological sections obtained by\nthe method of high-precision layer-by-layer grinding of frozen biological\ntissues. The software allows restoring the image of the tissue for an arbitrary\ncross-section of the tissue sample. Thus, our method is designed to create a\nfull-color 3D reconstruction of the biological tissue structure. The resolution\nof 3D reconstruction is determined by the quality of the initial histological\nsections. The newly developed technology available to us provides a resolution\nof up to 5 - 10 {\\mu}m in three dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 11:11:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Khoperskov", "A. V.", ""], ["Kovalev", "M. E.", ""], ["Astakhov", "A. S.", ""], ["Novochadov", "V. V.", ""], ["Terpilovskiy", "A. A.", ""], ["Tiras", "K. P.", ""], ["Malanin", "D. A.", ""]]}, {"id": "1711.04194", "submitter": "Christos Mousas", "authors": "Christos Mousas", "title": "Full-Body Locomotion Reconstruction of Virtual Characters Using a Single\n  IMU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method of reconstructing full-body locomotion sequences\nfor virtual characters in real-time, using data from a single inertial\nmeasurement unit (IMU). This process can be characterized by its difficulty\nbecause of the need to reconstruct a high number of degrees of freedom (DOFs)\nfrom a very low number of DOFs. To solve such a complex problem, the presented\nmethod is divided into several steps. The user's full-body locomotion and the\nIMU's data are recorded simultaneously. Then, the data is preprocessed in such\na way that would be handled more efficiently. By developing a hierarchical\nmultivariate hidden Markov model with reactive interpolation functionality the\nsystem learns the structure of the motion sequences. Specifically, the phases\nof the locomotion sequence are assigned in the higher hierarchical level, and\nthe frame structure of the motion sequences are assigned at the lower\nhierarchical level. During the runtime of the method, the forward algorithm is\nused for reconstructing the full-body motion of a virtual character. Firstly,\nthe method predicts the phase where the input motion belongs (higher\nhierarchical level). Secondly, the method predicts the closest trajectories and\ntheir progression and interpolates the most probable of them to reconstruct the\nvirtual character's full-body motion (lower hierarchical level). Evaluating the\nproposed method shows that it works on reasonable framerates and minimizes the\nreconstruction errors compared with previous approaches.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 20:47:22 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mousas", "Christos", ""]]}, {"id": "1711.04419", "submitter": "Jian Chen", "authors": "Guohao Zhang and Alexander P. Auchus and Peter Kochunov and Niklas\n  Elmqvist and Jian Chen", "title": "Overlaying Quantitative Measurement on Networks: An Evaluation of Three\n  Positioning and Nine Visual Marker Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report results from an experiment on ranking visual markers and node\npositioning techniques for network visualizations. Inspired by prior ranking\nstudies, we rethink the ranking when the dataset size increases and when the\nmarkers are distributed in space. Centrality indices are visualized as node\nattributes. Our experiment studies nine visual markers and three positioning\nmethods. Our results suggest that direct encoding of quantities improves\naccuracy by about 20% compared to previous results. Of the three positioning\ntechniques, circular was always in the top group, and matrix and projection\nswitch orders depending on two factors: whether or not the tasks demand\nsymmetry, or the nodes are within closely proximity. Among the most interesting\nresults of ranking the visual markers for comparison tasks are that hue and\narea fall into the top groups for nearly all multi-scale comparison tasks;\nShape (ordered by curvature) is perhaps not as scalable as we have thought and\ncan support more accurate answers only when two quantities are compared;\nLightness and slope are least accurate for quantitative comparisons regardless\nof scale of the comparison tasks. Our experiment is among the first to acquire\na complete picture of ranking visual markers in different scales for comparison\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 04:29:44 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Zhang", "Guohao", ""], ["Auchus", "Alexander P.", ""], ["Kochunov", "Peter", ""], ["Elmqvist", "Niklas", ""], ["Chen", "Jian", ""]]}, {"id": "1711.04592", "submitter": "Jan Egger", "authors": "Jan Egger, Christopher Nimsky, Xiaojun Chen", "title": "Vertebral body segmentation with GrowCut: Initial experience, workflow\n  and practical application", "comments": "10 pages", "journal-ref": "SAGE Open Medicine, Volume 5, pp. 1-10, Nov. 2017", "doi": "10.1177/2050312117740984", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we used the GrowCut segmentation algorithm publicly\navailable in three-dimensional Slicer for three-dimensional segmentation of\nvertebral bodies. To the best of our knowledge, this is the first time that the\nGrowCut method has been studied for the usage of vertebral body segmentation.\nIn brief, we found that the GrowCut segmentation times were consistently less\nthan the manual segmentation times. Hence, GrowCut provides an alternative to a\nmanual slice-by-slice segmentation process.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 14:19:05 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Egger", "Jan", ""], ["Nimsky", "Christopher", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1711.05075", "submitter": "Morad Behandish", "authors": "Morad Behandish and Horea T. Ilies", "title": "Analytic Methods for Geometric Modeling via Spherical Decomposition", "comments": "Special Issue on SIAM/ACM symposium on Solid and Physical Modeling\n  (SPM'2015) (Best Paper Award, 2nd Place)", "journal-ref": "Journal of Computer-Aided Design, 70, pp.100-115, 2016", "doi": "10.1016/j.cad.2015.06.016", "report-no": "CDL-TR-15-07", "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic methods are emerging in solid and configuration modeling, while\nproviding new insights into a variety of shape and motion related problems by\nexploiting tools from group morphology, convolution algebras, and harmonic\nanalysis. However, most convolution-based methods have used uniform grid-based\nsampling to take advantage of the fast Fourier transform (FFT) algorithm. We\npropose a new paradigm for more efficient computation of analytic correlations\nthat relies on a grid-free discretization of arbitrary shapes as countable\nunions of balls, in turn described as sublevel sets of summations of smooth\nradial kernels at adaptively sampled 'knots'. Using a simple geometric lifting\ntrick, we interpret this combination as a convolution of an impulsive skeletal\ndensity and primitive kernels with conical support, which faithfully embeds\ninto the convolution formulation of interactions across different objects. Our\napproach enables fusion of search-efficient combinatorial data structures\nprevalent in time-critical collision and proximity queries with analytic\nmethods popular in path planning and protein docking, and outperforms uniform\ngrid-based FFT methods by leveraging nonequispaced FFTs. We provide example\napplications in formulating holonomic collision constraints, shape\ncomplementarity metrics, and morphological operations, unified within a single\nanalytic framework.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 12:16:34 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""]]}, {"id": "1711.05341", "submitter": "Sunil Kumar Yadav", "authors": "Sunil Kumar Yadav, Ulrich Reitebuch, and Konrad Polthier", "title": "Robust and High Fidelity Mesh Denoising", "comments": "Revised Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple and effective two-stage mesh denoising\nalgorithm, where in the first stage, the face normal filtering is done by using\nthe bilateral normal filtering in the robust statistics framework. Tukey's\nbi-weight function is used as similarity function in the bilateral weighting,\nwhich is a robust estimator and stops the diffusion at sharp edges to retain\nfeatures and removes noise from flat regions effectively. In the second stage,\nan edge weighted Laplace operator is introduced to compute a differential\ncoordinate. This differential coordinate helps the algorithm to produce a\nhigh-quality mesh without any face normal flips and makes the method robust\nagainst high-intensity noise.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 22:50:01 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 14:49:39 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Yadav", "Sunil Kumar", ""], ["Reitebuch", "Ulrich", ""], ["Polthier", "Konrad", ""]]}, {"id": "1711.07790", "submitter": "Carl Lundholm", "authors": "Anders Logg, Carl Lundholm, Magne Nordaas", "title": "Solving Poisson's Equation on the Microsoft HoloLens", "comments": "2 pages, 9 figures", "journal-ref": "In Proceedings of the 23rd ACM Symposium on Virtual Reality\n  Software and Technology (VRST 2017). ACM, New York, NY, USA, Article 87", "doi": "10.1145/3139131.3141777", "report-no": null, "categories": "cs.GR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixed reality application (HoloFEM) for the Microsoft HoloLens.\nThe application lets a user define and solve a physical problem governed by\nPoisson's equation with the surrounding real world geometry as input data.\nHolograms are used to visualise both the problem and the solution. The finite\nelement method is used to solve Poisson's equation. Solving and visualising\npartial differential equations in mixed reality could have potential usage in\nareas such as building planning and safety engineering.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 13:44:33 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Logg", "Anders", ""], ["Lundholm", "Carl", ""], ["Nordaas", "Magne", ""]]}, {"id": "1711.07793", "submitter": "M\\'arcio Macedo", "authors": "M. C. F. Macedo, A. L. Apolin\\'ario, K. A. Ag\\\"uero", "title": "Optimized Visibility Functions for Revectorization-Based Shadow Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-PGCOMP-007/2017. Technical Report. Computer Science Graduate\n  Program. Federal University of Bahia", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality shadow anti-aliasing is a challenging problem in shadow mapping.\nRevectorization-based shadow mapping (RBSM) minimizes shadow aliasing by\nrevectorizing the jagged shadow edges generated with shadow mapping, keeping\nlow memory footprint and real-time performance for the shadow computation.\nHowever, the current implementation of RBSM is not so well optimized because\nits visibility functions are composed of a set of 43 cases, each one of them\nhandling a specific revectorization scenario and being implemented as a\nspecific branch in the shader. Here, we take advantage of the shadow shape\npatterns to reformulate the RBSM visibility functions, simplifying the\nimplementation of the technique and further providing an optimized version of\nthe RBSM. Our results indicate that our implementation runs faster than the\noriginal implementation of RBSM, while keeping its same visual quality and\nmemory consumption. Furthermore, we show GLSL source codes to ease the\nimplementation of our technique, provide a comparison between the optimized\nRBSM and related work, and discuss the limitations of the shadow\nrevectorization.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 14:11:24 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Macedo", "M. C. F.", ""], ["Apolin\u00e1rio", "A. L.", ""], ["Ag\u00fcero", "K. A.", ""]]}, {"id": "1711.08103", "submitter": "Marc Walton", "authors": "Johanna Salvant, Marc Walton, Dale Kronkright, Chia-Kai Yeh, Fengqiang\n  Li, Oliver Cossairt, Aggelos K. Katsaggelos", "title": "Photometric Stereo by UV-Induced Fluorescence to Detect Protrusions on\n  Georgia O'Keeffe's Paintings", "comments": "Accepted for publication in the Springer Nature book: Metal Soaps in\n  Art-Conservation & Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.app-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A significant number of oil paintings produced by Georgia O'Keeffe\n(1887-1986) show surface protrusions of varying width, up to several hundreds\nof microns. These protrusions are similar to those described in the art\nconservation literature as metallic soaps. Since the presence of these\nprotrusions raises questions about the state of conservation and long-term\nprospects for deterioration of these artworks, a 3D-imaging technique,\nphotometric stereo using ultraviolet illumination, was developed for the\nlong-term monitoring of the surface-shape of the protrusions and the\nsurrounding paint. Because the UV fluorescence response of painting materials\nis isotropic, errors typically caused by non-Lambertian (anisotropic)\nspecularities when using visible reflected light can be avoided providing a\nmore accurate estimation of shape. As an added benefit, fluorescence provides\nadditional contrast information contributing to materials characterization. The\ndeveloped methodology aims to detect, characterize, and quantify the\ndistribution of micro-protrusions and their development over the surface of\nentire artworks. Combined with a set of analytical in-situ techniques, and\ncomputational tools, this approach constitutes a novel methodology to\ninvestigate the selective distribution of protrusions in correlation with the\ncomposition of painting materials at the macro-scale. While focused on\nO'Keeffe's paintings as a case study, we expect the proposed approach to have\nbroader significance by providing a non-invasive protocol to the conservation\ncommunity to probe topological changes for any relatively flat painted surface\nof an artwork, and more specifically to monitor the dynamic formation of\nprotrusions, in relation to paint composition and modifications of\nenvironmental conditions, loans, exhibitions and storage over the long-term.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 01:47:04 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Salvant", "Johanna", ""], ["Walton", "Marc", ""], ["Kronkright", "Dale", ""], ["Yeh", "Chia-Kai", ""], ["Li", "Fengqiang", ""], ["Cossairt", "Oliver", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1711.08126", "submitter": "Zihao Zhang", "authors": "Shihong Xia, Zihao Zhang, Le Su", "title": "Cascaded 3D Full-body Pose Regression from Single Depth Image at 100 FPS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are increasing real-time live applications in virtual reality, where it\nplays an important role in capturing and retargetting 3D human pose. But it is\nstill challenging to estimate accurate 3D pose from consumer imaging devices\nsuch as depth camera. This paper presents a novel cascaded 3D full-body pose\nregression method to estimate accurate pose from a single depth image at 100\nfps. The key idea is to train cascaded regressors based on Gradient Boosting\nalgorithm from pre-recorded human motion capture database. By incorporating\nhierarchical kinematics model of human pose into the learning procedure, we can\ndirectly estimate accurate 3D joint angles instead of joint positions. The\nbiggest advantage of this model is that the bone length can be preserved during\nthe whole 3D pose estimation procedure, which leads to more effective features\nand higher pose estimation accuracy. Our method can be used as an\ninitialization procedure when combining with tracking methods. We demonstrate\nthe power of our method on a wide range of synthesized human motion data from\nCMU mocap database, Human3.6M dataset and real human movements data captured in\nreal time. In our comparison against previous 3D pose estimation methods and\ncommercial system such as Kinect 2017, we achieve the state-of-the-art\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 04:24:43 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 08:11:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Xia", "Shihong", ""], ["Zhang", "Zihao", ""], ["Su", "Le", ""]]}, {"id": "1711.08279", "submitter": "Thomas Schultz", "authors": "Amin Abbasloo, Vitalis Wiens, Tobias Schmidt-Wilcke, Pia Sundgren,\n  Reinhard Klein, Thomas Schultz", "title": "Visual Analytics of Group Differences in Tensor Fields: Application to\n  Clinical DTI", "comments": "Submitted to IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visual analytics system for exploring group differences in\ntensor fields with respect to all six degrees of freedom that are inherent in\nsymmetric second-order tensors. Our framework closely integrates quantitative\nanalysis, based on multivariate hypothesis testing and spatial cluster\nenhancement, with suitable visualization tools that facilitate interpretation\nof results, and forming of new hypotheses. Carefully chosen and linked spatial\nand abstract views show clusters of strong differences, and allow the analyst\nto relate them to the affected structures, to reveal the exact nature of the\ndifferences, and to investigate potential correlations. A mechanism for\nvisually comparing the results of different tests or levels of smoothing is\nalso provided.\n  We carefully justify the need for such a visual analytics tool from a\npractical and theoretical point of view. In close collaboration with our\nclinical co-authors, we apply it to the results of a diffusion tensor imaging\nstudy of systemic lupus erythematosus, in which it revealed previously unknown\ngroup differences.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 13:48:31 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Abbasloo", "Amin", ""], ["Wiens", "Vitalis", ""], ["Schmidt-Wilcke", "Tobias", ""], ["Sundgren", "Pia", ""], ["Klein", "Reinhard", ""], ["Schultz", "Thomas", ""]]}, {"id": "1711.08925", "submitter": "Alessandro Artusi PhD", "authors": "E. Sikudova, T. Pouli, A. Artusi, A. O. Akyuz, F. Banterle, Z. M.\n  Mazlumoglu and E. Reinhard", "title": "A Gamut-Mapping Framework for Color-Accurate Reproduction of HDR Images", "comments": null, "journal-ref": "IEEE Computer Graphics and Applications, Volume 36, Issue 4,\n  p.78-90, ISSN 0272-1716, July-August 2016", "doi": "10.1109/MCG.2015.116", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few tone mapping operators (TMOs) take color management into consideration,\nlimiting compression to luminance values only. This may lead to changes in\nimage chroma and hues which are typically managed with a post-processing step.\nHowever, current post-processing techniques for tone reproduction do not\nexplicitly consider the target display gamut. Gamut mapping on the other hand,\ndeals with mapping images from one color gamut to another, usually smaller,\ngamut but has traditionally focused on smaller scale, chromatic changes. In\nthis context, we present a novel gamut and tone management framework for\ncolor-accurate reproduction of high dynamic range (HDR) images, which is\nconceptually and computationally simple, parameter-free, and compatible with\nexisting TMOs. In the CIE LCh color space, we compress chroma to fit the gamut\nof the output color space. This prevents hue and luminance shifts while taking\ngamut boundaries into consideration. We also propose a compatible lightness\ncompression scheme that minimizes the number of color space conversions. Our\nresults show that our gamut management method effectively compresses the chroma\nof tone mapped images, respecting the target gamut and without reducing image\nquality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 11:06:07 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Sikudova", "E.", ""], ["Pouli", "T.", ""], ["Artusi", "A.", ""], ["Akyuz", "A. O.", ""], ["Banterle", "F.", ""], ["Mazlumoglu", "Z. M.", ""], ["Reinhard", "E.", ""]]}, {"id": "1711.08937", "submitter": "Shangzhe Wu", "authors": "Shangzhe Wu, Jiarui Xu, Yu-Wing Tai, Chi-Keung Tang", "title": "Deep High Dynamic Range Imaging with Large Foreground Motions", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the first non-flow-based deep framework for high dynamic\nrange (HDR) imaging of dynamic scenes with large-scale foreground motions. In\nstate-of-the-art deep HDR imaging, input images are first aligned using optical\nflows before merging, which are still error-prone due to occlusion and large\nmotions. In stark contrast to flow-based methods, we formulate HDR imaging as\nan image translation problem without optical flows. Moreover, our simple\ntranslation network can automatically hallucinate plausible HDR details in the\npresence of total occlusion, saturation and under-exposure, which are otherwise\nalmost impossible to recover by conventional optimization approaches. Our\nframework can also be extended for different reference images. We performed\nextensive qualitative and quantitative comparisons to show that our approach\nproduces excellent results where color artifacts and geometric distortions are\nsignificantly reduced compared to existing state-of-the-art methods, and is\nrobust across various inputs, including images without radiometric calibration.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:12:01 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 07:23:49 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 11:07:30 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wu", "Shangzhe", ""], ["Xu", "Jiarui", ""], ["Tai", "Yu-Wing", ""], ["Tang", "Chi-Keung", ""]]}, {"id": "1711.09377", "submitter": "Shiva Alemzadeh", "authors": "Shiva Alemzadeh, Tommy Hielscher, Uli Niemann, Lena Cibulski, Till\n  Ittermann, Henry V\\\"olzke, Myra Spiliopoulou, Bernhard Preim", "title": "Visual Subpopulation Discovery and Validation in Cohort Study Data", "comments": "12 pages. This work was originally reported in \"EuroVis Workshop on\n  Visual Analytics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiology aims at identifying subpopulations of cohort participants that\nshare common characteristics (e.g. alcohol consumption) to explain risk factors\nof diseases in cohort study data. These data contain information about the\nparticipants' health status gathered from questionnaires, medical examinations,\nand image acquisition. Due to the growing volume and heterogeneity of\nepidemiological data, the discovery of meaningful subpopulations is\nchallenging. Subspace clustering can be leveraged to find subpopulations in\nlarge and heterogeneous cohort study datasets. In our collaboration with\nepidemiologists, we realized their need for a tool to validate discovered\nsubpopulations. For this purpose, identified subpopulations should be searched\nfor independent cohorts to check whether the findings apply there as well. In\nthis paper we describe our interactive Visual Analytics framework S-ADVIsED for\nSubpopulAtion Discovery and Validation In Epidemiological Data. S-ADVIsED\nenables epidemiologists to explore and validate findings derived from subspace\nclustering. We provide a coordinated multiple view system, which includes a\nsummary view of all subpopulations, detail views, and statistical information.\nUsers can assess the quality of subspace clusters by considering different\ncriteria via visualization. Furthermore, intervals for variables involved in a\nsubspace cluster can be adjusted. This extension was suggested by\nepidemiologists. We investigated the replication of a selected subpopulation\nwith multiple variables in another population by considering different\nmeasurements. As a specific result, we observed that study participants\nexhibiting high liver fat accumulation deviate strongly from other\nsubpopulations and from the total study population with respect to age, body\nmass index, thyroid volume and thyroid-stimulating hormone.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 12:51:36 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Alemzadeh", "Shiva", ""], ["Hielscher", "Tommy", ""], ["Niemann", "Uli", ""], ["Cibulski", "Lena", ""], ["Ittermann", "Till", ""], ["V\u00f6lzke", "Henry", ""], ["Spiliopoulou", "Myra", ""], ["Preim", "Bernhard", ""]]}, {"id": "1711.10824", "submitter": "Virginia Estellers", "authors": "V. Estellers, F. R. Schmidt, D. Cremers", "title": "Compression for Smooth Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D shape analysis methods use triangular meshes to discretize both the\nshape and functions on it as piecewise linear functions. With this\nrepresentation, shape analysis requires fine meshes to represent smooth shapes\nand geometric operators like normals, curvatures, or Laplace-Beltrami\neigenfunctions at large computational and memory costs.\n  We avoid this bottleneck with a compression technique that represents a\nsmooth shape as subdivision surfaces and exploits the subdivision scheme to\nparametrize smooth functions on that shape with a few control parameters. This\ncompression does not affect the accuracy of the Laplace-Beltrami operator and\nits eigenfunctions and allow us to compute shape descriptors and shape\nmatchings at an accuracy comparable to triangular meshes but a fraction of the\ncomputational cost.\n  Our framework can also compress surfaces represented by point clouds to do\nshape analysis of 3D scanning data.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:46:08 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Estellers", "V.", ""], ["Schmidt", "F. R.", ""], ["Cremers", "D.", ""]]}, {"id": "1711.10939", "submitter": "Paul Henderson", "authors": "Paul Henderson, Kartic Subr, Vittorio Ferrari", "title": "Automatic Generation of Constrained Furniture Layouts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient authoring of vast virtual environments hinges on algorithms that\nare able to automatically generate content while also being controllable. We\npropose a method to automatically generate furniture layouts for indoor\nenvironments. Our method is simple, efficient, human-interpretable and amenable\nto a wide variety of constraints. We model the composition of rooms into\nclasses of objects and learn joint (co-occurrence) statistics from a database\nof training layouts. We generate new layouts by performing a sequence of\nconditional sampling steps, exploiting the statistics learned from the\ndatabase. The generated layouts are specified as 3D object models, along with\ntheir positions and orientations. We show they are of equivalent perceived\nquality to the training layouts, and compare favorably to a state-of-the-art\nmethod. We incorporate constraints using a general mechanism -- rejection\nsampling -- which provides great flexibility at the cost of extra computation.\nWe demonstrate the versatility of our method by applying a wide variety of\nconstraints relevant to real-world applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:21:32 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 17:04:42 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 20:58:23 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Henderson", "Paul", ""], ["Subr", "Kartic", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1711.11123", "submitter": "Naimul Khan", "authors": "Naimul Khan, Riadh Ksantini, Ling Guan", "title": "A Novel Image-centric Approach Towards Direct Volume Rendering", "comments": "To appear in the ACM Transactions in Intelligent Systems and\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer Function (TF) generation is a fundamental problem in Direct Volume\nRendering (DVR). A TF maps voxels to color and opacity values to reveal inner\nstructures. Existing TF tools are complex and unintuitive for the users who are\nmore likely to be medical professionals than computer scientists. In this\npaper, we propose a novel image-centric method for TF generation where instead\nof complex tools, the user directly manipulates volume data to generate DVR.\nThe user's work is further simplified by presenting only the most informative\nvolume slices for selection. Based on the selected parts, the voxels are\nclassified using our novel Sparse Nonparametric Support Vector Machine\nclassifier, which combines both local and near-global distributional\ninformation of the training data. The voxel classes are mapped to aesthetically\npleasing and distinguishable color and opacity values using harmonic colors.\nExperimental results on several benchmark datasets and a detailed user survey\nshow the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 21:47:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Khan", "Naimul", ""], ["Ksantini", "Riadh", ""], ["Guan", "Ling", ""]]}, {"id": "1711.11326", "submitter": "Alessandro Artusi PhD", "authors": "Alessandro Artusi, Thomas Richter, Touradj Ebrahimi, Rafal K. Mantiuk", "title": "High Dynamic Range Imaging Technology", "comments": "Lecture Notes", "journal-ref": "IEEE Signal Processing Magazine ( Volume: 34, Issue: 5, Sept. 2017\n  )", "doi": "10.1109/MSP.2017.2716957", "report-no": null, "categories": "cs.GR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this lecture note, we describe high dynamic range (HDR) imaging systems;\nsuch systems are able to represent luminances of much larger brightness and,\ntypically, also a larger range of colors than conventional standard dynamic\nrange (SDR) imaging systems. The larger luminance range greatly improve the\noverall quality of visual content, making it appears much more realistic and\nappealing to observers. HDR is one of the key technologies of the future\nimaging pipeline, which will change the way the digital visual content is\nrepresented and manipulated today.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 11:38:11 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Artusi", "Alessandro", ""], ["Richter", "Thomas", ""], ["Ebrahimi", "Touradj", ""], ["Mantiuk", "Rafal K.", ""]]}, {"id": "1711.11470", "submitter": "Ryan Goldade", "authors": "Ryan Goldade and Christopher Batty", "title": "Constraint Bubbles: Adding Efficient Zero-Density Bubbles to\n  Incompressible Free Surface Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid simulations for computer animation often avoid simulating the air\nphase to reduce computational costs and ensure good conditioning of the linear\nsystems required to enforce incompressibility. However, this free surface\nassumption leads to an inability to realistically treat bubbles: submerged gaps\nin the liquid are interpreted as empty voids that immediately collapse. To\naddress this shortcoming, we present an efficient, practical, and conceptually\nsimple approach to augment free surface flows with negligible density bubbles.\nOur method adds a new constraint to each disconnected air region that\nguarantees zero net flux across its entire surface, and requires neither\nsimulating both phases nor reformulating into stream function variables.\nImplementation of the method requires only minor modifications to the pressure\nsolve of a standard grid-based fluid solver, and yields linear systems that\nremain sparse and symmetric positive definite. In our evaluations, solving the\nmodified pressure projection system took no more than 10% longer than the\ncorresponding free surface solve. We demonstrate the method's effectiveness and\nflexibility by incorporating it into commercial fluid animation software and\nusing it to generate a variety of dynamic bubble scenarios showcasing glugging\neffects, viscous and inviscid bubbles, interactions with irregularly-shaped and\nmoving solid boundaries, and surface tension effects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 15:47:15 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Goldade", "Ryan", ""], ["Batty", "Christopher", ""]]}, {"id": "1711.11585", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan\n  Catanzaro", "title": "High-Resolution Image Synthesis and Semantic Manipulation with\n  Conditional GANs", "comments": "v2: CVPR camera ready, adding more results for edge-to-photo examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for synthesizing high-resolution photo-realistic\nimages from semantic label maps using conditional generative adversarial\nnetworks (conditional GANs). Conditional GANs have enabled a variety of\napplications, but the results are often limited to low-resolution and still far\nfrom realistic. In this work, we generate 2048x1024 visually appealing results\nwith a novel adversarial loss, as well as new multi-scale generator and\ndiscriminator architectures. Furthermore, we extend our framework to\ninteractive visual manipulation with two additional features. First, we\nincorporate object instance segmentation information, which enables object\nmanipulations such as removing/adding objects and changing the object category.\nSecond, we propose a method to generate diverse results given the same input,\nallowing users to edit the object appearance interactively. Human opinion\nstudies demonstrate that our method significantly outperforms existing methods,\nadvancing both the quality and the resolution of deep image synthesis and\nediting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:57:21 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 17:55:56 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Zhu", "Jun-Yan", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1711.11586", "submitter": "Richard Zhang", "authors": "Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A.\n  Efros, Oliver Wang, Eli Shechtman", "title": "Toward Multimodal Image-to-Image Translation", "comments": "NIPS 2017 Final paper. v4 updated acknowledgment. Website:\n  https://junyanz.github.io/BicycleGAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image-to-image translation problems are ambiguous, as a single input\nimage may correspond to multiple possible outputs. In this work, we aim to\nmodel a \\emph{distribution} of possible outputs in a conditional generative\nmodeling setting. The ambiguity of the mapping is distilled in a\nlow-dimensional latent vector, which can be randomly sampled at test time. A\ngenerator learns to map the given input, combined with this latent code, to the\noutput. We explicitly encourage the connection between output and the latent\ncode to be invertible. This helps prevent a many-to-one mapping from the latent\ncode to the output during training, also known as the problem of mode collapse,\nand produces more diverse results. We explore several variants of this approach\nby employing different training objectives, network architectures, and methods\nof injecting the latent code. Our proposed method encourages bijective\nconsistency between the latent encoding and output modes. We present a\nsystematic comparison of our method and other variants on both perceptual\nrealism and diversity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:59:01 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 02:56:04 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 23:37:28 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 00:29:43 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Zhang", "Richard", ""], ["Pathak", "Deepak", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""]]}]