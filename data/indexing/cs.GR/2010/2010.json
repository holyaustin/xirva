[{"id": "2010.00004", "submitter": "Soraia Musse", "authors": "Estevso Testa, Rodrigo C. Barros, Soraia Raupp Musse", "title": "CrowdEst: A Method for Estimating (and not Simulating) Crowd Evacuation\n  Parameters in Generic Environments", "comments": null, "journal-ref": "THE VISUAL COMPUTER, 2019", "doi": "10.1007/s00371-019-01684-9", "report-no": null, "categories": "cs.MA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evacuation plans have been historically used as a safety measure for the\nconstruction of buildings. The existing crowd simulators require fully-modeled\n3D environments and enough time to prepare and simulate scenarios, where the\ndistribution and behavior of the crowd needs to be controlled. In addition, its\npopulation, routes or even doors and passages may change, so the 3D model and\nconfigurations have to be updated accordingly. This is a time-consuming task\nthat commonly has to be addressed within the crowd simulators. With that in\nmind, we present a novel approach to estimate the resulting data of a given\nevacuation scenario without actually simulating it. For such, we divide the\nenvironment into smaller modular rooms with different configurations, in a\ndivide-and-conquer fashion. Next, we train an artificial neural network to\nestimate all required data regarding the evacuation of a single room. After\ncollecting the estimated data from each room, we develop a heuristic capable of\naggregating per-room information so the full environment can be properly\nevaluated. Our method presents an average error of 5% when compared to\nevacuation time in a real-life environment. Our crowd estimator approach has\nseveral advantages, such as not requiring to model the 3D environment, nor\nlearning how to use and configure a crowd simulator, which means any user can\neasily use it. Furthermore, the computational time to estimate evacuation data\n(inference time) is virtually zero, which is much better even when compared to\nthe best-case scenario in a real-time crowd simulator.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:42:45 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Testa", "Estevso", ""], ["Barros", "Rodrigo C.", ""], ["Musse", "Soraia Raupp", ""]]}, {"id": "2010.00450", "submitter": "Mojtaba Bemana", "authors": "Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel", "title": "X-Fields: Implicit Neural View-, Light- and Time-Image Interpolation", "comments": "15 pages, 19 figures, accepted at SIGGRAPH Asia 2020, project\n  webpage: https://xfields.mpi-inf.mpg.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest to represent an X-Field -a set of 2D images taken across different\nview, time or illumination conditions, i.e., video, light field, reflectance\nfields or combinations thereof-by learning a neural network (NN) to map their\nview, time or light coordinates to 2D images. Executing this NN at new\ncoordinates results in joint view, time or light interpolation. The key idea to\nmake this workable is a NN that already knows the \"basic tricks\" of graphics\n(lighting, 3D projection, occlusion) in a hard-coded and differentiable form.\nThe NN represents the input to that rendering as an implicit map, that for any\nview, time, or light coordinate and for any pixel can quantify how it will move\nif view, time or light coordinates change (Jacobian of pixel position with\nrespect to view, time, illumination, etc.). Our X-Field representation is\ntrained for one scene within minutes, leading to a compact set of trainable\nparameters and hence real-time navigation in view, time and illumination.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:46:00 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Bemana", "Mojtaba", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2010.00560", "submitter": "Zhengfei Kuang", "authors": "Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming He, Karl Bladin and\n  Hao Li", "title": "Dynamic Facial Asset and Rig Generation from a Single Scan", "comments": "18 pages, 25 figures, ACM SIGGRAPH Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The creation of high-fidelity computer-generated (CG) characters used in film\nand gaming requires intensive manual labor and a comprehensive set of facial\nassets to be captured with complex hardware, resulting in high cost and long\nproduction cycles. In order to simplify and accelerate this digitization\nprocess, we propose a framework for the automatic generation of high-quality\ndynamic facial assets, including rigs which can be readily deployed for artists\nto polish. Our framework takes a single scan as input to generate a set of\npersonalized blendshapes, dynamic and physically-based textures, as well as\nsecondary facial components (e.g., teeth and eyeballs). Built upon a facial\ndatabase consisting of pore-level details, with over $4,000$ scans of varying\nexpressions and identities, we adopt a self-supervised neural network to learn\npersonalized blendshapes from a set of template expressions. We also model the\njoint distribution between identities and expressions, enabling the inference\nof the full set of personalized blendshapes with dynamic appearances from a\nsingle neutral input scan. Our generated personalized face rig assets are\nseamlessly compatible with cutting-edge industry pipelines for facial animation\nand rendering. We demonstrate that our framework is robust and effective by\ninferring on a wide range of novel subjects, and illustrate compelling\nrendering results while animating faces with generated customized\nphysically-based dynamic textures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 17:25:25 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 20:12:53 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Li", "Jiaman", ""], ["Kuang", "Zhengfei", ""], ["Zhao", "Yajie", ""], ["He", "Mingming", ""], ["Bladin", "Karl", ""], ["Li", "Hao", ""]]}, {"id": "2010.01256", "submitter": "Bernhard Jenny", "authors": "Bernhard Jenny, Magnus Heitzler, Dilpreet Singh, Marianna\n  Farmakis-Serebryakova, Jeffery Chieh Liu, Lorenz Hurni", "title": "Cartographic Relief Shading with Neural Networks", "comments": "Accepted preprint for IEEE InfoVis 2020 to appear in IEEE\n  Transactions on Visualization and Computer Graphics, including supplementary\n  materials. 16 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shaded relief is an effective method for visualising terrain on topographic\nmaps, especially when the direction of illumination is adapted locally to\nemphasise individual terrain features. However, digital shading algorithms are\nunable to fully match the expressiveness of hand-crafted masterpieces, which\nare created through a laborious process by highly specialised cartographers. We\nreplicate hand-drawn relief shading using U-Net neural networks. The deep\nneural networks are trained with manual shaded relief images of the Swiss\ntopographic map series and terrain models of the same area. The networks\ngenerate shaded relief that closely resemble hand-drawn shaded relief art. The\nnetworks learn essential design principles from manual relief shading such as\nremoving unnecessary terrain details, locally adjusting the illumination\ndirection to accentuate individual terrain features, and varying brightness to\nemphasise larger landforms. Neural network shadings are generated from digital\nelevation models in a few seconds, and a study with 18 relief shading experts\nfound that they are of high quality.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 02:03:32 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jenny", "Bernhard", ""], ["Heitzler", "Magnus", ""], ["Singh", "Dilpreet", ""], ["Farmakis-Serebryakova", "Marianna", ""], ["Liu", "Jeffery Chieh", ""], ["Hurni", "Lorenz", ""]]}, {"id": "2010.01391", "submitter": "Dan Reznik", "authors": "Dan Reznik and Ronaldo Garcia", "title": "An Infinite, Converging, Sequence of Brocard Porisms", "comments": "20 pages, 6 figures, 2 tables, 4 video links", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brocard porism is a known 1d family of triangles inscribed in a circle\nand circumscribed about an ellipse. Remarkably, the Brocard angle is invariant\nand the Brocard points are stationary at the foci of the ellipse. In this paper\nwe show that a certain derived triangle spawns off a second, smaller, Brocard\nporism so that repeating this calculation produces an infinite, converging\nsequence of porisms. We also show that this sequence is embedded in a\ncontinuous family of porisms.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 17:06:39 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 11:54:43 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 16:36:04 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Reznik", "Dan", ""], ["Garcia", "Ronaldo", ""]]}, {"id": "2010.01615", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya and Nicholas Rewkowski and Pooja Guhan and Niall\n  L. Williams and Trisha Mittal and Aniket Bera and Dinesh Manocha", "title": "Generating Emotive Gaits for Virtual Agents Using Affect-Based\n  Autoregression", "comments": "To be published in proceedings of the IEEE International Symposium on\n  Mixed and Augmented Reality 2020. 12 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel autoregression network to generate virtual agents that\nconvey various emotions through their walking styles or gaits. Given the 3D\npose sequences of a gait, our network extracts pertinent movement features and\naffective features from the gait. We use these features to synthesize\nsubsequent gaits such that the virtual agents can express and transition\nbetween emotions represented as combinations of happy, sad, angry, and neutral.\nWe incorporate multiple regularizations in the training of our network to\nsimultaneously enforce plausible movements and noticeable emotions on the\nvirtual agents. We also integrate our approach with an AR environment using a\nMicrosoft HoloLens and can generate emotive gaits at interactive rates to\nincrease the social presence. We evaluate how human observers perceive both the\nnaturalness and the emotions from the generated gaits of the virtual agents in\na web-based study. Our results indicate around 89% of the users found the\nnaturalness of the gaits satisfactory on a five-point Likert scale, and the\nemotions they perceived from the virtual agents are statistically similar to\nthe intended emotions of the virtual agents. We also use our network to augment\nexisting gait datasets with emotive gaits and will release this augmented\ndataset for future research in emotion prediction and emotive gait synthesis.\nOur project website is available at https://gamma.umd.edu/gen_emotive_gaits/.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 16:05:40 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Rewkowski", "Nicholas", ""], ["Guhan", "Pooja", ""], ["Williams", "Niall L.", ""], ["Mittal", "Trisha", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2010.01679", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R and Ayush Tewari and Hans-Peter Seidel and Mohamed\n  Elgharib and Christian Theobalt", "title": "Learning Complete 3D Morphable Face Models from Images and Videos", "comments": "Project Page - https://gvv.mpi-inf.mpg.de/projects/LeMoMo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most 3D face reconstruction methods rely on 3D morphable models, which\ndisentangle the space of facial deformations into identity geometry,\nexpressions and skin reflectance. These models are typically learned from a\nlimited number of 3D scans and thus do not generalize well across different\nidentities and expressions. We present the first approach to learn complete 3D\nmodels of face identity geometry, albedo and expression just from images and\nvideos. The virtually endless collection of such data, in combination with our\nself-supervised learning-based approach allows for learning face models that\ngeneralize beyond the span of existing approaches. Our network design and loss\nfunctions ensure a disentangled parameterization of not only identity and\nalbedo, but also, for the first time, an expression basis. Our method also\nallows for in-the-wild monocular reconstruction at test time. We show that our\nlearned models better generalize and lead to higher quality image-based\nreconstructions than existing approaches.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:51:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["R", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Seidel", "Hans-Peter", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2010.01775", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Zexiang Xu, Tiancheng Sun, Alexandr Kuznetsov, Mark Meyer,\n  Henrik Wann Jensen, Hao Su, Ravi Ramamoorthi", "title": "Photon-Driven Neural Path Guiding", "comments": "Keywords: computer graphics, rendering, path tracing, path guiding,\n  machine learning, neural networks, denoising, reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Monte Carlo path tracing is a simple and effective algorithm to\nsynthesize photo-realistic images, it is often very slow to converge to\nnoise-free results when involving complex global illumination. One of the most\nsuccessful variance-reduction techniques is path guiding, which can learn\nbetter distributions for importance sampling to reduce pixel noise. However,\nprevious methods require a large number of path samples to achieve reliable\npath guiding. We present a novel neural path guiding approach that can\nreconstruct high-quality sampling distributions for path guiding from a sparse\nset of samples, using an offline trained neural network. We leverage photons\ntraced from light sources as the input for sampling density reconstruction,\nwhich is highly effective for challenging scenes with strong global\nillumination. To fully make use of our deep neural network, we partition the\nscene space into an adaptive hierarchical grid, in which we apply our network\nto reconstruct high-quality sampling distributions for any local region in the\nscene. This allows for highly efficient path guiding for any path bounce at any\nlocation in path tracing. We demonstrate that our photon-driven neural path\nguiding method can generalize well on diverse challenging testing scenes that\nare not seen in training. Our approach achieves significantly better rendering\nresults of testing scenes than previous state-of-the-art path guiding methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 04:54:01 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Zhu", "Shilin", ""], ["Xu", "Zexiang", ""], ["Sun", "Tiancheng", ""], ["Kuznetsov", "Alexandr", ""], ["Meyer", "Mark", ""], ["Jensen", "Henrik Wann", ""], ["Su", "Hao", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2010.01944", "submitter": "Selma Rizvic", "authors": "Selma Rizvic, Dusanka Boskovic, Fabio Bruno, Barbara Davidde\n  Petriaggi, Sanda Sljivo, Marco Cozza", "title": "Actors in VR storytelling", "comments": "Pre-print version", "journal-ref": null, "doi": "10.1109/VS-Games.2019.8864520", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Reality (VR) storytelling enhances the immersion of users into\nvirtual environments (VE). Its use in virtual cultural heritage presentations\nhelps the revival of the genius loci (the spirit of the place) of cultural\nmonuments. This paper aims to show that the use of actors in VR storytelling\nadds to the quality of user experience and improves the edutainment value of\nvirtual cultural heritage applications. We will describe the Baiae dry visit\napplication which takes us to a time travel in the city considered by the Roman\nelite as \"Little Rome (Pusilla Roma)\" and presently is only partially preserved\nunder the sea.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:14:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Rizvic", "Selma", ""], ["Boskovic", "Dusanka", ""], ["Bruno", "Fabio", ""], ["Petriaggi", "Barbara Davidde", ""], ["Sljivo", "Sanda", ""], ["Cozza", "Marco", ""]]}, {"id": "2010.01959", "submitter": "Divya Banesh", "authors": "Divya Banesh, Li-Ta Lo, Patrick Kilian, Fan Guo, Bernd Hamann", "title": "Topological Analysis of Magnetic Reconnection in Kinetic Plasma\n  Simulations", "comments": "To to published in IEEE Transactions on Visualization and Computer\n  Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.plasm-ph cs.GR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic reconnection is a ubiquitous plasma process in which oppositely\ndirected magnetic field lines break and rejoin, resulting in a change of the\nmagnetic field topology. Reconnection generates magnetic islands: regions\nenclosed by magnetic field lines and separated by reconnection points. Proper\nidentification of these features is important to understand particle\nacceleration and overall behavior of plasma. We present a contour-tree based\nvisualization for robust and objective identification of islands and\nreconnection points in two-dimensional (2D) magnetic reconnection simulations.\nThe application of this visualization to a simple simulation has revealed a\nphysical phenomenon previously not reported, resulting in a more comprehensive\nunderstanding of magnetic reconnection.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 20:46:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Banesh", "Divya", ""], ["Lo", "Li-Ta", ""], ["Kilian", "Patrick", ""], ["Guo", "Fan", ""], ["Hamann", "Bernd", ""]]}, {"id": "2010.02015", "submitter": "Priyadarshini Kumari", "authors": "Praseedha Krishnan Aniyath, Sreeni Kamalalayam Gopalan, Priyadarshini\n  K and Subhasis Chaudhuri", "title": "Combined Hapto-Visual and Auditory Rendering of Cultural Heritage\n  Objects", "comments": "Accepted to ACCVw 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a multi-modal rendering framework comprising of\nhapto-visual and auditory data. The prime focus is to haptically render point\ncloud data representing virtual 3-D models of cultural significance and also to\nhandle their affine transformations. Cultural heritage objects could\npotentially be very large and one may be required to render the object at\nvarious scales of details. Further, surface effects such as texture and\nfriction are incorporated in order to provide a realistic haptic perception to\nthe users. Moreover, the proposed framework includes an appropriate sound\nsynthesis to bring out the acoustic properties of the object. It also includes\na graphical user interface with varied options such as choosing the desired\norientation of 3-D objects and selecting the desired level of spatial\nresolution adaptively at runtime. A fast, point proxy-based haptic rendering\ntechnique is proposed with proxy update loop running 100 times faster than the\nrequired haptic update frequency of 1 kHz. The surface properties are\nintegrated in the system by applying a bilateral filter on the depth data of\nthe virtual 3-D models. Position dependent sound synthesis is incorporated with\nthe incorporation of appropriate audio clips.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:43:46 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Aniyath", "Praseedha Krishnan", ""], ["Gopalan", "Sreeni Kamalalayam", ""], ["K", "Priyadarshini", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.02319", "submitter": "Jaya Sreevalsan-Nair", "authors": "Jaya Sreevalsan-Nair and Komal Dadhich and Siri Chandana Daggubati", "title": "Tensor Fields for Data Extraction from Chart Images: Bar Charts and\n  Scatter Plots", "comments": "17 pages, 7 figures, 1 table, peer-reviewed and accepted for\n  publication in \"Topological Methods in Visualization: Theory, Software and\n  Applications,\" Ingrid Hotz, Talha Bin Masood, Filip Sadlo, and Julien Tierny\n  (Eds.). Springer-Verlag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Charts are an essential part of both graphicacy (graphical literacy), and\nstatistical literacy. As chart understanding has become increasingly relevant\nin data science, automating chart analysis by processing raster images of the\ncharts has become a significant problem. Automated chart reading involves data\nextraction and contextual understanding of the data from chart images. In this\npaper, we perform the first step of determining the computational model of\nchart images for data extraction for selected chart types, namely, bar charts,\nand scatter plots. We demonstrate the use of positive semidefinite second-order\ntensor fields as an effective model. We identify an appropriate tensor field as\nthe model and propose a methodology for the use of its degenerate point\nextraction for data extraction from chart images. Our results show that tensor\nvoting is effective for data extraction from bar charts and scatter plots, and\nhistograms, as a special case of bar charts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 20:19:40 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Sreevalsan-Nair", "Jaya", ""], ["Dadhich", "Komal", ""], ["Daggubati", "Siri Chandana", ""]]}, {"id": "2010.02392", "submitter": "Karl Willis", "authors": "Karl D.D. Willis, Yewen Pu, Jieliang Luo, Hang Chu, Tao Du, Joseph G.\n  Lambourne, Armando Solar-Lezama, Wojciech Matusik", "title": "Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD\n  Construction from Human Design Sequences", "comments": "Accepted to SIGGRAPH 2021; data/code available at\n  https://github.com/AutodeskAILab/Fusion360GalleryDataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric computer-aided design (CAD) is a standard paradigm used to design\nmanufactured objects, where a 3D shape is represented as a program supported by\nthe CAD software. Despite the pervasiveness of parametric CAD and a growing\ninterest from the research community, currently there does not exist a dataset\nof realistic CAD models in a concise programmatic form. In this paper we\npresent the Fusion 360 Gallery, consisting of a simple language with just the\nsketch and extrude modeling operations, and a dataset of 8,625 human design\nsequences expressed in this language. We also present an interactive\nenvironment called the Fusion 360 Gym, which exposes the sequential\nconstruction of a CAD program as a Markov decision process, making it amendable\nto machine learning approaches. As a use case for our dataset and environment,\nwe define the CAD reconstruction task of recovering a CAD program from a target\ngeometry. We report results of applying state-of-the-art methods of program\nsynthesis with neurally guided search on this task.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 23:18:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 03:58:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Willis", "Karl D. D.", ""], ["Pu", "Yewen", ""], ["Luo", "Jieliang", ""], ["Chu", "Hang", ""], ["Du", "Tao", ""], ["Lambourne", "Joseph G.", ""], ["Solar-Lezama", "Armando", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2010.02822", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari, Sreeni K.G and Subhasis Chaudhuri", "title": "Scalable Rendering of Variable Density Point Cloud Data", "comments": "Accepted to World Haptics Conference (WHC), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel proxy-based method of the adaptive haptic\nrendering of a variable density 3D point cloud data at different levels of\ndetail without pre-computing the mesh structure. We also incorporate features\nlike rotation, translation, and friction to provide a better realistic\nexperience to the user. A proxy-based rendering technique is used to avoid the\npop-through problem while rendering thin parts of the object. Instead of a\npoint proxy, a spherical proxy of a variable radius is used, which avoids the\nsinking of proxy during the haptic interaction of sparse data. The radius of\nthe proxy is adaptively varied depending upon the local density of the point\ndata using kernel bandwidth estimation. During the interaction, the proxy moves\nin small steps tangentially over the point cloud such that the new position\nalways minimizes the distance between the proxy and the haptic interaction\npoint (HIP). The raw point cloud data re-sampled in a regular 3D lattice of\nvoxels are loaded to the haptic space after proper smoothing to avoid aliasing\neffects. The rendering technique is validated with several subjects, and it is\nobserved that this functionality supplements the user's experience by allowing\nthe user to interact with an object at multiple resolutions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 15:37:08 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:39:32 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["G", "Sreeni K.", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.03169", "submitter": "Priyadarshini Kumari", "authors": "Sreeni K.G, Priyadarshini K, Praseedha A.K and Subhasis Chaudhuri", "title": "Haptic Rendering of Cultural Heritage Objects at Different Scales", "comments": "Accepted to EuroHaptics. arXiv admin note: text overlap with\n  arXiv:2010.02015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the issue of a virtual representation of objects of\ncultural heritage for haptic interaction. Our main focus is to provide haptic\naccess to artistic objects of any physical scale to the differently-abled\npeople. This is a low-cost system and, in conjunction with a stereoscopic\nvisual display, gives a better immersive experience even to the sighted\npersons. To achieve this, we propose a simple multilevel, proxy-based\nhapto-visual rendering technique for point cloud data, which includes the\nmuch-desired scalability feature which enables the users to change the scale of\nthe objects adaptively during the haptic interaction. For the proposed haptic\nrendering technique, the proxy updation loop runs at a rate 100 times faster\nthan the required haptic updation frequency of 1KHz. We observe that this\nfunctionality augments very well with the realism of the experience.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:32:47 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["G", "Sreeni K.", ""], ["K", "Priyadarshini", ""], ["K", "Praseedha A.", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.03936", "submitter": "Jonas Lukasczyk", "authors": "Jonas Lukasczyk, Christoph Garth, Matthew Larsen, Wito Engelke, Ingrid\n  Hotz, David Rogers, James Ahrens, and Ross Maciejewski", "title": "Cinema Darkroom: A Deferred Rendering Framework for Large-Scale Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework that fully leverages the advantages of a\ndeferred rendering approach for the interactive visualization of large-scale\ndatasets. Geometry buffers (G-Buffers) are generated and stored in situ, and\nshading is performed post hoc in an interactive image-based rendering front\nend. This decoupled framework has two major advantages. First, the G-Buffers\nonly need to be computed and stored once---which corresponds to the most\nexpensive part of the rendering pipeline. Second, the stored G-Buffers can\nlater be consumed in an image-based rendering front end that enables users to\ninteractively adjust various visualization parameters---such as the applied\ncolor map or the strength of ambient occlusion---where suitable choices are\noften not known a priori. This paper demonstrates the use of Cinema Darkroom on\nseveral real-world datasets, highlighting CD's ability to effectively decouple\nthe complexity and size of the dataset from its visualization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 12:51:44 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lukasczyk", "Jonas", ""], ["Garth", "Christoph", ""], ["Larsen", "Matthew", ""], ["Engelke", "Wito", ""], ["Hotz", "Ingrid", ""], ["Rogers", "David", ""], ["Ahrens", "James", ""], ["Maciejewski", "Ross", ""]]}, {"id": "2010.04077", "submitter": "Jakub Maksymilian Fober", "authors": "Jakub Maximilian Fober", "title": "Temporally-smooth Antialiasing and Lens Distortion with Rasterization\n  Map", "comments": "33 pages, 7 figures, 8 code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current GPU rasterization procedure is limited to narrow views in rectilinear\nperspective. While industries demand curvilinear perspective in wide-angle\nviews, like Virtual Reality and Virtual Film Production industry. This paper\ndelivers new rasterization method using industry-standard STMaps. Additionally\nnew antialiasing rasterization method is proposed, which outperforms MSAA in\nboth quality and performance. It is an improvement upon previous solutions\nfound in paper Perspective picture from Visual Sphere by yours truly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:59:52 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 16:02:31 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Fober", "Jakub Maximilian", ""]]}, {"id": "2010.04278", "submitter": "Ivan Sipiran", "authors": "Alexis Mendoza, Alexander Apaza, Ivan Sipiran, Cristian Lopez", "title": "Refinement of Predicted Missing Parts Enhance Point Cloud Completion", "comments": "11 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud completion is the task of predicting complete geometry from\npartial observations using a point set representation for a 3D shape. Previous\napproaches propose neural networks to directly estimate the whole point cloud\nthrough encoder-decoder models fed by the incomplete point set. By predicting\nthe complete model, the current methods compute redundant information because\nthe output also contains the known incomplete input geometry. This paper\nproposes an end-to-end neural network architecture that focuses on computing\nthe missing geometry and merging the known input and the predicted point cloud.\nOur method is composed of two neural networks: the missing part prediction\nnetwork and the merging-refinement network. The first module focuses on\nextracting information from the incomplete input to infer the missing geometry.\nThe second module merges both point clouds and improves the distribution of the\npoints. Our experiments on ShapeNet dataset show that our method outperforms\nthe state-of-the-art methods in point cloud completion. The code of our methods\nand experiments is available in\n\\url{https://github.com/ivansipiran/Refinement-Point-Cloud-Completion}.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:01:23 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Mendoza", "Alexis", ""], ["Apaza", "Alexander", ""], ["Sipiran", "Ivan", ""], ["Lopez", "Cristian", ""]]}, {"id": "2010.04595", "submitter": "Bo Yang", "authors": "Alex Trevithick, Bo Yang", "title": "GRF: Learning a General Radiance Field for 3D Scene Representation and\n  Rendering", "comments": "Code and data are available at: https://github.com/alextrevithick/GRF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet powerful implicit neural function that can represent\nand render arbitrarily complex 3D scenes in a single network only from 2D\nobservations. The function models 3D scenes as a general radiance field, which\ntakes a set of posed 2D images with camera poses and intrinsics as input,\nconstructs an internal representation for each 3D point of the scene, and\nrenders the corresponding appearance and geometry of any 3D point viewing from\nan arbitrary angle. The key to our approach is to explicitly integrate the\nprinciple of multi-view geometry to obtain the internal representations from\nobserved 2D views, such that the learned implicit representations empirically\nremain multi-view consistent. In addition, we introduce an effective neural\nmodule to learn general features for each pixel in 2D images, allowing the\nconstructed internal 3D representations to be general as well. Extensive\nexperiments demonstrate the superiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:21:43 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 06:33:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Trevithick", "Alex", ""], ["Yang", "Bo", ""]]}, {"id": "2010.04645", "submitter": "Emmanouil Potetsianakis", "authors": "Emmanuel Thomas, Emmanouil Potetsianakis, Thomas Stockhammer, Imed\n  Bouazizi, Mary-Luc Champel", "title": "MPEG Media Enablers For Richer XR Experiences", "comments": null, "journal-ref": "IBC (2020)", "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of immersive media applications, the requirements for the\nrepresentation and the consumption of such content has dramatically increased.\nThe ever-increasing size of the media asset combined with the stringent\nmotion-to-photon latency requirement makes the equation of a high quality of\nexperience for XR streaming services difficult to solve. The MPEG-I standards\naim at facilitating the wide deployment of immersive applications. This paper\ndescribes part 13, Video Decoding Interface, and part 14, Scene Description for\nMPEG Media of MPEG-I which address decoder management and the virtual scene\ncomposition, respectively. These new parts intend to make complex media\nrendering operations and hardware resources management hidden from the\napplication, hence lowering the barrier for XR application to become mainstream\nand accessible to XR experience developers and designers. Both parts are\nexpected to be published by ISO at the end of 2021.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:39:08 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Thomas", "Emmanuel", ""], ["Potetsianakis", "Emmanouil", ""], ["Stockhammer", "Thomas", ""], ["Bouazizi", "Imed", ""], ["Champel", "Mary-Luc", ""]]}, {"id": "2010.04865", "submitter": "Zhenyu Tang", "authors": "Zhenyu Tang, Hsien-Yu Meng, and Dinesh Manocha", "title": "Learning Acoustic Scattering Fields for Dynamic Interactive Sound\n  Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid sound propagation algorithm for interactive\napplications. Our approach is designed for dynamic scenes and uses a neural\nnetwork-based learned scattered field representation along with ray tracing to\ngenerate specular, diffuse, diffraction, and occlusion effects efficiently. We\nuse geometric deep learning to approximate the acoustic scattering field using\nspherical harmonics. We use a large 3D dataset for training, and compare its\naccuracy with the ground truth generated using an accurate wave-based solver.\nThe additional overhead of computing the learned scattered field at runtime is\nsmall and we demonstrate its interactive performance by generating plausible\nsound effects in dynamic scenes with diffraction and occlusion effects. We\ndemonstrate the perceptual benefits of our approach based on an audio-visual\nuser study.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:43:50 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:44:44 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Tang", "Zhenyu", ""], ["Meng", "Hsien-Yu", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2010.05066", "submitter": "Daniel Rebain", "authors": "Daniel Rebain, Baptiste Angles, Julien Valentin, Nicholas Vining, Jiju\n  Peethambaran, Shahram Izadi, Andrea Tagliasacchi", "title": "LSMAT Least Squares Medial Axis Transform", "comments": null, "journal-ref": "Computer Graphics Forum 38 (2019) 5-18", "doi": "10.1111/cgf.13599", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The medial axis transform has applications in numerous fields including\nvisualization, computer graphics, and computer vision. Unfortunately,\ntraditional medial axis transformations are usually brittle in the presence of\noutliers, perturbations and/or noise along the boundary of objects. To overcome\nthis limitation, we introduce a new formulation of the medial axis transform\nwhich is naturally robust in the presence of these artifacts. Unlike previous\nwork which has approached the medial axis from a computational geometry angle,\nwe consider it from a numerical optimization perspective. In this work, we\nfollow the definition of the medial axis transform as \"the set of maximally\ninscribed spheres\". We show how this definition can be formulated as a least\nsquares relaxation where the transform is obtained by minimizing a continuous\noptimization problem. The proposed approach is inherently parallelizable by\nperforming independant optimization of each sphere using Gauss-Newton, and its\nleast-squares form allows it to be significantly more robust compared to\ntraditional computational geometry approaches. Extensive experiments on 2D and\n3D objects demonstrate that our method provides superior results to the state\nof the art on both synthetic and real-data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 18:45:12 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Rebain", "Daniel", ""], ["Angles", "Baptiste", ""], ["Valentin", "Julien", ""], ["Vining", "Nicholas", ""], ["Peethambaran", "Jiju", ""], ["Izadi", "Shahram", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2010.05528", "submitter": "Fabien Danieau", "authors": "Ad\\`ele Colas, Florent Guiotte, Fabien Danieau, Fran\\c{c}ois Le Clerc,\n  Quentin Avril", "title": "Fat Pad Cages for Facial Posing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Fat Pad cages for posing facial meshes. It combines cage\nrepresentation and facial anatomical elements, and enables users with no\nartistic skill to quickly sketch realistic facial expressions. The model relies\non one or several cage(s) that deform(s) the mesh following the human fat pads\nmap. We propose a new function to filter Green Coordinates using geodesic\ndistances preventing global deformation while ensuring smooth deformations at\nthe borders. Lips, nostrils and eyelids are processed slightly differently to\nallow folding up and opening. Cages are automatically created and fit any new\nunknown facial mesh. To validate our approach, we present a user study\ncomparing our Fat Pad cages to regular Green Coordinates. Results show that Fat\nPad cages bring a significant improvement in reproducing existing facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 08:36:52 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Colas", "Ad\u00e8le", ""], ["Guiotte", "Florent", ""], ["Danieau", "Fabien", ""], ["Clerc", "Fran\u00e7ois Le", ""], ["Avril", "Quentin", ""]]}, {"id": "2010.05562", "submitter": "Linchao Bao", "authors": "Linchao Bao, Xiangkai Lin, Yajing Chen, Haoxian Zhang, Sheng Wang,\n  Xuefei Zhe, Di Kang, Haozhi Huang, Xinwei Jiang, Jue Wang, Dong Yu, Zhengyou\n  Zhang", "title": "High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies", "comments": "Code: https://github.com/tencent-ailab/hifi3dface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic system that can produce high-fidelity,\nphoto-realistic 3D digital human heads with a consumer RGB-D selfie camera. The\nsystem only needs the user to take a short selfie RGB-D video while rotating\nhis/her head, and can produce a high quality head reconstruction in less than\n30 seconds. Our main contribution is a new facial geometry modeling and\nreflectance synthesis procedure that significantly improves the\nstate-of-the-art. Specifically, given the input video a two-stage frame\nselection procedure is first employed to select a few high-quality frames for\nreconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM)\nfitting algorithm is applied to recover facial geometries from multiview RGB-D\ndata, which takes advantages of a powerful 3DMM basis constructed with\nextensive data generation and perturbation. Our 3DMM has much larger expressive\ncapacities than conventional 3DMM, allowing us to recover more accurate facial\ngeometry using merely linear basis. For reflectance synthesis, we present a\nhybrid approach that combines parametric fitting and CNNs to synthesize\nhigh-resolution albedo/normal maps with realistic hair/pore/wrinkle details.\nResults show that our system can produce faithful 3D digital human faces with\nextremely realistic details. The main code and the newly constructed 3DMM basis\nis publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:31:53 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 09:51:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bao", "Linchao", ""], ["Lin", "Xiangkai", ""], ["Chen", "Yajing", ""], ["Zhang", "Haoxian", ""], ["Wang", "Sheng", ""], ["Zhe", "Xuefei", ""], ["Kang", "Di", ""], ["Huang", "Haozhi", ""], ["Jiang", "Xinwei", ""], ["Wang", "Jue", ""], ["Yu", "Dong", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "2010.05655", "submitter": "Eloise Berson", "authors": "Elo\\\"ise Berson, Catherine Soladi\\'e, Nicolas Stoiber", "title": "Intuitive Facial Animation Editing Based On A Generative RNN Framework", "comments": null, "journal-ref": "Computer Graphics Forum 2020", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the last decades, the concern of producing convincing facial animation\nhas garnered great interest, that has only been accelerating with the recent\nexplosion of 3D content in both entertainment and professional activities. The\nuse of motion capture and retargeting has arguably become the dominant solution\nto address this demand. Yet, despite high level of quality and automation\nperformance-based animation pipelines still require manual cleaning and editing\nto refine raw results, which is a time- and skill-demanding process. In this\npaper, we look to leverage machine learning to make facial animation editing\nfaster and more accessible to non-experts. Inspired by recent image inpainting\nmethods, we design a generative recurrent neural network that generates\nrealistic motion into designated segments of an existing facial animation,\noptionally following user-provided guiding constraints. Our system handles\ndifferent supervised or unsupervised editing scenarios such as motion filling\nduring occlusions, expression corrections, semantic content modifications, and\nnoise filtering. We demonstrate the usability of our system on several\nanimation editing use cases.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 12:51:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Berson", "Elo\u00efse", ""], ["Soladi\u00e9", "Catherine", ""], ["Stoiber", "Nicolas", ""]]}, {"id": "2010.05907", "submitter": "Anand Bhattad", "authors": "Anand Bhattad and David A. Forsyth", "title": "Cut-and-Paste Neural Rendering", "comments": "Project page: https://anandbhattad.github.io/projects/reshading tldr:\n  Convincing cut-and-paste reshading by consistent image decomposition\n  inferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cut-and-paste methods take an object from one image and insert it into\nanother. Doing so often results in unrealistic looking images because the\ninserted object's shading is inconsistent with the target scene's shading.\nExisting reshading methods require a geometric and physical model of the\ninserted object, which is then rendered using environment parameters.\nAccurately constructing such a model only from a single image is beyond the\ncurrent understanding of computer vision. We describe an alternative procedure\n-- cut-and-paste neural rendering, to render the inserted fragment's shading\nfield consistent with the target scene. We use a Deep Image Prior (DIP) as a\nneural renderer trained to render an image with consistent image decomposition\ninferences. The resulting rendering from DIP should have an albedo consistent\nwith composite albedo; it should have a shading field that, outside the\ninserted fragment, is the same as the target scene's shading field; and\ncomposite surface normals are consistent with the final rendering's shading\nfield. The result is a simple procedure that produces convincing and realistic\nshading. Moreover, our procedure does not require rendered images or\nimage-decomposition from real images in the training or labeled annotations. In\nfact, our only use of simulated ground truth is our use of a pre-trained normal\nestimator. Qualitative results are strong, supported by a user study comparing\nagainst the state-of-the-art image harmonization baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 17:59:55 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bhattad", "Anand", ""], ["Forsyth", "David A.", ""]]}, {"id": "2010.06217", "submitter": "Lin Gao", "authors": "Lin Gao, Tong Wu, Yu-Jie Yuan, Ming-Xian Lin, Yu-Kun Lai, and Hao\n  Zhang", "title": "TM-NET: Deep Generative Networks for Textured Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TM-NET, a novel deep generative model for synthesizing textured\nmeshes in a part-aware manner. Once trained, the network can generate novel\ntextured meshes from scratch or predict textures for a given 3D mesh, without\nimage guidance. Plausible and diverse textures can be generated for the same\nmesh part, while texture compatibility between parts in the same shape is\nachieved via conditional generation. Specifically, our method produces texture\nmaps for individual shape parts, each as a deformable box, leading to a natural\nUV map with minimal distortion. The network separately embeds part geometry\n(via a PartVAE) and part texture (via a TextureVAE) into their respective\nlatent spaces, so as to facilitate learning texture probability distributions\nconditioned on geometry. We introduce a conditional autoregressive model for\ntexture generation, which can be conditioned on both part geometry and textures\nalready generated for other parts to achieve texture compatibility. To produce\nhigh-frequency texture details, our TextureVAE operates in a high-dimensional\nlatent space via dictionary-based vector quantization. We also exploit\ntransparencies in the texture as an effective means to model complex shape\nstructures including topological details. Extensive experiments demonstrate the\nplausibility, quality, and diversity of the textures and geometries generated\nby our network, while avoiding inconsistency issues that are common to novel\nview synthesis methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:01:20 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 04:21:49 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 07:09:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gao", "Lin", ""], ["Wu", "Tong", ""], ["Yuan", "Yu-Jie", ""], ["Lin", "Ming-Xian", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Hao", ""]]}, {"id": "2010.07498", "submitter": "Jun Fu", "authors": "Jun Fu and Wei Zhou and Zhibo Chen", "title": "Bayesian Spatio-Temporal Graph Convolutional Network for Traffic\n  Forecasting", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traffic forecasting, graph convolutional networks (GCNs), which model\ntraffic flows as spatio-temporal graphs, have achieved remarkable performance.\nHowever, existing GCN-based methods heuristically define the graph structure as\nthe physical topology of the road network, ignoring potential dependence of the\ngraph structure over traffic data. And the defined graph structure is\ndeterministic, which lacks investigation of uncertainty. In this paper, we\npropose a Bayesian Spatio-Temporal Graph Convolutional Network (BSTGCN) for\ntraffic prediction. The graph structure in our network is learned from the\nphysical topology of the road network and traffic data in an end-to-end manner,\nwhich discovers a more accurate description of the relationship among traffic\nflows. Moreover, a parametric generative model is proposed to represent the\ngraph structure, which enhances the generalization capability of GCNs. We\nverify the effectiveness of our method on two real-world datasets, and the\nexperimental results demonstrate that BSTGCN attains superior performance\ncompared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 03:41:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Fu", "Jun", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2010.08062", "submitter": "Przemyslaw Musialski", "authors": "Stefan Pillwein, Johanna K\\\"ubert, Florian Rist, Przemyslaw Musialski", "title": "Design and Fabrication of Elastic Geodesic Grid Structures", "comments": "11 pages, 15 figures", "journal-ref": null, "doi": "10.1145/3424630.3425412", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastic geodesic grids (EGG) are lightweight structures that can be easily\ndeployed to approximate designer provided free-form surfaces. In the initial\nconfiguration the grids are perfectly flat, during deployment, though,\ncurvature is induced to the structure, as grid elements bend and twist. Their\nlayout is found geometrically, it is based on networks of geodesic curves on\nfree-form design-surfaces. Generating a layout with this approach encodes an\nelasto-kinematic mechanism to the grid that creates the curved shape during\ndeployment. In the final state the grid can be fixed to supports and serve for\nall kinds of purposes like free-form sub-structures, paneling, sun and rain\nprotectors, pavilions, etc. However, so far these structures have only been\ninvestigated using small-scale desktop models. We investigate the scalability\nof such structures, presenting a medium sized model. It was designed by an\narchitecture student without expert knowledge on elastic structures or\ndifferential geometry, just using the elastic geodesic grids design-pipeline.\nWe further present a fabrication-process for EGG-models. They can be built\nquickly and with a small budget.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:10:08 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Pillwein", "Stefan", ""], ["K\u00fcbert", "Johanna", ""], ["Rist", "Florian", ""], ["Musialski", "Przemyslaw", ""]]}, {"id": "2010.08070", "submitter": "Przemyslaw Musialski", "authors": "Kurt Leimer, Przemyslaw Musialski", "title": "Reduced-Order Simulation of Flexible Meta-Materials", "comments": "10 pages, 8 figures, ACM Symposium on Computational Fabrication,\n  November 5--6, 2020", "journal-ref": null, "doi": "10.1145/3424630.3425411", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a reduced-order simulation and optimization technique for a type\nof digital materials which we denote as geometric meta-materials. They are\nplanar cellular structures, which can be fabricated in 2d and folded in 3d\nspace and thus well shaped into sophisticated 3d surfaces. They obtain their\nelasticity attributes mainly from the geometry of their cellular elements and\ntheir connections. While the physical properties of the base material (i.e.,\nthe physical substance) of course influence the behavior as well, our goal is\nto factor them out. However, the simulation of such complex structures still\ncomes with a high computational cost. We propose an approach to reduce this\ncomputational cost by abstracting the meso-structures and encoding the\nproperties of their elastic deformation behavior into a different set of\nmaterial parameters. We can thus obtain an approximation of the deformed\npattern by simulating a simplified version of the pattern using the computed\nmaterial parameters.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 23:31:41 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Leimer", "Kurt", ""], ["Musialski", "Przemyslaw", ""]]}, {"id": "2010.08276", "submitter": "Biao Zhang", "authors": "Biao Zhang, Peter Wonka", "title": "Training Data Generating Networks: Linking 3D Shapes and Few-Shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3d shape representation for 3d shape reconstruction from a\nsingle image. Rather than predicting a shape directly, we train a network to\ngenerate a training set which will be feed into another learning algorithm to\ndefine the shape. Training data generating networks establish a link between\nfew-shot learning and 3d shape analysis. We propose a novel meta-learning\nframework to jointly train the data generating network and other components. We\nimprove upon recent work on standard benchmarks for 3d shape reconstruction,\nbut our novel shape representation has many applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:52:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Zhang", "Biao", ""], ["Wonka", "Peter", ""]]}, {"id": "2010.08735", "submitter": "Eric Bruneton", "authors": "Eric Bruneton", "title": "Real-time High-Quality Rendering of Non-Rotating Black Holes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time method to render high-quality images of a non-rotating\nblack hole with an accretion disc and background stars. Our method is based on\nbeam tracing, but uses precomputed tables to find the intersections of each\ncurved light beam with the scene in constant time per pixel. It also uses a\nspecific texture filtering scheme to integrate the contribution of the light\nsources to each beam. Our method is simple to implement and achieves high frame\nrates.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 07:32:32 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Bruneton", "Eric", ""]]}, {"id": "2010.08788", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Paul Guerrero, Matt Fisher, Wilmot Li, Miloy J.Mitra", "title": "Discovering Pattern Structure Using Differentiable Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patterns, which are collections of elements arranged in regular or\nnear-regular arrangements, are an important graphic art form and widely used\ndue to their elegant simplicity and aesthetic appeal. When a pattern is encoded\nas a flat image without the underlying structure, manually editing the pattern\nis tedious and challenging as one has to both preserve the individual element\nshapes and their original relative arrangements. State-of-the-art deep learning\nframeworks that operate at the pixel level are unsuitable for manipulating such\npatterns. Specifically, these methods can easily disturb the shapes of the\nindividual elements or their arrangement, and thus fail to preserve the latent\nstructures of the input patterns. We present a novel differentiable compositing\noperator using pattern elements and use it to discover structures, in the form\nof a layered representation of graphical objects, directly from raw pattern\nimages. This operator allows us to adapt current deep learning based image\nmethods to effectively handle patterns. We evaluate our method on a range of\npatterns and demonstrate superiority in the context of pattern manipulations\nwhen compared against state-of-the-art\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 13:39:12 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Guerrero", "Paul", ""], ["Fisher", "Matt", ""], ["Li", "Wilmot", ""], ["Mitra", "Miloy J.", ""]]}, {"id": "2010.08888", "submitter": "Tiancheng Sun", "authors": "Tiancheng Sun, Zexiang Xu, Xiuming Zhang, Sean Fanello, Christoph\n  Rhemann, Paul Debevec, Yun-Ta Tsai, Jonathan T. Barron, Ravi Ramamoorthi", "title": "Light Stage Super-Resolution: Continuous High-Frequency Relighting", "comments": "Siggraph Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light stage has been widely used in computer graphics for the past two\ndecades, primarily to enable the relighting of human faces. By capturing the\nappearance of the human subject under different light sources, one obtains the\nlight transport matrix of that subject, which enables image-based relighting in\nnovel environments. However, due to the finite number of lights in the stage,\nthe light transport matrix only represents a sparse sampling on the entire\nsphere. As a consequence, relighting the subject with a point light or a\ndirectional source that does not coincide exactly with one of the lights in the\nstage requires interpolation and resampling the images corresponding to nearby\nlights, and this leads to ghosting shadows, aliased specularities, and other\nartifacts. To ameliorate these artifacts and produce better results under\narbitrary high-frequency lighting, this paper proposes a learning-based\nsolution for the \"super-resolution\" of scans of human faces taken from a light\nstage. Given an arbitrary \"query\" light direction, our method aggregates the\ncaptured images corresponding to neighboring lights in the stage, and uses a\nneural network to synthesize a rendering of the face that appears to be\nilluminated by a \"virtual\" light source at the query location. This neural\nnetwork must circumvent the inherent aliasing and regularity of the light stage\ndata that was used for training, which we accomplish through the use of\nregularized traditional interpolation methods within our network. Our learned\nmodel is able to produce renderings for arbitrary light directions that exhibit\nrealistic shadows and specular highlights, and is able to generalize across a\nwide variety of subjects.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 23:40:43 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sun", "Tiancheng", ""], ["Xu", "Zexiang", ""], ["Zhang", "Xiuming", ""], ["Fanello", "Sean", ""], ["Rhemann", "Christoph", ""], ["Debevec", "Paul", ""], ["Tsai", "Yun-Ta", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2010.09040", "submitter": "Alexandra Diehl PhD", "authors": "Alexandra Diehl (1) and Matthias Kraus (2) and Alfie Abdul-Rahman (3)\n  and Mennatallah El-Assady (2) and Benjamin Bach (4) and Robert Steven Laramee\n  (5) and Daniel Keim (2), Min Chen (6) ((1) University of Zurich, (2)\n  University of Konstanz, (3) King's College London, (4) Edinburgh University,\n  (5) University of Nottingham, (6) University of Oxford)", "title": "Studying Visualization Guidelines According to Grounded Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization guidelines, if defined properly, are invaluable to both\npractical applications and the theoretical foundation of visualization. In this\npaper, we present a collection of research activities for studying\nvisualization guidelines according to Grounded Theory (GT). We used the\ndiscourses at VisGuides, which is an online discussion forum for visualization\nguidelines, as the main data source for enabling data-driven research processes\nas advocated by the grounded theory methodology. We devised a categorization\nscheme focusing on observing how visualization guidelines were featured in\ndifferent threads and posts at VisGuides, and coded all 248 posts between\nSeptember 27, 2017 (when VisGuides was first launched) and March 13, 2019. To\ncomplement manual categorization and coding, we used text analysis and\nvisualization to help reveal patterns that may have been missed by the manual\neffort and summary statistics. To facilitate theoretical sampling and negative\ncase analysis, we made an in-depth analysis of the 148 posts (with both\nquestions and replies) related to a student assignment of a visualization\ncourse. Inspired by two discussion threads at VisGuides, we conducted two\ncontrolled empirical studies to collect further data to validate specific\nvisualization guidelines. Through these activities guided by grounded theory,\nwe have obtained some new findings about visualization guidelines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 17:35:40 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:13:20 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Diehl", "Alexandra", ""], ["Kraus", "Matthias", ""], ["Abdul-Rahman", "Alfie", ""], ["El-Assady", "Mennatallah", ""], ["Bach", "Benjamin", ""], ["Laramee", "Robert Steven", ""], ["Keim", "Daniel", ""], ["Chen", "Min", ""]]}, {"id": "2010.09714", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron", "title": "A Convenient Generalization of Schlick's Bias and Gain Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalization of Schlick's bias and gain functions -- simple\nparametric curve-shaped functions for inputs in [0, 1]. Our single function\nincludes both bias and gain as special cases, and is able to describe other\nsmooth and monotonic curves with variable degrees of asymmetry.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 03:25:55 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Barron", "Jonathan T.", ""]]}, {"id": "2010.09774", "submitter": "Nitin Agarwal", "authors": "Nitin Agarwal and M Gopi", "title": "GAMesh: Guided and Augmented Meshing for Deep Point Networks", "comments": "Accepted to 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new meshing algorithm called guided and augmented meshing,\nGAMesh, which uses a mesh prior to generate a surface for the output points of\na point network. By projecting the output points onto this prior and\nsimplifying the resulting mesh, GAMesh ensures a surface with the same topology\nas the mesh prior but whose geometric fidelity is controlled by the point\nnetwork. This makes GAMesh independent of both the density and distribution of\nthe output points, a common artifact in traditional surface reconstruction\nalgorithms. We show that such a separation of geometry from topology can have\nseveral advantages especially in single-view shape prediction, fair evaluation\nof point networks and reconstructing surfaces for networks which output sparse\npoint clouds. We further show that by training point networks with GAMesh, we\ncan directly optimize the vertex positions to generate adaptive meshes with\narbitrary topologies.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:23:53 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Agarwal", "Nitin", ""], ["Gopi", "M", ""]]}, {"id": "2010.09850", "submitter": "Renata Georgia Raidou", "authors": "Marwin Schindler, Hsiang-Yun Wu, Renata Georgia Raidou", "title": "The Anatomical Edutainer", "comments": null, "journal-ref": null, "doi": "10.1109/VIS47514.2020.00007", "report-no": null, "categories": "cs.HC cs.CY cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical visualizations (i.e., data representations by means of physical\nobjects) have been used for many centuries in medical and anatomical education.\nRecently, 3D printing techniques started also to emerge. Still, other medical\nphysicalizations that rely on affordable and easy-to-find materials are\nlimited, while smart strategies that take advantage of the optical properties\nof our physical world have not been thoroughly investigated. We propose the\nAnatomical Edutainer, a workflow to guide the easy, accessible, and affordable\ngeneration of physicalizations for tangible, interactive anatomical\nedutainment. The Anatomical Edutainer supports 2D printable and 3D foldable\nphysicalizations that change their visual properties (i.e., hues of the visible\nspectrum) under colored lenses or colored lights, to reveal distinct anatomical\nstructures through user interaction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:40:12 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Schindler", "Marwin", ""], ["Wu", "Hsiang-Yun", ""], ["Raidou", "Renata Georgia", ""]]}, {"id": "2010.09950", "submitter": "Saeed Ghorbani", "authors": "Saeed Ghorbani, Calden Wloka, Ali Etemad, Marcus A. Brubaker, Nikolaus\n  F. Troje", "title": "Probabilistic Character Motion Synthesis using a Hierarchical Deep\n  Latent Variable Model", "comments": null, "journal-ref": "Computer Graphics Forum, 39 (2002), 39-Issue 8", "doi": "10.1111/cgf.14116", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic framework to generate character animations based\non weak control signals, such that the synthesized motions are realistic while\nretaining the stochastic nature of human movement. The proposed architecture,\nwhich is designed as a hierarchical recurrent model, maps each sub-sequence of\nmotions into a stochastic latent code using a variational autoencoder extended\nover the temporal domain. We also propose an objective function which respects\nthe impact of each joint on the pose and compares the joint angles based on\nangular distance. We use two novel quantitative protocols and human qualitative\nassessment to demonstrate the ability of our model to generate convincing and\ndiverse periodic and non-periodic motion sequences without the need for strong\ncontrol signals.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 01:16:04 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Ghorbani", "Saeed", ""], ["Wloka", "Calden", ""], ["Etemad", "Ali", ""], ["Brubaker", "Marcus A.", ""], ["Troje", "Nikolaus F.", ""]]}, {"id": "2010.10178", "submitter": "Filippo Gabriele Prattic\\`o", "authors": "Alberto Cannav\\`o, Davide Calandra, F. Gabriele Prattic\\`o, Valentina\n  Gatteschi and Fabrizio Lamberti", "title": "An Evaluation Testbed for Locomotion in Virtual Reality", "comments": "This paper is accepted for inclusion in a future issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG). Copyright IEEE\n  2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3032440", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common operation performed in Virtual Reality (VR) environments is\nlocomotion. Although real walking can represent a natural and intuitive way to\nmanage displacements in such environments, its use is generally limited by the\nsize of the area tracked by the VR system (typically, the size of a room) or\nrequires expensive technologies to cover particularly extended settings. A\nnumber of approaches have been proposed to enable effective explorations in VR,\neach characterized by different hardware requirements and costs, and capable to\nprovide different levels of usability and performance. However, the lack of a\nwell-defined methodology for assessing and comparing available approaches makes\nit difficult to identify, among the various alternatives, the best solutions\nfor selected application domains. To deal with this issue, this paper\nintroduces a novel evaluation testbed which, by building on the outcomes of\nmany separate works reported in the literature, aims to support a comprehensive\nanalysis of the considered design space. An experimental protocol for\ncollecting objective and subjective measures is proposed, together with a\nscoring system able to rank locomotion approaches based on a weighted set of\nrequirements. Testbed usage is illustrated in a use case requesting to select\nthe technique to adopt in a given application scenario.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:21:15 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cannav\u00f2", "Alberto", ""], ["Calandra", "Davide", ""], ["Prattic\u00f2", "F. Gabriele", ""], ["Gatteschi", "Valentina", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2010.10557", "submitter": "Nitin Agarwal", "authors": "Tomer Weiss, Ilkay Yildiz, Nitin Agarwal, Esra Ataer-Cansizoglu,\n  Jae-Woo Choi", "title": "Image-Driven Furniture Style for Interactive 3D Scene Modeling", "comments": "Accepted to Pacific Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating realistic styled spaces is a complex task, which involves design\nknow-how for what furniture pieces go well together. Interior style follows\nabstract rules involving color, geometry and other visual elements. Following\nsuch rules, users manually select similar-style items from large repositories\nof 3D furniture models, a process which is both laborious and time-consuming.\nWe propose a method for fast-tracking style-similarity tasks, by learning a\nfurniture's style-compatibility from interior scene images. Such images contain\nmore style information than images depicting single furniture. To understand\nstyle, we train a deep learning network on a classification task. Based on\nimage embeddings extracted from our network, we measure stylistic compatibility\nof furniture. We demonstrate our method with several 3D model\nstyle-compatibility results, and with an interactive system for modeling\nstyle-consistent scenes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 18:19:28 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Weiss", "Tomer", ""], ["Yildiz", "Ilkay", ""], ["Agarwal", "Nitin", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Choi", "Jae-Woo", ""]]}, {"id": "2010.10726", "submitter": "Jesus Tordesillas Torres", "authors": "Jesus Tordesillas, Jonathan P. How", "title": "MINVO Basis: Finding Simplexes with Minimum Volume Enclosing Polynomial\n  Curves", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of finding the smallest $n$-simplex enclosing\na given $n^{\\text{th}}$-degree polynomial curve. Although the Bernstein and\nB-Spline polynomial bases provide feasible solutions to this problem, the\nsimplexes obtained by these bases are not the smallest possible, which leads to\nundesirably conservative results in many applications. We first prove that the\npolynomial basis that solves this problem (MINVO basis) also solves for the\n$n^\\text{th}$-degree polynomial curve with largest convex hull enclosed in a\ngiven $n$-simplex. Then, we present a formulation that is \\emph{independent} of\nthe $n$-simplex or $n^{\\text{th}}$-degree polynomial curve given. By using\nSum-Of-Squares (SOS) programming, branch and bound, and moment relaxations, we\nobtain high-quality feasible solutions for any $n\\in\\mathbb{N}$ and prove\nnumerical global optimality for $n=1,2,3$. The results obtained for $n=3$ show\nthat, for any given $3^{\\text{rd}}$-degree polynomial curve, the MINVO basis is\nable to obtain an enclosing simplex whose volume is $2.36$ and $254.9$ times\nsmaller than the ones obtained by the Bernstein and B-Spline bases,\nrespectively. When $n=7$, these ratios increase to $902.7$ and\n$2.997\\cdot10^{21}$, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 02:35:23 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 23:34:02 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 22:35:57 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Tordesillas", "Jesus", ""], ["How", "Jonathan P.", ""]]}, {"id": "2010.11488", "submitter": "Cheng Lin", "authors": "Cheng Lin, Lingjie Liu, Changjian Li, Leif Kobbelt, Bin Wang, Shiqing\n  Xin, Wenping Wang", "title": "SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": "10.1109/TVCG.2020.3032566", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting arbitrary 3D objects into constituent parts that are structurally\nmeaningful is a fundamental problem encountered in a wide range of computer\ngraphics applications. Existing methods for 3D shape segmentation suffer from\ncomplex geometry processing and heavy computation caused by using low-level\nfeatures and fragmented segmentation results due to the lack of global\nconsideration. We present an efficient method, called SEG-MAT, based on the\nmedial axis transform (MAT) of the input shape. Specifically, with the rich\ngeometrical and structural information encoded in the MAT, we are able to\ndevelop a simple and principled approach to effectively identify the various\ntypes of junctions between different parts of a 3D shape. Extensive evaluations\nand comparisons show that our method outperforms the state-of-the-art methods\nin terms of segmentation quality and is also one order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 07:15:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Cheng", ""], ["Liu", "Lingjie", ""], ["Li", "Changjian", ""], ["Kobbelt", "Leif", ""], ["Wang", "Bin", ""], ["Xin", "Shiqing", ""], ["Wang", "Wenping", ""]]}, {"id": "2010.11691", "submitter": "Jan Cejka", "authors": "Jan \\v{C}ejka, Fotis Liarokapis", "title": "Tackling problems of marker-based augmented reality under water", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-37191-3_11", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Underwater sites are a harsh environment for augmented reality applications.\nObstacles that must be battled include poor visibility conditions, difficult\nnavigation, and hard manipulation with devices under water. This chapter\nfocuses on the problem of localizing a device under water using markers. It\ndiscusses various filters that enhance and improve images recorded under water,\nand their impact on marker-based tracking. It presents various combinations of\n10 image improving algorithms and 4 marker detecting algorithms, and tests\ntheir performance in real situations. All solutions are designed to run\nreal-time on mobile devices to provide a solid basis for augmented reality.\nUsability of this solution is evaluated on locations in Mediterranean Sea. It\nis shown that image improving algorithms with carefully chosen parameters can\nreduce the problems with visibility under water and improve the detection of\nmarkers. The best results are obtained with marker detecting algorithms that\nare specifically designed for underwater environments.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:54:13 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["\u010cejka", "Jan", ""], ["Liarokapis", "Fotis", ""]]}, {"id": "2010.11995", "submitter": "Soraia Musse", "authors": "Rodolfo M. Favaretto, Roberto R. Santos, Marcio Ballotin, Paulo Knob,\n  Soraia R. Musse, Felipe Vilanova, Angelo B. Costa", "title": "Investigating Cultural Aspects in the Fundamental Diagram using\n  Convolutional Neural Networks and Simulation", "comments": "Computer Animation and Virtual Worlds, 2019", "journal-ref": null, "doi": "10.1002/cav.1899", "report-no": null, "categories": "cs.OH cs.CV cs.GR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study regarding group behavior in a controlled\nexperiment focused on differences in an important attribute that vary across\ncultures -- the personal spaces -- in two Countries: Brazil and Germany. In\norder to coherently compare Germany and Brazil evolutions with same population\napplying same task, we performed the pedestrian Fundamental Diagram experiment\nin Brazil, as performed in Germany. We use CNNs to detect and track people in\nvideo sequences. With this data, we use Voronoi Diagrams to find out the\nneighbor relation among people and then compute the walking distances to find\nout the personal spaces. Based on personal spaces analyses, we found out that\npeople behavior is more similar, in terms of their behaviours, in high dense\npopulations and vary more in low and medium densities. So, we focused our study\non cultural differences between the two Countries in low and medium densities.\nResults indicate that personal space analyses can be a relevant feature in\norder to understand cultural aspects in video sequences. In addition to the\ncultural differences, we also investigate the personality model in crowds,\nusing OCEAN. We also proposed a way to simulate the FD experiment from other\ncountries using the OCEAN psychological traits model as input. The simulated\ncountries were consistent with the literature.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:44:04 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Favaretto", "Rodolfo M.", ""], ["Santos", "Roberto R.", ""], ["Ballotin", "Marcio", ""], ["Knob", "Paulo", ""], ["Musse", "Soraia R.", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo B.", ""]]}, {"id": "2010.12930", "submitter": "Vasiliki Stamati", "authors": "Ioannis Fudos (1), Margarita Ntousia (1), Vasiliki Stamati (1),\n  Paschalis Charalampous (2), Theodora Kontodina (2), Ioannis Kostavelis (2),\n  Dimitrios Tzovaras (2), Leonardo Bilalis (3) ((1) Dept. of Computer Science\n  and Engineering, University of Ioannina, (2) Centre for Research and\n  Technology Hellas, Information Technologies Institute, (3) 3D Life)", "title": "A Characterization of 3D Printability", "comments": "To appear in Computer-Aided Design and Applications Journal\n  (http://www.cad-journal.net/open-access.html): I. Fudos, M. Ntousia, V.\n  Stamati, P. Charalampous, T. Kontodina, I. Kostavelis, D. Tzovaras and L.\n  Bilalis. A Characterization of 3D Printability. Accepted, Computer Aided\n  Design and Applications, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive manufacturing technologies are positioned to provide an\nunprecedented innovative transformation in how products are designed and\nmanufactured. Due to differences in the technical specifications of AM\ntechnologies, the final fabricated parts can vary significantly from the\noriginal CAD models, therefore raising issues regarding accuracy, surface\nfinish, robustness, mechanical properties, functional and geometrical\nconstraints. Various researchers have studied the correlation between AM\ntechnologies and design rules.\n  In this work we propose a novel approach to assessing the capability of a 3D\nmodel to be printed successfully (a.k.a printability) on a specific AM machine.\nThis is utilized by taking into consideration the model mesh complexity and\ncertain part characteristics. A printability score is derived for a model in\nreference to a specific 3D printing technology, expressing the probability of\nobtaining a robust and accurate end result for 3D printing on a specific AM\nmachine. The printability score can be used either to determine which 3D\ntechnology is more suitable for manufacturing a specific model or as a guide to\nredesign the model to ensure printability. We verify this framework by\nconducting 3D printing experiments for benchmark models which are printed on\nthree AM machines employing different technologies: Fused Deposition Modeling\n(FDM), Binder Jetting (3DP), and Material Jetting (Polyjet).\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 16:43:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fudos", "Ioannis", ""], ["Ntousia", "Margarita", ""], ["Stamati", "Vasiliki", ""], ["Charalampous", "Paschalis", ""], ["Kontodina", "Theodora", ""], ["Kostavelis", "Ioannis", ""], ["Tzovaras", "Dimitrios", ""], ["Bilalis", "Leonardo", ""]]}, {"id": "2010.13864", "submitter": "Vivien Cabannes", "authors": "Vivien Cabannes and Thomas Kerdreux and Louis Thiry", "title": "Diptychs of human and machine perceptions", "comments": "7 pages, 36 images", "journal-ref": "creativity workshop NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose visual creations that put differences in algorithms and humans\n\\emph{perceptions} into perspective. We exploit saliency maps of neural\nnetworks and visual focus of humans to create diptychs that are\nreinterpretations of an original image according to both machine and human\nattentions. Using those diptychs as a qualitative evaluation of perception, we\ndiscuss some crucial issues of current \\textit{task-oriented} artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 10:22:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cabannes", "Vivien", ""], ["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""]]}, {"id": "2010.14702", "submitter": "Eric Risser", "authors": "Eric Risser", "title": "Optimal Textures: Fast and Robust Texture Synthesis and Style Transfer\n  through Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a light-weight, high-quality texture synthesis algorithm\nthat easily generalizes to other applications such as style transfer and\ntexture mixing. We represent texture features through the deep neural\nactivation vectors within the bottleneck layer of an auto-encoder and frame the\ntexture synthesis problem as optimal transport between the activation values of\nthe image being synthesized and those of an exemplar texture. To find this\noptimal transport mapping, we utilize an N-dimensional probability density\nfunction (PDF) transfer process that iterates over multiple random rotations of\nthe PDF basis and matches the 1D marginal distributions across each dimension.\nThis achieves quality and flexibility on par with expensive back-propagation\nbased neural texture synthesis methods, but with the potential of achieving\ninteractive rates. We demonstrate that first order statistics offer a more\nrobust representation for texture than the second order statistics that are\nused today. We propose an extension of this algorithm that reduces the\ndimensionality of the neural feature space. We utilize a multi-scale\ncoarse-to-fine synthesis pyramid to capture and preserve larger image features;\nunify color and style transfer under one framework; and further augment this\nsystem with a novel masking scheme that re-samples and re-weights the feature\ndistribution for user-guided texture painting and targeted style transfer.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 02:19:07 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Risser", "Eric", ""]]}, {"id": "2010.15138", "submitter": "Jenny Wagner", "authors": "Fabian M. Schaller and Jenny Wagner and Sebastian C. Kapfer", "title": "papaya2: 2D Irreducible Minkowski Tensor computation", "comments": "5 pages, 3 figures, published in the Journal of Open Source Software,\n  code available at https://morphometry.org/software/papaya2/", "journal-ref": "Journal of Open Source Software, 5(54) (2020)", "doi": "10.21105/joss.02538", "report-no": null, "categories": "cs.GR astro-ph.IM math.MG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common challenge in scientific and technical domains is the quantitative\ndescription of geometries and shapes, e.g. in the analysis of microscope\nimagery or astronomical observation data. Frequently, it is desirable to go\nbeyond scalar shape metrics such as porosity and surface to volume ratios\nbecause the samples are anisotropic or because direction-dependent quantities\nsuch as conductances or elasticity are of interest. Minkowski Tensors are a\nsystematic family of versatile and robust higher-order shape descriptors that\nallow for shape characterization of arbitrary order and promise a path to\nsystematic structure-function relationships for direction-dependent properties.\nPapaya2 is a software to calculate 2D higher-order shape metrics with a library\ninterface, support for Irreducible Minkowski Tensors and interpolated marching\nsquares. Extensions to Matlab, JavaScript and Python are provided as well.\nWhile the tensor of inertia is computed by many tools, we are not aware of\nother open-source software which provides higher-rank shape characterization in\n2D.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:00:04 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Schaller", "Fabian M.", ""], ["Wagner", "Jenny", ""], ["Kapfer", "Sebastian C.", ""]]}, {"id": "2010.15399", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Yechen Liu, Lok Ming Lui", "title": "Free-boundary conformal parameterization of point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.NA math.CV math.DG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in 3D scanning technology, there has been a surge of\ninterest in the use of point clouds in science and engineering. To facilitate\nthe computations and analyses of point clouds, prior works have considered\nparameterizing them onto some simple planar domains with a fixed boundary shape\nsuch as a unit circle or a rectangle. However, the geometry of the fixed shape\nmay lead to some undesirable distortion in the parameterization. It is\ntherefore more natural to consider free-boundary conformal parameterizations of\npoint clouds, which minimize the local geometric distortion of the mapping\nwithout constraining the overall shape. In this work, we develop a\nfree-boundary conformal parameterization method for disk-type point clouds,\nwhich involves a novel approximation scheme of the point cloud Laplacian with\naccumulated cotangent weights together with a special treatment at the boundary\npoints. With the aid of the free-boundary conformal parameterization,\nhigh-quality point cloud meshing can be easily achieved. Furthermore, we show\nthat using the idea of conformal welding in complex analysis, the point cloud\nconformal parameterization can be computed in a divide-and-conquer manner.\nExperimental results are presented to demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 07:48:58 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:49:29 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Liu", "Yechen", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2010.15801", "submitter": "Henry Segerman", "authors": "R\\'emi Coulon, Elisabetta A. Matsumoto, Henry Segerman, Steve J.\n  Trettel", "title": "Ray-marching Thurston geometries", "comments": "139 pages, 198 figures and subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe algorithms that produce accurate real-time interactive in-space\nviews of the eight Thurston geometries using ray-marching. We give a\ntheoretical framework for our algorithms, independent of the geometry involved.\nIn addition to scenes within a geometry $X$, we also consider scenes within\nquotient manifolds and orbifolds $X / \\Gamma$. We adapt the Phong lighting\nmodel to non-euclidean geometries. The most difficult part of this is the\ncalculation of light intensity, which relates to the area density of geodesic\nspheres. We also give extensive practical details for each geometry.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:40:09 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Coulon", "R\u00e9mi", ""], ["Matsumoto", "Elisabetta A.", ""], ["Segerman", "Henry", ""], ["Trettel", "Steve J.", ""]]}, {"id": "2010.16404", "submitter": "Hanhan Li", "authors": "Hanhan Li, Ariel Gordon, Hang Zhao, Vincent Casser, Anelia Angelova", "title": "Unsupervised Monocular Depth Learning in Dynamic Scenes", "comments": "Accepted at 4th Conference on Robot Learning (CoRL 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for jointly training the estimation of depth, ego-motion,\nand a dense 3D translation field of objects relative to the scene, with\nmonocular photometric consistency being the sole source of supervision. We show\nthat this apparently heavily underdetermined problem can be regularized by\nimposing the following prior knowledge about 3D translation fields: they are\nsparse, since most of the scene is static, and they tend to be constant for\nrigid moving objects. We show that this regularization alone is sufficient to\ntrain monocular depth prediction models that exceed the accuracy achieved in\nprior work for dynamic scenes, including methods that require semantic input.\nCode is at\nhttps://github.com/google-research/google-research/tree/master/depth_and_motion_learning .\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:52:27 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 19:19:10 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Hanhan", ""], ["Gordon", "Ariel", ""], ["Zhao", "Hang", ""], ["Casser", "Vincent", ""], ["Angelova", "Anelia", ""]]}]