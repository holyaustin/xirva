[{"id": "2006.00084", "submitter": "Patrick Aleo", "authors": "Patrick D. Aleo, Simon J. Lock, Donna J. Cox, Stuart A. Levy, J. P.\n  Naiman, A. J. Christensen, Kalina Borkiewicz, Robert Patterson", "title": "Clustering-informed Cinematic Astrophysical Data Visualization with\n  Application to the Moon-forming Terrestrial Synestia", "comments": "19 pages, 16 figures, submitted to MNRAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific visualization tools are currently not optimized to create\ncinematic, production-quality representations of numerical data for the purpose\nof science communication. In our pipeline \\texttt{Estra}, we outline a\nstep-by-step process from a raw simulation into a finished render as a way to\nteach non-experts in the field of visualization how to achieve\nproduction-quality outputs on their own. We demonstrate feasibility of using\nthe visual effects software Houdini for cinematic astrophysical data\nvisualization, informed by machine learning clustering algorithms. To\ndemonstrate the capabilities of this pipeline, we used a post-impact,\nthermally-equilibrated Moon-forming synestia from \\cite{Lock18}. Our approach\naims to identify \"physically interpretable\" clusters, where clusters identified\nin an appropriate phase space (e.g. here we use a temperature-entropy\nphase-space) correspond to physically meaningful structures within the\nsimulation data. Clustering results can then be used to highlight these\nstructures by informing the color-mapping process in a simplified Houdini\nsoftware shading network, where dissimilar phase-space clusters are mapped to\ndifferent color values for easier visual identification. Cluster information\ncan also be used in 3D position space, via Houdini's Scene View, to aid in\nphysical cluster finding, simulation prototyping, and data exploration. Our\nclustering-based renders are compared to those created by the Advanced\nVisualization Lab (AVL) team for the full dome show \"Imagine the Moon\" as proof\nof concept. With \\texttt{Estra}, scientists have a tool to create their own\nproduction-quality, data-driven visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 21:18:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Aleo", "Patrick D.", ""], ["Lock", "Simon J.", ""], ["Cox", "Donna J.", ""], ["Levy", "Stuart A.", ""], ["Naiman", "J. P.", ""], ["Christensen", "A. J.", ""], ["Borkiewicz", "Kalina", ""], ["Patterson", "Robert", ""]]}, {"id": "2006.00114", "submitter": "Abdelaziz Lakhfif", "authors": "Abdelaziz Lakhfif", "title": "Design and Implementation of a Virtual 3D Educational Environment to\n  improve Deaf Education", "comments": "Proceedings of the 7th International Symposium ISKO-Maghreb Knowledge\n  Organization in the Perspective of Digital Humanities: Research &\n  Applications November 25th & 26th, 2018, pp. 201-205, Bejaia, Algeria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in NLP, knowledge representation and computer graphic technologies\ncan provide us insights into the development of educational tool for Deaf\npeople. Actual education materials and tools for deaf pupils present several\nproblems, since textbooks are designed to support normal students in the\nclassroom and most of them are not suitable for people with hearing\ndisabilities. Virtual Reality (VR) technologies appear to be a good tool and a\npromising framework in the education of pupils with hearing disabilities. In\nthis paper, we present a current research tasks surrounding the design and\nimplementation of a virtual 3D educational environment based on X3D and H-Anim\nstandards. The system generates and animates automatically Sign language\nsentence from a semantic representation that encode the whole meaning of the\nArabic input text. Some aspects and issues in Sign language generation will be\ndiscussed, including the model of Sign representation that facilitate reuse and\nreduces the time of Sign generation, conversion of semantic components to sign\nfeatures representation with regard to Sign language linguistics\ncharacteristics and how to generate realistic smooth gestural sequences using\nX3D content to performs transition between signs for natural-looking of\nanimated avatar. Sign language sentences were evaluated by Algerian native Deaf\npeople. The goal of the project is the development of a machine translation\nsystem from Arabic to Algerian Sign Language that can be used as educational\ntool for Deaf children in algerian primary schools.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:56:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lakhfif", "Abdelaziz", ""]]}, {"id": "2006.00190", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Rishabh Baghel, Ravi Kiran Sarvadevabhatla", "title": "OPAL-Net: A Generative Model for Part-based Object Layout Generation", "comments": "Code repository at https://github.com/atmacvit/opalnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose OPAL-Net, a novel hierarchical architecture for part-based layout\ngeneration of objects from multiple categories using a single unified model. We\nadopt a coarse-to-fine strategy involving semantically conditioned\nautoregressive generation of bounding box layouts and pixel-level part layouts\nfor objects. We use Graph Convolutional Networks, Deep Recurrent Networks along\nwith custom-designed Conditional Variational Autoencoders to enable flexible,\ndiverse and category-aware generation of object layouts. We train OPAL-Net on\nPASCAL-Parts dataset. The generated samples and corresponding evaluation scores\ndemonstrate the versatility of OPAL-Net compared to ablative variants and\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 06:25:19 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Baghel", "Rishabh", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2006.01047", "submitter": "Lin Gao", "authors": "Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, Hongbo Fu", "title": "Deep Generation of Face Images from Sketches", "comments": "Accepted to Siggraph 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep image-to-image translation techniques allow fast generation of\nface images from freehand sketches. However, existing solutions tend to overfit\nto sketches, thus requiring professional sketches or even edge maps as input.\nTo address this issue, our key idea is to implicitly model the shape space of\nplausible face images and synthesize a face image in this space to approximate\nan input sketch. We take a local-to-global approach. We first learn feature\nembeddings of key face components, and push corresponding parts of input\nsketches towards underlying component manifolds defined by the feature vectors\nof face component samples. We also propose another deep neural network to learn\nthe mapping from the embedded component features to realistic images with\nmulti-channel feature maps as intermediate results to improve the information\nflow. Our method essentially uses input sketches as soft constraints and is\nthus able to produce high-quality face images even from rough and/or incomplete\nsketches. Our tool is easy to use even for non-artists, while still supporting\nfine-grained control of shape details. Both qualitative and quantitative\nevaluations show the superior generation ability of our system to existing and\nalternative solutions. The usability and expressiveness of our system are\nconfirmed by a user study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:20:23 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 02:37:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Shu-Yu", ""], ["Su", "Wanchao", ""], ["Gao", "Lin", ""], ["Xia", "Shihong", ""], ["Fu", "Hongbo", ""]]}, {"id": "2006.01201", "submitter": "Mingyuan Meng", "authors": "Mingyuan Meng, Shaojun Liu", "title": "High-quality Panorama Stitching based on Asymmetric Bidirectional\n  Optical Flow", "comments": "Published at the 5th International Conference on Computational\n  Intelligence and Applications (ICCIA 2020)", "journal-ref": "2020 5th International Conference on Computational Intelligence\n  and Applications (ICCIA), Beijing, China, 2020, pp. 118-122", "doi": "10.1109/ICCIA49625.2020.00030", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a panorama stitching algorithm based on asymmetric\nbidirectional optical flow. This algorithm expects multiple photos captured by\nfisheye lens cameras as input, and then, through the proposed algorithm, these\nphotos can be merged into a high-quality 360-degree spherical panoramic image.\nFor photos taken from a distant perspective, the parallax among them is\nrelatively small, and the obtained panoramic image can be nearly seamless and\nundistorted. For photos taken from a close perspective or with a relatively\nlarge parallax, a seamless though partially distorted panoramic image can also\nbe obtained. Besides, with the help of Graphics Processing Unit (GPU), this\nalgorithm can complete the whole stitching process at a very fast speed:\ntypically, it only takes less than 30s to obtain a panoramic image of\n9000-by-4000 pixels, which means our panorama stitching algorithm is of high\nvalue in many real-time applications. Our code is available at\nhttps://github.com/MungoMeng/Panorama-OpticalFlow.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 18:54:11 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 08:15:12 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 00:35:50 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Meng", "Mingyuan", ""], ["Liu", "Shaojun", ""]]}, {"id": "2006.01524", "submitter": "Thomas M\\\"uller", "authors": "Thomas M\\\"uller, Fabrice Rousselle, Jan Nov\\'ak, Alexander Keller", "title": "Neural Control Variates", "comments": "To appear at SIGGRAPH Asia 2020. Updated with better loss function,\n  leading to better results. 19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose neural control variates (NCV) for unbiased variance reduction in\nparametric Monte Carlo integration. So far, the core challenge of applying the\nmethod of control variates has been finding a good approximation of the\nintegrand that is cheap to integrate. We show that a set of neural networks can\nface that challenge: a normalizing flow that approximates the shape of the\nintegrand and another neural network that infers the solution of the integral\nequation. We also propose to leverage a neural importance sampler to estimate\nthe difference between the original integrand and the learned control variate.\nTo optimize the resulting parametric estimator, we derive a theoretically\noptimal, variance-minimizing loss function, and propose an alternative,\ncomposite loss for stable online training in practice. When applied to light\ntransport simulation, neural control variates are capable of matching the\nstate-of-the-art performance of other unbiased approaches, while providing\nmeans to develop more performant, practical solutions. Specifically, we show\nthat the learned light-field approximation is of sufficient quality for\nhigh-order bounces, allowing us to omit the error correction and thereby\ndramatically reduce the noise at the cost of negligible visible bias.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 11:17:55 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 06:47:36 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["M\u00fcller", "Thomas", ""], ["Rousselle", "Fabrice", ""], ["Nov\u00e1k", "Jan", ""], ["Keller", "Alexander", ""]]}, {"id": "2006.01746", "submitter": "Michael Reed", "authors": "Steven L. Song, Weiqi Shi, Michael Reed", "title": "Accurate Face Rig Approximation with Deep Differential Subspace\n  Reconstruction", "comments": "12 pages, ACM Trans. on Graphics (Proceedings of SIGGRAPH 2020)", "journal-ref": null, "doi": "10.1145/3386569.3392491", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be suitable for film-quality animation, rigs for character deformation\nmust fulfill a broad set of requirements. They must be able to create highly\nstylized deformation, allow a wide variety of controls to permit artistic\nfreedom, and accurately reflect the design intent. Facial deformation is\nespecially challenging due to its nonlinearity with respect to the animation\ncontrols and its additional precision requirements, which often leads to highly\ncomplex face rigs that are not generalizable to other characters. This lack of\ngenerality creates a need for approximation methods that encode the deformation\nin simpler structures. We propose a rig approximation method that addresses\nthese issues by learning localized shape information in differential\ncoordinates and, separately, a subspace for mesh reconstruction. The use of\ndifferential coordinates produces a smooth distribution of errors in the\nresulting deformed surface, while the learned subspace provides constraints\nthat reduce the low frequency error in the reconstruction. Our method can\nreconstruct both face and body deformations with high fidelity and does not\nrequire a set of well-posed animation examples, as we demonstrate with a\nvariety of production characters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 16:18:55 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 03:41:48 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Song", "Steven L.", ""], ["Shi", "Weiqi", ""], ["Reed", "Michael", ""]]}, {"id": "2006.02138", "submitter": "Namwoo Kang", "authors": "Soyoung Yoo, Sunghee Lee, Seongsin Kim, Kwang Hyeon Hwang, Jong Ho\n  Park, Namwoo Kang", "title": "Integrating Deep Learning into CAD/CAE System: Generative Design and\n  Evaluation of 3D Conceptual Wheel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering design research integrating artificial intelligence (AI) into\ncomputer-aided design (CAD) and computer-aided engineering (CAE) is actively\nbeing conducted. This study proposes a deep learning-based CAD/CAE framework in\nthe conceptual design phase that automatically generates 3D CAD designs and\nevaluates their engineering performance. The proposed framework comprises seven\nstages: (1) 2D generative design, (2) dimensionality reduction, (3) design of\nexperiment in latent space, (4) CAD automation, (5) CAE automation, (6)\ntransfer learning, and (7) visualization and analysis. The proposed framework\nis demonstrated through a road wheel design case study and indicates that AI\ncan be practically incorporated into an end-use product design project.\nEngineers and industrial designers can jointly review a large number of\ngenerated 3D CAD models by using this framework along with the engineering\nperformance results estimated by AI and find conceptual design candidates for\nthe subsequent detailed design stage.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 16:48:13 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 13:36:30 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 05:35:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yoo", "Soyoung", ""], ["Lee", "Sunghee", ""], ["Kim", "Seongsin", ""], ["Hwang", "Kwang Hyeon", ""], ["Park", "Jong Ho", ""], ["Kang", "Namwoo", ""]]}, {"id": "2006.02532", "submitter": "Jing Ren", "authors": "Jing Ren, Simone Melzi, Maks Ovsjanikov, and Peter Wonka", "title": "MapTree: Recovering Multiple Solutions in the Space of Maps", "comments": "17 pages, 26 figures, published in ACM Transactions on Graphics\n  (Proc. SIGGRAPH Asia), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose an approach for computing multiple high-quality\nnear-isometric dense correspondences between a pair of 3D shapes. Our method is\nfully automatic and does not rely on user-provided landmarks or descriptors.\nThis allows us to analyze the full space of maps and extract multiple diverse\nand accurate solutions, rather than optimizing for a single optimal\ncorrespondence as done in most previous approaches. To achieve this, we propose\na compact tree structure based on the spectral map representation for encoding\nand enumerating possible rough initializations, and a novel efficient approach\nfor refining them to dense pointwise maps. This leads to a new method capable\nof both producing multiple high-quality correspondences across shapes and\nrevealing the symmetry structure of a shape without a priori information. In\naddition, we demonstrate through extensive experiments that our method is\nrobust and results in more accurate correspondences than state-of-the-art for\nshape matching and symmetry detection.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 12:47:27 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 05:07:55 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ren", "Jing", ""], ["Melzi", "Simone", ""], ["Ovsjanikov", "Maks", ""], ["Wonka", "Peter", ""]]}, {"id": "2006.02535", "submitter": "Hamid Laga", "authors": "Hamid Laga, Laurent Valentin Jospin, Farid Boussaid, Mohammed\n  Bennamoun", "title": "A Survey on Deep Learning Techniques for Stereo-based Depth Estimation", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.3032602", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from RGB images is a long-standing ill-posed problem, which\nhas been explored for decades by the computer vision, graphics, and machine\nlearning communities. Among the existing techniques, stereo matching remains\none of the most widely used in the literature due to its strong connection to\nthe human binocular system. Traditionally, stereo-based depth estimation has\nbeen addressed through matching hand-crafted features across multiple images.\nDespite the extensive amount of research, these traditional techniques still\nsuffer in the presence of highly textured areas, large uniform regions, and\nocclusions. Motivated by their growing success in solving various 2D and 3D\nvision problems, deep learning for stereo-based depth estimation has attracted\ngrowing interest from the community, with more than 150 papers published in\nthis area between 2014 and 2019. This new generation of methods has\ndemonstrated a significant leap in performance, enabling applications such as\nautonomous driving and augmented reality. In this article, we provide a\ncomprehensive survey of this new and continuously growing field of research,\nsummarize the most commonly used pipelines, and discuss their benefits and\nlimitations. In retrospect of what has been achieved so far, we also conjecture\nwhat the future may hold for deep learning-based stereo for depth estimation\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:09:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Laga", "Hamid", ""], ["Jospin", "Laurent Valentin", ""], ["Boussaid", "Farid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "2006.02759", "submitter": "Damien Chapon", "authors": "Loic Strafella, Damien Chapon", "title": "Boosting I/O and visualization for exascale era using Hercule: test case\n  on RAMSES", "comments": "9 pages, 8 figures. Proceedings of ASTRONUM 2019, July 2019, Paris,\n  France. Submitted to Journal of Physics Conference Series", "journal-ref": "Journal of Physics: Conference Series, Volume 1623, 14th Int.\n  Conf. on Numerical Modeling of Space Plasma Flows: ASTRONUM-2019", "doi": "10.1088/1742-6596/1623/1/012019", "report-no": null, "categories": "cs.DC astro-ph.CO astro-ph.GA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been clearly identified that I/O is one of the bottleneck to extend\napplication for the exascale era. New concepts such as 'in transit' and 'in\nsitu' visualization and analysis have been identified as key technologies to\ncircumvent this particular issue. A new parallel I/O and data management\nlibrary called Hercule, developed at CEA-DAM, has been integrated to Ramses, an\nAMR simulation code for self-gravitating fluids. Splitting the original Ramses\noutput format in Hercule database formats dedicated to either\ncheckpoints/restarts (HProt format) or post-processing (HDep format) not only\nimproved I/O performance and scalability of the Ramses code but also introduced\nmuch more flexibility in the simulation outputs to help astrophysicists prepare\ntheir DMP (Data Management Plan). Furthermore, the very lightweight and\npurpose-specific post-processing format (HDep) will significantly improve the\noverall performance of analysis and visualization tools such as PyMSES 5. An\nintroduction to the Hercule parallel I/O library as well as I/O benchmark\nresults will be discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:36:04 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Strafella", "Loic", ""], ["Chapon", "Damien", ""]]}, {"id": "2006.03743", "submitter": "Han Gong", "authors": "Han Gong, Luwen Yu, Stephen Westland", "title": "Simple Primary Colour Editing for Consumer Product Images", "comments": "This is a working paper (pre-print). Code available at\n  https://github.com/hangong/prod_recolor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple primary colour editing method for consumer product\nimages. We show that by using colour correction and colour blending, we can\nautomate the pain-staking colour editing task and save time for consumer colour\npreference researchers. To improve the colour harmony between the primary\ncolour and its complementary colours, our algorithm also tunes the other\ncolours in the image. Preliminary experiment has shown some promising results\ncompared with a state-of-the-art method and human editing.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 00:24:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Gong", "Han", ""], ["Yu", "Luwen", ""], ["Westland", "Stephen", ""]]}, {"id": "2006.03762", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang and Yang Liu and Xin Tong", "title": "Deep Octree-based CNNs with Output-Guided Skip Connections for 3D Shape\n  and Scene Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring complete and clean 3D shape and scene data is challenging due to\ngeometric occlusion and insufficient views during 3D capturing. We present a\nsimple yet effective deep learning approach for completing the input noisy and\nincomplete shapes or scenes. Our network is built upon the octree-based CNNs\n(O-CNN) with U-Net like structures, which enjoys high computational and memory\nefficiency and supports to construct a very deep network structure for 3D CNNs.\nA novel output-guided skip-connection is introduced to the network structure\nfor better preserving the input geometry and learning geometry prior from data\neffectively. We show that with these simple adaptions -- output-guided\nskip-connection and deeper O-CNN (up to 70 layers), our network achieves\nstate-of-the-art results in 3D shape completion and semantic scene computation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 02:51:26 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "2006.04096", "submitter": "Amir Zamir", "authors": "Amir Zamir, Alexander Sax, Teresa Yeo, O\\u{g}uzhan Kar, Nikhil\n  Cheerla, Rohan Suri, Zhangjie Cao, Jitendra Malik, Leonidas Guibas", "title": "Robust Learning Through Cross-Task Consistency", "comments": "CVPR 2020 (Oral). Project website, models, live demo at\n  http://consistency.epfl.ch/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual perception entails solving a wide set of tasks, e.g., object\ndetection, depth estimation, etc. The predictions made for multiple tasks from\nthe same image are not independent, and therefore, are expected to be\nconsistent. We propose a broadly applicable and fully computational method for\naugmenting learning with Cross-Task Consistency. The proposed formulation is\nbased on inference-path invariance over a graph of arbitrary tasks. We observe\nthat learning with cross-task consistency leads to more accurate predictions\nand better generalization to out-of-distribution inputs. This framework also\nleads to an informative unsupervised quantity, called Consistency Energy, based\non measuring the intrinsic consistency of the system. Consistency Energy\ncorrelates well with the supervised error (r=0.67), thus it can be employed as\nan unsupervised confidence metric as well as for detection of\nout-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on\nmultiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and\nthey benchmark cross-task consistency versus various baselines including\nconventional multi-task learning, cycle consistency, and analytical\nconsistency.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 09:24:33 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Zamir", "Amir", ""], ["Sax", "Alexander", ""], ["Yeo", "Teresa", ""], ["Kar", "O\u011fuzhan", ""], ["Cheerla", "Nikhil", ""], ["Suri", "Rohan", ""], ["Cao", "Zhangjie", ""], ["Malik", "Jitendra", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2006.04115", "submitter": "Moshe Eliasof", "authors": "Moshe Eliasof, Eran Treister", "title": "DiffGCN: Graph Convolutional Networks via Differential Operators and\n  Algebraic Multigrid Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have shown to be effective in handling\nunordered data like point clouds and meshes. In this work we propose novel\napproaches for graph convolution, pooling and unpooling, inspired from finite\ndifferences and algebraic multigrid frameworks. We form a parameterized\nconvolution kernel based on discretized differential operators, leveraging the\ngraph mass, gradient and Laplacian. This way, the parameterization does not\ndepend on the graph structure, only on the meaning of the network convolutions\nas differential operators. To allow hierarchical representations of the input,\nwe propose pooling and unpooling operations that are based on algebraic\nmultigrid methods, which are mainly used to solve partial differential\nequations on unstructured grids. To motivate and explain our method, we compare\nit to standard convolutional neural networks, and show their similarities and\nrelations in the case of a regular grid. Our proposed method is demonstrated in\nvarious experiments like classification and part-segmentation, achieving on par\nor better than state of the art results. We also analyze the computational cost\nof our method compared to other GCNs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:08:37 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:36:52 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Eliasof", "Moshe", ""], ["Treister", "Eran", ""]]}, {"id": "2006.04686", "submitter": "Fabio Rinaldi", "authors": "Rinaldi Fabio, Dolci Daniele", "title": "RBF Solver for Quaternions Interpolation", "comments": "23 pages, 21 figures, 2 txt files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we adapt the RBF Solver to work with quaternions by taking\nadvantage of their Lie Algebra and exponential map. This will allow to work\nwith quaternions as if they were normal vectors in R^3 and blend them in a very\nefficient way.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:36:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fabio", "Rinaldi", ""], ["Daniele", "Dolci", ""]]}, {"id": "2006.04753", "submitter": "Zhigao Guo", "authors": "Zhigao Guo and Anthony C. Constantinou", "title": "Approximate learning of high dimensional Bayesian network structures via\n  pruning of Candidate Parent Sets", "comments": null, "journal-ref": null, "doi": "10.3390/e22101142", "report-no": null, "categories": "cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based algorithms that learn Bayesian Network (BN) structures provide\nsolutions ranging from different levels of approximate learning to exact\nlearning. Approximate solutions exist because exact learning is generally not\napplicable to networks of moderate or higher complexity. In general,\napproximate solutions tend to sacrifice accuracy for speed, where the aim is to\nminimise the loss in accuracy and maximise the gain in speed. While some\napproximate algorithms are optimised to handle thousands of variables, these\nalgorithms may still be unable to learn such high dimensional structures. Some\nof the most efficient score-based algorithms cast the structure learning\nproblem as a combinatorial optimisation of candidate parent sets. This paper\nexplores a strategy towards pruning the size of candidate parent sets, aimed at\nhigh dimensionality problems. The results illustrate how different levels of\npruning affect the learning speed relative to the loss in accuracy in terms of\nmodel fitting, and show that aggressive pruning may be required to produce\napproximate solutions for high complexity problems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:09:18 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 15:48:29 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Guo", "Zhigao", ""], ["Constantinou", "Anthony C.", ""]]}, {"id": "2006.04951", "submitter": "Giancarlo Perrone", "authors": "Giancarlo Perrone, Jose Unpingco, Haw-minn Lu", "title": "Network visualizations with Pyvis and VisJS", "comments": "Accepted and submitted to 19th Python in Science Conference. (SciPy\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyvis is a Python module that enables visualizing and interactively\nmanipulating network graphs in the Jupyter notebook, or as a standalone web\napplication. Pyvis is built on top of the powerful and mature VisJS JavaScript\nlibrary, which allows for fast and responsive interactions while also\nabstracting away the low-level JavaScript and HTML. This means that elements of\nthe rendered graph visualization, such as node/edge attributes can be specified\nwithin Python and shipped to the JavaScript layer for VisJS to render. This\ndeclarative approach makes it easy to quickly explore graph visualizations and\ninvestigate data relationships. In addition, Pyvis is highly customizable so\nthat colors, sizes, and hover tooltips can be assigned to the rendered graph.\nThe network graph layout is controlled by a front-end physics engine that is\nconfigurable from a Python interface, allowing for the detailed placement of\nthe graph elements. In this paper, we outline use cases for Pyvis with specific\nexamples to highlight key features for any analysis workflow. A brief overview\nof Pyvis' implementation describes how the Python front-end binding uses simple\nPyvis calls.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:32:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Perrone", "Giancarlo", ""], ["Unpingco", "Jose", ""], ["Lu", "Haw-minn", ""]]}, {"id": "2006.05400", "submitter": "Matan Atzmon", "authors": "Matan Atzmon and Yaron Lipman", "title": "SALD: Sign Agnostic Learning with Derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning 3D geometry directly from raw data, such as point clouds, triangle\nsoups, or unoriented meshes is still a challenging task that feeds many\ndownstream computer vision and graphics applications.\n  In this paper, we introduce SALD: a method for learning implicit neural\nrepresentations of shapes directly from raw data. We generalize sign agnostic\nlearning (SAL) to include derivatives: given an unsigned distance function to\nthe input raw data, we advocate a novel sign agnostic regression loss,\nincorporating both pointwise values and gradients of the unsigned distance\nfunction. Optimizing this loss leads to a signed implicit function solution,\nthe zero level set of which is a high quality and valid manifold approximation\nto the input 3D data. The motivation behind SALD is that incorporating\nderivatives in a regression loss leads to a lower sample complexity, and\nconsequently better fitting. In addition, we prove that SAL enjoys a minimal\nlength property in 2D, favoring minimal length solutions. More importantly, we\nare able to show that this property still holds for SALD, i.e., with\nderivatives included.\n  We demonstrate the efficacy of SALD for shape space learning on two\nchallenging datasets: ShapeNet that contains inconsistent orientation and\nnon-manifold meshes, and D-Faust that contains raw 3D scans (triangle soups).\nOn both these datasets, we present state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:54:57 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 17:24:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "2006.05724", "submitter": "Matteo Poggi", "authors": "Filippo Aleotti, Giulio Zaccaroni, Luca Bartolomei, Matteo Poggi,\n  Fabio Tosi, Stefano Mattoccia", "title": "Real-time single image depth perception in the wild with handheld\n  devices", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth perception is paramount to tackle real-world problems, ranging from\nautonomous driving to consumer applications. For the latter, depth estimation\nfrom a single image represents the most versatile solution, since a standard\ncamera is available on almost any handheld device. Nonetheless, two main issues\nlimit its practical deployment: i) the low reliability when deployed\nin-the-wild and ii) the demanding resource requirements to achieve real-time\nperformance, often not compatible with such devices. Therefore, in this paper,\nwe deeply investigate these issues showing how they are both addressable\nadopting appropriate network design and training strategies -- also outlining\nhow to map the resulting networks on handheld devices to achieve real-time\nperformance. Our thorough evaluation highlights the ability of such fast\nnetworks to generalize well to new environments, a crucial feature required to\ntackle the extremely varied contexts faced in real applications. Indeed, to\nfurther support this evidence, we report experimental results concerning\nreal-time depth-aware augmented reality and image blurring with smartphones\nin-the-wild.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:30:20 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Aleotti", "Filippo", ""], ["Zaccaroni", "Giulio", ""], ["Bartolomei", "Luca", ""], ["Poggi", "Matteo", ""], ["Tosi", "Fabio", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2006.05743", "submitter": "Wenlin Zhuang", "authors": "Wenlin Zhuang, Yangang Wang, Joseph Robinson, Congyi Wang, Ming Shao,\n  Yun Fu, Siyu Xia", "title": "Towards 3D Dance Motion Synthesis and Control", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human dance motion is a cooperative and elegant social movement. Unlike\nregular simple locomotion, it is challenging to synthesize artistic dance\nmotions due to the irregularity, kinematic complexity and diversity. It\nrequires the synthesized dance is realistic, diverse and controllable. In this\npaper, we propose a novel generative motion model based on temporal convolution\nand LSTM,TC-LSTM, to synthesize realistic and diverse dance motion. We\nintroduce a unique control signal, dance melody line, to heighten\ncontrollability. Hence, our model, and its switch for control signals, promote\na variety of applications: random dance synthesis, music-to-dance, user\ncontrol, and more. Our experiments demonstrate that our model can synthesize\nartistic dance motion in various dance types. Compared with existing methods,\nour method achieved start-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:19:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhuang", "Wenlin", ""], ["Wang", "Yangang", ""], ["Robinson", "Joseph", ""], ["Wang", "Congyi", ""], ["Shao", "Ming", ""], ["Fu", "Yun", ""], ["Xia", "Siyu", ""]]}, {"id": "2006.05921", "submitter": "Nurcan Gecer Ulu", "authors": "Nurcan Gecer Ulu", "title": "Computational Design and Evaluation Methods for Empowering Non-Experts\n  in Digital Fabrication", "comments": "PhD Thesis, Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasing availability of personal fabrication hardware and\nservices, the true potential of digital fabrication remains unrealized due to\nlack of computational techniques that can support 3D shape design by\nnon-experts. This work develops computational methods that address two key\naspects of content creation:(1) Function-driven design synthesis, (2) Design\nassessment.\n  For design synthesis, a generative shape modeling algorithm that facilitates\nautomatic geometry synthesis and user-driven modification for non-experts is\nintroduced. A critical observation that arises from this study is that the most\ngeometrical specifications are dictated by functional requirements. To support\ndesign by high-level functional prescriptions, a physics based shape\noptimization method for compliant coupling behavior design has been developed.\nIn line with this idea, producing complex 3D surfaces from flat 2D sheets by\nexploiting the concept of buckling beams has also been explored. Effective\ndesign assessment, the second key aspect, becomes critical for problems in\nwhich computational solutions do not exist. For these problems, this work\nproposes crowdsourcing as a way to empower non-experts in esoteric design\ndomains that traditionally require expertise and specialized knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:15:16 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ulu", "Nurcan Gecer", ""]]}, {"id": "2006.06071", "submitter": "Ali Samadani", "authors": "Ali Samadani, Rob Gorbet, Dana Kulic", "title": "Affective Movement Generation using Laban Effort and Shape and Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body movements are an important communication medium through which affective\nstates can be discerned. Movements that convey affect can also give machines\nlife-like attributes and help to create a more engaging human-machine\ninteraction. This paper presents an approach for automatic affective movement\ngeneration that makes use of two movement abstractions: 1) Laban movement\nanalysis (LMA), and 2) hidden Markov modeling. The LMA provides a systematic\ntool for an abstract representation of the kinematic and expressive\ncharacteristics of movements. Given a desired motion path on which a target\nemotion is to be overlaid, the proposed approach searches a labeled dataset in\nthe LMA Effort and Shape space for similar movements to the desired motion path\nthat convey the target emotion. An HMM abstraction of the identified movements\nis obtained and used with the desired motion path to generate a novel movement\nthat is a modulated version of the desired motion path that conveys the target\nemotion. The extent of modulation can be varied, trading-off between kinematic\nand affective constraints in the generated movement. The proposed approach is\ntested using a full-body movement dataset. The efficacy of the proposed\napproach in generating movements with recognizable target emotions is assessed\nusing a validated automatic recognition model and a user study. The target\nemotions were correctly recognized from the generated movements at a rate of\n72% using the recognition model. Furthermore, participants in the user study\nwere able to correctly perceive the target emotions from a sample of generated\nmovements, although some cases of confusion were also observed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:24:26 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Samadani", "Ali", ""], ["Gorbet", "Rob", ""], ["Kulic", "Dana", ""]]}, {"id": "2006.06080", "submitter": "Alec Jacobson", "authors": "Alec Jacobson", "title": "Least-Squares Affine Reflection Using Eigen Decomposition", "comments": "1 page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note summarizes the steps to computing the best-fitting affine\nreflection that aligns two sets of corresponding points.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:50:02 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Jacobson", "Alec", ""]]}, {"id": "2006.06748", "submitter": "Leonardo Fernandez-Jambrina", "authors": "A. Cant\\'on, L. Fern\\'andez-Jambrina, M.J. V\\'azquez-Gallo", "title": "Curvature of planar aesthetic curves", "comments": "32 pages, 16 figures. To appear in Journal of Computational and\n  Applied Mathematics", "journal-ref": "Journal of Computational and Applied Mathematics 381, 113042\n  (2021)", "doi": "10.1016/j.cam.2020.113042", "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Farin proposed a method for designing Bezier curves with monotonic curvature\nand torsion. Such curves are relevant in design due to their aesthetic shape.\nThe method relies on applying a matrix M to the first edge of the control\npolygon of the curve in order to obtain by iteration the remaining edges. With\nthis method, sufficient conditions on the matrix $M$ are provided, which lead\nto the definition of Class A curves, generalising a previous result by Mineur\net al for plane curves with M being the composition of a dilatation and a\nrotation. However, Cao and Wang have shown counterexamples for such conditions.\nIn this paper, we revisit Farin's idea of using the subdivision algorithm to\nrelate the curvature at every point of the curve to the curvature at the\ninitial point in order to produce a closed formula for the curvature of planar\ncurves in terms of the eigenvalues of the matrix M and the seed vector for the\ncurve, the first edge of the control polygon. Moreover, we give new conditions\nin order to produce planar curves with monotonic curvature. The main difference\nis that we do not require our conditions on the eigenvalues to be preserved\nunder subdivision of the curve. This facilitates giving a unified derivation of\nthe existing results and obtain more general results in the planar case.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:05:45 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cant\u00f3n", "A.", ""], ["Fern\u00e1ndez-Jambrina", "L.", ""], ["V\u00e1zquez-Gallo", "M. J.", ""]]}, {"id": "2006.07364", "submitter": "Ye Yuan", "authors": "Ye Yuan, Kris Kitani", "title": "Residual Force Control for Agile Human Behavior Imitation and Extended\n  Motion Synthesis", "comments": "NeurIPS 2020. Code: https://github.com/Khrylx/RFC. Project page:\n  https://www.ye-yuan.com/rfc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has shown great promise for synthesizing realistic\nhuman behaviors by learning humanoid control policies from motion capture data.\nHowever, it is still very challenging to reproduce sophisticated human skills\nlike ballet dance, or to stably imitate long-term human behaviors with complex\ntransitions. The main difficulty lies in the dynamics mismatch between the\nhumanoid model and real humans. That is, motions of real humans may not be\nphysically possible for the humanoid model. To overcome the dynamics mismatch,\nwe propose a novel approach, residual force control (RFC), that augments a\nhumanoid control policy by adding external residual forces into the action\nspace. During training, the RFC-based policy learns to apply residual forces to\nthe humanoid to compensate for the dynamics mismatch and better imitate the\nreference motion. Experiments on a wide range of dynamic motions demonstrate\nthat our approach outperforms state-of-the-art methods in terms of convergence\nspeed and the quality of learned motions. Notably, we showcase a physics-based\nvirtual character empowered by RFC that can perform highly agile ballet dance\nmoves such as pirouette, arabesque and jet\\'e. Furthermore, we propose a\ndual-policy control framework, where a kinematic policy and an RFC-based policy\nwork in tandem to synthesize multi-modal infinite-horizon human motions without\nany task guidance or user input. Our approach is the first humanoid control\nmethod that successfully learns from a large-scale human motion dataset\n(Human3.6M) and generates diverse long-term motions. Code and videos are\navailable at https://www.ye-yuan.com/rfc.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:56:16 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 17:57:12 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2006.07660", "submitter": "Dario Pavllo", "authors": "Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-Francine Moens,\n  Aurelien Lucchi", "title": "Convolutional Generation of Textured 3D Meshes", "comments": "NeurIPS 2020, Oral presentation. Code at\n  https://github.com/dariopavllo/convmesh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent generative models for 2D images achieve impressive visual\nresults, they clearly lack the ability to perform 3D reasoning. This heavily\nrestricts the degree of control over generated objects as well as the possible\napplications of such models. In this work, we bridge this gap by leveraging\nrecent advances in differentiable rendering. We design a framework that can\ngenerate triangle meshes and associated high-resolution texture maps, using\nonly 2D supervision from single-view natural images. A key contribution of our\nwork is the encoding of the mesh and texture as 2D representations, which are\nsemantically aligned and can be easily modeled by a 2D convolutional GAN. We\ndemonstrate the efficacy of our method on Pascal3D+ Cars and CUB, both in an\nunconditional setting and in settings where the model is conditioned on class\nlabels, attributes, and text. Finally, we propose an evaluation methodology\nthat assesses the mesh and texture quality separately.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 15:23:29 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 10:21:45 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pavllo", "Dario", ""], ["Spinks", "Graham", ""], ["Hofmann", "Thomas", ""], ["Moens", "Marie-Francine", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "2006.07818", "submitter": "Congyue Deng", "authors": "Congyue Deng, Tai-Jiang Mu, Shi-Min Hu", "title": "Alternating ConvLSTM: Learning Force Propagation with Alternate State\n  Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven simulation is an important step-forward in computational physics\nwhen traditional numerical methods meet their limits. Learning-based simulators\nhave been widely studied in past years; however, most previous works view\nsimulation as a general spatial-temporal prediction problem and take little\nphysical guidance in designing their neural network architectures. In this\npaper, we introduce the alternating convolutional Long Short-Term Memory\n(Alt-ConvLSTM) that models the force propagation mechanisms in a deformable\nobject with near-uniform material properties. Specifically, we propose an\naccumulation state, and let the network update its cell state and the\naccumulation state alternately. We demonstrate how this novel scheme imitates\nthe alternate updates of the first and second-order terms in the forward Euler\nmethod of numerical PDE solvers. Benefiting from this, our network only\nrequires a small number of parameters, independent of the number of the\nsimulated particles, and also retains the essential features in ConvLSTM,\nmaking it naturally applicable to sequential data with spatial inputs and\noutputs. We validate our Alt-ConvLSTM on human soft tissue simulation with\nthousands of particles and consistent body pose changes. Experimental results\nshow that Alt-ConvLSTM efficiently models the material kinetic features and\ngreatly outperforms vanilla ConvLSTM with only the single state update.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 06:43:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Deng", "Congyue", ""], ["Mu", "Tai-Jiang", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2006.07859", "submitter": "Keenan Crane", "authors": "Christopher Yu, Henrik Schumacher, Keenan Crane", "title": "Repulsive Curves", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curves play a fundamental role across computer graphics, physical simulation,\nand mathematical visualization, yet most tools for curve design do nothing to\nprevent crossings or self-intersections. This paper develops efficient\nalgorithms for (self-)repulsion of plane and space curves that are well-suited\nto problems in computational design. Our starting point is the so-called\ntangent-point energy, which provides an infinite barrier to self-intersection.\nIn contrast to local collision detection strategies used in, e.g., physical\nsimulation, this energy considers interactions between all pairs of points, and\nis hence useful for global shape optimization: local minima tend to be\naesthetically pleasing, physically valid, and nicely distributed in space. A\nreformulation of gradient descent, based on a Sobolev-Slobodeckij inner product\nenables us to make rapid progress toward local minima---independent of curve\nresolution. We also develop a hierarchical multigrid scheme that significantly\nreduces the per-step cost of optimization. The energy is easily integrated with\na variety of constraints and penalties (e.g., inextensibility, or obstacle\navoidance), which we use for applications including curve packing, knot\nuntangling, graph embedding, non-crossing spline interpolation, flow\nvisualization, and robotic path planning.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 10:26:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Yu", "Christopher", ""], ["Schumacher", "Henrik", ""], ["Crane", "Keenan", ""]]}, {"id": "2006.07982", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Jingwei Huang, Andrea Tagliasacchi, Leonidas Guibas", "title": "ShapeFlow: Learnable Deformations Among 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ShapeFlow, a flow-based model for learning a deformation space for\nentire classes of 3D shapes with large intra-class variations. ShapeFlow allows\nlearning a multi-template deformation space that is agnostic to shape topology,\nyet preserves fine geometric details. Different from a generative space where a\nlatent vector is directly decoded into a shape, a deformation space decodes a\nvector into a continuous flow that can advect a source shape towards a target.\nSuch a space naturally allows the disentanglement of geometric style (coming\nfrom the source) and structural pose (conforming to the target). We parametrize\nthe deformation between geometries as a learned continuous flow field via a\nneural network and show that such deformations can be guaranteed to have\ndesirable properties, such as be bijectivity, freedom from self-intersections,\nor volume preservation. We illustrate the effectiveness of this learned\ndeformation space for various downstream applications, including shape\ngeneration via deformation, geometric style transfer, unsupervised learning of\na consistent parameterization for entire classes of shapes, and shape\ninterpolation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:03:35 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 22:06:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Huang", "Jingwei", ""], ["Tagliasacchi", "Andrea", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2006.08072", "submitter": "Tong He", "authors": "Tong He, John Collomosse, Hailin Jin, Stefano Soatto", "title": "Geo-PIFu: Geometry and Pixel Aligned Implicit Functions for Single-view\n  Human Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Geo-PIFu, a method to recover a 3D mesh from a monocular color\nimage of a clothed person. Our method is based on a deep implicit\nfunction-based representation to learn latent voxel features using a\nstructure-aware 3D U-Net, to constrain the model in two ways: first, to resolve\nfeature ambiguities in query point encoding, second, to serve as a coarse human\nshape proxy to regularize the high-resolution mesh and encourage global shape\nregularity. We show that, by both encoding query points and constraining global\nshape using latent voxel features, the reconstruction we obtain for clothed\nhuman meshes exhibits less shape distortion and improved surface details\ncompared to competing methods. We evaluate Geo-PIFu on a recent human mesh\npublic dataset that is $10 \\times$ larger than the private commercial dataset\nused in PIFu and previous derivative work. On average, we exceed the state of\nthe art by $42.7\\%$ reduction in Chamfer and Point-to-Surface Distances, and\n$19.4\\%$ reduction in normal estimation errors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:11:48 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 00:20:35 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["He", "Tong", ""], ["Collomosse", "John", ""], ["Jin", "Hailin", ""], ["Soatto", "Stefano", ""]]}, {"id": "2006.08819", "submitter": "Ioannis Ivrissimtzis", "authors": "Xin Zhang and Ning Jia and Ioannis Ivrissimtzis", "title": "A study of the effect of the illumination model on the generation of\n  synthetic training datasets", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of computer generated images to train Deep Neural Networks is a\nviable alternative to real images when the latter are scarce or expensive. In\nthis paper, we study how the illumination model used by the rendering software\naffects the quality of the generated images. We created eight training sets,\neach one with a different illumination model, and tested them on three\ndifferent network architectures, ResNet, U-Net and a combined architecture\ndeveloped by us. The test set consisted of photos of 3D printed objects\nproduced from the same CAD models used to generate the training set. The effect\nof the other parameters of the rendering process, such as textures and camera\nposition, was randomized.\n  Our results show that the effect of the illumination model is important,\ncomparable in significance to the network architecture. We also show that both\nlight probes capturing natural environmental light, and modelled lighting\nenvironments, can give good results. In the case of light probes, we identified\nas two significant factors affecting performance the similarity between the\nlight probe and the test environment, as well as the light probe's resolution.\nRegarding modelled lighting environment, similarity with the test environment\nwas again identified as a significant factor.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 23:22:24 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Xin", ""], ["Jia", "Ning", ""], ["Ivrissimtzis", "Ioannis", ""]]}, {"id": "2006.08821", "submitter": "Vismay Modi", "authors": "Vismay Modi, Lawson Fulton, Shinjiro Sueda, Alec Jacobson, David I.W.\n  Levin", "title": "EMU: Efficient Muscle Simulation In Deformation Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EMU is an efficient and scalable model to simulate bulk musculoskeletal\nmotion with heterogenous materials. First, EMU requires no model reductions, or\ngeometric coarsening, thereby producing results visually accurate when compared\nto an FEM simulation. Second, EMU is efficient and scales much better than\nstate-of-the-art FEM with the number of elements in the mesh, and is more\neasily parallelizable. Third, EMU can handle heterogeneously stiff meshes with\nan arbitrary constitutive model, thus allowing it to simulate soft muscles,\nstiff tendons and even stiffer bones all within one unified system. These three\nkey characteristics of EMU enable us to efficiently orchestrate muscle\nactivated skeletal movements. We demonstrate the efficacy of our approach via a\nnumber of examples with tendons, muscles, bones and joints.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 23:30:46 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 20:51:23 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 20:48:01 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 16:48:05 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Modi", "Vismay", ""], ["Fulton", "Lawson", ""], ["Sueda", "Shinjiro", ""], ["Jacobson", "Alec", ""], ["Levin", "David I. W.", ""]]}, {"id": "2006.09662", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Eric R. Chan, Richard Tucker, Noah Snavely, Gordon\n  Wetzstein", "title": "MetaSDF: Meta-learning Signed Distance Functions", "comments": "Project website: https://vsitzmann.github.io/metasdf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit shape representations are an emerging paradigm that offers\nmany potential benefits over conventional discrete representations, including\nmemory efficiency at a high spatial resolution. Generalizing across shapes with\nsuch neural implicit representations amounts to learning priors over the\nrespective function space and enables geometry reconstruction from partial or\nnoisy observations. Existing generalization methods rely on conditioning a\nneural network on a low-dimensional latent code that is either regressed by an\nencoder or jointly optimized in the auto-decoder framework. Here, we formalize\nlearning of a shape space as a meta-learning problem and leverage\ngradient-based meta-learning algorithms to solve this task. We demonstrate that\nthis approach performs on par with auto-decoder based approaches while being an\norder of magnitude faster at test-time inference. We further demonstrate that\nthe proposed gradient-based method outperforms encoder-decoder based methods\nthat leverage pooling-based set encoders.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:14:53 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Chan", "Eric R.", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2006.09819", "submitter": "Ning Liu", "authors": "N. Liu, K. Ren, W. Zhang, Y.F. Zhang, Y.X. Chew, J.Y.H. Fuh, G.J. Bi", "title": "An Evolutional Algorithm for Automatic 2D Layer Segmentation in\n  Laser-aided Additive Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toolpath planning is an important task in laser aided additive manufacturing\n(LAAM) and other direct energy deposition (DED) processes. The deposition\ntoolpaths for complex geometries with slender structures can be further\noptimized by partitioning the sliced 2D layers into sub-regions, and enable the\ndesign of appropriate infill toolpaths for different sub-regions. However,\nreported approaches for 2D layer segmentation generally require manual\noperations that are tedious and time-consuming. To increase segmentation\nefficiency, this paper proposes an autonomous approach based on evolutional\ncomputation for 2D layer segmentation. The algorithm works in an\nidentify-and-segment manner. Specifically, the largest quasi-quadrilateral is\nidentified and segmented from the target layer iteratively. Results from case\nstudies have validated the effectiveness and efficacy of the developed\nalgorithm. To further improve its performance, a roughing-finishing strategy is\nproposed. Via multi-processing, the strategy can remarkably increase the\nsolution variety without affecting solution quality and search time, thus\nproviding great application potential in LAAM toolpath planning. To the best of\nthe authors knowledge, this work is the first to address automatic 2D layer\nsegmentation problem in LAAM process. Therefore, it may be a valuable\nsupplement to the state of the art in this area.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 15:39:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:27:01 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 08:20:04 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Liu", "N.", ""], ["Ren", "K.", ""], ["Zhang", "W.", ""], ["Zhang", "Y. F.", ""], ["Chew", "Y. X.", ""], ["Fuh", "J. Y. H.", ""], ["Bi", "G. J.", ""]]}, {"id": "2006.10172", "submitter": "Orly Liba", "authors": "Orly Liba, Longqi Cai, Yun-Ta Tsai, Elad Eban, Yair Movshovitz-Attias,\n  Yael Pritch, Huizhong Chen, Jonathan T. Barron", "title": "Sky Optimization: Semantically aware image processing of skies in\n  low-light photography", "comments": "Published in Proceedings of the IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition Workshops. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The sky is a major component of the appearance of a photograph, and its color\nand tone can strongly influence the mood of a picture. In nighttime\nphotography, the sky can also suffer from noise and color artifacts. For this\nreason, there is a strong desire to process the sky in isolation from the rest\nof the scene to achieve an optimal look. In this work, we propose an automated\nmethod, which can run as a part of a camera pipeline, for creating accurate sky\nalpha-masks and using them to improve the appearance of the sky. Our method\nperforms end-to-end sky optimization in less than half a second per image on a\nmobile device. We introduce a method for creating an accurate sky-mask dataset\nthat is based on partially annotated images that are inpainted and refined by\nour modified weighted guided filter. We use this dataset to train a neural\nnetwork for semantic sky segmentation. Due to the compute and power constraints\nof mobile devices, sky segmentation is performed at a low image resolution. Our\nmodified weighted guided filter is used for edge-aware upsampling to resize the\nalpha-mask to a higher resolution. With this detailed mask we automatically\napply post-processing steps to the sky in isolation, such as automatic\nspatially varying white-balance, brightness adjustments, contrast enhancement,\nand noise reduction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:19:12 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Liba", "Orly", ""], ["Cai", "Longqi", ""], ["Tsai", "Yun-Ta", ""], ["Eban", "Elad", ""], ["Movshovitz-Attias", "Yair", ""], ["Pritch", "Yael", ""], ["Chen", "Huizhong", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2006.10509", "submitter": "Peter Christopher", "authors": "Peter J. Christopher and Timothy D. Wilkinson", "title": "Structure and Design of HoloGen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing popularity of augmented and mixed reality systems has seen a\nsimilar increase of interest in 2D and 3D computer generated holography (CGH).\nUnlike stereoscopic approaches, CGH can fully represent a light field including\ndepth of focus, accommodation and vergence. Along with existing\ntelecommunications, imaging, projection, lithography, beam shaping and optical\ntweezing applications, CGH is an exciting technique applicable to a wide array\nof photonic problems including full 3D representation. Traditionally, the\nprimary roadblock to acceptance has been the significant numerical processing\nrequired to generate holograms requiring both significant expertise and\nsignificant computational power. This article discusses the structure and\ndesign of HoloGen. HoloGen is an MIT licensed application that may be used to\ngenerate holograms using a wide array of algorithms without expert guidance.\nHoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation\nFramework graphical user interface. The article begins by introducing HoloGen\nbefore providing an in-depth discussion of its design and structure. Particular\nfocus is given to the communication, data transfer and algorithmic aspects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 13:29:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Christopher", "Peter J.", ""], ["Wilkinson", "Timothy D.", ""]]}, {"id": "2006.10738", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, Song Han", "title": "Differentiable Augmentation for Data-Efficient GAN Training", "comments": "NeurIPS 2020. Project: https://data-efficient-gans.mit.edu/ Code:\n  https://github.com/mit-han-lab/data-efficient-gans", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of generative adversarial networks (GANs) heavily\ndeteriorates given a limited amount of training data. This is mainly because\nthe discriminator is memorizing the exact training set. To combat it, we\npropose Differentiable Augmentation (DiffAugment), a simple method that\nimproves the data efficiency of GANs by imposing various types of\ndifferentiable augmentations on both real and fake samples. Previous attempts\nto directly augment the training data manipulate the distribution of real\nimages, yielding little benefit; DiffAugment enables us to adopt the\ndifferentiable augmentation for the generated samples, effectively stabilizes\ntraining, and leads to better convergence. Experiments demonstrate consistent\ngains of our method over a variety of GAN architectures and loss functions for\nboth unconditional and class-conditional generation. With DiffAugment, we\nachieve a state-of-the-art FID of 6.80 with an IS of 100.8 on ImageNet 128x128\nand 2-4x reductions of FID given 1,000 images on FFHQ and LSUN. Furthermore,\nwith only 20% training data, we can match the top performance on CIFAR-10 and\nCIFAR-100. Finally, our method can generate high-fidelity images using only 100\nimages without pre-training, while being on par with existing transfer learning\nalgorithms. Code is available at\nhttps://github.com/mit-han-lab/data-efficient-gans.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:59:01 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 17:57:20 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 07:28:46 GMT"}, {"version": "v4", "created": "Mon, 7 Dec 2020 05:51:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhao", "Shengyu", ""], ["Liu", "Zhijian", ""], ["Lin", "Ji", ""], ["Zhu", "Jun-Yan", ""], ["Han", "Song", ""]]}, {"id": "2006.11348", "submitter": "Vin\\'icius Da Silva", "authors": "Vinicius da Silva and Luiz Velho", "title": "Ray-VR: Ray Tracing Virtual Reality in Falcor", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NVidia RTX platform has been changing and extending the possibilities for\nreal time Computer Graphics applications. It is the first time in history that\nretail graphics cards have full hardware support for ray tracing primitives. It\nstill a long way to fully understand and optimize its use and this task itself\nis a fertile field for scientific progression. However, another path is to\nexplore the platform as an expansion of paradigms for other problems. For\nexample, the integration of real time Ray Tracing and Virtual Reality can\nresult in interesting applications for visualization of Non-Euclidean Geometry\nand 3D Manifolds. In this paper we present Ray-VR, a novel algorithm for real\ntime stereo ray tracing, constructed on top of Falcor, NVidia's scientific\nprototyping framework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 19:54:50 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["da Silva", "Vinicius", ""], ["Velho", "Luiz", ""]]}, {"id": "2006.11620", "submitter": "Hubert P. H. Shum", "authors": "Hubert P. H. Shum, Taku Komura", "title": "Technical Note: Generating Realistic Fighting Scenes by Game Tree", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been a lot of researches to synthesize / edit the motion\nof a single avatar in the virtual environment. However, there has not been so\nmuch work of simulating continuous interactions of multiple avatars such as\nfighting. In this paper, we propose a new method to generate a realistic\nfighting scene based on motion capture data. We propose a new algorithm called\nthe temporal expansion approach which maps the continuous time action plan to a\ndiscrete causality space such that turn-based evaluation methods can be used.\nAs a result, it is possible to use many mature algorithms available in strategy\ngames such as the Minimax algorithm and $\\alpha-\\beta$ pruning. We also propose\na method to generate and use an offense/defense table, which illustrates the\nspatial-temporal relationship of attacks and dodges, to incorporate tactical\nmaneuvers of defense into the scene. Using our method, avatars will plan their\nstrategies taking into account the reaction of the opponent. Fighting scenes\nwith multiple avatars are generated to demonstrate the effectiveness of our\nalgorithm. The proposed method can also be applied to other kinds of continuous\nactivities that require strategy planning such as sport games.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 17:11:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shum", "Hubert P. H.", ""], ["Komura", "Taku", ""]]}, {"id": "2006.11840", "submitter": "Sizhuo Ma", "authors": "Sizhuo Ma, Shantanu Gupta, Arin C. Ulku, Claudio Bruschini, Edoardo\n  Charbon, Mohit Gupta", "title": "Quanta Burst Photography", "comments": "A version with better-quality images can be found on the project\n  webpage: http://wisionlab.cs.wisc.edu/project/quanta-burst-photography/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-photon avalanche diodes (SPADs) are an emerging sensor technology\ncapable of detecting individual incident photons, and capturing their\ntime-of-arrival with high timing precision. While these sensors were limited to\nsingle-pixel or low-resolution devices in the past, recently, large (up to 1\nMPixel) SPAD arrays have been developed. These single-photon cameras (SPCs) are\ncapable of capturing high-speed sequences of binary single-photon images with\nno read noise. We present quanta burst photography, a computational photography\ntechnique that leverages SPCs as passive imaging devices for photography in\nchallenging conditions, including ultra low-light and fast motion. Inspired by\nrecent success of conventional burst photography, we design algorithms that\nalign and merge binary sequences captured by SPCs into intensity images with\nminimal motion blur and artifacts, high signal-to-noise ratio (SNR), and high\ndynamic range. We theoretically analyze the SNR and dynamic range of quanta\nburst photography, and identify the imaging regimes where it provides\nsignificant benefits. We demonstrate, via a recently developed SPAD array, that\nthe proposed method is able to generate high-quality images for scenes with\nchallenging lighting, complex geometries, high dynamic range and moving\nobjects. With the ongoing development of SPAD arrays, we envision quanta burst\nphotography finding applications in both consumer and scientific photography.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 16:20:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ma", "Sizhuo", ""], ["Gupta", "Shantanu", ""], ["Ulku", "Arin C.", ""], ["Bruschini", "Claudio", ""], ["Charbon", "Edoardo", ""], ["Gupta", "Mohit", ""]]}, {"id": "2006.11851", "submitter": "Syed Muhammad Arsalan Bashir Mr.", "authors": "Syed Muhammad Arsalan Bashir and Farhan Ali Khan Ghouri", "title": "Perspective Texture Synthesis Based on Improved Energy Optimization", "comments": "Published in PLOS One", "journal-ref": "PLoS ONE 9 (10), e110622 (2014)", "doi": "10.1371/journal.pone.0110622", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Perspective texture synthesis has great significance in many fields like\nvideo editing, scene capturing etc., due to its ability to read and control\nglobal feature information. In this paper, we present a novel example-based,\nspecifically energy optimization-based algorithm, to synthesize perspective\ntextures. Energy optimization technique is a pixel-based approach, so it is\ntime-consuming. We improve it from two aspects with the purpose of achieving\nfaster synthesis and high quality. Firstly, we change this pixel-based\ntechnique by replacing the pixel computation with a little patch. Secondly, we\npresent a novel technique to accelerate searching nearest neighborhoods in\nenergy optimization. Using k- means clustering technique to build a search tree\nto accelerate the search. Hence, we make use of principal component analysis\n(PCA) technique to reduce dimensions of input vectors. The high quality results\nprove that our approach is feasible. Besides, our proposed algorithm needs\nshorter time relative to other similar methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 17:12:58 GMT"}], "update_date": "2020-06-28", "authors_parsed": [["Bashir", "Syed Muhammad Arsalan", ""], ["Ghouri", "Farhan Ali Khan", ""]]}, {"id": "2006.12057", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato, Deniz Beker, Mihai Morariu, Takahiro Ando, Toru\n  Matsuoka, Wadim Kehl, Adrien Gaidon", "title": "Differentiable Rendering: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown remarkable performance improvements on\nvision-related tasks such as object detection or image segmentation. Despite\ntheir success, they generally lack the understanding of 3D objects which form\nthe image, as it is not always possible to collect 3D information about the\nscene or to easily annotate it. Differentiable rendering is a novel field which\nallows the gradients of 3D objects to be calculated and propagated through\nimages. It also reduces the requirement of 3D data collection and annotation,\nwhile enabling higher success rate in various applications. This paper reviews\nexisting literature and discusses the current state of differentiable\nrendering, its applications and open research problems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:14:52 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 00:01:27 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Kato", "Hiroharu", ""], ["Beker", "Deniz", ""], ["Morariu", "Mihai", ""], ["Ando", "Takahiro", ""], ["Matsuoka", "Toru", ""], ["Kehl", "Wadim", ""], ["Gaidon", "Adrien", ""]]}, {"id": "2006.12075", "submitter": "Mingyi Shi", "authors": "Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Komura, Dani\n  Lischinski, Daniel Cohen-Or, Baoquan Chen", "title": "MotioNet: 3D Human Motion Reconstruction from Monocular Video with\n  Skeleton Consistency", "comments": "Accepted to Transactions on Graphics (ToG) 2020. Project page:\n  {https://rubbly.cn/publications/motioNet} Video:\n  {https://youtu.be/8YubchlzvFA}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce MotioNet, a deep neural network that directly reconstructs the\nmotion of a 3D human skeleton from monocular video.While previous methods rely\non either rigging or inverse kinematics (IK) to associate a consistent skeleton\nwith temporally coherent joint rotations, our method is the first data-driven\napproach that directly outputs a kinematic skeleton, which is a complete,\ncommonly used, motion representation. At the crux of our approach lies a deep\nneural network with embedded kinematic priors, which decomposes sequences of 2D\njoint positions into two separate attributes: a single, symmetric, skeleton,\nencoded by bone lengths, and a sequence of 3D joint rotations associated with\nglobal root positions and foot contact labels. These attributes are fed into an\nintegrated forward kinematics (FK) layer that outputs 3D positions, which are\ncompared to a ground truth. In addition, an adversarial loss is applied to the\nvelocities of the recovered rotations, to ensure that they lie on the manifold\nof natural joint rotations. The key advantage of our approach is that it learns\nto infer natural joint rotations directly from the training data, rather than\nassuming an underlying model, or inferring them from joint positions using a\ndata-agnostic IK solver. We show that enforcing a single consistent skeleton\nalong with temporally coherent joint rotations constrains the solution space,\nleading to a more robust handling of self-occlusions and depth ambiguities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 08:50:09 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shi", "Mingyi", ""], ["Aberman", "Kfir", ""], ["Aristidou", "Andreas", ""], ["Komura", "Taku", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2006.12449", "submitter": "Jan Egger", "authors": "Jianning Li, Antonio Pepe, Christina Gsaxner, Gord von Campe, Jan\n  Egger", "title": "A Baseline Approach for AutoImplant: the MICCAI 2020 Cranial Implant\n  Design Challenge", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a baseline approach for AutoImplant\n(https://autoimplant.grand-challenge.org/) - the cranial implant design\nchallenge, which, as suggested by the organizers, can be formulated as a\nvolumetric shape learning task. In this task, the defective skull, the complete\nskull and the cranial implant are represented as binary voxel grids. To\naccomplish this task, the implant can be either reconstructed directly from the\ndefective skull or obtained by taking the difference between a defective skull\nand a complete skull. In the latter case, a complete skull has to be\nreconstructed given a defective skull, which defines a volumetric shape\ncompletion problem. Our baseline approach for this task is based on the former\nformulation, i.e., a deep neural network is trained to predict the implants\ndirectly from the defective skulls. The approach generates high-quality\nimplants in two steps: First, an encoder-decoder network learns a coarse\nrepresentation of the implant from down-sampled, defective skulls; The coarse\nimplant is only used to generate the bounding box of the defected region in the\noriginal high-resolution skull. Second, another encoder-decoder network is\ntrained to generate a fine implant from the bounded area. On the test set, the\nproposed approach achieves an average dice similarity score (DSC) of 0.8555 and\nHausdorff distance (HD) of 5.1825 mm. The code is publicly available at\nhttps://github.com/Jianningli/autoimplant.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:27:56 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 11:22:39 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Jianning", ""], ["Pepe", "Antonio", ""], ["Gsaxner", "Christina", ""], ["von Campe", "Gord", ""], ["Egger", "Jan", ""]]}, {"id": "2006.12661", "submitter": "Timur Zhukov", "authors": "T.A. Zhukov", "title": "SN-Engine, a Scale-free Geometric Modelling Environment", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new scale-free geometric modelling environment designed by the\nauthor of the paper. It allows one to consistently treat geometric objects of\narbitrary size and offers extensive analytic and computational support for\nvisualization of both real and artificial sceneries.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 23:02:10 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Zhukov", "T. A.", ""]]}, {"id": "2006.13166", "submitter": "Dan Reznik", "authors": "Ronaldo Garcia and Dan Reznik and Hellmuth Stachel and Mark Helman", "title": "Steiner's Hat: a Constant-Area Deltoid Associated with the Ellipse", "comments": "23 pages, 16 figures, 4 tables, 6 video links", "journal-ref": "KoG, 24. (24.), 12-28 (2020)", "doi": "10.31896/k.24.2", "report-no": null, "categories": "math.DS cs.CG cs.GR math.DG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Negative Pedal Curve (NPC) of the Ellipse with respect to a boundary\npoint M is a 3-cusp closed-curve which is the affine image of the Steiner\nDeltoid. Over all M the family has invariant area and displays an array of\ninteresting properties.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:15:18 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 10:33:05 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 00:44:14 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 13:55:03 GMT"}, {"version": "v5", "created": "Mon, 5 Oct 2020 15:36:26 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Garcia", "Ronaldo", ""], ["Reznik", "Dan", ""], ["Stachel", "Hellmuth", ""], ["Helman", "Mark", ""]]}, {"id": "2006.13188", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Benedict Brown, David Koller, Tim Weyrich, Szymon\n  Rusinkiewicz, Michael Kazhdan", "title": "Efficient Spatially Adaptive Convolution and Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast methods for convolution and correlation underlie a variety of\napplications in computer vision and graphics, including efficient filtering,\nanalysis, and simulation. However, standard convolution and correlation are\ninherently limited to fixed filters: spatial adaptation is impossible without\nsacrificing efficient computation. In early work, Freeman and Adelson have\nshown how steerable filters can address this limitation, providing a way for\nrotating the filter as it is passed over the signal. In this work, we provide a\ngeneral, representation-theoretic, framework that allows for spatially varying\nlinear transformations to be applied to the filter. This framework allows for\nefficient implementation of extended convolution and correlation for\ntransformation groups such as rotation (in 2D and 3D) and scale, and provides a\nnew interpretation for previous methods including steerable filters and the\ngeneralized Hough transform. We present applications to pattern matching, image\nfeature description, vector field visualization, and adaptive image filtering.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:41:10 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 16:36:04 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Brown", "Benedict", ""], ["Koller", "David", ""], ["Weyrich", "Tim", ""], ["Rusinkiewicz", "Szymon", ""], ["Kazhdan", "Michael", ""]]}, {"id": "2006.13240", "submitter": "Pablo Palafox", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Michael Zollh\\\"ofer, Angela\n  Dai, Justus Thies, Matthias Nie{\\ss}ner", "title": "Neural Non-Rigid Tracking", "comments": "Video: https://youtu.be/nqYaxM6Rj8I, Code:\n  https://github.com/DeformableFriends/NeuralTracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel, end-to-end learnable, differentiable non-rigid tracker\nthat enables state-of-the-art non-rigid reconstruction by a learned robust\noptimization. Given two input RGB-D frames of a non-rigidly moving object, we\nemploy a convolutional neural network to predict dense correspondences and\ntheir confidences. These correspondences are used as constraints in an\nas-rigid-as-possible (ARAP) optimization problem. By enabling gradient\nback-propagation through the weighted non-linear least squares solver, we are\nable to learn correspondences and confidences in an end-to-end manner such that\nthey are optimal for the task of non-rigid tracking. Under this formulation,\ncorrespondence confidences can be learned via self-supervision, informing a\nlearned robust optimization, where outliers and wrong correspondences are\nautomatically down-weighted to enable effective tracking. Compared to\nstate-of-the-art approaches, our algorithm shows improved reconstruction\nperformance, while simultaneously achieving 85 times faster correspondence\nprediction than comparable deep-learning based methods. We make our code\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:00:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 18:15:37 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Zollh\u00f6fer", "Michael", ""], ["Dai", "Angela", ""], ["Thies", "Justus", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2006.13266", "submitter": "Vin\\'icius da Silva", "authors": "Vin\\'icius da Silva, Claudio Esperan\\c{c}a and Ricardo Marroquim", "title": "OMiCroN -- Oblique Multipass Hierarchy Creation while Navigating", "comments": "13 pages, 15 figures", "journal-ref": "Computers & Graphics, Volume 84, November 2019, Pages 42-54", "doi": "10.1016/j.cag.2019.08.016", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering large point clouds ordinarily requires building a hierarchical data\nstructure for accessing the points that best represent the object for a given\nviewing frustum and level-of-detail. The building of such data structures\nfrequently represents a large portion of the cost of the rendering pipeline\nboth in terms of time and space complexity, especially when rendering is done\nfor inspection purposes only. This problem has been addressed in the past by\nincremental construction approaches, but these either result in low quality\nhierarchies or in longer construction times. In this work we present OMiCroN --\nOblique Multipass Hierarchy Creation while Navigating -- which is the first\nalgorithm capable of immediately displaying partial renders of the geometry,\nprovided the cloud is made available sorted in Morton order. OMiCroN is fast,\nbeing capable of building the entire data structure in memory spending an\namount of time that is comparable to that of just reading the cloud from disk.\nThus, there is no need for storing an expensive hierarchy, nor for delaying the\nrendering until the whole hierarchy is read from disk. In fact, a pipeline\ncoupling OMiCroN with an incremental sorting algorithm running in parallel can\nstart rendering as soon as the first sorted prefix is produced, making this\nsetup very convenient for streamed viewing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:46:16 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["da Silva", "Vin\u00edcius", ""], ["Esperan\u00e7a", "Claudio", ""], ["Marroquim", "Ricardo", ""]]}, {"id": "2006.13378", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Sen He, Sunzhou Huang, Danny Tsang, Lingjia Tang, Jason\n  Mars, and Wei Wang", "title": "A Benchmarking Framework for Interactive 3D Applications in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of cloud gaming and cloud virtual reality (VR),\ninteractive 3D applications have become a major type of workloads for the\ncloud. However, despite their growing importance, there is limited public\nresearch on how to design cloud systems to efficiently support these\napplications, due to the lack of an open and reliable research infrastructure,\nincluding benchmarks and performance analysis tools. The challenges of\ngenerating human-like inputs under various system/application randomness and\ndissecting the performance of complex graphics systems make it very difficult\nto design such an infrastructure. In this paper, we present the design of a\nnovel cloud graphics rendering research infrastructure, Pictor. Pictor employs\nAI to mimic human interactions with complex 3D applications. It can also\nprovide in-depth performance measurements for the complex software and hardware\nstack used for cloud 3D graphics rendering. With Pictor, we designed a\nbenchmark suite with six interactive 3D applications. Performance analyses were\nconducted with these benchmarks to characterize 3D applications in the cloud\nand reveal new performance bottlenecks. To demonstrate the effectiveness of\nPictor, we also implemented two optimizations to address two performance\nbottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering\nsystem, which improved the frame rate by 57.7% on average.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:11:30 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 16:06:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Tianyi", ""], ["He", "Sen", ""], ["Huang", "Sunzhou", ""], ["Tsang", "Danny", ""], ["Tang", "Lingjia", ""], ["Mars", "Jason", ""], ["Wang", "Wei", ""]]}, {"id": "2006.13782", "submitter": "Francis Williams", "authors": "Francis Williams, Matthew Trager, Joan Bruna, Denis Zorin", "title": "Neural Splines: Fitting 3D Surfaces with Infinitely-Wide Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Splines, a technique for 3D surface reconstruction that is\nbased on random feature kernels arising from infinitely-wide shallow ReLU\nnetworks. Our method achieves state-of-the-art results, outperforming recent\nneural network-based techniques and widely used Poisson Surface Reconstruction\n(which, as we demonstrate, can also be viewed as a type of kernel method).\nBecause our approach is based on a simple kernel formulation, it is easy to\nanalyze and can be accelerated by general techniques designed for kernel-based\nlearning. We provide explicit analytical expressions for our kernel and argue\nthat our formulation can be seen as a generalization of cubic spline\ninterpolation to higher dimensions. In particular, the RKHS norm associated\nwith Neural Splines biases toward smooth interpolants.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:54:59 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 21:57:44 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 13:56:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Williams", "Francis", ""], ["Trager", "Matthew", ""], ["Bruna", "Joan", ""], ["Zorin", "Denis", ""]]}, {"id": "2006.13846", "submitter": "Jim Nilsson", "authors": "Jim Nilsson and Tomas Akenine-M\\\"oller", "title": "Understanding SSIM", "comments": "8 pages. Please visit\n  https://research.nvidia.com/publication/2020-07_Understanding-SSIM for\n  supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of the structural similarity index (SSIM) is widespread. For almost\ntwo decades, it has played a major role in image quality assessment in many\ndifferent research disciplines. Clearly, its merits are indisputable in the\nresearch community. However, little deep scrutiny of this index has been\nperformed. Contrary to popular belief, there are some interesting properties of\nSSIM that merit such scrutiny. In this paper, we analyze the mathematical\nfactors of SSIM and show that it can generate results, in both synthetic and\nrealistic use cases, that are unexpected, sometimes undefined, and\nnonintuitive. As a consequence, assessing image quality based on SSIM can lead\nto incorrect conclusions and using SSIM as a loss function for deep learning\ncan guide neural network training in the wrong direction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:19:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 23:15:28 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Nilsson", "Jim", ""], ["Akenine-M\u00f6ller", "Tomas", ""]]}, {"id": "2006.14120", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Kim Marriott, Bernhard Jenny and Sarah Goodwin", "title": "Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and\n  Bar Chart in Immersive Environments", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": "10.1109/TVCG.2020.3004137", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Tilt Map, a novel interaction technique for intuitively\ntransitioning between 2D and 3D map visualisations in immersive environments.\nOur focus is visualising data associated with areal features on maps, for\nexample, population density by state. Tilt Map transitions from 2D choropleth\nmaps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our\npaper includes two user studies. The first study compares subjects' task\nperformance interpreting population density data using 2D choropleth maps and\n3D prism maps in virtual reality (VR). We observed greater task accuracy with\nprism maps, but faster response times with choropleth maps. The complementarity\nof these views inspired our hybrid Tilt Map design. Our second study compares\nTilt Map to: a side-by-side arrangement of the various views; and interactive\ntoggling between views. The results indicate benefits for Tilt Map in user\npreference; and accuracy (versus side-by-side) and time (versus toggle).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 00:52:57 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Jenny", "Bernhard", ""], ["Goodwin", "Sarah", ""]]}, {"id": "2006.14539", "submitter": "Bailin Deng", "authors": "Wenqing Ouyang and Yue Peng and Yuxin Yao and Juyong Zhang and Bailin\n  Deng", "title": "Anderson Acceleration for Nonconvex ADMM Based on Douglas-Rachford\n  Splitting", "comments": "To be published in Computer Graphis Forum and presented at\n  Eurographics Symposium on Geometry Processing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction multiplier method (ADMM) is widely used in computer\ngraphics for solving optimization problems that can be nonsmooth and nonconvex.\nIt converges quickly to an approximate solution, but can take a long time to\nconverge to a solution of high-accuracy. Previously, Anderson acceleration has\nbeen applied to ADMM, by treating it as a fixed-point iteration for the\nconcatenation of the dual variables and a subset of the primal variables. In\nthis paper, we note that the equivalence between ADMM and Douglas-Rachford\nsplitting reveals that ADMM is in fact a fixed-point iteration in a\nlower-dimensional space. By applying Anderson acceleration to such\nlower-dimensional fixed-point iteration, we obtain a more effective approach\nfor accelerating ADMM. We analyze the convergence of the proposed acceleration\nmethod on nonconvex problems, and verify its effectiveness on a variety of\ncomputer graphics problems including geometry processing and physical\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:37:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 09:18:20 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Ouyang", "Wenqing", ""], ["Peng", "Yue", ""], ["Yao", "Yuxin", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""]]}, {"id": "2006.14726", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann", "title": "Augmenting Image Warping-Based Remote Volume Rendering with Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image warping-based remote rendering technique for volumes that\ndecouples the rendering and display phases. Our work builds on prior work that\nsamples the volume on the client using ray casting and reconstructs a z-value\nbased on some heuristic. The color and depth buffer are then sent to the client\nthat reuses this depth image as a stand-in for subsequent frames by warping it\naccording to the current camera position until new data was received from the\nserver. We augment that method by implementing the client renderer using ray\ntracing. By representing the pixel contributions as spheres, this allows us to\neffectively vary their footprint based on the distance to the viewer, which we\nfind to give better results than point-based rasterization when applied to\nvolumetric data sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 22:56:45 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zellmann", "Stefan", ""]]}, {"id": "2006.14865", "submitter": "Zihao Yan", "authors": "Zihao Yan, Ruizhen Hu, Xingguang Yan, Luanmin Chen, Oliver van Kaick,\n  Hao Zhang, Hui Huang", "title": "RPM-Net: Recurrent Prediction of Motion and Parts from Point Cloud", "comments": "Accepted to SIGGRAPH Asia 2019, project page at\n  https://vcc.tech/research/2019/RPMNet", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia),\n  volume 38, number 6, pages 240:1--240:15, year 2019", "doi": "10.1145/3355089.3356573", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RPM-Net, a deep learning-based approach which simultaneously\ninfers movable parts and hallucinates their motions from a single,\nun-segmented, and possibly partial, 3D point cloud shape. RPM-Net is a novel\nRecurrent Neural Network (RNN), composed of an encoder-decoder pair with\ninterleaved Long Short-Term Memory (LSTM) components, which together predict a\ntemporal sequence of pointwise displacements for the input point cloud. At the\nsame time, the displacements allow the network to learn movable parts,\nresulting in a motion-based shape segmentation. Recursive applications of\nRPM-Net on the obtained parts can predict finer-level part motions, resulting\nin a hierarchical object segmentation. Furthermore, we develop a separate\nnetwork to estimate part mobilities, e.g., per-part motion parameters, from the\nsegmented motion sequence. Both networks learn deep predictive models from a\ntraining set that exemplifies a variety of mobilities for diverse objects. We\nshow results of simultaneous motion and part predictions from synthetic and\nreal scans of 3D objects exhibiting a variety of part mobilities, possibly\ninvolving multiple movable parts.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 08:51:11 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Yan", "Zihao", ""], ["Hu", "Ruizhen", ""], ["Yan", "Xingguang", ""], ["Chen", "Luanmin", ""], ["van Kaick", "Oliver", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2006.15059", "submitter": "Jos Stam", "authors": "Jos Stam", "title": "Computing Light Transport Gradients using the Adjoint Method", "comments": "23 pages, 8 figures, unpublished manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new equation from continuous adjoint theory to compute\nthe gradient of quantities governed by the Transport Theory of light. Unlike\ndiscrete gradients ala autograd, which work at the code level, we first\nformulate the continuous theory and then discretize it. The key insight of this\npaper is that computing gradients in Transport Theory is akin to computing the\nimportance, a quantity adjoint to radiance that satisfies an adjoint equation.\nImportance tells us where to look for light that matters. This is one of the\nkey insights of this paper. In fact, this mathematical journey started from a\nwhimsical thought that these adjoints might be related. Computing gradients is\ntherefore no more complicated than computing the importance field. This insight\nand the following paper hopefully will shed some light on this complicated\nproblem and ease the implementations of gradient computations in existing path\ntracers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:38:14 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Stam", "Jos", ""]]}, {"id": "2006.15432", "submitter": "Thiago Porcino", "authors": "Thiago Porcino, Esteban Clua, Daniela Trevisan, \\'Erick Rodrigues,\n  Alexandre Silva", "title": "Automatic Recommendation of Strategies for Minimizing Discomfort in\n  Virtual Environments", "comments": "Accepted at the IEEE 8th International Conference on Serious Games\n  and Applications for Health (SeGAH) - SeGAH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) is an imminent trend in games, education, entertainment,\nmilitary, and health applications, as the use of head-mounted displays is\nbecoming accessible to the mass market. Virtual reality provides immersive\nexperiences but still does not offer an entirely perfect situation, mainly due\nto Cybersickness (CS) issues. In this work, we first present a detailed review\nabout possible causes of CS. Following, we propose a novel CS prediction\nsolution. Our system is able to suggest if the user may be entering in the next\nmoments of the application into an illness situation. We use Random Forest\nclassifiers, based on a dataset we have produced. The CSPQ (Cybersickness\nProfile Questionnaire) is also proposed, which is used to identify the player's\nsusceptibility to CS and the dataset construction. In addition, we designed two\nimmersive environments for empirical studies where participants are asked to\ncomplete the questionnaire and describe (orally) the degree of discomfort\nduring their gaming experience. Our data was achieved through 84 individuals on\ndifferent days, using VR devices. Our proposal also allows us to identify which\nare the most frequent attributes (causes) in the observed discomfort\nsituations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 19:28:48 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Porcino", "Thiago", ""], ["Clua", "Esteban", ""], ["Trevisan", "Daniela", ""], ["Rodrigues", "\u00c9rick", ""], ["Silva", "Alexandre", ""]]}, {"id": "2006.15510", "submitter": "Xianzhi Li", "authors": "Xianzhi Li and Ruihui Li and Lei Zhu and Chi-Wing Fu and Pheng-Ann\n  Heng", "title": "DNF-Net: a Deep Normal Filtering Network for Mesh Denoising", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3001681", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep normal filtering network, called DNF-Net, for mesh\ndenoising. To better capture local geometry, our network processes the mesh in\nterms of local patches extracted from the mesh. Overall, DNF-Net is an\nend-to-end network that takes patches of facet normals as inputs and directly\noutputs the corresponding denoised facet normals of the patches. In this way,\nwe can reconstruct the geometry from the denoised normals with feature\npreservation. Besides the overall network architecture, our contributions\ninclude a novel multi-scale feature embedding unit, a residual learning\nstrategy to remove noise, and a deeply-supervised joint loss function. Compared\nwith the recent data-driven works on mesh denoising, DNF-Net does not require\nmanual input to extract features and better utilizes the training data to\nenhance its denoising performance. Finally, we present comprehensive\nexperiments to evaluate our method and demonstrate its superiority over the\nstate of the art on both synthetic and real-scanned meshes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:04:01 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Xianzhi", ""], ["Li", "Ruihui", ""], ["Zhu", "Lei", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2006.15520", "submitter": "Zihao Yan", "authors": "Ruizhen Hu, Zihao Yan, Jingwen Zhang, Oliver van Kaick, Ariel Shamir,\n  Hao Zhang, Hui Huang", "title": "Predictive and Generative Neural Networks for Object Functionality", "comments": "Accepted to SIGGRAPH 2018, project page at\n  https://vcc.tech/research/2018/ICON4", "journal-ref": "ACM Transactions on Graphics (Proc. SIGGRAPH), volume 37, number\n  4, pages 151:1--151:14, year 2018", "doi": "10.1145/3197517.3201287", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can predict the functionality of an object even without any\nsurroundings, since their knowledge and experience would allow them to\n\"hallucinate\" the interaction or usage scenarios involving the object. We\ndevelop predictive and generative deep convolutional neural networks to\nreplicate this feat. Specifically, our work focuses on functionalities of\nman-made 3D objects characterized by human-object or object-object\ninteractions. Our networks are trained on a database of scene contexts, called\ninteraction contexts, each consisting of a central object and one or more\nsurrounding objects, that represent object functionalities. Given a 3D object\nin isolation, our functional similarity network (fSIM-NET), a variation of the\ntriplet network, is trained to predict the functionality of the object by\ninferring functionality-revealing interaction contexts. fSIM-NET is\ncomplemented by a generative network (iGEN-NET) and a segmentation network\n(iSEG-NET). iGEN-NET takes a single voxelized 3D object with a functionality\nlabel and synthesizes a voxelized surround, i.e., the interaction context which\nvisually demonstrates the corresponding functionality. iSEG-NET further\nseparates the interacting objects into different groups according to their\ninteraction types.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 05:40:05 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Hu", "Ruizhen", ""], ["Yan", "Zihao", ""], ["Zhang", "Jingwen", ""], ["van Kaick", "Oliver", ""], ["Shamir", "Ariel", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2006.16011", "submitter": "Hassan Abu Alhaija", "authors": "Hassan Abu Alhaija, Siva Karthik Mustikovela, Justus Thies, Varun\n  Jampani, Matthias Nie{\\ss}ner, Andreas Geiger, Carsten Rother", "title": "Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural rendering techniques promise efficient photo-realistic image synthesis\nwhile at the same time providing rich control over scene parameters by learning\nthe physical image formation process. While several supervised methods have\nbeen proposed for this task, acquiring a dataset of images with accurately\naligned 3D models is very difficult. The main contribution of this work is to\nlift this restriction by training a neural rendering algorithm from unpaired\ndata. More specifically, we propose an autoencoder for joint generation of\nrealistic images from synthetic 3D models while simultaneously decomposing real\nimages into their intrinsic shape and appearance properties. In contrast to a\ntraditional graphics pipeline, our approach does not require to specify all\nscene properties, such as material parameters and lighting by hand. Instead, we\nlearn photo-realistic deferred rendering from a small set of 3D models and a\nlarger set of unaligned real images, both of which are easy to acquire in\npractice. Simultaneously, we obtain accurate intrinsic decompositions of real\nimages while not requiring paired ground truth. Our experiments confirm that a\njoint treatment of rendering and decomposition is indeed beneficial and that\nour approach outperforms state-of-the-art image-to-image translation baselines\nboth qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 12:53:58 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:45:04 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 10:27:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Alhaija", "Hassan Abu", ""], ["Mustikovela", "Siva Karthik", ""], ["Thies", "Justus", ""], ["Jampani", "Varun", ""], ["Nie\u00dfner", "Matthias", ""], ["Geiger", "Andreas", ""], ["Rother", "Carsten", ""]]}, {"id": "2006.16112", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier, Siavash Bigdeli, Orcun Goksel", "title": "GramGAN: Deep 3D Texture Synthesis From 2D Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel texture synthesis framework, enabling the generation of\ninfinite, high-quality 3D textures given a 2D exemplar image. Inspired by\nrecent advances in natural texture synthesis, we train deep neural models to\ngenerate textures by non-linearly combining learned noise frequencies. To\nachieve a highly realistic output conditioned on an exemplar patch, we propose\na novel loss function that combines ideas from both style transfer and\ngenerative adversarial networks. In particular, we train the synthesis network\nto match the Gram matrices of deep features from a discriminator network. In\naddition, we propose two architectural concepts and an extrapolation strategy\nthat significantly improve generalization performance. In particular, we inject\nboth model input and condition into hidden network layers by learning to scale\nand bias hidden activations. Quantitative and qualitative evaluations on a\ndiverse set of exemplars motivate our design decisions and show that our system\nperforms superior to previous state of the art. Finally, we conduct a user\nstudy that confirms the benefits of our framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 15:22:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 10:33:59 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Portenier", "Tiziano", ""], ["Bigdeli", "Siavash", ""], ["Goksel", "Orcun", ""]]}]