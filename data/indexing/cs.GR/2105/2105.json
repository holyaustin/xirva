[{"id": "2105.00371", "submitter": "KangKang Yin", "authors": "Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin", "title": "Discovering Diverse Athletic Jumping Strategies", "comments": "17 pages; SIGGRAPH 2021", "journal-ref": "ACM Trans. Graph. 40, 4, Article 91 (August 2021), 17 pages (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that enables the discovery of diverse and\nnatural-looking motion strategies for athletic skills such as the high jump.\nThe strategies are realized as control policies for physics-based characters.\nGiven a task objective and an initial character configuration, the combination\nof physics simulation and deep reinforcement learning (DRL) provides a suitable\nstarting point for automatic control policy training. To facilitate the\nlearning of realistic human motions, we propose a Pose Variational Autoencoder\n(P-VAE) to constrain the actions to a subspace of natural poses. In contrast to\nmotion imitation methods, a rich variety of novel strategies can naturally\nemerge by exploring initial character states through a sample-efficient\nBayesian diversity search (BDS) algorithm. A second stage of optimization that\nencourages novel policies can further enrich the unique strategies discovered.\nOur method allows for the discovery of diverse and novel strategies for\nathletic jumping motions such as high jumps and obstacle jumps with no motion\nexamples and less reward engineering than prior work.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 01:37:16 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Yin", "Zhiqi", ""], ["Yang", "Zeshi", ""], ["van de Panne", "Michiel", ""], ["Yin", "KangKang", ""]]}, {"id": "2105.00854", "submitter": "Pei Lv", "authors": "Pei Lv, Boya Xu, Chaochao Li, Qingqing Yu, Bing Zhou, Mingliang Xu", "title": "Antagonistic Crowd Simulation Model Integrating Emotion Contagion and\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.GR physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The antagonistic behavior of the crowd often exacerbates the seriousness of\nthe situation in sudden riots, where the spreading of antagonistic emotion and\nbehavioral decision making in the crowd play very important roles. However, the\nmechanism of complex emotion influencing decision making, especially in the\nenvironment of sudden confrontation, has not yet been explored clearly. In this\npaper, we propose one new antagonistic crowd simulation model by combing\nemotional contagion and deep reinforcement learning (ACSED). Firstly, we build\na group emotional contagion model based on the improved SIS contagion disease\nmodel, and estimate the emotional state of the group at each time step during\nthe simulation. Then, the tendency of group antagonistic behavior is modeled\nbased on Deep Q Network (DQN), where the agent can learn the combat behavior\nautonomously, and leverages the mean field theory to quickly calculate the\ninfluence of other surrounding individuals on the central one. Finally, the\nrationality of the predicted behaviors by the DQN is further analyzed in\ncombination with group emotion, and the final combat behavior of the agent is\ndetermined. The method proposed in this paper is verified through several\ndifferent settings of experiments. The results prove that emotions have a vital\nimpact on the group combat, and positive emotional states are more conducive to\ncombat. Moreover, by comparing the simulation results with real scenes, the\nfeasibility of the method is further verified, which can provide good reference\nfor formulating battle plans and improving the winning rate of righteous groups\nbattles in a variety of situations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:18:13 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lv", "Pei", ""], ["Xu", "Boya", ""], ["Li", "Chaochao", ""], ["Yu", "Qingqing", ""], ["Zhou", "Bing", ""], ["Xu", "Mingliang", ""]]}, {"id": "2105.01057", "submitter": "Soshi Shimada", "authors": "Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Patrick\n  P\\'erez and Christian Theobalt", "title": "Neural Monocular 3D Human Motion Capture with Physical Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new trainable system for physically plausible markerless 3D\nhuman motion capture, which achieves state-of-the-art results in a broad range\nof challenging scenarios. Unlike most neural methods for human motion capture,\nour approach, which we dub physionical, is aware of physical and environmental\nconstraints. It combines in a fully differentiable way several key innovations,\ni.e., 1. a proportional-derivative controller, with gains predicted by a neural\nnetwork, that reduces delays even in the presence of fast motions, 2. an\nexplicit rigid body dynamics model and 3. a novel optimisation layer that\nprevents physically implausible foot-floor penetration as a hard constraint.\nThe inputs to our system are 2D joint keypoints, which are canonicalised in a\nnovel way so as to reduce the dependency on intrinsic camera parameters -- both\nat train and test time. This enables more accurate global translation\nestimation without generalisability loss. Our model can be finetuned only with\n2D annotations when the 3D annotations are not available. It produces smooth\nand physically principled 3D motions in an interactive frame rate in a wide\nvariety of challenging scenes, including newly recorded ones. Its advantages\nare especially noticeable on in-the-wild sequences that significantly differ\nfrom common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP.\nQualitative results are available at\nhttp://gvv.mpi-inf.mpg.de/projects/PhysAware/\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:57:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Xu", "Weipeng", ""], ["P\u00e9rez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "2105.01500", "submitter": "Wei-Chang Yeh", "authors": "Wei-Chang Yeh", "title": "Novel Algorithm for Computing All-Pairs Homogeneity-Arc Binary-State\n  Undirected Network Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.GR math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among various real-life emerging applications, wireless sensor networks,\nInternet of Things, smart grids, social networks, communication networks,\ntransportation networks, and computer grid systems, etc., the binary-state\nnetwork is the fundamental network structure and model with either working or\nfailed binary components. The network reliability is an effective index for\nassessing the network function and performance. Hence, the network reliability\nbetween two specific nodes has been widely adopted and more efficient network\nreliability algorithm is always needed. To have complete information for a\nbetter decision, all-pairs network reliability thus arises correspondingly. In\nthis study, a new algorithm called the all-pairs BAT is proposed by revising\nthe binary-addition-tree algorithm (BAT) and the layered-search algorithm\n(LSA). From both the theoretical analysis and the practical experiments\nconducted on 20 benchmark problems, the proposed all-pairs BAT is more\nefficient than these algorithms by trying all combinations of any pairs of\nnodes.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:58:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Yeh", "Wei-Chang", ""]]}, {"id": "2105.01604", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Denis Zorin, Raja Giryes, Daniele Panozzo,\n  Daniel Cohen-Or", "title": "Orienting Point Clouds with Dipole Propagation", "comments": "SIGGRAPH 2021", "journal-ref": null, "doi": "10.1145/3450626.3459835", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing a consistent normal orientation for point clouds is a\nnotoriously difficult problem in geometry processing, requiring attention to\nboth local and global shape characteristics. The normal direction of a point is\na function of the local surface neighborhood; yet, point clouds do not disclose\nthe full underlying surface structure. Even assuming known geodesic proximity,\ncalculating a consistent normal orientation requires the global context. In\nthis work, we introduce a novel approach for establishing a globally consistent\nnormal orientation for point clouds. Our solution separates the local and\nglobal components into two different sub-problems. In the local phase, we train\na neural network to learn a coherent normal direction per patch (i.e.,\nconsistently oriented normals within a single patch). In the global phase, we\npropagate the orientation across all coherent patches using a dipole\npropagation. Our dipole propagation decides to orient each patch using the\nelectric field defined by all previously orientated patches. This gives rise to\na global propagation that is stable, as well as being robust to nearby\nsurfaces, holes, sharp features and noise.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:25:36 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Zorin", "Denis", ""], ["Giryes", "Raja", ""], ["Panozzo", "Daniele", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.01610", "submitter": "Daniel Bogdoll", "authors": "Lars T\\\"ottel, Maximilian Zipfl, Daniel Bogdoll, Marc Ren\\'e Zofka, J.\n  Marius Z\\\"ollner", "title": "Reliving the Dataset: Combining the Visualization of Road Users'\n  Interactions with Scenario Reconstruction in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One core challenge in the development of automated vehicles is their\ncapability to deal with a multitude of complex trafficscenarios with many, hard\nto predict traffic participants. As part of the iterative development process,\nit is necessary to detect criticalscenarios and generate knowledge from them to\nimprove the highly automated driving (HAD) function. In order to tackle this\nchallenge,numerous datasets have been released in the past years, which act as\nthe basis for the development and testing of such algorithms.Nevertheless, the\nremaining challenges are to find relevant scenes, such as safety-critical\ncorner cases, in these datasets and tounderstand them completely.Therefore,\nthis paper presents a methodology to process and analyze naturalistic motion\ndatasets in two ways: On the one hand, ourapproach maps scenes of the datasets\nto a generic semantic scene graph which allows for a high-level and objective\nanalysis. Here,arbitrary criticality measures, e.g. TTC, RSS or SFF, can be set\nto automatically detect critical scenarios between traffic participants.On the\nother hand, the scenarios are recreated in a realistic virtual reality (VR)\nenvironment, which allows for a subjective close-upanalysis from multiple,\ninteractive perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 16:39:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["T\u00f6ttel", "Lars", ""], ["Zipfl", "Maximilian", ""], ["Bogdoll", "Daniel", ""], ["Zofka", "Marc Ren\u00e9", ""], ["Z\u00f6llner", "J. Marius", ""]]}, {"id": "2105.01768", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Texture for Colors: Natural Representations of Colors Using Variable\n  Bit-Depth Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous methods have been proposed to transform color and grayscale images\nto their single bit-per-pixel binary counterparts. Commonly, the goal is to\nenhance specific attributes of the original image to make it more amenable for\nanalysis. However, when the resulting binarized image is intended for human\nviewing, aesthetics must also be considered. Binarization techniques, such as\nhalf-toning, stippling, and hatching, have been widely used for modeling the\noriginal image's intensity profile. We present an automated method to transform\nan image to a set of binary textures that represent not only the intensities,\nbut also the colors of the original. The foundation of our method is\ninformation preservation: creating a set of textures that allows for the\nreconstruction of the original image's colors solely from the binarized\nrepresentation. We present techniques to ensure that the textures created are\nnot visually distracting, preserve the intensity profile of the images, and are\nnatural in that they map sets of colors that are perceptually similar to\npatterns that are similar. The approach uses deep-neural networks and is\nentirely self-supervised; no examples of good vs. bad binarizations are\nrequired. The system yields aesthetically pleasing binary images when tested on\na variety of image sources.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:22:02 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "2105.01937", "submitter": "Brian Gordon", "authors": "Brian Gordon, Sigal Raab, Guy Azov, Raja Giryes, Daniel Cohen-Or", "title": "FLEX: Parameter-free Multi-view 3D Human Motion Reconstruction", "comments": "Project page: https://briang13.github.io/FLEX/ Video:\n  https://youtu.be/nMMmfWxA3xI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing availability of video recordings made by multiple cameras has\noffered new means for mitigating occlusion and depth ambiguities in pose and\nmotion reconstruction methods. Yet, multi-view algorithms strongly depend on\ncamera parameters, in particular, the relative positions among the cameras.\nSuch dependency becomes a hurdle once shifting to dynamic capture in\nuncontrolled settings. We introduce FLEX (Free muLti-view rEconstruXion), an\nend-to-end parameter-free multi-view model. FLEX is parameter-free in the sense\nthat it does not require any camera parameters, neither intrinsic nor\nextrinsic. Our key idea is that the 3D angles between skeletal parts, as well\nas bone lengths, are invariant to the camera position. Hence, learning 3D\nrotations and bone lengths rather than locations allows predicting common\nvalues for all camera views. Our network takes multiple video streams, learns\nfused deep features through a novel multi-view fusion layer, and reconstructs a\nsingle consistent skeleton with temporally coherent joint rotations. We\ndemonstrate quantitative and qualitative results on the Human3.6M and KTH\nMulti-view Football II datasets. We compare our model to state-of-the-art\nmethods that are not parameter-free and show that in the absence of camera\nparameters, we outperform them by a large margin while obtaining comparable\nresults when camera parameters are available. Code, trained models, video\ndemonstration, and additional materials will be available on our project page.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:08:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gordon", "Brian", ""], ["Raab", "Sigal", ""], ["Azov", "Guy", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.02451", "submitter": "Peizhuo Li", "authors": "Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga\n  Sorkine-Hornung, Baoquan Chen", "title": "Learning Skeletal Articulations with Neural Blend Shapes", "comments": "SIGGRAPH 2021. Project page:\n  https://peizhuoli.github.io/neural-blend-shapes/ , Video:\n  https://youtu.be/antc20EFh6k", "journal-ref": null, "doi": "10.1145/3450626.3459852", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animating a newly designed character using motion capture (mocap) data is a\nlong standing problem in computer animation. A key consideration is the\nskeletal structure that should correspond to the available mocap data, and the\nshape deformation in the joint regions, which often requires a tailored,\npose-specific refinement. In this work, we develop a neural technique for\narticulating 3D characters using enveloping with a pre-defined skeletal\nstructure which produces high quality pose dependent deformations. Our\nframework learns to rig and skin characters with the same articulation\nstructure (e.g., bipeds or quadrupeds), and builds the desired skeleton\nhierarchy into the network architecture. Furthermore, we propose neural blend\nshapes--a set of corrective pose-dependent shapes which improve the deformation\nquality in the joint regions in order to address the notorious artifacts\nresulting from standard rigging and skinning. Our system estimates neural blend\nshapes for input meshes with arbitrary connectivity, as well as weighting\ncoefficients which are conditioned on the input joint rotations. Unlike recent\ndeep learning techniques which supervise the network with ground-truth rigging\nand skinning parameters, our approach does not assume that the training data\nhas a specific underlying deformation model. Instead, during training, the\nnetwork observes deformed shapes and learns to infer the corresponding rig,\nskin and blend shapes using indirect supervision. During inference, we\ndemonstrate that our network generalizes to unseen characters with arbitrary\nmesh connectivity, including unrigged characters built by 3D artists.\nConforming to standard skeletal animation models enables direct plug-and-play\nin standard animation software, as well as game engines.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 05:58:13 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Peizhuo", ""], ["Aberman", "Kfir", ""], ["Hanocka", "Rana", ""], ["Liu", "Libin", ""], ["Sorkine-Hornung", "Olga", ""], ["Chen", "Baoquan", ""]]}, {"id": "2105.02475", "submitter": "Zahra Montazeri", "authors": "Zahra Montazeri, Soren Gammelmark, Henrik W. Jensen, Shuang Zhao", "title": "A Practical Ply-Based Appearance Modeling for Knitted Fabrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling the geometry and the appearance of knitted fabrics has been\nchallenging due to their complex geometries and interactions with light.\n  Previous surface-based models have difficulties capturing fine-grained knit\ngeometries; Micro-appearance models, on the other hands, typically store\nindividual cloth fibers explicitly and are expensive to be generated and\nrendered.\n  Further, neither of the models have been matched the photographs to capture\nboth the reflection and the transmission of light simultaneously.\n  In this paper, we introduce an efficient technique to generate knit models\nwith user-specified knitting patterns.\n  Our model stores individual knit plies with fiber-level detailed depicted\nusing normal and tangent mapping.\n  We evaluate our generated models using a wide array of knitting patterns.\nFurther, we compare qualitatively renderings to our models to photos of real\nsamples.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:13:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Montazeri", "Zahra", ""], ["Gammelmark", "Soren", ""], ["Jensen", "Henrik W.", ""], ["Zhao", "Shuang", ""]]}, {"id": "2105.02507", "submitter": "Michal Edelstein", "authors": "Kacper Pluta, Michal Edelstein, Amir Vaxman, Mirela Ben-Chen", "title": "PH-CPF: Planar Hexagonal Meshing using Coordinate Power Fields", "comments": "19 pages (excluding supplementary material), 25 figures, SIGGRAPH\n  2021", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 156. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459770", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new approach for computing planar hexagonal meshes that\napproximate a given surface, represented as a triangle mesh. Our method is\nbased on two novel technical contributions. First, we introduce Coordinate\nPower Fields, which are a pair of tangent vector fields on the surface that\nfulfill a certain continuity constraint. We prove that the fulfillment of this\nconstraint guarantees the existence of a seamless parameterization with\nquantized rotational jumps, which we then use to regularly remesh the surface.\nWe additionally propose an optimization framework for finding Coordinate Power\nFields, which also fulfill additional constraints, such as alignment, sizing\nand bijectivity. Second, we build upon this framework to address a challenging\nmeshing problem: planar hexagonal meshing. To this end, we suggest a\ncombination of conjugacy, scaling and alignment constraints, which together\nlead to planarizable hexagons. We demonstrate our approach on a variety of\nsurfaces, automatically generating planar hexagonal meshes on complicated\nmeshes, which were not achievable with existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:16:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Pluta", "Kacper", ""], ["Edelstein", "Michal", ""], ["Vaxman", "Amir", ""], ["Ben-Chen", "Mirela", ""]]}, {"id": "2105.02788", "submitter": "Julien Martel", "authors": "Julien N. P. Martel, David B. Lindell, Connor Z. Lin, Eric R. Chan,\n  Marco Monteiro and Gordon Wetzstein", "title": "ACORN: Adaptive Coordinate Networks for Neural Scene Representation", "comments": "J. N. P. Martel and D. B. Lindell equally contributed to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural representations have emerged as a new paradigm for applications in\nrendering, imaging, geometric modeling, and simulation. Compared to traditional\nrepresentations such as meshes, point clouds, or volumes they can be flexibly\nincorporated into differentiable learning-based pipelines. While recent\nimprovements to neural representations now make it possible to represent\nsignals with fine details at moderate resolutions (e.g., for images and 3D\nshapes), adequately representing large-scale or complex scenes has proven a\nchallenge. Current neural representations fail to accurately represent images\nat resolutions greater than a megapixel or 3D scenes with more than a few\nhundred thousand polygons. Here, we introduce a new hybrid implicit-explicit\nnetwork architecture and training strategy that adaptively allocates resources\nduring training and inference based on the local complexity of a signal of\ninterest. Our approach uses a multiscale block-coordinate decomposition,\nsimilar to a quadtree or octree, that is optimized during training. The network\narchitecture operates in two stages: using the bulk of the network parameters,\na coordinate encoder generates a feature grid in a single forward pass. Then,\nhundreds or thousands of samples within each block can be efficiently evaluated\nusing a lightweight feature decoder. With this hybrid implicit-explicit network\narchitecture, we demonstrate the first experiments that fit gigapixel images to\nnearly 40 dB peak signal-to-noise ratio. Notably this represents an increase in\nscale of over 1000x compared to the resolution of previously demonstrated\nimage-fitting experiments. Moreover, our approach is able to represent 3D\nshapes significantly faster and better than previous techniques; it reduces\ntraining times from days to hours or minutes and memory requirements by over an\norder of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 16:21:38 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Martel", "Julien N. P.", ""], ["Lindell", "David B.", ""], ["Lin", "Connor Z.", ""], ["Chan", "Eric R.", ""], ["Monteiro", "Marco", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2105.02875", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, Yiming Lin and Abhijeet Ghosh", "title": "Deep Polarization Imaging for 3D shape and SVBRDF Acquisition", "comments": "CVPR 2021 Oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for efficient acquisition of shape and spatially\nvarying reflectance of 3D objects using polarization cues. Unlike previous\nworks that have exploited polarization to estimate material or object\nappearance under certain constraints (known shape or multiview acquisition), we\nlift such restrictions by coupling polarization imaging with deep learning to\nachieve high quality estimate of 3D object shape (surface normals and depth)\nand SVBRDF using single-view polarization imaging under frontal flash\nillumination. In addition to acquired polarization images, we provide our deep\nnetwork with strong novel cues related to shape and reflectance, in the form of\na normalized Stokes map and an estimate of diffuse color. We additionally\ndescribe modifications to network architecture and training loss which provide\nfurther qualitative improvements. We demonstrate our approach to achieve\nsuperior results compared to recent works employing deep learning in\nconjunction with flash illumination.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:43 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Lin", "Yiming", ""], ["Ghosh", "Abhijeet", ""]]}, {"id": "2105.02976", "submitter": "Gengshan Yang", "authors": "Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester\n  Cole, Huiwen Chang, Deva Ramanan, William T. Freeman, Ce Liu", "title": "LASR: Learning Articulated Shape Reconstruction from a Monocular Video", "comments": "CVPR 2021. Project page: https://lasr-google.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Remarkable progress has been made in 3D reconstruction of rigid structures\nfrom a video or a collection of images. However, it is still challenging to\nreconstruct nonrigid structures from RGB inputs, due to its under-constrained\nnature. While template-based approaches, such as parametric shape models, have\nachieved great success in modeling the \"closed world\" of known object\ncategories, they cannot well handle the \"open-world\" of novel object categories\nor outlier shapes. In this work, we introduce a template-free approach to learn\n3D shapes from a single video. It adopts an analysis-by-synthesis strategy that\nforward-renders object silhouette, optical flow, and pixel values to compare\nwith video observations, which generates gradients to adjust the camera, shape\nand motion parameters. Without using a category-specific shape template, our\nmethod faithfully reconstructs nonrigid 3D structures from videos of human,\nanimals, and objects of unknown classes. Code will be available at\nlasr-google.github.io .\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:41:11 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yang", "Gengshan", ""], ["Sun", "Deqing", ""], ["Jampani", "Varun", ""], ["Vlasic", "Daniel", ""], ["Cole", "Forrester", ""], ["Chang", "Huiwen", ""], ["Ramanan", "Deva", ""], ["Freeman", "William T.", ""], ["Liu", "Ce", ""]]}, {"id": "2105.04106", "submitter": "Zheng Lyu", "authors": "Zheng Lyu, Krithin Kripakaran, Max Furth, Eric Tang, Brian Wandell,\n  and Joyce Farrell", "title": "Validation of image systems simulation technology using a Cornell Box", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe and experimentally validate an end-to-end simulation of a digital\ncamera. The simulation models the spectral radiance of 3D-scenes, formation of\nthe spectral irradiance by multi-element optics, and conversion of the\nirradiance to digital values by the image sensor. We quantify the accuracy of\nthe simulation by comparing real and simulated images of a precisely\nconstructed, three-dimensional high dynamic range test scene. Validated\nend-to-end software simulation of a digital camera can accelerate innovation by\nreducing many of the time-consuming and expensive steps in designing, building\nand evaluating image systems.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:14:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lyu", "Zheng", ""], ["Kripakaran", "Krithin", ""], ["Furth", "Max", ""], ["Tang", "Eric", ""], ["Wandell", "Brian", ""], ["Farrell", "Joyce", ""]]}, {"id": "2105.04605", "submitter": "Xinyu Yi", "authors": "Xinyu Yi, Yuxiao Zhou, Feng Xu", "title": "TransPose: Real-time 3D Human Translation and Pose Estimation with Six\n  Inertial Sensors", "comments": "Accepted by SIGGRAPH 2021. Project page:\n  https://xinyu-yi.github.io/TransPose/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion capture is facing some new possibilities brought by the inertial\nsensing technologies which do not suffer from occlusion or wide-range\nrecordings as vision-based solutions do. However, as the recorded signals are\nsparse and quite noisy, online performance and global translation estimation\nturn out to be two key difficulties. In this paper, we present TransPose, a\nDNN-based approach to perform full motion capture (with both global\ntranslations and body poses) from only 6 Inertial Measurement Units (IMUs) at\nover 90 fps. For body pose estimation, we propose a multi-stage network that\nestimates leaf-to-full joint positions as intermediate results. This design\nmakes the pose estimation much easier, and thus achieves both better accuracy\nand lower computation cost. For global translation estimation, we propose a\nsupporting-foot-based method and an RNN-based method to robustly solve for the\nglobal translations with a confidence-based fusion technique. Quantitative and\nqualitative comparisons show that our method outperforms the state-of-the-art\nlearning- and optimization-based methods with a large margin in both accuracy\nand efficiency. As a purely inertial sensor-based approach, our method is not\nlimited by environmental settings (e.g., fixed cameras), making the capture\nfree from common difficulties such as wide-range motion space and strong\nocclusion.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:41:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Yi", "Xinyu", ""], ["Zhou", "Yuxiao", ""], ["Xu", "Feng", ""]]}, {"id": "2105.04619", "submitter": "Stephan R Richter", "authors": "Stephan R. Richter and Hassan Abu AlHaija and Vladlen Koltun", "title": "Enhancing Photorealism Enhancement", "comments": "Code and data available at\n  https://github.com/intel-isl/PhotorealismEnhancement Video available at\n  https://youtu.be/P1IcaBn3ej0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to enhancing the realism of synthetic images. The\nimages are enhanced by a convolutional network that leverages intermediate\nrepresentations produced by conventional rendering pipelines. The network is\ntrained via a novel adversarial objective, which provides strong supervision at\nmultiple perceptual levels. We analyze scene layout distributions in commonly\nused datasets and find that they differ in important ways. We hypothesize that\nthis is one of the causes of strong artifacts that can be observed in the\nresults of many prior methods. To address this we propose a new strategy for\nsampling image patches during training. We also introduce multiple\narchitectural improvements in the deep network modules used for photorealism\nenhancement. We confirm the benefits of our contributions in controlled\nexperiments and report substantial gains in stability and realism in comparison\nto recent image-to-image translation methods and a variety of other baselines.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:00:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Richter", "Stephan R.", ""], ["AlHaija", "Hassan Abu", ""], ["Koltun", "Vladlen", ""]]}, {"id": "2105.05600", "submitter": "Jiazhao Zhang", "authors": "Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu", "title": "ROSEFusion: Random Optimization for Online Dense Reconstruction under\n  Fast Camera Motion", "comments": "Accepted by SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online reconstruction based on RGB-D sequences has thus far been restrained\nto relatively slow camera motions (<1m/s). Under very fast camera motion (e.g.,\n3m/s), the reconstruction can easily crumble even for the state-of-the-art\nmethods. Fast motion brings two challenges to depth fusion: 1) the high\nnonlinearity of camera pose optimization due to large inter-frame rotations and\n2) the lack of reliably trackable features due to motion blur. We propose to\ntackle the difficulties of fast-motion camera tracking in the absence of\ninertial measurements using random optimization, in particular, the Particle\nFilter Optimization (PFO). To surmount the computation-intensive particle\nsampling and update in standard PFO, we propose to accelerate the randomized\nsearch via updating a particle swarm template (PST). PST is a set of particles\npre-sampled uniformly within the unit sphere in the 6D space of camera pose.\nThrough moving and rescaling the pre-sampled PST guided by swarm intelligence,\nour method is able to drive tens of thousands of particles to locate and cover\na good local optimum extremely fast and robustly. The particles, representing\ncandidate poses, are evaluated with a fitness function defined based on\ndepth-model conformance. Therefore, our method, being depth-only and\ncorrespondence-free, mitigates the motion blur impediment as ToF-based depths\nare often resilient to motion blur. Thanks to the efficient template-based\nparticle set evolution and the effective fitness function, our method attains\ngood quality pose tracking under fast camera motion (up to 4m/s) in a realtime\nframerate without including loop closure or global pose optimization. Through\nextensive evaluations on public datasets of RGB-D sequences, especially on a\nnewly proposed benchmark of fast camera motion, we demonstrate the significant\nadvantage of our method over the state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 11:37:34 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Jiazhao", ""], ["Zhu", "Chenyang", ""], ["Zheng", "Lintao", ""], ["Xu", "Kai", ""]]}, {"id": "2105.05787", "submitter": "Ionut Mironica", "authors": "Ionut Mironica and Andrei Zugravu", "title": "A Fast Deep Learning Network for Automatic Image Auto-Straightening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rectifying the orientation of images represents a daily task for every\nphotographer. This task may be complicated even for the human eye, especially\nwhen the horizon or other horizontal and vertical lines in the image are\nmissing. In this paper we address this problem and propose a new deep learning\nnetwork specially adapted for image rotation correction: we introduce the\nrectangle-shaped depthwise convolutions which are specialized in detecting long\nlines from the image and a new adapted loss function that addresses the problem\nof orientation errors.\n  Compared to other methods that are able to detect rotation errors only on few\nimage categories, like man-made structures, the proposed method can be used on\na larger variety of photographs e.g., portraits, landscapes, sport, night\nphotos etc. Moreover, the model is adapted to mobile devices and can be run in\nreal time, both for pictures and for videos. An extensive evaluation of our\nmodel on different datasets shows that it remarkably generalizes, not being\ndependent on any particular type of image. Finally, we significantly outperform\nthe state-of-the-art methods, providing superior results.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:01:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Mironica", "Ionut", ""], ["Zugravu", "Andrei", ""]]}, {"id": "2105.06194", "submitter": "Vincenzo Ciancia", "authors": "Nick Bezhanishvili and Vincenzo Ciancia and David Gabelaia and\n  Gianluca Grilletti and Diego Latella and Mieke Massink", "title": "Geometric Model Checking of Continuous Space", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Spatial Model Checking is a recent paradigm that combines Model\nChecking with the topological interpretation of Modal Logic. The Spatial Logic\nof Closure Spaces, SLCS, extends Modal Logic with reachability connectives\nthat, in turn, can be used for expressing interesting spatial properties, such\nas \"being near to\" or \"being surrounded by\". SLCS constitutes the kernel of a\nsolid logical framework for reasoning about discrete space, such as graphs and\ndigital images, interpreted as quasi discrete closure spaces. In particular,\nthe spatial model checker VoxLogicA, that uses an extended version of SLCS, has\nbeen used successfully in the domain of medical imaging. However, SLCS is not\nrestricted to discrete space. Following a recently developed geometric\nsemantics of Modal Logic, we show that it is possible to assign an\ninterpretation to SLCS in continuous space, admitting a model checking\nprocedure, by resorting to models based on polyhedra. In medical imaging such\nrepresentations of space are increasingly relevant, due to recent developments\nof 3D scanning and visualisation techniques that exploit mesh processing. We\ndemonstrate feasibility of our approach via a new tool, PolyLogicA, aimed at\nefficient verification of SLCS formulas on polyhedra, while inheriting some\nwell-established optimization techniques already adopted in VoxLogicA. Finally,\nwe cater for a geometric definition of bisimilarity, proving that it\ncharacterises logical equivalence.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 11:25:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Bezhanishvili", "Nick", ""], ["Ciancia", "Vincenzo", ""], ["Gabelaia", "David", ""], ["Grilletti", "Gianluca", ""], ["Latella", "Diego", ""], ["Massink", "Mieke", ""]]}, {"id": "2105.06407", "submitter": "Robin Kips", "authors": "Robin Kips, Ruowei Jiang, Sileye Ba, Edmund Phung, Parham Aarabi,\n  Pietro Gori, Matthieu Perrot, Isabelle Bloch", "title": "Deep Graphics Encoder for Real-Time Video Makeup Synthesis from Example", "comments": "CVPR 2021 Workshop AI for Content Creation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While makeup virtual-try-on is now widespread, parametrizing a computer\ngraphics rendering engine for synthesizing images of a given cosmetics product\nremains a challenging task. In this paper, we introduce an inverse computer\ngraphics method for automatic makeup synthesis from a reference image, by\nlearning a model that maps an example portrait image with makeup to the space\nof rendering parameters. This method can be used by artists to automatically\ncreate realistic virtual cosmetics image samples, or by consumers, to virtually\ntry-on a makeup extracted from their favorite reference image.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:28:32 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Kips", "Robin", ""], ["Jiang", "Ruowei", ""], ["Ba", "Sileye", ""], ["Phung", "Edmund", ""], ["Aarabi", "Parham", ""], ["Gori", "Pietro", ""], ["Perrot", "Matthieu", ""], ["Bloch", "Isabelle", ""]]}, {"id": "2105.06466", "submitter": "Steven Liu", "authors": "Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu,\n  Bryan Russell", "title": "Editing Conditional Radiance Fields", "comments": "Code: https://github.com/stevliu/editnerf Website:\n  http://editnerf.csail.mit.edu/, v2 updated figure 8 and included additional\n  details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A neural radiance field (NeRF) is a scene model supporting high-quality view\nsynthesis, optimized per scene. In this paper, we explore enabling user editing\nof a category-level NeRF - also known as a conditional radiance field - trained\non a shape category. Specifically, we introduce a method for propagating coarse\n2D user scribbles to the 3D space, to modify the color or shape of a local\nregion. First, we propose a conditional radiance field that incorporates new\nmodular network components, including a shape branch that is shared across\nobject instances. Observing multiple instances of the same category, our model\nlearns underlying part semantics without any supervision, thereby allowing the\npropagation of coarse 2D user scribbles to the entire 3D region (e.g., chair\nseat). Next, we propose a hybrid network update strategy that targets specific\nnetwork components, which balances efficiency and accuracy. During user\ninteraction, we formulate an optimization problem that both satisfies the\nuser's constraints and preserves the original object structure. We demonstrate\nour approach on various editing tasks over three shape datasets and show that\nit outperforms prior neural editing approaches. Finally, we edit the appearance\nand shape of a real photograph and show that the edit propagates to\nextrapolated novel views.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:59:48 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:30:47 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Steven", ""], ["Zhang", "Xiuming", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Richard", ""], ["Zhu", "Jun-Yan", ""], ["Russell", "Bryan", ""]]}, {"id": "2105.06820", "submitter": "Charalambos Poullis", "authors": "Farhan Rahman Wasee, Alen Joy, Charalambos Poullis", "title": "Predicting Surface Reflectance Properties of Outdoor Scenes Under\n  Unknown Natural Illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Estimating and modelling the appearance of an object under outdoor\nillumination conditions is a complex process. Although there have been several\nstudies on illumination estimation and relighting, very few of them focus on\nestimating the reflectance properties of outdoor objects and scenes. This paper\naddresses this problem and proposes a complete framework to predict surface\nreflectance properties of outdoor scenes under unknown natural illumination.\nUniquely, we recast the problem into its two constituent components involving\nthe BRDF incoming light and outgoing view directions: (i) surface points'\nradiance captured in the images, and outgoing view directions are aggregated\nand encoded into reflectance maps, and (ii) a neural network trained on\nreflectance maps of renders of a unit sphere under arbitrary light directions\ninfers a low-parameter reflection model representing the reflectance properties\nat each surface in the scene. Our model is based on a combination of\nphenomenological and physics-based scattering models and can relight the scenes\nfrom novel viewpoints. We present experiments that show that rendering with the\npredicted reflectance properties results in a visually similar appearance to\nusing textures that cannot otherwise be disentangled from the reflectance\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:31:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wasee", "Farhan Rahman", ""], ["Joy", "Alen", ""], ["Poullis", "Charalambos", ""]]}, {"id": "2105.06858", "submitter": "Andrea Raffo", "authors": "Chiara Romanengo, Andrea Raffo, Yifan Qie, Nabil Anwer, Bianca\n  Falcidieno", "title": "Fit4CAD: A point cloud benchmark for fitting simple geometric primitives\n  in CAD models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Fit4CAD, a benchmark for the evaluation and comparison of methods\nfor fitting simple geometric primitives in point clouds representing CAD\nmodels. This benchmark is meant to help both method developers and those who\nwant to identify the best performing tools. The Fit4CAD dataset is composed by\n225 high quality point clouds, each of which has been obtained by sampling a\nCAD model. The way these elements were created by using existing platforms and\ndatasets makes the benchmark easily expandable. The dataset is already split\ninto a training set and a test set. To assess performance and accuracy of the\ndifferent primitive fitting methods, various measures are defined. To\ndemonstrate the effective use of Fit4CAD, we have tested it on two methods\nbelonging to two different categories of approaches to the primitive fitting\nproblem: a clustering method based on a primitive growing framework and a\nparametric method based on the Hough transform.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:32:08 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 11:55:02 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Romanengo", "Chiara", ""], ["Raffo", "Andrea", ""], ["Qie", "Yifan", ""], ["Anwer", "Nabil", ""], ["Falcidieno", "Bianca", ""]]}, {"id": "2105.07112", "submitter": "Celong Liu", "authors": "Celong Liu, Zhong Li, Junsong Yuan, Yi Xu", "title": "NeLF: Practical Novel View Synthesis with Neural Light Field", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we present an efficient and robust deep learning solution for\nnovel view synthesis of complex scenes. In our approach, a 3D scene is\nrepresented as a light field, i.e., a set of rays, each of which has a\ncorresponding color when reaching the image plane. For efficient novel view\nrendering, we adopt a 4D parameterization of the light field, where each ray is\ncharacterized by a 4D parameter. We then formulate the light field as a 4D\nfunction that maps 4D coordinates to corresponding color values. We train a\ndeep fully connected network to optimize this implicit function and memorize\nthe 3D scene. Then, the scene-specific model is used to synthesize novel views.\nDifferent from previous light field approaches which require dense view\nsampling to reliably render novel views, our method can render novel views by\nsampling rays and querying the color for each ray from the network directly,\nthus enabling high-quality light field rendering with a sparser set of training\nimages. Our method achieves state-of-the-art novel view synthesis results while\nmaintaining an interactive frame rate.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:20:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 11:39:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liu", "Celong", ""], ["Li", "Zhong", ""], ["Yuan", "Junsong", ""], ["Xu", "Yi", ""]]}, {"id": "2105.07299", "submitter": "Eyvind Niklasson", "authors": "Alexander Mordvintsev, Eyvind Niklasson, Ettore Randazzo", "title": "Texture Generation with Neural Cellular Automata", "comments": "AI for Content Creation Workshop, CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural Cellular Automata (NCA) have shown a remarkable ability to learn the\nrequired rules to \"grow\" images, classify morphologies, segment images, as well\nas to do general computation such as path-finding. We believe the inductive\nprior they introduce lends itself to the generation of textures. Textures in\nthe natural world are often generated by variants of locally interacting\nreaction-diffusion systems. Human-made textures are likewise often generated in\na local manner (textile weaving, for instance) or using rules with local\ndependencies (regular grids or geometric patterns). We demonstrate learning a\ntexture generator from a single template image, with the generation method\nbeing embarrassingly parallel, exhibiting quick convergence and high fidelity\nof output, and requiring only some minimal assumptions around the underlying\nstate manifold. Furthermore, we investigate properties of the learned models\nthat are both useful and interesting, such as non-stationary dynamics and an\ninherent robustness to damage. Finally, we make qualitative claims that the\nbehaviour exhibited by the NCA model is a learned, distributed, local algorithm\nto generate a texture, setting our method apart from existing work on texture\ngeneration. We discuss the advantages of such a paradigm.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 22:05:46 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Mordvintsev", "Alexander", ""], ["Niklasson", "Eyvind", ""], ["Randazzo", "Ettore", ""]]}, {"id": "2105.07656", "submitter": "Mengdi Wang", "authors": "Mengdi Wang, Yitong Deng, Xiangxin Kong, Aditya H. Prasad, Shiying\n  Xiong, Bo Zhu", "title": "Thin-Film Smoothed Particle Hydrodynamics Fluid", "comments": "SIGGRAPH 2021 Technical Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a particle-based method to simulate thin-film fluid that jointly\nfacilitates aggressive surface deformation and vigorous tangential flows. We\nbuild our dynamics model from the surface tension driven Navier-Stokes equation\nwith the dimensionality reduced using the asymptotic lubrication theory and\ncustomize a set of differential operators based on the weakly compressible\nSmoothed Particle Hydrodynamics (SPH) for evolving pointset surfaces. The key\ninsight is that the compressible nature of SPH, which is unfavorable in its\ntypical usage, is helpful in our application to co-evolve the thickness,\ncalculate the surface tension, and enforce the fluid incompressibility on a\nthin film. In this way, we are able to two-way couple the surface deformation\nwith the in-plane flows in a physically based manner. We can simulate complex\nvortical swirls, fingering effects due to Rayleigh-Taylor instability,\ncapillary waves, Newton's interference fringes, and the Marangoni effect on\nliberally deforming surfaces by presenting both realistic visual results and\nnumerical validations. The particle-based nature of our system also enables it\nto conveniently handle topology changes and codimension transitions, allowing\nus to marry the thin-film simulation with a wide gamut of 3D phenomena, such as\npinch-off of unstable catenoids, dripping under gravity, merging of droplets,\nas well as bubble rupture.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 08:10:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wang", "Mengdi", ""], ["Deng", "Yitong", ""], ["Kong", "Xiangxin", ""], ["Prasad", "Aditya H.", ""], ["Xiong", "Shiying", ""], ["Zhu", "Bo", ""]]}, {"id": "2105.08051", "submitter": "Soumyadip Sengupta", "authors": "Soumyadip Sengupta, Brian Curless, Ira Kemelmacher-Shlizerman, Steve\n  Seitz", "title": "A Light Stage on Every Desk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Every time you sit in front of a TV or monitor, your face is actively\nilluminated by time-varying patterns of light. This paper proposes to use this\ntime-varying illumination for synthetic relighting of your face with any new\nillumination condition. In doing so, we take inspiration from the light stage\nwork of Debevec et al., who first demonstrated the ability to relight people\ncaptured in a controlled lighting environment. Whereas existing light stages\nrequire expensive, room-scale spherical capture gantries and exist in only a\nfew labs in the world, we demonstrate how to acquire useful data from a normal\nTV or desktop monitor. Instead of subjecting the user to uncomfortable rapidly\nflashing light patterns, we operate on images of the user watching a YouTube\nvideo or other standard content. We train a deep network on images plus monitor\npatterns of a given user and learn to predict images of that user under any\ntarget illumination (monitor pattern). Experimental evaluation shows that our\nmethod produces realistic relighting results. Video results are available at\nhttp://grail.cs.washington.edu/projects/Light_Stage_on_Every_Desk/.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:56:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sengupta", "Soumyadip", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""], ["Seitz", "Steve", ""]]}, {"id": "2105.08177", "submitter": "Hsien-Yu Meng", "authors": "Hsien-Yu Meng, Zhenyu Tang, Dinesh Manocha", "title": "Point-based Acoustic Scattering for Interactive Sound Propagation via\n  Surface Encoding", "comments": "IJCAI 2021 main track paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel geometric deep learning method to compute the acoustic\nscattering properties of geometric objects. Our learning algorithm uses a point\ncloud representation of objects to compute the scattering properties and\nintegrates them with ray tracing for interactive sound propagation in dynamic\nscenes. We use discrete Laplacian-based surface encoders and approximate the\nneighborhood of each point using a shared multi-layer perceptron. We show that\nour formulation is permutation invariant and present a neural network that\ncomputes the scattering function using spherical harmonics. Our approach can\nhandle objects with arbitrary topologies and deforming models, and takes less\nthan 1ms per object on a commodity GPU. We have analyzed the accuracy and\nperform validation on thousands of unseen 3D objects and highlight the benefits\nover other point-based geometric deep learning methods. To the best of our\nknowledge, this is the first real-time learning algorithm that can approximate\nthe acoustic scattering properties of arbitrary objects with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 21:49:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Meng", "Hsien-Yu", ""], ["Tang", "Zhenyu", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2105.08222", "submitter": "Chen Zhang", "authors": "Chen Zhang, Yinghao Xu, Yujun Shen", "title": "Decorating Your Own Bedroom: Locally Controlling Image Generation with\n  Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have made great success in\nsynthesizing high-quality images. However, how to steer the generation process\nof a well-trained GAN model and customize the output image is much less\nexplored. It has been recently found that modulating the input latent code used\nin GANs can reasonably alter some variation factors in the output image, but\nsuch manipulation usually presents to change the entire image as a whole. In\nthis work, we propose an effective approach, termed as LoGAN, to support local\nediting of the output image. Concretely, we introduce two operators, i.e.,\ncontent modulation and style modulation, together with a priority mask to\nfacilitate the precise control of the intermediate generative features. Taking\nbedroom synthesis as an instance, we are able to seamlessly remove, insert,\nshift, and rotate the individual objects inside a room. Furthermore, our method\ncan completely clear out a room and then refurnish it with customized furniture\nand styles. Experimental results show the great potentials of steering the\nimage generation of pre-trained GANs for versatile image editing.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 01:31:49 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Zhang", "Chen", ""], ["Xu", "Yinghao", ""], ["Shen", "Yujun", ""]]}, {"id": "2105.08471", "submitter": "Liangwang Ruan", "authors": "Liangwang Ruan, Jinyuan Liu, Bo Zhu, Shinjiro Sueda, Bin Wang, Baoquan\n  Chen", "title": "Solid-Fluid Interaction with Surface-Tension-Dominant Contact", "comments": null, "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 120. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459862", "report-no": null, "categories": "physics.flu-dyn cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel three-way coupling method to model the contact interaction\nbetween solid and fluid driven by strong surface tension. At the heart of our\nphysical model is a thin liquid membrane that simultaneously couples to both\nthe liquid volume and the rigid objects, facilitating accurate momentum\ntransfer, collision processing, and surface tension calculation. This model is\nimplemented numerically under a hybrid Eulerian-Lagrangian framework where the\nmembrane is modelled as a simplicial mesh and the liquid volume is simulated on\na background Cartesian grid. We devise a monolithic solver to solve the\ninteractions among the three systems of liquid, solid, and membrane. We\ndemonstrate the efficacy of our method through an array of rigid-fluid contact\nsimulations dominated by strong surface tension, which enables the faithful\nmodeling of a host of new surface-tension-dominant phenomena including: objects\nwith higher density than water can keep afloat on top of it; 'Cheerios effect'\nabout floating objects that do not normally float attract one another; surface\ntension weakening effect caused by surface-active constituents.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:27:41 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Ruan", "Liangwang", ""], ["Liu", "Jinyuan", ""], ["Zhu", "Bo", ""], ["Sueda", "Shinjiro", ""], ["Wang", "Bin", ""], ["Chen", "Baoquan", ""]]}, {"id": "2105.08535", "submitter": "Kai Jia", "authors": "Kai Jia", "title": "SANM: A Symbolic Asymptotic Numerical Solver with Applications in Mesh\n  Deformation", "comments": "SIGGRAPH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving nonlinear systems is an important problem. Numerical continuation\nmethods efficiently solve certain nonlinear systems. The Asymptotic Numerical\nMethod (ANM) is a powerful continuation method that usually converges faster\nthan Newtonian methods. ANM explores the landscape of the function by following\na parameterized solution curve approximated with a high-order power series.\nAlthough ANM has successfully solved a few graphics and engineering problems,\nprior to our work, applying ANM to new problems required significant effort\nbecause the standard ANM assumes quadratic functions, while manually deriving\nthe power series expansion for nonquadratic systems is a tedious and\nchallenging task.\n  This paper presents a novel solver, SANM, that applies ANM to solve\nsymbolically represented nonlinear systems. SANM solves such systems in a fully\nautomated manner. SANM also extends ANM to support many nonquadratic operators,\nincluding intricate ones such as singular value decomposition. Furthermore,\nSANM generalizes ANM to support the implicit homotopy form. Moreover, SANM\nachieves high computing performance via optimized system design and\nimplementation.\n  We deploy SANM to solve forward and inverse elastic force equilibrium\nproblems and controlled mesh deformation problems with a few constitutive\nmodels. Our results show that SANM converges faster than Newtonian solvers,\nrequires little programming effort for new problems, and delivers comparable or\nbetter performance than a hand-coded, specialized ANM solver. While we\ndemonstrate on mesh deformation problems, SANM is generic and potentially\napplicable to many tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:04:06 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jia", "Kai", ""]]}, {"id": "2105.08612", "submitter": "Yuan-Ting Hu", "authors": "Yuan-Ting Hu, Jiahong Wang, Raymond A. Yeh, Alexander G. Schwing", "title": "SAIL-VOS 3D: A Synthetic Dataset and Baselines for Object Detection and\n  3D Mesh Reconstruction from Video Data", "comments": "CVPR 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Extracting detailed 3D information of objects from video data is an important\ngoal for holistic scene understanding. While recent methods have shown\nimpressive results when reconstructing meshes of objects from a single image,\nresults often remain ambiguous as part of the object is unobserved. Moreover,\nexisting image-based datasets for mesh reconstruction don't permit to study\nmodels which integrate temporal information. To alleviate both concerns we\npresent SAIL-VOS 3D: a synthetic video dataset with frame-by-frame mesh\nannotations which extends SAIL-VOS. We also develop first baselines for\nreconstruction of 3D meshes from video data via temporal models. We demonstrate\nefficacy of the proposed baseline on SAIL-VOS 3D and Pix3D, showing that\ntemporal information improves reconstruction quality. Resources and additional\ninformation are available at http://sailvos.web.illinois.edu.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:42:37 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Hu", "Yuan-Ting", ""], ["Wang", "Jiahong", ""], ["Yeh", "Raymond A.", ""], ["Schwing", "Alexander G.", ""]]}, {"id": "2105.08935", "submitter": "Lin Gao", "authors": "Shu-Yu Chen, Feng-Lin Liu, Yu-Kun Lai, Paul L. Rosin, Chunpeng Li,\n  Hongbo Fu, Lin Gao", "title": "DeepFaceEditing: Deep Face Generation and Editing with Disentangled\n  Geometry and Appearance Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent facial image synthesis methods have been mainly based on conditional\ngenerative models. Sketch-based conditions can effectively describe the\ngeometry of faces, including the contours of facial components, hair\nstructures, as well as salient edges (e.g., wrinkles) on face surfaces but lack\neffective control of appearance, which is influenced by color, material,\nlighting condition, etc. To have more control of generated results, one\npossible approach is to apply existing disentangling works to disentangle face\nimages into geometry and appearance representations. However, existing\ndisentangling methods are not optimized for human face editing, and cannot\nachieve fine control of facial details such as wrinkles. To address this issue,\nwe propose DeepFaceEditing, a structured disentanglement framework specifically\ndesigned for face images to support face generation and editing with\ndisentangled control of geometry and appearance. We adopt a local-to-global\napproach to incorporate the face domain knowledge: local component images are\ndecomposed into geometry and appearance representations, which are fused\nconsistently using a global fusion module to improve generation quality. We\nexploit sketches to assist in extracting a better geometry representation,\nwhich also supports intuitive geometry editing via sketching. The resulting\nmethod can either extract the geometry and appearance representations from face\nimages, or directly extract the geometry representation from face sketches.\nSuch representations allow users to easily edit and synthesize face images,\nwith decoupled control of their geometry and appearance. Both qualitative and\nquantitative evaluations show the superior detail and appearance control\nabilities of our method compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 05:35:44 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 01:44:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Shu-Yu", ""], ["Liu", "Feng-Lin", ""], ["Lai", "Yu-Kun", ""], ["Rosin", "Paul L.", ""], ["Li", "Chunpeng", ""], ["Fu", "Hongbo", ""], ["Gao", "Lin", ""]]}, {"id": "2105.09034", "submitter": "Keiichiro Shirai", "authors": "Keiichiro Shirai, Tatsuya Baba, Shunsuke Ono, Masahiro Okuda, Yusuke\n  Tatesumi, and Paul Perrotin", "title": "Guided Facial Skin Color Correction", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic image correction method for portrait\nphotographs, which promotes consistency of facial skin color by suppressing\nskin color changes due to background colors. In portrait photographs, skin\ncolor is often distorted due to the lighting environment (e.g., light reflected\nfrom a colored background wall and over-exposure by a camera strobe), and if\nthe photo is artificially combined with another background color, this color\nchange is emphasized, resulting in an unnatural synthesized result. In our\nframework, after roughly extracting the face region and rectifying the skin\ncolor distribution in a color space, we perform color and brightness correction\naround the face in the original image to achieve a proper color balance of the\nfacial image, which is not affected by luminance and background colors. Unlike\nconventional algorithms for color correction, our final result is attained by a\ncolor correction process with a guide image. In particular, our guided image\nfiltering for the color correction does not require a perfectly-aligned guide\nimage required in the original guide image filtering method proposed by He et\nal. Experimental results show that our method generates more natural results\nthan conventional methods on not only headshot photographs but also natural\nscene photographs. We also show automatic yearbook style photo generation as an\nanother application.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:59:55 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Shirai", "Keiichiro", ""], ["Baba", "Tatsuya", ""], ["Ono", "Shunsuke", ""], ["Okuda", "Masahiro", ""], ["Tatesumi", "Yusuke", ""], ["Perrotin", "Paul", ""]]}, {"id": "2105.09476", "submitter": "Emiliya Tyuleneva", "authors": "Nikita Glushkov and Tyuleneva Emiliya", "title": "Projection matrices and related viewing frustums: new ways to create and\n  apply", "comments": "16 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In computer graphics, the field of view of a camera is represented by a\nviewing frustum and a corresponding projection matrix, the properties of which,\nin the absence of restrictions on rectangular shape of the near plane and its\nparallelism to the far plane are currently not fully explored and structured.\nThis study aims to consider the properties of arbitrary affine frustums, as\nwell as various techniques for their transformation for practical use in\ndevices with limited resources. Additionally, this article explores the methods\nof working with the visible volume as an arbitrary frustum that is not\nassociated with the projection matrix. To study the properties of affine\nfrustums, the dependencies between its planes and formulas for obtaining key\npoints from the inverse projection matrix were derived. Methods of constructing\nfrustum by key points and given planes were also considered. Moreover, frustum\ntransformation formulas were obtained to simulate the effects of reflection,\nrefraction and cropping in devices with limited resources. In conclusion, a\nmethod was proposed for applying an arbitrary frustum, which does not have a\ncorresponding projection matrix, to limit the visible volume and then transform\nthe points into NDC space.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:53:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Glushkov", "Nikita", ""], ["Emiliya", "Tyuleneva", ""]]}, {"id": "2105.09492", "submitter": "Rundi Wu", "authors": "Rundi Wu, Chang Xiao, Changxi Zheng", "title": "DeepCAD: A Deep Generative Network for Computer-Aided Design Models", "comments": "14 pages, 14 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models of 3D shapes have received a great deal of research\ninterest. Yet, almost all of them generate discrete shape representations, such\nas voxels, point clouds, and polygon meshes. We present the first 3D generative\nmodel for a drastically different shape representation -- describing a shape as\na sequence of computer-aided design (CAD) operations. Unlike meshes and point\nclouds, CAD models encode the user creation process of 3D shapes, widely used\nin numerous industrial and engineering design tasks. However, the sequential\nand irregular structure of CAD operations poses significant challenges for\nexisting 3D generative models. Drawing an analogy between CAD operations and\nnatural language, we propose a CAD generative network based on the Transformer.\nWe demonstrate the performance of our model for both shape autoencoding and\nrandom shape generation. To train our network, we create a new CAD dataset\nconsisting of 179,133 models and their CAD construction sequences. We have made\nthis dataset publicly available to promote future research on this topic.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:29:18 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Rundi", ""], ["Xiao", "Chang", ""], ["Zheng", "Changxi", ""]]}, {"id": "2105.09762", "submitter": "Ferenc Nagy", "authors": "Ferenc Nagy, Norimasa Yoshida, Mikl\\'os Hoffmann", "title": "Interactive $G^1$ and $G^2$ Hermite Interpolation Using Extended\n  Log-aesthetic Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the field of aesthetic design, log-aesthetic curves have a significant\nrole to meet the high industrial requirements. In this paper, we propose a new\ninteractive $G^1$ Hermite interpolation method based on the algorithm of\nYoshida et al. with a minor boundary condition. In this novel approach, we\ncompute an extended log-aesthetic curve segment that may include inflection\npoint (S-shaped curve) or cusp. The curve segment is defined by its endpoints,\na tangent vector at the first point, and a tangent direction at the second\npoint. The algorithm also determines the shape parameter of the log-aesthetic\ncurve based on the length of the first tangent that provides control over the\ncurvature of the first point and makes the method capable of joining\nlog-aesthetic curve segments with $G^2$ continuity.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:18:14 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nagy", "Ferenc", ""], ["Yoshida", "Norimasa", ""], ["Hoffmann", "Mikl\u00f3s", ""]]}, {"id": "2105.09852", "submitter": "Naftali Waxman", "authors": "Naftali Waxman, Noam Hazon, Sarit Kraus", "title": "Manipulation of k-Coalitional Games on Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many coalition formation games the utility of the agents depends on a\nsocial network. In such scenarios there might be a manipulative agent that\nwould like to manipulate his connections in the social network in order to\nincrease his utility. We study a model of coalition formation in which a\ncentral organizer, who needs to form $k$ coalitions, obtains information about\nthe social network from the agents. The central organizer has her own\nobjective: she might want to maximize the utilitarian social welfare, maximize\nthe egalitarian social welfare, or simply guarantee that every agent will have\nat least one connection within her coalition. In this paper we study the\nsusceptibility to manipulation of these objectives, given the abilities and\ninformation that the manipulator has. Specifically, we show that if the\nmanipulator has very limited information, namely he is only familiar with his\nimmediate neighbours in the network, then a manipulation is almost always\nimpossible. Moreover, if the manipulator is only able to add connections to the\nsocial network, then a manipulation is still impossible for some objectives,\neven if the manipulator has full information on the structure of the network.\nOn the other hand, if the manipulator is able to hide some of his connections,\nthen all objectives are susceptible to manipulation, even if the manipulator\nhas limited information, i.e., when he is familiar with his immediate\nneighbours and with their neighbours.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:59:32 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Waxman", "Naftali", ""], ["Hazon", "Noam", ""], ["Kraus", "Sarit", ""]]}, {"id": "2105.09878", "submitter": "Nosakhare Edoimioya", "authors": "Nosakhare Edoimioya, Keval S. Ramani, Chinedum E. Okwudire", "title": "Software Compensation of Undesirable Racking Motion of H-frame 3D\n  Printers using Filtered B-Splines", "comments": "12 pages, 11 figures, pending journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The H-frame (also known as H-Bot) architecture is a simple and elegant\ntwo-axis parallel positioning system used to construct the XY stage of 3D\nprinters. It holds potential for high speed and excellent dynamic performance\ndue to the use of frame-mounted motors that reduce the moving mass of the\nprinter while allowing for the use of (heavy) higher torque motors. However,\nthe H-frame's dynamic accuracy is limited during high-acceleration and\nhigh-speed motion due to racking -- i.e., parasitic torsional motions of the\nprinter's gantry due to a force couple. Mechanical solutions to the racking\nproblem are either costly or detract from the simplicity of the H-frame. In\nthis paper, we introduce a feedforward software compensation algorithm, based\non the filtered B-splines (FBS) method, that rectifies errors due to racking.\nThe FBS approach expresses the motion command to the machine as a linear\ncombination of B-splines. The B-splines are filtered through an identified\nmodel of the machine dynamics and the control points of the B-spline based\nmotion command are optimized such that the tracking error is minimized. To\ncompensate racking using the FBS algorithm, an accurate frequency response\nfunction of the racking motion is obtained and coupled to the H-frame's x- and\ny-axis dynamics with a kinematic model. The result is a coupled linear\nparameter varying model of the H-frame that is utilized in the FBS framework to\ncompensate racking. An approximation of the proposed racking compensation\nalgorithm, that decouples the x- and y-axis compensation, is developed to\nsignificantly improve its computational efficiency with almost no loss of\ncompensation accuracy. Experiments on an H-frame 3D printer demonstrate a 43\npercent improvement in the shape accuracy of a printed part using the proposed\nalgorithm compared to the standard FBS approach without racking compensation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:25:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Edoimioya", "Nosakhare", ""], ["Ramani", "Keval S.", ""], ["Okwudire", "Chinedum E.", ""]]}, {"id": "2105.10066", "submitter": "Pei Xu", "authors": "Pei Xu and Ioannis Karamouzas", "title": "A GAN-Like Approach for Physics-Based Imitation Learning and Interactive\n  Character Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple and intuitive approach for interactive control of\nphysically simulated characters. Our work builds upon generative adversarial\nnetworks (GAN) and reinforcement learning, and introduces an imitation learning\nframework where an ensemble of classifiers and an imitation policy are trained\nin tandem given pre-processed reference clips. The classifiers are trained to\ndiscriminate the reference motion from the motion generated by the imitation\npolicy, while the policy is rewarded for fooling the discriminators. Using our\nGAN-based approach, multiple motor control policies can be trained separately\nto imitate different behaviors. In runtime, our system can respond to external\ncontrol signal provided by the user and interactively switch between different\npolicies. Compared to existing methods, our proposed approach has the following\nattractive properties: 1) achieves state-of-the-art imitation performance\nwithout manually designing and fine tuning a reward function; 2) directly\ncontrols the character without having to track any target reference pose\nexplicitly or implicitly through a phase state; and 3) supports interactive\npolicy switching without requiring any motion generation or motion matching\nmechanism. We highlight the applicability of our approach in a range of\nimitation and interactive control tasks, while also demonstrating its ability\nto withstand external perturbations as well as to recover balance. Overall, our\napproach generates high-fidelity motion, has low runtime cost, and can be\neasily integrated into interactive applications and games.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 00:03:29 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Pei", ""], ["Karamouzas", "Ioannis", ""]]}, {"id": "2105.10441", "submitter": "Timur Bagautdinov", "authors": "Timur Bagautdinov, Chenglei Wu, Tomas Simon, Fabian Prada, Takaaki\n  Shiratori, Shih-En Wei, Weipeng Xu, Yaser Sheikh, Jason Saragih", "title": "Driving-Signal Aware Full-Body Avatars", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459850", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 16:22:38 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 18:30:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bagautdinov", "Timur", ""], ["Wu", "Chenglei", ""], ["Simon", "Tomas", ""], ["Prada", "Fabian", ""], ["Shiratori", "Takaaki", ""], ["Wei", "Shih-En", ""], ["Xu", "Weipeng", ""], ["Sheikh", "Yaser", ""], ["Saragih", "Jason", ""]]}, {"id": "2105.10498", "submitter": "Josef Spjut", "authors": "Josef Spjut and Ben Boudaoud and Joohwan Kim", "title": "A Case Study of First Person Aiming at Low Latency for Esports", "comments": "4 pages, 7 figures, presented at EHPHCI 2021", "journal-ref": null, "doi": "10.31219/osf.io/nu9p3", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lower computer system input-to-output latency substantially reduces many task\ncompletion times. In fact, literature shows that reduction in targeting task\ncompletion time from decreased latency often exceeds the decrease in latency\nalone. However, for aiming in first person shooter (FPS) games, some prior work\nhas demonstrated diminishing returns below 40 ms of local input-to-output\ncomputer system latency. In this paper, we review this prior art and provide an\nadditional case study with data demonstrating the importance of local system\nlatency improvement, even at latency values below 20 ms. Though other factors\nmay determine victory in a particular esports challenge, ensuring balanced\nlocal computer latency among competitors is essential to fair competition.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:27:01 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Kim", "Joohwan", ""]]}, {"id": "2105.11599", "submitter": "Ziang Cheng", "authors": "Ziang Cheng, Hongdong Li, Yuta Asano, Yinqiang Zheng, Imari Sato", "title": "Multi-view 3D Reconstruction of a Texture-less Smooth Surface of Unknown\n  Generic Reflectance", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recovering the 3D geometry of a purely texture-less object with generally\nunknown surface reflectance (e.g. non-Lambertian) is regarded as a challenging\ntask in multi-view reconstruction. The major obstacle revolves around\nestablishing cross-view correspondences where photometric constancy is\nviolated. This paper proposes a simple and practical solution to overcome this\nchallenge based on a co-located camera-light scanner device. Unlike existing\nsolutions, we do not explicitly solve for correspondence. Instead, we argue the\nproblem is generally well-posed by multi-view geometrical and photometric\nconstraints, and can be solved from a small number of input views. We formulate\nthe reconstruction task as a joint energy minimization over the surface\ngeometry and reflectance. Despite this energy is highly non-convex, we develop\nan optimization algorithm that robustly recovers globally optimal shape and\nreflectance even from a random initialization. Extensive experiments on both\nsimulated and real data have validated our method, and possible future\nextensions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 01:28:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Cheng", "Ziang", ""], ["Li", "Hongdong", ""], ["Asano", "Yuta", ""], ["Zheng", "Yinqiang", ""], ["Sato", "Imari", ""]]}, {"id": "2105.12238", "submitter": "Benjamin Jones", "authors": "Benjamin Jones, Dalton Hildreth, Duowen Chen, Ilya Baran, Vova Kim,\n  Adriana Schulz", "title": "SB-GCN: Structured BREP Graph Convolutional Network for Automatic Mating\n  of CAD Assemblies", "comments": "16 pages, 17 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assembly modeling is a core task of computer aided design (CAD), comprising\naround one third of the work in a CAD workflow. Optimizing this process\ntherefore represents a huge opportunity in the design of a CAD system, but\ncurrent research of assembly based modeling is not directly applicable to\nmodern CAD systems because it eschews the dominant data structure of modern\nCAD: parametric boundary representations (BREPs). CAD assembly modeling defines\nassemblies as a system of pairwise constraints, called mates, between parts,\nwhich are defined relative to BREP topology rather than in world coordinates\ncommon to existing work. We propose SB-GCN, a representation learning scheme on\nBREPs that retains the topological structure of parts, and use these learned\nrepresentations to predict CAD type mates. To train our system, we compiled the\nfirst large scale dataset of BREP CAD assemblies, which we are releasing along\nwith benchmark mate prediction tasks. Finally, we demonstrate the compatibility\nof our model with an existing commercial CAD system by building a tool that\nassists users in mate creation by suggesting mate completions, with 72.2%\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:07:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Jones", "Benjamin", ""], ["Hildreth", "Dalton", ""], ["Chen", "Duowen", ""], ["Baran", "Ilya", ""], ["Kim", "Vova", ""], ["Schulz", "Adriana", ""]]}, {"id": "2105.12256", "submitter": "Mathew Schwartz", "authors": "Mathew Schwartz, Tomer Weiss, Esra Ataer-Cansizoglu, Jae-Woo Choi", "title": "Style Similarity as Feedback for Product Design", "comments": "15 pages, 9 figures, interdisciplinary book chapter on using computer\n  vision and style similarity for industrial design", "journal-ref": "In: Lee JH. (eds) A New Perspective of Cultural DNA. KAIST\n  Research Series. Springer, Singapore (2021)", "doi": "10.1007/978-981-15-7707-9_3", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching and recommending products is beneficial for both customers and\ncompanies. With the rapid increase in home goods e-commerce, there is an\nincreasing demand for quantitative methods for providing such recommendations\nfor millions of products. This approach is facilitated largely by online stores\nsuch as Amazon and Wayfair, in which the goal is to maximize overall sales.\nInstead of focusing on overall sales, we take a product design perspective, by\nemploying big-data analysis for determining the design qualities of a highly\nrecommended product. Specifically, we focus on the visual style compatibility\nof such products. We build off previous work which implemented a style-based\nsimilarity metric for thousands of furniture products. Using analysis and\nvisualization, we extract attributes of furniture products that are highly\ncompatible style-wise. We propose a designer in-the-loop workflow that mirrors\nmethods of displaying similar products to consumers browsing e-commerce\nwebsites. Our findings are useful when designing new products, since they\nprovide insight regarding what furniture will be strongly compatible across\nmultiple styles, and hence, more likely to be recommended.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:30:29 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Schwartz", "Mathew", ""], ["Weiss", "Tomer", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Choi", "Jae-Woo", ""]]}, {"id": "2105.12319", "submitter": "Saeed Hadadan", "authors": "Saeed Hadadan, Shuhong Chen, Matthias Zwicker", "title": "Neural Radiosity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Radiosity, an algorithm to solve the rendering equation\nby minimizing the norm of its residual similar as in traditional radiosity\ntechniques. Traditional basis functions used in radiosity techniques, such as\npiecewise polynomials or meshless basis functions are typically limited to\nrepresenting isotropic scattering from diffuse surfaces. Instead, we propose to\nleverage neural networks to represent the full four-dimensional radiance\ndistribution, directly optimizing network parameters to minimize the norm of\nthe residual. Our approach decouples solving the rendering equation from\nrendering (perspective) images similar as in traditional radiosity techniques,\nand allows us to efficiently synthesize arbitrary views of a scene. In\naddition, we propose a network architecture using geometric learnable features\nthat improves convergence of our solver compared to previous techniques. Our\napproach leads to an algorithm that is simple to implement, and we demonstrate\nits effectiveness on a variety of scenes with non-diffuse surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 04:10:00 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hadadan", "Saeed", ""], ["Chen", "Shuhong", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2105.12620", "submitter": "Laurent Belcour", "authors": "Laurent Belcour and Eric Heitz", "title": "Lessons Learned and Improvements when Building Screen-Space Samplers\n  with Blue-Noise Error Distribution", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3450623.3464645", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that the error of Monte-Carlo rendering is visually\nmore acceptable when distributed as blue-noise in screen-space. Despite recent\nefforts, building a screen-space sampler is still an open problem. In this\ntalk, we present the lessons we learned while improving our previous\nscreen-space sampler. Specifically: we advocate for a new criterion to assess\nthe quality of such samplers; we introduce a new screen-space sampler based on\nrank-1 lattices; we provide a parallel optimization method that is compatible\nwith a GPU implementation and that achieves better quality; we detail the\npitfalls of using such samplers in renderers and how to cope with many\ndimensions; and we provide empirical proofs of the versatility of the\noptimization process.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:19:07 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:28:48 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Belcour", "Laurent", ""], ["Heitz", "Eric", ""]]}, {"id": "2105.13012", "submitter": "Laurent Belcour", "authors": "Thomas Chambon, Eric Heitz, and Laurent Belcour", "title": "Passing Multi-Channel Material Textures to a 3-Channel Loss", "comments": "2 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3450623.3464685", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our objective is to compute a textural loss that can be used to train texture\ngenerators with multiple material channels typically used for physically based\nrendering such as albedo, normal, roughness, metalness, ambient occlusion, etc.\nNeural textural losses often build on top of the feature spaces of pretrained\nconvolutional neural networks. Unfortunately, these pretrained models are only\navailable for 3-channel RGB data and hence limit neural textural losses to this\nformat. To overcome this limitation, we show that passing random triplets to a\n3-channel loss provides a multi-channel loss that can be used to generate\nhigh-quality material textures.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:58:51 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Chambon", "Thomas", ""], ["Heitz", "Eric", ""], ["Belcour", "Laurent", ""]]}, {"id": "2105.13168", "submitter": "Rhaleb Zayer", "authors": "Alexander Weinrauch, Hans-Peter Seidel, Daniel Mlakar, Markus\n  Steinberger, Rhaleb Zayer", "title": "A Variational Loop Shrinking Analogy for Handle and Tunnel Detection and\n  Reeb Graph Construction on Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The humble loop shrinking property played a central role in the inception of\nmodern topology but it has been eclipsed by more abstract algebraic formalism.\nThis is particularly true in the context of detecting relevant non-contractible\nloops on surfaces where elaborate homological and/or graph theoretical\nconstructs are favored in algorithmic solutions. In this work, we devise a\nvariational analogy to the loop shrinking property and show that it yields a\nsimple, intuitive, yet powerful solution allowing a streamlined treatment of\nthe problem of handle and tunnel loop detection. Our formalization tracks the\nevolution of a diffusion front randomly initiated on a single location on the\nsurface. Capitalizing on a diffuse interface representation combined with a set\nof rules for concurrent front interactions, we develop a dynamic data structure\nfor tracking the evolution on the surface encoded as a sparse matrix which\nserves for performing both diffusion numerics and loop detection and acts as\nthe workhorse of our fully parallel implementation. The substantiated results\nsuggest our approach outperforms state of the art and robustly copes with\nhighly detailed geometric models. As a byproduct, our approach can be used to\nconstruct Reeb graphs by diffusion thus avoiding commonly encountered issues\nwhen using Morse functions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:22:34 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Weinrauch", "Alexander", ""], ["Seidel", "Hans-Peter", ""], ["Mlakar", "Daniel", ""], ["Steinberger", "Markus", ""], ["Zayer", "Rhaleb", ""]]}, {"id": "2105.13240", "submitter": "Haoyu Li", "authors": "Haoyu Li and Han-Wei Shen", "title": "Time Varying Particle Data Feature Extraction and Tracking with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analyzing particle data plays an important role in many scientific\napplications such as fluid simulation, cosmology simulation and molecular\ndynamics. While there exist methods that can perform feature extraction and\ntracking for volumetric data, performing those tasks for particle data is more\nchallenging because of the lack of explicit connectivity information. Although\none may convert the particle data to volume first, this approach is at risk of\nincurring error and increasing the size of the data. In this paper, we take a\ndeep learning approach to create feature representations for scientific\nparticle data to assist feature extraction and tracking. We employ a deep\nlearning model, which produces latent vectors to represent the relation between\nspatial locations and physical attributes in a local neighborhood. With the\nlatent vectors, features can be extracted by clustering these vectors. To\nachieve fast feature tracking, the mean-shift tracking algorithm is applied in\nthe feature space, which only requires inference of the latent vector for\nselected regions of interest. We validate our approach using two datasets and\ncompare our method with other existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:38:14 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Haoyu", ""], ["Shen", "Han-Wei", ""]]}, {"id": "2105.13277", "submitter": "Yotam Erel", "authors": "Amir Barda, Yotam Erel, Amit H. Bermano", "title": "MeshCNN Fundamentals: Geometric Learning through a Reconstructable\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mesh-based learning is one of the popular approaches nowadays to learn\nshapes. The most established backbone in this field is MeshCNN. In this paper,\nwe propose infusing MeshCNN with geometric reasoning to achieve higher quality\nlearning. Through careful analysis of the way geometry is represented\nthrough-out the network, we submit that this representation should be rigid\nmotion invariant, and should allow reconstructing the original geometry.\nAccordingly, we introduce the first and second fundamental forms as an\nedge-centric, rotation and translation invariant, reconstructable\nrepresentation. In addition, we update the originally proposed pooling scheme\nto be more geometrically driven. We validate our analysis through\nexperimentation, and present consistent improvement upon the MeshCNN baseline,\nas well as other more elaborate state-of-the-art architectures. Furthermore, we\ndemonstrate this fundamental forms-based representation opens the door to\naccessible generative machine learning over meshes.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:22:44 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Barda", "Amir", ""], ["Erel", "Yotam", ""], ["Bermano", "Amit H.", ""]]}, {"id": "2105.14548", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Raja Giryes, Niloy J. Mitra, Daniel Cohen-Or", "title": "Z2P: Instant Rendering of Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for rendering point clouds using a neural network.\nExisting point rendering techniques either use splatting, or first reconstruct\na surface mesh that can then be rendered. Both of these techniques require\nsolving for global point normal orientation, which is a challenging problem on\nits own. Furthermore, splatting techniques result in holes and overlaps,\nwhereas mesh reconstruction is particularly challenging, especially in the\ncases of thin surfaces and sheets.\n  We cast the rendering problem as a conditional image-to-image translation\nproblem. In our formulation, Z2P, i.e., depth-augmented point features as\nviewed from target camera view, are directly translated by a neural network to\nrendered images, conditioned on control variables (e.g., color, light). We\navoid inevitable issues with splatting (i.e., holes and overlaps), and bypass\nsolving the notoriously challenging surface reconstruction problem or\nestimating oriented normals. Yet, our approach results in a rendered image as\nif a surface mesh was reconstructed. We demonstrate that our framework produces\na plausible image, and can effectively handle noise, non-uniform sampling, thin\nsurfaces / sheets, and is fast.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:58:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Mitra", "Niloy J.", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2105.14739", "submitter": "Jichao Zhang", "authors": "Jichao Zhang, Aliaksandr Siarohin, Hao Tang, Jingjing Chen, Enver\n  Sangineto, Wei Wang, Nicu Sebe", "title": "Controllable Person Image Synthesis with Spatially-Adaptive Warped\n  Normalization", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controllable person image generation aims to produce realistic human images\nwith desirable attributes (e.g., the given pose, cloth textures or hair style).\nHowever, the large spatial misalignment between the source and target images\nmakes the standard architectures for image-to-image translation not suitable\nfor this task. Most of the state-of-the-art architectures avoid the alignment\nstep during the generation, which causes many artifacts, especially for person\nimages with complex textures. To solve this problem, we introduce a novel\nSpatially-Adaptive Warped Normalization (SAWN), which integrates a learned\nflow-field to warp modulation parameters. This allows us to align person\nspatial-adaptive styles with pose features efficiently. Moreover, we propose a\nnovel self-training part replacement strategy to refine the pretrained model\nfor the texture-transfer task, significantly improving the quality of the\ngenerated cloth and the preservation ability of irrelevant regions. Our\nexperimental results on the widely used DeepFashion dataset demonstrate a\nsignificant improvement of the proposed method over the state-of-the-art\nmethods on both pose-transfer and texture-transfer tasks. The source code is\navailable at https://github.com/zhangqianhui/Sawn.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:07:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 01:24:00 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Jichao", ""], ["Siarohin", "Aliaksandr", ""], ["Tang", "Hao", ""], ["Chen", "Jingjing", ""], ["Sangineto", "Enver", ""], ["Wang", "Wei", ""], ["Sebe", "Nicu", ""]]}]