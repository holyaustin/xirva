[{"id": "1905.00637", "submitter": "Chang-Hwan Son", "authors": "Chang-Hwan Son", "title": "Inverse Halftoning Through Structure-Aware Deep Convolutional Neural\n  Networks", "comments": null, "journal-ref": "Signal Processing, vol. 173, Aug. 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary issue in inverse halftoning is removing noisy dots on flat areas\nand restoring image structures (e.g., lines, patterns) on textured areas.\nHence, a new structure-aware deep convolutional neural network that\nincorporates two subnetworks is proposed in this paper. One subnetwork is for\nimage structure prediction while the other is for continuous-tone image\nreconstruction. First, to predict image structures, patch pairs comprising\ncontinuous-tone patches and the corresponding halftoned patches generated\nthrough digital halftoning are trained. Subsequently, gradient patches are\ngenerated by convolving gradient filters with the continuous-tone patches. The\nsubnetwork for the image structure prediction is trained using the mini-batch\ngradient descent algorithm given the halftoned patches and gradient patches,\nwhich are fed into the input and loss layers of the subnetwork, respectively.\nNext, the predicted map including the image structures is stacked on the top of\nthe input halftoned image through a fusion layer and fed into the image\nreconstruction subnetwork such that the entire network is trained adaptively to\nthe image structures. The experimental results confirm that the proposed\nstructure-aware network can remove noisy dot-patterns well on flat areas and\nrestore details clearly on textured areas. Furthermore, it is demonstrated that\nthe proposed method surpasses the conventional state-of-the-art methods based\non deep convolutional neural networks and locally learned dictionaries.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:39:54 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 06:49:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Son", "Chang-Hwan", ""]]}, {"id": "1905.00824", "submitter": "Tiancheng Sun", "authors": "Tiancheng Sun, Jonathan T. Barron, Yun-Ta Tsai, Zexiang Xu, Xueming\n  Yu, Graham Fyffe, Christoph Rhemann, Jay Busch, Paul Debevec, and Ravi\n  Ramamoorthi", "title": "Single Image Portrait Relighting", "comments": "SIGGRAPH 2019 Technical Paper accepted", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)", "doi": "10.1145/3306346.3323008", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lighting plays a central role in conveying the essence and depth of the\nsubject in a portrait photograph. Professional photographers will carefully\ncontrol the lighting in their studio to manipulate the appearance of their\nsubject, while consumer photographers are usually constrained to the\nillumination of their environment. Though prior works have explored techniques\nfor relighting an image, their utility is usually limited due to requirements\nof specialized hardware, multiple images of the subject under controlled or\nknown illuminations, or accurate models of geometry and reflectance. To this\nend, we present a system for portrait relighting: a neural network that takes\nas input a single RGB image of a portrait taken with a standard cellphone\ncamera in an unconstrained environment, and from that image produces a relit\nimage of that subject as though it were illuminated according to any provided\nenvironment map. Our method is trained on a small database of 18 individuals\ncaptured under different directional light sources in a controlled light stage\nsetup consisting of a densely sampled sphere of lights. Our proposed technique\nproduces quantitatively superior results on our dataset's validation set\ncompared to prior works, and produces convincing qualitative relighting results\non a dataset of hundreds of real-world cellphone portraits. Because our\ntechnique can produce a 640 $\\times$ 640 image in only 160 milliseconds, it may\nenable interactive user-facing photographic applications in the future.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 15:56:15 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Sun", "Tiancheng", ""], ["Barron", "Jonathan T.", ""], ["Tsai", "Yun-Ta", ""], ["Xu", "Zexiang", ""], ["Yu", "Xueming", ""], ["Fyffe", "Graham", ""], ["Rhemann", "Christoph", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1905.00889", "submitter": "Ben Mildenhall", "authors": "Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima\n  Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, Abhishek Kar", "title": "Local Light Field Fusion: Practical View Synthesis with Prescriptive\n  Sampling Guidelines", "comments": "SIGGRAPH 2019. Project page with video and code:\n  http://people.eecs.berkeley.edu/~bmild/llff/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a practical and robust deep learning solution for capturing and\nrendering novel views of complex real world scenes for virtual exploration.\nPrevious approaches either require intractably dense view sampling or provide\nlittle to no guidance for how users should sample views of a scene to reliably\nrender high-quality novel views. Instead, we propose an algorithm for view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image (MPI) scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. We extend traditional plenoptic sampling theory to derive a bound that\nspecifies precisely how densely users should sample views of a given scene when\nusing our algorithm. In practice, we apply this bound to capture and render\nviews of real world scenes that achieve the perceptual quality of Nyquist rate\nview sampling while using up to 4000x fewer views. We demonstrate our\napproach's practicality with an augmented reality smartphone app that guides\nusers to capture input images of a scene and viewers that enable realtime\nvirtual exploration on desktop and mobile platforms.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:58:52 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Ortiz-Cayon", "Rodrigo", ""], ["Kalantari", "Nima Khademi", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""], ["Kar", "Abhishek", ""]]}, {"id": "1905.00992", "submitter": "Joshua Levine", "authors": "Justin Crum, Joshua A. Levine and Andrew Gillette", "title": "Extending discrete exterior calculus to a fractional derivative", "comments": "18 pages, 11 figures. Work is to be presented at Solid and Physical\n  Modeling 2019", "journal-ref": "Computer-Aided Design 114 (2019) 64-72", "doi": "10.1016/j.cad.2019.05.018", "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional partial differential equations (FDEs) are used to describe\nphenomena that involve a \"non-local\" or \"long-range\" interaction of some kind.\nAccurate and practical numerical approximation of their solutions is\nchallenging due to the dense matrices arising from standard discretization\nprocedures. In this paper, we begin to extend the well-established\ncomputational toolkit of Discrete Exterior Calculus (DEC) to the fractional\nsetting, focusing on proper discretization of the fractional derivative. We\ndefine a Caputo-like fractional discrete derivative, in terms of the standard\ndiscrete exterior derivative operator from DEC, weighted by a measure of\ndistance between $p$-simplices in a simplicial complex. We discuss key\ntheoretical properties of the fractional discrete derivative and compare it to\nthe continuous fractional derivative via a series of numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 23:57:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:48:07 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Crum", "Justin", ""], ["Levine", "Joshua A.", ""], ["Gillette", "Andrew", ""]]}, {"id": "1905.01562", "submitter": "Manuel Lagunas", "authors": "Manuel Lagunas, Sandra Malpica, Ana Serrano, Elena Garces, Diego\n  Gutierrez, Belen Masia", "title": "A Similarity Measure for Material Appearance", "comments": "12 pages, 17 figures", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019)", "doi": "10.1145/3306346.3323036", "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model to measure the similarity in appearance between different\nmaterials, which correlates with human similarity judgments. We first create a\ndatabase of 9,000 rendered images depicting objects with varying materials,\nshape and illumination. We then gather data on perceived similarity from\ncrowdsourced experiments; our analysis of over 114,840 answers suggests that\nindeed a shared perception of appearance similarity exists. We feed this data\nto a deep learning architecture with a novel loss function, which learns a\nfeature space for materials that correlates with such perceived appearance\nsimilarity. Our evaluation shows that our model outperforms existing metrics.\nLast, we demonstrate several applications enabled by our metric, including\nappearance-based search for material suggestions, database visualization,\nclustering and summarization, and gamut mapping.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 22:48:27 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Lagunas", "Manuel", ""], ["Malpica", "Sandra", ""], ["Serrano", "Ana", ""], ["Garces", "Elena", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "1905.01684", "submitter": "Lequan Yu", "authors": "Xianzhi Li, Lequan Yu, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng", "title": "Unsupervised Detection of Distinctive Regions on 3D Shapes", "comments": "Accepted by ACM TOG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to learn and detect distinctive regions\non 3D shapes. Unlike previous works, which require labeled data, our method is\nunsupervised. We conduct the analysis on point sets sampled from 3D shapes,\nthen formulate and train a deep neural network for an unsupervised shape\nclustering task to learn local and global features for distinguishing shapes\nwith respect to a given shape set. To drive the network to learn in an\nunsupervised manner, we design a clustering-based nonparametric softmax\nclassifier with an iterative re-clustering of shapes, and an adapted\ncontrastive loss for enhancing the feature embedding quality and stabilizing\nthe learning process. By then, we encourage the network to learn the point\ndistinctiveness on the input shapes. We extensively evaluate various aspects of\nour approach and present its applications for distinctiveness-guided shape\nretrieval, sampling, and view selection in 3D scenes.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 13:41:05 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 22:59:05 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Xianzhi", ""], ["Yu", "Lequan", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1905.01723", "submitter": "Ming-Yu Liu", "authors": "Ming-Yu Liu and Xun Huang and Arun Mallya and Tero Karras and Timo\n  Aila and Jaakko Lehtinen and Jan Kautz", "title": "Few-Shot Unsupervised Image-to-Image Translation", "comments": "The paper will be presented at the International Conference on\n  Computer Vision (ICCV) 2019", "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation methods learn to map images in a\ngiven class to an analogous image in a different class, drawing on unstructured\n(non-registered) datasets of images. While remarkably successful, current\nmethods require access to many images in both source and destination classes at\ntraining time. We argue this greatly limits their use. Drawing inspiration from\nthe human capability of picking up the essence of a novel object from a small\nnumber of examples and generalizing from there, we seek a few-shot,\nunsupervised image-to-image translation algorithm that works on previously\nunseen target classes that are specified, at test time, only by a few example\nimages. Our model achieves this few-shot generation capability by coupling an\nadversarial training scheme with a novel network design. Through extensive\nexperimental validation and comparisons to several baseline methods on\nbenchmark datasets, we verify the effectiveness of the proposed framework. Our\nimplementation and datasets are available at https://github.com/NVlabs/FUNIT .\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 17:41:31 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 06:11:56 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Ming-Yu", ""], ["Huang", "Xun", ""], ["Mallya", "Arun", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Lehtinen", "Jaakko", ""], ["Kautz", "Jan", ""]]}, {"id": "1905.02366", "submitter": "Henan Zhao", "authors": "Henan Zhao, Garnett W. Bryant, Wesley Griffin, Judith E. Terrill, and\n  Jian Chen", "title": "What Do People See in a Twenty-Second Glimpse of Bivariate Vector Field\n  Visualizations?", "comments": "We want to withdraw this paper in the current stage, because (1)\n  Terminology and argumentation may need major improvement; (2) There are\n  several typos in this paper; (3) The title could be adjusted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little is known about how people learn from a brief glimpse of\nthree-dimensional (3D) bivariate vector field visualizations and about how well\nvisual features can guide behavior. Here we report empirical study results on\nthe use of color, texture, and length to guide viewing of bivariate glyphs:\nthese three visual features are mapped to the first integer variable (v1) and\nlength to the second quantitative variable (v2). Participants performed two\ntasks within 20 seconds: (1) MAX: find the largest v2 when v1 is fixed; (2)\nSEARCH: find a specific bivariate variable shown on the screen in a vector\nfield. Our first study with eighteen participants performing these tasks showed\nthat the randomized vector positions, although they lessened viewers' ability\nto group vectors, did not reduce task accuracy compared to structured vector\nfields. This result may support that these color, texture, and length can\nprovide to a certain degree, guide viewers' attention to task-relevant regions.\nThe second study measured eye movement to quantify viewers' behaviors with\nthree-errors (scanning, recognition, and decision errors) and one-behavior\n(refixation) metrics. Our results showed two dominant search strategies:\ndrilling and scanning. Coloring tended to restrict eye movement to the\ntask-relevant regions of interest, enabling drilling. Length tended to support\nscanners who quickly wandered around at different v1 levels. Drillers had\nsignificantly less errors than scanners and the error rates for color and\ntexture were also lowest. And length had limited discrimination power than\ncolor and texture as a 3D visual guidance. Our experiment results may suggest\nthat using categorical visual feature could help obtain the global structure of\na vector field visualization. We provide the first benchmark of the attention\ncost of seeing a bivariate vector on average about 5 items per second.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 06:15:28 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 21:50:57 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 03:34:36 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Zhao", "Henan", ""], ["Bryant", "Garnett W.", ""], ["Griffin", "Wesley", ""], ["Terrill", "Judith E.", ""], ["Chen", "Jian", ""]]}, {"id": "1905.02586", "submitter": "Henan Zhao", "authors": "Henan Zhao and Jian Chen", "title": "Picturing Bivariate Separable-Features for Univariate Vector Magnitudes\n  in Large-Magnitude-Range Quantum Physics Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.02333", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present study results from two experiments to empirically validate that\nseparable bivariate pairs for univariate representations of\nlarge-magnitude-range vectors are more efficient than integral pairs. The first\nexperiment with 20 participants compared: one integral pair, three separable\npairs, and one redundant pair, which is a mix of the integral and separable\nfeatures. Participants performed three local tasks requiring reading numerical\nvalues, estimating ratio, and comparing two points. The second 18-participant\nstudy compared three separable pairs using three global tasks when participants\nmust look at the entire field to get an answer: find a specific target in 20\nseconds, find the maximum magnitude in 20 seconds, and estimate the total\nnumber of vector exponents within 2 seconds. Our results also reveal the\nfollowing: separable pairs led to the most accurate answers and the shortest\ntask execution time, while integral dimensions were among the least accurate;\nit achieved high performance only when a pop-out separable feature (here color)\nwas added. To reconcile this finding with the existing literature, our second\nexperiment suggests that the higher the separability, the higher the accuracy;\nthe reason is probably that the emergent global scene created by the separable\npairs reduces the subsequent search space.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 07:58:41 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Zhao", "Henan", ""], ["Chen", "Jian", ""]]}, {"id": "1905.02876", "submitter": "Giorgos Bouritsas", "authors": "Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael\n  Bronstein, Stefanos Zafeiriou", "title": "Neural 3D Morphable Models: Spiral Convolutional Networks for 3D Shape\n  Representation Learning and Generation", "comments": "to appear at ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for 3D geometric data arise in many important applications\nin 3D computer vision and graphics. In this paper, we focus on 3D deformable\nshapes that share a common topological structure, such as human faces and\nbodies. Morphable Models and their variants, despite their linear formulation,\nhave been widely used for shape representation, while most of the recently\nproposed nonlinear approaches resort to intermediate representations, such as\n3D voxel grids or 2D views. In this work, we introduce a novel graph\nconvolutional operator, acting directly on the 3D mesh, that explicitly models\nthe inductive bias of the fixed underlying graph. This is achieved by enforcing\nconsistent local orderings of the vertices of the graph, through the spiral\noperator, thus breaking the permutation invariance property that is adopted by\nall the prior work on Graph Neural Networks. Our operator comes by construction\nwith desirable properties (anisotropic, topology-aware, lightweight,\neasy-to-optimise), and by using it as a building block for traditional deep\ngenerative architectures, we demonstrate state-of-the-art results on a variety\nof 3D shape datasets compared to the linear Morphable Model and other graph\nconvolutional operators.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 02:37:27 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 21:14:27 GMT"}, {"version": "v3", "created": "Sat, 3 Aug 2019 00:14:45 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Bouritsas", "Giorgos", ""], ["Bokhnyak", "Sergiy", ""], ["Ploumpis", "Stylianos", ""], ["Bronstein", "Michael", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1905.02902", "submitter": "Jun Wu", "authors": "Jun Wu, Weiming Wang and Xifeng Gao", "title": "Design and Optimization of Conforming Lattice Structures", "comments": "14 pages", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2019", "doi": "10.1109/TVCG.2019.2938946", "report-no": null, "categories": "cs.CE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by natural cellular materials such as trabecular bone, lattice\nstructures have been developed as a new type of lightweight material. In this\npaper we present a novel method to design lattice structures that conform with\nboth the principal stress directions and the boundary of the optimized shape.\nOur method consists of two major steps: the first optimizes concurrently the\nshape (including its topology) and the distribution of orthotropic lattice\nmaterials inside the shape to maximize stiffness under application-specific\nexternal loads; the second takes the optimized configuration (i.e.\nlocally-defined orientation, porosity, and anisotropy) of lattice materials\nfrom the previous step, and extracts a globally consistent lattice structure by\nfield-aligned parameterization. Our approach is robust and works for both 2D\nplanar and 3D volumetric domains. Numerical results and physical verifications\ndemonstrate remarkable structural properties of conforming lattice structures\ngenerated by our method.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 04:17:38 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wu", "Jun", ""], ["Wang", "Weiming", ""], ["Gao", "Xifeng", ""]]}, {"id": "1905.03705", "submitter": "Tao Wang", "authors": "Tao Wang and Anup Basu", "title": "A note on 'A fully parallel 3D thinning algorithm and its applications'", "comments": null, "journal-ref": "Pattern Recognition Letters, 2007", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A 3D thinning algorithm erodes a 3D binary image layer by layer to extract\nthe skeletons. This paper presents a correction to Ma and Sonka's thinning\nalgorithm, A fully parallel 3D thinning algorithm and its applications, which\nfails to preserve connectivity of 3D objects. We start with Ma and Sonka's\nalgorithm and examine its verification of connectivity preservation. Our\nanalysis leads to a group of different deleting templates, which can preserve\nconnectivity of 3D objects.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 19:57:49 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Wang", "Tao", ""], ["Basu", "Anup", ""]]}, {"id": "1905.03908", "submitter": "Dawei Wang Mr", "authors": "Xin Yang, Wenbo Hu, Dawei Wang, Lijing Zhao, Baocai Yin, Qiang Zhang,\n  Xiaopeng Wei, Hongbo Fu", "title": "DEMC: A Deep Dual-Encoder Network for Denoising Monte Carlo Rendering", "comments": "Published in Journal of Computer Science and Technology. The final\n  publication is available at springerlink.com", "journal-ref": "Journal of Computer Science and Technology, 2019, 34.5: 1123-1135", "doi": "10.1007/s11390-019-1964-2", "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present DEMC, a deep Dual-Encoder network to remove Monte\nCarlo noise efficiently while preserving details. Denoising Monte Carlo\nrendering is different from natural image denoising since inexpensive\nby-products (feature buffers) can be extracted in the rendering stage. Most of\nthem are noise-free and can provide sufficient details for image\nreconstruction. However, these feature buffers also contain redundant\ninformation, which makes Monte Carlo denoising different from natural image\ndenoising. Hence, the main challenge of this topic is how to extract useful\ninformation and reconstruct clean images. To address this problem, we propose a\nnovel network structure, Dual-Encoder network with a feature fusion\nsub-network, to fuse feature buffers firstly, then encode the fused feature\nbuffers and a noisy image simultaneously, and finally reconstruct a clean image\nby a decoder network. Compared with the state-of-the-art methods, our model is\nmore robust on a wide range of scenes and is able to generate satisfactory\nresults in a significantly faster way.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 01:42:06 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 08:16:45 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yang", "Xin", ""], ["Hu", "Wenbo", ""], ["Wang", "Dawei", ""], ["Zhao", "Lijing", ""], ["Yin", "Baocai", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Fu", "Hongbo", ""]]}, {"id": "1905.04000", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Jia-Kai Chou, Shilpika, Panpan Xu, Liu Ren,\n  Kwan-Liu Ma", "title": "An Incremental Dimensionality Reduction Method for Visualizing Streaming\n  Multidimensional Data", "comments": "This is the author's version of the article that has been published\n  in IEEE Transactions on Visualization and Computer Graphics. The final\n  version of this record is available at: 10.1109/TVCG.2019.2934433", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934433", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) methods are commonly used for analyzing and\nvisualizing multidimensional data. However, when data is a live streaming feed,\nconventional DR methods cannot be directly used because of their computational\ncomplexity and inability to preserve the projected data positions at previous\ntime points. In addition, the problem becomes even more challenging when the\ndynamic data records have a varying number of dimensions as often found in\nreal-world applications. This paper presents an incremental DR solution. We\nenhance an existing incremental PCA method in several ways to ensure its\nusability for visualizing streaming multidimensional data. First, we use\ngeometric transformation and animation methods to help preserve a viewer's\nmental map when visualizing the incremental results. Second, to handle data\ndimension variants, we use an optimization method to estimate the projected\ndata positions, and also convey the resulting uncertainty in the visualization.\nWe demonstrate the effectiveness of our design with two case studies using\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 08:15:42 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 05:38:20 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 04:16:00 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Chou", "Jia-Kai", ""], ["Shilpika", "", ""], ["Xu", "Panpan", ""], ["Ren", "Liu", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1905.04153", "submitter": "Shiyu Song", "authors": "Weixin Lu, Guowei Wan, Yao Zhou, Xiangyu Fu, Pengfei Yuan, Shiyu Song", "title": "DeepICP: An End-to-End Deep Neural Network for 3D Point Cloud\n  Registration", "comments": "10 pages, 6 figures, 3 tables, typos corrected, experimental results\n  updated, accepted by ICCV 2019", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 12-21", "doi": "10.1109/ICCV.2019.00010", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepICP - a novel end-to-end learning-based 3D point cloud\nregistration framework that achieves comparable registration accuracy to prior\nstate-of-the-art geometric methods. Different from other keypoint based methods\nwhere a RANSAC procedure is usually needed, we implement the use of various\ndeep neural network structures to establish an end-to-end trainable network.\nOur keypoint detector is trained through this end-to-end structure and enables\nthe system to avoid the inference of dynamic objects, leverages the help of\nsufficiently salient features on stationary objects, and as a result, achieves\nhigh robustness. Rather than searching the corresponding points among existing\npoints, the key contribution is that we innovatively generate them based on\nlearned matching probabilities among a group of candidates, which can boost the\nregistration accuracy. Our loss function incorporates both the local similarity\nand the global geometric constraints to ensure all above network designs can\nconverge towards the right direction. We comprehensively validate the\neffectiveness of our approach using both the KITTI dataset and the\nApollo-SouthBay dataset. Results demonstrate that our method achieves\ncomparable or better performance than the state-of-the-art geometry-based\nmethods. Detailed ablation and visualization analysis are included to further\nillustrate the behavior and insights of our network. The low registration error\nand high robustness of our method makes it attractive for substantial\napplications relying on the point cloud registration task.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:08:28 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 08:49:52 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lu", "Weixin", ""], ["Wan", "Guowei", ""], ["Zhou", "Yao", ""], ["Fu", "Xiangyu", ""], ["Yuan", "Pengfei", ""], ["Song", "Shiyu", ""]]}, {"id": "1905.04678", "submitter": "Wei Pan", "authors": "Wei Pan, Xuequan Lu, Yuanhao Gong, Wenming Tang, Jun Liu, Ying He,\n  Guoping Qiu", "title": "HLO: Half-kernel Laplacian Operator for Surface Smoothing", "comments": "Accepted to Computer Aided Design; Binary (exe) program avaliable:\n  https://github.com/WillPanSUTD/hlo", "journal-ref": null, "doi": "10.1016/j.cad.2019.102807", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple yet effective method for feature-preserving\nsurface smoothing. Through analyzing the differential property of surfaces, we\nshow that the conventional discrete Laplacian operator with uniform weights is\nnot applicable to feature points at which the surface is non-differentiable and\nthe second order derivatives do not exist. To overcome this difficulty, we\npropose a Half-kernel Laplacian Operator (HLO) as an alternative to the\nconventional Laplacian. Given a vertex v, HLO first finds all pairs of its\nneighboring vertices and divides each pair into two subsets (called half\nwindows); then computes the uniform Laplacians of all such subsets and\nsubsequently projects the computed Laplacians to the full-window uniform\nLaplacian to alleviate flipping and degeneration. The half window with least\nregularization energy is then chosen for v. We develop an iterative approach to\napply HLO for surface denoising. Our method is conceptually simple and easy to\nuse because it has a single parameter, i.e., the number of iterations for\nupdating vertices. We show that our method can preserve features better than\nthe popular uniform Laplacian-based denoising and it significantly alleviates\nthe shrinkage artifact. Extensive experimental results demonstrate that HLO is\nbetter than or comparable to state-of-the-art techniques both qualitatively and\nquantitatively and that it is particularly good at handling meshes with high\nnoise. We will make our source code publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 09:32:39 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 00:42:49 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Pan", "Wei", ""], ["Lu", "Xuequan", ""], ["Gong", "Yuanhao", ""], ["Tang", "Wenming", ""], ["Liu", "Jun", ""], ["He", "Ying", ""], ["Qiu", "Guoping", ""]]}, {"id": "1905.05161", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Alec Jacobson, Maks Ovsjanikov", "title": "Spectral Coarsening of Geometric Operators", "comments": "13 pages, 30 figures. ACM Transactions on Graphics (SIGGRAPH) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to measure the behavior of a geometric operator\nbefore and after coarsening. By comparing eigenvectors of the input operator\nand its coarsened counterpart, we can quantitatively and visually analyze how\nwell the spectral properties of the operator are maintained. Using this\nmeasure, we show that standard mesh simplification and algebraic coarsening\ntechniques fail to maintain spectral properties. In response, we introduce a\nnovel approach for spectral coarsening. We show that it is possible to\nsignificantly reduce the sampling density of an operator derived from a 3D\nshape without affecting the low-frequency eigenvectors. By marrying techniques\ndeveloped within the algebraic multigrid and the functional maps literatures,\nwe successfully coarsen a variety of isotropic and anisotropic operators while\nmaintaining sparsity and positive semi-definiteness. We demonstrate the utility\nof this approach for applications including operator-sensitive sampling, shape\nmatching, and graph pooling for convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 17:50:37 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Jacobson", "Alec", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1905.05172", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo\n  Kanazawa, Hao Li", "title": "PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human\n  Digitization", "comments": "project page: https://shunsukesaito.github.io/PIFu", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 2304-2314", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Pixel-aligned Implicit Function (PIFu), a highly effective\nimplicit representation that locally aligns pixels of 2D images with the global\ncontext of their corresponding 3D object. Using PIFu, we propose an end-to-end\ndeep learning method for digitizing highly detailed clothed humans that can\ninfer both 3D surface and texture from a single image, and optionally, multiple\ninput images. Highly intricate shapes, such as hairstyles, clothing, as well as\ntheir variations and deformations can be digitized in a unified way. Compared\nto existing representations used for 3D deep learning, PIFu can produce\nhigh-resolution surfaces including largely unseen regions such as the back of a\nperson. In particular, it is memory efficient unlike the voxel representation,\ncan handle arbitrary topology, and the resulting surface is spatially aligned\nwith the input image. Furthermore, while previous techniques are designed to\nprocess either a single image or multiple views, PIFu extends naturally to\narbitrary number of views. We demonstrate high-resolution and robust\nreconstructions on real world images from the DeepFashion dataset, which\ncontains a variety of challenging clothing types. Our method achieves\nstate-of-the-art performance on a public benchmark and outperforms the prior\nwork for clothed human digitization from a single image.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 17:59:56 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 21:51:04 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2019 19:00:16 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Saito", "Shunsuke", ""], ["Huang", "Zeng", ""], ["Natsume", "Ryota", ""], ["Morishima", "Shigeo", ""], ["Kanazawa", "Angjoo", ""], ["Li", "Hao", ""]]}, {"id": "1905.05298", "submitter": "Vinicius Carid\\'a", "authors": "Amir Jalilifard, Vinicius Carid\\'a, Alex Mansano, Rogers Cristo", "title": "Can NetGAN be improved on short random walks?", "comments": "6 pages, 6 figures, 1 table, 10 equations, 22 citations Paper was\n  submited to BRACIS conferece 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are useful structures that can model several important real-world\nproblems. Recently, learning graphs have drawn considerable attention, leading\nto the proposal of new methods for learning these data structures. One of these\nstudies produced NetGAN, a new approach for generating graphs via random walks.\nAlthough NetGAN has shown promising results in terms of accuracy in the tasks\nof generating graphs and link prediction, the choice of vertices from which it\nstarts random walks can lead to inconsistent and highly variable results,\nespecially when the length of walks is short. As an alternative to random\nstarting, this study aims to establish a new method for initializing random\nwalks from a set of dense vertices. We purpose estimating the importance of a\nnode based on the inverse of its influence over the whole vertices of its\nneighborhood through random walks of different sizes. The proposed method\nmanages to achieve significantly better accuracy, less variance and lesser\noutliers.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 21:42:50 GMT"}, {"version": "v2", "created": "Sun, 5 Jan 2020 22:21:53 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Jalilifard", "Amir", ""], ["Carid\u00e1", "Vinicius", ""], ["Mansano", "Alex", ""], ["Cristo", "Rogers", ""]]}, {"id": "1905.05411", "submitter": "Nicolas Holliman Professor", "authors": "Richard Cloete, Nick Holliman", "title": "Measuring and simulating latency in interactive remote rendering systems", "comments": "Minor update to typos and acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: The computationally intensive task of real-time rendering can be\noffloaded to remote cloud systems. However, due to network latency, interactive\nremote rendering (IRR) introduces the challenge of interaction latency (IL),\nwhich is the time between an action and response to that action. Objectives: to\nmodel sources of latency, measure it in a real-world network and to use this\nunderstanding to simulate latency so that we have a controlled platform for\nexperimental work in latency management. Method: we present a seven-parameter\nmodel of latency for a typical IRR system; we describe new, minimally intrusive\nsoftware methods for measuring latency in a 3D graphics environment and create\na novel latency simulator tool in software. Results: We demonstrate our latency\nsimulator is comparable to real-world behavior and confirm that real-world\nlatency exceeds the interactive limit of 70ms over long distance connections.\nWe also find that current approaches to measuring IL are not general enough for\nmost situations and therefore propose a novel general-purpose solution.\nConclusion: to ameliorate latency in IRR systems we need controllable\nsimulation tools for experimentation. In addition to a new measurement\ntechnique, we propose a new approach that will be of interest to IRR\nresearchers and developers when designing IL compensation techniques.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 06:38:36 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 19:58:34 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cloete", "Richard", ""], ["Holliman", "Nick", ""]]}, {"id": "1905.05622", "submitter": "Boyi Jiang", "authors": "Boyi Jiang, Juyong Zhang, Jianfei Cai, Jianmin Zheng", "title": "Disentangled Human Body Embedding Based on Deep Hierarchical Neural\n  Network", "comments": "This manuscript is accepted for publication in the IEEE Transactions\n  on Visualization and Computer Graphics Journal (IEEE TVCG). The Code is\n  available at https://github.com/Juyong/DHNN_BodyRepresentation", "journal-ref": null, "doi": "10.1109/TVCG.2020.2988476", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human bodies exhibit various shapes for different identities or poses, but\nthe body shape has certain similarities in structure and thus can be embedded\nin a low-dimensional space. This paper presents an autoencoder-like network\narchitecture to learn disentangled shape and pose embedding specifically for\nthe 3D human body. This is inspired by recent progress of deformation-based\nlatent representation learning. To improve the reconstruction accuracy, we\npropose a hierarchical reconstruction pipeline for the disentangling process\nand construct a large dataset of human body models with consistent connectivity\nfor the learning of the neural network. Our learned embedding can not only\nachieve superior reconstruction accuracy but also provide great flexibility in\n3D human body generation via interpolation, bilinear interpolation, and latent\nspace sampling. The results from extensive experiments demonstrate the\npowerfulness of our learned 3D human body embedding in various applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:06:54 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 10:09:32 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Jiang", "Boyi", ""], ["Zhang", "Juyong", ""], ["Cai", "Jianfei", ""], ["Zheng", "Jianmin", ""]]}, {"id": "1905.06229", "submitter": "Josef Spjut", "authors": "Josef Spjut and Ben Boudaoud and Jonghyun Kim and Trey Greer and\n  Rachel Albert and Michael Stengel and Kaan Aksit and David Luebke", "title": "Toward Standardized Classification of Foveated Displays", "comments": "9 pages, 8 figures, presented at IEEE VR 2020", "journal-ref": "in IEEE Transactions on Visualization and Computer Graphics, vol.\n  26, no. 5, pp. 2126-2134, May 2020", "doi": "10.1109/TVCG.2020.2973053", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent in the field of head mounted display design is a desire to leverage\nthe limitations of the human visual system to reduce the computation,\ncommunication, and display workload in power and form-factor constrained\nsystems. Fundamental to this reduced workload is the ability to match display\nresolution to the acuity of the human visual system, along with a resulting\nneed to follow the gaze of the eye as it moves, a process referred to as\nfoveation. A display that moves its content along with the eye may be called a\nFoveated Display, though this term is also commonly used to describe displays\nwith non-uniform resolution that attempt to mimic human visual acuity. We\ntherefore recommend a definition for the term Foveated Display that accepts\nboth of these interpretations. Furthermore, we include a simplified model for\nhuman visual Acuity Distribution Functions (ADFs) at various levels of visual\nacuity, across wide fields of view and propose comparison of this ADF with the\nResolution Distribution Function of a foveated display for evaluation of its\nresolution at a particular gaze direction. We also provide a taxonomy to allow\nthe field to meaningfully compare and contrast various aspects of foveated\ndisplays in a display and optical technology-agnostic manner.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 20:45:05 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:14:09 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Kim", "Jonghyun", ""], ["Greer", "Trey", ""], ["Albert", "Rachel", ""], ["Stengel", "Michael", ""], ["Aksit", "Kaan", ""], ["Luebke", "David", ""]]}, {"id": "1905.06326", "submitter": "Xuaner Cecilia Zhang", "authors": "Xuaner Zhang, Kevin Matzen, Vivien Nguyen, Dillon Yao, You Zhang, Ren\n  Ng", "title": "Synthetic Defocus and Look-Ahead Autofocus for Casual Videography", "comments": "(V2 author name corrected) SIGGRAPH 2019; project website:\n  https://ceciliavision.github.io/vid-auto-focus/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cinema, large camera lenses create beautiful shallow depth of field (DOF),\nbut make focusing difficult and expensive. Accurate cinema focus usually relies\non a script and a person to control focus in realtime. Casual videographers\noften crave cinematic focus, but fail to achieve it. We either sacrifice\nshallow DOF, as in smartphone videos; or we struggle to deliver accurate focus,\nas in videos from larger cameras. This paper is about a new approach in the\npursuit of cinematic focus for casual videography. We present a system that\nsynthetically renders refocusable video from a deep DOF video shot with a\nsmartphone, and analyzes future video frames to deliver context-aware autofocus\nfor the current frame. To create refocusable video, we extend recent machine\nlearning methods designed for still photography, contributing a new dataset for\nmachine training, a rendering model better suited to cinema focus, and a\nfiltering solution for temporal coherence. To choose focus accurately for each\nframe, we demonstrate autofocus that looks at upcoming video frames and applies\nAI-assist modules such as motion, face, audio and saliency detection. We also\nshow that autofocus benefits from machine learning and a large-scale video\ndataset with focus annotation, where we use our RVR-LAAF GUI to create this\nsizable dataset efficiently. We deliver, for example, a shallow DOF video where\nthe autofocus transitions onto each person before she begins to speak. This is\nimpossible for conventional camera autofocus because it would require seeing\ninto the future.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:59:05 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 08:09:48 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 13:26:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Zhang", "Xuaner", ""], ["Matzen", "Kevin", ""], ["Nguyen", "Vivien", ""], ["Yao", "Dillon", ""], ["Zhang", "You", ""], ["Ng", "Ren", ""]]}, {"id": "1905.06598", "submitter": "Gustav Eje Henter", "authors": "Gustav Eje Henter, Simon Alexanderson, Jonas Beskow", "title": "MoGlow: Probabilistic and controllable motion synthesis using\n  normalising flows", "comments": "14 pages, 5 figures, published in ACM Transactions on Graphics and\n  presented at SIGGRAPH Asia 2020", "journal-ref": "ACM Trans. Graph. 39, 4, Article 236 (November 2020), 14 pages", "doi": "10.1145/3414685.3417836", "report-no": null, "categories": "cs.LG cs.GR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven modelling and synthesis of motion is an active research area with\napplications that include animation, games, and social robotics. This paper\nintroduces a new class of probabilistic, generative, and controllable\nmotion-data models based on normalising flows. Models of this kind can describe\nhighly complex distributions, yet can be trained efficiently using exact\nmaximum likelihood, unlike GANs or VAEs. Our proposed model is autoregressive\nand uses LSTMs to enable arbitrarily long time-dependencies. Importantly, is is\nalso causal, meaning that each pose in the output sequence is generated without\naccess to poses or control inputs from future time steps; this absence of\nalgorithmic latency is important for interactive applications with real-time\nmotion control. The approach can in principle be applied to any type of motion\nsince it does not make restrictive, task-specific assumptions regarding the\nmotion or the character morphology. We evaluate the models on motion-capture\ndatasets of human and quadruped locomotion. Objective and subjective results\nshow that randomly-sampled motion from the proposed method outperforms\ntask-agnostic baselines and attains a motion quality close to recorded motion\ncapture.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 08:41:12 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 16:19:40 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 16:25:25 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Henter", "Gustav Eje", ""], ["Alexanderson", "Simon", ""], ["Beskow", "Jonas", ""]]}, {"id": "1905.06812", "submitter": "Guan Wang", "authors": "Guan Wang, Hamid Laga, Jinyuan Jia, Stanley J. Miklavcic, Anuj\n  Srivastava", "title": "Statistical Analysis and Modeling of the Geometry and Topology of Plant\n  Roots", "comments": null, "journal-ref": "Journal of Theoretical Biology, 2020", "doi": "10.1016/j.jtbi.2019.110108", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The root is an important organ of a plant since it is responsible for water\nand nutrient uptake. Analyzing and modelling variabilities in the geometry and\ntopology of roots can help in assessing the plant's health, understanding its\ngrowth patterns, and modeling relations between plant species and between\nplants and their environment. In this article, we develop a framework for the\nstatistical analysis and modeling of the geometry and topology of plant roots.\nWe represent root structures as points in a tree-shape space equipped with a\nmetric that quantifies geometric and topological differences between pairs of\nroots. We then use these building blocks to compute geodesics, i.e., optimal\ndeformations under the metric between root structures, and to perform\nstatistical analysis on root populations. We demonstrate the utility of the\nproposed framework through an application to a dataset of wheat roots grown in\ndifferent environmental conditions. We also show that the framework can be used\nin various applications including classification and regression.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 14:53:05 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 09:45:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Guan", ""], ["Laga", "Hamid", ""], ["Jia", "Jinyuan", ""], ["Miklavcic", "Stanley J.", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1905.07432", "submitter": "David Barina", "authors": "David Barina and Tomas Chlubna and Marek Solony and Drahomir Dlabaja\n  and Pavel Zemcik", "title": "Evaluation of 4D Light Field Compression Methods", "comments": "accepted for publication and presentation at the WSCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field data records the amount of light at multiple points in space,\ncaptured e.g. by an array of cameras or by a light-field camera that uses\nmicrolenses. Since the storage and transmission requirements for such data are\ntremendous, compression techniques for light fields are gaining momentum in\nrecent years. Although plenty of efficient compression formats do exist for\nstill and moving images, only a little research on the impact of these methods\non light field imagery is performed. In this paper, we evaluate the impact of\nstate-of-the-art image and video compression methods on quality of images\nrendered from light field data. The methods include recent video compression\nstandards, especially AV1 and XVC finalised in 2018. To fully exploit the\npotential of common image compression methods on four-dimensional light field\nimagery, we have extended these methods into three and four dimensions. In this\npaper, we show that the four-dimensional light field data can be compressed\nmuch more than independent still images while maintaining the same visual\nquality of a perceived picture. We gradually compare the compression\nperformance of all image and video compression methods, and eventually answer\nthe question, \"What is the best compression method for light field data?\".\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:37:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Barina", "David", ""], ["Chlubna", "Tomas", ""], ["Solony", "Marek", ""], ["Dlabaja", "Drahomir", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1905.07442", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler", "title": "Transport-Based Neural Style Transfer for Smoke Simulations", "comments": "ACM Transaction on Graphics (SIGGRAPH ASIA 2019), additional\n  materials: http://www.byungsoo.me/project/neural-flow-style", "journal-ref": "ACM Trans. Graph. 38, 6, Article 188 (November 2019), 11 pages", "doi": "10.1145/3355089.3356560", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistically controlling fluids has always been a challenging task.\nOptimization techniques rely on approximating simulation states towards target\nvelocity or density field configurations, which are often handcrafted by\nartists to indirectly control smoke dynamics. Patch synthesis techniques\ntransfer image textures or simulation features to a target flow field. However,\nthese are either limited to adding structural patterns or augmenting coarse\nflows with turbulent structures, and hence cannot capture the full spectrum of\ndifferent styles and semantically complex structures. In this paper, we propose\nthe first Transport-based Neural Style Transfer (TNST) algorithm for volumetric\nsmoke data. Our method is able to transfer features from natural images to\nsmoke simulations, enabling general content-aware manipulations ranging from\nsimple patterns to intricate motifs. The proposed algorithm is physically\ninspired, since it computes the density transport from a source input smoke to\na desired target configuration. Our transport-based approach allows direct\ncontrol over the divergence of the stylization velocity field by optimizing\nincompressible and irrotational potentials that transport smoke towards\nstylization. Temporal consistency is ensured by transporting and aligning\nsubsequent stylized velocities, and 3D reconstructions are computed by\nseamlessly merging stylizations from different camera viewpoints.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:55:27 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 14:33:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Gross", "Markus", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1905.07515", "submitter": "Yajie Zhao", "authors": "Yajie Zhao, Zeng Huang, Tianye Li, Weikai Chen, Chloe LeGendre,\n  Xinglei Ren, Jun Xing, Ari Shapiro, and Hao Li", "title": "Learning Perspective Undistortion of Portraits", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near-range portrait photographs often contain perspective distortion\nartifacts that bias human perception and challenge both facial recognition and\nreconstruction techniques. We present the first deep learning based approach to\nremove such artifacts from unconstrained portraits. In contrast to the previous\nstate-of-the-art approach, our method handles even portraits with extreme\nperspective distortion, as we avoid the inaccurate and error-prone step of\nfirst fitting a 3D face model. Instead, we predict a distortion correction flow\nmap that encodes a per-pixel displacement that removes distortion artifacts\nwhen applied to the input image. Our method also automatically infers missing\nfacial features, i.e. occluded ears caused by strong perspective distortion,\nwith coherent details. We demonstrate that our approach significantly\noutperforms the previous state-of-the-art both qualitatively and\nquantitatively, particularly for portraits with extreme perspective distortion\nor facial expressions. We further show that our technique benefits a number of\nfundamental tasks, significantly improving the accuracy of both face\nrecognition and 3D reconstruction and enables a novel camera calibration\ntechnique from a single portrait. Moreover, we also build the first perspective\nportrait database with a large diversity in identities, expression and poses,\nwhich will benefit the related research in this area.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 01:08:47 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Zhao", "Yajie", ""], ["Huang", "Zeng", ""], ["Li", "Tianye", ""], ["Chen", "Weikai", ""], ["LeGendre", "Chloe", ""], ["Ren", "Xinglei", ""], ["Xing", "Jun", ""], ["Shapiro", "Ari", ""], ["Li", "Hao", ""]]}, {"id": "1905.07518", "submitter": "Pengbo Bo", "authors": "Pengbo Bo and Yujian Zheng and Caiming Zhang", "title": "Smooth quasi-developable surfaces bounded by smooth curves", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing a quasi-developable strip surface bounded by design curves finds\nwide industrial applications. Existing methods compute discrete surfaces\ncomposed of developable lines connecting sampling points on input curves which\nare not adequate for generating smooth quasi-developable surfaces. We propose\nthe first method which is capable of exploring the full solution space of\ncontinuous input curves to compute a smooth quasi-developable ruled surface\nwith as large developability as possible. The resulting surface is exactly\nbounded by the input smooth curves and is guaranteed to have no\nself-intersections. The main contribution is a variational approach to compute\na continuous mapping of parameters of input curves by minimizing a function\nevaluating surface developability. Moreover, we also present an algorithm to\nrepresent a resulting surface as a B-spline surface when input curves are\nB-spline curves.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 01:37:38 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bo", "Pengbo", ""], ["Zheng", "Yujian", ""], ["Zhang", "Caiming", ""]]}, {"id": "1905.08233", "submitter": "Egor Zakharov", "authors": "Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, Victor Lempitsky", "title": "Few-Shot Adversarial Learning of Realistic Neural Talking Head Models", "comments": "UPDATE: the data we used for evaluation is available for download!\n  See https://drive.google.com/open?id=1PeGG6zO3ZjrHk2GAXItB8khwMhPPyDHe and\n  refer to the README for description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have shown how highly realistic human head images can be\nobtained by training convolutional neural networks to generate them. In order\nto create a personalized talking head model, these works require training on a\nlarge dataset of images of a single person. However, in many practical\nscenarios, such personalized talking head models need to be learned from a few\nimage views of a person, potentially even a single image. Here, we present a\nsystem with such few-shot capability. It performs lengthy meta-learning on a\nlarge dataset of videos, and after that is able to frame few- and one-shot\nlearning of neural talking head models of previously unseen people as\nadversarial training problems with high capacity generators and discriminators.\nCrucially, the system is able to initialize the parameters of both the\ngenerator and the discriminator in a person-specific way, so that training can\nbe based on just a few images and done quickly, despite the need to tune tens\nof millions of parameters. We show that such an approach is able to learn\nhighly realistic and personalized talking head models of new people and even\nportrait paintings.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:58:04 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 11:16:01 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Zakharov", "Egor", ""], ["Shysheya", "Aliaksandra", ""], ["Burkov", "Egor", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1905.08627", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Arman Eshaghi, Daniel C. Alexander, Polina\n  Golland", "title": "BrainPainter: A software for the visualisation of brain structures,\n  biomarkers and associated pathological processes", "comments": "Accepted at the MICCAI Multimodal Brain Imaging Analysis (MBIA)\n  workshop, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present BrainPainter, a software that automatically generates images of\nhighlighted brain structures given a list of numbers corresponding to the\noutput colours of each region. Compared to existing visualisation software\n(i.e. Freesurfer, SPM, 3D Slicer), BrainPainter has three key advantages: (1)\nit does not require the input data to be in a specialised format, allowing\nBrainPainter to be used in combination with any neuroimaging analysis tools,\n(2) it can visualise both cortical and subcortical structures and (3) it can be\nused to generate movies showing dynamic processes, e.g. propagation of\npathology on the brain. We highlight three use cases where BrainPainter was\nused in existing neuroimaging studies: (1) visualisation of the degree of\natrophy through interpolation along a user-defined gradient of colours, (2)\nvisualisation of the progression of pathology in Alzheimer's disease as well as\n(3) visualisation of pathology in subcortical regions in Huntington's disease.\nMoreover, through the design of BrainPainter we demonstrate the possibility of\nusing a powerful 3D computer graphics engine such as Blender to generate brain\nvisualisations for the neuroscience community. Blender's capabilities, e.g.\nparticle simulations, motion graphics, UV unwrapping, raster graphics editing,\nraytracing and illumination effects, open a wealth of possibilities for brain\nvisualisation not available in current neuroimaging software. BrainPainter is\ncustomisable, easy to use, and can run straight from the web browser:\nhttps://brainpainter.csail.mit.edu , as well as from source-code packaged in a\ndocker container: https://github.com/mrazvan22/brain-coloring . It can be used\nto visualise biomarker data from any brain imaging modality, or simply to\nhighlight a particular brain structure for e.g. anatomy courses.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 13:37:14 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 22:56:09 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Marinescu", "Razvan V.", ""], ["Eshaghi", "Arman", ""], ["Alexander", "Daniel C.", ""], ["Golland", "Polina", ""]]}, {"id": "1905.08853", "submitter": "Chao Wang", "authors": "Chao Wang and Xiaohu Guo", "title": "Efficient Plane-Based Optimization of Geometry and Texture for Indoor\n  RGB-D Reconstruction", "comments": "In the SUMO Workshop of CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to reconstruct RGB-D indoor scene based on plane\nprimitives. Our approach takes as input a RGB-D sequence and a dense coarse\nmesh reconstructed from it, and generates a lightweight, low-polygonal mesh\nwith clear face textures and sharp features without losing geometry details\nfrom the original scene. Compared to existing methods which only cover large\nplanar regions in the scene, our method builds the entire scene by adaptive\nplanes without losing geometry details and also preserves sharp features in the\nmesh. Experiments show that our method is more efficient to generate textured\nmesh from RGB-D data than state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 20:11:03 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Wang", "Chao", ""], ["Guo", "Xiaohu", ""]]}, {"id": "1905.09395", "submitter": "Dirk Van Essendelft", "authors": "Kyle Buchheit, Opeoluwa Owoyele, Terry Jordan, Dirk Van Essendelft", "title": "The Stabilized Explicit Variable-Load Solver with Machine Learning\n  Acceleration for the Rapid Solution of Stiff Chemical Kinetics", "comments": "25 pages, 14 Figures, 2 Tables, 56 Equations, Original research into\n  accelerating CFD with ML/AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.GR cs.LG cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, a fast and stable machine-learned hybrid algorithm implemented\nin TensorFlow for the integration of stiff chemical kinetics is introduced.\nNumerical solutions to differential equations are at the core of computational\nfluid dynamics calculations. As the size and complexity of the simulations\ngrow, so does the need for computational power and time. Many efforts have been\nmade to implement stiff chemistry solvers on GPUs but have not been highly\nsuccessful because of the logical divergence in traditional stiff solver\nalgorithms. Because of these constrains, a novel Explicit Stabilized\nVariable-load (STEV) solver has been developed. Overstepping due to the\nrelatively large time steps is prevented by introducing limits to the maximum\nchanges of chemical species per time step. To prevent oscillations, a discrete\nFourier transform is introduced to dampen ringing. In contrast to conventional\nexplicit approaches, a variable-load approach is used where each cell in the\ncomputational domain is advanced with its unique time step. This approach\nallows cells to be integrated simultaneously while maintaining warp convergence\nbut finish at different iterations and be removed from the workload. To improve\nthe computational performance of the introduced solver, specific thermodynamic\nquantities of interest were estimated using shallow neural networks in place of\npolynomial fits, leading to an additional 10% savings in clock time with\nminimal training and implementation requirements. However ML specific hardware\ncould increase the time savings to as much as 28%. While the complexity of\nthese particular machine learning models is not high by modern standards, the\nimpact on computational efficiency should not be ignored. The results show a\ndramatic decrease in total chemistry solution time (over 200 times) while\nmaintaining a similar degree of accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:10:35 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 12:26:27 GMT"}, {"version": "v3", "created": "Fri, 21 Jun 2019 21:14:31 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Buchheit", "Kyle", ""], ["Owoyele", "Opeoluwa", ""], ["Jordan", "Terry", ""], ["Van Essendelft", "Dirk", ""]]}, {"id": "1905.09661", "submitter": "Javad Amirian", "authors": "Javad Amirian, Wouter van Toll, Jean-Bernard Hayet, Julien Pettr\\'e", "title": "Data-Driven Crowd Simulation with Generative Adversarial Networks", "comments": "Accepted in CASA '19 (Computer Animation and Social Agents)", "journal-ref": null, "doi": "10.1145/3328756.3328769", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel data-driven crowd simulation method that can\nmimic the observed traffic of pedestrians in a given environment. Given a set\nof observed trajectories, we use a recent form of neural networks, Generative\nAdversarial Networks (GANs), to learn the properties of this set and generate\nnew trajectories with similar properties. We define a way for simulated\npedestrians (agents) to follow such a trajectory while handling local collision\navoidance. As such, the system can generate a crowd that behaves similarly to\nobservations, while still enabling real-time interactions between agents. Via\nexperiments with real-world data, we show that our simulated trajectories\npreserve the statistical properties of their input. Our method simulates crowds\nin real time that resemble existing crowds, while also allowing insertion of\nextra agents, combination with other simulation methods, and user interaction.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:53:31 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Amirian", "Javad", ""], ["van Toll", "Wouter", ""], ["Hayet", "Jean-Bernard", ""], ["Pettr\u00e9", "Julien", ""]]}, {"id": "1905.09777", "submitter": "Oded Stein", "authors": "Oded Stein, Alec Jacobson, Max Wardetzky and Eitan Grinspun", "title": "A Smoothness Energy without Boundary Distortion for Curved Surfaces", "comments": "17 pages, 18 figures", "journal-ref": null, "doi": "10.1145/3377406", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current quadratic smoothness energies for curved surfaces either exhibit\ndistortions near the boundary due to zero Neumann boundary conditions, or they\ndo not correctly account for intrinsic curvature, which leads to\nunnatural-looking behavior away from the boundary. This leads to an unfortunate\ntrade-off: one can either have natural behavior in the interior, or a\ndistortion-free result at the boundary, but not both. We introduce a\ngeneralized Hessian energy for curved surfaces, expressed in terms of the\ncovariant one-form Dirichlet energy, the Gaussian curvature, and the exterior\nderivative. Energy minimizers solve the Laplace-Beltrami biharmonic equation,\ncorrectly accounting for intrinsic curvature, leading to natural-looking\nisolines. On the boundary, minimizers are as-linear-as-possible, which reduces\nthe distortion of isolines at the boundary. We discretize the covariant\none-form Dirichlet energy using Crouzeix-Raviart finite elements, arriving at a\ndiscrete formulation of the Hessian energy for applications on curved surfaces.\nWe observe convergence of the discretization in our experiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:04:31 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 23:08:09 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Stein", "Oded", ""], ["Jacobson", "Alec", ""], ["Wardetzky", "Max", ""], ["Grinspun", "Eitan", ""]]}, {"id": "1905.10035", "submitter": "Vahid Partovi Nia", "authors": "Shaima Tilouche, Vahid Partovi Nia, Samuel Bassetto", "title": "Parallel Coordinate Order for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of high-dimensional data is counter-intuitive using\nconventional graphs. Parallel coordinates are proposed as an alternative to\nexplore multivariate data more effectively. However, it is difficult to extract\nrelevant information through the parallel coordinates when the data are\nhigh-dimensional with thousands of lines overlapping. The order of the axes\ndetermines the perception of information on parallel coordinates. Thus, the\ninformation between attributes remain hidden if coordinates are improperly\nordered. Here we propose a general framework to reorder the coordinates. This\nframework is general to cover a large range of data visualization objective. It\nis also flexible to contain many conventional ordering measures. Consequently,\nwe present the coordinate ordering binary optimization problem and enhance\ntowards a computationally efficient greedy approach that suites\nhigh-dimensional data. Our approach is applied on wine data and on genetic\ndata. The purpose of dimension reordering of wine data is highlighting\nattributes dependence. Genetic data are reordered to enhance cluster detection.\nThe presented framework shows that it is able to adapt the measures and\ncriteria tested.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 05:08:01 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Tilouche", "Shaima", ""], ["Nia", "Vahid Partovi", ""], ["Bassetto", "Samuel", ""]]}, {"id": "1905.10290", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Michael Zollh\\\"ofer, Vladislav Golyanik,\n  Christian Theobalt", "title": "DEMEA: Deep Mesh Autoencoders for Non-Rigidly Deforming Objects", "comments": "27 pages, including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh autoencoders are commonly used for dimensionality reduction, sampling\nand mesh modeling. We propose a general-purpose DEep MEsh Autoencoder (DEMEA)\nwhich adds a novel embedded deformation layer to a graph-convolutional mesh\nautoencoder. The embedded deformation layer (EDL) is a differentiable\ndeformable geometric proxy which explicitly models point displacements of\nnon-rigid deformations in a lower dimensional space and serves as a local\nrigidity regularizer. DEMEA decouples the parameterization of the deformation\nfrom the final mesh resolution since the deformation is defined over a lower\ndimensional embedded deformation graph. We perform a large-scale study on four\ndifferent datasets of deformable objects. Reasoning about the local rigidity of\nmeshes using EDL allows us to achieve higher-quality results for highly\ndeformable objects, compared to directly regressing vertex positions. We\ndemonstrate multiple applications of DEMEA, including non-rigid 3D\nreconstruction from depth and shading cues, non-rigid surface tracking, as well\nas the transfer of deformations over different meshes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:35:37 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:35:07 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Golyanik", "Vladislav", ""], ["Theobalt", "Christian", ""]]}, {"id": "1905.10444", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov and Joshua S. Harvey and Hannah E. Smithson and Jon Y.\n  Hardeberg", "title": "Overt visual attention on rendered 3D objects", "comments": "Draft submitted to a conference. To be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work covers multiple aspects of overt visual attention on 3D renders:\nmeasurement, projection, visualization, and application to studying the\ninfluence of material appearance on looking behaviour. In the scope of this\nwork, we ran an eye-tracking experiment in which the observers are presented\nwith animations of rotating 3D objects. The objects were rendered to simulate\ndifferent metallic appearance, particularly smooth (glossy), rough (matte), and\ncoated gold. The eye-tracking results illustrate how material appearance itself\ninfluences the observer's attention, while all the other parameters remain\nunchanged. In order to make visualization of the attention maps more natural\nand also make the analysis more accurate, we develop a novel technique of\nprojection of gaze fixations on the 3D surface of the figure itself, instead of\nthe conventional 2D plane of the screen. The proposed methodology will be\nuseful for further studies of attention and saliency in the computer graphics\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 21:09:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Harvey", "Joshua S.", ""], ["Smithson", "Hannah E.", ""], ["Hardeberg", "Jon Y.", ""]]}, {"id": "1905.10763", "submitter": "Michal Edelstein", "authors": "Michal Edelstein, Danielle Ezuz, Mirela Ben-Chen", "title": "ENIGMA: Evolutionary Non-Isometric Geometry Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a fully automatic method for shape correspondence\nthat is widely applicable, and especially effective for non isometric shapes\nand shapes of different topology. We observe that fully-automatic shape\ncorrespondence can be decomposed as a hybrid discrete/continuous optimization\nproblem, and we find the best sparse landmark correspondence, whose\nsparse-to-dense extension minimizes a local metric distortion. To tackle the\ncombinatorial task of landmark correspondence we use an evolutionary genetic\nalgorithm, where the local distortion of the sparse-to-dense extension is used\nas the objective function. We design novel geometrically guided genetic\noperators, which, when combined with our objective, are highly effective for\nnon isometric shape matching. Our method outperforms state of the art methods\nfor automatic shape correspondence both quantitatively and qualitatively on\nchallenging datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 08:43:44 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 00:41:03 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 20:44:21 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Edelstein", "Michal", ""], ["Ezuz", "Danielle", ""], ["Ben-Chen", "Mirela", ""]]}, {"id": "1905.10793", "submitter": "Aron Monszpart", "authors": "S\\'ebastien Ehrhardt, Aron Monszpart, Niloy J. Mitra, Andrea Vedaldi", "title": "Unsupervised Intuitive Physics from Past Experiences", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning models of intuitive physics similar to the ones\nthat animals use for navigation, manipulation and planning. In addition to\nlearning general physical principles, however, we are also interested in\nlearning ``on the fly'', from a few experiences, physical properties specific\nto new environments. We do all this in an unsupervised manner, using a\nmeta-learning formulation where the goal is to predict videos containing\ndemonstrations of physical phenomena, such as objects moving and colliding with\na complex background. We introduce the idea of summarizing past experiences in\na very compact manner, in our case using dynamic images, and show that this can\nbe used to solve the problem well and efficiently. Empirically, we show via\nextensive experiments and ablation studies, that our model learns to perform\nphysical predictions that generalize well in time and space, as well as to a\nvariable number of interacting physical objects.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 12:22:56 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ehrhardt", "S\u00e9bastien", ""], ["Monszpart", "Aron", ""], ["Mitra", "Niloy J.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1905.10822", "submitter": "Mohamed Elgharib", "authors": "Mohamed Elgharib, Mallikarjun BR, Ayush Tewari, Hyeongwoo Kim, Wentao\n  Liu, Hans-Peter Seidel, Christian Theobalt", "title": "EgoFace: Egocentric Face Performance Capture and Videorealistic\n  Reenactment", "comments": "Project Page: http://gvv.mpi-inf.mpg.de/projects/EgoFace/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face performance capture and reenactment techniques use multiple cameras and\nsensors, positioned at a distance from the face or mounted on heavy wearable\ndevices. This limits their applications in mobile and outdoor environments. We\npresent EgoFace, a radically new lightweight setup for face performance capture\nand front-view videorealistic reenactment using a single egocentric RGB camera.\nOur lightweight setup allows operations in uncontrolled environments, and lends\nitself to telepresence applications such as video-conferencing from dynamic\nenvironments. The input image is projected into a low dimensional latent space\nof the facial expression parameters. Through careful adversarial training of\nthe parameter-space synthetic rendering, a videorealistic animation is\nproduced. Our problem is challenging as the human visual system is sensitive to\nthe smallest face irregularities that could occur in the final results. This\nsensitivity is even stronger for video results. Our solution is trained in a\npre-processing stage, through a supervised manner without manual annotations.\nEgoFace captures a wide variety of facial expressions, including mouth\nmovements and asymmetrical expressions. It works under varying illuminations,\nbackground, movements, handles people from different ethnicities and can\noperate in real time.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 15:57:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Elgharib", "Mohamed", ""], ["BR", "Mallikarjun", ""], ["Tewari", "Ayush", ""], ["Kim", "Hyeongwoo", ""], ["Liu", "Wentao", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1905.12512", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Zorah L\\\"ahner, Daniel Cremers", "title": "Smooth Shells: Multi-Scale Shape Registration with Functional Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D shape correspondence method based on the iterative\nalignment of so-called smooth shells. Smooth shells define a series of\ncoarse-to-fine shape approximations designed to work well with multiscale\nalgorithms. The main idea is to first align rough approximations of the\ngeometry and then add more and more details to refine the correspondence. We\nfuse classical shape registration with Functional Maps by embedding the input\nshapes into an intrinsic-extrinsic product space. Moreover, we disambiguate\nintrinsic symmetries by applying a surrogate based Markov chain Monte Carlo\ninitialization. Our method naturally handles various types of noise that\ncommonly occur in real scans, like non-isometry or incompatible meshing.\nFinally, we demonstrate state-of-the-art quantitative results on several\ndatasets and show that our pipeline produces smoother, more realistic results\nthan other automatic matching methods in real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 15:01:10 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 16:33:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Eisenberger", "Marvin", ""], ["L\u00e4hner", "Zorah", ""], ["Cremers", "Daniel", ""]]}, {"id": "1905.13149", "submitter": "Fangda Han", "authors": "Fangda Han, Ricardo Guerrero, Vladimir Pavlovic", "title": "The Art of Food: Meal Image Synthesis from Ingredients", "comments": "12 pages, 6 figures, 2 tables, under review as a conference paper at\n  BMVC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new computational framework, based on generative\ndeep models, for synthesis of photo-realistic food meal images from textual\ndescriptions of its ingredients. Previous works on synthesis of images from\ntext typically rely on pre-trained text models to extract text features,\nfollowed by a generative neural networks (GANs) aimed to generate realistic\nimages conditioned on the text features. These works mainly focus on generating\nspatially compact and well-defined categories of objects, such as birds or\nflowers. In contrast, meal images are significantly more complex, consisting of\nmultiple ingredients whose appearance and spatial qualities are further\nmodified by cooking methods. We propose a method that first builds an\nattention-based ingredients-image association model, which is then used to\ncondition a generative neural network tasked with synthesizing meal images.\nFurthermore, a cycle-consistent constraint is added to further improve image\nquality and control appearance. Extensive experiments show our model is able to\ngenerate meal image corresponding to the ingredients, which could be used to\naugment existing dataset for solving other computational food analysis\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 20:57:51 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Han", "Fangda", ""], ["Guerrero", "Ricardo", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1905.13187", "submitter": "Victor Churchill", "authors": "Victor Churchill", "title": "Use of convexity in contour detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we formulate a simple algorithm that detects contours around a\nregion of interest in an image. After an initial smoothing, the method is based\non viewing an image as a topographic surface and finding convex and/or concave\nregions using simple calculus-based testing. The algorithm can achieve\nmulti-scale contour detection by altering the initial smoothing. We show that\nthe method has promise by comparing results on several images with the\nwatershed transform performed on the gradient images.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:13:22 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Churchill", "Victor", ""]]}, {"id": "1905.13607", "submitter": "Richard Jiang", "authors": "Gary Storey, Richard Jiang, Shelagh Keogh, Ahmed Bouridane and\n  Chang-Tsun Li", "title": "3DPalsyNet: A Facial Palsy Grading and Motion Recognition Framework\n  using Fully 3D Convolutional Neural Networks", "comments": null, "journal-ref": "IEEE Access 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capability to perform facial analysis from video sequences has\nsignificant potential to positively impact in many areas of life. One such area\nrelates to the medical domain to specifically aid in the diagnosis and\nrehabilitation of patients with facial palsy. With this application in mind,\nthis paper presents an end-to-end framework, named 3DPalsyNet, for the tasks of\nmouth motion recognition and facial palsy grading. 3DPalsyNet utilizes a 3D CNN\narchitecture with a ResNet backbone for the prediction of these dynamic tasks.\nLeveraging transfer learning from a 3D CNNs pre-trained on the Kinetics data\nset for general action recognition, the model is modified to apply joint\nsupervised learning using center and softmax loss concepts. 3DPalsyNet is\nevaluated on a test set consisting of individuals with varying ranges of facial\npalsy and mouth motions and the results have shown an attractive level of\nclassification accuracy in these task of 82% and 86% respectively. The frame\nduration and the loss function affect was studied in terms of the predictive\nqualities of the proposed 3DPalsyNet, where it was found shorter frame\nduration's of 8 performed best for this specific task. Centre loss and softmax\nhave shown improvements in spatio-temporal feature learning than softmax loss\nalone, this is in agreement with earlier work involving the spatial domain.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:24:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Storey", "Gary", ""], ["Jiang", "Richard", ""], ["Keogh", "Shelagh", ""], ["Bouridane", "Ahmed", ""], ["Li", "Chang-Tsun", ""]]}]