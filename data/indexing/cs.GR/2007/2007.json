[{"id": "2007.00074", "submitter": "Rana Hanocka", "authors": "Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Deep Geometric Texture Synthesis", "comments": "SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392471", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep generative adversarial networks for image generation have\nadvanced rapidly; yet, only a small amount of research has focused on\ngenerative models for irregular structures, particularly meshes. Nonetheless,\nmesh generation and synthesis remains a fundamental topic in computer graphics.\nIn this work, we propose a novel framework for synthesizing geometric textures.\nIt learns geometric texture statistics from local neighborhoods (i.e., local\ntriangular patches) of a single reference 3D model. It learns deep features on\nthe faces of the input triangulation, which is used to subdivide and generate\noffsets across multiple scales, without parameterization of the reference or\ntarget mesh. Our network displaces mesh vertices in any direction (i.e., in the\nnormal and tangential direction), enabling synthesis of geometric textures,\nwhich cannot be expressed by a simple 2D displacement map. Learning and\nsynthesizing on local geometric patches enables a genus-oblivious framework,\nfacilitating texture transfer between shapes of different genus.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 19:36:38 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hertz", "Amir", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2007.00201", "submitter": "Przemyslaw Musialski", "authors": "Stefan Pillwein, Kurt Leimer, Michael Birsak, Przemyslaw Musialski", "title": "On Elastic Geodesic Grids and Their Planar to Spatial Deployment", "comments": "12 pages, 14 figures", "journal-ref": "ACM Transactions on Graphics (Proc. SIGGRAPH 2020), 39(4):1-12,\n  Jul. 2020", "doi": "10.1145/3386569.3392490", "report-no": null, "categories": "cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel type of planar-to-spatial deployable structures that we\ncall elastic geodesic grids. Our approach aims at the approximation of freeform\nsurfaces with spatial grids of bent lamellas which can be deployed from a\nplanar configuration using a simple kinematic mechanism. Such elastic\nstructures are easy-to-fabricate and easy-to-deploy and approximate shapes\nwhich combine physics and aesthetics. We propose a solution based on networks\nof geodesic curves on target surfaces and we introduce a set of conditions and\nassumptions which can be closely met in practice. Our formulation allows for a\npurely geometric approach which avoids the necessity of numerical shape\noptimization by building on top of theoretical insights from differential\ngeometry. We propose a solution for the design, computation, and physical\nsimulation of elastic geodesic grids, and present several fabricated\nsmall-scale examples with varying complexity. Moreover, we provide an empirical\nproof of our method by comparing the results to laser-scans of the fabricated\nmodels. Our method is intended as a form-finding tool for elastic gridshells in\narchitecture and other creative disciplines and should give the designer an\neasy-to-handle way for the exploration of such structures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 03:27:15 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Pillwein", "Stefan", ""], ["Leimer", "Kurt", ""], ["Birsak", "Michael", ""], ["Musialski", "Przemyslaw", ""]]}, {"id": "2007.00308", "submitter": "Mark J. Kilgard", "authors": "Mark J. Kilgard", "title": "Polar Stroking: New Theory and Methods for Stroking Paths", "comments": "15 pages, 19 figures, ACM Trans. on Graphics (Proceedings of SIGGRAPH\n  2020); corrected Fig. 8 and Eq. 6", "journal-ref": "ACM Transactions on Graphics, Vol. 39, No. 4 (2020) 145:1-15", "doi": "10.1145/3386569.3392458", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroking and filling are the two basic rendering operations on paths in\nvector graphics. The theory of filling a path is well-understood in terms of\ncontour integrals and winding numbers, but when path rendering standards\nspecify stroking, they resort to the analogy of painting pixels with a brush\nthat traces the outline of the path. This means important standards such as\nPDF, SVG, and PostScript lack a rigorous way to say what samples are inside or\noutside a stroked path. Our work fills this gap with a principled theory of\nstroking.\n  Guided by our theory, we develop a novel polar stroking method to render\nstroked paths robustly with an intuitive way to bound the tessellation error\nwithout needing recursion. Because polar stroking guarantees small uniform\nsteps in tangent angle, it provides an efficient way to accumulate arc length\nalong a path for texturing or dashing. While this paper focuses on developing\nthe theory of our polar stroking method, we have successfully implemented our\nmethods on modern programmable GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:03:09 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 18:45:28 GMT"}, {"version": "v3", "created": "Sat, 31 Oct 2020 04:12:57 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Kilgard", "Mark J.", ""]]}, {"id": "2007.00324", "submitter": "Zhenghai Chen", "authors": "Zhenghai Chen, Tiow-Seng Tan and Hong-Yang Ong", "title": "On Designing GPU Algorithms with Applications to Mesh Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of rules to guide the design of GPU algorithms. These rules\nare grounded on the principle of reducing waste in GPU utility to achieve good\nspeed up. In accordance to these rules, we propose GPU algorithms for 2D\nconstrained, 3D constrained and 3D Restricted Delaunay refinement problems\nrespectively. Our algorithms take a 2D planar straight line graph (PSLG) or 3D\npiecewise linear complex (PLC) $\\mathcal{G}$ as input, and generate quality\nmeshes conforming or approximating to $\\mathcal{G}$. The implementation of our\nalgorithms shows that they are the first to run an order of magnitude faster\nthan current state-of-the-art counterparts in sequential and parallel manners\nwhile using similar numbers of Steiner points to produce triangulations of\ncomparable qualities. It thus reduces the computing time of mesh refinement\nfrom possibly hours to a few seconds or minutes for possible use in interactive\ngraphics applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:43:12 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Zhenghai", ""], ["Tan", "Tiow-Seng", ""], ["Ong", "Hong-Yang", ""]]}, {"id": "2007.00413", "submitter": "Yamin Li", "authors": "Yamin Li, Kai Tang, Dong He, Xiangyu Wang", "title": "Multi-Axis Support-Free Printing of Freeform Parts with Lattice Infill\n  Structures", "comments": "arXiv admin note: text overlap with arXiv:2003.05938", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In additive manufacturing, infill structures are commonly used to reduce the\nweight and cost of a solid part. Currently, most infill structure generation\nmethods are based on the conventional 2.5-axis printing configuration, which,\nalthough able to satisfy the self-supporting condition on the infills, suffer\nfrom the well-known stair-case effect on the finished surface and the need of\nextensive support for overhang features. In this paper, based on the emerging\ncontinuous multi-axis printing configuration, we present a new lattice infill\nstructure generation algorithm, which is able to achieve both the\nself-supporting condition for the infills and the support-free requirement at\nthe boundary surface of the part. The algorithm critically relies on the use of\nthree mutually orthogonal geodesic distance fields that are embedded in the\ntetrahedral mesh of the solid model. The intersection between the iso-geodesic\ndistance surfaces of these three geodesic distance fields naturally forms the\ndesired lattice of infill structure, while the density of the infills can be\nconveniently controlled by adjusting the iso-values. The lattice infill pattern\nin each curved slicing layer is trimmed to conform to an Eulerian graph so to\ngenerate a continuous printing path, which can effectively reduce the nozzle\nretractions during the printing process. In addition, to cater to the\ncollision-free requirement and to improve the printing efficiency, we also\npropose a printing sequence optimization algorithm for determining a\ncollision-free order of printing of the connected lattice infills, which seeks\nto reduce the air-move length of the nozzle. Ample experiments in both computer\nsimulation and physical printing are performed, and the results give a\npreliminary confirmation of the advantages of our methodology.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 06:08:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Li", "Yamin", ""], ["Tang", "Kai", ""], ["He", "Dong", ""], ["Wang", "Xiangyu", ""]]}, {"id": "2007.00653", "submitter": "Taesung Park", "authors": "Taesung Park, Jun-Yan Zhu, Oliver Wang, Jingwan Lu, Eli Shechtman,\n  Alexei A. Efros, Richard Zhang", "title": "Swapping Autoencoder for Deep Image Manipulation", "comments": "NeurIPS 2020. Please visit https://taesung.me/SwappingAutoencoder/\n  for an introductory video. v2 mainly contains reorganization of the\n  Introduction and Broader Impact section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models have become increasingly effective at producing\nrealistic images from randomly sampled seeds, but using such models for\ncontrollable manipulation of existing images remains challenging. We propose\nthe Swapping Autoencoder, a deep model designed specifically for image\nmanipulation, rather than random sampling. The key idea is to encode an image\nwith two independent components and enforce that any swapped combination maps\nto a realistic image. In particular, we encourage the components to represent\nstructure and texture, by enforcing one component to encode co-occurrent patch\nstatistics across different parts of an image. As our method is trained with an\nencoder, finding the latent codes for a new input image becomes trivial, rather\nthan cumbersome. As a result, it can be used to manipulate real input images in\nvarious ways, including texture swapping, local and global editing, and latent\ncode vector arithmetic. Experiments on multiple datasets show that our model\nproduces better results and is substantially more efficient compared to recent\ngenerative models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:59:57 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:41:33 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Park", "Taesung", ""], ["Zhu", "Jun-Yan", ""], ["Wang", "Oliver", ""], ["Lu", "Jingwan", ""], ["Shechtman", "Eli", ""], ["Efros", "Alexei A.", ""], ["Zhang", "Richard", ""]]}, {"id": "2007.00842", "submitter": "Martin Skrodzki", "authors": "Sunil Kumar Yadav and Martin Skrodzki and Eric Zimmermann and Konrad\n  Polthier", "title": "Surface Denoising based on Normal Filtering in a Robust Statistics\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a surface acquisition process using 3D scanners, noise is inevitable\nand an important step in geometry processing is to remove these noise\ncomponents from these surfaces (given as points-set or triangulated mesh). The\nnoise-removal process (denoising) can be performed by filtering the surface\nnormals first and by adjusting the vertex positions according to filtered\nnormals afterwards. Therefore, in many available denoising algorithms, the\ncomputation of noise-free normals is a key factor. A variety of filters have\nbeen introduced for noise-removal from normals, with different focus points\nlike robustness against outliers or large amplitude of noise. Although these\nfilters are performing well in different aspects, a unified framework is\nmissing to establish the relation between them and to provide a theoretical\nanalysis beyond the performance of each method.\n  In this paper, we introduce such a framework to establish relations between a\nnumber of widely-used nonlinear filters for face normals in mesh denoising and\nvertex normals in point set denoising. We cover robust statistical estimation\nwith M-smoothers and their application to linear and non-linear normal\nfiltering. Although these methods originate in different mathematical theories\n- which include diffusion-, bilateral-, and directional curvature-based\nalgorithms - we demonstrate that all of them can be cast into a unified\nframework of robust statistics using robust error norms and their corresponding\ninfluence functions. This unification contributes to a better understanding of\nthe individual methods and their relations with each other. Furthermore, the\npresented framework provides a platform for new techniques to combine the\nadvantages of known filters and to compare them with available methods.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:31:24 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Yadav", "Sunil Kumar", ""], ["Skrodzki", "Martin", ""], ["Zimmermann", "Eric", ""], ["Polthier", "Konrad", ""]]}, {"id": "2007.00977", "submitter": "Dorien Herremans", "authors": "Kanish Garg, Ajeet kumar Singh, Dorien Herremans, Brejesh Lall", "title": "PerceptionGAN: Real-world Image Construction from Provided Text through\n  Perceptual Understanding", "comments": "Proceedings of IEEE International Conference on Imaging, Vision &\n  Pattern Recognition, (IVPR 2020, Japan)", "journal-ref": "Proceedings of IEEE International Conference on Imaging, Vision &\n  Pattern Recognition, (IVPR 2020, Japan)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating an image from a provided descriptive text is quite a challenging\ntask because of the difficulty in incorporating perceptual information (object\nshapes, colors, and their interactions) along with providing high relevancy\nrelated to the provided text. Current methods first generate an initial\nlow-resolution image, which typically has irregular object shapes, colors, and\ninteraction between objects. This initial image is then improved by\nconditioning on the text. However, these methods mainly address the problem of\nusing text representation efficiently in the refinement of the initially\ngenerated image, while the success of this refinement process depends heavily\non the quality of the initially generated image, as pointed out in the DM-GAN\npaper. Hence, we propose a method to provide good initialized images by\nincorporating perceptual understanding in the discriminator module. We improve\nthe perceptual information at the first stage itself, which results in\nsignificant improvement in the final generated image. In this paper, we have\napplied our approach to the novel StackGAN architecture. We then show that the\nperceptual information included in the initial image is improved while modeling\nimage distribution at multiple stages. Finally, we generated realistic\nmulti-colored images conditioned by text. These images have good quality along\nwith containing improved basic perceptual information. More importantly, the\nproposed method can be integrated into the pipeline of other state-of-the-art\ntext-based-image-generation models to generate initial low-resolution images.\nWe also worked on improving the refinement process in StackGAN by augmenting\nthe third stage of the generator-discriminator pair in the StackGAN\narchitecture. Our experimental analysis and comparison with the\nstate-of-the-art on a large but sparse dataset MS COCO further validate the\nusefulness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:23:08 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Garg", "Kanish", ""], ["Singh", "Ajeet kumar", ""], ["Herremans", "Dorien", ""], ["Lall", "Brejesh", ""]]}, {"id": "2007.00987", "submitter": "Moritz Geilinger", "authors": "Moritz Geilinger, David Hahn, Jonas Zehnder, Moritz B\\\"acher, Bernhard\n  Thomaszewski, Stelian Coros", "title": "ADD: Analytically Differentiable Dynamics for Multi-Body Systems with\n  Frictional Contact", "comments": "Moritz Geilinger and David Hahn contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a differentiable dynamics solver that is able to handle frictional\ncontact for rigid and deformable objects within a unified framework. Through a\nprincipled mollification of normal and tangential contact forces, our method\ncircumvents the main difficulties inherent to the non-smooth nature of\nfrictional contact. We combine this new contact model with fully-implicit time\nintegration to obtain a robust and efficient dynamics solver that is\nanalytically differentiable. In conjunction with adjoint sensitivity analysis,\nour formulation enables gradient-based optimization with adaptive trade-offs\nbetween simulation accuracy and smoothness of objective function landscapes. We\nthoroughly analyse our approach on a set of simulation examples involving rigid\nbodies, visco-elastic materials, and coupled multi-body systems. We furthermore\nshowcase applications of our differentiable simulator to parameter estimation\nfor deformable objects, motion planning for robotic manipulation, trajectory\noptimization for compliant walking robots, as well as efficient self-supervised\nlearning of control policies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:51:36 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Geilinger", "Moritz", ""], ["Hahn", "David", ""], ["Zehnder", "Jonas", ""], ["B\u00e4cher", "Moritz", ""], ["Thomaszewski", "Bernhard", ""], ["Coros", "Stelian", ""]]}, {"id": "2007.01481", "submitter": "Mark J. Kilgard", "authors": "Mark J. Kilgard", "title": "Ordinary Facet Angles of a Stroked Path Tessellated by Uniform Tangent\n  Angle Steps Are Bounded by Twice the Step Angle", "comments": "9 pages, supplemental paper for \"Polar Stroking: New Theory and\n  Methods for Stroking Paths\" (SIGGRAPH 2020) arXiv:2007.00308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain geometrically why ordinary facet angles of a stroked path\ntessellated from uniform tangent angle steps are bounded by twice the step\nangle. This fact means---excluding a small number of extraordinary facet angles\nstraddling offset cusps---our polar stroking method bounds the facet angle size\nto less than $2 \\theta$ where $\\theta$ is the tangent step angle.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:38:24 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kilgard", "Mark J.", ""]]}, {"id": "2007.01629", "submitter": "Daniel Preu{\\ss}", "authors": "Daniel Preu{\\ss}, Tino Weinkauf, and Jens Kr\\\"uger", "title": "A Discrete Probabilistic Approach to Dense Flow Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense flow visualization is a popular visualization paradigm. Traditionally,\nthe various models and methods in this area use a continuous formulation,\nresting upon the solid foundation of functional analysis. In this work, we\nexamine a discrete formulation of dense flow visualization. From probability\ntheory, we derive a similarity matrix that measures the similarity between\ndifferent points in the flow domain, leading to the discovery of a whole new\nclass of visualization models. Using this matrix, we propose a novel\nvisualization approach consisting of the computation of spectral embeddings,\ni.e., characteristic domain maps, defined by particle mixture probabilities.\nThese embeddings are scalar fields that give insight into the mixing processes\nof the flow on different scales. The approach of spectral embeddings is already\nwell studied in image segmentation, and we see that spectral embeddings are\nconnected to Fourier expansions and frequencies. We showcase the utility of our\nmethod using different 2D and 3D flows.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 11:45:51 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Preu\u00df", "Daniel", ""], ["Weinkauf", "Tino", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2007.01971", "submitter": "Ping Yu", "authors": "Ping Yu, Yang Zhao, Chunyuan Li, Junsong Yuan, Changyou Chen", "title": "Structure-Aware Human-Action Generation", "comments": "accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generating long-range skeleton-based human actions has been a challenging\nproblem since small deviations of one frame can cause a malformed action\nsequence. Most existing methods borrow ideas from video generation, which\nnaively treat skeleton nodes/joints as pixels of images without considering the\nrich inter-frame and intra-frame structure information, leading to potential\ndistorted actions. Graph convolutional networks (GCNs) is a promising way to\nleverage structure information to learn structure representations. However,\ndirectly adopting GCNs to tackle such continuous action sequences both in\nspatial and temporal spaces is challenging as the action graph could be huge.\nTo overcome this issue, we propose a variant of GCNs to leverage the powerful\nself-attention mechanism to adaptively sparsify a complete action graph in the\ntemporal space. Our method could dynamically attend to important past frames\nand construct a sparse graph to apply in the GCN framework, well-capturing the\nstructure information in action sequences. Extensive experimental results\ndemonstrate the superiority of our method on two standard human action datasets\ncompared with existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:18:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 21:31:44 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 20:05:43 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Ping", ""], ["Zhao", "Yang", ""], ["Li", "Chunyuan", ""], ["Yuan", "Junsong", ""], ["Chen", "Changyou", ""]]}, {"id": "2007.01975", "submitter": "Ricardo Bigolin Lanfredi", "authors": "Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga\n  Tasdizen", "title": "Interpretation of Disease Evidence for Medical Images Using Adversarial\n  Deformation Fields", "comments": "Accepted for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high complexity of deep learning models is associated with the difficulty\nof explaining what evidence they recognize as correlating with specific disease\nlabels. This information is critical for building trust in models and finding\ntheir biases. Until now, automated deep learning visualization solutions have\nidentified regions of images used by classifiers, but these solutions are too\ncoarse, too noisy, or have a limited representation of the way images can\nchange. We propose a novel method for formulating and presenting spatial\nexplanations of disease evidence, called deformation field interpretation with\ngenerative adversarial networks (DeFI-GAN). An adversarially trained generator\nproduces deformation fields that modify images of diseased patients to resemble\nimages of healthy patients. We validate the method studying chronic obstructive\npulmonary disease (COPD) evidence in chest x-rays (CXRs) and Alzheimer's\ndisease (AD) evidence in brain MRIs. When extracting disease evidence in\nlongitudinal data, we show compelling results against a baseline producing\ndifference maps. DeFI-GAN also highlights disease biomarkers not found by\nprevious methods and potential biases that may help in investigations of the\ndataset and of the adopted learning methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 00:51:54 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Lanfredi", "Ricardo Bigolin", ""], ["Schroeder", "Joyce D.", ""], ["Vachet", "Clement", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "2007.02072", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Pranay Gupta, Anirudh Thatipelli, Aditya Aggarwal, Shubh Maheshwari,\n  Neel Trivedi, Sourav Das, Ravi Kiran Sarvadevabhatla", "title": "Quo Vadis, Skeleton Action Recognition ?", "comments": "To appear in International Journal of Computer Vision (IJCV). Project\n  page: https://skeleton.iiit.ac.in/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study current and upcoming frontiers across the landscape\nof skeleton-based human action recognition. To study skeleton-action\nrecognition in the wild, we introduce Skeletics-152, a curated and 3-D\npose-annotated subset of RGB videos sourced from Kinetics-700, a large-scale\naction dataset. We extend our study to include out-of-context actions by\nintroducing Skeleton-Mimetics, a dataset derived from the recently introduced\nMimetics dataset. We also introduce Metaphorics, a dataset with caption-style\nannotated YouTube videos of the popular social game Dumb Charades and\ninterpretative dance performances. We benchmark state-of-the-art models on the\nNTU-120 dataset and provide multi-layered assessment of the results. The\nresults from benchmarking the top performers of NTU-120 on the newly introduced\ndatasets reveal the challenges and domain gap induced by actions in the wild.\nOverall, our work characterizes the strengths and limitations of existing\napproaches and datasets. Via the introduced datasets, our work enables new\nfrontiers for human action recognition.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 11:02:21 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 16:30:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Gupta", "Pranay", ""], ["Thatipelli", "Anirudh", ""], ["Aggarwal", "Aditya", ""], ["Maheshwari", "Shubh", ""], ["Trivedi", "Neel", ""], ["Das", "Sourav", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2007.02168", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, Ming C. Lin", "title": "Scalable Differentiable Physics for Learning and Control", "comments": null, "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning, ICML 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable physics is a powerful approach to learning and control\nproblems that involve physical objects and environments. While notable progress\nhas been made, the capabilities of differentiable physics solvers remain\nlimited. We develop a scalable framework for differentiable physics that can\nsupport a large number of objects and their interactions. To accommodate\nobjects with arbitrary geometry and topology, we adopt meshes as our\nrepresentation and leverage the sparsity of contacts for scalable\ndifferentiable collision handling. Collisions are resolved in localized regions\nto minimize the number of optimization variables even when the number of\nsimulated objects is high. We further accelerate implicit differentiation of\noptimization with nonlinear constraints. Experiments demonstrate that the\npresented framework requires up to two orders of magnitude less memory and\ncomputation in comparison to recent particle-based methods. We further validate\nthe approach on inverse problems and control scenarios, where it outperforms\nderivative-free and model-free baselines by at least an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 19:07:51 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Liang", "Junbang", ""], ["Koltun", "Vladlen", ""], ["Lin", "Ming C.", ""]]}, {"id": "2007.02245", "submitter": "Hao Xu", "authors": "Hao Xu and Ka-Hei Hui and Chi-Wing Fu and Hao Zhang", "title": "Computational LEGO Technic Design", "comments": "SIGGRAPH Asia 2019, Technical paper", "journal-ref": "ACM Trans. Graph., Vol. 38, No. 6, Article 196. Publication date:\n  November 2019", "doi": "10.1145/3355089.3356504", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to automatically compute LEGO Technic models from user\ninput sketches, optionally with motion annotations. The generated models\nresemble the input sketches with coherently-connected bricks and simple\nlayouts, while respecting the intended symmetry and mechanical properties\nexpressed in the inputs. This complex computational assembly problem involves\nan immense search space, and a much richer brick set and connection mechanisms\nthan regular LEGO. To address it, we first comprehensively model the brick\nproperties and connection mechanisms, then formulate the construction\nrequirements into an objective function, accounting for faithfulness to input\nsketch, model simplicity, and structural integrity. Next, we model the problem\nas a sketch cover, where we iteratively refine a random initial layout to cover\nthe input sketch, while guided by the objective. At last, we provide a working\nsystem to analyze the balance, stress, and assemblability of the generated\nmodel. To evaluate our method, we compared it with four baselines and\nprofessional designs by a LEGO expert, demonstrating the superiority of our\nautomatic designs. Also, we recruited several users to try our system, employed\nit to create models of varying forms and complexities, and physically built\nmost of them.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:59:26 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Xu", "Hao", ""], ["Hui", "Ka-Hei", ""], ["Fu", "Chi-Wing", ""], ["Zhang", "Hao", ""]]}, {"id": "2007.02278", "submitter": "Hao Xu", "authors": "Hao Xu and Ka Hei Hui and Chi-Wing Fu and Hao Zhang", "title": "TilinGNN: Learning to Tile with Self-Supervised Graph Neural Network", "comments": "SIGGRAPH 2020, Technical paper. ACM Trans. Graph., Vol. 39, No. 4,\n  Article 129. Homapage:\n  https://appsrv.cse.cuhk.edu.hk/~haoxu/projects/TilinGnn/index.html", "journal-ref": null, "doi": "10.1145/3386569.3392380", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first neural optimization framework to solve a classical\ninstance of the tiling problem. Namely, we seek a non-periodic tiling of an\narbitrary 2D shape using one or more types of tiles: the tiles maximally fill\nthe shape's interior without overlaps or holes. To start, we reformulate tiling\nas a graph problem by modeling candidate tile locations in the target shape as\ngraph nodes and connectivity between tile locations as edges. Further, we build\na graph convolutional neural network, coined TilinGNN, to progressively\npropagate and aggregate features over graph edges and predict tile placements.\nTilinGNN is trained by maximizing the tiling coverage on target shapes, while\navoiding overlaps and holes between the tiles. Importantly, our network is\nself-supervised, as we articulate these criteria as loss terms defined on the\nnetwork outputs, without the need of ground-truth tiling solutions. After\ntraining, the runtime of TilinGNN is roughly linear to the number of candidate\ntile locations, significantly outperforming traditional combinatorial search.\nWe conducted various experiments on a variety of shapes to showcase the speed\nand versatility of TilinGNN. We also present comparisons to alternative methods\nand manual solutions, robustness analysis, and ablation studies to demonstrate\nthe quality of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 10:06:06 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Xu", "Hao", ""], ["Hui", "Ka Hei", ""], ["Fu", "Chi-Wing", ""], ["Zhang", "Hao", ""]]}, {"id": "2007.02578", "submitter": "Diego Valsesia", "authors": "Francesca Pistilli, Giulia Fracastoro, Diego Valsesia, Enrico Magli", "title": "Learning Graph-Convolutional Representations for Point Cloud Denoising", "comments": "European Conference on Computer Vision (ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are an increasingly relevant data type but they are often\ncorrupted by noise. We propose a deep neural network based on\ngraph-convolutional layers that can elegantly deal with the\npermutation-invariance problem encountered by learning-based point cloud\nprocessing methods. The network is fully-convolutional and can build complex\nhierarchies of features by dynamically constructing neighborhood graphs from\nsimilarity among the high-dimensional feature representations of the points.\nWhen coupled with a loss promoting proximity to the ideal surface, the proposed\napproach significantly outperforms state-of-the-art methods on a variety of\nmetrics. In particular, it is able to improve in terms of Chamfer measure and\nof quality of the surface normals that can be estimated from the denoised data.\nWe also show that it is especially robust both at high noise levels and in\npresence of structured noise such as the one encountered in real LiDAR scans.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 08:11:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pistilli", "Francesca", ""], ["Fracastoro", "Giulia", ""], ["Valsesia", "Diego", ""], ["Magli", "Enrico", ""]]}, {"id": "2007.03059", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, George Drettakis and Adrien Bousseau", "title": "Guided Fine-Tuning for Large-Scale Material Transfer", "comments": "Published in Computer Graphics Forum, 39(4); Proceedings of the\n  Eurographics Symposium on Rendering 2020", "journal-ref": null, "doi": "10.1111/cgf.14056", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to transfer the appearance of one or a few exemplar\nSVBRDFs to a target image representing similar materials. Our solution is\nextremely simple: we fine-tune a deep appearance-capture network on the\nprovided exemplars, such that it learns to extract similar SVBRDF values from\nthe target image. We introduce two novel material capture and design workflows\nthat demonstrate the strength of this simple approach. Our first workflow\nallows to produce plausible SVBRDFs of large-scale objects from only a few\npictures. Specifically, users only need take a single picture of a large\nsurface and a few close-up flash pictures of some of its details. We use\nexisting methods to extract SVBRDF parameters from the close-ups, and our\nmethod to transfer these parameters to the entire surface, enabling the\nlightweight capture of surfaces several meters wide such as murals, floors and\nfurniture. In our second workflow, we provide a powerful way for users to\ncreate large SVBRDFs from internet pictures by transferring the appearance of\nexisting, pre-designed SVBRDFs. By selecting different exemplars, users can\ncontrol the materials assigned to the target image, greatly enhancing the\ncreative possibilities offered by deep appearance capture.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 20:55:37 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 17:12:21 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Drettakis", "George", ""], ["Bousseau", "Adrien", ""]]}, {"id": "2007.03483", "submitter": "Andreas B{\\ae}rentzen", "authors": "Andreas B{\\ae}rentzen and Eva Rotenberg", "title": "Skeletonization via Local Separators", "comments": "preprint, 25 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for curve skeleton computation which differs from\nprevious algorithms by being based on the notion of local separators. The main\nbenefits of this approach are that it is able to capture relatively fine\ndetails and that it works robustly on a range of shape representations.\nSpecifically, our method works on shape representations that can be construed\nas a spatially embedded graphs. Such representations include meshes, volumetric\nshapes, and graphs computed from point clouds. We describe a simple pipeline\nwhere geometric data is initially converted to a graph, optionally simplified,\nlocal separators are computed and selected, and finally a skeleton is\nconstructed. We test our pipeline on polygonal meshes, volumetric shapes, and\npoint clouds. Finally, we compare our results to other methods for\nskeletonization according to performance and quality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:15:26 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 20:50:47 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["B\u00e6rentzen", "Andreas", ""], ["Rotenberg", "Eva", ""]]}, {"id": "2007.03780", "submitter": "Anpei Chen", "authors": "Anpei Chen, Ruiyang Liu, Ling Xie, Jingyi Yu", "title": "A Free Viewpoint Portrait Generator with Dynamic Styling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating portrait images from a single latent space facing the problem of\nentangled attributes, making it difficult to explicitly adjust the generation\non specific attributes, e.g., contour and viewpoint control or dynamic styling.\nTherefore, we propose to decompose the generation space into two subspaces:\ngeometric and texture space. We first encode portrait scans with a semantic\noccupancy field (SOF), which represents semantic-embedded geometry structure\nand output free-viewpoint semantic segmentation maps. Then we design a semantic\ninstance wised(SIW) StyleGAN to regionally styling the segmentation map. We\ncapture 664 3D portrait scans for our SOF training and use real capture\nphotos(FFHQ and CelebA-HQ) for SIW StyleGAN training. Adequate experiments show\nthat our representations enable appearance consistent shape, pose, regional\nstyles controlling, achieve state-of-the-art results, and generalize well in\nvarious application scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:28:47 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Chen", "Anpei", ""], ["Liu", "Ruiyang", ""], ["Xie", "Ling", ""], ["Yu", "Jingyi", ""]]}, {"id": "2007.04464", "submitter": "Manos Kamarianakis", "authors": "Manos Kamarianakis and George Papagiannakis", "title": "Deform, Cut and Tear a skinned model using Conformal Geometric Algebra", "comments": "12 Pages, 4 figures, accepted in CGI 2020 conference (ENGAGE\n  workshop), will appear in Springer LNCS proceedings; updated based on the\n  comments of the reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel, integrated rigged character simulation\nframework in Conformal Geometric Algebra (CGA) that supports, for the first\ntime, real-time cuts and tears, before and/or after the animation, while\nmaintaining deformation topology. The purpose of using CGA is to lift several\nrestrictions posed by current state-of-the-art character animation &\ndeformation methods. Previous implementations originally required weighted\nmatrices to perform deformations, whereas, in the current state-of-the-art,\ndual-quaternions handle both rotations and translations, but cannot handle\ndilations. CGA is a suitable extension of dual-quaternion algebra that amends\nthese two major previous shortcomings: the need to constantly transmute between\nmatrices and dual-quaternions as well as the inability to properly dilate a\nmodel during animation. Our CGA algorithm also provides easy interpolation and\napplication of all deformations in each intermediate steps, all within the same\ngeometric framework. Furthermore we also present two novel algorithms that\nenable cutting and tearing of the input rigged, animated model, while the\noutput model can be further re-deformed. These interactive, real-time cut and\ntear operations can enable a new suite of applications, especially under the\nscope of a medical surgical simulation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:45:15 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 16:57:01 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Kamarianakis", "Manos", ""], ["Papagiannakis", "George", ""]]}, {"id": "2007.04954", "submitter": "Chuang Gan", "authors": "Chuang Gan, Jeremy Schwartz, Seth Alter, Martin Schrimpf, James Traer,\n  Julian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi\n  Sano, Kuno Kim, Elias Wang, Damian Mrowca, Michael Lingelbach, Aidan Curtis,\n  Kevin Feigelis, Daniel M. Bear, Dan Gutfreund, David Cox, James J. DiCarlo,\n  Josh McDermott, Joshua B. Tenenbaum, Daniel L.K. Yamins", "title": "ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation", "comments": "Project page: http://www.threedworld.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ThreeDWorld (TDW), a platform for interactive multi-modal\nphysical simulation. With TDW, users can simulate high-fidelity sensory data\nand physical interactions between mobile agents and objects in a wide variety\nof rich 3D environments. TDW has several unique properties: 1) realtime near\nphoto-realistic image rendering quality; 2) a library of objects and\nenvironments with materials for high-quality rendering, and routines enabling\nuser customization of the asset library; 3) generative procedures for\nefficiently building classes of new environments 4) high-fidelity audio\nrendering; 5) believable and realistic physical interactions for a wide variety\nof material types, including cloths, liquid, and deformable objects; 6) a range\nof \"avatar\" types that serve as embodiments of AI agents, with the option for\nuser avatar customization; and 7) support for human interactions with VR\ndevices. TDW also provides a rich API enabling multiple agents to interact\nwithin a simulation and return a range of sensor and physics data representing\nthe state of the world. We present initial experiments enabled by the platform\naround emerging research directions in computer vision, machine learning, and\ncognitive science, including multi-modal physical scene understanding,\nmulti-agent interactions, models that \"learn like a child\", and attention\nstudies in humans and neural networks. The simulation platform will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:33:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gan", "Chuang", ""], ["Schwartz", "Jeremy", ""], ["Alter", "Seth", ""], ["Schrimpf", "Martin", ""], ["Traer", "James", ""], ["De Freitas", "Julian", ""], ["Kubilius", "Jonas", ""], ["Bhandwaldar", "Abhishek", ""], ["Haber", "Nick", ""], ["Sano", "Megumi", ""], ["Kim", "Kuno", ""], ["Wang", "Elias", ""], ["Mrowca", "Damian", ""], ["Lingelbach", "Michael", ""], ["Curtis", "Aidan", ""], ["Feigelis", "Kevin", ""], ["Bear", "Daniel M.", ""], ["Gutfreund", "Dan", ""], ["Cox", "David", ""], ["DiCarlo", "James J.", ""], ["McDermott", "Josh", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "2007.05661", "submitter": "Xuequan Lu", "authors": "Dongbo Zhang, Zheng Fang, Xuequan Lu, Hong Qin, Antonio Robles-Kelly,\n  Chao Zhang, Ying He", "title": "Deep Patch-based Human Segmentation", "comments": "submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human segmentation has seen noticeable progress in re-cent years. It,\nhowever, still remains a challenge to date. In this paper, weintroduce a deep\npatch-based method for 3D human segmentation. Wefirst extract a local surface\npatch for each vertex and then parameterizeit into a 2D grid (or image). We\nthen embed identified shape descriptorsinto the 2D grids which are further fed\ninto the powerful 2D Convolu-tional Neural Network for regressing corresponding\nsemantic labels (e.g.,head, torso). Experiments demonstrate that our method is\neffective inhuman segmentation, and achieves state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 01:51:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhang", "Dongbo", ""], ["Fang", "Zheng", ""], ["Lu", "Xuequan", ""], ["Qin", "Hong", ""], ["Robles-Kelly", "Antonio", ""], ["Zhang", "Chao", ""], ["He", "Ying", ""]]}, {"id": "2007.06237", "submitter": "Madison Elliott", "authors": "Rebecca Vandenberg, Madison Elliott, Nicholas Harvey, and Tamara\n  Munzner", "title": "LSQT: Low-Stretch Quasi-Trees for Bundling and Layout", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce low-stretch trees to the visualization community with LSQT, our\nnovel technique that uses quasi-trees for both layout and edge bundling. Our\nmethod offers strong computational speed and complexity guarantees by\nleveraging the convenient properties of low-stretch trees, which accurately\nreflect the topological structure of arbitrary graphs with superior fidelity\ncompared to arbitrary spanning trees. Low-stretch quasi-trees also have\nprovable sparseness guarantees, providing algorithmic support for aggressive\nde-cluttering of hairball graphs. LSQT does not rely on previously computed\nvertex positions and computes bundles based on topological structure before any\ngeometric layout occurs. Edge bundles are computed efficiently and stored in an\nexplicit data structure that supports sophisticated visual encoding and\ninteraction techniques, including dynamic layout adjustment and interactive\nbundle querying. Our unoptimized implementation handles graphs of over 100,000\nedges in eight seconds, providing substantially higher performance than\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:39:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Vandenberg", "Rebecca", ""], ["Elliott", "Madison", ""], ["Harvey", "Nicholas", ""], ["Munzner", "Tamara", ""]]}, {"id": "2007.07177", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Stephanie Fu, Mindren Lu, Johnny Bui, Darius Bopp,\n  Zhenbang Chen, Felix Tran, Margaret Wang, Marina Rogers, Lei Zhang, Chris\n  Hoder, William T. Freeman", "title": "MosAIc: Finding Artistic Connections across Culture with Conditional\n  Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MosAIc, an interactive web app that allows users to find pairs\nof semantically related artworks that span different cultures, media, and\nmillennia. To create this application, we introduce Conditional Image Retrieval\n(CIR) which combines visual similarity search with user supplied filters or\n\"conditions\". This technique allows one to find pairs of similar images that\nspan distinct subsets of the image corpus. We provide a generic way to adapt\nexisting image retrieval data-structures to this new domain and provide\ntheoretical bounds on our approach's efficiency. To quantify the performance of\nCIR systems, we introduce new datasets for evaluating CIR methods and show that\nCIR performs non-parametric style transfer. Finally, we demonstrate that our\nCIR data-structures can identify \"blind spots\" in Generative Adversarial\nNetworks (GAN) where they fail to properly model the true data distribution.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:50:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 18:25:23 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 01:08:22 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hamilton", "Mark", ""], ["Fu", "Stephanie", ""], ["Lu", "Mindren", ""], ["Bui", "Johnny", ""], ["Bopp", "Darius", ""], ["Chen", "Zhenbang", ""], ["Tran", "Felix", ""], ["Wang", "Margaret", ""], ["Rogers", "Marina", ""], ["Zhang", "Lei", ""], ["Hoder", "Chris", ""], ["Freeman", "William T.", ""]]}, {"id": "2007.07243", "submitter": "Guilin Liu", "authors": "Guilin Liu, Rohan Taori, Ting-Chun Wang, Zhiding Yu, Shiqiu Liu,\n  Fitsum A. Reda, Karan Sapra, Andrew Tao, Bryan Catanzaro", "title": "Transposer: Universal Texture Synthesis Using Feature Maps as Transposed\n  Convolution Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional CNNs for texture synthesis consist of a sequence of\n(de)-convolution and up/down-sampling layers, where each layer operates locally\nand lacks the ability to capture the long-term structural dependency required\nby texture synthesis. Thus, they often simply enlarge the input texture, rather\nthan perform reasonable synthesis. As a compromise, many recent methods\nsacrifice generalizability by training and testing on the same single (or fixed\nset of) texture image(s), resulting in huge re-training time costs for unseen\nimages. In this work, based on the discovery that the assembling/stitching\noperation in traditional texture synthesis is analogous to a transposed\nconvolution operation, we propose a novel way of using transposed convolution\noperation. Specifically, we directly treat the whole encoded feature map of the\ninput texture as transposed convolution filters and the features'\nself-similarity map, which captures the auto-correlation information, as input\nto the transposed convolution. Such a design allows our framework, once\ntrained, to be generalizable to perform synthesis of unseen textures with a\nsingle forward pass in nearly real-time. Our method achieves state-of-the-art\ntexture synthesis quality based on various metrics. While self-similarity helps\npreserve the input textures' regular structural patterns, our framework can\nalso take random noise maps for irregular input textures instead of\nself-similarity maps as transposed convolution inputs. It allows to get more\ndiverse results as well as generate arbitrarily large texture outputs by\ndirectly sampling large noise maps in a single pass as well.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 17:57:59 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Liu", "Guilin", ""], ["Taori", "Rohan", ""], ["Wang", "Ting-Chun", ""], ["Yu", "Zhiding", ""], ["Liu", "Shiqiu", ""], ["Reda", "Fitsum A.", ""], ["Sapra", "Karan", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2007.07627", "submitter": "Bailin Deng", "authors": "Juyong Zhang and Yuxin Yao and Bailin Deng", "title": "Fast and Robust Iterative Closest Point", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2021", "doi": "10.1109/TPAMI.2021.3054619", "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iterative Closest Point (ICP) algorithm and its variants are a\nfundamental technique for rigid registration between two point sets, with wide\napplications in different areas from robotics to 3D reconstruction. The main\ndrawbacks for ICP are its slow convergence as well as its sensitivity to\noutliers, missing data, and partial overlaps. Recent work such as Sparse ICP\nachieves robustness via sparsity optimization at the cost of computational\nspeed. In this paper, we propose a new method for robust registration with fast\nconvergence. First, we show that the classical point-to-point ICP can be\ntreated as a majorization-minimization (MM) algorithm, and propose an Anderson\nacceleration approach to speed up its convergence. In addition, we introduce a\nrobust error metric based on the Welsch's function, which is minimized\nefficiently using the MM algorithm with Anderson acceleration. On challenging\ndatasets with noises and partial overlaps, we achieve similar or better\naccuracy than Sparse ICP while being at least an order of magnitude faster.\nFinally, we extend the robust formulation to point-to-plane ICP, and solve the\nresulting problem using a similar Anderson-accelerated MM strategy. Our robust\nICP methods improve the registration accuracy on benchmark datasets while being\ncompetitive in computational time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:32:53 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 14:24:22 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhang", "Juyong", ""], ["Yao", "Yuxin", ""], ["Deng", "Bailin", ""]]}, {"id": "2007.08211", "submitter": "Yichen Sheng", "authors": "Yichen Sheng, Jianming Zhang, Bedrich Benes", "title": "SSN: Soft Shadow Network for Image Compositing", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an interactive Soft Shadow Network (SSN) to generates\ncontrollable soft shadows for image compositing. SSN takes a 2D object mask as\ninput and thus is agnostic to image types such as painting and vector art. An\nenvironment light map is used to control the shadow's characteristics, such as\nangle and softness. SSN employs an Ambient Occlusion Prediction module to\npredict an intermediate ambient occlusion map, which can be further refined by\nthe user to provides geometric cues to modulate the shadow generation. To train\nour model, we design an efficient pipeline to produce diverse soft shadow\ntraining data using 3D object models. In addition, we propose an inverse shadow\nmap representation to improve model training. We demonstrate that our model\nproduces realistic soft shadows in real-time. Our user studies show that the\ngenerated shadows are often indistinguishable from shadows calculated by a\nphysics-based renderer and users can easily use SSN through an interactive\napplication to generate specific shadow effects in minutes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:36:39 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 11:28:47 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 19:14:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sheng", "Yichen", ""], ["Zhang", "Jianming", ""], ["Benes", "Bedrich", ""]]}, {"id": "2007.08457", "submitter": "Ning Yu", "authors": "Ning Yu, Vladislav Skripniuk, Sahar Abdelnabi, Mario Fritz", "title": "Artificial Fingerprinting for Generative Models: Rooting Deepfake\n  Attribution in Training Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic image generation has reached a new level of quality due to the\nbreakthroughs of generative adversarial networks (GANs). Yet, the dark side of\nsuch deepfakes, the malicious use of generated media, raises concerns about\nvisual misinformation. While existing research work on deepfake detection\ndemonstrates high accuracy, it is subject to advances in generation techniques\nand adversarial iterations on detection countermeasure techniques. Thus, we\nseek a proactive and sustainable solution on deepfake detection, that is\nagnostic to the evolution of generative models, by introducing artificial\nfingerprints into the models.\n  Our approach is simple and effective. We first embed artificial fingerprints\ninto training data, then validate a surprising discovery on the transferability\nof such fingerprints from training data to generative models, which in turn\nappears in the generated deepfakes. Experiments show that our fingerprinting\nsolution (1) holds for a variety of cutting-edge generative models, (2) leads\nto a negligible side effect on generation quality, (3) stays robust against\nimage-level and model-level perturbations, (4) stays hard to be detected by\nadversaries, and (5) converts deepfake detection and attribution into trivial\ntasks and outperforms the recent state-of-the-art baselines. Our solution\ncloses the responsibility loop between publishing pre-trained generative model\ninventions and their possible misuses, which makes it independent of the\ncurrent arms race.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:49:55 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 21:46:54 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 04:17:39 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2020 00:32:00 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 00:49:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Skripniuk", "Vladislav", ""], ["Abdelnabi", "Sahar", ""], ["Fritz", "Mario", ""]]}, {"id": "2007.08501", "submitter": "Georgia Gkioxari", "authors": "Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon,\n  Wan-Yen Lo, Justin Johnson, Georgia Gkioxari", "title": "Accelerating 3D Deep Learning with PyTorch3D", "comments": "tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly improved 2D image recognition. Extending into\n3D may advance many new applications including autonomous vehicles, virtual and\naugmented reality, authoring 3D content, and even improving 2D recognition.\nHowever despite growing interest, 3D deep learning remains relatively\nunderexplored. We believe that some of this disparity is due to the engineering\nchallenges involved in 3D deep learning, such as efficiently processing\nheterogeneous data and reframing graphics operations to be differentiable. We\naddress these challenges by introducing PyTorch3D, a library of modular,\nefficient, and differentiable operators for 3D deep learning. It includes a\nfast, modular differentiable renderer for meshes and point clouds, enabling\nanalysis-by-synthesis approaches. Compared with other differentiable renderers,\nPyTorch3D is more modular and efficient, allowing users to more easily extend\nit while also gracefully scaling to large meshes and images. We compare the\nPyTorch3D operators and renderer with other implementations and demonstrate\nsignificant speed and memory improvements. We also use PyTorch3D to improve the\nstate-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D\nimages on ShapeNet. PyTorch3D is open-source and we hope it will help\naccelerate research in 3D deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:53:02 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ravi", "Nikhila", ""], ["Reizenstein", "Jeremy", ""], ["Novotny", "David", ""], ["Gordon", "Taylor", ""], ["Lo", "Wan-Yen", ""], ["Johnson", "Justin", ""], ["Gkioxari", "Georgia", ""]]}, {"id": "2007.08547", "submitter": "Lele Chen", "authors": "Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou, Yi Xu, and\n  Chenliang Xu", "title": "Talking-head Generation with Rhythmic Head Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people deliver a speech, they naturally move heads, and this rhythmic\nhead motion conveys prosodic information. However, generating a lip-synced\nvideo while moving head naturally is challenging. While remarkably successful,\nexisting works either generate still talkingface videos or rely on\nlandmark/video frames as sparse/dense mapping guidance to generate head\nmovements, which leads to unrealistic or uncontrollable video synthesis. To\novercome the limitations, we propose a 3D-aware generative network along with a\nhybrid embedding module and a non-linear composition module. Through modeling\nthe head motion and facial expressions1 explicitly, manipulating 3D animation\ncarefully, and embedding reference images dynamically, our approach achieves\ncontrollable, photo-realistic, and temporally coherent talking-head videos with\nnatural head movements. Thoughtful experiments on several standard benchmarks\ndemonstrate that our method achieves significantly better results than the\nstate-of-the-art methods in both quantitative and qualitative comparisons. The\ncode is available on https://github.com/\nlelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:13:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chen", "Lele", ""], ["Cui", "Guofeng", ""], ["Liu", "Celong", ""], ["Li", "Zhong", ""], ["Kou", "Ziyi", ""], ["Xu", "Yi", ""], ["Xu", "Chenliang", ""]]}, {"id": "2007.09077", "submitter": "Siyu Huang", "authors": "Siyu Huang, Haoyi Xiong, Zhi-Qi Cheng, Qingzhong Wang, Xingran Zhou,\n  Bihan Wen, Jun Huan, Dejing Dou", "title": "Generating Person Images with Appearance-aware Pose Stylizer", "comments": "Appearing at IJCAI 2020. The code is available at\n  https://github.com/siyuhuang/PoseStylizer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generation of high-quality person images is challenging, due to the\nsophisticated entanglements among image factors, e.g., appearance, pose,\nforeground, background, local details, global structures, etc. In this paper,\nwe present a novel end-to-end framework to generate realistic person images\nbased on given person poses and appearances. The core of our framework is a\nnovel generator called Appearance-aware Pose Stylizer (APS) which generates\nhuman images by coupling the target pose with the conditioned person appearance\nprogressively. The framework is highly flexible and controllable by effectively\ndecoupling various complex person image factors in the encoding phase, followed\nby re-coupling them in the decoding phase. In addition, we present a new\nnormalization method named adaptive patch normalization, which enables\nregion-specific normalization and shows a good performance when adopted in\nperson image generation model. Experiments on two benchmark datasets show that\nour method is capable of generating visually appealing and realistic-looking\nresults using arbitrary image and pose inputs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 15:58:05 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Huang", "Siyu", ""], ["Xiong", "Haoyi", ""], ["Cheng", "Zhi-Qi", ""], ["Wang", "Qingzhong", ""], ["Zhou", "Xingran", ""], ["Wen", "Bihan", ""], ["Huan", "Jun", ""], ["Dou", "Dejing", ""]]}, {"id": "2007.09170", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Dai Hasegawa, Naoshi Kaneko, Gustav Eje Henter,\n  Hedvig Kjellstr\\\"om", "title": "Moving fast and slow: Analysis of representations and post-processing in\n  speech-driven automatic gesture generation", "comments": "Extension of our IVA'19 paper. Accepted at the International Journal\n  of Human-Computer Interaction. See more at\n  https://svito-zar.github.io/audio2gestures/. arXiv admin note: substantial\n  text overlap with arXiv:1903.03369", "journal-ref": "Int. J. Hum. Comput.Interact.(2021)", "doi": "10.1080/10447318.2021.1883883", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for speech-driven gesture production,\napplicable to virtual agents to enhance human-computer interaction.\nSpecifically, we extend recent deep-learning-based, data-driven methods for\nspeech-driven gesture generation by incorporating representation learning. Our\nmodel takes speech as input and produces gestures as output, in the form of a\nsequence of 3D coordinates. We provide an analysis of different representations\nfor the input (speech) and the output (motion) of the network by both objective\nand subjective evaluations. We also analyse the importance of smoothing of the\nproduced motion. Our results indicated that the proposed method improved on our\nbaseline in terms of objective measures. For example, it better captured the\nmotion dynamics and better matched the motion-speed distribution. Moreover, we\nperformed user studies on two different datasets. The studies confirmed that\nour proposed method is perceived as more natural than the baseline, although\nthe difference in the studies was eliminated by appropriate post-processing:\nhip-centering and smoothing. We conclude that it is important to take both\nmotion representation and post-processing into account when designing an\nautomatic gesture-production method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:32:00 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:30:30 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 12:49:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kucherenko", "Taras", ""], ["Hasegawa", "Dai", ""], ["Kaneko", "Naoshi", ""], ["Henter", "Gustav Eje", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2007.09267", "submitter": "Minghua Liu", "authors": "Minghua Liu, Xiaoshuai Zhang, Hao Su", "title": "Meshing Point Clouds with Predicted Intrinsic-Extrinsic Ratio Guidance", "comments": "ECCV 2020, code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in reconstructing the mesh representation of object\nsurfaces from point clouds. Surface reconstruction is a prerequisite for\ndownstream applications such as rendering, collision avoidance for planning,\nanimation, etc. However, the task is challenging if the input point cloud has a\nlow resolution, which is common in real-world scenarios (e.g., from LiDAR or\nKinect sensors). Existing learning-based mesh generative methods mostly predict\nthe surface by first building a shape embedding that is at the whole object\nlevel, a design that causes issues in generating fine-grained details and\ngeneralizing to unseen categories. Instead, we propose to leverage the input\npoint cloud as much as possible, by only adding connectivity information to\nexisting points. Particularly, we predict which triplets of points should form\nfaces. Our key innovation is a surrogate of local connectivity, calculated by\ncomparing the intrinsic/extrinsic metrics. We learn to predict this surrogate\nusing a deep point cloud network and then feed it to an efficient\npost-processing module for high-quality mesh generation. We demonstrate that\nour method can not only preserve details, handle ambiguous structures, but also\npossess strong generalizability to unseen categories by experiments on\nsynthetic and real data. The code is available at\nhttps://github.com/Colin97/Point2Mesh.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:36:00 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:49:28 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Liu", "Minghua", ""], ["Zhang", "Xiaoshuai", ""], ["Su", "Hao", ""]]}, {"id": "2007.09367", "submitter": "Eloise Berson", "authors": "Elo\\\"ise Berson, Catherine Soladi\\'e, Vincent Barrielle, Nicolas\n  Stoiber", "title": "A Robust Interactive Facial Animation Editing System", "comments": null, "journal-ref": "Motion, Interaction and Games (MIG '19), October 28--30, 2019,\n  Newcastle upon Tyne, United Kingdom", "doi": "10.1145/3359566.3360076", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, the automatic generation of facial animation for\nvirtual characters has garnered interest among the animation research and\nindustry communities. Recent research contributions leverage machine-learning\napproaches to enable impressive capabilities at generating plausible facial\nanimation from audio and/or video signals. However, these approaches do not\naddress the problem of animation edition, meaning the need for correcting an\nunsatisfactory baseline animation or modifying the animation content itself. In\nfacial animation pipelines, the process of editing an existing animation is\njust as important and time-consuming as producing a baseline. In this work, we\npropose a new learning-based approach to easily edit a facial animation from a\nset of intuitive control parameters. To cope with high-frequency components in\nfacial movements and preserve a temporal coherency in the animation, we use a\nresolution-preserving fully convolutional neural network that maps control\nparameters to blendshapes coefficients sequences. We stack an additional\nresolution-preserving animation autoencoder after the regressor to ensure that\nthe system outputs natural-looking animation. The proposed system is robust and\ncan handle coarse, exaggerated edits from non-specialist users. It also retains\nthe high-frequency motion of the facial animation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:31:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Berson", "Elo\u00efse", ""], ["Soladi\u00e9", "Catherine", ""], ["Barrielle", "Vincent", ""], ["Stoiber", "Nicolas", ""]]}, {"id": "2007.09740", "submitter": "Paul Zhang", "authors": "Paul Zhang, Josh Vekhter, Edward Chien, David Bommes, Etienne Vouga,\n  Justin Solomon", "title": "Octahedral Frames for Feature-Aligned Cross-Fields", "comments": "Code: https://github.com/pzpzpzp1/CreaseAlignedCrossFields", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for designing smooth cross fields on surfaces that\nautomatically align to sharp features of an underlying geometry. Our approach\nintroduces a novel class of energies based on a representation of cross fields\nin the spherical harmonic basis. We provide theoretical analysis of these\nenergies in the smooth setting, showing that they penalize deviations from\nsurface creases while otherwise promoting intrinsically smooth fields. We\ndemonstrate the applicability of our method to quad-meshing and include an\nextensive benchmark comparing our fields to other automatic approaches for\ngenerating feature-aligned cross fields on triangle meshes.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:28:09 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Zhang", "Paul", ""], ["Vekhter", "Josh", ""], ["Chien", "Edward", ""], ["Bommes", "David", ""], ["Vouga", "Etienne", ""], ["Solomon", "Justin", ""]]}, {"id": "2007.09892", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Kalyan Sunkavalli, Milo\\v{s} Ha\\v{s}an, Yannick\n  Hold-Geoffroy, David Kriegman, Ravi Ramamoorthi", "title": "Deep Reflectance Volumes: Relightable Reconstructions from Multi-View\n  Photometric Images", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach to reconstruct scene appearance from\nunstructured images captured under collocated point lighting. At the heart of\nDeep Reflectance Volumes is a novel volumetric scene representation consisting\nof opacity, surface normal and reflectance voxel grids. We present a novel\nphysically-based differentiable volume ray marching framework to render these\nscene volumes under arbitrary viewpoint and lighting. This allows us to\noptimize the scene volumes to minimize the error between their rendered images\nand the captured images. Our method is able to reconstruct real scenes with\nchallenging non-Lambertian reflectance and complex geometry with occlusions and\nshadowing. Moreover, it accurately generalizes to novel viewpoints and\nlighting, including non-collocated lighting, rendering photorealistic images\nthat are significantly better than state-of-the-art mesh-based methods. We also\nshow that our learned reflectance volumes are editable, allowing for modifying\nthe materials of the captured scenes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 05:38:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Sunkavalli", "Kalyan", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Hold-Geoffroy", "Yannick", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2007.10093", "submitter": "Sebastian Weiss", "authors": "Sebastian Weiss, Mustafa I\\c{s}{\\i}k, Justus Thies, R\\\"udiger\n  Westermann", "title": "Learning Adaptive Sampling and Reconstruction for Volume Visualization", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3039340", "report-no": null, "categories": "cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central challenge in data visualization is to understand which data samples\nare required to generate an image of a data set in which the relevant\ninformation is encoded. In this work, we make a first step towards answering\nthe question of whether an artificial neural network can predict where to\nsample the data with higher or lower density, by learning of correspondences\nbetween the data, the sampling patterns and the generated images. We introduce\na novel neural rendering pipeline, which is trained end-to-end to generate a\nsparse adaptive sampling structure from a given low-resolution input image, and\nreconstructs a high-resolution image from the sparse set of samples. For the\nfirst time, to the best of our knowledge, we demonstrate that the selection of\nstructures that are relevant for the final visual representation can be jointly\nlearned together with the reconstruction of this representation from these\nstructures. Therefore, we introduce differentiable sampling and reconstruction\nstages, which can leverage back-propagation based on supervised losses solely\non the final image. We shed light on the adaptive sampling patterns generated\nby the network pipeline and analyze its use for volume visualization including\nisosurface and direct volume rendering.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:36:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Weiss", "Sebastian", ""], ["I\u015f\u0131k", "Mustafa", ""], ["Thies", "Justus", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2007.10294", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed and Matthew Fisher and Noam Aigerman and Vladimir G.\n  Kim", "title": "Coupling Explicit and Implicit Surface Representations for Generative 3D\n  Modeling", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel neural architecture for representing 3D surfaces, which\nharnesses two complementary shape representations: (i) an explicit\nrepresentation via an atlas, i.e., embeddings of 2D domains into 3D; (ii) an\nimplicit-function representation, i.e., a scalar function over the 3D volume,\nwith its levels denoting surfaces. We make these two representations\nsynergistic by introducing novel consistency losses that ensure that the\nsurface created from the atlas aligns with the level-set of the implicit\nfunction. Our hybrid architecture outputs results which are superior to the\noutput of the two equivalent single-representation networks, yielding smoother\nexplicit surfaces with more accurate normals, and a more accurate implicit\noccupancy function. Additionally, our surface reconstruction step can directly\nleverage the explicit atlas-based representation. This process is\ncomputationally efficient, and can be directly used by differentiable\nrasterizers, enabling training our hybrid representation with image-based\nlosses.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:24:51 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 02:10:58 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poursaeed", "Omid", ""], ["Fisher", "Matthew", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir G.", ""]]}, {"id": "2007.10430", "submitter": "Enrico Puppo", "authors": "Keenan Crane, Marco Livesu, Enrico Puppo, Yipeng Qin", "title": "A Survey of Algorithms for Geodesic Paths and Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Numerical computation of shortest paths or geodesics on curved domains, as\nwell as the associated geodesic distance, arises in a broad range of\napplications across digital geometry processing, scientific computing, computer\ngraphics, and computer vision. Relative to Euclidean distance computation,\nthese tasks are complicated by the influence of curvature on the behavior of\nshortest paths, as well as the fact that the representation of the domain may\nitself be approximate. In spite of the difficulty of this problem, recent\nliterature has developed a wide variety of sophisticated methods that enable\nrapid queries of geodesic information, even on relatively large models. This\nsurvey reviews the major categories of approaches to the computation of\ngeodesic paths and distances, highlighting common themes and opportunities for\nfuture improvement.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 19:41:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Crane", "Keenan", ""], ["Livesu", "Marco", ""], ["Puppo", "Enrico", ""], ["Qin", "Yipeng", ""]]}, {"id": "2007.10701", "submitter": "Man M. Ho", "authors": "Man M. Ho, Jinjia Zhou", "title": "Deep Preset: Blending and Retouching Photos with Color Style Transfer", "comments": "Revised and Accepted to WACV'2021. Our work is available at\n  https://minhmanho.github.io/deep_preset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-users, without knowledge in photography, desire to beautify their photos\nto have a similar color style as a well-retouched reference. However, the\ndefinition of style in recent image style transfer works is inappropriate. They\nusually synthesize undesirable results due to transferring exact colors to the\nwrong destination. It becomes even worse in sensitive cases such as portraits.\nIn this work, we concentrate on learning low-level image transformation,\nespecially color-shifting methods, rather than mixing contextual features, then\npresent a novel scheme to train color style transfer with ground-truth.\nFurthermore, we propose a color style transfer named Deep Preset. It is\ndesigned to 1) generalize the features representing the color transformation\nfrom content with natural colors to retouched reference, then blend it into the\ncontextual features of content, 2) predict hyper-parameters (settings or\npreset) of the applied low-level color transformation methods, 3) stylize\ncontent to have a similar color style as reference. We script Lightroom, a\npowerful tool in editing photos, to generate 600,000 training samples using\n1,200 images from the Flick2K dataset and 500 user-generated presets with 69\nsettings. Experimental results show that our Deep Preset outperforms the\nprevious works in color style transfer quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 10:41:03 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 10:53:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ho", "Man M.", ""], ["Zhou", "Jinjia", ""]]}, {"id": "2007.10918", "submitter": "G Nazzaro", "authors": "Giacomo Nazzaro, Enrico Puppo, Fabio Pellacini", "title": "DecoSurf: Recursive Geodesic Patterns on Triangle Meshes", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that many complex patterns, which characterize the\ndecorative style of many artisanal objects, can be generated by the recursive\napplication of only four operators. Each operator is derived from tracing the\nisolines or the integral curves of geodesics fields generated from selected\nseeds on the surface. Based on this formulation, we present an interactive\napplication that lets designers model complex recursive patterns directly on\nthe object surface, without relying on parametrization. We support interaction\non commodity hardware on meshes of a few million triangles, by combining light\ndata structures together with an efficient approximate graph-based geodesic\nsolver. We validate our approach by matching decoration styles from real-world\nphotos, by analyzing the speed and accuracy of our geodesic solver, and by\nvalidating the interface with a user study.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 16:23:10 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Nazzaro", "Giacomo", ""], ["Puppo", "Enrico", ""], ["Pellacini", "Fabio", ""]]}, {"id": "2007.11250", "submitter": "Wei Xiang", "authors": "Wei Xiang, Xinran Yao, He Wang, Xiaogang Jin", "title": "FASTSWARM: A Data-driven FrAmework for Real-time Flying InSecT SWARM\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insect swarms are common phenomena in nature and therefore have been actively\npursued in computer animation. Realistic insect swarm simulation is difficult\ndue to two challenges: high-fidelity behaviors and large scales, which make the\nsimulation practice subject to laborious manual work and excessive\ntrial-and-error processes. To address both challenges, we present a novel\ndata-driven framework, FASTSWARM, to model complex behaviors of flying insects\nbased on real-world data and simulate plausible animations of flying insect\nswarms. FASTSWARM has a linear time complexity and achieves real-time\nperformance for large swarms. The high-fidelity behavior model of FASTSWARM\nexplicitly takes into consideration the most common behaviors of flying\ninsects, including the interactions among insects such as repulsion and\nattraction, the self-propelled behaviors such as target following and obstacle\navoidance, and other characteristics such as the random movements. To achieve\nscalability, an energy minimization problem is formed with different behaviors\nmodelled as energy terms, where the minimizer is the desired behavior. The\nminimizer is computed from the real-world data, which ensures the plausibility\nof the simulation results. Extensive simulation results and evaluations show\nthat FASTSWARM is versatile in simulating various swarm behaviors, high\nfidelity measured by various metrics, easily controllable in inducing user\ncontrols and highly scalable.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 08:08:23 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Xiang", "Wei", ""], ["Yao", "Xinran", ""], ["Wang", "He", ""], ["Jin", "Xiaogang", ""]]}, {"id": "2007.11512", "submitter": "Thomas Ortner MMSc", "authors": "Thomas Ortner, Andreas Walch, Rebecca Nowak, Robert Barnes, Thomas\n  H\\\"ollt, Eduard Gr\\\"oller", "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of\nancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022\nRosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in\nseeking signs of past life on Mars. Geologists measure and interpret 3D DOMs,\ncreate sedimentary logs and combine them in `correlation panels' to map the\nextents of key geological horizons, and build a stratigraphic model to\nunderstand their position in the ancient landscape. Currently, the creation of\ncorrelation panels is completely manual and therefore time-consuming, and\ninflexible. With InCorr we present a visualization solution that encompasses a\n3D logging tool and an interactive data-driven correlation panel that evolves\nwith the stratigraphic analysis. For the creation of InCorr we closely\ncooperated with leading planetary geologists in the form of a design study. We\nverify our results by recreating an existing correlation analysis with InCorr\nand validate our correlation panel against a manually created illustration.\nFurther, we conducted a user-study with a wider circle of geologists. Our\nevaluation shows that InCorr efficiently supports the domain experts in\ntackling their research questions and that it has the potential to\nsignificantly impact how geologists work with digital outcrop representations\nin general.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:08:31 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 11:10:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ortner", "Thomas", ""], ["Walch", "Andreas", ""], ["Nowak", "Rebecca", ""], ["Barnes", "Robert", ""], ["H\u00f6llt", "Thomas", ""], ["Gr\u00f6ller", "Eduard", ""]]}, {"id": "2007.11571", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, Christian\n  Theobalt", "title": "Neural Sparse Voxel Fields", "comments": "20 pages, in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic free-viewpoint rendering of real-world scenes using classical\ncomputer graphics techniques is challenging, because it requires the difficult\nstep of capturing detailed appearance and geometry models. Recent studies have\ndemonstrated promising results by learning scene representations that\nimplicitly encode both geometry and appearance without 3D supervision. However,\nexisting approaches in practice often show blurry renderings caused by the\nlimited network capacity or the difficulty in finding accurate intersections of\ncamera rays with the scene geometry. Synthesizing high-resolution imagery from\nthese representations often requires time-consuming optical ray marching. In\nthis work, we introduce Neural Sparse Voxel Fields (NSVF), a new neural scene\nrepresentation for fast and high-quality free-viewpoint rendering. NSVF defines\na set of voxel-bounded implicit fields organized in a sparse voxel octree to\nmodel local properties in each cell. We progressively learn the underlying\nvoxel structures with a differentiable ray-marching operation from only a set\nof posed RGB images. With the sparse voxel octree structure, rendering novel\nviews can be accelerated by skipping the voxels containing no relevant scene\ncontent. Our method is typically over 10 times faster than the state-of-the-art\n(namely, NeRF(Mildenhall et al., 2020)) at inference time while achieving\nhigher quality results. Furthermore, by utilizing an explicit sparse voxel\nrepresentation, our method can easily be applied to scene editing and scene\ncomposition. We also demonstrate several challenging tasks, including\nmulti-scene learning, free-viewpoint rendering of a moving human, and\nlarge-scale scene rendering. Code and data are available at our website:\nhttps://github.com/facebookresearch/NSVF.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 17:51:31 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 21:04:24 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Liu", "Lingjie", ""], ["Gu", "Jiatao", ""], ["Lin", "Kyaw Zaw", ""], ["Chua", "Tat-Seng", ""], ["Theobalt", "Christian", ""]]}, {"id": "2007.11632", "submitter": "Giuseppe Patan\\`e", "authors": "M. Kirgo, S. Melzi, G. Patan\\`e, E. Rodol\\`a, M. Ovsjanikov", "title": "Wavelet-based Heat Kernel Derivatives: Towards Informative Localized\n  Shape Analysis", "comments": "14 lages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new construction for the Mexican hat wavelets on\nshapes with applications to partial shape matching. Our approach takes its main\ninspiration from the well-established methodology of diffusion wavelets. This\nnovel construction allows us to rapidly compute a multiscale family of Mexican\nhat wavelet functions, by approximating the derivative of the heat kernel. We\ndemonstrate that it leads to a family of functions that inherit many attractive\nproperties of the heat kernel (e.g., a local support, ability to recover\nisometries from a single point, efficient computation). Due to its natural\nability to encode high-frequency details on a shape, the proposed method\nreconstructs and transfers $\\delta$-functions more accurately than the\nLaplace-Beltrami eigenfunction basis and other related bases. Finally, we apply\nour method to the challenging problems of partial and large-scale shape\nmatching. An extensive comparison to the state-of-the-art shows that it is\ncomparable in performance, while both simpler and much faster than competing\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 19:06:07 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 14:59:35 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 21:01:21 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Kirgo", "M.", ""], ["Melzi", "S.", ""], ["Patan\u00e8", "G.", ""], ["Rodol\u00e0", "E.", ""], ["Ovsjanikov", "M.", ""]]}, {"id": "2007.12117", "submitter": "Yuchen He", "authors": "Yuchen He, Sung Ha Kang, Jean-Michel Morel", "title": "Silhouette Vectorization by Affine Scale-space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Silhouettes or 2D planar shapes are extremely important in human\ncommunication, which involves many logos, graphics symbols and fonts in vector\nform. Many more shapes can be extracted from image by binarization or\nsegmentation, thus in raster form that requires a vectorization. There is a\nneed for disposing of a mathematically well defined and justified shape\nvectorization process, which in addition provides a minimal set of control\npoints with geometric meaning. In this paper we propose a silhouette\nvectorization method which extracts the outline of a 2D shape from a raster\nbinary image, and converts it to a combination of cubic B\\'{e}zier polygons and\nperfect circles. Starting from the boundary curvature extrema computed at\nsub-pixel level, we identify a set of control points based on the affine\nscale-space induced by the outline. These control points capture similarity\ninvariant geometric features of the given silhouette and give precise locations\nof the shape's corners.of the given silhouette. Then, piecewise B\\'{e}zier\ncubics are computed by least-square fitting combined with an adaptive splitting\nto guarantee a predefined accuracy. When there are no curvature extrema\nidentified, either the outline is recognized as a circle using the\nisoperimetric inequality, or a pair of the most distant outline points are\nchosen to initiate the fitting. Given their construction, most of our control\npoints are geometrically stable under affine transformations. By comparing with\nother feature detectors, we show that our method can be used as a reliable\nfeature point detector for silhouettes. Compared to state-of-the-art image\nvectorization software, our algorithm demonstrates superior reduction on the\nnumber of control points, while maintaining high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:42:45 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["He", "Yuchen", ""], ["Kang", "Sung Ha", ""], ["Morel", "Jean-Michel", ""]]}, {"id": "2007.12254", "submitter": "Mark J. Kilgard", "authors": "Mark J. Kilgard", "title": "Anecdotal Survey of Variations in Path Stroking among Real-world\n  Implementations", "comments": "14 pages, supplemental paper for \"Polar Stroking: New Theory and\n  Methods for Stroking Paths\" (SIGGRAPH 2020) arXiv:2007.00308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stroking a path is one of the two basic rendering operations in vector\ngraphics standards (e.g., PostScript, PDF, SVG). We survey path stroking\nrendering results from real-world software implementations of path stroking for\nanecdotal evidence that such implementations are prone to rendering variances.\nWhile our survey is limited and informal, the rendering results we gathered\nindicate widespread rendering variations for simple-but-problematic stroked\npaths first identified decades ago. We conclude that creators of vector\ngraphics content would benefit from a mathematically grounded standardization\nfor how a stroked path should be rasterized.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 21:01:57 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kilgard", "Mark J.", ""]]}, {"id": "2007.13344", "submitter": "Jinxian Liu", "authors": "Jinxian Liu, Minghui Yu, Bingbing Ni and Ye Chen", "title": "Self-Prediction for Joint Instance and Semantic Segmentation of Point\n  Clouds", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel learning scheme named Self-Prediction for 3D instance and\nsemantic segmentation of point clouds. Distinct from most existing methods that\nfocus on designing convolutional operators, our method designs a new learning\nscheme to enhance point relation exploring for better segmentation. More\nspecifically, we divide a point cloud sample into two subsets and construct a\ncomplete graph based on their representations. Then we use label propagation\nalgorithm to predict labels of one subset when given labels of the other\nsubset. By training with this Self-Prediction task, the backbone network is\nconstrained to fully explore relational context/geometric/shape information and\nlearn more discriminative features for segmentation. Moreover, a general\nassociated framework equipped with our Self-Prediction scheme is designed for\nenhancing instance and semantic segmentation simultaneously, where instance and\nsemantic representations are combined to perform Self-Prediction. Through this\nway, instance and semantic segmentation are collaborated and mutually\nreinforced. Significant performance improvements on instance and semantic\nsegmentation compared with baseline are achieved on S3DIS and ShapeNet. Our\nmethod achieves state-of-the-art instance segmentation results on S3DIS and\ncomparable semantic segmentation results compared with state-of-the-arts on\nS3DIS and ShapeNet when we only take PointNet++ as the backbone network.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 07:58:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Jinxian", ""], ["Yu", "Minghui", ""], ["Ni", "Bingbing", ""], ["Chen", "Ye", ""]]}, {"id": "2007.13371", "submitter": "Lia Morra", "authors": "Lia Morra, Fabrizio Lamberti, F. Gabriele Prattic\\'o, Salvatore La\n  Rosa, Paolo Montuschi", "title": "Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving\n  Simulators in HMI Design", "comments": null, "journal-ref": "IEEE Transactions on Vehicular Technology, 68(10), pp.9438-9450,\n  2019", "doi": "10.1109/TVT.2019.2933601", "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The investigation of factors contributing at making humans trust Autonomous\nVehicles (AVs) will play a fundamental role in the adoption of such technology.\nThe user's ability to form a mental model of the AV, which is crucial to\nestablish trust, depends on effective user-vehicle communication; thus, the\nimportance of Human-Machine Interaction (HMI) is poised to increase. In this\nwork, we propose a methodology to validate the user experience in AVs based on\ncontinuous, objective information gathered from physiological signals, while\nthe user is immersed in a Virtual Reality-based driving simulation. We applied\nthis methodology to the design of a head-up display interface delivering visual\ncues about the vehicle' sensory and planning systems. Through this approach, we\nobtained qualitative and quantitative evidence that a complete picture of the\nvehicle's surrounding, despite the higher cognitive load, is conducive to a\nless stressful experience. Moreover, after having been exposed to a more\ninformative interface, users involved in the study were also more willing to\ntest a real AV. The proposed methodology could be extended by adjusting the\nsimulation environment, the HMI and/or the vehicle's Artificial Intelligence\nmodules to dig into other aspects of the user experience.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:42:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""], ["Prattic\u00f3", "F. Gabriele", ""], ["La Rosa", "Salvatore", ""], ["Montuschi", "Paolo", ""]]}, {"id": "2007.13601", "submitter": "Giuseppe Patan\\`e", "authors": "Giuseppe Patan\\`e", "title": "Continuous Fuzzy Transform as Integral Operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fuzzy transform is ubiquitous in different research fields and\napplications, such as image and data compression, data mining, knowledge\ndiscovery, and the analysis of linguistic expressions. As a generalisation of\nthe Fuzzy transform, we introduce the continuous Fuzzy transform and its\ninverse, as an integral operator induced by a kernel function. Through the\nrelation between membership functions and integral kernels, we show that the\nmain properties (e.g., continuity, symmetry) of the membership functions are\ninherited by the continuous Fuzzy transform. Then, the relation between the\ncontinuous Fuzzy transform and integral operators is used to introduce a\ndata-driven Fuzzy transform, which encodes intrinsic information (e.g.,\nstructure, geometry, sampling density) about the input data. In this way, we\navoid coarse fuzzy partitions, which group data into large clusters that do not\nadapt to their local behaviour, or a too dense fuzzy partition, which generally\nhas cells that are not covered by the data, thus being redundant and resulting\nin a higher computational cost. To this end, the data-driven membership\nfunctions are defined by properly filtering the spectrum of the\nLaplace-Beltrami operator associated with the input data. Finally, we introduce\nthe space of continuous Fuzzy transforms, which is useful for the comparison of\ndifferent continuous Fuzzy transforms and for their efficient computation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:34:12 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Patan\u00e8", "Giuseppe", ""]]}, {"id": "2007.13866", "submitter": "Bowen Wen", "authors": "Bowen Wen, Chaitanya Mitash, Baozhang Ren, Kostas E. Bekris", "title": "se(3)-TrackNet: Data-driven 6D Pose Tracking by Calibrating Image\n  Residuals in Synthetic Domains", "comments": null, "journal-ref": "International Conference on Intelligent Robots and Systems (IROS)\n  2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the 6D pose of objects in video sequences is important for robot\nmanipulation. This task, however, introduces multiple challenges: (i) robot\nmanipulation involves significant occlusions; (ii) data and annotations are\ntroublesome and difficult to collect for 6D poses, which complicates machine\nlearning solutions, and (iii) incremental error drift often accumulates in long\nterm tracking to necessitate re-initialization of the object's pose. This work\nproposes a data-driven optimization approach for long-term, 6D pose tracking.\nIt aims to identify the optimal relative pose given the current RGB-D\nobservation and a synthetic image conditioned on the previous best estimate and\nthe object's model. The key contribution in this context is a novel neural\nnetwork architecture, which appropriately disentangles the feature encoding to\nhelp reduce domain shift, and an effective 3D orientation representation via\nLie Algebra. Consequently, even when the network is trained only with synthetic\ndata can work effectively over real images. Comprehensive experiments over\nbenchmarks - existing ones as well as a new dataset with significant occlusions\nrelated to object manipulation - show that the proposed approach achieves\nconsistently robust estimates and outperforms alternatives, even though they\nhave been trained with real images. The approach is also the most\ncomputationally efficient among the alternatives and achieves a tracking\nfrequency of 90.9Hz.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:09:36 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Wen", "Bowen", ""], ["Mitash", "Chaitanya", ""], ["Ren", "Baozhang", ""], ["Bekris", "Kostas E.", ""]]}, {"id": "2007.13988", "submitter": "Kyle Olszewski", "authors": "Ruilong Li, Yuliang Xiu, Shunsuke Saito, Zeng Huang, Kyle Olszewski,\n  Hao Li", "title": "Monocular Real-Time Volumetric Performance Capture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first approach to volumetric performance capture and\nnovel-view rendering at real-time speed from monocular video, eliminating the\nneed for expensive multi-view systems or cumbersome pre-acquisition of a\npersonalized template model. Our system reconstructs a fully textured 3D human\nfrom each frame by leveraging Pixel-Aligned Implicit Function (PIFu). While\nPIFu achieves high-resolution reconstruction in a memory-efficient manner, its\ncomputationally expensive inference prevents us from deploying such a system\nfor real-time applications. To this end, we propose a novel hierarchical\nsurface localization algorithm and a direct rendering method without explicitly\nextracting surface meshes. By culling unnecessary regions for evaluation in a\ncoarse-to-fine manner, we successfully accelerate the reconstruction by two\norders of magnitude from the baseline without compromising the quality.\nFurthermore, we introduce an Online Hard Example Mining (OHEM) technique that\neffectively suppresses failure modes due to the rare occurrence of challenging\nexamples. We adaptively update the sampling probability of the training data\nbased on the current reconstruction accuracy, which effectively alleviates\nreconstruction artifacts. Our experiments and evaluations demonstrate the\nrobustness of our system to various challenging angles, illuminations, poses,\nand clothing styles. We also show that our approach compares favorably with the\nstate-of-the-art monocular performance capture. Our proposed approach removes\nthe need for multi-view studio settings and enables a consumer-accessible\nsolution for volumetric capture.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 04:45:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Ruilong", ""], ["Xiu", "Yuliang", ""], ["Saito", "Shunsuke", ""], ["Huang", "Zeng", ""], ["Olszewski", "Kyle", ""], ["Li", "Hao", ""]]}, {"id": "2007.14394", "submitter": "Jinkai Hu", "authors": "Jinkai Hu, Milo Yip, G. Elias Alonso, Shihao Gu, Xiangjun Tang,\n  Xiaogang Jin", "title": "Signed Distance Fields Dynamic Diffuse Global Illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Illumination (GI) is of utmost importance in the field of\nphoto-realistic rendering. However, its computation has always been very\ncomplex, especially diffuse GI. State of the art real-time GI methods have\nlimitations of different nature, such as light leaking, performance issues,\nspecial hardware requirements, noise corruption, bounce number limitations,\namong others. To overcome these limitations, we propose a novel approach of\ncomputing dynamic diffuse GI with a signed distance fields approximation of the\nscene and discretizing the space domain of the irradiance function. With this\napproach, we are able to estimate real-time diffuse GI for dynamic lighting and\ngeometry, without any precomputations and supporting multi-bounce GI, providing\ngood quality lighting and high performance at the same time. Our algorithm is\nalso able to achieve better scalability, and manage both large open scenes and\nindoor high-detailed scenes without being corrupted by noise.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:58:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Hu", "Jinkai", ""], ["Yip", "Milo", ""], ["Alonso", "G. Elias", ""], ["Gu", "Shihao", ""], ["Tang", "Xiangjun", ""], ["Jin", "Xiaogang", ""]]}, {"id": "2007.14456", "submitter": "Alex Gaudio", "authors": "Alex Gaudio and Asim Smailagic and Aur\\'elio Campilho", "title": "Enhancement of Retinal Fundus Images via Pixel Color Amplification", "comments": "Accepted to International Conference on Image Analysis and\n  Recognition, ICIAR 2020 ; // Published at\n  https://doi.org/10.1007/978-3-030-50516-5_26 ;// CODE, SLIDES, and an\n  expanded/modified 20 page version https://github.com/adgaudio/ietk-ret", "journal-ref": null, "doi": "10.1007/978-3-030-50516-5_26", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pixel color amplification theory and family of enhancement\nmethods to facilitate segmentation tasks on retinal images. Our novel\nre-interpretation of the image distortion model underlying dehazing theory\nshows how three existing priors commonly used by the dehazing community and a\nnovel fourth prior are related. We utilize the theory to develop a family of\nenhancement methods for retinal images, including novel methods for whole image\nbrightening and darkening. We show a novel derivation of the Unsharp Masking\nalgorithm. We evaluate the enhancement methods as a pre-processing step to a\nchallenging multi-task segmentation problem and show large increases in\nperformance on all tasks, with Dice score increases over a no-enhancement\nbaseline by as much as 0.491. We provide evidence that our enhancement\npreprocessing is useful for unbalanced and difficult data. We show that the\nenhancements can perform class balancing by composing them together.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:56:34 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Gaudio", "Alex", ""], ["Smailagic", "Asim", ""], ["Campilho", "Aur\u00e9lio", ""]]}, {"id": "2007.14766", "submitter": "Jules Vidal", "authors": "Jules Vidal, Pierre Guillou and Julien Tierny", "title": "A Progressive Approach to Scalar Field Topology", "comments": "Accepted to IEEE TVCG on February 17th, 2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3060500", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces progressive algorithms for the topological analysis of\nscalar data. Our approach is based on a hierarchical representation of the\ninput data and the fast identification of topologically invariant vertices,\nwhich are vertices that have no impact on the topological description of the\ndata and for which we show that no computation is required as they are\nintroduced in the hierarchy. This enables the definition of efficient\ncoarse-to-fine topological algorithms, which leverage fast update mechanisms\nfor ordinary vertices and avoid computation for the topologically invariant\nones. We demonstrate our approach with two examples of topological algorithms\n(critical point extraction and persistence diagram computation), which generate\ninterpretable outputs upon interruption requests and which progressively refine\nthem otherwise. Experiments on real-life datasets illustrate that our\nprogressive strategy, in addition to the continuous visual feedback it\nprovides, even improves run time performance with regard to non-progressive\nalgorithms and we describe further accelerations with shared-memory\nparallelism. We illustrate the utility of our approach in batch-mode and\ninteractive setups, where it respectively enables the control of the execution\ntime of complete topological pipelines as well as previews of the topological\nfeatures found in a dataset, with progressive updates delivered within\ninteractive times.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 12:08:18 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:37:19 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Vidal", "Jules", ""], ["Guillou", "Pierre", ""], ["Tierny", "Julien", ""]]}, {"id": "2007.15219", "submitter": "Harsh Bhatia", "authors": "Harsh Bhatia, Duong Hoang, Garrett Morrison, Will Usher, Valerio\n  Pascucci, Peer-Timo Bremer, Peter Lindstrom", "title": "AMM: Adaptive Multilinear Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Adaptive Multilinear Meshes (AMM), a new framework that\nsignificantly reduces the memory footprint compared to existing data\nstructures. AMM uses a hierarchy of cuboidal cells to create continuous,\npiecewise multilinear representation of uniformly sampled data. Furthermore,\nAMM can selectively relax or enforce constraints on conformity, continuity, and\ncoverage, creating a highly adaptive and flexible representation to support a\nwide range of use cases. AMM supports incremental updates in both spatial\nresolution and numerical precision establishing the first practical data\nstructure that can seamlessly explore the tradeoff between resolution and\nprecision. We use tensor products of linear B-spline wavelets to create an\nadaptive representation and illustrate the advantages of our framework. AMM\nprovides a simple interface for evaluating the function defined on the adaptive\nmesh, efficiently traversing the mesh, and manipulating the mesh, including\nincremental, partial updates. Our framework is easy to adopt for standard\nvisualization and analysis tasks. As an example, we provide a VTK interface,\nthrough efficient on-demand conversion, which can be used directly by\ncorresponding tools, such as VisIt, disseminating the advantages of faster\nprocessing and a smaller memory footprint to a wider audience. We demonstrate\nthe advantages of our approach for simplifying scalar-valued data for commonly\nused visualization and analysis tasks using incremental construction, according\nto mixed resolution and precision data streams.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:18:14 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bhatia", "Harsh", ""], ["Hoang", "Duong", ""], ["Morrison", "Garrett", ""], ["Usher", "Will", ""], ["Pascucci", "Valerio", ""], ["Bremer", "Peer-Timo", ""], ["Lindstrom", "Peter", ""]]}, {"id": "2007.15227", "submitter": "Yating Wei", "authors": "Wei Chen, Yating Wei, Zhiyong Wang, Shuyue Zhou, Bingru Lin, Zhiguang\n  Zhou", "title": "Federated Visualization: A Privacy-preserving Strategy for Decentralized\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel privacy preservation strategy for decentralized\nvisualization. The key idea is to imitate the flowchart of the federated\nlearning framework, and reformulate the visualization process within a\nfederated infrastructure. The federation of visualization is fulfilled by\nleveraging a shared global module that composes the encrypted externalizations\nof transformed visual features of data pieces in local modules. We design two\nimplementations of federated visualization: a prediction-based scheme, and a\nquery-based scheme. We demonstrate the effectiveness of our approach with a set\nof visual forms, and verify its robustness with evaluations. We report the\nvalue of federated visualization in real scenarios with an expert review.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:57:26 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chen", "Wei", ""], ["Wei", "Yating", ""], ["Wang", "Zhiyong", ""], ["Zhou", "Shuyue", ""], ["Lin", "Bingru", ""], ["Zhou", "Zhiguang", ""]]}, {"id": "2007.15242", "submitter": "Hwangpil Park", "authors": "Hwangpil Park, Ri Yu, Yoonsang Lee, Kyungho Lee and Jehee Lee", "title": "Understanding the Stability of Deep Control Policies for Biped\n  Locomotion", "comments": "11 pages, 12 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving stability and robustness is the primary goal of biped locomotion\ncontrol. Recently, deep reinforce learning (DRL) has attracted great attention\nas a general methodology for constructing biped control policies and\ndemonstrated significant improvements over the previous state-of-the-art.\nAlthough deep control policies have advantages over previous controller design\napproaches, many questions remain unanswered. Are deep control policies as\nrobust as human walking? Does simulated walking use similar strategies as human\nwalking to maintain balance? Does a particular gait pattern similarly affect\nhuman and simulated walking? What do deep policies learn to achieve improved\ngait stability? The goal of this study is to answer these questions by\nevaluating the push-recovery stability of deep policies compared to human\nsubjects and a previous feedback controller. We also conducted experiments to\nevaluate the effectiveness of variants of DRL algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 05:48:58 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Park", "Hwangpil", ""], ["Yu", "Ri", ""], ["Lee", "Yoonsang", ""], ["Lee", "Kyungho", ""], ["Lee", "Jehee", ""]]}, {"id": "2007.15272", "submitter": "Xumeng Wang", "authors": "Xumeng Wang, Wei Chen, Jiazhi Xia, Zexian Chen, Dongshi Xu, Xiangyang\n  Wu, Mingliang Xu and Tobias Schreck", "title": "ConceptExplorer: Visual Analysis of Concept Driftsin Multi-source\n  Time-series Data", "comments": "12 pages, 14 figures. Accepted by the IEEE VAST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data is widely studied in various scenarios, like weather\nforecast, stock market, customer behavior analysis. To comprehensively learn\nabout the dynamic environments, it is necessary to comprehend features from\nmultiple data sources. This paper proposes a novel visual analysis approach for\ndetecting and analyzing concept drifts from multi-sourced time-series. We\npropose a visual detection scheme for discovering concept drifts from multiple\nsourced time-series based on prediction models. We design a drift level index\nto depict the dynamics, and a consistency judgment model to justify whether the\nconcept drifts from various sources are consistent. Our integrated visual\ninterface, ConceptExplorer, facilitates visual exploration, extraction,\nunderstanding, and comparison of concepts and concept drifts from multi-source\ntime-series data. We conduct three case studies and expert interviews to verify\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:21:52 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:48:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Wang", "Xumeng", ""], ["Chen", "Wei", ""], ["Xia", "Jiazhi", ""], ["Chen", "Zexian", ""], ["Xu", "Dongshi", ""], ["Wu", "Xiangyang", ""], ["Xu", "Mingliang", ""], ["Schreck", "Tobias", ""]]}, {"id": "2007.15311", "submitter": "Minseok Kim", "authors": "Hoseok Ryu, Minseok Kim, Seunghwan Lee, Moon Seok Park, Kyoungmin Lee\n  and Jehee Lee", "title": "Functionality-Driven Musculature Retargeting", "comments": "15 pages, 20 figures", "journal-ref": "Computer Graphics Forum,40(2021)341-356", "doi": "10.1111/cgf.14191", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel retargeting algorithm that transfers the musculature of a\nreference anatomical model to new bodies with different sizes, body\nproportions, muscle capability, and joint range of motion while preserving the\nfunctionality of the original musculature as closely as possible. The geometric\nconfiguration and physiological parameters of musculotendon units are estimated\nand optimized to adapt to new bodies. The range of motion around joints is\nestimated from a motion capture dataset and edited further for individual\nmodels. The retargeted model is simulation-ready, so we can physically simulate\nmuscle-actuated motor skills with the model. Our system is capable of\ngenerating a wide variety of anatomical bodies that can be simulated to walk,\nrun, jump and dance while maintaining balance under gravity. We will also\ndemonstrate the construction of individualized musculoskeletal models from\nbi-planar X-ray images and medical examinations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:47:04 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:57:22 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 07:42:00 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ryu", "Hoseok", ""], ["Kim", "Minseok", ""], ["Lee", "Seunghwan", ""], ["Park", "Moon Seok", ""], ["Lee", "Kyoungmin", ""], ["Lee", "Jehee", ""]]}, {"id": "2007.15446", "submitter": "Alexander Kumpf", "authors": "Alexander Kumpf, Josef Stumpfegger, Patrick Fabian H\\\"artl, R\\\"udiger\n  Westermann", "title": "Visual Analysis of Multi-Parameter Distributions across Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an ensemble of data points in a multi-parameter space, we present a\nvisual analytics technique to select a representative distribution of parameter\nvalues, and analyse how representative this distribution is in all ensemble\nmembers. A multi-parameter cluster in a representative ensemble member is\nvisualized via a parallel coordinates plot, to provide initial distributions\nand let domain experts interactively select relevant parameters and value\nranges. Since unions of value ranges select hyper-cubes in parameter space,\ndata points in these unions are not necessarily contained in the cluster. By\nusing a multi-parameter kD-tree to further refine the selected parameter\nranges, in combination with a covariance analysis of refined sets of data\npoints, a tight partition in multi-parameter space with reduced number of\nfalsely selected points is obtained. To assess the representativeness of the\nselected multi-parameter distribution across the ensemble, a linked\nside-by-side view of per-member violin plots is provided. We propose\nmodifications of violin plots to show multi-parameter distributions\nsimultaneously, and investigate the visual design that effectively conveys\n(dis-)similarities in multi-parameter distributions. In a linked spatial view,\nusers can analyse and compare the spatial distribution of selected points in\ndifferent ensemble members via interval-based isosurface raycasting. In two\nreal-world application cases we show how our approach is used to analyse the\nmulti-parameter distributions across an ensemble of 3D fields.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 13:21:33 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kumpf", "Alexander", ""], ["Stumpfegger", "Josef", ""], ["H\u00e4rtl", "Patrick Fabian", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2007.15538", "submitter": "Filippo Gabriele Prattic\\`o", "authors": "F. Gabriele Prattic\\`o, Fabrizio Lamberti (Politecnico di Torino)", "title": "Mixed-Reality Robotic Games: Design Guidelines for Effective\n  Entertainment with Consumer Robots", "comments": "This paper is accepted for inclusion in future issue of IEEE Consumer\n  Electronic Magazine. Copyright IEEE 2020", "journal-ref": "IEEE Consumer Electronics Magazine, vol. 10, no. 1, pp. 6-16, Jan.\n  2021", "doi": "10.1109/MCE.2020.2988578", "report-no": null, "categories": "cs.HC cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in the use of robotic\ntechnology at home. A number of service robots appeared on the market,\nsupporting customers in the execution of everyday tasks. Roughly at the same\ntime, consumer level robots started to be used also as toys or gaming\ncompanions. However, gaming possibilities provided by current off-the-shelf\nrobotic products are generally quite limited, and this fact makes them quickly\nloose their attractiveness. A way that has been proven capable to boost robotic\ngaming and related devices consists in creating playful experiences in which\nphysical and digital elements are combined together using Mixed Reality\ntechnologies. However, these games differ significantly from digital- or\nphysical only experiences, and new design principles are required to support\ndevelopers in their creative work. This papers addresses such need, by drafting\na set of guidelines which summarize developments carried out by the research\ncommunity and their findings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:47:17 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Prattic\u00f2", "F. Gabriele", "", "Politecnico di Torino"], ["Lamberti", "Fabrizio", "", "Politecnico di Torino"]]}, {"id": "2007.15646", "submitter": "David Bau iii", "authors": "David Bau, Steven Liu, Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba", "title": "Rewriting a Deep Generative Model", "comments": "ECCV 2020 (oral). Code at https://github.com/davidbau/rewriting. For\n  videos and demos see https://rewriting.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model such as a GAN learns to model a rich set of semantic\nand physical rules about the target distribution, but up to now, it has been\nobscure how such rules are encoded in the network, or how a rule could be\nchanged. In this paper, we introduce a new problem setting: manipulation of\nspecific rules encoded by a deep generative model. To address the problem, we\npropose a formulation in which the desired rule is changed by manipulating a\nlayer of a deep network as a linear associative memory. We derive an algorithm\nfor modifying one entry of the associative memory, and we demonstrate that\nseveral interesting structural rules can be located and modified within the\nlayers of state-of-the-art generative models. We present a user interface to\nenable users to interactively change the rules of a generative model to achieve\ndesired effects, and we show several proof-of-concept applications. Finally,\nresults on multiple datasets demonstrate the advantage of our method against\nstandard fine-tuning methods and edit transfer algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:58:16 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Bau", "David", ""], ["Liu", "Steven", ""], ["Wang", "Tongzhou", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""]]}, {"id": "2007.15820", "submitter": "Ekim Yurtsever", "authors": "Ekim Yurtsever, Dongfang Yang, Ibrahim Mert Koc, Keith A. Redmill", "title": "Blending Generative Adversarial Image Synthesis with Rendering for\n  Computer Graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional computer graphics pipelines require detailed 3D models, meshes,\ntextures, and rendering engines to generate 2D images from 3D scenes. These\nprocesses are labor-intensive. We introduce Hybrid Neural Computer Graphics\n(HNCG) as an alternative. The contribution is a novel image formation strategy\nto reduce the 3D model and texture complexity of computer graphics pipelines.\nOur main idea is straightforward: Given a 3D scene, render only important\nobjects of interest and use generative adversarial processes for synthesizing\nthe rest of the image. To this end, we propose a novel image formation strategy\nto form 2D semantic images from 3D scenery consisting of simple object models\nwithout textures. These semantic images are then converted into photo-realistic\nRGB images with a state-of-the-art conditional Generative Adversarial Network\n(cGAN) based image synthesizer trained on real-world data. Meanwhile, objects\nof interest are rendered using a physics-based graphics engine. This is\nnecessary as we want to have full control over the appearance of objects of\ninterest. Finally, the partially-rendered and cGAN synthesized images are\nblended with a blending GAN. We show that the proposed framework outperforms\nconventional rendering with ablation and comparison studies. Semantic retention\nand Fr\\'echet Inception Distance (FID) measurements were used as the main\nperformance metrics.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:25:17 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Yurtsever", "Ekim", ""], ["Yang", "Dongfang", ""], ["Koc", "Ibrahim Mert", ""], ["Redmill", "Keith A.", ""]]}]