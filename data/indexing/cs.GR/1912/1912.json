[{"id": "1912.00321", "submitter": "Bo Li", "authors": "Bo Li, Jie Feng and Bingfeng Zhou", "title": "A SVBRDF Modeling Pipeline using Pixel Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pipeline for modeling spatially varying BRDFs (svBRDFs) of\nplanar materials which only requires a mobile phone for data acquisition. With\na minimum of two photos under the ambient and point light source, our pipeline\nproduces svBRDF parameters, a normal map and a tangent map for the material\nsample. The BRDF fitting is achieved via a pixel clustering strategy and an\noptimization based scheme. Our method is light-weight, easy-to-use and capable\nof producing high-quality BRDF textures.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 04:59:20 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Li", "Bo", ""], ["Feng", "Jie", ""], ["Zhou", "Bingfeng", ""]]}, {"id": "1912.00416", "submitter": "Keunhong Park", "authors": "Keunhong Park, Arsalan Mousavian, Yu Xiang, Dieter Fox", "title": "LatentFusion: End-to-End Differentiable Reconstruction and Rendering for\n  Unseen Object Pose Estimation", "comments": "CVPR 2020, Project Page:\n  https://keunhong.com/publications/latentfusion/ , Video:\n  https://youtu.be/tlzcq1KYXd8 , Code: https://github.com/NVlabs/latentfusion .\n  We have added experiments for LINEMOD and have updated the experiments on\n  MOPED. We've also added more technical and implementation details to the\n  methods section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current 6D object pose estimation methods usually require a 3D model for each\nobject. These methods also require additional training in order to incorporate\nnew objects. As a result, they are difficult to scale to a large number of\nobjects and cannot be directly applied to unseen objects.\n  We propose a novel framework for 6D pose estimation of unseen objects. We\npresent a network that reconstructs a latent 3D representation of an object\nusing a small number of reference views at inference time. Our network is able\nto render the latent 3D representation from arbitrary views. Using this neural\nrenderer, we directly optimize for pose given an input image. By training our\nnetwork with a large number of 3D shapes for reconstruction and rendering, our\nnetwork generalizes well to unseen objects. We present a new dataset for unseen\nobject pose estimation--MOPED. We evaluate the performance of our method for\nunseen object pose estimation on MOPED as well as the ModelNet and LINEMOD\ndatasets. Our method performs competitively to supervised methods that are\ntrained on those objects. Code and data is available at\nhttps://keunhong.com/publications/latentfusion/.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 14:32:58 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 07:03:48 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 02:56:30 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Park", "Keunhong", ""], ["Mousavian", "Arsalan", ""], ["Xiang", "Yu", ""], ["Fox", "Dieter", ""]]}, {"id": "1912.00739", "submitter": "Talha Bin Masood", "authors": "Talha Bin Masood and Ingrid Hotz", "title": "Continuous Histograms for Anisotropy of 2D Symmetric Piece-wise Linear\n  Tensor Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of contours of scalar fields plays an important role in\nvisualization. For example the contour tree and contour statistics can be used\nas a means for interaction and filtering or as signatures. In the context of\ntensor field analysis, such methods are also interesting for the analysis of\nderived scalar invariants. While there are standard algorithms to compute and\nanalyze contours, they are not directly applicable to tensor invariants when\nusing component-wise tensor interpolation. In this chapter we present an\naccurate derivation of the contour spectrum for invariants with quadratic\nbehavior computed from two-dimensional piece-wise linear tensor fields. For\nthis work, we are mostly motivated by a consistent treatment of the anisotropy\nfield, which plays an important role as stability measure for tensor field\ntopology. We show that it is possible to derive an analytical expression for\nthe distribution of the invariant values in this setting, which is exemplary\ngiven for the anisotropy in all details. Our derivation is based on a\ntopological sub-division of the mesh in triangles that exhibit a monotonic\nbehavior. This triangulation can also directly be used to compute the accurate\ncontour tree with standard algorithms. We compare the results to a na\\\"ive\napproach based on linear interpolation on the original mesh or the subdivision.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 21:57:54 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Masood", "Talha Bin", ""], ["Hotz", "Ingrid", ""]]}, {"id": "1912.01067", "submitter": "Yu Guo", "authors": "Yu Guo, Milos Hasan, Lingqi Yan, Shuang Zhao", "title": "A Bayesian Inference Framework for Procedural Material Parameter\n  Estimation", "comments": "12 pages, 13 figures, Pacific Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural material models have been gaining traction in many applications\nthanks to their flexibility, compactness, and easy editability. We explore the\ninverse rendering problem of procedural material parameter estimation from\nphotographs, presenting a unified view of the problem in a Bayesian framework.\nIn addition to computing point estimates of the parameters by optimization, our\nframework uses a Markov Chain Monte Carlo approach to sample the space of\nplausible material parameters, providing a collection of plausible matches that\na user can choose from, and efficiently handling both discrete and continuous\nmodel parameters. To demonstrate the effectiveness of our framework, we fit\nprocedural models of a range of materials---wall plaster, leather, wood,\nanisotropic brushed metals and layered metallic paints---to both synthetic and\nreal target images.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 20:19:07 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 21:58:33 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 00:41:02 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 12:07:55 GMT"}, {"version": "v5", "created": "Wed, 4 Nov 2020 01:23:13 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Guo", "Yu", ""], ["Hasan", "Milos", ""], ["Yan", "Lingqi", ""], ["Zhao", "Shuang", ""]]}, {"id": "1912.01248", "submitter": "Maxence Reberol", "authors": "Maxence Reberol, Alexandre Chemin, Jean-Francois Remacle", "title": "Multiple Approaches to Frame Field Correction for CAD Models", "comments": "Accepted for 28th International Meshing Roundtable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional frame fields computed on CAD models often contain singular\ncurves that are not compatible with hexahedral meshing. In this paper, we show\nhow CAD feature curves can induce non meshable 3-5 singular curves and we study\nfour different approaches that aims at correcting the frame field topology. All\napproaches consist in modifying the frame field computation, the two first ones\nconsisting in applying internal constraints and the two last ones consisting in\nmodifying the boundary conditions. Approaches based on internal constraints are\nshown not to be very reliable because of their interactions with other\nsingularities. On the other hand, boundary condition modifications are more\npromising as their impact is very localized. We eventually recommend the 3-5\nsingular curve boundary snapping strategy, which is simple to implement and\nallows to generate topologically correct frame fields.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 09:10:24 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 09:52:35 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Reberol", "Maxence", ""], ["Chemin", "Alexandre", ""], ["Remacle", "Jean-Francois", ""]]}, {"id": "1912.01942", "submitter": "Johannes Sappl", "authors": "Fernando Zorrilla and Johannes Sappl and Wolfgang Rauch and Matthias\n  Harders", "title": "Accelerating Surface Tension Calculation in SPH via Particle\n  Classification & Monte Carlo Integration", "comments": "9 pages, 7 figures, 3 tables, presented at Computer Graphics & Visual\n  Computing (CGVC EGUK 2019) in Bangor, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface tension has a strong influence on the shape of fluid interfaces. We\npropose a method to calculate the corresponding forces efficiently. In contrast\nto several previous approaches, we discriminate to this end between surface and\nnon-surface SPH particles. Our method effectively smooths the fluid interface,\nminimizing its curvature. We make use of an approach inspired by Monte Carlo\nintegration to estimate local normals as well as curvatures, based on which the\nforce can be calculated. The technique is applicable, but not limited to 2D and\n3D simulations, and can be coupled with any common SPH formulation. It\noutperforms prior approaches with regard to total computation time per time\nstep, while being stable and avoiding artifacts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 13:04:29 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Zorrilla", "Fernando", ""], ["Sappl", "Johannes", ""], ["Rauch", "Wolfgang", ""], ["Harders", "Matthias", ""]]}, {"id": "1912.02125", "submitter": "Josef Musil", "authors": "Josef Musil, Jakub Knir, Athanasios Vitsas, Irene Gallou", "title": "Towards Sustainable Architecture: 3D Convolutional Neural Networks for\n  Computational Fluid Dynamics Simulation and Reverse DesignWorkflow", "comments": "NeurIPS Workshop on Machine Learning for Creativity and Design 3.0,\n  33rd Conference on Neural Information Processing Systems (NeurIPS 2019),\n  Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general and flexible approximation model for near real-time\nprediction of steady turbulent flow in a 3D domain based on residual\nConvolutional Neural Networks (CNNs). This approach can provide immediate\nfeedback for real-time iterations at the early stage of architectural design.\nThis work-flow is then reversed and offers a designer a tool that generates\nbuilding volumes based on target wind flow.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 17:48:22 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Musil", "Josef", ""], ["Knir", "Jakub", ""], ["Vitsas", "Athanasios", ""], ["Gallou", "Irene", ""]]}, {"id": "1912.03207", "submitter": "Boyang Deng", "authors": "Boyang Deng, JP Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey\n  Hinton, Mohammad Norouzi, Andrea Tagliasacchi", "title": "NASA: Neural Articulated Shape Approximation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient representation of articulated objects such as human bodies is an\nimportant problem in computer vision and graphics. To efficiently simulate\ndeformation, existing approaches represent 3D objects using polygonal meshes\nand deform them using skinning techniques. This paper introduces neural\narticulated shape approximation (NASA), an alternative framework that enables\nefficient representation of articulated deformable objects using neural\nindicator functions that are conditioned on pose. Occupancy testing using NASA\nis straightforward, circumventing the complexity of meshes and the issue of\nwater-tightness. We demonstrate the effectiveness of NASA for 3D tracking\napplications, and discuss other potential extensions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 16:18:35 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:23:51 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 14:49:13 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 18:57:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Deng", "Boyang", ""], ["Lewis", "JP", ""], ["Jeruzalski", "Timothy", ""], ["Pons-Moll", "Gerard", ""], ["Hinton", "Geoffrey", ""], ["Norouzi", "Mohammad", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1912.03310", "submitter": "Nitish Srivastava", "authors": "Nitish Srivastava, Hanlin Goh, Ruslan Salakhutdinov", "title": "Geometric Capsule Autoencoders for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to learn object representations from 3D point clouds\nusing bundles of geometrically interpretable hidden units, which we call\ngeometric capsules. Each geometric capsule represents a visual entity, such as\nan object or a part, and consists of two components: a pose and a feature. The\npose encodes where the entity is, while the feature encodes what it is. We use\nthese capsules to construct a Geometric Capsule Autoencoder that learns to\ngroup 3D points into parts (small local surfaces), and these parts into the\nwhole object, in an unsupervised manner. Our novel Multi-View Agreement voting\nmechanism is used to discover an object's canonical pose and its pose-invariant\nfeature vector. Using the ShapeNet and ModelNet40 datasets, we analyze the\nproperties of the learned representations and show the benefits of having\nmultiple votes agree. We perform alignment and retrieval of arbitrarily rotated\nobjects -- tasks that evaluate our model's object identification and canonical\npose recovery capabilities -- and obtained insightful results.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 00:10:14 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Srivastava", "Nitish", ""], ["Goh", "Hanlin", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1912.03416", "submitter": "Zhanhang Liang", "authors": "Marco (Zhanhang) Liang, Michael T. Goodrich, Shuang Zhao", "title": "On the Optical Accuracy of the Salvator Mundi", "comments": "16 pages, 8 figures, technical article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A debate in the scientific literature has arisen regarding whether the orb\ndepicted in Salvator Mundi, which has been attributed by some experts to\nLeonardo da Vinci, was rendered in a optically faithful manner or not. Some\nhypothesize that it was solid crystal while others hypothesize that it was\nhollow, with competing explanations for its apparent lack of background\ndistortion and its three white spots. In this paper, we study the optical\naccuracy of the Salvator Mundi using physically based rendering, a\nsophisticated computer graphics tool that produces optically accurate images by\nsimulating light transport in virtual scenes. We created a virtual model of the\ncomposition centered on the translucent orb in the subject's hand. By\nsynthesizing images under configurations that vary illuminations and orb\nmaterial properties, we tested whether it is optically possible to produce an\nimage that renders the orb similarly to how it appears in the painting. Our\nexperiments show that an optically accurate rendering qualitatively matching\nthat of the painting is indeed possible using materials, light sources, and\nscientific knowledge available to Leonardo da Vinci circa 1500. We additionally\ntested alternative theories regarding the composition of the orb, such as that\nit was a solid calcite ball, which provide empirical evidence that such\nalternatives are unlikely to produce images similar to the painting, and that\nthe orb is instead hollow.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2019 02:26:31 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Marco", "", "", "Zhanhang"], ["Liang", "", ""], ["Goodrich", "Michael T.", ""], ["Zhao", "Shuang", ""]]}, {"id": "1912.03629", "submitter": "Francis Williams", "authors": "Francis Williams, Daniele Panozzo, Kwang Moo Yi, Andrea Tagliasacchi", "title": "VoronoiNet: General Functional Approximators with Local Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voronoi diagrams are highly compact representations that are used in various\nGraphics applications. In this work, we show how to embed a differentiable\nversion of it -- via a novel deep architecture -- into a generative deep\nnetwork. By doing so, we achieve a highly compact latent embedding that is able\nto provide much more detailed reconstructions, both in 2D and 3D, for various\nshapes. In this tech report, we introduce our representation and present a set\nof preliminary results comparing it with recently proposed implicit occupancy\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 07:12:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Williams", "Francis", ""], ["Panozzo", "Daniele", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1912.04000", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Patrick Callet, Frederic Magoules", "title": "Spectral domain decomposition method for physically-based rendering of\n  Royaumont abbey", "comments": null, "journal-ref": null, "doi": "10.1109/CSE-EUC-DCABES.2016.212", "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a virtual reconstitution of the destroyed Royaumont abbey\nchurch, this paper investigates computer sciences issues intrinsic to the\nphysically-based image rendering. First, a virtual model was designed from\nhistorical sources and archaeological descriptions. Then some materials\nphysical properties were measured on remains of the church and on pieces from\nsimilar ancient churches. We specify the properties of our lighting source\nwhich is a representation of the sun, and present the rendering algorithm\nimplemented in our software Virtuelium. In order to accelerate the computation\nof the interactions between light-rays and objects, this ray-tracing algorithm\nis parallelized by means of domain decomposition techniques. Numerical\nexperiments show that the computational time saved by a classic parallelization\nis much less significant than that gained with our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:36:39 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Callet", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04158", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Niloy J. Mitra, Tobias Ritschel", "title": "Learning a Neural 3D Texture Space from 2D Exemplars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative model of 2D and 3D natural textures with diversity,\nvisual fidelity and at high computational efficiency. This is enabled by a\nfamily of methods that extend ideas from classic stochastic procedural\ntexturing (Perlin noise) to learned, deep, non-linearities. The key idea is a\nhard-coded, tunable and differentiable step that feeds multiple transformed\nrandom 2D or 3D fields into an MLP that can be sampled over infinite domains.\nOur model encodes all exemplars from a diverse set of textures without a need\nto be re-trained for each exemplar. Applications include texture interpolation,\nand learning 3D textures from 2D exemplars.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 16:35:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 18:26:48 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Henzler", "Philipp", ""], ["Mitra", "Niloy J.", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1912.04302", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Michael Zollh\\\"ofer, Christian Theobalt,\n  Matthias Nie{\\ss}ner", "title": "DeepDeform: Learning Non-rigid RGB-D Reconstruction with Semi-supervised\n  Data", "comments": "Video: https://youtu.be/OrHLacCDZVQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying data-driven approaches to non-rigid 3D reconstruction has been\ndifficult, which we believe can be attributed to the lack of a large-scale\ntraining corpus. Unfortunately, this method fails for important cases such as\nhighly non-rigid deformations. We first address this problem of lack of data by\nintroducing a novel semi-supervised strategy to obtain dense inter-frame\ncorrespondences from a sparse set of annotations. This way, we obtain a large\ndataset of 400 scenes, over 390,000 RGB-D frames, and 5,533 densely aligned\nframe pairs; in addition, we provide a test set along with several metrics for\nevaluation. Based on this corpus, we introduce a data-driven non-rigid feature\nmatching approach, which we integrate into an optimization-based reconstruction\npipeline. Here, we propose a new neural network that operates on RGB-D frames,\nwhile maintaining robustness under large non-rigid deformations and producing\naccurate predictions. Our approach significantly outperforms existing non-rigid\nreconstruction methods that do not use learned data terms, as well as\nlearning-based approaches that only use self-supervision.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 19:00:04 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 19:00:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.04356", "submitter": "Mengchen Wang", "authors": "Mengchen Wang, Nicolas Ferey, Patrick Bourdot, Frederic Magoules", "title": "Interactive 3D fluid simulation: steering the simulation in progress\n  using Lattice Boltzmann Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a work in progress about software and hardware\narchitecture to steer and control an ongoing fluid simulation in a context of a\nserious game application. We propose to use the Lattice Boltzmann Method as the\nsimulation approach considering that it can provide fully parallel algorithms\nto reach interactive time and because it is easier to change parameters while\nthe simulation is in progress remaining physically relevant than more classical\nsimulation approaches. We describe which parameters we can modify and how we\nsolve technical issues of interactive steering and we finally show an\napplication of our interactive fluid simulation approach of water dam\nphenomena.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 20:16:48 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Mengchen", ""], ["Ferey", "Nicolas", ""], ["Bourdot", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.04583", "submitter": "Nicolas Mellado", "authors": "Baptiste Delos and Nicolas Mellado and David Vanderhaeghe and Remi\n  Cozot", "title": "RGB Point Cloud Manipulation with Triangular Structures for Artistic\n  Image Recoloring", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usual approaches for image recoloring, such as local filtering by transfer\nfunctions and global histogram remapping, lack of accurate control or miss\nsmall groups of important pixels. In this paper, we introduce a triangle-based\nstructuring of the colors of an image in the RGB space. We present an analysis\nof image colors in the RGB space showing the theoretical motivation of our\ntriangular abstraction. We illustrate the usefulness of our structure to\nrecolor images.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:04:09 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 08:45:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Delos", "Baptiste", ""], ["Mellado", "Nicolas", ""], ["Vanderhaeghe", "David", ""], ["Cozot", "Remi", ""]]}, {"id": "1912.04591", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas and Vittorio Ferrari", "title": "Neural Voxel Renderer: Learning an Accurate and Controllable Rendering\n  Tool", "comments": "Additional results: http://www.krematas.com/nvr/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural rendering framework that maps a voxelized scene into a\nhigh quality image. Highly-textured objects and scene element interactions are\nrealistically rendered by our method, despite having a rough representation as\nan input. Moreover, our approach allows controllable rendering: geometric and\nappearance modifications in the input are accurately propagated to the output.\nThe user can move, rotate and scale an object, change its appearance and\ntexture or modify the position of the light and all these edits are represented\nin the final rendering. We demonstrate the effectiveness of our approach by\nrendering scenes with varying appearance, from single color per object to\ncomplex, high-frequency textures. We show that our rerendering network can\ngenerate very detailed images that represent precisely the appearance of the\ninput scene. Our experiments illustrate that our approach achieves more\naccurate image synthesis results compared to alternatives and can also handle\nlow voxel grid resolutions. Finally, we show how our neural rendering framework\ncan capture and faithfully render objects from real images and from a diverse\nset of classes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:30:03 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 14:34:47 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1912.04898", "submitter": "Sasikanth Raghava", "authors": "Sasikanth Raghava Goteti", "title": "Modelling curvature of a bent paper leaf", "comments": "5 pages , 5 figures", "journal-ref": null, "doi": "10.7287/PEERJ.PREPRINTS.161", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we briefly describe various tools and approaches that\nalgebraic geometry has to offer to straighten bent objects. Throughout this\narticle we will consider a specific example of a bent or curved piece of paper\nwhich in our case acts very much like an elastica curve. We conclude this\narticle with a suggestion to algebraic geometry as a viable and fast\nperformance alternative of neural networks in vision and machine learning. The\npurpose of this article is not to build a full blown framework but to show\npossibility of using algebraic geometry as an alternative to neural networks\nfor recognizing or extracting features on manifolds.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 09:10:31 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Goteti", "Sasikanth Raghava", ""]]}, {"id": "1912.05019", "submitter": "Emmanuel Iarussi", "authors": "Pablo Navarro, Jos\\'e Ignacio Orlando, Claudio Delrieux, and Emmanuel\n  Iarussi", "title": "SketchZooms: Deep multi-view descriptors for matching line drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding point-wise correspondences between images is a long-standing problem\nin image analysis. This becomes particularly challenging for sketch images, due\nto the varying nature of human drawing style, projection distortions and\nviewport changes. In this paper we present the first attempt to obtain a\nlearned descriptor for dense registration in line drawings. Based on recent\ndeep learning techniques for corresponding photographs, we designed descriptors\nto locally match image pairs where the object of interest belongs to the same\nsemantic category, yet still differ drastically in shape, form, and projection\nangle. To this end, we have specifically crafted a data set of synthetic\nsketches using non-photorealistic rendering over a large collection of\npart-based registered 3D models. After training, a neural network generates\ndescriptors for every pixel in an input image, which are shown to generalize\ncorrectly in unseen sketches hand-drawn by humans. We evaluate our method\nagainst a baseline of correspondences data collected from expert designers, in\naddition to comparisons with other descriptors that have been proven effective\nin sketches. Code, data and further resources will be publicly released by the\ntime of publication.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 14:31:33 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 19:50:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Navarro", "Pablo", ""], ["Orlando", "Jos\u00e9 Ignacio", ""], ["Delrieux", "Claudio", ""], ["Iarussi", "Emmanuel", ""]]}, {"id": "1912.05099", "submitter": "Tianying Wang", "authors": "Tianying Wang, Wei Qi Toh, Hao Zhang, Xiuchao Sui, Shaohua Li, Yong\n  Liu, Wei Jing", "title": "RoboCoDraw: Robotic Avatar Drawing with GAN-based Style Transfer and\n  Time-efficient Path Optimization", "comments": "Accepted by AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic drawing has become increasingly popular as an entertainment and\ninteractive tool. In this paper we present RoboCoDraw, a real-time\ncollaborative robot-based drawing system that draws stylized human face\nsketches interactively in front of human users, by using the Generative\nAdversarial Network (GAN)-based style transfer and a Random-Key Genetic\nAlgorithm (RKGA)-based path optimization. The proposed RoboCoDraw system takes\na real human face image as input, converts it to a stylized avatar, then draws\nit with a robotic arm. A core component in this system is the Avatar-GAN\nproposed by us, which generates a cartoon avatar face image from a real human\nface. AvatarGAN is trained with unpaired face and avatar images only and can\ngenerate avatar images of much better likeness with human face images in\ncomparison with the vanilla CycleGAN. After the avatar image is generated, it\nis fed to a line extraction algorithm and converted to sketches. An RKGA-based\npath optimization algorithm is applied to find a time-efficient robotic drawing\npath to be executed by the robotic arm. We demonstrate the capability of\nRoboCoDraw on various face images using a lightweight, safe collaborative robot\nUR5.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 03:20:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wang", "Tianying", ""], ["Toh", "Wei Qi", ""], ["Zhang", "Hao", ""], ["Sui", "Xiuchao", ""], ["Li", "Shaohua", ""], ["Liu", "Yong", ""], ["Jing", "Wei", ""]]}, {"id": "1912.05494", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Remi Cerise, Patrick Callet, Frederic\n  Magoules", "title": "Spectral Domain Decomposition Method for Natural Lighting and Medieval\n  Glass Rendering", "comments": null, "journal-ref": null, "doi": "10.1109/HPCC.2014.17", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use an original ray-tracing domain decomposition method to\naddress image rendering of naturally lighted scenes. This new method allows to\nparticularly analyze rendering problems on parallel architectures, in the case\nof interactions between light-rays and glass material. Numerical experiments,\nfor medieval glass rendering within the church of the Royaumont abbey,\nillustrate the performance of the proposed ray-tracing domain decomposition\nmethod (DDM) on multi-cores and multi-processors architectures. On one hand,\napplying domain decomposition techniques increases speedups obtained by\nparallelizing the computation. On the other hand, for a fixed number of\nparallel processes, we notice that speedups increase as the number of\nsub-domains do.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:33:17 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Cerise", "Remi", ""], ["Callet", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.05566", "submitter": "Justus Thies", "authors": "Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt,\n  Matthias Nie{\\ss}ner", "title": "Neural Voice Puppetry: Audio-driven Facial Reenactment", "comments": "Video: https://youtu.be/s74_yQiJMXA Project/Demo/Code:\n  https://justusthies.github.io/posts/neural-voice-puppetry/", "journal-ref": "ECCV2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Neural Voice Puppetry, a novel approach for audio-driven facial\nvideo synthesis. Given an audio sequence of a source person or digital\nassistant, we generate a photo-realistic output video of a target person that\nis in sync with the audio of the source input. This audio-driven facial\nreenactment is driven by a deep neural network that employs a latent 3D face\nmodel space. Through the underlying 3D representation, the model inherently\nlearns temporal stability while we leverage neural rendering to generate\nphoto-realistic output frames. Our approach generalizes across different\npeople, allowing us to synthesize videos of a target actor with the voice of\nany unknown source actor or even synthetic voices that can be generated\nutilizing standard text-to-speech approaches. Neural Voice Puppetry has a\nvariety of use-cases, including audio-driven video avatars, video dubbing, and\ntext-driven video synthesis of a talking head. We demonstrate the capabilities\nof our method in a series of audio- and text-based puppetry examples, including\ncomparisons to state-of-the-art techniques and a user study.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 19:00:18 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:49:45 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Thies", "Justus", ""], ["Elgharib", "Mohamed", ""], ["Tewari", "Ayush", ""], ["Theobalt", "Christian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1912.06126", "submitter": "Kyle Genova", "authors": "Kyle Genova, Forrester Cole, Avneesh Sud, Aaron Sarna, Thomas\n  Funkhouser", "title": "Local Deep Implicit Functions for 3D Shape", "comments": "Camera ready version for CVPR 2020 Oral. Prior to review, this paper\n  was referred to as DSIF, \"Deep Structured Implicit Functions.\" 11 pages, 9\n  figures. Project video at https://youtu.be/3RAITzNWVJs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to learn a 3D shape representation that enables\naccurate surface reconstruction, compact storage, efficient computation,\nconsistency for similar shapes, generalization across diverse shape categories,\nand inference from depth camera observations. Towards this end, we introduce\nLocal Deep Implicit Functions (LDIF), a 3D shape representation that decomposes\nspace into a structured set of learned implicit functions. We provide networks\nthat infer the space decomposition and local deep implicit functions from a 3D\nmesh or posed depth image. During experiments, we find that it provides 10.3\npoints higher surface reconstruction accuracy (F-Score) than the\nstate-of-the-art (OccNet), while requiring fewer than 1 percent of the network\nparameters. Experiments on posed depth image completion and generalization to\nunseen classes show 15.8 and 17.8 point improvements over the state-of-the-art,\nwhile producing a structured 3D representation for each input with consistency\nacross diverse shape collections.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 18:50:46 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 03:26:47 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Genova", "Kyle", ""], ["Cole", "Forrester", ""], ["Sud", "Avneesh", ""], ["Sarna", "Aaron", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1912.06332", "submitter": "Archit Rathore", "authors": "Archit Rathore, Nithin Chalapathi, Sourabh Palande, Bei Wang", "title": "TopoAct: Visually Exploring the Shape of Activations in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks such as GoogLeNet, ResNet, and BERT have achieved\nimpressive performance in tasks such as image and text classification. To\nunderstand how such performance is achieved, we probe a trained deep neural\nnetwork by studying neuron activations, i.e., combinations of neuron firings,\nat various layers of the network in response to a particular input. With a\nlarge number of inputs, we aim to obtain a global view of what neurons detect\nby studying their activations. In particular, we develop visualizations that\nshow the shape of the activation space, the organizational principle behind\nneuron activations, and the relationships of these activations within a layer.\nApplying tools from topological data analysis, we present TopoAct, a visual\nexploration system to study topological summaries of activation vectors. We\npresent exploration scenarios using TopoAct that provide valuable insights into\nlearned representations of neural networks. We expect TopoAct to give a\ntopological perspective that enriches the current toolbox of neural network\nanalysis, and to provide a basis for network architecture diagnosis and data\nanomaly detection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 06:15:08 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:01:41 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 23:41:17 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 06:27:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rathore", "Archit", ""], ["Chalapathi", "Nithin", ""], ["Palande", "Sourabh", ""], ["Wang", "Bei", ""]]}, {"id": "1912.06341", "submitter": "Bei Wang", "authors": "Tushar Athawale, Dan Maljovec, Chris R. Johnson, Valerio Pascucci, Bei\n  Wang", "title": "Uncertainty Visualization of 2D Morse Complex Ensembles Using\n  Statistical Summary Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morse complexes are gradient-based topological descriptors with close\nconnections to Morse theory. They are widely applicable in scientific\nvisualization as they serve as important abstractions for gaining insights into\nthe topology of scalar fields. Noise inherent to scalar field data due to\nacquisitions and processing, however, limits our understanding of the Morse\ncomplexes as structural abstractions. We, therefore, explore uncertainty\nvisualization of an ensemble of 2D Morse complexes that arise from scalar\nfields coupled with data uncertainty. We propose statistical summary maps as\nnew entities for capturing structural variations and visualizing positional\nuncertainties of Morse complexes in ensembles. Specifically, we introduce two\ntypes of statistical summary maps -- the Probabilistic Map and the Survival Map\n-- to characterize the uncertain behaviors of local extrema and local gradient\nflows, respectively. We demonstrate the utility of our proposed approach using\nsynthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:06:52 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Athawale", "Tushar", ""], ["Maljovec", "Dan", ""], ["Johnson", "Chris R.", ""], ["Pascucci", "Valerio", ""], ["Wang", "Bei", ""]]}, {"id": "1912.06395", "submitter": "Wang Yifan", "authors": "Wang Yifan, Noam Aigerman, Vladimir G. Kim, Siddhartha Chaudhuri, Olga\n  Sorkine-Hornung", "title": "Neural Cages for Detail-Preserving 3D Deformations", "comments": "accepted for oral presentation at CVPR 2020, code available at\n  https://github.com/yifita/deep_cage", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel learnable representation for detail-preserving shape\ndeformation. The goal of our method is to warp a source shape to match the\ngeneral structure of a target shape, while preserving the surface details of\nthe source. Our method extends a traditional cage-based deformation technique,\nwhere the source shape is enclosed by a coarse control mesh termed \\emph{cage},\nand translations prescribed on the cage vertices are interpolated to any point\non the source mesh via special weight functions. The use of this sparse cage\nscaffolding enables preserving surface details regardless of the shape's\nintricacy and topology. Our key contribution is a novel neural network\narchitecture for predicting deformations by controlling the cage. We\nincorporate a differentiable cage-based deformation module in our architecture,\nand train our network end-to-end. Our method can be trained with common\ncollections of 3D models in an unsupervised fashion, without any cage-specific\nannotations. We demonstrate the utility of our method for synthesizing shape\nvariations and deformation transfer.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 10:25:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 13:33:27 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Yifan", "Wang", ""], ["Aigerman", "Noam", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1912.06474", "submitter": "Guillaume Gbikpi-Benissan", "authors": "Guillaume Gbikpi-Benissan, Patrick Callet, Frederic Magoules", "title": "Spectral domain decomposition method for physically-based rendering of\n  photochromic/electrochromic glass windows", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.05494", "journal-ref": null, "doi": "10.1109/DCABES.2014.27", "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the time consuming issues intrinsic to physically-based\nimage rendering algorithms. First, glass materials optical properties were\nmeasured on samples of real glasses and other objects materials inside an hotel\nroom were characterized by deducing spectral data from multiple trichromatic\nimages. We then present the rendering model and ray-tracing algorithm\nimplemented in Virtuelium, an open source software. In order to accelerate the\ncomputation of the interactions between light rays and objects, the ray-tracing\nalgorithm is parallelized by means of domain decomposition method techniques.\nNumerical experiments show that the speedups obtained with classical\nparallelization techniques are significantly less significant than those\nachieved with parallel domain decomposition methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 12:31:50 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Gbikpi-Benissan", "Guillaume", ""], ["Callet", "Patrick", ""], ["Magoules", "Frederic", ""]]}, {"id": "1912.07109", "submitter": "Yue Jiang", "authors": "Yue Jiang, Dantong Ji, Zhizhong Han, Matthias Zwicker", "title": "SDFDiff: Differentiable Rendering of Signed Distance Fields for 3D Shape\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SDFDiff, a novel approach for image-based shape optimization using\ndifferentiable rendering of 3D shapes represented by signed distance functions\n(SDF). Compared to other representations, SDFs have the advantage that they can\nrepresent shapes with arbitrary topology, and that they guarantee watertight\nsurfaces. We apply our approach to the problem of multi-view 3D reconstruction,\nwhere we achieve high reconstruction quality and can capture complex topology\nof 3D objects. In addition, we employ a multi-resolution strategy to obtain a\nrobust optimization algorithm. We further demonstrate that our SDF-based\ndifferentiable renderer can be integrated with deep learning models, which\nopens up options for learning approaches on 3D objects without 3D supervision.\nIn particular, we apply our method to single-view 3D reconstruction and achieve\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2019 21:06:46 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jiang", "Yue", ""], ["Ji", "Dantong", ""], ["Han", "Zhizhong", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1912.07827", "submitter": "Yue Jiang", "authors": "Yue Jiang, Ruofei Du, Christof Lutteroth, Wolfgang Stuerzlinger", "title": "ORC Layout: Adaptive GUI Layout with OR-Constraints", "comments": null, "journal-ref": null, "doi": "10.1145/3290605.3300643", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for constraint-based graphical user interface\n(GUI) layout based on OR-constraints (ORC) in standard soft/hard linear\nconstraint systems. ORC layout unifies grid layout and flow layout, supporting\nboth their features as well as cases where grid and flow layouts individually\nfail. We describe ORC design patterns that enable designers to safely create\nflexible layouts that work across different screen sizes and orientations. We\nalso present the ORC Editor, a GUI editor that enables designers to apply ORC\nin a safe and effective manner, mixing grid, flow and new ORC layout features\nas appropriate. We demonstrate that our prototype can adapt layouts to screens\nwith different aspect ratios with only a single layout specification, easing\nthe burden of GUI maintenance. Finally, we show that ORC specifications can be\nmodified interactively and solved efficiently at runtime.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 05:41:42 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Jiang", "Yue", ""], ["Du", "Ruofei", ""], ["Lutteroth", "Christof", ""], ["Stuerzlinger", "Wolfgang", ""]]}, {"id": "1912.08485", "submitter": "Michael Kern", "authors": "Michael Kern, Christoph Neuhauser, Torben Maack, Mengjiao Han, Will\n  Usher, R\\\"udiger Westermann", "title": "A Comparison of Rendering Techniques for Large 3D Line Sets with\n  Transparency", "comments": "16 pages, 8 pages appendix", "journal-ref": null, "doi": "10.1109/TVCG.2020.2975795", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive study of interactive rendering techniques\nfor large 3D line sets with transparency. The rendering of transparent lines is\nwidely used for visualizing trajectories of tracer particles in flow fields.\nTransparency is then used to fade out lines deemed unimportant, based on, for\ninstance, geometric properties or attributes defined along them. Since accurate\nblending of transparent lines requires rendering the lines in back-to-front or\nfront-to-back order, enforcing this order for 3D line sets with tens or even\nhundreds of thousands of elements becomes challenging. In this paper, we study\nCPU and GPU rendering techniques for large transparent 3D line sets. We compare\naccurate and approximate techniques using optimized implementations and a\nnumber of benchmark data sets. We discuss the effects of data size and\ntransparency on quality, performance and memory consumption. Based on our\nstudy, we propose two improvements to per-pixel fragment lists and multi-layer\nalpha blending. The first improves the rendering speed via an improved GPU\nsorting operation, and the second improves rendering quality via a\ntransparency-based bucketing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 09:44:34 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 09:54:48 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Kern", "Michael", ""], ["Neuhauser", "Christoph", ""], ["Maack", "Torben", ""], ["Han", "Mengjiao", ""], ["Usher", "Will", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "1912.08558", "submitter": "Christian Tominski", "authors": "Christian Eichner and Heidrun Schumann and Christian Tominski", "title": "Multi-display Visual Analysis: Model, Interface, and Layout Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern display environments offer great potential for involving multiple\nusers in presentations, discussions, and data analysis sessions. By showing\nmultiple views on multiple displays, information exchange can be improved,\nseveral perspectives on the data can be combined, and different analysis\nstrategies can be pursued.\n  In this report, we describe concepts to support display composition,\ninformation distribution, and analysis coordination for visual data analysis in\nmulti-display environments. In particular, a basic model for layout modeling is\nintroduced, a graphical interface for interactive generation of the model is\npresented, and a layout mechanism is described that arranges multiple views on\nmultiple displays automatically. Furthermore, approaches to meta-analysis will\nbe discussed. The developed approaches are demonstrated in a use case that\nfocuses on parameter space analysis for the segmentation of time series data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 12:21:01 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Eichner", "Christian", ""], ["Schumann", "Heidrun", ""], ["Tominski", "Christian", ""]]}, {"id": "1912.08757", "submitter": "Byungsoo Kim", "authors": "Fabienne Christen, Byungsoo Kim, Vinicius C. Azevedo and Barbara\n  Solenthaler", "title": "Neural Smoke Stylization with Color Transfer", "comments": "Submitted to Eurographics2020", "journal-ref": "Eurographics 2020 - Short Papers", "doi": "10.2312/egs.20201015", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistically controlling fluid simulations requires a large amount of manual\nwork by an artist. The recently presented transportbased neural style transfer\napproach simplifies workflows as it transfers the style of arbitrary input\nimages onto 3D smoke simulations. However, the method only modifies the shape\nof the fluid but omits color information. In this work, we therefore extend the\nprevious approach to obtain a complete pipeline for transferring shape and\ncolor information onto 2D and 3D smoke simulations with neural networks. Our\nresults demonstrate that our method successfully transfers colored style\nfeatures consistently in space and time to smoke data for different input\ntextures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 17:44:47 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Christen", "Fabienne", ""], ["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1912.08776", "submitter": "Byungsoo Kim", "authors": "Simon Biland, Vinicius C. Azevedo, Byungsoo Kim and Barbara\n  Solenthaler", "title": "Frequency-Aware Reconstruction of Fluid Simulations with Generative\n  Networks", "comments": "Submitted to Eurographics2020", "journal-ref": "Eurographics 2020 - Short Papers", "doi": "10.2312/egs.20201019", "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks were recently employed to fully reconstruct\nfluid simulation data from a set of reduced parameters. However, since\n(de-)convolutions traditionally trained with supervised L1-loss functions do\nnot discriminate between low and high frequencies in the data, the error is not\nminimized efficiently for higher bands. This directly correlates with the\nquality of the perceived results, since missing high frequency details are\neasily noticeable. In this paper, we analyze the reconstruction quality of\ngenerative networks and present a frequency-aware loss function that is able to\nfocus on specific bands of the dataset during training time. We show that our\napproach improves reconstruction quality of fluid simulation data in\nmid-frequency bands, yielding perceptually better results while requiring\ncomparable training time.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 18:13:22 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Biland", "Simon", ""], ["Azevedo", "Vinicius C.", ""], ["Kim", "Byungsoo", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1912.09580", "submitter": "Bei Wang", "authors": "Youjia Zhou, Janis Lazovskis, Michael J. Catanzaro, Matthew Zabka, Bei\n  Wang", "title": "MVF Designer: Design and Visualization of Morse Vector Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.AT math.CO math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector field design on surfaces was originally motivated by applications in\ngraphics such as texture synthesis and rendering. In this paper, we consider\nthe idea of vector field design with a new motivation from computational\ntopology. We are interested in designing and visualizing vector fields to aid\nthe study of Morse functions, Morse vector fields, and Morse-Smale complexes.\nTo achieve such a goal, we present MVF Designer, a new interactive design\nsystem that provides fine-grained control over vector field geometry, enables\nthe editing of vector field topology, and supports a design process in a simple\nand efficient way using elementary moves, which are actions that initiate or\nadvance our design process. Our system allows mathematicians to explore the\ncomplex configuration spaces of Morse functions, their gradients, and their\nassociated Morse-Smale complexes. Understanding these spaces will help us\nexpand further their applicability in topological data analysis and\nvisualization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 22:39:40 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zhou", "Youjia", ""], ["Lazovskis", "Janis", ""], ["Catanzaro", "Michael J.", ""], ["Zabka", "Matthew", ""], ["Wang", "Bei", ""]]}, {"id": "1912.09596", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann", "title": "Comparing Hierarchical Data Structures for Sparse Volume Rendering with\n  Empty Space Skipping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empty space skipping can be efficiently implemented with hierarchical data\nstructures such as k-d trees and bounding volume hierarchies. This paper\ncompares several recently published hierarchical data structures with regard to\nconstruction and rendering performance. The papers that form our prior work\nhave primarily focused on interactively building the data structures and only\nshowed that rendering performance is superior to using simple acceleration data\nstructures such as uniform grids with macro cells. In the area of surface ray\ntracing, there exists a trade-off between construction and rendering\nperformance of hierarchical data structures. In this paper we present\nperformance comparisons for several empty space skipping data structures in\norder to determine if such a trade-off also exists for volume rendering with\nuniform data topologies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 00:33:03 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Zellmann", "Stefan", ""]]}, {"id": "1912.10194", "submitter": "Xunnian Yang", "authors": "Xunnian Yang", "title": "Anisotropic Mesh Filtering by Homogeneous MLS Fitting", "comments": "13pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel geometric filter, a homogeneous moving least\nsquares fitting-based filter (H-MLS filter), for anisotropic mesh filtering.\nInstead of fitting the noisy data by a moving parametric surface and projecting\nthe noisy data onto the surface, we compute new positions of mesh vertices as\nthe solutions to homogeneous least squares fitting of moving constants to local\nneighboring vertices and tangent planes that pass through the vertices. The\nnormals for defining the tangent planes need not be filtered beforehand but the\nparameters for balancing the influences between neighboring vertices and\nneighboring tangent planes are computed robustly from the original data under\nthe assumption of quadratic precision in each tangent direction. The weights\nfor respective neighboring points for the least squares fitting are computed\nadaptively for anisotropic filtering. The filter is easy to implement and has\ndistinctive features for mesh filtering. (1) The filter is locally implemented\nand has circular precision, spheres and cylinders can be recovered exactly by\nthe filter. (2) The filtered mesh has a high fidelity to the original data\nwithout any position constraint and salient or sharp features can be preserved\nwell. (3) The filter can be used to filter meshes with various kinds of noise\nas well as meshes with highly irregular triangulation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 04:18:34 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yang", "Xunnian", ""]]}, {"id": "1912.10589", "submitter": "Yuan Yao", "authors": "Yuan Yao, Nico Schertler, Enrique Rosales, Helge Rhodin, Leonid Sigal,\n  Alla Sheffer", "title": "Front2Back: Single View 3D Shape Reconstruction via Front to Back\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of a 3D shape from a single 2D image is a classical computer\nvision problem, whose difficulty stems from the inherent ambiguity of\nrecovering occluded or only partially observed surfaces. Recent methods address\nthis challenge through the use of largely unstructured neural networks that\neffectively distill conditional mapping and priors over 3D shape. In this work,\nwe induce structure and geometric constraints by leveraging three core\nobservations: (1) the surface of most everyday objects is often almost entirely\nexposed from pairs of typical opposite views; (2) everyday objects often\nexhibit global reflective symmetries which can be accurately predicted from\nsingle views; (3) opposite orthographic views of a 3D shape share consistent\nsilhouettes. Following these observations, we first predict orthographic 2.5D\nvisible surface maps (depth, normal and silhouette) from perspective 2D images,\nand detect global reflective symmetries in this data; second, we predict the\nback facing depth and normal maps using as input the front maps and, when\navailable, the symmetric reflections of these maps; and finally, we reconstruct\na 3D mesh from the union of these maps using a surface reconstruction method\nbest suited for this data. Our experiments demonstrate that our framework\noutperforms state-of-the art approaches for 3D shape reconstructions from 2D\nand 2.5D data in terms of input fidelity and details preservation.\nSpecifically, we achieve 12% better performance on average in ShapeNet\nbenchmark dataset, and up to 19% for certain classes of objects (e.g., chairs\nand vessels).\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 02:27:05 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 23:51:41 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yao", "Yuan", ""], ["Schertler", "Nico", ""], ["Rosales", "Enrique", ""], ["Rhodin", "Helge", ""], ["Sigal", "Leonid", ""], ["Sheffer", "Alla", ""]]}, {"id": "1912.10637", "submitter": "Xiaowei Hu", "authors": "Xiao Tang, Xiaowei Hu, Chi-Wing Fu, Daniel Cohen-Or", "title": "GrabAR: Occlusion-aware Grabbing Virtual Objects in AR", "comments": "conditionally accepted to UIST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing augmented reality (AR) applications often ignore occlusion between\nreal hands and virtual objects when incorporating virtual objects in our views.\nThe challenges come from the lack of accurate depth and mismatch between real\nand virtual depth. This paper presents GrabAR, a new approach that directly\npredicts the real-and-virtual occlusion, and bypasses the depth acquisition and\ninference. Our goal is to enhance AR applications with interactions between\nhand (real) and grabbable objects (virtual). With paired images of hand and\nobject as inputs, we formulate a neural network that learns to generate the\nocclusion mask. To train the network, we compile a synthetic dataset to\npre-train it and a real dataset to fine-tune it, thus reducing the burden of\nmanual labels and addressing the domain difference. Then, we embed the trained\nnetwork in a prototyping AR system that supports hand grabbing of various\nvirtual objects, demonstrate the system performance, both quantitatively and\nqualitatively, and showcase interaction scenarios, in which we can use bare\nhand to grab virtual objects and directly manipulate them.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:47:21 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 13:26:35 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 05:37:34 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tang", "Xiao", ""], ["Hu", "Xiaowei", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1912.10787", "submitter": "Austin Dill", "authors": "Austin Dill, Songwei Ge, Eunsu Kang, Chun-Liang Li, Barnabas Poczos", "title": "Learned Interpolation for 3D Generation", "comments": "Creativity and Design Workshop at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to generate novel 3D shapes with machine learning, one must allow\nfor interpolation. The typical approach for incorporating this creative process\nis to interpolate in a learned latent space so as to avoid the problem of\ngenerating unrealistic instances by exploiting the model's learned structure.\nThe process of the interpolation is supposed to form a semantically smooth\nmorphing. While this approach is sound for synthesizing realistic media such as\nlifelike portraits or new designs for everyday objects, it subjectively fails\nto directly model the unexpected, unrealistic, or creative. In this work, we\npresent a method for learning how to interpolate point clouds. By encoding\nprior knowledge about real-world objects, the intermediate forms are both\nrealistic and unlike any existing forms. We show not only how this method can\nbe used to generate \"creative\" point clouds, but how the method can also be\nleveraged to generate 3D models suitable for sculpture.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 23:44:33 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 20:12:32 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Dill", "Austin", ""], ["Ge", "Songwei", ""], ["Kang", "Eunsu", ""], ["Li", "Chun-Liang", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1912.11565", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Varsha Hedau, David Forsyth, Derek Hoiem", "title": "Rendering Synthetic Objects into Legacy Photographs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to realistically insert synthetic objects into existing\nphotographs without requiring access to the scene or any additional scene\nmeasurements. With a single image and a small amount of annotation, our method\ncreates a physical model of the scene that is suitable for realistically\nrendering synthetic objects with diffuse, specular, and even glowing materials\nwhile accounting for lighting interactions between the objects and the scene.\nWe demonstrate in a user study that synthetic images produced by our method are\nconfusable with real scenes, even for people who believe they are good at\ntelling the difference. Further, our study shows that our method is competitive\nwith other insertion methods while requiring less scene information. We also\ncollected new illumination and reflectance datasets; renderings produced by our\nsystem compare well to ground truth. Our system has applications in the movie\nand gaming industry, as well as home decorating and user content creation,\namong others.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:32:42 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Karsch", "Kevin", ""], ["Hedau", "Varsha", ""], ["Forsyth", "David", ""], ["Hoiem", "Derek", ""]]}, {"id": "1912.11567", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Mani Golparvar-Fard, David Forsyth", "title": "ConstructAide: Analyzing and Visualizing Construction Sites through\n  Photographs and Building Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of tools for analyzing, visualizing, and assessing\narchitectural/construction progress with unordered photo collections and 3D\nbuilding models. With our interface, a user guides the registration of the\nmodel in one of the images, and our system automatically computes the alignment\nfor the rest of the photos using a novel Structure-from-Motion (SfM) technique;\nimages with nearby viewpoints are also brought into alignment with each other.\nAfter aligning the photo(s) and model(s), our system allows a user, such as a\nproject manager or facility owner, to explore the construction site seamlessly\nin time, monitor the progress of construction, assess errors and deviations,\nand create photorealistic architectural visualizations. These interactions are\nfacilitated by automatic reasoning performed by our system: static and dynamic\nocclusions are removed automatically, rendering information is collected, and\nsemantic selection tools help guide user input. We also demonstrate that our\nuser-assisted SfM method outperforms existing techniques on both real-world\nconstruction data and established multi-view datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:57:22 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Karsch", "Kevin", ""], ["Golparvar-Fard", "Mani", ""], ["Forsyth", "David", ""]]}, {"id": "1912.11568", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, David Forsyth", "title": "Blind Recovery of Spatially Varying Reflectance from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique for estimating spatially varying parametric\nmaterials from a single image of an object with unknown shape in unknown\nillumination. Our method uses a low-order parametric reflectance model, and\nincorporates strong assumptions about lighting and shape. We develop new priors\nabout how materials mix over space, and jointly infer all of these properties\nfrom a single image. This produces a decomposition of an image which\ncorresponds, in one sense, to microscopic features (material reflectance) and\nmacroscopic features (weights defining the mixing properties of materials over\nspace). We have built a large dataset of real objects rendered with different\nmaterial models under different illumination fields for training and ground\ntruth evaluation. Extensive experiments on both our synthetic dataset images as\nwell as real images show that (a) our method recovers parameters with\nreasonable accuracy; (b) material parameters recovered by our method give\naccurate predictions of new renderings of the object; and (c) our low-order\nreflectance model still provides a good fit to many real-world reflectances.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 00:00:25 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Karsch", "Kevin", ""], ["Forsyth", "David", ""]]}, {"id": "1912.11616", "submitter": "Bin Liu", "authors": "Bin Liu, Xiuping Liu, Zhixin Yang, Charlie C.L. Wang", "title": "Concise and Effective Network for 3D Human Modeling from Orthogonal\n  Silhouettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit the problem of 3D human modeling from two\northogonal silhouettes of individuals (i.e., front and side views). Different\nfrom our prior work {\\cite{wang2003virtual}}, a supervised learning approach\nbased on \\textit{convolutional neural network} (CNN) is investigated to solve\nthe problem by establishing a mapping function that can effectively extract\nfeatures from two silhouettes and fuse them into coefficients in the shape\nspace of human bodies. A new CNN structure is proposed in our work to exact not\nonly the discriminative features of front and side views and also their mixed\nfeatures for the mapping function. 3D human models with high accuracy are\nsynthesized from coefficients generated by the mapping function. Existing CNN\napproaches for 3D human modeling usually learn a large number of parameters\n(from {8.5M} to {355.4M}) from two binary images. Differently, we investigate a\nnew network architecture and conduct the samples on silhouettes as input. As a\nconsequence, more accurate models can be generated by our network with only\n{2.4M} coefficients. The training of our network is conducted on samples\nobtained by augmenting a publicly accessible dataset. Learning transfer by\nusing datasets with a smaller number of scanned models is applied to our\nnetwork to enable the function of generating results with gender-oriented (or\ngeographical) patterns.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 08:05:37 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 02:52:13 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liu", "Bin", ""], ["Liu", "Xiuping", ""], ["Yang", "Zhixin", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "1912.11932", "submitter": "Vijai Thottathil Jayadevan", "authors": "Vijai Jayadevan, Edward Delp, and Zygmunt Pizlo", "title": "Skeleton Extraction from 3D Point Clouds by Decomposing the Object into\n  Parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing a point cloud into its components and extracting curve skeletons\nfrom point clouds are two related problems. Decomposition of a shape into its\ncomponents is often obtained as a byproduct of skeleton extraction. In this\nwork, we propose to extract curve skeletons, from unorganized point clouds, by\ndecomposing the object into its parts, identifying part skeletons and then\nlinking these part skeletons together to obtain the complete skeleton. We\nbelieve it is the most natural way to extract skeletons in the sense that this\nwould be the way a human would approach the problem. Our parts are generalized\ncylinders (GCs). Since, the axis of a GC is an integral part of its definition,\nthe parts have natural skeletal representations. We use translational symmetry,\nthe fundamental property of GCs, to extract parts from point clouds. We\ndemonstrate how this method can handle a large variety of shapes. We compare\nour method with state of the art methods and show how a part based approach can\ndeal with some of the limitations of other methods. We present an improved\nversion of an existing point set registration algorithm and demonstrate its\nutility in extracting parts from point clouds. We also show how this method can\nbe used to extract skeletons from and identify parts of noisy point clouds. A\npart based approach also provides a natural and intuitive interface for user\ninteraction. We demonstrate the ease with which mistakes, if any, can be fixed\nwith minimal user interaction with the help of a graphical user interface.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:52:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Jayadevan", "Vijai", ""], ["Delp", "Edward", ""], ["Pizlo", "Zygmunt", ""]]}, {"id": "1912.12098", "submitter": "Tolga Birdal", "authors": "Yongheng Zhao, Tolga Birdal, Jan Eric Lenssen, Emanuele Menegatti,\n  Leonidas Guibas, Federico Tombari", "title": "Quaternion Equivariant Capsule Networks for 3D Point Clouds", "comments": "Oral Presentation at ECCV 2020. Find our video under:\n  https://youtu.be/LHh56snwhTA. We release our sources at:\n  http://tolgabirdal.github.io/qecnetworks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D capsule module for processing point clouds that is\nequivariant to 3D rotations and translations, as well as invariant to\npermutations of the input points. The operator receives a sparse set of local\nreference frames, computed from an input point cloud and establishes end-to-end\ntransformation equivariance through a novel dynamic routing procedure on\nquaternions. Further, we theoretically connect dynamic routing between capsules\nto the well-known Weiszfeld algorithm, a scheme for solving \\emph{iterative\nre-weighted least squares} (IRLS) problems with provable convergence\nproperties. It is shown that such group dynamic routing can be interpreted as\nrobust IRLS rotation averaging on capsule votes, where information is routed\nbased on the final inlier scores. Based on our operator, we build a capsule\nnetwork that disentangles geometry from pose, paving the way for more\ninformative descriptors and a structured latent space. Our architecture allows\njoint object classification and orientation estimation without explicit\nsupervision of rotations. We validate our algorithm empirically on common\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 13:51:17 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 19:09:44 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 13:12:46 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhao", "Yongheng", ""], ["Birdal", "Tolga", ""], ["Lenssen", "Jan Eric", ""], ["Menegatti", "Emanuele", ""], ["Guibas", "Leonidas", ""], ["Tombari", "Federico", ""]]}, {"id": "1912.12297", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, Kalyan Sunkavalli, Sunil Hadap, Nathan Carr, Hailin Jin,\n  Rafael Fonte, Michael Sittig", "title": "Automatic Scene Inference for 3D Object Compositing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a user-friendly image editing system that supports a drag-and-drop\nobject insertion (where the user merely drags objects into the image, and the\nsystem automatically places them in 3D and relights them appropriately),\npost-process illumination editing, and depth-of-field manipulation. Underlying\nour system is a fully automatic technique for recovering a comprehensive 3D\nscene model (geometry, illumination, diffuse albedo and camera parameters) from\na single, low dynamic range photograph. This is made possible by two novel\ncontributions: an illumination inference algorithm that recovers a full\nlighting model of the scene (including light sources that are not directly\nvisible in the photograph), and a depth estimation algorithm that combines\ndata-driven depth transfer with geometric reasoning about the scene layout. A\nuser study shows that our system produces perceptually convincing results, and\nachieves the same level of realism as techniques that require significant user\ninteraction.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 23:43:52 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Karsch", "Kevin", ""], ["Sunkavalli", "Kalyan", ""], ["Hadap", "Sunil", ""], ["Carr", "Nathan", ""], ["Jin", "Hailin", ""], ["Fonte", "Rafael", ""], ["Sittig", "Michael", ""]]}, {"id": "1912.12763", "submitter": "Hemal Naik", "authors": "Hemal Naik, Renaud Bastien, Nassir Navab, Iain Couzin", "title": "Animals in Virtual Environments", "comments": "Conditional acceptance in IEEE TVCG", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics Feb 13\n  2020", "doi": "10.1109/TVCG.2020.2973063", "report-no": null, "categories": "cs.GR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core idea in an XR (VR/MR/AR) application is to digitally stimulate one\nor more sensory systems (e.g. visual, auditory, olfactory) of the human user in\nan interactive way to achieve an immersive experience. Since the early 2000s\nbiologists have been using Virtual Environments (VE) to investigate the\nmechanisms of behavior in non-human animals including insect, fish, and\nmammals. VEs have become reliable tools for studying vision, cognition, and\nsensory-motor control in animals. In turn, the knowledge gained from studying\nsuch behaviors can be harnessed by researchers designing biologically inspired\nrobots, smart sensors, and multi-agent artificial intelligence. VE for animals\nis becoming a widely used application of XR technology but such applications\nhave not previously been reported in the technical literature related to XR.\nBiologists and computer scientists can benefit greatly from deepening\ninterdisciplinary research in this emerging field and together we can develop\nnew methods for conducting fundamental research in behavioral sciences and\nengineering. To support our argument we present this review which provides an\noverview of animal behavior experiments conducted in virtual environments.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 00:14:57 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 18:39:18 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Naik", "Hemal", ""], ["Bastien", "Renaud", ""], ["Navab", "Nassir", ""], ["Couzin", "Iain", ""]]}, {"id": "1912.12786", "submitter": "Stefan Zellmann", "authors": "Stefan Zellmann", "title": "Adding Custom Intersectors to the C++ Ray Tracing Template Library\n  Visionaray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most ray tracing libraries allow the user to provide custom functionality\nthat is executed when a potential ray surface interaction was encountered to\ndetermine if the interaction was valid or traversal should be continued. This\nis e.g. useful for alpha mask validation and allows the user to reuse existing\nray object intersection routines rather than reimplementing them. Augmenting\nray traversal with custom intersection logic requires some kind of callback\nmechanism that injects user code into existing library routines. With template\nlibraries, this injection can happen statically since the user compiles the\nbinary code herself. We present an implementation of this \"custom intersector\"\napproach and its integration into the C++ ray tracing template library\nVisionaray.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 02:40:42 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Zellmann", "Stefan", ""]]}, {"id": "1912.13140", "submitter": "Jianhui Nie", "authors": "Jianhui Nie, Wenkai Shi, Ye Liu, Hao Gao, Feng Xu, Zhaochen Zhang,\n  Guoping Jiang", "title": "Bas-relief Generation from Point Clouds Based on Normal Space\n  Compression with Real-time Adjustment on CPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bas-relief generation based on 3d models is a hot topic in computer graphics.\nState-of-the-art algorithms take a mesh surface as input, but real-time\ninteraction via CPU cannot be realized. In this paper, a bas-relief generation\nalgorithm that takes a scattered point cloud as input is proposed. The\nalgorithm takes normal vectors as the operation object and the variation of the\nlocal surface as the compression criterion. By constructing and solving linear\nequations of bas-relief vertices, the closed-form solution can be obtained.\nSince there is no need to compute discrete gradients on a point cloud lacking\ntopology information, it is easier to implement and more intuitive than\ngradient domain methods. The algorithm provides parameters to adjust the\nbas-relief height, saturation and detail richness. At the same time, through\nthe solution strategy based on the subspace, it realizes the real-time\nadjustment of the bas-relief effect based on the computing power of a consumer\nCPU. In addition, an iterative solution to generate a bas-relief model of a\nspecified height is presented to meet specific application requirements.\nExperiments show that our algorithm provides a unified solution for various\ntypes of bas-relief creation and can generate bas-reliefs with good saturation\nand rich details.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 01:54:13 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Nie", "Jianhui", ""], ["Shi", "Wenkai", ""], ["Liu", "Ye", ""], ["Gao", "Hao", ""], ["Xu", "Feng", ""], ["Zhang", "Zhaochen", ""], ["Jiang", "Guoping", ""]]}, {"id": "1912.13243", "submitter": "Pavol Bielik", "authors": "Philippe Schlattner, Pavol Bielik, Martin Vechev", "title": "Learning to Infer User Interface Attributes from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a new domain of learning to infer user interface attributes that\nhelps developers automate the process of user interface implementation.\nConcretely, given an input image created by a designer, we learn to infer its\nimplementation which when rendered, looks visually the same as the input image.\nTo achieve this, we take a black box rendering engine and a set of attributes\nit supports (e.g., colors, border radius, shadow or text properties), use it to\ngenerate a suitable synthetic training dataset, and then train specialized\nneural models to predict each of the attribute values. To improve pixel-level\naccuracy, we additionally use imitation learning to train a neural policy that\nrefines the predicted attribute values by learning to compute the similarity of\nthe original and rendered images in their attribute space, rather than based on\nthe difference of pixel values. We instantiate our approach to the task of\ninferring Android Button attribute values and achieve 92.5% accuracy on a\ndataset consisting of real-world Google Play Store applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 09:45:59 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Schlattner", "Philippe", ""], ["Bielik", "Pavol", ""], ["Vechev", "Martin", ""]]}]