[{"id": "2012.00478", "submitter": "Dimas Mart\\'inez", "authors": "Victoria Hern\\'andez-Mederos, Dimas Mart\\'inez, Jorge\n  Estrada-Sarlabous and Valia Guerra-Ones", "title": "Farthest sampling segmentation of triangulated surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce Farthest Sampling Segmentation (FSS), a new method\nfor segmentation of triangulated surfaces, which consists of two fundamental\nsteps: the computation of a submatrix $W^k$ of the affinity matrix $W$ and the\napplication of the k-means clustering algorithm to the rows of $W^k$. The\nsubmatrix $W^k$ is obtained computing the affinity between all triangles and\nonly a few special triangles: those which are farthest in the defined metric.\nThis is equivalent to select a sample of columns of $W$ without constructing it\ncompletely. The proposed method is computationally cheaper than other\nsegmentation algorithms, since it only calculates few columns of $W$ and it\ndoes not require the eigendecomposition of $W$ or of any submatrix of $W$.\n  We prove that the orthogonal projection of $W$ on the space generated by the\ncolumns of $W^k$ coincides with the orthogonal projection of $W$ on the space\ngenerated by the $k$ eigenvectors computed by Nystr\\\"om's method using the\ncolumns of $W^k$ as a sample of $W$. Further, it is shown that for increasing\nsize $k$, the proximity relationship among the rows of $W^k$ tends to\nfaithfully reflect the proximity among the corresponding rows of $W$.\n  The FSS method does not depend on parameters that must be tuned by hand and\nit is very flexible, since it can handle any metric to define the distance\nbetween triangles. Numerical experiments with several metrics and a large\nvariety of 3D triangular meshes show that the segmentations obtained computing\nless than the 10% of columns $W$ are as good as those obtained from clustering\nthe rows of the full matrix $W$.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:31:44 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Hern\u00e1ndez-Mederos", "Victoria", ""], ["Mart\u00ednez", "Dimas", ""], ["Estrada-Sarlabous", "Jorge", ""], ["Guerra-Ones", "Valia", ""]]}, {"id": "2012.00595", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Martin R. Oswald, Vittorio Ferrari, Jiri Matas, Marc\n  Pollefeys", "title": "DeFMO: Deblurring and Shape Recovery of Fast Moving Objects", "comments": "CVPR 2021 camera-ready", "journal-ref": "2021 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objects moving at high speed appear significantly blurred when captured with\ncameras. The blurry appearance is especially ambiguous when the object has\ncomplex shape or texture. In such cases, classical methods, or even humans, are\nunable to recover the object's appearance and motion. We propose a method that,\ngiven a single image with its estimated background, outputs the object's\nappearance and position in a series of sub-frames as if captured by a\nhigh-speed camera (i.e. temporal super-resolution). The proposed generative\nmodel embeds an image of the blurred object into a latent space representation,\ndisentangles the background, and renders the sharp appearance. Inspired by the\nimage formation model, we design novel self-supervised loss function terms that\nboost performance and show good generalization capabilities. The proposed DeFMO\nmethod is trained on a complex synthetic dataset, yet it performs well on\nreal-world data from several datasets. DeFMO outperforms the state of the art\nand generates high-quality temporal super-resolution frames.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:02:04 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:02:41 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 09:14:34 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Oswald", "Martin R.", ""], ["Ferrari", "Vittorio", ""], ["Matas", "Jiri", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2012.00926", "submitter": "Marco Monteiro", "authors": "Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, Gordon\n  Wetzstein", "title": "pi-GAN: Periodic Implicit Generative Adversarial Networks for 3D-Aware\n  Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed rapid progress on 3D-aware image synthesis, leveraging\nrecent advances in generative visual models and neural rendering. Existing\napproaches however fall short in two ways: first, they may lack an underlying\n3D representation or rely on view-inconsistent rendering, hence synthesizing\nimages that are not multi-view consistent; second, they often depend upon\nrepresentation network architectures that are not expressive enough, and their\nresults thus lack in image quality. We propose a novel generative model, named\nPeriodic Implicit Generative Adversarial Networks ($\\pi$-GAN or pi-GAN), for\nhigh-quality 3D-aware image synthesis. $\\pi$-GAN leverages neural\nrepresentations with periodic activation functions and volumetric rendering to\nrepresent scenes as view-consistent 3D representations with fine detail. The\nproposed approach obtains state-of-the-art results for 3D-aware image synthesis\nwith multiple real and synthetic datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:57:46 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 23:18:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chan", "Eric R.", ""], ["Monteiro", "Marco", ""], ["Kellnhofer", "Petr", ""], ["Wu", "Jiajun", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2012.01044", "submitter": "Sebastian Bullinger", "authors": "Sebastian Bullinger, Christoph Bodensteiner, Michael Arens", "title": "A Photogrammetry-based Framework to Facilitate Image-based Modeling and\n  Automatic Camera Tracking", "comments": null, "journal-ref": "In Proceedings of the 16th International Joint Conference on\n  Computer Vision, Imaging and Computer Graphics Theory and Applications -\n  Volume 1: GRAPP, 106-112, 2021", "doi": "10.5220/0010319801060112", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework that extends Blender to exploit Structure from Motion\n(SfM) and Multi-View Stereo (MVS) techniques for image-based modeling tasks\nsuch as sculpting or camera and motion tracking. Applying SfM allows us to\ndetermine camera motions without manually defining feature tracks or\ncalibrating the cameras used to capture the image data. With MVS we are able to\nautomatically compute dense scene models, which is not feasible with the\nbuilt-in tools of Blender. Currently, our framework supports several\nstate-of-the-art SfM and MVS pipelines. The modular system design enables us to\nintegrate further approaches without additional effort. The framework is\npublicly available as an open source software package.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 09:26:37 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bullinger", "Sebastian", ""], ["Bodensteiner", "Christoph", ""], ["Arens", "Michael", ""]]}, {"id": "2012.01158", "submitter": "Oran Gafni", "authors": "Oran Gafni, Oron Ashual, Lior Wolf", "title": "Single-Shot Freestyle Dance Reenactment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of motion transfer between a source dancer and a target person is a\nspecial case of the pose transfer problem, in which the target person changes\ntheir pose in accordance with the motions of the dancer.\n  In this work, we propose a novel method that can reanimate a single image by\narbitrary video sequences, unseen during training. The method combines three\nnetworks: (i) a segmentation-mapping network, (ii) a realistic frame-rendering\nnetwork, and (iii) a face refinement network. By separating this task into\nthree stages, we are able to attain a novel sequence of realistic frames,\ncapturing natural motion and appearance. Our method obtains significantly\nbetter visual quality than previous methods and is able to animate diverse body\ntypes and appearances, which are captured in challenging poses, as shown in the\nexperiments and supplementary video.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 12:57:43 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 14:11:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gafni", "Oran", ""], ["Ashual", "Oron", ""], ["Wolf", "Lior", ""]]}, {"id": "2012.01203", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Paul Guerrero, Noam Aigerman, Niloy Mitra,\n  Maks Ovsjanikov", "title": "Learning Delaunay Surface Elements for Mesh Reconstruction", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for reconstructing triangle meshes from point clouds.\nExisting learning-based methods for mesh reconstruction mostly generate\ntriangles individually, making it hard to create manifold meshes. We leverage\nthe properties of 2D Delaunay triangulations to construct a mesh from manifold\nsurface elements. Our method first estimates local geodesic neighborhoods\naround each point. We then perform a 2D projection of these neighborhoods using\na learned logarithmic map. A Delaunay triangulation in this 2D domain is\nguaranteed to produce a manifold patch, which we call a Delaunay surface\nelement. We synchronize the local 2D projections of neighboring elements to\nmaximize the manifoldness of the reconstructed mesh. Our results show that we\nachieve better overall manifoldness of our reconstructed meshes than current\nmethods to reconstruct meshes with arbitrary topology. Our code, data and\npretrained models can be found online:\nhttps://github.com/mrakotosaon/dse-meshing\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:42:07 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 17:17:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["Guerrero", "Paul", ""], ["Aigerman", "Noam", ""], ["Mitra", "Niloy", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2012.01451", "submitter": "Aljaz Bozic", "authors": "Alja\\v{z} Bo\\v{z}i\\v{c}, Pablo Palafox, Michael Zollh\\\"ofer, Justus\n  Thies, Angela Dai, Matthias Nie{\\ss}ner", "title": "Neural Deformation Graphs for Globally-consistent Non-rigid\n  Reconstruction", "comments": "Video: https://youtu.be/vyq36eFkdWo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Deformation Graphs for globally-consistent deformation\ntracking and 3D reconstruction of non-rigid objects. Specifically, we\nimplicitly model a deformation graph via a deep neural network. This neural\ndeformation graph does not rely on any object-specific structure and, thus, can\nbe applied to general non-rigid deformation tracking. Our method globally\noptimizes this neural graph on a given sequence of depth camera observations of\na non-rigidly moving object. Based on explicit viewpoint consistency as well as\ninter-frame graph and surface consistency constraints, the underlying network\nis trained in a self-supervised fashion. We additionally optimize for the\ngeometry of the object with an implicit deformable multi-MLP shape\nrepresentation. Our approach does not assume sequential input data, thus\nenabling robust tracking of fast motions or even temporally disconnected\nrecordings. Our experiments demonstrate that our Neural Deformation Graphs\noutperform state-of-the-art non-rigid reconstruction approaches both\nqualitatively and quantitatively, with 64% improved reconstruction and 62%\nimproved deformation tracking performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 19:00:13 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Bo\u017ei\u010d", "Alja\u017e", ""], ["Palafox", "Pablo", ""], ["Zollh\u00f6fer", "Michael", ""], ["Thies", "Justus", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.01714", "submitter": "David Lindell", "authors": "David B. Lindell, Julien N. P. Martel, Gordon Wetzstein", "title": "AutoInt: Automatic Integration for Fast Neural Volume Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical integration is a foundational technique in scientific computing and\nis at the core of many computer vision applications. Among these applications,\nneural volume rendering has recently been proposed as a new paradigm for view\nsynthesis, achieving photorealistic image quality. However, a fundamental\nobstacle to making these methods practical is the extreme computational and\nmemory requirements caused by the required volume integrations along the\nrendered rays during training and inference. Millions of rays, each requiring\nhundreds of forward passes through a neural network are needed to approximate\nthose integrations with Monte Carlo sampling. Here, we propose automatic\nintegration, a new framework for learning efficient, closed-form solutions to\nintegrals using coordinate-based neural networks. For training, we instantiate\nthe computational graph corresponding to the derivative of the network. The\ngraph is fitted to the signal to integrate. After optimization, we reassemble\nthe graph to obtain a network that represents the antiderivative. By the\nfundamental theorem of calculus, this enables the calculation of any definite\nintegral in two evaluations of the network. Applying this approach to neural\nrendering, we improve a tradeoff between rendering speed and image quality:\nimproving render times by greater than 10 times with a tradeoff of slightly\nreduced image quality.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 05:46:10 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 02:23:47 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lindell", "David B.", ""], ["Martel", "Julien N. P.", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2012.02087", "submitter": "Mohamed Sayed", "authors": "Mohamed Sayed, Robert Cinca, Enrico Costanza, Gabriel Brostow", "title": "LookOut! Interactive Camera Gimbal Controller for Filming Long Takes", "comments": "V2: - Fixed typos. - Cleaner supplemental. - New plot in control\n  section with same data from a supplemental video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The job of a camera operator is more challenging, and potentially dangerous,\nwhen filming long moving camera shots. Broadly, the operator must keep the\nactors in-frame while safely navigating around obstacles, and while fulfilling\nan artistic vision. We propose a unified hardware and software system that\ndistributes some of the camera operator's burden, freeing them up to focus on\nsafety and aesthetics during a take. Our real-time system provides a solo\noperator with end-to-end control, so they can balance on-set responsiveness to\naction vs planned storyboards and framing, while looking where they're going.\nBy default, we film without a field monitor.\n  Our LookOut system is built around a lightweight commodity camera gimbal\nmechanism, with heavy modifications to the controller, which would normally\njust provide active stabilization. Our control algorithm reacts to speech\ncommands, video, and a pre-made script. Specifically, our automatic monitoring\nof the live video feed saves the operator from distractions. In pre-production,\nan artist uses our GUI to design a sequence of high-level camera \"behaviors.\"\nThose can be specific, based on a storyboard, or looser objectives, such as\n\"frame both actors.\" Then during filming, a machine-readable script, exported\nfrom the GUI, ties together with the sensor readings to drive the gimbal. To\nvalidate our algorithm, we compared tracking strategies, interfaces, and\nhardware protocols, and collected impressions from a) film-makers who used all\naspects of our system, and b) film-makers who watched footage filmed using\nLookOut.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:20:45 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 22:04:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sayed", "Mohamed", ""], ["Cinca", "Robert", ""], ["Costanza", "Enrico", ""], ["Brostow", "Gabriel", ""]]}, {"id": "2012.02135", "submitter": "Li-Yi Wei", "authors": "Li-Yi Wei, Arjun V Anand, Shally Kumar, Tarun Beri", "title": "Simple Methods to Represent Shapes with Sample Spheres", "comments": "SIGGRAPH Asia 2020 Technical Communications", "journal-ref": null, "doi": "10.1145/3410700.3425424", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representing complex shapes with simple primitives in high accuracy is\nimportant for a variety of applications in computer graphics and geometry\nprocessing. Existing solutions may produce suboptimal samples or are complex to\nimplement. We present methods to approximate given shapes with user-tunable\nnumber of spheres to balance between accuracy and simplicity: touching\nmedial/scale-axis polar balls and k-means smallest enclosing circles. Our\nmethods are easy to implement, run efficiently, and can approach quality\nsimilar to manual construction.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:14:30 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Wei", "Li-Yi", ""], ["Anand", "Arjun V", ""], ["Kumar", "Shally", ""], ["Beri", "Tarun", ""]]}, {"id": "2012.02190", "submitter": "Alex Yu", "authors": "Alex Yu, Vickie Ye, Matthew Tancik, Angjoo Kanazawa", "title": "pixelNeRF: Neural Radiance Fields from One or Few Images", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose pixelNeRF, a learning framework that predicts a continuous neural\nscene representation conditioned on one or few input images. The existing\napproach for constructing neural radiance fields involves optimizing the\nrepresentation to every scene independently, requiring many calibrated views\nand significant compute time. We take a step towards resolving these\nshortcomings by introducing an architecture that conditions a NeRF on image\ninputs in a fully convolutional manner. This allows the network to be trained\nacross multiple scenes to learn a scene prior, enabling it to perform novel\nview synthesis in a feed-forward manner from a sparse set of views (as few as\none). Leveraging the volume rendering approach of NeRF, our model can be\ntrained directly from images with no explicit 3D supervision. We conduct\nextensive experiments on ShapeNet benchmarks for single image novel view\nsynthesis tasks with held-out objects as well as entire unseen categories. We\nfurther demonstrate the flexibility of pixelNeRF by demonstrating it on\nmulti-object ShapeNet scenes and real scenes from the DTU dataset. In all\ncases, pixelNeRF outperforms current state-of-the-art baselines for novel view\nsynthesis and single image 3D reconstruction. For the video and code, please\nvisit the project website: https://alexyu.net/pixelnerf\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:59:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 05:41:26 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 17:52:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yu", "Alex", ""], ["Ye", "Vickie", ""], ["Tancik", "Matthew", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2012.02224", "submitter": "Funda Durupinar Babur", "authors": "Funda Durupinar", "title": "Personality-Driven Gaze Animation with Conditional Generative\n  Adversarial Networks", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generative adversarial learning approach to synthesize gaze\nbehavior of a given personality. We train the model using an existing data set\nthat comprises eye-tracking data and personality traits of 42 participants\nperforming an everyday task. Given the values of Big-Five personality traits\n(openness, conscientiousness, extroversion, agreeableness, and neuroticism),\nour model generates time series data consisting of gaze target, blinking times,\nand pupil dimensions. We use the generated data to synthesize the gaze motion\nof virtual agents on a game engine.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 00:31:45 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Durupinar", "Funda", ""]]}, {"id": "2012.02344", "submitter": "Vassillen Chizhov", "authors": "Vassillen Chizhov, Iliyan Georgiev, Karol Myszkowski, Gurprit Singh", "title": "Perceptual error optimization for Monte Carlo rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic image synthesis involves computing high-dimensional light transport\nintegrals which in practice are numerically estimated using Monte Carlo\nintegration. The error of this estimation manifests itself in the image as\nvisually displeasing aliasing or noise. To ameliorate this, we develop a\ntheoretical framework for optimizing screen-space error distribution. Our model\nis flexible and works for arbitrary target error power spectra. We focus on\nperceptual error optimization by leveraging models of the human visual system's\n(HVS) point spread function (PSF) from halftoning literature. This results in a\nspecific optimization problem whose solution distributes the error as visually\npleasing blue noise in image space. We develop a set of algorithms that provide\na trade-off between quality and speed, showing substantial improvements over\nprior state of the art. We perform evaluations using both quantitative and\nperceptual error metrics to support our analysis, and provide extensive\nsupplemental material to help evaluate the perceptual improvements achieved by\nour methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 00:30:45 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 16:04:18 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 12:16:31 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 17:52:16 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chizhov", "Vassillen", ""], ["Georgiev", "Iliyan", ""], ["Myszkowski", "Karol", ""], ["Singh", "Gurprit", ""]]}, {"id": "2012.02346", "submitter": "Takashi Matsubara", "authors": "Takumi Kimura, Takashi Matsubara, Kuniaki Uehara", "title": "ChartPointFlow for Topology-Aware 3D Point Cloud Generation", "comments": "9 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point cloud serves as a representation of the surface of a\nthree-dimensional shape. Deep generative models have been adapted to model\ntheir variations typically by a map from a ball-like set of latent variables.\nHowever, previous approaches have not paid much attention to the topological\nstructure of a point cloud; a continuous map cannot express the varying number\nof holes and intersections. Moreover, a point cloud is often composed of\nmultiple subparts, and it is also hardly expressed. In this paper, we propose\nChartPointFlow, which is a flow-based generative model with multiple latent\nlabels. By maximizing the mutual information, a map conditioned by a label is\nassigned to a continuous subset of a given point cloud, like a chart of a\nmanifold. This enables our proposed model to preserve the topological structure\nwith clear boundaries, while previous approaches tend to suffer from blurs and\nto fail in generating holes. Experimental results demonstrate that\nChartPointFlow achieves the state-of-the-art performance in generation and\nreconstruction among sampling-based point cloud generators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 00:49:25 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kimura", "Takumi", ""], ["Matsubara", "Takashi", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "2012.02459", "submitter": "Jie Yang", "authors": "Jie Yang, Lin Gao, Qingyang Tan, Yihua Huang, Shihong Xia and Yu-Kun\n  Lai", "title": "Multiscale Mesh Deformation Component Analysis with Attention-based\n  Autoencoders", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation component analysis is a fundamental problem in geometry\nprocessing and shape understanding. Existing approaches mainly extract\ndeformation components in local regions at a similar scale while deformations\nof real-world objects are usually distributed in a multi-scale manner. In this\npaper, we propose a novel method to exact multiscale deformation components\nautomatically with a stacked attention-based autoencoder. The attention\nmechanism is designed to learn to softly weight multi-scale deformation\ncomponents in active deformation regions, and the stacked attention-based\nautoencoder is learned to represent the deformation components at different\nscales. Quantitative and qualitative evaluations show that our method\noutperforms state-of-the-art methods. Furthermore, with the multiscale\ndeformation components extracted by our method, the user can edit shapes in a\ncoarse-to-fine fashion which facilitates effective modeling of new shapes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 08:30:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Yang", "Jie", ""], ["Gao", "Lin", ""], ["Tan", "Qingyang", ""], ["Huang", "Yihua", ""], ["Xia", "Shihong", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2012.02596", "submitter": "Alex Deakyne", "authors": "Alex J. Deakyne, Erik N. Gaasedelen, Tinen L. Iles, and Paul A. Iaizzo", "title": "Immersive Anatomical Scenes that Enable Multiple Users to Occupy the\n  Same Virtual Space: A Tool for Surgical Planning and Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D modeling is becoming a well-developed field of medicine, but its\napplicability can be limited due to the lack of software allowing for easy\nutilizations of generated 3D visualizations. By leveraging recent advances in\nvirtual reality, we can rapidly create immersive anatomical scenes as well as\nallow multiple users to occupy the same virtual space: i.e., over a local or\ndistributed network. This setup is ideal for pre-surgical planning and\neducation, allowing users to identify and study structures of interest. I\ndemonstrate here such a pipeline on a broad spectrum of anatomical models and\ndiscuss its applicability to the medical field and its future prospects.3D\nmodeling is becoming a well-developed field of medicine, but its applicability\ncan be limited due to the lack of software allowing for easy utilizations of\ngenerated 3D visualizations. By leveraging recent advances in virtual reality,\nwe can rapidly create immersive anatomical scenes as well as allow multiple\nusers to occupy the same virtual space: i.e., over a local or distributed\nnetwork. This setup is ideal for pre-surgical planning and education, allowing\nusers to identify and study structures of interest. I demonstrate here such a\npipeline on a broad spectrum of anatomical models and discuss its applicability\nto the medical field and its future prospects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:57:06 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Deakyne", "Alex J.", ""], ["Gaasedelen", "Erik N.", ""], ["Iles", "Tinen L.", ""], ["Iaizzo", "Paul A.", ""]]}, {"id": "2012.03065", "submitter": "Guy Gafni", "authors": "Guy Gafni, Justus Thies, Michael Zollh\\\"ofer, Matthias Nie{\\ss}ner", "title": "Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar\n  Reconstruction", "comments": "Video: https://youtu.be/m7oROLdQnjk | Project page:\n  https://gafniguy.github.io/4D-Facial-Avatars/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present dynamic neural radiance fields for modeling the appearance and\ndynamics of a human face. Digitally modeling and reconstructing a talking human\nis a key building-block for a variety of applications. Especially, for\ntelepresence applications in AR or VR, a faithful reproduction of the\nappearance including novel viewpoints or head-poses is required. In contrast to\nstate-of-the-art approaches that model the geometry and material properties\nexplicitly, or are purely image-based, we introduce an implicit representation\nof the head based on scene representation networks. To handle the dynamics of\nthe face, we combine our scene representation network with a low-dimensional\nmorphable model which provides explicit control over pose and expressions. We\nuse volumetric rendering to generate images from this hybrid representation and\ndemonstrate that such a dynamic neural scene representation can be learned from\nmonocular input data only, without the need of a specialized capture setup. In\nour experiments, we show that this learned volumetric representation allows for\nphoto-realistic image generation that surpasses the quality of state-of-the-art\nvideo-based reenactment methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 16:01:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Gafni", "Guy", ""], ["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2012.03229", "submitter": "Deepesh Toshniwal", "authors": "Hendrik Speleers and Deepesh Toshniwal", "title": "A general class of $C^1$ smooth rational splines: Application to\n  construction of exact ellipses and ellipsoids", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe a general class of $C^1$ smooth rational splines\nthat enables, in particular, exact descriptions of ellipses and ellipsoids -\nsome of the most important primitives for CAD and CAE. The univariate rational\nsplines are assembled by transforming multiple sets of NURBS basis functions\nvia so-called design-through-analysis compatible extraction matrices; different\nsets of NURBS are allowed to have different polynomial degrees and weight\nfunctions. Tensor products of the univariate splines yield multivariate\nsplines. In the bivariate setting, we describe how similar\ndesign-through-analysis compatible transformations of the tensor-product\nsplines enable the construction of smooth surfaces containing one or two polar\nsingularities. The material is self-contained, and is presented such that all\ntools can be easily implemented by CAD or CAE practitioners within existing\nsoftware that support NURBS. To this end, we explicitly present the matrices\n(a) that describe our splines in terms of NURBS, and (b) that help refine the\nsplines by performing (local) degree elevation and knot insertion. Finally, all\n$C^1$ spline constructions yield spline basis functions that are locally\nsupported and form a convex partition of unity.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 10:34:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Speleers", "Hendrik", ""], ["Toshniwal", "Deepesh", ""]]}, {"id": "2012.03325", "submitter": "Radu Alexandru Rosu", "authors": "Radu Alexandru Rosu and Sven Behnke", "title": "EasyPBR: A Lightweight Physically-Based Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern rendering libraries provide unprecedented realism, producing real-time\nphotorealistic 3D graphics on commodity hardware. Visual fidelity, however,\ncomes at the cost of increased complexity and difficulty of usage, with many\nrendering parameters requiring a deep understanding of the pipeline. We propose\nEasyPBR as an alternative rendering library that strikes a balance between\nease-of-use and visual quality. EasyPBR consists of a deferred renderer that\nimplements recent state-of-the-art approaches in physically based rendering. It\noffers an easy-to-use Python and C++ interface that allows high-quality images\nto be created in only a few lines of code or directly through a graphical user\ninterface. The user can choose between fully controlling the rendering pipeline\nor letting EasyPBR automatically infer the best parameters based on the current\nscene composition. The EasyPBR library can help the community to more easily\nleverage the power of current GPUs to create realistic images. These can then\nbe used as synthetic data for deep learning or for creating animations for\nacademic purposes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:06:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rosu", "Radu Alexandru", ""], ["Behnke", "Sven", ""]]}, {"id": "2012.03918", "submitter": "Mark Boss", "authors": "Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Barron, Ce Liu,\n  Hendrik P.A. Lensch", "title": "NeRD: Neural Reflectance Decomposition from Image Collections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing a scene into its shape, reflectance, and illumination is a\nchallenging but essential problem in computer vision and graphics. This problem\nis inherently more challenging when the illumination is not a single light\nsource under laboratory conditions but is instead an unconstrained\nenvironmental illumination. Though recent work has shown that implicit\nrepresentations can be used to model the radiance field of an object, these\ntechniques only enable view synthesis and not relighting. Additionally,\nevaluating these radiance fields is resource and time-intensive. By decomposing\na scene into explicit representations, any rendering framework can be leveraged\nto generate novel views under any illumination in real-time. NeRD is a method\nthat achieves this decomposition by introducing physically-based rendering to\nneural radiance fields. Even challenging non-Lambertian reflectances, complex\ngeometry, and unknown illumination can be decomposed into high-quality models.\nThe datasets and code is available on the project page:\nhttps://markboss.me/publication/2021-nerd/\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:45:57 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 15:48:18 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 07:39:00 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Boss", "Mark", ""], ["Braun", "Raphael", ""], ["Jampani", "Varun", ""], ["Barron", "Jonathan T.", ""], ["Liu", "Ce", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "2012.03927", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan and Boyang Deng and Xiuming Zhang and Matthew\n  Tancik and Ben Mildenhall and Jonathan T. Barron", "title": "NeRV: Neural Reflectance and Visibility Fields for Relighting and View\n  Synthesis", "comments": "Project page: https://people.eecs.berkeley.edu/~pratul/nerv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that takes as input a set of images of a scene\nilluminated by unconstrained known lighting, and produces as output a 3D\nrepresentation that can be rendered from novel viewpoints under arbitrary\nlighting conditions. Our method represents the scene as a continuous volumetric\nfunction parameterized as MLPs whose inputs are a 3D location and whose outputs\nare the following scene properties at that input location: volume density,\nsurface normal, material parameters, distance to the first surface intersection\nin any direction, and visibility of the external environment in any direction.\nTogether, these allow us to render novel views of the object under arbitrary\nlighting, including indirect illumination effects. The predicted visibility and\nsurface intersection fields are critical to our model's ability to simulate\ndirect and indirect illumination during training, because the brute-force\ntechniques used by prior work are intractable for lighting conditions outside\nof controlled setups with a single light. Our method outperforms alternative\napproaches for recovering relightable 3D scene representations, and performs\nwell in complex lighting settings that have posed a significant challenge to\nprior work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:56:08 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Deng", "Boyang", ""], ["Zhang", "Xiuming", ""], ["Tancik", "Matthew", ""], ["Mildenhall", "Ben", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2012.03939", "submitter": "Purvi Goel", "authors": "Purvi Goel, Loudon Cohen, James Guesman, Vikas Thamizharasan, James\n  Tompkin, Daniel Ritchie", "title": "Shape From Tracing: Towards Reconstructing 3D Object Geometry and SVBRDF\n  Material from Images via Differentiable Path Tracing", "comments": "Will be published at 3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing object geometry and material from multiple views typically\nrequires optimization. Differentiable path tracing is an appealing framework as\nit can reproduce complex appearance effects. However, it is difficult to use\ndue to high computational cost. In this paper, we explore how to use\ndifferentiable ray tracing to refine an initial coarse mesh and per-mesh-facet\nmaterial representation. In simulation, we find that it is possible to\nreconstruct fine geometric and material detail from low resolution input views,\nallowing high-quality reconstructions in a few hours despite the expense of\npath tracing. The reconstructions successfully disambiguate shading, shadow,\nand global illumination effects such as diffuse interreflection from material\nproperties. We demonstrate the impact of different geometry initializations,\nincluding space carving, multi-view stereo, and 3D neural networks. Finally,\nwith input captured using smartphone video and a consumer 360? camera for\nlighting estimation, we also show how to refine initial reconstructions of\nreal-world objects in unconstrained environments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 18:55:35 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Goel", "Purvi", ""], ["Cohen", "Loudon", ""], ["Guesman", "James", ""], ["Thamizharasan", "Vikas", ""], ["Tompkin", "James", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2012.04457", "submitter": "Minchen Li", "authors": "Minchen Li, Danny M. Kaufman, Chenfanfu Jiang", "title": "Codimensional Incremental Potential Contact", "comments": null, "journal-ref": null, "doi": "10.1145/3450626.3459767", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the incremental potential contact (IPC) model for contacting\nelastodynamics to resolve systems composed of codimensional DOFs in arbitrary\ncombination. This enables a unified, interpenetration-free, robust, and stable\nsimulation framework that couples codimension-0,1,2, and 3 geometries\nseamlessly with frictional contact. Extending IPC to thin structures poses new\nchallenges in computing strain, modeling thickness and determining collisions.\nTo address these challenges we propose three corresponding contributions.\nFirst, we introduce a C2 constitutive barrier model that directly enforces\nstrain limiting as an energy potential while preserving rest state. This\nprovides energetically-consistent strain limiting models (both isotropic and\nanisotropic) for cloth that enable strict satisfaction of strain-limit\ninequalities with direct coupling to both elastodynamics and contact via\nminimization of the incremental potential. Second, to capture the geometric\nthickness of codimensional domains we extend the IPC model to directly enforce\ndistance offsets. Our treatment imposes a strict guarantee that mid-surfaces\n(resp. mid-lines) of shells (resp. rods) will not move closer than applied\nthickness values. This enables us to account for thickness in the contact\nbehavior of codimensional structures and so robustly capture challenging\ncontacting geometries; a number of which, to our knowledge, have not been\nsimulated before. Third, codimensional models, especially with modeled\nthickness, mandate strict accuracy requirements that pose a severe challenge to\nall existing continuous collision detection (CCD) methods. To address these\nlimitations we develop a new, efficient, simple-to-implement additive CCD\n(ACCD) method that applies conservative advancement to iteratively refine a\nlower bound for deforming primitives, converging to time of impact.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 13:26:52 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 21:14:56 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 22:49:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Li", "Minchen", ""], ["Kaufman", "Danny M.", ""], ["Jiang", "Chenfanfu", ""]]}, {"id": "2012.04644", "submitter": "Dongdong Chen", "authors": "Zhentao Tan and Dongdong Chen and Qi Chu and Menglei Chai and Jing\n  Liao and Mingming He and Lu Yuan and Gang Hua and Nenghai Yu", "title": "Efficient Semantic Image Synthesis via Class-Adaptive Normalization", "comments": "To appear at TPAMI 2021, code is available\n  https://github.com/tzt101/CLADE.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-adaptive normalization (SPADE) is remarkably successful recently in\nconditional semantic image synthesis \\cite{park2019semantic}, which modulates\nthe normalized activation with spatially-varying transformations learned from\nsemantic layouts, to prevent the semantic information from being washed away.\nDespite its impressive performance, a more thorough understanding of the\nadvantages inside the box is still highly demanded to help reduce the\nsignificant computation and parameter overhead introduced by this novel\nstructure. In this paper, from a return-on-investment point of view, we conduct\nan in-depth analysis of the effectiveness of this spatially-adaptive\nnormalization and observe that its modulation parameters benefit more from\nsemantic-awareness rather than spatial-adaptiveness, especially for\nhigh-resolution input masks. Inspired by this observation, we propose\nclass-adaptive normalization (CLADE), a lightweight but equally-effective\nvariant that is only adaptive to semantic class. In order to further improve\nspatial-adaptiveness, we introduce intra-class positional map encoding\ncalculated from semantic layouts to modulate the normalization parameters of\nCLADE and propose a truly spatially-adaptive variant of CLADE, namely\nCLADE-ICPE.Through extensive experiments on multiple challenging datasets, we\ndemonstrate that the proposed CLADE can be generalized to different SPADE-based\nmethods while achieving comparable generation quality compared to SPADE, but it\nis much more efficient with fewer extra parameters and lower computational\ncost. The code and pretrained models are available at\n\\url{https://github.com/tzt101/CLADE.git}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:59:32 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 23:20:35 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tan", "Zhentao", ""], ["Chen", "Dongdong", ""], ["Chu", "Qi", ""], ["Chai", "Menglei", ""], ["Liao", "Jing", ""], ["He", "Mingming", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""]]}, {"id": "2012.05348", "submitter": "Toni Tan", "authors": "Toni Tan, Rene Weller and Gabriel Zachmann", "title": "Compressed Bounding Volume Hierarchies for Collision Detection &\n  Proximity Query", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel representation of compressed data structure for\nsimultaneous bounding volume hierarchy (BVH) traversals like they appear for\ninstance in collision detection & proximity query. The main idea is to compress\nbounding volume (BV) descriptors and cluster BVH into a smaller parts 'treelet'\nthat fit into CPU cache while at the same time maintain random-access and\nautomatic cache-aware data structure layouts. To do that, we quantify BV and\ncompress 'treelet' using predictor-corrector scheme with the predictor at a\nspecific node in the BVH based on the chain of BVs upwards.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:36:29 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Tan", "Toni", ""], ["Weller", "Rene", ""], ["Zachmann", "Gabriel", ""]]}, {"id": "2012.05536", "submitter": "Radu Horaud P", "authors": "Andrei Zaharescu, Edmond Boyer, and Radu Horaud", "title": "Topology-Adaptive Mesh Deformation for Surface Evolution, Morphing, and\n  Multi-View Reconstruction", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  33(4), April 2011", "doi": "10.1109/TPAMI.2010.116", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Triangulated meshes have become ubiquitous discrete-surface representations.\nIn this paper we address the problem of how to maintain the manifold properties\nof a surface while it undergoes strong deformations that may cause topological\nchanges. We introduce a new self-intersection removal algorithm, TransforMesh,\nand we propose a mesh evolution framework based on this algorithm. Numerous\nshape modelling applications use surface evolution in order to improve shape\nproperties, such as appearance or accuracy. Both explicit and implicit\nrepresentations can be considered for that purpose. However, explicit mesh\nrepresentations, while allowing for accurate surface modelling, suffer from the\ninherent difficulty of reliably dealing with self-intersections and topological\nchanges such as merges and splits. As a consequence, a majority of methods rely\non implicit representations of surfaces, e.g. level-sets, that naturally\novercome these issues. Nevertheless, these methods are based on volumetric\ndiscretizations, which introduce an unwanted precision-complexity trade-off.\nThe method that we propose handles topological changes in a robust manner and\nremoves self intersections, thus overcoming the traditional limitations of\nmesh-based approaches. To illustrate the effectiveness of TransforMesh, we\ndescribe two challenging applications, namely surface morphing and 3-D\nreconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:26:40 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zaharescu", "Andrei", ""], ["Boyer", "Edmond", ""], ["Horaud", "Radu", ""]]}, {"id": "2012.05551", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, Shi-Sheng Huang, Haoxuan Song, Shi-Min Hu", "title": "DI-Fusion: Online Implicit 3D Reconstruction with Deep Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous online 3D dense reconstruction methods struggle to achieve the\nbalance between memory storage and surface quality, largely due to the usage of\nstagnant underlying geometry representation, such as TSDF (truncated signed\ndistance functions) or surfels, without any knowledge of the scene priors. In\nthis paper, we present DI-Fusion (Deep Implicit Fusion), based on a novel 3D\nrepresentation, i.e. Probabilistic Local Implicit Voxels (PLIVoxs), for online\n3D reconstruction with a commodity RGB-D camera. Our PLIVox encodes scene\npriors considering both the local geometry and uncertainty parameterized by a\ndeep neural network. With such deep priors, we are able to perform online\nimplicit 3D reconstruction achieving state-of-the-art camera trajectory\nestimation accuracy and mapping quality, while achieving better storage\nefficiency compared with previous online 3D reconstruction approaches. Our\nimplementation is available at https://www.github.com/huangjh-pub/di-fusion.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 09:46:35 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:51:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Jiahui", ""], ["Huang", "Shi-Sheng", ""], ["Song", "Haoxuan", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2012.05694", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Lookahead optimizer improves the performance of Convolutional\n  Autoencoders for reconstruction of natural images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders are a class of artificial neural networks which have gained a\nlot of attention in the recent past. Using the encoder block of an autoencoder\nthe input image can be compressed into a meaningful representation. Then a\ndecoder is employed to reconstruct the compressed representation back to a\nversion which looks like the input image. It has plenty of applications in the\nfield of data compression and denoising. Another version of Autoencoders (AE)\nexist, called Variational AE (VAE) which acts as a generative model like GAN.\nRecently, an optimizer was introduced which is known as lookahead optimizer\nwhich significantly enhances the performances of Adam as well as SGD. In this\npaper, we implement Convolutional Autoencoders (CAE) and Convolutional\nVariational Autoencoders (CVAE) with lookahead optimizer (with Adam) and\ncompare them with the Adam (only) optimizer counterparts. For this purpose, we\nhave used a movie dataset comprising of natural images for the former case and\nCIFAR100 for the latter case. We show that lookahead optimizer (with Adam)\nimproves the performance of CAEs for reconstruction of natural images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 03:18:28 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "2012.06434", "submitter": "Wang Yifan", "authors": "Wang Yifan, Shihao Wu, Cengiz Oztireli, Olga Sorkine-Hornung", "title": "Iso-Points: Optimizing Neural Implicit Surfaces with Hybrid\n  Representations", "comments": "CVPR 2021 code: https://github.com/yifita/iso-points", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural implicit functions have emerged as a powerful representation for\nsurfaces in 3D. Such a function can encode a high quality surface with\nintricate details into the parameters of a deep neural network. However,\noptimizing for the parameters for accurate and robust reconstructions remains a\nchallenge, especially when the input data is noisy or incomplete. In this work,\nwe develop a hybrid neural surface representation that allows us to impose\ngeometry-aware sampling and regularization, which significantly improves the\nfidelity of reconstructions. We propose to use \\emph{iso-points} as an explicit\nrepresentation for a neural implicit function. These points are computed and\nupdated on-the-fly during training to capture important geometric features and\nimpose geometric constraints on the optimization. We demonstrate that our\nmethod can be adopted to improve state-of-the-art techniques for reconstructing\nneural implicit surfaces from multi-view images or point clouds. Quantitative\nand qualitative evaluations show that, compared with existing sampling and\noptimization methods, our approach allows faster convergence, better\ngeneralization, and accurate recovery of details and topology.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:51:04 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 20:11:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yifan", "Wang", ""], ["Wu", "Shihao", ""], ["Oztireli", "Cengiz", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2012.07959", "submitter": "Li-Yi Wei", "authors": "Peihan Tu, Li-Yi Wei, Koji Yatani, Takeo Igarashi, Matthias Zwicker", "title": "Continuous Curve Textures", "comments": null, "journal-ref": null, "doi": "10.1145/3414685.3417780", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Repetitive patterns are ubiquitous in natural and human-made objects, and can\nbe created with a variety of tools and methods. Manual authoring provides\nunmatched degree of freedom and control, but can require significant artistic\nexpertise and manual labor. Computational methods can automate parts of the\nmanual creation process, but are mainly tailored for discrete pixels or\nelements instead of more general continuous structures. We propose an\nexample-based method to synthesize continuous curve patterns from exemplars.\nOur main idea is to extend prior sample-based discrete element synthesis\nmethods to consider not only sample positions (geometry) but also their\nconnections (topology). Since continuous structures can exhibit higher\ncomplexity than discrete elements, we also propose robust, hierarchical\nsynthesis to enhance output quality. Our algorithm can generate a variety of\ncontinuous curve patterns fully automatically. For further quality improvement\nand customization, we also present an autocomplete user interface to facilitate\ninteractive creation and iterative editing. We evaluate our methods and\ninterface via different patterns, ablation studies, and comparisons with\nalternative methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 21:51:17 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Tu", "Peihan", ""], ["Wei", "Li-Yi", ""], ["Yatani", "Koji", ""], ["Igarashi", "Takeo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2012.08141", "submitter": "Mingkuan Xu", "authors": "Yuanming Hu, Mingkuan Xu, Ye Kuang, Fr\\'edo Durand", "title": "AsyncTaichi: On-the-fly Inter-kernel Optimizations for Imperative and\n  Spatially Sparse Programming", "comments": "18 pages, 20 figures, submitted to ACM SIGGRAPH Asia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging spatial sparsity has become a popular approach to accelerate 3D\ncomputer graphics applications. Spatially sparse data structures and efficient\nsparse kernels (such as parallel stencil operations on active voxels), are key\nto achieve high performance. Existing work focuses on improving performance\nwithin a single sparse computational kernel. We show that a system that looks\nbeyond a single kernel, plus additional domain-specific sparse data structure\nanalysis, opens up exciting new space for optimizing sparse computations.\nSpecifically, we propose a domain-specific data-flow graph model of imperative\nand sparse computation programs, which describes kernel relationships and\nenables easy analysis and optimization. Combined with an asynchronous execution\nengine that exposes a wide window of kernels, the inter-kernel optimizer can\nthen perform effective sparse computation optimizations, such as eliminating\nunnecessary voxel list generations and removing voxel activation checks. These\ndomain-specific optimizations further make way for classical general-purpose\noptimizations that are originally challenging to directly apply to computations\nwith sparse data structures. Without any computational code modification, our\nnew system leads to $4.02\\times$ fewer kernel launches and $1.87\\times$ speed\nup on our GPU benchmarks, including computations on Eulerian grids, Lagrangian\nparticles, meshes, and automatic differentiation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:09:31 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 15:15:08 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hu", "Yuanming", ""], ["Xu", "Mingkuan", ""], ["Kuang", "Ye", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "2012.08143", "submitter": "Nicolas Wagner", "authors": "Nicolas Wagner, Ulrich Schwanecke", "title": "NeuralQAAD: An Efficient Differentiable Framework for High Resolution\n  Point Cloud Compression", "comments": "Prepublication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we propose NeuralQAAD, a differentiable point cloud\ncompression framework that is fast, robust to sampling, and applicable to high\nresolutions. Previous work that is able to handle complex and non-smooth\ntopologies is hardly scaleable to more than just a few thousand points. We\ntackle the task with a novel neural network architecture characterized by\nweight sharing and autodecoding. Our architecture uses parameters much more\nefficiently than previous work, allowing us to be deeper and scalable.\nFuthermore, we show that the currently only tractable training criterion for\npoint cloud compression, the Chamfer distance, performances poorly for high\nresolutions. To overcome this issue, we pair our architecture with a new\ntraining procedure based upon a quadratic assignment problem (QAP) for which we\nstate two approximation algorithms. We solve the QAP in parallel to gradient\ndescent. This procedure acts as a surrogate loss and allows to implicitly\nminimize the more expressive Earth Movers Distance (EMD) even for point clouds\nwith way more than $10^6$ points. As evaluating the EMD on high resolution\npoint clouds is intractable, we propose a divide-and-conquer approach based on\nk-d trees, the EM-kD, as a scaleable and fast but still reliable upper bound\nfor the EMD. NeuralQAAD is demonstrated on COMA, D-FAUST, and Skulls to\nsignificantly outperform the current state-of-the-art visually and in terms of\nthe EM-kD. Skulls is a novel dataset of skull CT-scans which we will make\npublicly available together with our implementation of NeuralQAAD.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:18:38 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wagner", "Nicolas", ""], ["Schwanecke", "Ulrich", ""]]}, {"id": "2012.08451", "submitter": "Tanasai Sucontphunt", "authors": "Tanasai Sucontphunt", "title": "Geometric Surface Image Prediction for Image Recognition Enhancement", "comments": null, "journal-ref": "SmartCom 2020 (LNCS)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method to predict a geometric surface image from a\nphotograph to assist in image recognition. To recognize objects, several images\nfrom different conditions are required for training a model or fine-tuning a\npre-trained model. In this work, a geometric surface image is introduced as a\nbetter representation than its color image counterpart to overcome lighting\nconditions. The surface image is predicted from a color image. To do so, the\ngeometric surface image together with its color photographs are firstly trained\nwith Generative Adversarial Networks (GAN) model. The trained generator model\nis then used to predict the geometric surface image from the input color image.\nThe evaluation on a case study of an amulet recognition shows that the\npredicted geometric surface images contain less ambiguity than their color\nimages counterpart under different lighting conditions and can be used\neffectively for assisting in image recognition task.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:44:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sucontphunt", "Tanasai", ""]]}, {"id": "2012.08503", "submitter": "Michelle Guo", "authors": "Michelle Guo, Alireza Fathi, Jiajun Wu, Thomas Funkhouser", "title": "Object-Centric Neural Scene Rendering", "comments": "Summary Video: https://youtu.be/NtR7xgxSL1U Project Webpage:\n  https://shellguo.com/osf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for composing photorealistic scenes from captured images\nof objects. Our work builds upon neural radiance fields (NeRFs), which\nimplicitly model the volumetric density and directionally-emitted radiance of a\nscene. While NeRFs synthesize realistic pictures, they only model static scenes\nand are closely tied to specific imaging conditions. This property makes NeRFs\nhard to generalize to new scenarios, including new lighting or new arrangements\nof objects. Instead of learning a scene radiance field as a NeRF does, we\npropose to learn object-centric neural scattering functions (OSFs), a\nrepresentation that models per-object light transport implicitly using a\nlighting- and view-dependent neural network. This enables rendering scenes even\nwhen objects or lights move, without retraining. Combined with a volumetric\npath tracing procedure, our framework is capable of rendering both intra- and\ninter-object light transport effects including occlusions, specularities,\nshadows, and indirect illumination. We evaluate our approach on scene\ncomposition and show that it generalizes to novel illumination conditions,\nproducing photorealistic, physically accurate renderings of multi-object\nscenes.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:55:02 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Guo", "Michelle", ""], ["Fathi", "Alireza", ""], ["Wu", "Jiajun", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2012.08726", "submitter": "Ning Yu", "authors": "Ning Yu, Vladislav Skripniuk, Dingfan Chen, Larry Davis, Mario Fritz", "title": "Responsible Disclosure of Generative Models Using Scalable\n  Fingerprinting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past six years, deep generative models have achieved a qualitatively\nnew level of performance. Generated data has become difficult, if not\nimpossible, to be distinguished from real data. While there are plenty of use\ncases that benefit from this technology, there are also strong concerns on how\nthis new technology can be misused to spoof sensors, generate deep fakes, and\nenable misinformation at scale. Unfortunately, current deep fake detection\nmethods are not sustainable, as the gap between real and fake continues to\nclose. In contrast, our work enables a responsible disclosure of such\nstate-of-the-art generative models, that allows researchers and companies to\nfingerprint their models, so that the generated samples containing a\nfingerprint can be accurately detected and attributed to a source. Our\ntechnique achieves this by an efficient and scalable ad-hoc generation of a\nlarge population of models with distinct fingerprints. Our recommended\noperation point uses a 128-bit fingerprint which in principle results in more\nthan $10^{36}$ identifiable models. Experiments show that our method fulfills\nkey properties of a fingerprinting mechanism and achieves effectiveness in deep\nfake detection and attribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 03:51:54 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 04:19:56 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 08:17:25 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 23:51:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Ning", ""], ["Skripniuk", "Vladislav", ""], ["Chen", "Dingfan", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "2012.08804", "submitter": "Jianan Li", "authors": "Jianan Li, Xuemei Xie, Zhifu Zhao, Yuhan Cao, Qingzhe Pan and\n  Guangming Shi", "title": "Temporal Graph Modeling for Skeleton-based Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs), which model skeleton data as graphs,\nhave obtained remarkable performance for skeleton-based action recognition.\nParticularly, the temporal dynamic of skeleton sequence conveys significant\ninformation in the recognition task. For temporal dynamic modeling, GCN-based\nmethods only stack multi-layer 1D local convolutions to extract temporal\nrelations between adjacent time steps. With the repeat of a lot of local\nconvolutions, the key temporal information with non-adjacent temporal distance\nmay be ignored due to the information dilution. Therefore, these methods still\nremain unclear how to fully explore temporal dynamic of skeleton sequence. In\nthis paper, we propose a Temporal Enhanced Graph Convolutional Network (TE-GCN)\nto tackle this limitation. The proposed TE-GCN constructs temporal relation\ngraph to capture complex temporal dynamic. Specifically, the constructed\ntemporal relation graph explicitly builds connections between semantically\nrelated temporal features to model temporal relations between both adjacent and\nnon-adjacent time steps. Meanwhile, to further explore the sufficient temporal\ndynamic, multi-head mechanism is designed to investigate multi-kinds of\ntemporal relations. Extensive experiments are performed on two widely used\nlarge-scale datasets, NTU-60 RGB+D and NTU-120 RGB+D. And experimental results\nshow that the proposed model achieves the state-of-the-art performance by\nmaking contribution to temporal modeling for action recognition.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 09:02:47 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Jianan", ""], ["Xie", "Xuemei", ""], ["Zhao", "Zhifu", ""], ["Cao", "Yuhan", ""], ["Pan", "Qingzhe", ""], ["Shi", "Guangming", ""]]}, {"id": "2012.09036", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, Yipeng Qin, John Femiani, Peter Wonka", "title": "Improved StyleGAN Embedding: Where are the Good Latents?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  StyleGAN is able to produce photorealistic images that are almost\nindistinguishable from real ones. The reverse problem of finding an embedding\nfor a given image poses a challenge. Embeddings that reconstruct an image well\nare not always robust to editing operations. In this paper, we address the\nproblem of finding an embedding that both reconstructs images and also supports\nimage editing tasks. First, we introduce a new normalized space to analyze the\ndiversity and the quality of the reconstructed latent codes. This space can\nhelp answer the question of where good latent codes are located in latent\nspace. Second, we propose an improved embedding algorithm using a novel\nregularization method based on our analysis. Finally, we analyze the quality of\ndifferent embedding algorithms. We compare our results with the current\nstate-of-the-art methods and achieve a better trade-off between reconstruction\nquality and editing quality.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 18:01:24 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 00:01:21 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Femiani", "John", ""], ["Wonka", "Peter", ""]]}, {"id": "2012.09159", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Vladimir G. Kim, Matthew Fisher, Noam Aigerman, Hao\n  Zhang, Siddhartha Chaudhuri", "title": "DECOR-GAN: 3D Shape Detailization by Conditional Refinement", "comments": "CVPR 2021 (oral). Code: https://github.com/czq142857/DECOR-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep generative network for 3D shape detailization, akin to\nstylization with the style being geometric details. We address the challenge of\ncreating large varieties of high-resolution and detailed 3D geometry from a\nsmall set of exemplars by treating the problem as that of geometric detail\ntransfer. Given a low-resolution coarse voxel shape, our network refines it,\nvia voxel upsampling, into a higher-resolution shape enriched with geometric\ndetails. The output shape preserves the overall structure (or content) of the\ninput, while its detail generation is conditioned on an input \"style code\"\ncorresponding to a detailed exemplar. Our 3D detailization via conditional\nrefinement is realized by a generative adversarial network, coined DECOR-GAN.\nThe network utilizes a 3D CNN generator for upsampling coarse voxels and a 3D\nPatchGAN discriminator to enforce local patches of the generated model to be\nsimilar to those in the training detailed shapes. During testing, a style code\nis fed into the generator to condition the refinement. We demonstrate that our\nmethod can refine a coarse shape into a variety of detailed shapes with\ndifferent styles. The generated results are evaluated in terms of content\npreservation, plausibility, and diversity. Comprehensive ablation studies are\nconducted to validate our network designs. Code is available at\nhttps://github.com/czq142857/DECOR-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:52:10 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:04:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Zhiqin", ""], ["Kim", "Vladimir G.", ""], ["Fisher", "Matthew", ""], ["Aigerman", "Noam", ""], ["Zhang", "Hao", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "2012.09235", "submitter": "Mehdi Bahri", "authors": "Mehdi Bahri, Eimear O' Sullivan, Shunwang Gong, Feng Liu, Xiaoming\n  Liu, Michael M. Bronstein, Stefanos Zafeiriou", "title": "Shape My Face: Registering 3D Face Scans by Surface-to-Surface\n  Translation", "comments": "In review with International Journal of Computer Vision (IJCV) -\n  Revision 1 (minor revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Standard registration algorithms need to be independently applied to each\nsurface to register, following careful pre-processing and hand-tuning.\nRecently, learning-based approaches have emerged that reduce the registration\nof new scans to running inference with a previously-trained model. In this\npaper, we cast the registration task as a surface-to-surface translation\nproblem, and design a model to reliably capture the latent geometric\ninformation directly from raw 3D face scans. We introduce Shape-My-Face (SMF),\na powerful encoder-decoder architecture based on an improved point cloud\nencoder, a novel visual attention mechanism, graph convolutional decoders with\nskip connections, and a specialized mouth model that we smoothly integrate with\nthe mesh convolutions. Compared to the previous state-of-the-art learning\nalgorithms for non-rigid registration of face scans, SMF only requires the raw\ndata to be rigidly aligned (with scaling) with a pre-defined face template.\nAdditionally, our model provides topologically-sound meshes with minimal\nsupervision, offers faster training time, has orders of magnitude fewer\ntrainable parameters, is more robust to noise, and can generalize to previously\nunseen datasets. We extensively evaluate the quality of our registrations on\ndiverse data. We demonstrate the robustness and generalizability of our model\nwith in-the-wild face scans across different modalities, sensor types, and\nresolutions. Finally, we show that, by learning to register scans, SMF produces\na hybrid linear and non-linear morphable model. Manipulation of the latent\nspace of SMF allows for shape generation, and morphing applications such as\nexpression transfer in-the-wild. We train SMF on a dataset of human faces\ncomprising 9 large-scale databases on commodity hardware.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:02:36 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:25:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bahri", "Mehdi", ""], ["Sullivan", "Eimear O'", ""], ["Gong", "Shunwang", ""], ["Liu", "Feng", ""], ["Liu", "Xiaoming", ""], ["Bronstein", "Michael M.", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2012.09290", "submitter": "Bingchen Liu", "authors": "Bingchen Liu, Yizhe Zhu, Kunpeng Song, Ahmed Elgammal", "title": "Self-Supervised Sketch-to-Image Synthesis", "comments": "AAAI-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagining a colored realistic image from an arbitrarily drawn sketch is one\nof the human capabilities that we eager machines to mimic. Unlike previous\nmethods that either requires the sketch-image pairs or utilize low-quantity\ndetected edges as sketches, we study the exemplar-based sketch-to-image (s2i)\nsynthesis task in a self-supervised learning manner, eliminating the necessity\nof the paired sketch data. To this end, we first propose an unsupervised method\nto efficiently synthesize line-sketches for general RGB-only datasets. With the\nsynthetic paired-data, we then present a self-supervised Auto-Encoder (AE) to\ndecouple the content/style features from sketches and RGB-images, and\nsynthesize images that are both content-faithful to the sketches and\nstyle-consistent to the RGB-images. While prior works employ either the\ncycle-consistence loss or dedicated attentional modules to enforce the\ncontent/style fidelity, we show AE's superior performance with pure\nself-supervisions. To further improve the synthesis quality in high resolution,\nwe also leverage an adversarial network to refine the details of synthetic\nimages. Extensive experiments on 1024*1024 resolution demonstrate a new\nstate-of-art-art performance of the proposed model on CelebA-HQ and Wiki-Art\ndatasets. Moreover, with the proposed sketch generator, the model shows a\npromising performance on style mixing and style transfer, which require\nsynthesized images to be both style-consistent and semantically meaningful. Our\ncode is available on\nhttps://github.com/odegeasslbc/Self-Supervised-Sketch-to-Image-Synthesis-PyTorch,\nand please visit https://create.playform.io/my-projects?mode=sketch for an\nonline demo of our model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 22:14:06 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 20:40:27 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liu", "Bingchen", ""], ["Zhu", "Yizhe", ""], ["Song", "Kunpeng", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "2012.09633", "submitter": "Tim Krake", "authors": "Tim Krake, Stefan Reinhardt, Marcel Hlawatsch, Bernhard Eberhardt,\n  Daniel Weiskopf", "title": "Visualization and Selection of Dynamic Mode Decomposition Components for\n  Unsteady Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Mode Decomposition (DMD) is a data-driven and model-free\ndecomposition technique. It is suitable for revealing spatio-temporal features\nof both numerically and experimentally acquired data. Conceptually, DMD\nperforms a low-dimensional spectral decomposition of the data into the\nfollowing components: The modes, called DMD modes, encode the spatial\ncontribution of the decomposition, whereas the DMD amplitudes specify their\nimpact. Each associated eigenvalue, referred to as DMD eigenvalue,\ncharacterizes the frequency and growth rate of the DMD mode. In this paper, we\ndemonstrate how the components of DMD can be utilized to obtain temporal and\nspatial information from time-dependent flow fields. We begin with the\ntheoretical background of DMD and its application to unsteady flow. Next, we\nexamine the conventional process with DMD mathematically and put it in\nrelationship to the discrete Fourier transform. Our analysis shows that the\ncurrent use of DMD components has several drawbacks. To resolve these problems\nwe adjust the components and provide new and meaningful insights into the\ndecomposition: We show that our improved components describe the flow more\nadequately. Moreover, we remove redundancies in the decomposition and clarify\nthe interplay between components, allowing users to understand the impact of\ncomponents. These new representations ,which respect the spatio-temporal\ncharacter of DMD, enable two clustering methods that segment the flow into\nphysically relevant sections and can therefore be used for the selection of DMD\ncomponents. With a number of typical examples, we demonstrate that the\ncombination of these techniques allow new insights with DMD for unsteady flow.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:42:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Krake", "Tim", ""], ["Reinhardt", "Stefan", ""], ["Hlawatsch", "Marcel", ""], ["Eberhardt", "Bernhard", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2012.09854", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Nikhila Ravi, Alex Berg, Deepak Pathak", "title": "Worldsheet: Wrapping the World in a 3D Sheet for View Synthesis from a\n  Single Image", "comments": "v2 diff: Added occlusion handling via layered Wordsheets. Webpage at\n  https://worldsheet.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Worldsheet, a method for novel view synthesis using just a single\nRGB image as input. The main insight is that simply shrink-wrapping a planar\nmesh sheet onto the input image, consistent with the learned intermediate\ndepth, captures underlying geometry sufficient to generate photorealistic\nunseen views with large viewpoint changes. To operationalize this, we propose a\nnovel differentiable texture sampler that allows our wrapped mesh sheet to be\ntextured and rendered differentiably into an image from a target viewpoint. Our\napproach is category-agnostic, end-to-end trainable without using any 3D\nsupervision, and requires a single image at test time. We also explore a simple\nextension by stacking multiple layers of Worldsheets to better handle\nocclusions. Worldsheet consistently outperforms prior state-of-the-art methods\non single-image view synthesis across several datasets. Furthermore, this\nsimple idea captures novel views surprisingly well on a wide range of\nhigh-resolution in-the-wild images, converting them into navigable 3D pop-ups.\nVideo results and code at https://worldsheet.github.io.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:52 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:46:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Ronghang", ""], ["Ravi", "Nikhila", ""], ["Berg", "Alex", ""], ["Pathak", "Deepak", ""]]}, {"id": "2012.09855", "submitter": "Andrew Liu", "authors": "Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah\n  Snavely, Angjoo Kanazawa", "title": "Infinite Nature: Perpetual View Generation of Natural Scenes from a\n  Single Image", "comments": "Project page at https://infinite-nature.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce the problem of perpetual view generation -- long-range\ngeneration of novel views corresponding to an arbitrarily long camera\ntrajectory given a single image. This is a challenging problem that goes far\nbeyond the capabilities of current view synthesis methods, which work for a\nlimited range of viewpoints and quickly degenerate when presented with a large\ncamera motion. Methods designed for video generation also have limited ability\nto produce long video sequences and are often agnostic to scene geometry. We\ntake a hybrid approach that integrates both geometry and image synthesis in an\niterative render, refine, and repeat framework, allowing for long-range\ngeneration that cover large distances after hundreds of frames. Our approach\ncan be trained from a set of monocular video sequences without any manual\nannotation. We propose a dataset of aerial footage of natural coastal scenes,\nand compare our method with recent view synthesis and conditional video\ngeneration baselines, showing that it can generate plausible scenes for much\nlonger time horizons over large camera trajectories compared to existing\nmethods. Please visit our project page at https://infinite-nature.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:59:57 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:49:19 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Liu", "Andrew", ""], ["Tucker", "Richard", ""], ["Jampani", "Varun", ""], ["Makadia", "Ameesh", ""], ["Snavely", "Noah", ""], ["Kanazawa", "Angjoo", ""]]}, {"id": "2012.09955", "submitter": "Ziyan Wang", "authors": "Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas Simon, Jason\n  Saragih, Jessica Hodgins, Michael Zollh\\\"ofer", "title": "Learning Compositional Radiance Fields of Dynamic Human Heads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic rendering of dynamic humans is an important ability for\ntelepresence systems, virtual shopping, synthetic data generation, and more.\nRecently, neural rendering methods, which combine techniques from computer\ngraphics and machine learning, have created high-fidelity models of humans and\nobjects. Some of these methods do not produce results with high-enough fidelity\nfor driveable human models (Neural Volumes) whereas others have extremely long\nrendering times (NeRF). We propose a novel compositional 3D representation that\ncombines the best of previous methods to produce both higher-resolution and\nfaster results. Our representation bridges the gap between discrete and\ncontinuous volumetric representations by combining a coarse 3D-structure-aware\ngrid of animation codes with a continuous learned scene function that maps\nevery position and its corresponding local animation code to its view-dependent\nemitted radiance and local volume density. Differentiable volume rendering is\nemployed to compute photo-realistic novel views of the human head and upper\nbody as well as to train our novel representation end-to-end using only 2D\nsupervision. In addition, we show that the learned dynamic radiance field can\nbe used to synthesize novel unseen expressions based on a global animation\ncode. Our approach achieves state-of-the-art results for synthesizing novel\nviews of dynamic human heads and the upper body.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 22:19:27 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Wang", "Ziyan", ""], ["Bagautdinov", "Timur", ""], ["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Hodgins", "Jessica", ""], ["Zollh\u00f6fer", "Michael", ""]]}, {"id": "2012.10357", "submitter": "Vin\\'icius da Silva", "authors": "Vin\\'icius da Silva, Tiago Novello, H\\'elio Lopes, Luiz Velho", "title": "Proceduray -- A light-weight engine for procedural primitive ray tracing", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Proceduray, an engine for real-time ray tracing of procedural\ngeometry. Its motivation is the current lack of mid-level abstraction tools for\nscenes with primitives involving intersection shaders. Those scenes impose\nstrict engine design choices since they need flexibility in the shader table\nsetup. Proceduray aims at providing a fair tradeoff between that flexibility\nand productivity. It also aims to be didactic. Shader table behavior can be\nconfusing because parameters for indexing come from different parts of a\nsystem, involving both host and device code. This is different in essence from\nray tracing triangle meshes (which must use a built-in intersection shader for\nall objects) or rendering with the traditional graphics or compute pipelines.\nAdditionals goals of the project include fomenting deeper discussions about\nDirectX RayTracing (DXR) host code and providing a good starting point for\ndevelopers trying to deal with procedural geometry using DXR.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:59:49 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:23:17 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["da Silva", "Vin\u00edcius", ""], ["Novello", "Tiago", ""], ["Lopes", "H\u00e9lio", ""], ["Velho", "Luiz", ""]]}, {"id": "2012.10565", "submitter": "Edward Zhang", "authors": "Edward Zhang, Ricardo Martin-Brualla, Janne Kontkanen, Brian Curless", "title": "No Shadow Left Behind: Removing Objects and their Shadows using\n  Approximate Lighting and Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing objects from images is a challenging problem that is important for\nmany applications, including mixed reality. For believable results, the shadows\nthat the object casts should also be removed. Current inpainting-based methods\nonly remove the object itself, leaving shadows behind, or at best require\nspecifying shadow regions to inpaint. We introduce a deep learning pipeline for\nremoving a shadow along with its caster. We leverage rough scene models in\norder to remove a wide variety of shadows (hard or soft, dark or subtle, large\nor thin) from surfaces with a wide variety of textures. We train our pipeline\non synthetically rendered data, and show qualitative and quantitative results\non both synthetic and real scenes.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 01:05:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Edward", ""], ["Martin-Brualla", "Ricardo", ""], ["Kontkanen", "Janne", ""], ["Curless", "Brian", ""]]}, {"id": "2012.10974", "submitter": "Vladislav Golyanik", "authors": "Moritz Kappel and Vladislav Golyanik and Mohamed Elgharib and Jann-Ole\n  Henningson and Hans-Peter Seidel and Susana Castillo and Christian Theobalt\n  and Marcus Magnor", "title": "High-Fidelity Neural Human Motion Transfer from Monocular Video", "comments": "14 pages, 8 figures; project page:\n  https://graphics.tu-bs.de/publications/kappel2020high-fidelity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based human motion transfer creates video animations of humans\nfollowing a source motion. Current methods show remarkable results for\ntightly-clad subjects. However, the lack of temporally consistent handling of\nplausible clothing dynamics, including fine and high-frequency details,\nsignificantly limits the attainable visual quality. We address these\nlimitations for the first time in the literature and present a new framework\nwhich performs high-fidelity and temporally-consistent human motion transfer\nwith natural pose-dependent non-rigid deformations, for several types of loose\ngarments. In contrast to the previous techniques, we perform image generation\nin three subsequent stages, synthesizing human shape, structure, and\nappearance. Given a monocular RGB video of an actor, we train a stack of\nrecurrent deep neural networks that generate these intermediate representations\nfrom 2D poses and their temporal derivatives. Splitting the difficult motion\ntransfer problem into subtasks that are aware of the temporal motion context\nhelps us to synthesize results with plausible dynamics and pose-dependent\ndetail. It also allows artistic control of results by manipulation of\nindividual framework stages. In the experimental results, we significantly\noutperform the state-of-the-art in terms of video realism. Our code and data\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 16:54:38 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kappel", "Moritz", ""], ["Golyanik", "Vladislav", ""], ["Elgharib", "Mohamed", ""], ["Henningson", "Jann-Ole", ""], ["Seidel", "Hans-Peter", ""], ["Castillo", "Susana", ""], ["Theobalt", "Christian", ""], ["Magnor", "Marcus", ""]]}, {"id": "2012.11310", "submitter": "Hugo Bertiche", "authors": "Hugo Bertiche, Meysam Madadi and Sergio Escalera", "title": "PBNS: Physically Based Neural Simulator for Unsupervised Garment Pose\n  Space Deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a methodology to automatically obtain Pose Space Deformation (PSD)\nbasis for rigged garments through deep learning. Classical approaches rely on\nPhysically Based Simulations (PBS) to animate clothes. These are general\nsolutions that, given a sufficiently fine-grained discretization of space and\ntime, can achieve highly realistic results. However, they are computationally\nexpensive and any scene modification prompts the need of re-simulation. Linear\nBlend Skinning (LBS) with PSD offers a lightweight alternative to PBS, though,\nit needs huge volumes of data to learn proper PSD. We propose using deep\nlearning, formulated as an implicit PBS, to unsupervisedly learn realistic\ncloth Pose Space Deformations in a constrained scenario: dressed humans.\nFurthermore, we show it is possible to train these models in an amount of time\ncomparable to a PBS of a few sequences. To the best of our knowledge, we are\nthe first to propose a neural simulator for cloth. While deep-based approaches\nin the domain are becoming a trend, these are data-hungry models. Moreover,\nauthors often propose complex formulations to better learn wrinkles from PBS\ndata. Supervised learning leads to physically inconsistent predictions that\nrequire collision solving to be used. Also, dependency on PBS data limits the\nscalability of these solutions, while their formulation hinders its\napplicability and compatibility. By proposing an unsupervised methodology to\nlearn PSD for LBS models (3D animation standard), we overcome both of these\ndrawbacks. Results obtained show cloth-consistency in the animated garments and\nmeaningful pose-dependant folds and wrinkles. Our solution is extremely\nefficient, handles multiple layers of cloth, allows unsupervised outfit\nresizing and can be easily applied to any custom 3D avatar.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 13:22:10 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:35:47 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 18:41:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bertiche", "Hugo", ""], ["Madadi", "Meysam", ""], ["Escalera", "Sergio", ""]]}, {"id": "2012.12247", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\\\"ofer,\n  Christoph Lassner, Christian Theobalt", "title": "Non-Rigid Neural Radiance Fields: Reconstruction and Novel View\n  Synthesis of a Dynamic Scene From Monocular Video", "comments": "Project page (incl. supplemental videos and code):\n  https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Non-Rigid Neural Radiance Fields (NR-NeRF), a reconstruction and\nnovel view synthesis approach for general non-rigid dynamic scenes. Our\napproach takes RGB images of a dynamic scene as input, e.g., from a monocular\nvideo recording, and creates a high-quality space-time geometry and appearance\nrepresentation. In particular, we show that even a single handheld\nconsumer-grade camera is sufficient to synthesize sophisticated renderings of a\ndynamic scene from novel virtual camera views, for example a `bullet-time'\nvideo effect. Our method disentangles the dynamic scene into a canonical volume\nand its deformation. Scene deformation is implemented as ray bending, where\nstraight rays are deformed non-rigidly to represent scene motion. We also\npropose a novel rigidity regression network that enables us to better constrain\nrigid regions of the scene, which leads to more stable results. The ray bending\nand rigidity network are trained without any explicit supervision. In addition\nto novel view synthesis, our formulation enables dense correspondence\nestimation across views and time, as well as compelling video editing\napplications such as motion exaggeration. We demonstrate the effectiveness of\nour method using extensive evaluations, including ablation studies and\ncomparisons to the state of the art. We urge the reader to watch the\nsupplemental video for qualitative results. Our code will be open sourced.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:46:12 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 07:24:46 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 18:08:43 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Zollh\u00f6fer", "Michael", ""], ["Lassner", "Christoph", ""], ["Theobalt", "Christian", ""]]}, {"id": "2012.12614", "submitter": "Yuri Nesterenko R.", "authors": "Yuri Nesterenko", "title": "On spherical harmonics possessing octahedral symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the implicit representation of one special class of\nreal-valued spherical harmonics with octahedral symmetry. Based on this\nrepresentation we construct the rotationally invariant measure of deviation\nfrom the specified symmetry. The spherical harmonics we consider have some\napplications in the area of directional fields design due to their ability to\nrepresent mutually orthogonal axes in 3D space not relatively to their order\nand orientation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 11:53:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 17:16:22 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Nesterenko", "Yuri", ""]]}, {"id": "2012.12884", "submitter": "Chung-Yi Weng", "authors": "Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman", "title": "Vid2Actor: Free-viewpoint Animatable Person Synthesis from Video in the\n  Wild", "comments": "Project Page: https://grail.cs.washington.edu/projects/vid2actor/\n  Supplementary Video: https://youtu.be/Zec8Us0v23o", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Given an \"in-the-wild\" video of a person, we reconstruct an animatable model\nof the person in the video. The output model can be rendered in any body pose\nto any camera view, via the learned controls, without explicit 3D mesh\nreconstruction. At the core of our method is a volumetric 3D human\nrepresentation reconstructed with a deep network trained on input video,\nenabling novel pose/view synthesis. Our method is an advance over GAN-based\nimage-to-image translation since it allows image synthesis for any pose and\ncamera via the internal 3D representation, while at the same time it does not\nrequire a pre-rigged model or ground truth meshes for training, as in\nmesh-based learning. Experiments validate the design choices and yield results\non synthetic data and on real videos of diverse people performing unconstrained\nactivities (e.g. dancing or playing tennis). Finally, we demonstrate motion\nre-targeting and bullet-time rendering with the learned models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:50:42 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Weng", "Chung-Yi", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2012.13184", "submitter": "Arpit Agarwal Mr.", "authors": "Arpit Agarwal, Tim Man and Wenzhen Yuan", "title": "Simulation of Vision-based Tactile Sensors using Physics based Rendering", "comments": "The paper is accepted in 2021 IEEE International Conference on\n  Robotics and Automation. v3 updates the discussion related to figure 11 v2\n  incorporates the changes from the reviewers and inverse rendering\n  optimization results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tactile sensing has seen a rapid adoption with the advent of vision-based\ntactile sensors. Vision-based tactile sensors provide high resolution, compact\nand inexpensive data to perform precise in-hand manipulation and human-robot\ninteraction. However, the simulation of tactile sensors is still a challenge.\nIn this paper, we built the first fully general optical tactile simulation\nsystem for a GelSight sensor using physics-based rendering techniques. We\npropose physically accurate light models and show in-depth analysis of\nindividual components of our simulation pipeline. Our system outperforms\nprevious simulation techniques qualitatively and quantitative on image\nsimilarity metrics. Our code and experimental data is open-sourced at\nhttps://labs.ri.cmu.edu/robotouch/tactile-optical-simulation/\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:06:15 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 18:24:58 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 23:25:56 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Agarwal", "Arpit", ""], ["Man", "Tim", ""], ["Yuan", "Wenzhen", ""]]}, {"id": "2012.13257", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov", "title": "Interpolating Points on a Non-Uniform Grid using a Mixture of Gaussians", "comments": "5 figures, 2 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose an approach to perform non-uniform image\ninterpolation based on a Gaussian Mixture Model. Traditional image\ninterpolation methods, like nearest neighbor, bilinear, Hamming, Lanczos, etc.\nassume that the coordinates you want to interpolate from, are positioned on a\nuniform grid. However, it is not always the case in practice and we develop an\ninterpolation method that is able to generate an image from arbitrarily\npositioned pixel values. We do this by representing each known pixel as a 2D\nnormal distribution and considering each output image pixel as a sample from\nthe mixture of all the known ones. Apart from the ability to reconstruct an\nimage from arbitrarily positioned set of pixels, this also allows us to\ndifferentiate through the interpolation procedure, which might be helpful for\ndownstream applications. Our optimized CUDA kernel and the source code to\nreproduce the benchmarks is located at\nhttps://github.com/universome/non-uniform-interpolation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 13:59:39 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Skorokhodov", "Ivan", ""]]}, {"id": "2012.13392", "submitter": "Chen Chen", "authors": "Ce Zheng and Wenhan Wu and Taojiannan Yang and Sijie Zhu and Chen Chen\n  and Ruixu Liu and Ju Shen and Nasser Kehtarnavaz and Mubarak Shah", "title": "Deep Learning-Based Human Pose Estimation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation aims to locate the human body parts and build human\nbody representation (e.g., body skeleton) from input data such as images and\nvideos. It has drawn increasing attention during the past decade and has been\nutilized in a wide range of applications including human-computer interaction,\nmotion analysis, augmented reality, and virtual reality. Although the recently\ndeveloped deep learning-based solutions have achieved high performance in human\npose estimation, there still remain challenges due to insufficient training\ndata, depth ambiguities, and occlusion. The goal of this survey paper is to\nprovide a comprehensive review of recent deep learning-based solutions for both\n2D and 3D pose estimation via a systematic analysis and comparison of these\nsolutions based on their input data and inference procedures. More than 240\nresearch papers since 2014 are covered in this survey. Furthermore, 2D and 3D\nhuman pose estimation datasets and evaluation metrics are included.\nQuantitative performance comparisons of the reviewed methods on popular\ndatasets are summarized and discussed. Finally, the challenges involved,\napplications, and future research directions are concluded. We also provide a\nregularly updated project page: \\url{https://github.com/zczcwh/DL-HPE}\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 18:49:06 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 21:50:33 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 19:24:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zheng", "Ce", ""], ["Wu", "Wenhan", ""], ["Yang", "Taojiannan", ""], ["Zhu", "Sijie", ""], ["Chen", "Chen", ""], ["Liu", "Ruixu", ""], ["Shen", "Ju", ""], ["Kehtarnavaz", "Nasser", ""], ["Shah", "Mubarak", ""]]}, {"id": "2012.14495", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Michael DeZeeuw, Richard Baraniuk, Ashok\n  Veeraraghavan, and Aswin Sankaranarayanan", "title": "SASSI -- Super-Pixelated Adaptive Spatio-Spectral Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel video-rate hyperspectral imager with high spatial, and\ntemporal resolutions. Our key hypothesis is that spectral profiles of pixels in\na super-pixel of an oversegmented image tend to be very similar. Hence, a\nscene-adaptive spatial sampling of an hyperspectral scene, guided by its\nsuper-pixel segmented image, is capable of obtaining high-quality\nreconstructions. To achieve this, we acquire an RGB image of the scene, compute\nits super-pixels, from which we generate a spatial mask of locations where we\nmeasure high-resolution spectrum. The hyperspectral image is subsequently\nestimated by fusing the RGB image and the spectral measurements using a\nlearnable guided filtering approach. Due to low computational complexity of the\nsuperpixel estimation step, our setup can capture hyperspectral images of the\nscenes with little overhead over traditional snapshot hyperspectral cameras,\nbut with significantly higher spatial and spectral resolutions. We validate the\nproposed technique with extensive simulations as well as a lab prototype that\nmeasures hyperspectral video at a spatial resolution of $600 \\times 900$\npixels, at a spectral resolution of 10 nm over visible wavebands, and achieving\na frame rate at $18$fps.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 21:34:18 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["DeZeeuw", "Michael", ""], ["Baraniuk", "Richard", ""], ["Veeraraghavan", "Ashok", ""], ["Sankaranarayanan", "Aswin", ""]]}, {"id": "2012.14901", "submitter": "Daniel Perry", "authors": "Daniel J Perry, Vahid Keshavarzzadeh, Shireen Y Elhabian, Robert M\n  Kirby, Michael Gleicher, Ross T Whitaker", "title": "Visualization of topology optimization designs with representative\n  subset selection", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important new trend in additive manufacturing is the use of optimization\nto automatically design industrial objects, such as beams, rudders or wings.\nTopology optimization, as it is often called, computes the best configuration\nof material over a 3D space, typically represented as a grid, in order to\nsatisfy or optimize physical parameters. Designers using these automated\nsystems often seek to understand the interaction of physical constraints with\nthe final design and its implications for other physical characteristics. Such\nunderstanding is challenging because the space of designs is large and small\nchanges in parameters can result in radically different designs. We propose to\naddress these challenges using a visualization approach for exploring the space\nof design solutions. The core of our novel approach is to summarize the space\n(ensemble of solutions) by automatically selecting a set of examples and to\nrepresent the complete set of solutions as combinations of these examples. The\nrepresentative examples create a meaningful parameterization of the design\nspace that can be explored using standard visualization techniques for\nhigh-dimensional spaces. We present evaluations of our subset selection\ntechnique and that the overall approach addresses the needs of expert\ndesigners.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:54:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Perry", "Daniel J", ""], ["Keshavarzzadeh", "Vahid", ""], ["Elhabian", "Shireen Y", ""], ["Kirby", "Robert M", ""], ["Gleicher", "Michael", ""], ["Whitaker", "Ross T", ""]]}, {"id": "2012.15176", "submitter": "Alexander Tereshin", "authors": "A. Tereshin, A. Pasko, O. Fryazinov, V. Adzhiev", "title": "Hybrid Function Representation for Heterogeneous Objects", "comments": "26 pages, 16 figures, submitted to Graphical Models Journal (minor\n  revision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Heterogeneous object modelling is an emerging area where geometric shapes are\nconsidered in concert with their internal physically-based attributes. This\npaper describes a novel theoretical and practical framework for modelling\nvolumetric heterogeneous objects on the basis of a novel unifying\nfunctionally-based hybrid representation called HFRep. This new representation\nallows for obtaining a continuous smooth distance field in Euclidean space and\npreserves the advantages of the conventional representations based on scalar\nfields of different kinds without their drawbacks. We systematically describe\nthe mathematical and algorithmic basics of HFRep. The steps of the basic\nalgorithm are presented in detail for both geometry and attributes. To solve\nsome problematic issues, we have suggested several practical solutions,\nincluding a new algorithm for solving the eikonal equation on hierarchical\ngrids. Finally, we show the practicality of the approach by modelling several\nrepresentative heterogeneous objects, including those of a time-variant nature.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 14:39:46 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Tereshin", "A.", ""], ["Pasko", "A.", ""], ["Fryazinov", "O.", ""], ["Adzhiev", "V.", ""]]}, {"id": "2012.15461", "submitter": "Sipu Ruan", "authors": "Sipu Ruan, Gregory S. Chirikjian", "title": "Closed-Form Minkowski Sums of Convex Bodies with Smooth Positively\n  Curved Boundaries", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a closed-form parametric formula of the Minkowski sum\nboundary for broad classes of convex bodies in d-dimensional Euclidean space.\nWith positive sectional curvatures at every point, the boundary that encloses\neach body can be characterized by the surface gradient. The first theorem\ndirectly parameterizes the Minkowski sums using the unit normal vector at each\nbody surface. Although simple to express mathematically, such a\nparameterization is not always practical to obtain computationally. Therefore,\nthe second theorem derives a more useful parametric closed-form expression\nusing the gradient that is not normalized. In the special case of two\nellipsoids, the proposed expressions are identical to those derived previously\nusing geometric interpretations. In order to further examine the results,\nnumerical verifications and comparisons of the Minkowski sums between two\nsuperquadric bodies are conducted. The application for the generation of\nconfiguration space obstacles in motion planning problems is introduced and\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 06:04:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ruan", "Sipu", ""], ["Chirikjian", "Gregory S.", ""]]}]