[{"id": "1806.01005", "submitter": "Johannes Jendersie", "authors": "Johannes Jendersie", "title": "Path Throughput Importance Weights", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many Monte Carlo light transport simulations use multiple importance sampling\n(MIS) to weight between different path sampling strategies. We propose to use\nthe path throughput to compute the MIS weights instead of the commonly used\nprobability density per area measure. This new formulation is equivalent to the\nprevious approach and results in the same weights as well as implementation.\nHowever, it is more intuitive and can help in understanding the effects of\nmodifications to the weight function. We show some examples of required\nmodifications which are often neglected in implementations. Also, our new\nperspective might help to derive MIS strategies for new samplers in the future.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:28:26 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 07:29:24 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Jendersie", "Johannes", ""]]}, {"id": "1806.02071", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Vinicius C. Azevedo, Nils Thuerey, Theodore Kim, Markus\n  Gross, Barbara Solenthaler", "title": "Deep Fluids: A Generative Network for Parameterized Fluid Simulations", "comments": "Computer Graphics Forum (Proceedings of EUROGRAPHICS 2019),\n  additional materials: http://www.byungsoo.me/project/deep-fluids/", "journal-ref": "Computer Graphics Forum (Proc. Eurographics), 38, 2 (2019), 59-70", "doi": "10.1111/cgf.13619", "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel generative model to synthesize fluid simulations\nfrom a set of reduced parameters. A convolutional neural network is trained on\na collection of discrete, parameterizable fluid simulation velocity fields. Due\nto the capability of deep learning architectures to learn representative\nfeatures of the data, our generative model is able to accurately approximate\nthe training data set, while providing plausible interpolated in-betweens. The\nproposed generative model is optimized for fluids by a novel loss function that\nguarantees divergence-free velocity fields at all times. In addition, we\ndemonstrate that we can handle complex parameterizations in reduced spaces, and\nadvance simulations in time by integrating in the latent space with a second\nnetwork. Our method models a wide variety of fluid behaviors, thus enabling\napplications such as fast construction of simulations, interpolation of fluids\nwith different parameters, time re-sampling, latent space simulations, and\ncompression of fluid simulation data. Reconstructed velocity fields are\ngenerated up to 700x faster than re-simulating the data with the underlying CPU\nsolver, while achieving compression rates of up to 1300x.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:57:18 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 14:44:57 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Thuerey", "Nils", ""], ["Kim", "Theodore", ""], ["Gross", "Markus", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "1806.02918", "submitter": "Maria Shugrina", "authors": "Maria Shugrina, Amlan Kar, Karan Singh, Sanja Fidler", "title": "Color Sails: Discrete-Continuous Palettes for Deep Color Exploration", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present color sails, a discrete-continuous color gamut representation that\nextends the color gradient analogy to three dimensions and allows interactive\ncontrol of the color blending behavior. Our representation models a wide\nvariety of color distributions in a compact manner, and lends itself to\napplications such as color exploration for graphic design, illustration and\nsimilar fields. We propose a Neural Network that can fit a color sail to any\nimage. Then, the user can adjust color sail parameters to change the base\ncolors, their blending behavior and the number of colors, exploring a wide\nrange of options for the original design. In addition, we propose a Deep\nLearning model that learns to automatically segment an image into\ncolor-compatible alpha masks, each equipped with its own color sail. This\nallows targeted color exploration by either editing their corresponding color\nsails or using standard software packages. Our model is trained on a custom\ndiverse dataset of art and design. We provide both quantitative evaluations,\nand a user study, demonstrating the effectiveness of color sail interaction.\nInteractive demos are available at www.colorsails.com.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 22:42:00 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shugrina", "Maria", ""], ["Kar", "Amlan", ""], ["Singh", "Karan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1806.03053", "submitter": "Fabien Feschet", "authors": "Fabien Feschet (IP)", "title": "The Saturated Subpaths Decomposition in Z 2 : a short note on\n  generalized Tangential Cover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we generalized the Tangential Cover used in Digital\nGeometry in order to use very general geometric predicates. We present the\nrequired notions of saturated $\\alpha$-paths of a digital curve as well as\nconservative predicates which indeed cover nearly all geometric digital\nprimitives published so far. The goal of this note is to prove that under a\nvery general situation, the size of the Tangential Cover is linear with the\nnumber of points of the input curve. The computation complexity of the\nTangential Cover depends on the complexity of incremental recognition of\ngeometric predicates. Moreover, in the discussion, we show that our approach\ndoes not rely on connectivity of points as it might be though first.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 09:56:38 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Feschet", "Fabien", "", "IP"]]}, {"id": "1806.03589", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas Huang", "title": "Free-Form Image Inpainting with Gated Convolution", "comments": "Accepted in ICCV 2019 Oral; open sourced; interactive demo available:\n  http://jiahuiyu.com/deepfill/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generative image inpainting system to complete images with\nfree-form mask and guidance. The system is based on gated convolutions learned\nfrom millions of images without additional labelling efforts. The proposed\ngated convolution solves the issue of vanilla convolution that treats all input\npixels as valid ones, generalizes partial convolution by providing a learnable\ndynamic feature selection mechanism for each channel at each spatial location\nacross all layers. Moreover, as free-form masks may appear anywhere in images\nwith any shape, global and local GANs designed for a single rectangular mask\nare not applicable. Thus, we also present a patch-based GAN loss, named\nSN-PatchGAN, by applying spectral-normalized discriminator on dense image\npatches. SN-PatchGAN is simple in formulation, fast and stable in training.\nResults on automatic image inpainting and user-guided extension demonstrate\nthat our system generates higher-quality and more flexible results than\nprevious methods. Our system helps user quickly remove distracting objects,\nmodify image layouts, clear watermarks and edit faces. Code, demo and models\nare available at: https://github.com/JiahuiYu/generative_inpainting\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 05:51:32 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 03:06:37 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Yu", "Jiahui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Shen", "Xiaohui", ""], ["Lu", "Xin", ""], ["Huang", "Thomas", ""]]}, {"id": "1806.03967", "submitter": "Ruqi Huang", "authors": "Ruqi Huang, Panos Achlioptas, Leonidas Guibas, Maks Ovsjanikov", "title": "Latent Space Representation for Shape Analysis and Learning", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel shape representation useful for analyzing and processing\nshape collections, as well for a variety of learning and inference tasks.\nUnlike most approaches that capture variability in a collection by using a\ntemplate model or a base shape, we show that it is possible to construct a full\nshape representation by using the latent space induced by a functional map net-\nwork, allowing us to represent shapes in the context of a collection without\nthe bias induced by selecting a template shape. Key to our construction is a\nnovel analysis of latent functional spaces, which shows that after proper\nregularization they can be endowed with a natural geometric structure, giving\nrise to a well-defined, stable and fully informative shape representation. We\ndemonstrate the utility of our representation in shape analysis tasks, such as\nhighlighting the most distorted shape parts in a collection or separating\nvariability modes between shape classes. We further exploit our representation\nin learning applications by showing how it can naturally be used within deep\nlearning and convolutional neural networks for shape classi cation or\nreconstruction, signi cantly outperforming existing point-based techniques.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:40:35 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 02:29:05 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Huang", "Ruqi", ""], ["Achlioptas", "Panos", ""], ["Guibas", "Leonidas", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1806.04171", "submitter": "Neal Wadhwa", "authors": "Neal Wadhwa, Rahul Garg, David E. Jacobs, Bryan E. Feldman, Nori\n  Kanazawa, Robert Carroll, Yair Movshovitz-Attias, Jonathan T. Barron, Yael\n  Pritch, and Marc Levoy", "title": "Synthetic Depth-of-Field with a Single-Camera Mobile Phone", "comments": "Accepted to SIGGRAPH 2018. Basis for Portrait Mode on Google Pixel 2\n  and Pixel 2 XL", "journal-ref": null, "doi": "10.1145/3197517.3201329", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shallow depth-of-field is commonly used by photographers to isolate a subject\nfrom a distracting background. However, standard cell phone cameras cannot\nproduce such images optically, as their short focal lengths and small apertures\ncapture nearly all-in-focus images. We present a system to computationally\nsynthesize shallow depth-of-field images with a single mobile camera and a\nsingle button press. If the image is of a person, we use a person segmentation\nnetwork to separate the person and their accessories from the background. If\navailable, we also use dense dual-pixel auto-focus hardware, effectively a\n2-sample light field with an approximately 1 millimeter baseline, to compute a\ndense depth map. These two signals are combined and used to render a defocused\nimage. Our system can process a 5.4 megapixel image in 4 seconds on a mobile\nphone, is fully automatic, and is robust enough to be used by non-experts. The\nmodular nature of our system allows it to degrade naturally in the absence of a\ndual-pixel sensor or a human subject.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:29:12 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wadhwa", "Neal", ""], ["Garg", "Rahul", ""], ["Jacobs", "David E.", ""], ["Feldman", "Bryan E.", ""], ["Kanazawa", "Nori", ""], ["Carroll", "Robert", ""], ["Movshovitz-Attias", "Yair", ""], ["Barron", "Jonathan T.", ""], ["Pritch", "Yael", ""], ["Levoy", "Marc", ""]]}, {"id": "1806.04455", "submitter": "Jing Ren", "authors": "Jing Ren, Adrien Poulenard, Peter Wonka, and Maks Ovsjanikov", "title": "Continuous and Orientation-preserving Correspondences via Functional\n  Maps", "comments": "16 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for efficiently computing orientation-preserving and\napproximately continuous correspondences between non-rigid shapes, using the\nfunctional maps framework. We first show how orientation preservation can be\nformulated directly in the functional (spectral) domain without using landmark\nor region correspondences and without relying on external symmetry information.\nThis allows us to obtain functional maps that promote orientation preservation,\neven when using descriptors, that are invariant to orientation changes. We then\nshow how higher quality, approximately continuous and bijective pointwise\ncorrespondences can be obtained from initial functional maps by introducing a\nnovel refinement technique that aims to simultaneously improve the maps both in\nthe spectral and spatial domains. This leads to a general pipeline for\ncomputing correspondences between shapes that results in high-quality maps,\nwhile admitting an efficient optimization scheme. We show through extensive\nevaluation that our approach improves upon state-of-the-art results on\nchallenging isometric and non-isometric correspondence benchmarks according to\nboth measures of continuity and coverage as well as producing semantically\nmeaningful correspondences as measured by the distance to ground truth maps.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:07:05 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 10:26:54 GMT"}, {"version": "v3", "created": "Wed, 3 Oct 2018 10:18:23 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Ren", "Jing", ""], ["Poulenard", "Adrien", ""], ["Wonka", "Peter", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1806.04924", "submitter": "Ana Serrano", "authors": "Ana Serrano, Vincent Sitzmann, Jaime Ruiz-Borau, Gordon Wetzstein,\n  Diego Gutierrez, Belen Masia", "title": "Movie Editing and Cognitive Event Segmentation in Virtual Reality Video", "comments": null, "journal-ref": "ACM Transactions on Graphics 36, 4, Article 47 (July 2017)", "doi": "10.1145/3072959.3073668", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional cinematography has relied for over a century on a\nwell-established set of editing rules, called continuity editing, to create a\nsense of situational continuity. Despite massive changes in visual content\nacross cuts, viewers in general experience no trouble perceiving the\ndiscontinuous flow of information as a coherent set of events. However, Virtual\nReality (VR) movies are intrinsically different from traditional movies in that\nthe viewer controls the camera orientation at all times. As a consequence,\ncommon editing techniques that rely on camera orientations, zooms, etc., cannot\nbe used. In this paper we investigate key relevant questions to understand how\nwell traditional movie editing carries over to VR. To do so, we rely on recent\ncognition studies and the event segmentation theory, which states that our\nbrains segment continuous actions into a series of discrete, meaningful events.\nWe first replicate one of these studies to assess whether the predictions of\nsuch theory can be applied to VR. We next gather gaze data from viewers\nwatching VR videos containing different edits with varying parameters, and\nprovide the first systematic analysis of viewers' behavior and the perception\nof continuity in VR. From this analysis we make a series of relevant findings;\nfor instance, our data suggests that predictions from the cognitive event\nsegmentation theory are useful guides for VR editing; that different types of\nedits are equally well understood in terms of continuity; and that spatial\nmisalignments between regions of interest at the edit boundaries favor a more\nexploratory behavior even after viewers have fixated on a new region of\ninterest. In addition, we propose a number of metrics to describe viewers'\nattentional behavior in VR. We believe the insights derived from our work can\nbe useful as guidelines for VR content creation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:08:48 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Sitzmann", "Vincent", ""], ["Ruiz-Borau", "Jaime", ""], ["Wetzstein", "Gordon", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "1806.04935", "submitter": "Ana Serrano", "authors": "Ana Serrano, Elena Garces, Diego Gutierrez, Belen Masia", "title": "Convolutional sparse coding for capturing high speed video content", "comments": null, "journal-ref": "Computer Graphics Forum 36, 8, Pages 380-389 (February 2017)", "doi": "10.1111/cgf.13086", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video capture is limited by the trade-off between spatial and temporal\nresolution: when capturing videos of high temporal resolution, the spatial\nresolution decreases due to bandwidth limitations in the capture system.\nAchieving both high spatial and temporal resolution is only possible with\nhighly specialized and very expensive hardware, and even then the same basic\ntrade-off remains. The recent introduction of compressive sensing and sparse\nreconstruction techniques allows for the capture of single-shot high-speed\nvideo, by coding the temporal information in a single frame, and then\nreconstructing the full video sequence from this single coded image and a\ntrained dictionary of image patches. In this paper, we first analyze this\napproach, and find insights that help improve the quality of the reconstructed\nvideos. We then introduce a novel technique, based on convolutional sparse\ncoding (CSC), and show how it outperforms the state-of-the-art, patch-based\napproach in terms of flexibility and efficiency, due to the convolutional\nnature of its filter banks. The key idea for CSC high-speed video acquisition\nis extending the basic formulation by imposing an additional constraint in the\ntemporal dimension, which enforces sparsity of the first-order derivatives over\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:31:07 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Garces", "Elena", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""]]}, {"id": "1806.04942", "submitter": "Ana Serrano", "authors": "Ana Serrano, Felix Heide, Diego Gutierrez, Gordon Wetzstein, Belen\n  Masia", "title": "Convolutional Sparse Coding for High Dynamic Range Imaging", "comments": null, "journal-ref": "Computer Graphics Forum 35, 2, Pages 153-163 (May 2016)", "doi": "10.1111/cgf.12819", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current HDR acquisition techniques are based on either (i) fusing\nmultibracketed, low dynamic range (LDR) images, (ii) modifying existing\nhardware and capturing different exposures simultaneously with multiple\nsensors, or (iii) reconstructing a single image with spatially-varying pixel\nexposures. In this paper, we propose a novel algorithm to recover high-quality\nHDRI images from a single, coded exposure. The proposed reconstruction method\nbuilds on recently-introduced ideas of convolutional sparse coding (CSC); this\npaper demonstrates how to make CSC practical for HDR imaging. We demonstrate\nthat the proposed algorithm achieves higher-quality reconstructions than\nalternative methods, we evaluate optical coding schemes, analyze algorithmic\nparameters, and build a prototype coded HDR camera that demonstrates the\nutility of convolutional sparse HDRI coding with a custom hardware platform.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:48:33 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Heide", "Felix", ""], ["Gutierrez", "Diego", ""], ["Wetzstein", "Gordon", ""], ["Masia", "Belen", ""]]}, {"id": "1806.04950", "submitter": "Ana Serrano", "authors": "Ana Serrano, Diego Gutierrez, Karol Myszkowski, Hans-Peter Seidel,\n  Belen Masia", "title": "An intuitive control space for material appearance", "comments": null, "journal-ref": "ACM Transactions on Graphics 35, 6, Article 186 (November 2016)", "doi": "10.1145/2980179.2980242", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different techniques for measuring material appearance have been\nproposed in the last few years. These have produced large public datasets,\nwhich have been used for accurate, data-driven appearance modeling. However,\nalthough these datasets have allowed us to reach an unprecedented level of\nrealism in visual appearance, editing the captured data remains a challenge. In\nthis paper, we present an intuitive control space for predictable editing of\ncaptured BRDF data, which allows for artistic creation of plausible novel\nmaterial appearances, bypassing the difficulty of acquiring novel samples. We\nfirst synthesize novel materials, extending the existing MERL dataset up to 400\nmathematically valid BRDFs. We then design a large-scale experiment, gathering\n56,000 subjective ratings on the high-level perceptual attributes that best\ndescribe our extended dataset of materials. Using these ratings, we build and\ntrain networks of radial basis functions to act as functionals mapping the\nperceptual attributes to an underlying PCA-based representation of BRDFs. We\nshow that our functionals are excellent predictors of the perceived attributes\nof appearance. Our control space enables many applications, including intuitive\nmaterial editing of a wide range of visual properties, guidance for gamut\nmapping, analysis of the correlation between perceptual attributes, or novel\nappearance similarity metrics. Moreover, our methodology can be used to derive\nfunctionals applicable to classic analytic BRDF representations. We release our\ncode and dataset publicly, in order to support and encourage further research\nin this direction.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:19:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Serrano", "Ana", ""], ["Gutierrez", "Diego", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Masia", "Belen", ""]]}, {"id": "1806.05299", "submitter": "Takayuki Yamada", "authors": "Takayuki Yamada", "title": "Geometric Shape Features Extraction Using a Steady State Partial\n  Differential Equation System", "comments": "31 pages, 10 figures", "journal-ref": "Journal of Computational Design and Engineering, 2019", "doi": "10.1016/j.jcde.2019.03.006", "report-no": null, "categories": "cs.CV cs.AI cs.GR math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified method for extracting geometric shape features from binary image\ndata using a steady state partial differential equation (PDE) system as a\nboundary value problem is presented in this paper. The PDE and functions are\nformulated to extract the thickness, orientation, and skeleton simultaneously.\nThe main advantages of the proposed method is that the orientation is defined\nwithout derivatives and thickness computation is not imposed a topological\nconstraint on the target shape. A one-dimensional analytical solution is\nprovided to validate the proposed method. In addition, two-dimensional\nnumerical examples are presented to confirm the usefulness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:33:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:57:20 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2019 22:57:30 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yamada", "Takayuki", ""]]}, {"id": "1806.05385", "submitter": "Tobias Ritschel", "authors": "Tobias Ritschel, Sebastian Friston, Anthony Steed", "title": "Perceptual Rasterization for Head-mounted Display Image Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a rasterization pipeline tailored towards the need of head-mounted\ndisplays (HMD), where latency and field-of-view requirements pose new\nchallenges beyond those of traditional desktop displays. Instead of rendering\nand warping for low latency, or using multiple passes for foveation, we show\nhow both can be produced directly in a single perceptual rasterization pass. We\ndo this with per-fragment ray-casting. This is enabled by derivations of tight\nspace-time-fovea pixel bounds, introducing just enough flexibility for\nrequisite geometric tests, but retaining most of the the simplicity and\nefficiency of the traditional rasterizaton pipeline. To produce foveated\nimages, we rasterize to an image with spatially varying pixel density. To\nreduce latency, we extend the image formation model to directly produce\n\"rolling\" images where the time at each pixel depends on its display location.\nOur approach overcomes limitations of warping with respect to disocclusions,\nobject motion and view-dependent shading, as well as geometric aliasing\nartifacts in other foveated rendering techniques. A set of perceptual user\nstudies demonstrates the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:34:37 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Ritschel", "Tobias", ""], ["Friston", "Sebastian", ""], ["Steed", "Anthony", ""]]}, {"id": "1806.05952", "submitter": "Rafael Ballester-Ripoll", "authors": "Rafael Ballester-Ripoll, Peter Lindstrom, Renato Pajarola", "title": "TTHRESH: Tensor Compression for Multidimensional Visual Data", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2904063", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory and network bandwidth are decisive bottlenecks when handling\nhigh-resolution multidimensional data sets in visualization applications, and\nthey increasingly demand suitable data compression strategies. We introduce a\nnovel lossy compression algorithm for multidimensional data over regular grids.\nIt leverages the higher-order singular value decomposition (HOSVD), a\ngeneralization of the SVD to three dimensions and higher, together with\nbit-plane, run-length and arithmetic coding to compress the HOSVD transform\ncoefficients. Our scheme degrades the data particularly smoothly and achieves\nlower mean squared error than other state-of-the-art algorithms at\nlow-to-medium bit rates, as it is required in data archiving and management for\nvisualization purposes. Further advantages of the proposed algorithm include\nvery fine bit rate selection granularity and the ability to manipulate data at\nvery small cost in the compression domain, for example to reconstruct filtered\nand/or subsampled versions of all (or selected parts) of the data set.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:35:17 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 08:30:25 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ballester-Ripoll", "Rafael", ""], ["Lindstrom", "Peter", ""], ["Pajarola", "Renato", ""]]}, {"id": "1806.05999", "submitter": "Nicolas Bonneel", "authors": "Nicolas Bonneel (1), David Coeurjolly (1), Pierre Gueth (2) and\n  Jacques-Olivier Lachaud (3) ((1) CNRS, Univ. Lyon, (2) Arskan, (3)\n  Universit\\'e Savoie Mont Blanc)", "title": "Mumford-Shah Mesh Processing using the Ambrosio-Tortorelli Functional", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mumford-Shah functional approximates a function by a piecewise smooth\nfunction. Its versatility makes it ideal for tasks such as image segmentation\nor restoration, and it is now a widespread tool of image processing. Recent\nwork has started to investigate its use for mesh segmentation and feature lines\ndetection, but we take the stance that the power of this functional could reach\nfar beyond these tasks and integrate the everyday mesh processing toolbox. In\nthis paper, we discretize an Ambrosio-Tortorelli approximation via a Discrete\nExterior Calculus formulation. We show that, combined with a new shape\noptimization routine, several mesh processing problems can be readily tackled\nwithin the same framework. In particular, we illustrate applications in mesh\ndenoising, normal map embossing, mesh inpainting and mesh segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 14:32:32 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 21:33:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Bonneel", "Nicolas", ""], ["Coeurjolly", "David", ""], ["Gueth", "Pierre", ""], ["Lachaud", "Jacques-Olivier", ""]]}, {"id": "1806.06613", "submitter": "Marie-Lena Eckert", "authors": "Marie-Lena Eckert, Wolfgang Heidrich, Nils Thuerey", "title": "Coupled Fluid Density and Motion from Single Views", "comments": "Computer Graphics Forum (2018), further information:\n  https://ge.in.tum.de/publications/2018-cgf-eckert/, video:\n  https://www.youtube.com/watch?v=J2wkPNBJLaI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to reconstruct a fluid's 3D density and motion\nbased on just a single sequence of images. This is rendered possible by using\npowerful physical priors for this strongly under-determined problem. More\nspecifically, we propose a novel strategy to infer density updates strongly\ncoupled to previous and current estimates of the flow motion. Additionally, we\nemploy an accurate discretization and depth-based regularizers to compute\nstable solutions. Using only one view for the reconstruction reduces the\ncomplexity of the capturing setup drastically and could even allow for online\nvideo databases or smart-phone videos as inputs. The reconstructed 3D velocity\ncan then be flexibly utilized, e.g., for re-simulation, domain modification or\nguiding purposes. We will demonstrate the capacity of our method with a series\nof synthetic test cases and the reconstruction of real smoke plumes captured\nwith a Raspberry Pi camera.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:05:20 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Eckert", "Marie-Lena", ""], ["Heidrich", "Wolfgang", ""], ["Thuerey", "Nils", ""]]}, {"id": "1806.06639", "submitter": "Marco Livesu", "authors": "Matteo Bracci and Marco Tarini and Nico Pietroni and Marco Livesu and\n  Paolo Cignoni", "title": "HexaLab.net: an online viewer for hexahedral meshes", "comments": null, "journal-ref": "Computer-Aided Design, Volume 110, May 2019, Pages 24-36", "doi": "10.1016/j.cad.2018.12.003", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HexaLab: a WebGL application for real time visualization,\nexploration and assessment of hexahedral meshes. HexaLab can be used by simply\nopening www.hexalab.net. Our visualization tool targets both users and\nscholars. Practitioners who employ hexmeshes for Finite Element Analysis, can\nreadily check mesh quality and assess its usability for simulation. Researchers\ninvolved in mesh generation may use HexaLab to perform a detailed analysis of\nthe mesh structure, isolating weak points and testing new solutions to improve\non the state of the art and generate high quality images. To this end, we\nsupport a wide variety of visualization and volume inspection tools. Our system\noffers also immediate access to a repository containing all the publicly\navailable meshes produced with the most recent techniques for hexmesh\ngeneration. We believe HexaLab, providing a common tool for visualizing,\nassessing and distributing results, will push forward the recent strive for\nreplicability in our scientific community.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 12:58:08 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 11:04:43 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Bracci", "Matteo", ""], ["Tarini", "Marco", ""], ["Pietroni", "Nico", ""], ["Livesu", "Marco", ""], ["Cignoni", "Paolo", ""]]}, {"id": "1806.06710", "submitter": "Tobias Ritschel", "authors": "Thomas Leimk\\\"uhler, Gurprit Singh, Karol Myszkowski, Hans-Peter\n  Seidel, Tobias Ritschel", "title": "End-to-end Sampling Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample patterns have many uses in Computer Graphics, ranging from procedural\nobject placement over Monte Carlo image synthesis to non-photorealistic\ndepiction. Their properties such as discrepancy, spectra, anisotropy, or\nprogressiveness have been analyzed extensively. However, designing methods to\nproduce sampling patterns with certain properties can require substantial\nhand-crafting effort, both in coding, mathematical derivation and compute time.\nIn particular, there is no systematic way to derive the best sampling algorithm\nfor a specific end-task.\n  Tackling this issue, we suggest another level of abstraction: a toolkit to\nend-to-end optimize over all sampling methods to find the one producing\nuser-prescribed properties such as discrepancy or a spectrum that best fit the\nend-task. A user simply implements the forward losses and the sampling method\nis found automatically -- without coding or mathematical derivation -- by\nmaking use of back-propagation abilities of modern deep learning frameworks.\nWhile this optimization takes long, at deployment time the sampling method is\nquick to execute as iterated unstructured non-linear filtering using radial\nbasis functions (RBFs) to represent high-dimensional kernels. Several important\nprevious methods are special cases of this approach, which we compare to\nprevious work and demonstrate its usefulness in several typical Computer\nGraphics applications. Finally, we propose sampling patterns with properties\nnot shown before, such as high-dimensional blue noise with projective\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:02:14 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Leimk\u00fchler", "Thomas", ""], ["Singh", "Gurprit", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1806.07179", "submitter": "Tom Kelly", "authors": "Tom Kelly, Paul Guerrero, Anthony Steed, Peter Wonka, and Niloy J.\n  Mitra", "title": "FrankenGAN: Guided Detail Synthesis for Building Mass-Models Using\n  Style-Synchonized GANs", "comments": "project page: http://geometry.cs.ucl.ac.uk/projects/2018/frankengan/", "journal-ref": "ACM Trans. Graph. 2018, (37), 6", "doi": "10.1145/3272127.3275065", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coarse building mass models are now routinely generated at scales ranging\nfrom individual buildings through to whole cities. For example, they can be\nabstracted from raw measurements, generated procedurally, or created manually.\nHowever, these models typically lack any meaningful semantic or texture\ndetails, making them unsuitable for direct display. We introduce the problem of\nautomatically and realistically decorating such models by adding semantically\nconsistent geometric details and textures. Building on the recent success of\ngenerative adversarial networks (GANs), we propose FrankenGAN, a cascade of\nGANs to create plausible details across multiple scales over large\nneighborhoods. The various GANs are synchronized to produce consistent style\ndistributions over buildings and neighborhoods. We provide the user with direct\ncontrol over the variability of the output. We allow her to interactively\nspecify style via images and manipulate style-adapted sliders to control style\nvariability. We demonstrate our system on several large-scale examples. The\ngenerated outputs are qualitatively evaluated via a set of user studies and are\nfound to be realistic, semantically-plausible, and style-consistent.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:29:13 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 18:24:01 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Kelly", "Tom", ""], ["Guerrero", "Paul", ""], ["Steed", "Anthony", ""], ["Wonka", "Peter", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1806.07467", "submitter": "Yi Zhou", "authors": "Yi Zhou, Liwen Hu, Jun Xing, Weikai Chen, Han-Wei Kung, Xin Tong, Hao\n  Li", "title": "HairNet: Single-View Hair Reconstruction using Convolutional Neural\n  Networks", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning-based method to generate full 3D hair geometry\nfrom an unconstrained image. Our method can recover local strand details and\nhas real-time performance. State-of-the-art hair modeling techniques rely on\nlarge hairstyle collections for nearest neighbor retrieval and then perform\nad-hoc refinement. Our deep learning approach, in contrast, is highly efficient\nin storage and can run 1000 times faster while generating hair with 30K\nstrands. The convolutional neural network takes the 2D orientation field of a\nhair image as input and generates strand features that are evenly distributed\non the parameterized 2D scalp. We introduce a collision loss to synthesize more\nplausible hairstyles, and the visibility of each strand is also used as a\nweight term to improve the reconstruction accuracy. The encoder-decoder\narchitecture of our network naturally provides a compact and continuous\nrepresentation for hairstyles, which allows us to interpolate naturally between\nhairstyles. We use a large set of rendered synthetic hair models to train our\nnetwork. Our method scales to real images because an intermediate 2D\norientation field, automatically calculated from the real image, factors out\nthe difference between synthetic and real hairs. We demonstrate the\neffectiveness and robustness of our method on a wide range of challenging real\nInternet pictures and show reconstructed hair sequences from videos.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:08:07 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 16:11:17 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Zhou", "Yi", ""], ["Hu", "Liwen", ""], ["Xing", "Jun", ""], ["Chen", "Weikai", ""], ["Kung", "Han-Wei", ""], ["Tong", "Xin", ""], ["Li", "Hao", ""]]}, {"id": "1806.07729", "submitter": "Pedro Hermosilla Casajus", "authors": "Julian Kreiser, Pedro Hermosilla, Timo Ropinski", "title": "Void Space Surfaces to Convey Depth in Vessel Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance depth perception and thus data comprehension, additional depth\ncues are often used in 3D visualizations of complex vascular structures.\nAccordingly, there is a variety of different approaches described in the\nliterature, ranging from chromadepth color coding over depth of field to\nglyph-based encodings. Unfortunately, the majority of existing approaches\nsuffers from the same problem. As these cues are directly applied to the\ngeometry's surface, the display of additional information, such as other\nmodalities or derived attributes, associated with a vessel is impaired. To\novercome this limitation we propose Void Space Surfaces which utilize the empty\nspace in between vessel branches to communicate depth and their relative\npositioning. This allows us to enhance the depth perception of vascular\nstructures without interfering with the spatial data and potentially\nsuperimposed parameter information. Within this paper we introduce Void Space\nSurfaces, describe their technical realization, and show their application to\nvarious vessel trees. Moreover, we report the outcome of a user study which we\nhave conducted in order to evaluate the perceptual impact of Void Space\nSurfaces as compared to existing vessel visualization techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:45:33 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kreiser", "Julian", ""], ["Hermosilla", "Pedro", ""], ["Ropinski", "Timo", ""]]}, {"id": "1806.07889", "submitter": "Aron Monszpart", "authors": "Aron Monszpart, Paul Guerrero, Duygu Ceylan, Ersin Yumer, Niloy J.\n  Mitra", "title": "iMapper: Interaction-guided Joint Scene and Human Motion Mapping from\n  Monocular Videos", "comments": null, "journal-ref": "Siggraph 2019", "doi": "10.1145/3306346.3322961", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing challenge in scene analysis is the recovery of scene\narrangements under moderate to heavy occlusion, directly from monocular video.\nWhile the problem remains a subject of active research, concurrent advances\nhave been made in the context of human pose reconstruction from monocular\nvideo, including image-space feature point detection and 3D pose recovery.\nThese methods, however, start to fail under moderate to heavy occlusion as the\nproblem becomes severely under-constrained. We approach the problems\ndifferently. We observe that people interact similarly in similar scenes.\nHence, we exploit the correlation between scene object arrangement and motions\nperformed in that scene in both directions: first, typical motions performed\nwhen interacting with objects inform us about possible object arrangements; and\nsecond, object arrangements, in turn, constrain the possible motions.\n  We present iMapper, a data-driven method that focuses on identifying\nhuman-object interactions, and jointly reasons about objects and human movement\nover space-time to recover both a plausible scene arrangement and consistent\nhuman interactions. We first introduce the notion of characteristic\ninteractions as regions in space-time when an informative human-object\ninteraction happens. This is followed by a novel occlusion-aware matching\nprocedure that searches and aligns such characteristic snapshots from an\ninteraction database to best explain the input monocular video. Through\nextensive evaluations, both quantitative and qualitative, we demonstrate that\niMapper significantly improves performance over both dedicated state-of-the-art\nscene analysis and 3D human pose recovery approaches, especially under medium\nto heavy occlusion.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:47:50 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Monszpart", "Aron", ""], ["Guerrero", "Paul", ""], ["Ceylan", "Duygu", ""], ["Yumer", "Ersin", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1806.08126", "submitter": "Julien Tierny", "authors": "Guillaume Favelier (PEQUAN), Charles Gueunet (PEQUAN), Attila Gyulassy\n  (SCI Institute), Julien Kitware, Joshua Levine, Jonas Lukasczyk (TU\n  Kaiserslautern), Daisuke Sakurai (ZIB), Maxime Soler (PEQUAN), Julien Tierny\n  (PEQUAN), Will Usher (SCI Institute), Qi Wu (SCI Institute, UC Davis)", "title": "Topological Data Analysis Made Easy with the Topology ToolKit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial presents topological methods for the analysis and visualization\nof scientific data from a user's perspective, with the Topology ToolKit (TTK),\na recently released open-source library for topological data analysis.\nTopological methods have gained considerably in popularity and maturity over\nthe last twenty years and success stories of established methods have been\ndocumented in a wide range of applications (combustion, chemistry,\nastrophysics, material sciences, etc.) with both acquired and simulated data,\nin both post-hoc and in-situ contexts. While reference textbooks have been\npublished on the topic, no tutorial at IEEE VIS has covered this area in recent\nyears, and never at a software level and from a user's point-of-view. This\ntutorial fills this gap by providing a beginner's introduction to topological\nmethods for practitioners, researchers, students, and lecturers. In particular,\ninstead of focusing on theoretical aspects and algorithmic details, this\ntutorial focuses on how topological methods can be useful in practice for\nconcrete data analysis tasks such as segmentation, feature extraction or\ntracking. The tutorial describes in detail how to achieve these tasks with TTK.\nFirst, after an introduction to topological methods and their application in\ndata analysis, a brief overview of TTK's main entry point for end users, namely\nParaView, will be presented. Second, an overview of TTK's main features will be\ngiven. A running example will be described in detail, showcasing how to access\nTTK's features via ParaView, Python, VTK/C++, and C++. Third, hands-on sessions\nwill concretely show how to use TTK in ParaView for multiple, representative\ndata analysis tasks. Fourth, the usage of TTK will be presented for developers,\nin particular by describing several examples of visualization and data analysis\nprojects that were built on top of TTK. Finally, some feedback regarding the\nusage of TTK as a teaching platform for topological analysis will be given.\nPresenters of this tutorial include experts in topological methods, core\nauthors of TTK as well as active users, coming from academia, labs, or\nindustry. A large part of the tutorial will be dedicated to hands-on exercises\nand a rich material package (including TTK pre-installs in virtual machines,\ncode, data, demos, video tutorials, etc.) will be provided to the participants.\nThis tutorial mostly targets students, practitioners and researchers who are\nnot experts in topological methods but who are interested in using them in\ntheir daily tasks. We also target researchers already familiar to topological\nmethods and who are interested in using or contributing to TTK.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:18:58 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Favelier", "Guillaume", "", "PEQUAN"], ["Gueunet", "Charles", "", "PEQUAN"], ["Gyulassy", "Attila", "", "SCI Institute"], ["Kitware", "Julien", "", "TU\n  Kaiserslautern"], ["Levine", "Joshua", "", "TU\n  Kaiserslautern"], ["Lukasczyk", "Jonas", "", "TU\n  Kaiserslautern"], ["Sakurai", "Daisuke", "", "ZIB"], ["Soler", "Maxime", "", "PEQUAN"], ["Tierny", "Julien", "", "PEQUAN"], ["Usher", "Will", "", "SCI Institute"], ["Wu", "Qi", "", "SCI Institute, UC Davis"]]}, {"id": "1806.08460", "submitter": "Lin Yan", "authors": "Lin Yan, Yaodong Zhao, Paul Rosen, Carlos Scheidegger, Bei Wang", "title": "Homology-Preserving Dimensionality Reduction via Manifold Landmarking\n  and Tearing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is an integral part of data visualization. It is a\nprocess that obtains a structure preserving low-dimensional representation of\nthe high-dimensional data. Two common criteria can be used to achieve a\ndimensionality reduction: distance preservation and topology preservation.\nInspired by recent work in topological data analysis, we are on the quest for a\ndimensionality reduction technique that achieves the criterion of homology\npreservation, a generalized version of topology preservation. Specifically, we\nare interested in using topology-inspired manifold landmarking and manifold\ntearing to aid such a process and evaluate their effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 00:44:23 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Yan", "Lin", ""], ["Zhao", "Yaodong", ""], ["Rosen", "Paul", ""], ["Scheidegger", "Carlos", ""], ["Wang", "Bei", ""]]}, {"id": "1806.08485", "submitter": "Zhongping Ji", "authors": "Zhongping Ji, Xiao Qi, Yigang Wang, Gang Xu, Peng Du, and Qing Wu", "title": "Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction\n  from Binary Mask Images", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D content creation is referred to as one of the most fundamental tasks of\ncomputer graphics. And many 3D modeling algorithms from 2D images or curves\nhave been developed over the past several decades. Designers are allowed to\nalign some conceptual images or sketch some suggestive curves, from front,\nside, and top views, and then use them as references in constructing a 3D model\nautomatically or manually. However, to the best of our knowledge, no studies\nhave investigated on 3D human body reconstruction in a similar manner. In this\npaper, we propose a deep learning based reconstruction of 3D human body shape\nfrom 2D orthographic views. A novel CNN-based regression network, with two\nbranches corresponding to frontal and lateral views respectively, is designed\nfor estimating 3D human body shape from 2D mask images. We train our networks\nseparately to decouple the feature descriptors which encode the body parameters\nfrom different views, and fuse them to estimate an accurate human body shape.\nIn addition, to overcome the shortage of training data required for this\npurpose, we propose some significantly data augmentation schemes for 3D human\nbody shapes, which can be used to promote further research on this topic.\nExtensive experimen- tal results demonstrate that visually realistic and\naccurate reconstructions can be achieved effectively using our algorithm.\nRequiring only binary mask images, our method can help users create their own\ndigital avatars quickly, and also make it easy to create digital human body for\n3D game, virtual reality, online fashion shopping.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 04:00:37 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Ji", "Zhongping", ""], ["Qi", "Xiao", ""], ["Wang", "Yigang", ""], ["Xu", "Gang", ""], ["Du", "Peng", ""], ["Wu", "Qing", ""]]}, {"id": "1806.08572", "submitter": "Omair Hassaan", "authors": "Omair Hassaan, Abeera Shamail, Zain Butt, Murtaza Taj", "title": "Point cloud segmentation using hierarchical tree for architectural\n  models", "comments": "9 pages. 10 figures. Submitted in EuroGraphics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in the 3D scanning technologies have made the generation\nof highly accurate 3D point clouds relatively easy but the segmentation of\nthese point clouds remains a challenging area. A number of techniques have set\nprecedent of either planar or primitive based segmentation in literature. In\nthis work, we present a novel and an effective primitive based point cloud\nsegmentation algorithm. The primary focus, i.e. the main technical contribution\nof our method is a hierarchical tree which iteratively divides the point cloud\ninto segments. This tree uses an exclusive energy function and a 3D\nconvolutional neural network, HollowNets to classify the segments. We test the\nefficacy of our proposed approach using both real and synthetic data obtaining\nan accuracy greater than 90% for domes and minarets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:36:21 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Hassaan", "Omair", ""], ["Shamail", "Abeera", ""], ["Butt", "Zain", ""], ["Taj", "Murtaza", ""]]}, {"id": "1806.08666", "submitter": "Zhiyong Wang", "authors": "Zhiyong Wang, Jinxiang Chai, Shihong Xia", "title": "Combining Recurrent Neural Networks and Adversarial Training for Human\n  Motion Synthesis and Control", "comments": "12 pages. arXiv admin note: text overlap with arXiv:1612.07828 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new generative deep learning network for human motion\nsynthesis and control. Our key idea is to combine recurrent neural networks\n(RNNs) and adversarial training for human motion modeling. We first describe an\nefficient method for training a RNNs model from prerecorded motion data. We\nimplement recurrent neural networks with long short-term memory (LSTM) cells\nbecause they are capable of handling nonlinear dynamics and long term temporal\ndependencies present in human motions. Next, we train a refiner network using\nan adversarial loss, similar to Generative Adversarial Networks (GANs), such\nthat the refined motion sequences are indistinguishable from real motion\ncapture data using a discriminative network. We embed contact information into\nthe generative deep learning model to further improve the performance of our\ngenerative model. The resulting model is appealing to motion synthesis and\ncontrol because it is compact, contact-aware, and can generate an infinite\nnumber of naturally looking motions with infinite lengths. Our experiments show\nthat motions generated by our deep learning model are always highly realistic\nand comparable to high-quality motion capture data. We demonstrate the power\nand effectiveness of our models by exploring a variety of applications, ranging\nfrom random motion synthesis, online/offline motion control, and motion\nfiltering. We show the superiority of our generative model by comparison\nagainst baseline models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:50:09 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Wang", "Zhiyong", ""], ["Chai", "Jinxiang", ""], ["Xia", "Shihong", ""]]}, {"id": "1806.09058", "submitter": "Jincai Chang", "authors": "Ying He, Jincai Chang", "title": "Golden interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the classic aesthetic interpolation problem, we propose an entirely new\nthought: apply the golden section. For how to apply the golden section to\ninterpolation methods, we present three examples: the golden step\ninterpolation, the golden piecewise linear interpolation and the golden curve\ninterpolation, which respectively deal with the applications of golden section\nin the interpolation of degree 0, 1, and 2 in the plane. In each example, we\npresent our basic ideas, the specific methods, comparative examples and\napplications, and relevant criteria. And it is worth mentioning that for\naesthetics, we propose two novel concepts: the golden cuspidal hill and the\ngolden domed hill. This paper aims to provide the reference for the combination\nof golden section and interpolation, and stimulate more and better related\nresearches.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 00:31:35 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 06:01:38 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["He", "Ying", ""], ["Chang", "Jincai", ""]]}, {"id": "1806.09070", "submitter": "Gokul Swamy", "authors": "Patrick Chao, Alexander Li, Gokul Swamy", "title": "Generative Models for Pose Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate nearest neighbor and generative models for transferring pose\nbetween persons. We take in a video of one person performing a sequence of\nactions and attempt to generate a video of another person performing the same\nactions. Our generative model (pix2pix) outperforms k-NN at both generating\ncorresponding frames and generalizing outside the demonstrated action set. Our\nmost salient contribution is determining a pipeline (pose detection, face\ndetection, k-NN based pairing) that is effective at perform-ing the desired\ntask. We also detail several iterative improvements and failure modes.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 02:33:00 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chao", "Patrick", ""], ["Li", "Alexander", ""], ["Swamy", "Gokul", ""]]}, {"id": "1806.09174", "submitter": "Noshaba Cheema", "authors": "Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han\n  Du, Klaus Fischer, Philipp Slusallek", "title": "Dilated Temporal Fully-Convolutional Network for Semantic Segmentation\n  of Motion Capture Data", "comments": "Eurographics/ ACM SIGGRAPH Symposium on Computer Animation - Posters\n  2018;\n  $\\href{http://people.mpi-inf.mpg.de/~ncheema/SCA2018_poster.pdf}{\\textit{Poster\n  can be found here.}}$", "journal-ref": null, "doi": "10.2312/sca.20181185", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of motion capture sequences plays a key part in many\ndata-driven motion synthesis frameworks. It is a preprocessing step in which\nlong recordings of motion capture sequences are partitioned into smaller\nsegments. Afterwards, additional methods like statistical modeling can be\napplied to each group of structurally-similar segments to learn an abstract\nmotion manifold. The segmentation task however often remains a manual task,\nwhich increases the effort and cost of generating large-scale motion databases.\nWe therefore propose an automatic framework for semantic segmentation of motion\ncapture data using a dilated temporal fully-convolutional network. Our model\noutperforms a state-of-the-art model in action segmentation, as well as three\nnetworks for sequence modeling. We further show our model is robust against\nhigh noisy training labels.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 16:40:07 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Cheema", "Noshaba", ""], ["Hosseini", "Somayeh", ""], ["Sprenger", "Janis", ""], ["Herrmann", "Erik", ""], ["Du", "Han", ""], ["Fischer", "Klaus", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1806.09594", "submitter": "Carl Vondrick", "authors": "Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama,\n  Kevin Murphy", "title": "Tracking Emerges by Colorizing Videos", "comments": "ECCV 2018. Blog post:\n  https://ai.googleblog.com/2018/06/self-supervised-tracking-via-video.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use large amounts of unlabeled video to learn models for visual tracking\nwithout manual human supervision. We leverage the natural temporal coherency of\ncolor to create a model that learns to colorize gray-scale videos by copying\ncolors from a reference frame. Quantitative and qualitative experiments suggest\nthat this task causes the model to automatically learn to track visual regions.\nAlthough the model is trained without any ground-truth labels, our method\nlearns to track well enough to outperform the latest methods based on optical\nflow. Moreover, our results suggest that failures to track are correlated with\nfailures to colorize, indicating that advancing video colorization may further\nimprove self-supervised visual tracking.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:44:40 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 22:38:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Vondrick", "Carl", ""], ["Shrivastava", "Abhinav", ""], ["Fathi", "Alireza", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin", ""]]}, {"id": "1806.09731", "submitter": "Jo\\~ao Correia", "authors": "Tiago Martins, Jo\\~ao Correia, Ernesto Costa, Penousal Machado", "title": "Evotype: Towards the Evolution of Type Stencils", "comments": "EvoMUSART 2018 Best paper", "journal-ref": null, "doi": "10.1007/978-3-319-77583-8_20", "report-no": null, "categories": "cs.NE cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typefaces are an essential resource employed by graphic designers. The\nincreasing demand for innovative type design work increases the need for good\ntechnological means to assist the designer in the creation of a typeface. We\npresent an evolutionary computation approach for the generation of type\nstencils to draw coherent glyphs for different characters. The proposed system\nemploys a Genetic Algorithm to evolve populations of type stencils. The\nevaluation of each candidate stencil uses a hill climbing algorithm to search\nthe best configurations to draw the target glyphs. We study the interplay\nbetween legibility, coherence and expressiveness, and show how our framework\ncan be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:03:23 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Martins", "Tiago", ""], ["Correia", "Jo\u00e3o", ""], ["Costa", "Ernesto", ""], ["Machado", "Penousal", ""]]}, {"id": "1806.10417", "submitter": "Marvin Eisenberger", "authors": "Marvin Eisenberger, Zorah L\\\"ahner, Daniel Cremers", "title": "Divergence-Free Shape Interpolation and Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to model and calculate deformation fields between\nshapes embedded in $\\mathbb{R}^D$. Our framework combines naturally\ninterpolating the two input shapes and calculating correspondences at the same\ntime. The key idea is to compute a divergence-free deformation field\nrepresented in a coarse-to-fine basis using the Karhunen-Lo\\`eve expansion. The\nadvantages are that there is no need to discretize the embedding space and the\ndeformation is volume-preserving. Furthermore, the optimization is done on\ndownsampled versions of the shapes but the morphing can be applied to any\nresolution without a heavy increase in complexity. We show results for shape\ncorrespondence, registration, inter- and extrapolation on the TOSCA and FAUST\ndata sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 11:37:24 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:52:34 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Eisenberger", "Marvin", ""], ["L\u00e4hner", "Zorah", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.10748", "submitter": "Ayushi Sinha", "authors": "Ayushi Sinha, Masaru Ishii, Russell H. Taylor, Gregory D. Hager and\n  Austin Reiter", "title": "Towards automatic initialization of registration algorithms using\n  simulated endoscopy images", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registering images from different modalities is an active area of research in\ncomputer aided medical interventions. Several registration algorithms have been\ndeveloped, many of which achieve high accuracy. However, these results are\ndependent on many factors, including the quality of the extracted features or\nsegmentations being registered as well as the initial alignment. Although\nseveral methods have been developed towards improving segmentation algorithms\nand automating the segmentation process, few automatic initialization\nalgorithms have been explored. In many cases, the initial alignment from which\na registration is initiated is performed manually, which interferes with the\nclinical workflow. Our aim is to use scene classification in endoscopic\nprocedures to achieve coarse alignment of the endoscope and a preoperative\nimage of the anatomy. In this paper, we show using simulated scenes that a\nneural network can predict the region of anatomy (with respect to a\npreoperative image) that the endoscope is located in by observing a single\nendoscopic video frame. With limited training and without any hyperparameter\ntuning, our method achieves an accuracy of 76.53 (+/-1.19)%. There are several\navenues for improvement, making this a promising direction of research. Code is\navailable at https://github.com/AyushiSinha/AutoInitialization.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:58:25 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Sinha", "Ayushi", ""], ["Ishii", "Masaru", ""], ["Taylor", "Russell H.", ""], ["Hager", "Gregory D.", ""], ["Reiter", "Austin", ""]]}, {"id": "1806.11335", "submitter": "Tuanfeng Wang", "authors": "Tuanfeng Y. Wang and Duygu Ceylan and Jovan Popovic and Niloy J. Mitra", "title": "Learning a Shared Shape Space for Multimodal Garment Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing real and virtual garments is becoming extremely demanding with\nrapidly changing fashion trends and increasing need for synthesizing realistic\ndressed digital humans for various applications. This necessitates creating\nsimple and effective workflows to facilitate authoring sewing patterns\ncustomized to garment and target body shapes to achieve desired looks.\nTraditional workflow involves a trial-and-error procedure wherein a mannequin\nis draped to judge the resultant folds and the sewing pattern iteratively\nadjusted until the desired look is achieved. This requires time and experience.\nInstead, we present a data-driven approach wherein the user directly indicates\ndesired fold patterns simply by sketching while our system estimates\ncorresponding garment and body shape parameters at interactive rates. The\nrecovered parameters can then be further edited and the updated draped garment\npreviewed. Technically, we achieve this via a novel shared shape space that\nallows the user to seamlessly specify desired characteristics across multimodal\ninput {\\em without} requiring to run garment simulation at design time. We\nevaluate our approach qualitatively via a user study and quantitatively against\ntest datasets, and demonstrate how our system can generate a rich quality of\non-body garments targeted for a range of body shapes while achieving desired\nfold characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:27:01 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 20:51:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wang", "Tuanfeng Y.", ""], ["Ceylan", "Duygu", ""], ["Popovic", "Jovan", ""], ["Mitra", "Niloy J.", ""]]}]