[{"id": "1706.00082", "submitter": "Marco Marchesi", "authors": "Marco Marchesi", "title": "Megapixel Size Image Creation using Generative Adversarial Networks", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its appearance, Generative Adversarial Networks (GANs) have received a\nlot of interest in the AI community. In image generation several projects\nshowed how GANs are able to generate photorealistic images but the results so\nfar did not look adequate for the quality standard of visual media production\nindustry. We present an optimized image generation process based on a Deep\nConvolutional Generative Adversarial Networks (DCGANs), in order to create\nphotorealistic high-resolution images (up to 1024x1024 pixels). Furthermore,\nthe system was fed with a limited dataset of images, less than two thousand\nimages. All these results give more clue about future exploitation of GANs in\nComputer Graphics and Visual Effects.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:43:19 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Marchesi", "Marco", ""]]}, {"id": "1706.00267", "submitter": "Ferhat Ta\\c{s}", "authors": "Ferhat Ta\\c{s}", "title": "On the Design and Invariants of a Ruled Surface", "comments": "27 pages, 4 figures, 1 figure list", "journal-ref": null, "doi": "10.1142/S0219887819500932", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a kind of design of a ruled surface. It combines\nconcepts from the fields of computer aided geometric design and kinematics. A\ndual unit spherical B\\'ezier-like curve on the dual unit sphere (DUS) is\nobtained with respect the control points by a new method. So, with the aid of\nStudy [1] transference principle, a dual unit spherical B\\'ezier-like curve\ncorresponds to a ruled surface. Furthermore, closed ruled surfaces are\ndetermined via control points and integral invariants of these surfaces are\ninvestigated. The results are illustrated by examples.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 20:29:05 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ta\u015f", "Ferhat", ""]]}, {"id": "1706.01021", "submitter": "Fuwen Tan", "authors": "Fuwen Tan, Crispin Bernier, Benjamin Cohen, Vicente Ordonez, Connelly\n  Barnes", "title": "Where and Who? Automatic Semantic-Aware Person Composition", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image compositing is a method used to generate realistic yet fake imagery by\ninserting contents from one image to another. Previous work in compositing has\nfocused on improving appearance compatibility of a user selected foreground\nsegment and a background image (i.e. color and illumination consistency). In\nthis work, we instead develop a fully automated compositing model that\nadditionally learns to select and transform compatible foreground segments from\na large collection given only an input image background. To simplify the task,\nwe restrict our problem by focusing on human instance composition, because\nhuman segments exhibit strong correlations with their background and because of\nthe availability of large annotated data. We develop a novel branching\nConvolutional Neural Network (CNN) that jointly predicts candidate person\nlocations given a background image. We then use pre-trained deep feature\nrepresentations to retrieve person instances from a large segment database.\nExperimental results show that our model can generate composite images that\nlook visually convincing. We also develop a user interface to demonstrate the\npotential application of our method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 03:28:48 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 05:10:30 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Tan", "Fuwen", ""], ["Bernier", "Crispin", ""], ["Cohen", "Benjamin", ""], ["Ordonez", "Vicente", ""], ["Barnes", "Connelly", ""]]}, {"id": "1706.01208", "submitter": "Yuting Yang", "authors": "Yuting Yang, Connelly Barnes", "title": "Approximate Program Smoothing Using Mean-Variance Statistics, with\n  Application to Procedural Shader Bandlimiting", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general method to approximate the convolution of an\narbitrary program with a Gaussian kernel. This process has the effect of\nsmoothing out a program. Our compiler framework models intermediate values in\nthe program as random variables, by using mean and variance statistics. Our\napproach breaks the input program into parts and relates the statistics of the\ndifferent parts, under the smoothing process. We give several approximations\nthat can be used for the different parts of the program. These include the\napproximation of Dorn et al., a novel adaptive Gaussian approximation, Monte\nCarlo sampling, and compactly supported kernels. Our adaptive Gaussian\napproximation is accurate up to the second order in the standard deviation of\nthe smoothing kernel, and mathematically smooth. We show how to construct a\ncompiler that applies chosen approximations to given parts of the input\nprogram. Because each expression can have multiple approximation choices, we\nuse a genetic search to automatically select the best approximations. We apply\nthis framework to the problem of automatically bandlimiting procedural shader\nprograms. We evaluate our method on a variety of complex shaders, including\nshaders with parallax mapping, animation, and spatially varying statistics. The\nresulting smoothed shader programs outperform previous approaches both\nnumerically, and aesthetically, due to the smoothing properties of our\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 06:24:05 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yang", "Yuting", ""], ["Barnes", "Connelly", ""]]}, {"id": "1706.01558", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Jean-S\\'ebastien Franco and Bruno Raffin", "title": "QuickCSG: Fast Arbitrary Boolean Combinations of N Solids", "comments": null, "journal-ref": null, "doi": null, "report-no": "INRIA RR-8687", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QuickCSG computes the result for general N-polyhedron boolean expressions\nwithout an intermediate tree of solids. We propose a vertex-centric view of the\nproblem, which simplifies the identification of final geometric contributions,\nand facilitates its spatial decomposition. The problem is then cast in a single\nKD-tree exploration, geared toward the result by early pruning of any region of\nspace not contributing to the final surface. We assume strong regularity\nproperties on the input meshes and that they are in general position. This\nsimplifying assumption, in combination with our vertex-centric approach,\nimproves the speed of the approach. Complemented with a task-stealing\nparallelization, the algorithm achieves breakthrough performance, one to two\norders of magnitude speedups with respect to state-of-the-art CPU algorithms,\non boolean operations over two to dozens of polyhedra. The algorithm also\noutperforms GPU implementations with approximate discretizations, while\nproducing an output without redundant facets. Despite the restrictive\nassumptions on the input, we show the usefulness of QuickCSG for applications\nwith large CSG problems and strong temporal constraints, e.g. modeling for 3D\nprinters, reconstruction from visual hulls and collision detection.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 23:20:20 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Douze", "Matthijs", ""], ["Franco", "Jean-S\u00e9bastien", ""], ["Raffin", "Bruno", ""]]}, {"id": "1706.02042", "submitter": "Xiaoguang Han", "authors": "Xiaoguang Han and Chang Gao and Yizhou Yu", "title": "DeepSketch2Face: A Deep Learning Based Sketching System for 3D Face and\n  Caricature Modeling", "comments": "12 pages, 16 figures, to appear in SIGGRAPH 2017", "journal-ref": null, "doi": "10.1145/3072959.3073629", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face modeling has been paid much attention in the field of visual computing.\nThere exist many scenarios, including cartoon characters, avatars for social\nmedia, 3D face caricatures as well as face-related art and design, where\nlow-cost interactive face modeling is a popular approach especially among\namateur users. In this paper, we propose a deep learning based sketching system\nfor 3D face and caricature modeling. This system has a labor-efficient\nsketching interface, that allows the user to draw freehand imprecise yet\nexpressive 2D lines representing the contours of facial features. A novel CNN\nbased deep regression network is designed for inferring 3D face models from 2D\nsketches. Our network fuses both CNN and shape based features of the input\nsketch, and has two independent branches of fully connected layers generating\nindependent subsets of coefficients for a bilinear face representation. Our\nsystem also supports gesture based interactions for users to further manipulate\ninitial face models. Both user studies and numerical results indicate that our\nsketching system can help users create face models quickly and effectively. A\nsignificantly expanded face database with diverse identities, expressions and\nlevels of exaggeration is constructed to promote further research and\nevaluation of face modeling techniques.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 04:02:27 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Han", "Xiaoguang", ""], ["Gao", "Chang", ""], ["Yu", "Yizhou", ""]]}, {"id": "1706.02823", "submitter": "Wenqi Xian", "authors": "Wenqi Xian, Patsorn Sangkloy, Varun Agrawal, Amit Raj, Jingwan Lu,\n  Chen Fang, Fisher Yu, James Hays", "title": "TextureGAN: Controlling Deep Image Synthesis with Texture Patches", "comments": "CVPR 2018 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate deep image synthesis guided by sketch, color,\nand texture. Previous image synthesis methods can be controlled by sketch and\ncolor strokes but we are the first to examine texture control. We allow a user\nto place a texture patch on a sketch at arbitrary locations and scales to\ncontrol the desired output texture. Our generative network learns to synthesize\nobjects consistent with these texture suggestions. To achieve this, we develop\na local texture loss in addition to adversarial and content loss to train the\ngenerative network. We conduct experiments using sketches generated from real\nimages and textures sampled from a separate texture database and results show\nthat our proposed algorithm is able to generate plausible images that are\nfaithful to user controls. Ablation studies show that our proposed pipeline can\ngenerate more realistic images than adapting existing methods directly.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 03:35:08 GMT"}, {"version": "v2", "created": "Sat, 23 Dec 2017 08:19:15 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 20:11:56 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Xian", "Wenqi", ""], ["Sangkloy", "Patsorn", ""], ["Agrawal", "Varun", ""], ["Raj", "Amit", ""], ["Lu", "Jingwan", ""], ["Fang", "Chen", ""], ["Yu", "Fisher", ""], ["Hays", "James", ""]]}, {"id": "1706.03024", "submitter": "Marwan Abdellah", "authors": "Marwan Abdellah, Ahmet Bilgili, Stefan Eilemann, Henry Markram, and\n  Felix Sch\\\"urmann", "title": "A Physically Plausible Model for Rendering Highly Scattering Fluorescent\n  Participating Media", "comments": "18 pages, 7 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel extension of the path tracing algorithm that is capable of\ntreating highly scattering participating media in the presence of fluorescent\nstructures. The extension is based on the formulation of the full radiative\ntransfer equation when solved on a per-wavelength-basis, resulting in accurate\nmodel and unbiased algorithm for rendering highly scattering fluorescent\nparticipating media. The model accounts for the intrinsic properties of\nfluorescent dyes including their absorption and emission spectra, molar\nabsorptivity and quantum yield and also their concentration. Our algorithm is\napplied to render highly scattering isotropic fluorescent solutions under\ndifferent illumination conditions. The spectral performance of the model is\nvalidated against emission spectra of different fluorescent dyes that are of\nsignificance in spectroscopy.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 16:32:33 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 08:18:21 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Abdellah", "Marwan", ""], ["Bilgili", "Ahmet", ""], ["Eilemann", "Stefan", ""], ["Markram", "Henry", ""], ["Sch\u00fcrmann", "Felix", ""]]}, {"id": "1706.03497", "submitter": "Yuichi Yagi", "authors": "Yuichi Yagi", "title": "A filter based approach for inbetweening", "comments": "10 pages, in Japanese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a filter based approach for inbetweening. We train a convolutional\nneural network to generate intermediate frames. This network aim to generate\nsmooth animation of line drawings. Our method can process scanned images\ndirectly. Our method does not need to compute correspondence of lines and\ntopological changes explicitly. We experiment our method with real animation\nproduction data. The results show that our method can generate intermediate\nframes partially.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 08:04:42 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Yagi", "Yuichi", ""]]}, {"id": "1706.03950", "submitter": "Alexandre Derouet-Jourdan", "authors": "Alexandre Derouet-Jourdan and Marc Salvati and Theo Jonchier", "title": "Procedural Wang Tile Algorithm for Stochastic Wall Patterns", "comments": "Submitted to ACM Transactions on Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game and movie industries always face the challenge of reproducing\nmaterials. This problem is tackled by combining illumination models and various\ntextures (painted or procedural patterns). Gnerating stochastic wall patterns\nis crucial in the creation of a wide range of backgrounds (castles, temples,\nruins...). A specific Wang tile set was introduced previously to tackle this\nproblem, in a non-procedural fashion. Long lines may appear as visual\nartifacts. We use this tile set in a new procedural algorithm to generate\nstochastic wall patterns. For this purpose, we introduce specific hash\nfunctions implementing a constrained Wang tiling. This technique makes possible\nthe generation of boundless textures while giving control over the maximum line\nlength. The algorithm is simple and easy to implement, and the wall structure\nwe get from the tiles allows to achieve visuals that reproduce all the small\ndetails of artist painted walls.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:21:02 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Derouet-Jourdan", "Alexandre", ""], ["Salvati", "Marc", ""], ["Jonchier", "Theo", ""]]}, {"id": "1706.04077", "submitter": "Juan Quiroz", "authors": "Juan C. Quiroz, Sergiu M. Dascalu", "title": "Interactive Shape Perturbation", "comments": "Preprint. arXiv admin note: substantial text overlap with\n  arXiv:1608.05231", "journal-ref": "International Journal of Computers and Their Applications, IJCA,\n  Vol. 24, No. 1, March 2017, 8 pages", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a web application for the procedural generation of perturbations\nof 3D models. We generate the perturbations by generating vertex shaders that\nchange the positions of vertices that make up the 3D model. The vertex shaders\nare created with an interactive genetic algorithm, which displays to the user\nthe visual effect caused by each vertex shader, allows the user to select the\nvisual effect the user likes best, and produces a new generation of vertex\nshaders using the user feedback as the fitness measure of the genetic\nalgorithm. We use genetic programming to represent each vertex shader as a\ncomputer program. This paper presents details of requirements specification,\nsoftware architecture, high and low-level design, and prototype user interface.\nWe discuss the project's current status and development challenges.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 05:17:18 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Quiroz", "Juan C.", ""], ["Dascalu", "Sergiu M.", ""]]}, {"id": "1706.04496", "submitter": "Haibin Huang", "authors": "Haibin Huang, Evangelos Kalogerakis, Siddhartha Chaudhuri, Duygu\n  Ceylan, Vladimir G. Kim, Ersin Yumer", "title": "Learning Local Shape Descriptors from Part Correspondences With\n  Multi-view Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new local descriptor for 3D shapes, directly applicable to a\nwide range of shape analysis problems such as point correspondences, semantic\nsegmentation, affordance prediction, and shape-to-scan matching. The descriptor\nis produced by a convolutional network that is trained to embed geometrically\nand semantically similar points close to one another in descriptor space. The\nnetwork processes surface neighborhoods around points on a shape that are\ncaptured at multiple scales by a succession of progressively zoomed out views,\ntaken from carefully selected camera positions. We leverage two extremely large\nsources of data to train our network. First, since our network processes\nrendered views in the form of 2D images, we repurpose architectures pre-trained\non massive image datasets. Second, we automatically generate a synthetic dense\npoint correspondence dataset by non-rigid alignment of corresponding shape\nparts in a large collection of segmented 3D models. As a result of these design\nchoices, our network effectively encodes multi-scale local context and\nfine-grained surface detail. Our network can be trained to produce either\ncategory-specific descriptors or more generic descriptors by learning from\nmultiple shape categories. Once trained, at test time, the network extracts\nlocal descriptors for shapes without requiring any part segmentation as input.\nOur method can produce effective local descriptors even for shapes whose\ncategory is unknown or different from the ones used while training. We\ndemonstrate through several experiments that our learned local descriptors are\nmore discriminative compared to state of the art alternatives, and are\neffective in a variety of shape analysis applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:56:07 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 03:35:20 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Huang", "Haibin", ""], ["Kalogerakis", "Evangelos", ""], ["Chaudhuri", "Siddhartha", ""], ["Ceylan", "Duygu", ""], ["Kim", "Vladimir G.", ""], ["Yumer", "Ersin", ""]]}, {"id": "1706.05170", "submitter": "Jerry Liu", "authors": "Jerry Liu and Fisher Yu and Thomas Funkhouser", "title": "Interactive 3D Modeling with a Generative Adversarial Network", "comments": "Published at International Conference on 3D Vision 2017\n  (http://irc.cs.sdu.edu.cn/3dv/index.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the idea of using a generative adversarial network (GAN)\nto assist a novice user in designing real-world shapes with a simple interface.\nThe user edits a voxel grid with a painting interface (like Minecraft). Yet, at\nany time, he/she can execute a SNAP command, which projects the current voxel\ngrid onto a latent shape manifold with a learned projection operator and then\ngenerates a similar, but more realistic, shape using a learned generator\nnetwork. Then the user can edit the resulting shape and snap again until he/she\nis satisfied with the result. The main advantage of this approach is that the\nprojection and generation operators assist novice users to create 3D models\ncharacteristic of a background distribution of object shapes, but without\nhaving to specify all the details. The core new research idea is to use a GAN\nto support this application. 3D GANs have previously been used for shape\ngeneration, interpolation, and completion, but never for interactive modeling.\nThe new challenge for this application is to learn a projection operator that\ntakes an arbitrary 3D voxel model and produces a latent vector on the shape\nmanifold from which a similar and realistic shape can be generated. We develop\nalgorithms for this and other steps of the SNAP processing pipeline and\nintegrate them into a simple modeling tool. Experiments with these algorithms\nand tool suggest that GANs provide a promising approach to computer-assisted\ninteractive modeling.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 08:01:09 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 08:55:45 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Liu", "Jerry", ""], ["Yu", "Fisher", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1706.06759", "submitter": "Chie Furusawa", "authors": "Chie Furusawa, Kazuyuki Hiroshiba, Keisuke Ogaki, Yuri Odagiri", "title": "Comicolorization: Semi-Automatic Manga Colorization", "comments": "to appear in SIGGRAPH Asia 2017 Technical Brief. Project page:\n  https://nico-opendata.jp/en/casestudy/comicolorization/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed \"Comicolorization\", a semi-automatic colorization system for\nmanga images. Given a monochrome manga and reference images as inputs, our\nsystem generates a plausible color version of the manga. This is the first work\nto address the colorization of an entire manga title (a set of manga pages).\nOur method colorizes a whole page (not a single panel) semi-automatically, with\nthe same color for the same character across multiple panels. To colorize the\ntarget character by the color from the reference image, we extract a color\nfeature from the reference and feed it to the colorization network to help the\ncolorization. Our approach employs adversarial loss to encourage the effect of\nthe color features. Optionally, our tool allows users to revise the\ncolorization result interactively. By feeding the color features to our deep\ncolorization network, we accomplish colorization of the entire manga using the\ndesired colors for each panel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 06:52:09 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 11:28:42 GMT"}, {"version": "v3", "created": "Wed, 23 Aug 2017 08:22:42 GMT"}, {"version": "v4", "created": "Thu, 28 Sep 2017 12:58:52 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Furusawa", "Chie", ""], ["Hiroshiba", "Kazuyuki", ""], ["Ogaki", "Keisuke", ""], ["Odagiri", "Yuri", ""]]}, {"id": "1706.06918", "submitter": "Kiyoharu Aizawa Dr. Prof.", "authors": "Paulina Hensman and Kiyoharu Aizawa", "title": "cGAN-based Manga Colorization Using a Single Training Image", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Japanese comic format known as Manga is popular all over the world. It is\ntraditionally produced in black and white, and colorization is time consuming\nand costly. Automatic colorization methods generally rely on greyscale values,\nwhich are not present in manga. Furthermore, due to copyright protection,\ncolorized manga available for training is scarce. We propose a manga\ncolorization method based on conditional Generative Adversarial Networks\n(cGAN). Unlike previous cGAN approaches that use many hundreds or thousands of\ntraining images, our method requires only a single colorized reference image\nfor training, avoiding the need of a large dataset. Colorizing manga using\ncGANs can produce blurry results with artifacts, and the resolution is limited.\nWe therefore also propose a method of segmentation and color-correction to\nmitigate these issues. The final results are sharp, clear, and in high\nresolution, and stay true to the character's original color scheme.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:11:32 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Hensman", "Paulina", ""], ["Aizawa", "Kiyoharu", ""]]}, {"id": "1706.08262", "submitter": "Chungang Zhu", "authors": "Yue Zhang, Chun-Gang Zhu", "title": "Degenerations of NURBS curves while all of weights approaching infinity", "comments": "23 pages, 47 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NURBS curve is widely used in Computer Aided Design and Computer Aided\nGeometric Design. When a single weight approaches infinity, the limit of a\nNURBS curve tends to the corresponding control point. In this paper, a kind of\ncontrol structure of a NURBS curve, called regular control curve, is defined.\nWe prove that the limit of the NURBS curve is exactly its regular control curve\nwhen all of weights approach infinity, where each weight is multiplied by a\ncertain one-parameter function tending to infinity, different for each control\npoint. Moreover, some representative examples are presented to show this\nproperty and indicate its application for shape deformation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 07:40:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhang", "Yue", ""], ["Zhu", "Chun-Gang", ""]]}, {"id": "1706.08891", "submitter": "Lap-Fai Yu", "authors": "Haikun Huang, Ni-Ching Lin, Lorenzo Barrett, Darian Springer,\n  Hsueh-Cheng Wang, Marc Pomplun, Lap-Fai Yu", "title": "Way to Go! Automatic Optimization of Wayfinding Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wayfinding signs play an important role in guiding users to navigate in a\nvirtual environment and in helping pedestrians to find their ways in a\nreal-world architectural site. Conventionally, the wayfinding design of a\nvirtual environment is created manually, so as the wayfinding design of a\nreal-world architectural site. The many possible navigation scenarios, as well\nas the interplay between signs and human navigation, can make the manual design\nprocess overwhelming and non-trivial. As a result, creating a wayfinding design\nfor a typical layout can take months to several years. In this paper, we\nintroduce the Way to Go! approach for automatically generating a wayfinding\ndesign for a given layout. The designer simply has to specify some navigation\nscenarios; our approach will automatically generate an optimized wayfinding\ndesign with signs properly placed considering human agents' visibility and\npossibility of making mistakes during a navigation. We demonstrate the\neffectiveness of our approach in generating wayfinding designs for different\nlayouts such as a train station, a downtown and a canyon. We evaluate our\nresults by comparing different wayfinding designs and show that our optimized\nwayfinding design can guide pedestrians to their destinations effectively and\nefficiently. Our approach can also help the designer visualize the\naccessibility of a destination from different locations, and correct any \"blind\nzone\" with additional signs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:40:08 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Huang", "Haikun", ""], ["Lin", "Ni-Ching", ""], ["Barrett", "Lorenzo", ""], ["Springer", "Darian", ""], ["Wang", "Hsueh-Cheng", ""], ["Pomplun", "Marc", ""], ["Yu", "Lap-Fai", ""]]}, {"id": "1706.09076", "submitter": "Jo\\~ao Miguel Cunha", "authors": "Jo\\~ao M. Cunha, Jo\\~ao Gon\\c{c}alves, Pedro Martins, Penousal Machado\n  and Am\\'ilcar Cardoso", "title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach\n  to Visual Blending", "comments": "9 pages, 8 figures, Proceedings of the Eighth International\n  Conference on Computational Creativity (ICCC-2017). Atlanta, GA, June\n  20th-June 22nd, 2017", "journal-ref": "Proceedings of the Eighth International Conference on\n  Computational Creativity (ICCC-2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A descriptive approach for automatic generation of visual blends is\npresented. The implemented system, the Blender, is composed of two components:\nthe Mapper and the Visual Blender. The approach uses structured visual\nrepresentations along with sets of visual relations which describe how the\nelements (in which the visual representation can be decomposed) relate among\neach other. Our system is a hybrid blender, as the blending process starts at\nthe Mapper (conceptual level) and ends at the Visual Blender (visual\nrepresentation level). The experimental results show that the Blender is able\nto create analogies from input mental spaces and produce well-composed blends,\nwhich follow the rules imposed by its base-analogy and its relations. The\nresulting blends are visually interesting and some can be considered as\nunexpected.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 23:37:22 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 15:59:01 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 16:24:27 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Cunha", "Jo\u00e3o M.", ""], ["Gon\u00e7alves", "Jo\u00e3o", ""], ["Martins", "Pedro", ""], ["Machado", "Penousal", ""], ["Cardoso", "Am\u00edlcar", ""]]}, {"id": "1706.09577", "submitter": "Rui Ma", "authors": "Rui Ma", "title": "Analysis and Modeling of 3D Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": "SFU-CMPT TR 2017-55-3", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a 3D world, performing activities and interacting with objects in\nthe indoor environments everyday. Indoor scenes are the most familiar and\nessential environments in everyone's life. In the virtual world, 3D indoor\nscenes are also ubiquitous in 3D games and interior design. With the fast\ndevelopment of VR/AR devices and the emerging applications, the demand of\nrealistic 3D indoor scenes keeps growing rapidly. Currently, designing detailed\n3D indoor scenes requires proficient 3D designing and modeling skills and is\noften time-consuming. For novice users, creating realistic and complex 3D\nindoor scenes is even more difficult and challenging.\n  Many efforts have been made in different research communities, e.g. computer\ngraphics, vision and robotics, to capture, analyze and generate the 3D indoor\ndata. This report mainly focuses on the recent research progress in graphics on\ngeometry, structure and semantic analysis of 3D indoor data and different\nmodeling techniques for creating plausible and realistic indoor scenes. We\nfirst review works on understanding and semantic modeling of scenes from\ncaptured 3D data of the real world. Then, we focus on the virtual scenes\ncomposed of 3D CAD models and study methods for 3D scene analysis and\nprocessing. After that, we survey various modeling paradigms for creating 3D\nindoor scenes and investigate human-centric scene analysis and modeling, which\nbridge indoor scene studies of graphics, vision and robotics. At last, we\ndiscuss open problems in indoor scene processing that might bring interests to\ngraphics and all related communities.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 04:42:34 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Ma", "Rui", ""]]}, {"id": "1706.10098", "submitter": "Stefan Eilemann", "authors": "Stefan Eilemann, Marwan Abdellah, Nicolas Antille, Ahmet Bilgili,\n  Grigory Chevtchenko, Raphael Dumusc, Cyrille Favreau, Juan Hernando, Daniel\n  Nachbaur, Pawel Podhajski, Jafet Villafranca, Felix Sch\\\"urmann", "title": "From Big Data to Big Displays: High-Performance Visualization at Blue\n  Brain", "comments": "ISC 2017 Visualization at Scale workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blue Brain has pushed high-performance visualization (HPV) to complement its\nHPC strategy since its inception in 2007. In 2011, this strategy has been\naccelerated to develop innovative visualization solutions through increased\nfunding and strategic partnerships with other research institutions.\n  We present the key elements of this HPV ecosystem, which integrates C++\nvisualization applications with novel collaborative display systems. We\nmotivate how our strategy of transforming visualization engines into services\nenables a variety of use cases, not only for the integration with high-fidelity\ndisplays, but also to build service oriented architectures, to link into web\napplications and to provide remote services to Python applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:08:11 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Eilemann", "Stefan", ""], ["Abdellah", "Marwan", ""], ["Antille", "Nicolas", ""], ["Bilgili", "Ahmet", ""], ["Chevtchenko", "Grigory", ""], ["Dumusc", "Raphael", ""], ["Favreau", "Cyrille", ""], ["Hernando", "Juan", ""], ["Nachbaur", "Daniel", ""], ["Podhajski", "Pawel", ""], ["Villafranca", "Jafet", ""], ["Sch\u00fcrmann", "Felix", ""]]}]