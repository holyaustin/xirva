[{"id": "1903.00438", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Ivan Sopin", "title": "Web-Based 3D and Haptic Interactive Environments for e-Learning,\n  Simulation, and Training", "comments": "ISBN:978-3-642-01343-0", "journal-ref": null, "doi": "10.1007/978-3-642-01344-7_26", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge creation occurs in the process of social interaction. As our\nservice-based society is evolving into a knowledge-based society there is an\nacute need for more effective collaboration and knowledge-sharing systems to be\nused by geographically scattered people. We present the use of Web3D components\nand standards, such as X3D, in combination with the haptic (tactile) paradigm,\nfor the development of new communication channels for e-Learning and\nsimulation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:58:06 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Sopin", "Ivan", ""]]}, {"id": "1903.00695", "submitter": "Noshaba Cheema", "authors": "Noshaba Cheema, Somayeh Hosseini, Janis Sprenger, Erik Herrmann, Han\n  Du, Klaus Fischer, Philipp Slusallek", "title": "Fine-Grained Semantic Segmentation of Motion Capture Data using Dilated\n  Temporal Fully-Convolutional Networks", "comments": "Eurographics 2019 - Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion capture data has been widely used in data-driven character\nanimation. In order to generate realistic, natural-looking motions, most\ndata-driven approaches require considerable efforts of pre-processing,\nincluding motion segmentation and annotation. Existing (semi-) automatic\nsolutions either require hand-crafted features for motion segmentation or do\nnot produce the semantic annotations required for motion synthesis and building\nlarge-scale motion databases. In addition, human labeled annotation data\nsuffers from inter- and intra-labeler inconsistencies by design. We propose a\nsemi-automatic framework for semantic segmentation of motion capture data based\non supervised machine learning techniques. It first transforms a motion capture\nsequence into a ``motion image'' and applies a convolutional neural network for\nimage segmentation. Dilated temporal convolutions enable the extraction of\ntemporal information from a large receptive field. Our model outperforms two\nstate-of-the-art models for action segmentation, as well as a popular network\nfor sequence modeling. Most of all, our method is very robust under noisy and\ninaccurate training labels and thus can handle human errors during the labeling\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 12:53:25 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cheema", "Noshaba", ""], ["Hosseini", "Somayeh", ""], ["Sprenger", "Janis", ""], ["Herrmann", "Erik", ""], ["Du", "Han", ""], ["Fischer", "Klaus", ""], ["Slusallek", "Philipp", ""]]}, {"id": "1903.00899", "submitter": "Long Zeng", "authors": "Zeng Long, Dong zhi-kai, Xu yi-fan", "title": "Robust corner and tangent point detection for strokes with deep learning\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust corner and tangent point detection (CTPD) tool is critical for\nsketch-based engineering modeling. This paper proposes a robust CTPD approach\nfor hand-drawn strokes with deep learning approach. Its robustness for users,\nstroke shapes and biased datasets is improved due to multiscaled point contexts\nand a vote scheme. Firstly, all stroke points are classified into segments by\ntwo deep learning networks, based on scaled point contexts which mimic human's\nperception. Then, a vote scheme is adopted to analyze the merge conditions and\noperations for adjacent segments. If most points agree with a stroke's type,\nthis type is accepted. Finally, new corners and tangent points are inserted at\ntransition points. The algorithm's performance is experimented with 1500\nstrokes of 20 shapes. Results show that our algorithm can achieve 95.3% for\nall-or-nothing accuracy and 88.6% accuracy for biased datasets, compared to\n84.6% and 71% of the state-of-the-art CTPD technique, which is heuristic and\nempirical-based.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:17:09 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Long", "Zeng", ""], ["zhi-kai", "Dong", ""], ["yi-fan", "Xu", ""]]}, {"id": "1903.01249", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Adrian Seitan, Dorin M. Popovici, Crenguta M.\n  Bogdan", "title": "Liver Pathology Simulation: Algorithm for Haptic Rendering and Force\n  Maps for Palpation Assessment", "comments": "arXiv admin note: text overlap with arXiv:1812.03325", "journal-ref": "Medicine Meets Virtual Reality MMVR, 2013, pp. 175-181", "doi": "10.3233/978-1-61499-209-7-175", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preoperative gestures include tactile sampling of the mechanical properties\nof biological tissue for both histological and pathological considerations.\nTactile properties used in conjunction with visual cues can provide useful\nfeedback to the surgeon. Development of novel cost effective haptic-based\nsimulators and their introduction in the minimally invasive surgery learning\ncycle can absorb the learning curve for your residents. Receiving pre-training\nin a core set of surgical skills can reduce skill acquisition time and risks.\nWe present the integration of a real-time surface stiffness adjustment\nalgorithm and a novel paradigm -- force maps -- in a visuo-haptic simulator\nmodule designed to train internal organs disease diagnostics through palpation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 18:16:20 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Seitan", "Adrian", ""], ["Popovici", "Dorin M.", ""], ["Bogdan", "Crenguta M.", ""]]}, {"id": "1903.03406", "submitter": "Zhenghai Chen", "authors": "Zhenghai Chen and Tiow-Seng Tan", "title": "Computing Three-dimensional Constrained Delaunay Refinement Using the\n  GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first GPU algorithm for the 3D triangulation refinement\nproblem. For an input of a piecewise linear complex $\\mathcal{G}$ and a\nconstant $B$, it produces, by adding Steiner points, a constrained Delaunay\ntriangulation conforming to $\\mathcal{G}$ and containing tetrahedra mostly of\nradius-edge ratios smaller than $B$. Our implementation of the algorithm shows\nthat it can be an order of magnitude faster than the best CPU algorithm while\nusing a similar amount of Steiner points to produce triangulations of\ncomparable quality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 04:07:04 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Chen", "Zhenghai", ""], ["Tan", "Tiow-Seng", ""]]}, {"id": "1903.03545", "submitter": "Adrian Dalca", "authors": "Adrian V. Dalca, Guha Balakrishnan, John Guttag, Mert R. Sabuncu", "title": "Unsupervised Learning of Probabilistic Diffeomorphic Registration for\n  Images and Surfaces", "comments": "MedIA: Medical Image Analysis (MICCAI2018 Special Issue). Expands on\n  MICCAI 2018 paper (arXiv:1805.04605) by introducing an extension to\n  anatomical surface registration, new experiments, and analysis of\n  diffeomorphic implementations. Keywords: medical image registration;\n  diffeomorphic; invertible; probabilistic modeling; variational inference.\n  Code available at http://voxelmorph.csail.mit.edu. arXiv admin note: text\n  overlap with arXiv:1805.04605", "journal-ref": null, "doi": "10.1016/j.media.2019.07.006", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical deformable registration techniques achieve impressive results and\noffer a rigorous theoretical treatment, but are computationally intensive since\nthey solve an optimization problem for each image pair. Recently,\nlearning-based methods have facilitated fast registration by learning spatial\ndeformation functions. However, these approaches use restricted deformation\nmodels, require supervised labels, or do not guarantee a diffeomorphic\n(topology-preserving) registration. Furthermore, learning-based registration\ntools have not been derived from a probabilistic framework that can offer\nuncertainty estimates.\n  In this paper, we build a connection between classical and learning-based\nmethods. We present a probabilistic generative model and derive an unsupervised\nlearning-based inference algorithm that uses insights from classical\nregistration methods and makes use of recent developments in convolutional\nneural networks (CNNs). We demonstrate our method on a 3D brain registration\ntask for both images and anatomical surfaces, and provide extensive empirical\nanalyses. Our principled approach results in state of the art accuracy and very\nfast runtimes, while providing diffeomorphic guarantees. Our implementation is\navailable at http://voxelmorph.csail.mit.edu.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 16:48:41 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 18:06:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Dalca", "Adrian V.", ""], ["Balakrishnan", "Guha", ""], ["Guttag", "John", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "1903.03837", "submitter": "Mathias Unberath", "authors": "Laura Fink, Sing Chun Lee, Jie Ying Wu, Xingtong Liu, Tianyu Song,\n  Yordanka Stoyanova, Marc Stamminger, Nassir Navab, Mathias Unberath", "title": "LumiPath -- Towards Real-time Physically-based Rendering on Embedded\n  Devices", "comments": "To be presented at MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computational power of today's workstations, real-time\nphysically-based rendering is within reach, rapidly gaining attention across a\nvariety of domains. These have expeditiously applied to medicine, where it is a\npowerful tool for intuitive 3D data visualization. Embedded devices such as\noptical see-through head-mounted displays (OST HMDs) have been a trend for\nmedical augmented reality. However, leveraging the obvious benefits of\nphysically-based rendering remains challenging on these devices because of\nlimited computational power, memory usage, and power consumption. We navigate\nthe compromise between device limitations and image quality to achieve\nreasonable rendering results by introducing a novel light field that can be\nsampled in real-time on embedded devices. We demonstrate its applications in\nmedicine and discuss limitations of the proposed method. An open-source version\nof this project is available at https://github.com/lorafib/LumiPath which\nprovides full insight on implementation and exemplary demonstrational material.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 17:49:44 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:07:40 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Fink", "Laura", ""], ["Lee", "Sing Chun", ""], ["Wu", "Jie Ying", ""], ["Liu", "Xingtong", ""], ["Song", "Tianyu", ""], ["Stoyanova", "Yordanka", ""], ["Stamminger", "Marc", ""], ["Navab", "Nassir", ""], ["Unberath", "Mathias", ""]]}, {"id": "1903.03911", "submitter": "Kai Xu", "authors": "Xiaogang Wang, Bin Zhou, Yahao Shi, Xiaowu Chen, Qinping Zhao, Kai Xu", "title": "Shape2Motion: Joint Analysis of Motion Parts and Attributes from 3D\n  Shapes", "comments": "CVPR 2019 (oral presentation); Corresponding author: Kai Xu\n  (kevin.kai.xu@gmail.com); Project page:\n  www.kevinkaixu.net/projects/shape2motion.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the task of mobility analysis of 3D shapes, we propose joint analysis for\nsimultaneous motion part segmentation and motion attribute estimation, taking a\nsingle 3D model as input. The problem is significantly different from those\ntackled in the existing works which assume the availability of either a\npre-existing shape segmentation or multiple 3D models in different motion\nstates. To that end, we develop Shape2Motion which takes a single 3D point\ncloud as input, and jointly computes a mobility-oriented segmentation and the\nassociated motion attributes. Shape2Motion is comprised of two deep neural\nnetworks designed for mobility proposal generation and mobility optimization,\nrespectively. The key contribution of these networks is the novel motion-driven\nfeatures and losses used in both motion part segmentation and motion attribute\nestimation. This is based on the observation that the movement of a functional\npart preserves the shape structure. We evaluate Shape2Motion with a newly\nproposed benchmark for mobility analysis of 3D shapes. Results demonstrate that\nour method achieves the state-of-the-art performance both in terms of motion\npart segmentation and motion attribute estimation.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 03:24:30 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 13:58:58 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wang", "Xiaogang", ""], ["Zhou", "Bin", ""], ["Shi", "Yahao", ""], ["Chen", "Xiaowu", ""], ["Zhao", "Qinping", ""], ["Xu", "Kai", ""]]}, {"id": "1903.04015", "submitter": "Wenbo Zhao", "authors": "Wenbo Zhao, Xianming Liu, Yongsen Zhao, Xiaopeng Fan, Debin Zhao", "title": "NormalNet: Learning-based Normal Filtering for Mesh Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh denoising is a critical technology in geometry processing that aims to\nrecover high-fidelity 3D mesh models of objects from their noise-corrupted\nversions. In this work, we propose a learning-based normal filtering scheme for\nmesh denoising called NormalNet, which maps the guided normal filtering (GNF)\ninto a deep network. The scheme follows the iterative framework of\nfiltering-based mesh denoising. During each iteration, first, the voxelization\nstrategy is applied on each face in a mesh to transform the irregular local\nstructure into the regular volumetric representation, therefore, both the\nstructure and face normal information are preserved and the convolution\noperations in CNN(Convolutional Neural Network) can be easily performed.\nSecond, instead of the guidance normal generation and the guided filtering in\nGNF, a deep CNN is designed, which takes the volumetric representation as\ninput, and outputs the learned filtered normals. At last, the vertex positions\nare updated according to the filtered normals. Specifically, the iterative\ntraining framework is proposed, in which the generation of training data and\nthe network training are alternately performed, whereas the ground truth\nnormals are taken as the guidance normals in GNF to get the target normals.\nCompared to state-of-the-art works, NormalNet can effectively remove noise\nwhile preserving the original features and avoiding pseudo-features.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 16:04:38 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 11:56:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zhao", "Wenbo", ""], ["Liu", "Xianming", ""], ["Zhao", "Yongsen", ""], ["Fan", "Xiaopeng", ""], ["Zhao", "Debin", ""]]}, {"id": "1903.04630", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang, Lixin Fan, Qiang Wu, Jian Zhang, Chun Yuan", "title": "Fast Registration for cross-source point clouds by using weak regional\n  affinity and pixel-wise refinement", "comments": "ICME 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of 3D acquisition sensors have emerged in recent years and point\ncloud has been widely used in many areas. Accurate and fast registration of\ncross-source 3D point clouds from different sensors is an emerged research\nproblem in computer vision. This problem is extremely challenging because\ncross-source point clouds contain a mixture of various variances, such as\ndensity, partial overlap, large noise and outliers, viewpoint changing. In this\npaper, an algorithm is proposed to align cross-source point clouds with both\nhigh accuracy and high efficiency. There are two main contributions: firstly,\ntwo components, the weak region affinity and pixel-wise refinement, are\nproposed to maintain the global and local information of 3D point clouds. Then,\nthese two components are integrated into an iterative tensor-based registration\nalgorithm to solve the cross-source point cloud registration problem. We\nconduct experiments on synthetic cross-source benchmark dataset and real\ncross-source datasets. Comparison with six state-of-the-art methods, the\nproposed method obtains both higher efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 22:13:46 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Huang", "Xiaoshui", ""], ["Fan", "Lixin", ""], ["Wu", "Qiang", ""], ["Zhang", "Jian", ""], ["Yuan", "Chun", ""]]}, {"id": "1903.05238", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Pablo Martinez-Gonzalez, Alberto Garcia-Garcia, John\n  Alejandro Castro-Vargas, Sergio Orts-Escolano, Jose Garcia-Rodriguez", "title": "A Visually Plausible Grasping System for Object Manipulation and\n  Interaction in Virtual Reality Environments", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2019.07.003", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction in virtual reality (VR) environments is essential to achieve a\npleasant and immersive experience. Most of the currently existing VR\napplications, lack of robust object grasping and manipulation, which are the\ncornerstone of interactive systems. Therefore, we propose a realistic, flexible\nand robust grasping system that enables rich and real-time interactions in\nvirtual environments. It is visually realistic because it is completely\nuser-controlled, flexible because it can be used for different hand\nconfigurations, and robust because it allows the manipulation of objects\nregardless their geometry, i.e. hand is automatically fitted to the object\nshape. In order to validate our proposal, an exhaustive qualitative and\nquantitative performance analysis has been carried out. On the one hand,\nqualitative evaluation was used in the assessment of the abstract aspects such\nas: hand movement realism, interaction realism and motor control. On the other\nhand, for the quantitative evaluation a novel error metric has been proposed to\nvisually analyze the performed grips. This metric is based on the computation\nof the distance from the finger phalanges to the nearest contact point on the\nobject surface. These contact points can be used with different application\npurposes, mainly in the field of robotics. As a conclusion, system evaluation\nreports a similar performance between users with previous experience in virtual\nreality applications and inexperienced users, referring to a steep learning\ncurve.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 22:15:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Oprea", "Sergiu", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Garcia-Garcia", "Alberto", ""], ["Castro-Vargas", "John Alejandro", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1903.06490", "submitter": "Achim Zeileis", "authors": "Achim Zeileis, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D.\n  McWhite, Paul Murrell, Reto Stauffer, Claus O. Wilke", "title": "colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package colorspace provides a flexible toolbox for selecting individual\ncolors or color palettes, manipulating these colors, and employing them in\nstatistical graphics and data visualizations. In particular, the package\nprovides a broad range of color palettes based on the HCL\n(Hue-Chroma-Luminance) color space. The three HCL dimensions have been shown to\nmatch those of the human visual system very well, thus facilitating intuitive\nselection of color palettes through trajectories in this space. Using the HCL\ncolor model general strategies for three types of palettes are implemented: (1)\nQualitative for coding categorical information, i.e., where no particular\nordering of categories is available. (2) Sequential for coding ordered/numeric\ninformation, i.e., going from high to low (or vice versa). (3) Diverging for\ncoding ordered/numeric information around a central neutral value, i.e., where\ncolors diverge from neutral to two extremes. To aid selection and application\nof these palettes the package also contains scales for use with ggplot2, shiny\n(and tcltk) apps for interactive exploration, visualizations of palette\nproperties, accompanying manipulation utilities (like desaturation and\nlighten/darken), and emulation of color vision deficiencies.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:17:58 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Zeileis", "Achim", ""], ["Fisher", "Jason C.", ""], ["Hornik", "Kurt", ""], ["Ihaka", "Ross", ""], ["McWhite", "Claire D.", ""], ["Murrell", "Paul", ""], ["Stauffer", "Reto", ""], ["Wilke", "Claus O.", ""]]}, {"id": "1903.06657", "submitter": "Christos Mousas", "authors": "Christos Mousas, Alexandros Koilias, Dimitris Anastasiou, Banafsheh\n  Rekabdar, Christos-Nikolaos Anagnostopoulos", "title": "Effects of Self-Avatar and Gaze on Avoidance Movement Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study investigates users' movement behavior in a virtual\nenvironment when they attempted to avoid a virtual character. At each iteration\nof the experiment, four conditions (Self-Avatar LookAt, No Self-Avatar LookAt,\nSelf-Avatar No LookAt, and No Self-Avatar No LookAt) were applied to examine\nusers' movement behavior based on kinematic measures. During the experiment, 52\nparticipants were asked to walk from a starting position to a target position.\nA virtual character was placed at the midpoint. Participants were asked to wear\na head-mounted display throughout the task, and their locomotion was captured\nusing a motion capture suit. We analyzed the captured trajectories of the\nparticipants' routes on four kinematic measures to explore whether the four\nexperimental conditions influenced the paths they took. The results indicated\nthat the Self-Avatar LookAt condition affected the path the participants chose\nmore significantly than the other three conditions in terms of length,\nduration, and deviation, but not in terms of speed. Overall, the length and\nduration of the task, as well as the deviation of the trajectory from the\nstraight line, were greater when a self-avatar represented participants. An\nadditional effect on kinematic measures was found in the LookAt (Gaze)\nconditions. Implications for future research are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 12:42:47 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Mousas", "Christos", ""], ["Koilias", "Alexandros", ""], ["Anastasiou", "Dimitris", ""], ["Rekabdar", "Banafsheh", ""], ["Anagnostopoulos", "Christos-Nikolaos", ""]]}, {"id": "1903.06658", "submitter": "Ayub Gubran", "authors": "Ayub A. Gubran, Felix Huang, Tor M. Aamodt", "title": "Surface Compression Using Dynamic Color Palettes", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-chip memory traffic is a major source of power and energy consumption on\nmobile platforms. A large amount of this off-chip traffic is used to manipulate\ngraphics framebuffer surfaces. To cut down the cost of accessing off-chip\nmemory, framebuffer surfaces are compressed to reduce the bandwidth consumed on\nsurface manipulation when rendering or displaying.\n  In this work, we study the compression properties of framebuffer surfaces and\nhighlight the fact that surfaces from different applications have different\ncompression characteristics. We use the results of our analysis to propose a\nscheme, Dynamic Color Palettes (DCP), which achieves higher compression rates\nwith UI and 2D surfaces.\n  DCP is a hardware mechanism for exploiting inter-frame coherence in lossless\nsurface compression; it implements a scheme that dynamically constructs color\npalettes, which are then used to efficiently compress framebuffer surfaces. To\nevaluate DCP, we created an extensive set of OpenGL workload traces from 124\nAndroid applications. We found that DCP improves compression rates by 91% for\nUI and 20% for 2D applications compared to previous proposals. We also evaluate\na hybrid scheme that combines DCP with a generic compression scheme. We found\nthat compression rates improve over previous proposals by 161%, 124% and 83%\nfor UI, 2D and 3D applications, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 01:35:56 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Gubran", "Ayub A.", ""], ["Huang", "Felix", ""], ["Aamodt", "Tor M.", ""]]}, {"id": "1903.06763", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier and Qiyang Hu and Paolo Favaro and Matthias Zwicker", "title": "Smart, Deep Copy-Paste", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel system for smart copy-paste, enabling the\nsynthesis of high-quality results given a masked source image content and a\ntarget image context as input. Our system naturally resolves both shading and\ngeometric inconsistencies between source and target image, resulting in a\nmerged result image that features the content from the pasted source image,\nseamlessly pasted into the target context. Our framework is based on a novel\ntraining image transformation procedure that allows to train a deep\nconvolutional neural network end-to-end to automatically learn a representation\nthat is suitable for copy-pasting. Our training procedure works with any image\ndataset without additional information such as labels, and we demonstrate the\neffectiveness of our system on two popular datasets, high-resolution face\nimages and the more complex Cityscapes dataset. Our technique outperforms the\ncurrent state of the art on face images, and we show promising results on the\nCityscapes dataset, demonstrating that our system generalizes to much higher\nresolution than the training data.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 19:07:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Portenier", "Tiziano", ""], ["Hu", "Qiyang", ""], ["Favaro", "Paolo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1903.07291", "submitter": "Ming-Yu Liu", "authors": "Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu", "title": "Semantic Image Synthesis with Spatially-Adaptive Normalization", "comments": "Accepted as a CVPR 2019 oral paper", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose spatially-adaptive normalization, a simple but effective layer for\nsynthesizing photorealistic images given an input semantic layout. Previous\nmethods directly feed the semantic layout as input to the deep network, which\nis then processed through stacks of convolution, normalization, and\nnonlinearity layers. We show that this is suboptimal as the normalization\nlayers tend to ``wash away'' semantic information. To address the issue, we\npropose using the input layout for modulating the activations in normalization\nlayers through a spatially-adaptive, learned transformation. Experiments on\nseveral challenging datasets demonstrate the advantage of the proposed method\nover existing approaches, regarding both visual fidelity and alignment with\ninput layouts. Finally, our model allows user control over both semantic and\nstyle. Code is available at https://github.com/NVlabs/SPADE .\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 08:12:23 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:41:27 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Park", "Taesung", ""], ["Liu", "Ming-Yu", ""], ["Wang", "Ting-Chun", ""], ["Zhu", "Jun-Yan", ""]]}, {"id": "1903.07614", "submitter": "Laurent Duval", "authors": "Jean-Luc Peyrot and Laurent Duval and Fr\\'ed\\'eric Payan and Lauriane\n  Bouard and L\\'ena\\\"ic Chizat and S\\'ebastien Schneider and Marc Antonini", "title": "HexaShrink, an exact scalable framework for hexahedral meshes with\n  attributes and discontinuities: multiresolution rendering and storage of\n  geoscience models", "comments": null, "journal-ref": null, "doi": "10.1007/s10596-019-9816-2", "report-no": null, "categories": "cs.GR cs.CV cs.DS physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With huge data acquisition progresses realized in the past decades and\nacquisition systems now able to produce high resolution grids and point clouds,\nthe digitization of physical terrains becomes increasingly more precise. Such\nextreme quantities of generated and modeled data greatly impact computational\nperformances on many levels of high-performance computing (HPC): storage media,\nmemory requirements, transfer capability, and finally simulation interactivity,\nnecessary to exploit this instance of big data. Efficient representations and\nstorage are thus becoming \"enabling technologies'' in HPC experimental and\nsimulation science. We propose HexaShrink, an original decomposition scheme for\nstructured hexahedral volume meshes. The latter are used for instance in\nbiomedical engineering, materials science, or geosciences. HexaShrink provides\na comprehensive framework allowing efficient mesh visualization and storage.\nIts exactly reversible multiresolution decomposition yields a hierarchy of\nmeshes of increasing levels of details, in terms of either geometry, continuous\nor categorical properties of cells. Starting with an overview of volume meshes\ncompression techniques, our contribution blends coherently different\nmultiresolution wavelet schemes in different dimensions. It results in a global\nframework preserving discontinuities (faults) across scales, implemented as a\nfully reversible upscaling at different resolutions. Experimental results are\nprovided on meshes of varying size and complexity. They emphasize the\nconsistency of the proposed representation, in terms of visualization,\nattribute downsampling and distribution at different resolutions. Finally,\nHexaShrink yields gains in storage space when combined to lossless compression\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:24:22 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 13:33:17 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Peyrot", "Jean-Luc", ""], ["Duval", "Laurent", ""], ["Payan", "Fr\u00e9d\u00e9ric", ""], ["Bouard", "Lauriane", ""], ["Chizat", "L\u00e9na\u00efc", ""], ["Schneider", "S\u00e9bastien", ""], ["Antonini", "Marc", ""]]}, {"id": "1903.08004", "submitter": "Daniela Giorgi", "authors": "Mario Salinas and Daniela Giorgi and Paolo Cignoni", "title": "ReviewerNet: Visualizing Citation and Authorship Relations for Finding\n  Reviewers", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ReviewerNet, an online, interactive visualization system aimed to\nimprove the reviewer selection process in the academic domain. Given a paper\nsubmitted for publication, we assume that good candidate reviewers can be\nchosen among the authors of a small set of relevant and pertinent papers;\nReviewerNet supports the construction of such set of papers, by visualizing and\nexploring a literature citation network. Then, the system helps to select\nreviewers that are both well distributed in the scientific community and that\ndo not have any conflict-of-interest, by visualising the careers and\nco-authorship relations of candidate reviewers. The system is publicly\navailable, and it has been evaluated by a set of experienced researchers in the\nfield of Computer Graphics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:05:19 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Salinas", "Mario", ""], ["Giorgi", "Daniela", ""], ["Cignoni", "Paolo", ""]]}, {"id": "1903.08356", "submitter": "Omid Alemi", "authors": "Omid Alemi and Philippe Pasquier", "title": "Machine Learning for Data-Driven Movement Generation: a Review of the\n  State of the Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of non-linear and interactive media such as video games has\nincreased the need for automatic movement animation generation. In this survey,\nwe review and analyze different aspects of building automatic movement\ngeneration systems using machine learning techniques and motion capture data.\nWe cover topics such as high-level movement characterization, training data,\nfeatures representation, machine learning models, and evaluation methods. We\nconclude by presenting a discussion of the reviewed literature and outlining\nthe research gaps and remaining challenges for future work.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 06:32:10 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Alemi", "Omid", ""], ["Pasquier", "Philippe", ""]]}, {"id": "1903.08642", "submitter": "Chen-Hsuan Lin", "authors": "Chen-Hsuan Lin, Oliver Wang, Bryan C. Russell, Eli Shechtman, Vladimir\n  G. Kim, Matthew Fisher, Simon Lucey", "title": "Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction", "comments": "Accepted to CVPR 2019 (project page & code:\n  https://chenhsuanlin.bitbucket.io/photometric-mesh-optim/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of 3D object mesh reconstruction from\nRGB videos. Our approach combines the best of multi-view geometric and\ndata-driven methods for 3D reconstruction by optimizing object meshes for\nmulti-view photometric consistency while constraining mesh deformations with a\nshape prior. We pose this as a piecewise image alignment problem for each mesh\nface projection. Our approach allows us to update shape parameters from the\nphotometric error without any depth or mask information. Moreover, we show how\nto avoid a degeneracy of zero photometric gradients via rasterizing from a\nvirtual viewpoint. We demonstrate 3D object mesh reconstruction results from\nboth synthetic and real-world videos with our photometric mesh optimization,\nwhich is unachievable with either na\\\"ive mesh generation networks or\ntraditional pipelines of surface reconstruction without heavy manual\npost-processing.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:58:38 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Lin", "Chen-Hsuan", ""], ["Wang", "Oliver", ""], ["Russell", "Bryan C.", ""], ["Shechtman", "Eli", ""], ["Kim", "Vladimir G.", ""], ["Fisher", "Matthew", ""], ["Lucey", "Simon", ""]]}, {"id": "1903.10170", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Zhiqin Chen, Hui Huang, Daniel Cohen-Or, Hao Zhang", "title": "LOGAN: Unpaired Shape Transform in Latent Overcomplete Space", "comments": "Download supplementary material here ->\n  https://kangxue.org/papers/logan_supp.pdf", "journal-ref": "ACM Transactions on Graphics(Proc. of SIGGRAPH Asia), 38(6),\n  198:1-198:13, 2019", "doi": "10.1145/3355089.3356494", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LOGAN, a deep neural network aimed at learning general-purpose\nshape transforms from unpaired domains. The network is trained on two sets of\nshapes, e.g., tables and chairs, while there is neither a pairing between\nshapes from the domains as supervision nor any point-wise correspondence\nbetween any shapes. Once trained, LOGAN takes a shape from one domain and\ntransforms it into the other. Our network consists of an autoencoder to encode\nshapes from the two input domains into a common latent space, where the latent\ncodes concatenate multi-scale shape features, resulting in an overcomplete\nrepresentation. The translator is based on a generative adversarial network\n(GAN), operating in the latent space, where an adversarial loss enforces\ncross-domain translation while a feature preservation loss ensures that the\nright shape features are preserved for a natural shape transform. We conduct\nablation studies to validate each of our key network designs and demonstrate\nsuperior capabilities in unpaired shape transforms on a variety of examples\nover baselines and state-of-the-art approaches. We show that LOGAN is able to\nlearn what shape features to preserve during shape translation, either local or\nnon-local, whether content or style, depending solely on the input domains for\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:20:48 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 05:37:33 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 01:06:02 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Yin", "Kangxue", ""], ["Chen", "Zhiqin", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.10255", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim and Tobias G\\\"unther", "title": "Robust Reference Frame Extraction from Unsteady 2D Vector Fields with\n  Convolutional Neural Networks", "comments": "Computer Graphics Forum (Proceedings of EuroVis 2019)", "journal-ref": "Computer Graphics Forum (Proc. EuroVis), 38, 3 (2019), 285-295", "doi": "10.1111/cgf.13689", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust feature extraction is an integral part of scientific visualization. In\nunsteady vector field analysis, researchers recently directed their attention\ntowards the computation of near-steady reference frames for vortex extraction,\nwhich is a numerically challenging endeavor. In this paper, we utilize a\nconvolutional neural network to combine two steps of the visualization pipeline\nin an end-to-end manner: the filtering and the feature extraction. We use\nneural networks for the extraction of a steady reference frame for a given\nunsteady 2D vector field. By conditioning the neural network to noisy inputs\nand resampling artifacts, we obtain numerically stabler results than existing\noptimization-based approaches. Supervised deep learning typically requires a\nlarge amount of training data. Thus, our second contribution is the creation of\na vector field benchmark data set, which is generally useful for any local deep\nlearning-based feature extraction. Based on Vatistas velocity profile, we\nformulate a parametric vector field mixture model that we parameterize based on\nnumerically-computed example vector fields in near-steady reference frames.\nGiven the parametric model, we can efficiently synthesize thousands of vector\nfields that serve as input to our deep learning architecture. The proposed\nnetwork is evaluated on an unseen numerical fluid flow simulation.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 11:54:44 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kim", "Byungsoo", ""], ["G\u00fcnther", "Tobias", ""]]}, {"id": "1903.10297", "submitter": "Chenyang Zhu", "authors": "Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Li Yi, Leonidas Guibas,\n  Hao Zhang", "title": "AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AdaCoSeg, a deep neural network architecture for adaptive\nco-segmentation of a set of 3D shapes represented as point clouds. Differently\nfrom the familiar single-instance segmentation problem, co-segmentation is\nintrinsically contextual: how a shape is segmented can vary depending on the\nset it is in. Hence, our network features an adaptive learning module to\nproduce a consistent shape segmentation which adapts to a set. Specifically,\ngiven an input set of unsegmented shapes, we first employ an offline\npre-trained part prior network to propose per-shape parts. Then, the\nco-segmentation network iteratively and} jointly optimizes the part labelings\nacross the set subjected to a novel group consistency loss defined by matrix\nranks. While the part prior network can be trained with noisy and\ninconsistently segmented shapes, the final output of AdaCoSeg is a consistent\npart labeling for the input set, with each shape segmented into up to (a\nuser-specified) K parts. Overall, our method is weakly supervised, producing\nsegmentations tailored to the test set, without consistent ground-truth\nsegmentations. We show qualitative and quantitative results from AdaCoSeg and\nevaluate it via ablation studies and comparisons to state-of-the-art\nco-segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 13:14:23 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 04:30:20 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 03:22:50 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 12:42:47 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 12:55:11 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Chenyang", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yi", "Li", ""], ["Guibas", "Leonidas", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.10754", "submitter": "Marco Livesu", "authors": "Marco Livesu and Nico Pietroni and Enrico Puppo and Alla Sheffer and\n  Paolo Cignoni", "title": "Loopy Cuts: Surface-Field Aware Block Decomposition for Hex-Meshing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fully automatic block-decomposition hexahedral meshing\nalgorithm capable of producing high quality meshes that strictly preserve\nfeature curve networks on the input surface and align with an input surface\ncross-field. We produce all-hex meshes on the vast majority of inputs, and\nintroduce localized non-hex elements only when the surface feature network\nnecessitates those. The input to our framework is a closed surface with a\ncollection of geometric or user-demarcated feature curves and a feature-aligned\nsurface cross-field. Its output is a compact set of blocks whose edges\ninterpolate these features and are loosely aligned with this cross-field. We\nobtain this block decomposition by cutting the input model using a collection\nof simple cutting surfaces bounded by closed surface loops. The set of cutting\nloops spans the input feature curves, ensuring feature preservation, and is\nobtained using a field-space sampling process. The computed loops are uniformly\ndistributed across the surface, cross orthogonally, and are loosely aligned\nwith the cross-field directions, inducing the desired block decomposition. We\nvalidate our method by applying it to a large range of complex inputs and\ncomparing our results to those produced by state-of-the-art alternatives.\nContrary to prior approaches, our framework consistently produces high-quality\nfield aligned meshes while strictly preserving geometric or user-specified\nsurface features.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 09:36:09 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 15:18:21 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Livesu", "Marco", ""], ["Pietroni", "Nico", ""], ["Puppo", "Enrico", ""], ["Sheffer", "Alla", ""], ["Cignoni", "Paolo", ""]]}, {"id": "1903.11228", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Kangxue Yin, Matthew Fisher, Siddhartha Chaudhuri, Hao\n  Zhang", "title": "BAE-NET: Branched Autoencoder for Shape Co-Segmentation", "comments": "Accepted to ICCV 2019. Code: https://github.com/czq142857/BAE-NET\n  Supplementary material: https://www.sfu.ca/~zhiqinc/imseg/sup.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We treat shape co-segmentation as a representation learning problem and\nintroduce BAE-NET, a branched autoencoder network, for the task. The\nunsupervised BAE-NET is trained with a collection of un-segmented shapes, using\na shape reconstruction loss, without any ground-truth labels. Specifically, the\nnetwork takes an input shape and encodes it using a convolutional neural\nnetwork, whereas the decoder concatenates the resulting feature code with a\npoint coordinate and outputs a value indicating whether the point is\ninside/outside the shape. Importantly, the decoder is branched: each branch\nlearns a compact representation for one commonly recurring part of the shape\ncollection, e.g., airplane wings. By complementing the shape reconstruction\nloss with a label loss, BAE-NET is easily tuned for one-shot learning. We show\nunsupervised, weakly supervised, and one-shot learning results by BAE-NET,\ndemonstrating that using only a couple of exemplars, our network can generally\noutperform state-of-the-art supervised methods trained on hundreds of segmented\nshapes. Code is available at https://github.com/czq142857/BAE-NET.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 02:33:20 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 20:38:10 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Chen", "Zhiqin", ""], ["Yin", "Kangxue", ""], ["Fisher", "Matthew", ""], ["Chaudhuri", "Siddhartha", ""], ["Zhang", "Hao", ""]]}, {"id": "1903.12270", "submitter": "Matias Valdenegro-Toro", "authors": "Matias Valdenegro-Toro, Hector Pincheira", "title": "Implementing Noise with Hash functions for Graphics Processing Units", "comments": "Proceedings XXVIII International Conference of the Chilean Computing\n  Science Society (SCCC, 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a modification to Perlin noise which use computable hash functions\ninstead of textures as lookup tables. We implemented the FNV1, Jenkins and\nMurmur hashes on Shader Model 4.0 Graphics Processing Units for noise\ngeneration. Modified versions of the FNV1 and Jenkins hashes provide very close\nperformance compared to a texture based Perlin noise implementation. Our noise\nmodification enables noise function evaluation without any texture fetches,\ntrading computational power for memory bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 21:09:57 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Valdenegro-Toro", "Matias", ""], ["Pincheira", "Hector", ""]]}, {"id": "1903.12359", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Yusan Leung-Liu, Xianfeng Gu, Lok Ming Lui", "title": "Parallelizable global conformal parameterization of simply-connected\n  surfaces via partial welding", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 13(3), 1049-1083 (2020)", "doi": "10.1137/19M125337X", "report-no": null, "categories": "cs.CG cs.DC cs.GR math.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal surface parameterization is useful in graphics, imaging and\nvisualization, with applications to texture mapping, atlas construction,\nregistration, remeshing and so on. With the increasing capability in scanning\nand storing data, dense 3D surface meshes are common nowadays. While meshes\nwith higher resolution better resemble smooth surfaces, they pose computational\ndifficulties for the existing parameterization algorithms. In this work, we\npropose a novel parallelizable algorithm for computing the global conformal\nparameterization of simply-connected surfaces via partial welding maps. A given\nsimply-connected surface is first partitioned into smaller subdomains. The\nlocal conformal parameterizations of all subdomains are then computed in\nparallel. The boundaries of the parameterized subdomains are subsequently\nintegrated consistently using a novel technique called partial welding, which\nis developed based on conformal welding theory. Finally, by solving the Laplace\nequation for each subdomain using the updated boundary conditions, we obtain a\nglobal conformal parameterization of the given surface, with bijectivity\nguaranteed by quasi-conformal theory. By including additional shape\nconstraints, our method can be easily extended to achieve disk conformal\nparameterization for simply-connected open surfaces and spherical conformal\nparameterization for genus-0 closed surfaces. Experimental results are\npresented to demonstrate the effectiveness of our proposed algorithm. When\ncompared to the state-of-the-art conformal parameterization methods, our method\nachieves a significant improvement in both computational time and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 05:43:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 05:56:11 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Leung-Liu", "Yusan", ""], ["Gu", "Xianfeng", ""], ["Lui", "Lok Ming", ""]]}]