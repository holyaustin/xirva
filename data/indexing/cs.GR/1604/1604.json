[{"id": "1604.00047", "submitter": "Niloy J. Mitra", "authors": "Bongjin Koo, Jean Hergel, Sylvain Lefebvre, Niloy J. Mitra", "title": "Towards Zero-Waste Furniture Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional design, shapes are first conceived, and then fabricated. While\nthis decoupling simplifies the design process, it can result in inefficient\nmaterial usage, especially where off-cut pieces are hard to reuse. The\ndesigner, in absence of explicit feedback on material usage remains helpless to\neffectively adapt the design -- even though design variabilities exist. In this\npaper, we investigate {\\em waste minimizing furniture design} wherein based on\nthe current design, the user is presented with design variations that result in\nmore effective usage of materials. Technically, we dynamically analyze material\nspace layout to determine {\\em which} parts to change and {\\em how}, while\nmaintaining original design intent specified in the form of design constraints.\nWe evaluate the approach on simple and complex furniture design scenarios, and\ndemonstrate effective material usage that is difficult, if not impossible, to\nachieve without computational support.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 20:40:14 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Koo", "Bongjin", ""], ["Hergel", "Jean", ""], ["Lefebvre", "Sylvain", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1604.01093", "submitter": "Angela Dai", "authors": "Angela Dai and Matthias Nie{\\ss}ner and Michael Zollh\\\"ofer and\n  Shahram Izadi and Christian Theobalt", "title": "BundleFusion: Real-time Globally Consistent 3D Reconstruction using\n  On-the-fly Surface Re-integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed\nreality and robotic applications. However, scalability brings challenges of\ndrift in pose estimation, introducing significant errors in the accumulated\nmodel. Approaches often require hours of offline processing to globally correct\nmodel errors. Recent online methods demonstrate compelling results, but suffer\nfrom: (1) needing minutes to perform online correction preventing true\nreal-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation\nresulting in many tracking failures; or (3) supporting only unstructured\npoint-based representations, which limit scan quality and applicability. We\nsystematically address these issues with a novel, real-time, end-to-end\nreconstruction framework. At its core is a robust pose estimation strategy,\noptimizing per frame for a global set of camera poses by considering the\ncomplete history of RGB-D input with an efficient hierarchical approach. We\nremove the heavy reliance on temporal tracking, and continually localize to the\nglobally optimized frames instead. We contribute a parallelizable optimization\nframework, which employs correspondences based on sparse features and dense\ngeometric and photometric matching. Our approach estimates globally optimized\n(i.e., bundle adjusted) poses in real-time, supports robust tracking with\nrecovery from gross tracking failures (i.e., relocalization), and re-estimates\nthe 3D model in real-time to ensure global consistency; all within a single\nframework. Our approach outperforms state-of-the-art online systems with\nquality on par to offline methods, but with unprecedented speed and scan\ncompleteness. Our framework leads to a comprehensive online scanning solution\nfor large indoor environments, enabling ease of use and high-quality results.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 00:06:39 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 02:19:47 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 19:00:46 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""], ["Zollh\u00f6fer", "Michael", ""], ["Izadi", "Shahram", ""], ["Theobalt", "Christian", ""]]}, {"id": "1604.01720", "submitter": "Eric Wengrowski", "authors": "Eric Wengrowski, Kristin Dana, Marco Gruteser, and Narayan Mandayam", "title": "Reading Between the Pixels: Photographic Steganography for Camera\n  Display Messaging", "comments": "16 pages with references 8 tables and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit human color metamers to send light-modulated messages less visible\nto the human eye, but recoverable by cameras. These messages are a key\ncomponent to camera-display messaging, such as handheld smartphones capturing\ninformation from electronic signage. Each color pixel in the display image is\nmodified by a particular color gradient vector. The challenge is to find the\ncolor gradient that maximizes camera response, while minimizing human response.\nThe mismatch in human spectral and camera sensitivity curves creates an\nopportunity for hidden messaging. Our approach does not require knowledge of\nthese sensitivity curves, instead we employ a data-driven method. We learn an\nellipsoidal partitioning of the six-dimensional space of colors and color\ngradients. This partitioning creates metamer sets defined by the base color at\nthe display pixel and the color gradient direction for message encoding. We\nsample from the resulting metamer sets to find color steps for each base color\nto embed a binary message into an arbitrary image with reduced visible\nartifacts. Unlike previous methods that rely on visually obtrusive intensity\nmodulation, we embed with color so that the message is more hidden. Ordinary\ndisplays and cameras are used without the need for expensive LEDs or high speed\ndevices. The primary contribution of this work is a framework to map the pixels\nin an arbitrary image to a metamer pair for steganographic photo messaging.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 18:43:18 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Wengrowski", "Eric", ""], ["Dana", "Kristin", ""], ["Gruteser", "Marco", ""], ["Mandayam", "Narayan", ""]]}, {"id": "1604.01910", "submitter": "\\'Agoston R\\'oth", "authors": "\\'Agoston R\\'oth", "title": "Nielson-type transfinite triangular interpolants by means of quadratic\n  energy functional optimizations", "comments": "17 + 7 = 24 pages (main body text + 2 appendices listing useful\n  univariate and double integrals), 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the transfinite triangular interpolant of (Nielson, 1987) in\norder to generate visually smooth (not necessarily polynomial) local\ninterpolating quasi-optimal triangular spline surfaces. Given as input a\ntriangular mesh stored in a half-edge data structure, at first we produce a\nlocal interpolating network of curves by optimizing quadratic energy\nfunctionals described along the arcs as weighted combinations of squared length\nvariations of first and higher order derivatives, then by optimizing weighted\ncombinations of first and higher order quadratic thin-plate-spline-like\nenergies we locally interpolate each curvilinear face of the previous curve\nnetwork with triangular patches that are usually only $C^0$ continuous along\ntheir common boundaries. In a following step, these local interpolating optimal\ntriangular surface patches are used to construct quasi-optimal continuous\nvector fields of averaged unit normals along the joints, and finally we extend\nthe $G^1$ continuous transfinite triangular interpolation scheme of (Nielson,\n1987) by imposing further optimality constraints concerning the isoparametric\nlines of those groups of three side-vertex interpolants that have to be\nconvexly blended in order to generate the final visually smooth local\ninterpolating quasi-optimal triangular spline surface. While we describe the\nproblem in a general context, we present examples in special polynomial,\ntrigonometric, hyperbolic and algebraic-trigonometric vector spaces of\nfunctions that may be useful both in computer-aided geometric design and in\ncomputer graphics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 08:06:34 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["R\u00f3th", "\u00c1goston", ""]]}, {"id": "1604.02013", "submitter": "Akira Kageyama", "authors": "Akira Kageyama", "title": "Keyboard Based Control of Four Dimensional Rotations", "comments": "Accepted for publication in Journal of Visualization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at applications to the scientific visualization of three dimensional\nsimulations with time evolution, a keyboard based control method to specify\nrotations in four dimensions is proposed. It is known that four dimensional\nrotations are generally so-called double rotations, and a double rotation is a\ncombination of simultaneously applied two simple rotations. The proposed method\ncan specify both the simple and double rotations by single key typings of the\nkeyboard. The method is tested in visualizations of a regular pentachoron in\nfour dimensional space by a hyperplane slicing.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2016 05:20:04 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Kageyama", "Akira", ""]]}, {"id": "1604.02245", "submitter": "Matthias Limmer", "authors": "Matthias Limmer and Hendrik P.A. Lensch", "title": "Infrared Colorization Using Deep Convolutional Neural Networks", "comments": "8 pages, 11 figures, 1 table, submitted to ICMLA2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for transferring the RGB color spectrum to\nnear-infrared (NIR) images using deep multi-scale convolutional neural\nnetworks. A direct and integrated transfer between NIR and RGB pixels is\ntrained. The trained model does not require any user guidance or a reference\nimage database in the recall phase to produce images with a natural appearance.\nTo preserve the rich details of the NIR image, its high frequency features are\ntransferred to the estimated RGB image. The presented approach is trained and\nevaluated on a real-world dataset containing a large amount of road scene\nimages in summer. The dataset was captured by a multi-CCD NIR/RGB camera, which\nensures a perfect pixel to pixel registration.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 07:10:47 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 13:07:59 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 09:35:51 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Limmer", "Matthias", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1604.02483", "submitter": "Yun (Raymond) Fei", "authors": "Yun Fei", "title": "On the Hessian of Shape Matching Energy", "comments": "6 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report we derive the analytic form of the Hessian matrix\nfor shape matching energy. Shape matching is a useful technique for meshless\ndeformation, which can be easily combined with multiple techniques in real-time\ndynamics. Nevertheless, it has been rarely applied in scenarios where implicit\nintegrators are required, and hence strong viscous damping effect, though\npopular in simulation systems nowadays, is forbidden for shape matching. The\nreason lies in the difficulty to derive the Hessian matrix of the shape\nmatching energy. Computing the Hessian matrix correctly, and stably, is the key\nto more broadly application of shape matching in implicitly-integrated systems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:59:46 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 00:16:24 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 09:59:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Fei", "Yun", ""]]}, {"id": "1604.03220", "submitter": "Khalid Khan", "authors": "Alaa Mohammed Obad, Khalid Khan, D.K. Lobiyal and Asif Khan", "title": "Algorithms and Identities for B$\\acute{e}$zier curves via Post Quantum\n  Blossom", "comments": "13 pages, 4 figures, name of two more authors who contributed in\n  revised form added, slight change in title of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new analogue of blossom based on post quantum calculus is\nintroduced. The post quantum blossom has been adapted for developing identities\nand algorithms for Bernstein bases and B$\\acute{e}$zier curves. By applying the\npost quantum blossom, various new identities and formulae expressing the\nmonomials in terms of the post quantun Bernstein basis functions and a post\nquantun variant of Marsden's identity are investigated. For each post quantum\nB$\\acute{e}$zier curves of degree $m,$ a collection of $m!$ new, affine\ninvariant, recursive evaluation algorithms are derived.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2016 16:47:06 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 14:54:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Obad", "Alaa Mohammed", ""], ["Khan", "Khalid", ""], ["Lobiyal", "D. K.", ""], ["Khan", "Asif", ""]]}, {"id": "1604.03755", "submitter": "Mario Fritz", "authors": "Abhishek Sharma, Oliver Grau, Mario Fritz", "title": "VConv-DAE: Deep Volumetric Shape Learning Without Object Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of affordable depth sensors, 3D capture becomes more and more\nubiquitous and already has made its way into commercial products. Yet,\ncapturing the geometry or complete shapes of everyday objects using scanning\ndevices (e.g. Kinect) still comes with several challenges that result in noise\nor even incomplete shapes. Recent success in deep learning has shown how to\nlearn complex shape distributions in a data-driven way from large scale 3D CAD\nModel collections and to utilize them for 3D processing on volumetric\nrepresentations and thereby circumventing problems of topology and\ntessellation. Prior work has shown encouraging results on problems ranging from\nshape completion to recognition. We provide an analysis of such approaches and\ndiscover that training as well as the resulting representation are strongly and\nunnecessarily tied to the notion of object labels. Thus, we propose a full\nconvolutional volumetric auto encoder that learns volumetric representation\nfrom noisy data by estimating the voxel occupancy grids. The proposed method\noutperforms prior work on challenging tasks like denoising and shape\ncompletion. We also show that the obtained deep embedding gives competitive\nperformance when used for classification and promising results for shape\ninterpolation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 13:14:53 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 10:16:33 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 20:36:36 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Sharma", "Abhishek", ""], ["Grau", "Oliver", ""], ["Fritz", "Mario", ""]]}, {"id": "1604.06525", "submitter": "Matthias Nie{\\ss}ner", "authors": "Zachary DeVito, Michael Mara, Michael Zollh\\\"ofer, Gilbert Bernstein,\n  Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher,\n  Matthias Nie{\\ss}ner", "title": "Opt: A Domain Specific Language for Non-linear Least Squares\n  Optimization in Graphics and Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graphics and vision problems can be expressed as non-linear least\nsquares optimizations of objective functions over visual data, such as images\nand meshes. The mathematical descriptions of these functions are extremely\nconcise, but their implementation in real code is tedious, especially when\noptimized for real-time performance on modern GPUs in interactive applications.\nIn this work, we propose a new language, Opt (available under\nhttp://optlang.org), for writing these objective functions over image- or\ngraph-structured unknowns concisely and at a high level. Our compiler\nautomatically transforms these specifications into state-of-the-art GPU solvers\nbased on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate\ndifferent variations of the solver, so users can easily explore tradeoffs in\nnumerical precision, matrix-free methods, and solver approaches. In our\nresults, we implement a variety of real-world graphics and vision applications.\nTheir energy functions are expressible in tens of lines of code, and produce\nhighly-optimized GPU solver implementations. These solver have performance\ncompetitive with the best published hand-tuned, application-specific GPU\nsolvers, and orders of magnitude beyond a general-purpose auto-generated\nsolver.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 03:02:59 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 00:19:31 GMT"}, {"version": "v3", "created": "Sat, 9 Sep 2017 13:23:33 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["DeVito", "Zachary", ""], ["Mara", "Michael", ""], ["Zollh\u00f6fer", "Michael", ""], ["Bernstein", "Gilbert", ""], ["Ragan-Kelley", "Jonathan", ""], ["Theobalt", "Christian", ""], ["Hanrahan", "Pat", ""], ["Fisher", "Matthew", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1604.07378", "submitter": "Tiantian Liu", "authors": "Tiantian Liu, Sofien Bouaziz, Ladislav Kavan", "title": "Towards Real-time Simulation of Hyperelastic Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for real-time physics-based simulation supporting\nmany different types of hyperelastic materials. Previous methods such as\nPosition Based or Projective Dynamics are fast, but support only limited\nselection of materials; even classical materials such as the Neo-Hookean\nelasticity are not supported. Recently, Xu et al. [2015] introduced new\n\"spline-based materials\" which can be easily controlled by artists to achieve\ndesired animation effects. Simulation of these types of materials currently\nrelies on Newton's method, which is slow, even with only one iteration per\ntimestep. In this paper, we show that Projective Dynamics can be interpreted as\na quasi-Newton method. This insight enables very efficient simulation of a\nlarge class of hyperelastic materials, including the Neo-Hookean, spline-based\nmaterials, and others. The quasi-Newton interpretation also allows us to\nleverage ideas from numerical optimization. In particular, we show that our\nsolver can be further accelerated using L-BFGS updates (Limited-memory\nBroyden-Fletcher-Goldfarb-Shanno algorithm). Our final method is typically more\nthan 10 times faster than one iteration of Newton's method without compromising\nquality. In fact, our result is often more accurate than the result obtained\nwith one iteration of Newton's method. Our method is also easier to implement,\nimplying reduced software development costs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:42:37 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Liu", "Tiantian", ""], ["Bouaziz", "Sofien", ""], ["Kavan", "Ladislav", ""]]}, {"id": "1604.07379", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell,\n  Alexei A. Efros", "title": "Context Encoders: Feature Learning by Inpainting", "comments": "New results on ImageNet Generation", "journal-ref": "CVPR 2016", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised visual feature learning algorithm driven by\ncontext-based pixel prediction. By analogy with auto-encoders, we propose\nContext Encoders -- a convolutional neural network trained to generate the\ncontents of an arbitrary image region conditioned on its surroundings. In order\nto succeed at this task, context encoders need to both understand the content\nof the entire image, as well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experimented with both a\nstandard pixel-wise reconstruction loss, as well as a reconstruction plus an\nadversarial loss. The latter produces much sharper results because it can\nbetter handle multiple modes in the output. We found that a context encoder\nlearns a representation that captures not just appearance but also the\nsemantics of visual structures. We quantitatively demonstrate the effectiveness\nof our learned features for CNN pre-training on classification, detection, and\nsegmentation tasks. Furthermore, context encoders can be used for semantic\ninpainting tasks, either stand-alone or as initialization for non-parametric\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 19:42:46 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 20:56:42 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Krahenbuhl", "Philipp", ""], ["Donahue", "Jeff", ""], ["Darrell", "Trevor", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1604.08239", "submitter": "Sam Royston", "authors": "Sam Royston, Connor DeFanti and Ken Perlin", "title": "A Collaborative Untethered Virtual Reality Environment for Interactive\n  Social Network Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prevalence of Virtual Reality technologies as a platform for\ngaming and video playback warrants research into how to best apply the current\nstate of the art to challenges in data visualization. Many current VR systems\nare noncollaborative, while data analysis and visualization is often a\nmulti-person process. Our goal in this paper is to address the technical and\nuser experience challenges that arise when creating VR environments for\ncollaborative data visualization. We focus on the integration of multiple\ntracking systems and the new interaction paradigms that this integration can\nenable, along with visual design considerations that apply specifically to\ncollaborative network visualization in virtual reality. We demonstrate a system\nfor collaborative interaction with large 3D layouts of Twitter friend/follow\nnetworks. The system is built by combining a 'Holojam' architecture (multiple\nGearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron\nmotion suits, to offer an untethered, full-room multi-person visualization\nexperience.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 20:54:37 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Royston", "Sam", ""], ["DeFanti", "Connor", ""], ["Perlin", "Ken", ""]]}, {"id": "1604.08256", "submitter": "Ricardo Fabbri", "authors": "Ricardo Fabbri and Benjamin Kimia", "title": "Multiview Differential Geometry of Curves", "comments": "International Journal of Computer Vision Final Accepted version.\n  International Journal of Computer Vision, 2016. The final publication is\n  available at Springer via http://dx.doi.org/10.1007/s11263-016-0912-7", "journal-ref": null, "doi": "10.1007/s11263-016-0912-7", "report-no": null, "categories": "cs.CV cs.CG cs.GR math.DG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of multiple view geometry has seen tremendous progress in\nreconstruction and calibration due to methods for extracting reliable point\nfeatures and key developments in projective geometry. Point features, however,\nare not available in certain applications and result in unstructured point\ncloud reconstructions. General image curves provide a complementary feature\nwhen keypoints are scarce, and result in 3D curve geometry, but face challenges\nnot addressed by the usual projective geometry of points and algebraic curves.\nWe address these challenges by laying the theoretical foundations of a\nframework based on the differential geometry of general curves, including\nstationary curves, occluding contours, and non-rigid curves, aiming at stereo\ncorrespondence, camera estimation (including calibration, pose, and multiview\nepipolar geometry), and 3D reconstruction given measured image curves. By\ngathering previous results into a cohesive theory, novel results were made\npossible, yielding three contributions. First we derive the differential\ngeometry of an image curve (tangent, curvature, curvature derivative) from that\nof the underlying space curve (tangent, curvature, curvature derivative,\ntorsion). Second, we derive the differential geometry of a space curve from\nthat of two corresponding image curves. Third, the differential motion of an\nimage curve is derived from camera motion and the differential geometry and\nmotion of the space curve. The availability of such a theory enables novel\ncurve-based multiview reconstruction and camera estimation systems to augment\nexisting point-based approaches. This theory has been used to reconstruct a \"3D\ncurve sketch\", to determine camera pose from local curve geometry, and\ntracking; other developments are underway.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 21:55:39 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Kimia", "Benjamin", ""]]}, {"id": "1604.08848", "submitter": "Markus H\\\"oll", "authors": "Markus H\\\"oll, Nikolaus Heran, Vincent Lepetit", "title": "Augmented Reality Oculus Rift", "comments": null, "journal-ref": null, "doi": null, "report-no": "ICG-CVARLab-TR-002", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper covers the whole process of developing an Augmented Reality\nStereoscopig Render Engine for the Oculus Rift. To capture the real world in\nform of a camera stream, two cameras with fish-eye lenses had to be installed\non the Oculus Rift DK1 hardware. The idea was inspired by Steptoe\n\\cite{steptoe2014presence}. After the introduction, a theoretical part covers\nall the most neccessary elements to achieve an AR System for the Oculus Rift,\nfollowing the implementation part where the code from the AR Stereo Engine is\nexplained in more detail. A short conclusion section shows some results,\nreflects some experiences and in the final chapter some future works will be\ndiscussed. The project can be accessed via the git repository\nhttps://github.com/MaXvanHeLL/ARift.git.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 14:26:55 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["H\u00f6ll", "Markus", ""], ["Heran", "Nikolaus", ""], ["Lepetit", "Vincent", ""]]}]