[{"id": "1910.00279", "submitter": "Richard Gerum", "authors": "Richard Gerum", "title": "pylustrator: Code generation for reproducible figures for publication", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": "10.21105/joss.01989", "report-no": null, "categories": "cs.GR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge in science is to make all results potentially\nreproducible. Thus, along with the raw data, every step from basic processing\nof the data, evaluation, to the generation of the figures, has to be documented\nas clearly as possible. While there are many programming libraries that cover\nthe basic processing and plotting steps (e.g. Matplotlib in Python), no library\nyet addresses the reproducible composing of single plots into meaningful\nfigures for publication. Thus, up to now it is still state-of-the-art to\ngenerate publishable figures using image-processing or vector-drawing software\nleading to unwanted alterations of the presented data in the worst case and to\nfigure quality reduction in the best case. Pylustrator a open source library\nbased on the Matplotlib aims to fill this gap and provides a tool to easily\ngenerate the code necessary to compose publication figures from single plots.\nIt provides a graphical user interface where the user can interactively compose\nthe figures. All changes are tracked and converted to code that is\nautomatically integrated into the calling script file. Thus, this software\nprovides the missing link from raw data to the complete plot published in\nscientific journals and thus contributes to the transparency of the complete\nevaluation procedure.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 09:46:46 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 11:22:30 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Gerum", "Richard", ""]]}, {"id": "1910.00935", "submitter": "Yuanming Hu", "authors": "Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan\n  Ragan-Kelley, Fr\\'edo Durand", "title": "DiffTaichi: Differentiable Programming for Physical Simulation", "comments": "Published at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DiffTaichi, a new differentiable programming language tailored for\nbuilding high-performance differentiable physical simulators. Based on an\nimperative programming language, DiffTaichi generates gradients of simulation\nsteps using source code transformations that preserve arithmetic intensity and\nparallelism. A light-weight tape is used to record the whole simulation program\nstructure and replay the gradient kernels in a reversed order, for end-to-end\nbackpropagation. We demonstrate the performance and productivity of our\nlanguage in gradient-based learning and optimization tasks on 10 different\nphysical simulators. For example, a differentiable elastic object simulator\nwritten in our language is 4.2x shorter than the hand-engineered CUDA version\nyet runs as fast, and is 188x faster than the TensorFlow implementation. Using\nour differentiable programs, neural network controllers are typically optimized\nwithin only tens of iterations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 05:00:26 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 13:13:28 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 06:21:07 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hu", "Yuanming", ""], ["Anderson", "Luke", ""], ["Li", "Tzu-Mao", ""], ["Sun", "Qi", ""], ["Carr", "Nathan", ""], ["Ragan-Kelley", "Jonathan", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "1910.01304", "submitter": "Francois Demoullin", "authors": "Francois Demoullin, Ayub Gubran, Tor Aamodt", "title": "Hash-Based Ray Path Prediction: Skipping BVH Traversal Computation by\n  Exploiting Ray Locality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art ray tracing techniques operate on hierarchical acceleration\nstructures such as BVH trees which wrap objects in a scene into bounding\nvolumes of decreasing sizes. Acceleration structures reduce the amount of\nray-scene intersections that a ray has to perform to find the intersecting\nobject. However, we observe a large amount of redundancy when rays are\ntraversing these acceleration structures. While modern acceleration structures\nexplore the spatial organization of the scene, they neglect similarities\nbetween rays that traverse the structures and thereby cause redundant\ntraversals. This paper provides a limit study of a new promising technique,\nHash-Based Ray Path Prediction (HRPP), which exploits the similarity between\nrays to predict leaf nodes to avoid redundant acceleration structure\ntraversals. Our data shows that acceleration structure traversal consumes a\nsignificant proportion of the ray tracing rendering time regardless of the\nplatform or the target image quality. Our study quantifies unused ray locality\nand evaluates the theoretical potential for improved ray traversal performance\nfor both coherent and seemingly incoherent rays. We show that HRPP is able to\nskip, on average, 40% of all hit-all traversal computations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 05:09:58 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Demoullin", "Francois", ""], ["Gubran", "Ayub", ""], ["Aamodt", "Tor", ""]]}, {"id": "1910.01546", "submitter": "Chih Han Chung", "authors": "Yi-Ting Chen, Chi-Hsuan Hsu, Chih-Han Chung, Yu-Shuen Wang, Sabarish\n  V. Babu", "title": "iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking\n  Interface for Study and Reflection in VR Learning Environments", "comments": "9 pages, IEEE VR 2019,\n  https://people.cs.nctu.edu.tw/~yushuen/data/iVRNote19.mp4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we design, implement and evaluate the pedagogical\nbenefits of a novel interactive note taking interface (iVRNote) in VR for the\npurpose of learning and reflection lectures. In future VR learning\nenvironments, students would have challenges in taking notes when they wear a\nhead mounted display (HMD). To solve this problem, we installed a digital\ntablet on the desk and provided several tools in VR to facilitate the learning\nexperience. Specifically, we track the stylus position and orientation in the\nphysical world and then render a virtual stylus in VR. In other words, when\nstudents see a virtual stylus somewhere on the desk, they can reach out with\ntheir hand for the physical stylus. The information provided will also enable\nthem to know where they will draw or write before the stylus touches the\ntablet. Since the presented iVRNote featuring our note taking system is a\ndigital environment, we also enable students save efforts in taking extensive\nnotes by providing several functions, such as post-editing and picture taking,\nso that they can pay more attention to lectures in VR. We also record the time\nof each stroke on the note to help students review a lecture. They can select a\npart of their note to revisit the corresponding segment in a virtual online\nlecture. Figures and the accompanying video demonstrate the feasibility of the\npresented iVRNote system. To evaluate the system, we conducted a user study\nwith 20 participants to assess the preference and pedagogical benefits of the\niVRNote interface.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 15:11:32 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Chen", "Yi-Ting", ""], ["Hsu", "Chi-Hsuan", ""], ["Chung", "Chih-Han", ""], ["Wang", "Yu-Shuen", ""], ["Babu", "Sabarish V.", ""]]}, {"id": "1910.01586", "submitter": "David Murphy", "authors": "David Murphy and Conor Higgins", "title": "Secondary Inputs for Measuring User Engagement in Immersive VR Education\n  Environments", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an experiment to assess the feasibility of using\nsecondary input data as a method of determining user engagement in immersive\nvirtual reality (VR). The work investigates whether secondary data (biosignals)\nacquired from users are useful as a method of detecting levels of\nconcentration, stress, relaxation etc. in immersive environments, and if they\ncould be used to create an affective feedback loop in immersive VR\nenvironments, including educational contexts. A VR Experience was developed in\nthe Unity game engine, with three different levels, each designed to expose the\nuser in one of three different states (relaxation, concentration, stress).\nWhile in the VR Experience users physiological responses were measured using\nECG and EEG sensors. After the experience users completed questionnaires to\nestablish their perceived state during the levels, and to established the\nusability of the system. Next a comparison between the reported levels of\nemotion and the measured signals is presented, which show a strong\ncorrespondence between the two measures indicating that biosignals are a useful\nindicator of emotional state while in VR. Finally we make some recommendations\non the practicalities of using biosensors, and design considerations for their\nincorporation in to a VR system, with particular focus on their integration in\nto task-based training and educational virtual environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 16:39:28 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Murphy", "David", ""], ["Higgins", "Conor", ""]]}, {"id": "1910.01812", "submitter": "Sebastian Weiss", "authors": "Sebastian Weiss, Robert Maier, R\\\"udiger Westermann, Daniel Cremers,\n  Nils Thuerey", "title": "Sparse Surface Constraints for Combining Physics-based Elasticity\n  Simulation and Correspondence-Free Object Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem to infer physical material parameters and boundary\nconditions from the observed motion of a homogeneous deformable object via the\nsolution of an inverse problem. Parameters are estimated from potentially\nunreliable real-world data sources such as sparse observations without\ncorrespondences. We introduce a novel Lagrangian-Eulerian optimization\nformulation, including a cost function that penalizes differences to\nobservations during an optimization run. This formulation matches\ncorrespondence-free, sparse observations from a single-view depth sequence with\na finite element simulation of deformable bodies. In conjunction with an\nefficient hexahedral discretization and a stable, implicit formulation of\ncollisions, our method can be used in demanding situation to recover a variety\nof material parameters, ranging from Young's modulus and Poisson ratio to\ngravity and stiffness damping, and even external boundaries. In a number of\ntests using synthetic datasets and real-world measurements, we analyse the\nrobustness of our approach and the convergence behavior of the numerical\noptimization scheme.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 06:34:17 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Weiss", "Sebastian", ""], ["Maier", "Robert", ""], ["Westermann", "R\u00fcdiger", ""], ["Cremers", "Daniel", ""], ["Thuerey", "Nils", ""]]}, {"id": "1910.02055", "submitter": "Hang Chu", "authors": "Hang Chu, Daiqing Li, David Acuna, Amlan Kar, Maria Shugrina, Xinkai\n  Wei, Ming-Yu Liu, Antonio Torralba, Sanja Fidler", "title": "Neural Turtle Graphics for Modeling City Road Layouts", "comments": "ICCV-2019 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Turtle Graphics (NTG), a novel generative model for spatial\ngraphs, and demonstrate its applications in modeling city road layouts.\nSpecifically, we represent the road layout using a graph where nodes in the\ngraph represent control points and edges in the graph represent road segments.\nNTG is a sequential generative model parameterized by a neural network. It\niteratively generates a new node and an edge connecting to an existing node\nconditioned on the current graph. We train NTG on Open Street Map data and show\nthat it outperforms existing approaches using a set of diverse performance\nmetrics. Moreover, our method allows users to control styles of generated road\nlayouts mimicking existing cities as well as to sketch parts of the city road\nlayout to be synthesized. In addition to synthesis, the proposed NTG finds uses\nin an analytical task of aerial road parsing. Experimental results show that it\nachieves state-of-the-art performance on the SpaceNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:30:00 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Chu", "Hang", ""], ["Li", "Daiqing", ""], ["Acuna", "David", ""], ["Kar", "Amlan", ""], ["Shugrina", "Maria", ""], ["Wei", "Xinkai", ""], ["Liu", "Ming-Yu", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1910.02060", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Vladimir G. Kim, Eli Shechtman, Jun Saito, Serge\n  Belongie", "title": "Neural Puppet: Generative Layered Cartoon Characters", "comments": "WACV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning based method for generating new animations of a cartoon\ncharacter given a few example images. Our method is designed to learn from a\ntraditionally animated sequence, where each frame is drawn by an artist, and\nthus the input images lack any common structure, correspondences, or labels. We\nexpress pose changes as a deformation of a layered 2.5D template mesh, and\ndevise a novel architecture that learns to predict mesh deformations matching\nthe template to a target image. This enables us to extract a common\nlow-dimensional structure from a diverse set of character poses. We combine\nrecent advances in differentiable rendering as well as mesh-aware models to\nsuccessfully align common template even if only a few character images are\navailable during training. In addition to coarse poses, character appearance\nalso varies due to shading, out-of-plane motions, and artistic effects. We\ncapture these subtle changes by applying an image translation network to refine\nthe mesh rendering, providing an end-to-end model to generate new animations of\na character with high visual quality. We demonstrate that our generative model\ncan be used to synthesize in-between frames and to create data-driven\ndeformation. Our template fitting procedure outperforms state-of-the-art\ngeneric techniques for detecting image correspondences.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 17:40:51 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:07:00 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 23:54:09 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Poursaeed", "Omid", ""], ["Kim", "Vladimir G.", ""], ["Shechtman", "Eli", ""], ["Saito", "Jun", ""], ["Belongie", "Serge", ""]]}, {"id": "1910.02480", "submitter": "Giulio Jiang", "authors": "Giulio Jiang, Bernhard Kainz", "title": "Deep Radiance Caching: Convolutional Autoencoders Deeper in Ray Tracing", "comments": "11 pages, 12 figures, accepted by Computer And Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering realistic images with global illumination is a computationally\ndemanding task and often requires dedicated hardware for feasible runtime.\nRecent research uses Deep Neural Networks to predict indirect lighting on image\nlevel, but such methods are commonly limited to diffuse materials and require\ntraining on each scene.We present Deep Radiance Caching (DRC), an efficient\nvariant of Radiance Caching utilizing Convolutional Autoencoders for rendering\nglobal illumination. DRC employs a denoising neural network with Radiance\nCaching to support a wide range of material types, without the requirement of\noffline pre-computation or training for each scene.This offers high performance\nCPU rendering for maximum accessibility. Our method has been evaluated on\ninterior scenes, and is able to produce high-quality images within 180 seconds\non a single CPU.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 17:14:43 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 17:32:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Jiang", "Giulio", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1910.02926", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Alec Jacobson", "title": "Cubic Stylization", "comments": "10 pages, 28 figures, SIGGRAPH Asia 2019", "journal-ref": "ACM Trans. Graph. 38, 6, Article 197 (November 2019)", "doi": "10.1145/3355089.3356495", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a 3D stylization algorithm that can turn an input shape into the\nstyle of a cube while maintaining the content of the original shape. The key\ninsight is that cubic style sculptures can be captured by the\nas-rigid-as-possible energy with an l1-regularization on rotated surface\nnormals. Minimizing this energy naturally leads to a detail-preserving, cubic\ngeometry. Our optimization can be solved efficiently without any mesh surgery.\nOur method serves as a non-realistic modeling tool where one can incorporate\nmany artistic controls to create stylized geometries.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 17:32:05 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 05:02:36 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 16:05:43 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "1910.03310", "submitter": "Ivan Viola", "authors": "Ivan Viola, Min Chen, Tobias Isenberg", "title": "Visual Abstraction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-34444-3_2", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we revisit the concept of abstraction as it is used in\nvisualization and put it on a solid formal footing. While the term\n\\emph{abstraction} is utilized in many scientific disciplines, arts, as well as\neveryday life, visualization inherits the notion of data abstraction or class\nabstraction from computer science, topological abstraction from mathematics,\nand visual abstraction from arts. All these notions have a lot in common, yet\nthere is a major discrepancy in the terminology and basic understanding about\nvisual abstraction in the context of visualization. We thus root the notion of\nabstraction in the philosophy of science, clarify the basic terminology, and\nprovide crisp definitions of visual abstraction as a process. Furthermore, we\nclarify how it relates to similar terms often used interchangeably in the field\nof visualization. Visual abstraction is characterized by a conceptual space\nwhere this process exists, by the purpose it should serve, and by the\nperceptual and cognitive qualities of the beholder. These characteristics can\nbe used to control the process of visual abstraction to produce effective and\ninformative visual representations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 10:00:52 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 08:30:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Viola", "Ivan", ""], ["Chen", "Min", ""], ["Isenberg", "Tobias", ""]]}, {"id": "1910.04037", "submitter": "Kyle Reeser", "authors": "Kyle Reeser, Christopher Conlon, Amber L. Doiron", "title": "Triangle Mesh Slicing and Contour Construction for Three-Dimensional\n  Printing on a Rotating Mandrel", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) printing is a powerful development tool both in\nindustry, as well as in biomedical research. Additive-lathe 3D printing is an\nemerging sub-class of 3D printing whereby material is layered outward from the\nsurface of a rotating cylindrical mandrel. While established additive\nmanufacturing technologies have developed robust toolpath generation software,\nadditive-lathe publications to date have been relegated to the most basic of\nproof-of-concept structures. This paper details the theory and implementation\nof a method for slicing a triangulated surface with a series of concentric,\nopen, right circular cylinders that represents a crucial step in creating\ntoolpaths to print complex models with additive-lathe technology. Valid edge\ncases are detailed which must be addressed when implementing a cylindrical\nslicer to produce non-intersecting closed contours; two classes of resultant\nclosed contour are described. Methodologies for generating infill patterns,\nsupport structures and other considerations for toolpath construction are\nrequired prior to full implementation of a machine capable of printing complex\ngeometry from a digital model onto a rotating cylindrical surface. This work\nrepresents the first thorough examination of the mathematics and algorithmic\nimplementation of triangle mesh slicing with concentric cylinders and offers\ninsights for future works in toolpath generation for the additive-lathe type 3D\nprinter.\n", "versions": [{"version": "v1", "created": "Sat, 5 Oct 2019 11:06:45 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Reeser", "Kyle", ""], ["Conlon", "Christopher", ""], ["Doiron", "Amber L.", ""]]}, {"id": "1910.04386", "submitter": "Thomas Kerdreux", "authors": "Vivien Cabannes and Thomas Kerdreux and Louis Thiry and Tina Campana\n  and Charly Ferrandes", "title": "Dialog on a canvas with a machine", "comments": "Accepted for poster at creativity workshop NeurIPS 2019", "journal-ref": "creativity workshop NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new form of human-machine interaction. It is a pictorial game\nconsisting of interactive rounds of creation between artists and a machine.\nThey repetitively paint one after the other. At its rounds, the computer\npartially completes the drawing using machine learning algorithms, and projects\nits additions directly on the canvas, which the artists are free to insert or\nmodify. Alongside fostering creativity, the process is designed to question the\ngrowing interaction between humans and machines.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 06:33:28 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2019 15:40:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Cabannes", "Vivien", ""], ["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""], ["Campana", "Tina", ""], ["Ferrandes", "Charly", ""]]}, {"id": "1910.04639", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Visual Indeterminacy in GAN Art", "comments": "Leonardo / SIGGRAPH 2020 Art Papers", "journal-ref": "Leonardo, Volume 53, Issue 4, August 2020", "doi": "10.1162/leon_a_01930", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores visual indeterminacy as a description for artwork created\nwith Generative Adversarial Networks (GANs). Visual indeterminacy describes\nimages which appear to depict real scenes, but, on closer examination, defy\ncoherent spatial interpretation. GAN models seem to be predisposed to producing\nindeterminate images, and indeterminacy is a key feature of much modern\nrepresentational art, as well as most GAN art. It is hypothesized that\nindeterminacy is a consequence of a powerful-but-imperfect image synthesis\nmodel that must combine general classes of objects, scenes, and textures.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 15:19:11 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 18:17:26 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 03:23:11 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1910.04699", "submitter": "Weston Aenchbacher", "authors": "Martin Alain, Weston Aenchbacher, Aljosa Smolic", "title": "Interactive Light Field Tilt-Shift Refocus with Generalized\n  Shift-and-Sum", "comments": "4 pages, 5 figures, to be published in Proceedings of the European\n  Light field Imaging Workshop 2019, authors Martin Alain and Weston\n  Aenchbacher contributed equally to this publication, additional results can\n  be found at https://v-sense.scss.tcd.ie/research/tilt-shift/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Since their introduction more than two decades ago, light fields have gained\nconsiderable interest in graphics and vision communities due to their ability\nto provide the user with interactive visual content. One of the earliest and\nmost common light field operations is digital refocus, enabling the user to\nchoose the focus and depth-of-field for the image after capture. A common\ninteractive method for such an operation utilizes disparity estimations,\nreadily available from the light field, to allow the user to point-and-click on\nthe image to chose the location of the refocus plane.\n  In this paper, we address the interactivity of a lesser-known light field\noperation: refocus to a non-frontoparallel plane, simulating the result of\ntraditional tilt-shift photography. For this purpose we introduce a generalized\nshift-and-sum framework. Further, we show that the inclusion of depth\ninformation allows for intuitive interactive methods for placement of the\nrefocus plane. In addition to refocusing, light fields also enable the user to\ninteract with the viewpoint, which can be easily included in the proposed\ngeneralized shift-and-sum framework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 16:56:52 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Alain", "Martin", ""], ["Aenchbacher", "Weston", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1910.04942", "submitter": "Jianhui Nie", "authors": "Jianhui Nie, Zhaochen Zhang, Ye Liu, Hao Gao, Feng Xu, WenKai Shi", "title": "Point cloud ridge-valley feature enhancement based on position and\n  normal guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge-valley features are important elements of point clouds, as they contain\nrich surface information. To recognize these features from point clouds, this\npaper introduces an extreme point distance (EPD) criterion with scale\nindependence. Compared with traditional methods, the EPD greatly reduces the\nnumber of potential feature points and improves the robustness of multiscale\nfeature point recognition. On this basis, a feature enhancement algorithm based\non user priori guidance is proposed that adjusts the coordinates of the feature\narea by solving an objective equation containing the expected position and\nnormal constraints. Since the expected normal can be expressed as a function of\nneighborhood point coordinates, the above objective equation can be converted\ninto linear sparse equations with enhanced feature positions as variables, and\nthus, the closed solution can be obtained. In addition, a parameterization\nmethod for scattered point clouds based on feature line guidance is proposed,\nwhich reduces the number of unknowns by 2/3 and eliminates lateral sliding in\nthe direction perpendicular to feature lines. Finally, the application of the\nalgorithm in multiscale ridge-valley feature recognition, freeform surface\nfeature enhancement and computer-aided design (CAD) workpiece sharp feature\nrestoration verifies its effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 02:21:30 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Nie", "Jianhui", ""], ["Zhang", "Zhaochen", ""], ["Liu", "Ye", ""], ["Gao", "Hao", ""], ["Xu", "Feng", ""], ["Shi", "WenKai", ""]]}, {"id": "1910.04987", "submitter": "Yue Gao", "authors": "Yue Gao, Yuan Guo, Zhouhui Lian, Yingmin Tang, Jianguo Xiao", "title": "Artistic Glyph Image Synthesis via One-Stage Few-Shot Learning", "comments": "Accepted by SIGGRAPH Asia 2019, code and datasets:\n  https://hologerry.github.io/AGIS-Net/", "journal-ref": "ACM Trans. Graph. 38, 6, Article 185 (November 2019), 12 pages", "doi": "10.1145/3355089.3356574", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic generation of artistic glyph images is a challenging task that\nattracts many research interests. Previous methods either are specifically\ndesigned for shape synthesis or focus on texture transfer. In this paper, we\npropose a novel model, AGIS-Net, to transfer both shape and texture styles in\none-stage with only a few stylized samples. To achieve this goal, we first\ndisentangle the representations for content and style by using two encoders,\nensuring the multi-content and multi-style generation. Then we utilize two\ncollaboratively working decoders to generate the glyph shape image and its\ntexture image simultaneously. In addition, we introduce a local texture\nrefinement loss to further improve the quality of the synthesized textures. In\nthis manner, our one-stage model is much more efficient and effective than\nother multi-stage stacked methods. We also propose a large-scale dataset with\nChinese glyph images in various shape and texture styles, rendered from 35\nprofessional-designed artistic fonts with 7,326 characters and 2,460 synthetic\nartistic fonts with 639 characters, to validate the effectiveness and\nextendability of our method. Extensive experiments on both English and Chinese\nartistic glyph image datasets demonstrate the superiority of our model in\ngenerating high-quality stylized glyph images against other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 06:23:26 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:20:50 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Gao", "Yue", ""], ["Guo", "Yuan", ""], ["Lian", "Zhouhui", ""], ["Tang", "Yingmin", ""], ["Xiao", "Jianguo", ""]]}, {"id": "1910.05148", "submitter": "Mark Boss", "authors": "Mark Boss, Hendrik P.A. Lensch", "title": "Single Image BRDF Parameter Estimation with a Conditional Adversarial\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating plausible surfaces is an essential component in achieving a high\ndegree of realism in rendering. To relieve artists, who create these surfaces\nin a time-consuming, manual process, automated retrieval of the\nspatially-varying Bidirectional Reflectance Distribution Function (SVBRDF) from\na single mobile phone image is desirable. By leveraging a deep neural network,\nthis casual capturing method can be achieved. The trained network can estimate\nper pixel normal, base color, metallic and roughness parameters from the Disney\nBRDF. The input image is taken with a mobile phone lit by the camera flash. The\nnetwork is trained to compensate for environment lighting and thus learned to\nreduce artifacts introduced by other light sources. These losses contain a\nmulti-scale discriminator with an additional perceptual loss, a rendering loss\nusing a differentiable renderer, and a parameter loss. Besides the local\nprecision, this loss formulation generates material texture maps which are\nglobally more consistent. The network is set up as a generator network trained\nin an adversarial fashion to ensure that only plausible maps are produced. The\nestimated parameters not only reproduce the material faithfully in rendering\nbut capture the style of hand-authored materials due to the more global loss\nterms compared to previous works without requiring additional post-processing.\nBoth the resolution and the quality is improved.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:00:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Boss", "Mark", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1910.05200", "submitter": "Gaurav Bharaj", "authors": "Abdallah Dib, Gaurav Bharaj, Junghyun Ahn, Cedric Thebault,\n  Philippe-Henri Gosselin, Louis Chevallier", "title": "Face Reflectance and Geometry Modeling via Differentiable Ray Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel strategy to automatically reconstruct 3D faces from\nmonocular images with explicitly disentangled facial geometry (pose, identity\nand expression), reflectance (diffuse and specular albedo), and self-shadows.\nThe scene lights are modeled as a virtual light stage with pre-oriented area\nlights used in conjunction with differentiable Monte-Carlo ray tracing to\noptimize the scene and face parameters. With correctly disentangled\nself-shadows and specular reflection parameters, we can not only obtain robust\nfacial geometry reconstruction, but also gain explicit control over these\nparameters, with several practical applications. We can change facial\nexpressions with accurate resultant self-shadows or relight the scene and\nobtain accurate specular reflection and several other parameter combinations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 13:39:33 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Dib", "Abdallah", ""], ["Bharaj", "Gaurav", ""], ["Ahn", "Junghyun", ""], ["Thebault", "Cedric", ""], ["Gosselin", "Philippe-Henri", ""], ["Chevallier", "Louis", ""]]}, {"id": "1910.05253", "submitter": "Chien-Hsun Lai", "authors": "Tsai-Ho Sun, Chien-Hsun Lai, Sai-Keung Wong, and Yu-Shuen Wang", "title": "Adversarial Colorization Of Icons Based On Structure And Color\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system to help designers create icons that are widely used in\nbanners, signboards, billboards, homepages, and mobile apps. Designers are\ntasked with drawing contours, whereas our system colorizes contours in\ndifferent styles. This goal is achieved by training a dual conditional\ngenerative adversarial network (GAN) on our collected icon dataset. One\ncondition requires the generated image and the drawn contour to possess a\nsimilar contour, while the other anticipates the image and the referenced icon\nto be similar in color style. Accordingly, the generator takes a contour image\nand a man-made icon image to colorize the contour, and then the discriminators\ndetermine whether the result fulfills the two conditions. The trained network\nis able to colorize icons demanded by designers and greatly reduces their\nworkload. For the evaluation, we compared our dual conditional GAN to several\nstate-of-the-art techniques. Experiment results demonstrate that our network is\nover the previous networks. Finally, we will provide the source code, icon\ndataset, and trained network for public use.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 22:48:46 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Sun", "Tsai-Ho", ""], ["Lai", "Chien-Hsun", ""], ["Wong", "Sai-Keung", ""], ["Wang", "Yu-Shuen", ""]]}, {"id": "1910.05998", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Allen Y. Yang, Woojin Ko, Luisa Caldas", "title": "Optimization and Manipulation of Contextual Mutual Spaces for Multi-User\n  Virtual and Augmented Reality Interaction", "comments": "Accepted at 2020 IEEE Conference on Virtual Reality and 3D User\n  Interfaces (VR)", "journal-ref": null, "doi": "10.1109/VR46266.2020.00055", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing experiences are physically constrained by the geometry and\nsemantics of the local user environment. This limitation is elevated in remote\nmulti-user interaction scenarios, where finding a common virtual ground\nphysically accessible for all participants becomes challenging. Locating a\ncommon accessible virtual ground is difficult for the users themselves,\nparticularly if they are not aware of the spatial properties of other\nparticipants. In this paper, we introduce a framework to generate an optimal\nmutual virtual space for a multi-user interaction setting where remote users'\nroom spaces can have different layout and sizes. The framework further\nrecommends movement of surrounding furniture objects that expand the size of\nthe mutual space with minimal physical effort. Finally, we demonstrate the\nperformance of our solution on real-world datasets and also a real HoloLens\napplication. Results show the proposed algorithm can effectively discover\noptimal shareable space for multi-user virtual interaction and hence facilitate\nremote spatial computing communication in various collaborative workflows.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 09:10:54 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 05:36:47 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Yang", "Allen Y.", ""], ["Ko", "Woojin", ""], ["Caldas", "Luisa", ""]]}, {"id": "1910.06402", "submitter": "Yun(Raymond) Fei", "authors": "Yun Fei, Christopher Batty, Eitan Grinspun", "title": "Addressing Troubles with Double Bubbles: Convergence and Stability at\n  Multi-Bubble Junctions", "comments": "3 pages, 3 figures, technical report of Columbia University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we discuss and propose a correction to a convergence and\nstability issue occurring in the work of Da et al.[2015], in which they\nproposed a numerical model to simulate soap bubbles.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 20:10:45 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 18:03:49 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Fei", "Yun", ""], ["Batty", "Christopher", ""], ["Grinspun", "Eitan", ""]]}, {"id": "1910.06511", "submitter": "Ling-Xiao Zhang", "authors": "Lin Gao, Ling-Xiao Zhang, Hsien-Yu Meng, Yi-Hui Ren, Yu-Kun Lai, Leif\n  Kobbelt", "title": "PRS-Net: Planar Reflective Symmetry Detection Net for 3D Models", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geometry processing, symmetry is a universal type of high-level structural\ninformation of 3D models and benefits many geometry processing tasks including\nshape segmentation, alignment, matching, and completion. Thus it is an\nimportant problem to analyze various symmetry forms of 3D shapes. Planar\nreflective symmetry is the most fundamental one. Traditional methods based on\nspatial sampling can be time-consuming and may not be able to identify all the\nsymmetry planes. In this paper, we present a novel learning framework to\nautomatically discover global planar reflective symmetry of a 3D shape. Our\nframework trains an unsupervised 3D convolutional neural network to extract\nglobal model features and then outputs possible global symmetry parameters,\nwhere input shapes are represented using voxels. We introduce a dedicated\nsymmetry distance loss along with a regularization loss to avoid generating\nduplicated symmetry planes. Our network can also identify generalized cylinders\nby predicting their rotation axes. We further provide a method to remove\ninvalid and duplicated planes and axes. We demonstrate that our method is able\nto produce reliable and accurate results. Our neural network based method is\nhundreds of times faster than the state-of-the-art methods, which are based on\nsampling. Our method is also robust even with noisy or incomplete input\nsurfaces.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 03:46:58 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 04:05:15 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 06:55:05 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 02:21:17 GMT"}, {"version": "v5", "created": "Mon, 1 Jun 2020 11:54:32 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Gao", "Lin", ""], ["Zhang", "Ling-Xiao", ""], ["Meng", "Hsien-Yu", ""], ["Ren", "Yi-Hui", ""], ["Lai", "Yu-Kun", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1910.06610", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Analyzing symmetry and symmetry breaking by computational aesthetic\n  measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study creating and analyzing symmetry and broken symmetry in digital art.\nOur focus is not so much on computer-generating artistic images, but rather on\nanalyzing concepts and templates for incorporating symmetry and symmetry\nbreaking into the creation process. Taking as a starting point patterns\ngenerated algorithmically by emulating the collective feeding behavior of\nsand-bubbler crabs, all four types of two-dimensional symmetry are used as\nisometric maps. Apart from a geometric interpretation of symmetry, we also\nconsider color as an object of symmetric transformations. Color symmetry is\nrealized as a color permutation consistent with the isometries. Moreover, we\nanalyze the abilities of computational aesthetic measures to serve as a metric\nthat reflects design parameters, i.e. the type of symmetry and the degree of\nsymmetry breaking.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 09:17:38 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "1910.07192", "submitter": "Yuki Endo", "authors": "Yuki Endo, Yoshihiro Kanamori, Shigeru Kuriyama", "title": "Animating Landscape: Self-Supervised Learning of Decoupled Motion and\n  Appearance for Single-Image Video Synthesis", "comments": "Published at SIGGRAPH Asia 2019 (ACM Transactions on Graphics)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of a high-quality video from a single image remains a\nchallenging task despite the recent advances in deep generative models. This\npaper proposes a method that can create a high-resolution, long-term animation\nusing convolutional neural networks (CNNs) from a single landscape image where\nwe mainly focus on skies and waters. Our key observation is that the motion\n(e.g., moving clouds) and appearance (e.g., time-varying colors in the sky) in\nnatural scenes have different time scales. We thus learn them separately and\npredict them with decoupled control while handling future uncertainty in both\npredictions by introducing latent codes. Unlike previous methods that infer\noutput frames directly, our CNNs predict spatially-smooth intermediate data,\ni.e., for motion, flow fields for warping, and for appearance, color transfer\nmaps, via self-supervised learning, i.e., without explicitly-provided ground\ntruth. These intermediate data are applied not to each previous output frame,\nbut to the input image only once for each output frame. This design is crucial\nto alleviate error accumulation in long-term predictions, which is the\nessential problem in previous recurrent approaches. The output frames can be\nlooped like cinemagraph, and also be controlled directly by specifying latent\ncodes or indirectly via visual annotations. We demonstrate the effectiveness of\nour method through comparisons with the state-of-the-arts on video prediction\nas well as appearance manipulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 07:20:58 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""], ["Kuriyama", "Shigeru", ""]]}, {"id": "1910.08398", "submitter": "Julien Tierny", "authors": "Max Kontak and Jules Vidal and Julien Tierny", "title": "Statistical Parameter Selection for Clustering Persistence Diagrams", "comments": "arXiv admin note: text overlap with arXiv:1907.04565", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In urgent decision making applications, ensemble simulations are an important\nway to determine different outcome scenarios based on currently available data.\nIn this paper, we will analyze the output of ensemble simulations by\nconsidering so-called persistence diagrams, which are reduced representations\nof the original data, motivated by the extraction of topological features.\nBased on a recently published progressive algorithm for the clustering of\npersistence diagrams, we determine the optimal number of clusters, and\ntherefore the number of significantly different outcome scenarios, by the\nminimization of established statistical score functions. Furthermore, we\npresent a proof-of-concept prototype implementation of the statistical\nselection of the number of clusters and provide the results of an experimental\nstudy, where this implementation has been applied to real-world ensemble data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 07:06:36 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Kontak", "Max", ""], ["Vidal", "Jules", ""], ["Tierny", "Julien", ""]]}, {"id": "1910.08462", "submitter": "Damien Rohmer", "authors": "Adrien Nivaggioli (LIX), Damien Rohmer (LIX)", "title": "Animation Synthesis Triggered by Vocal Mimics", "comments": null, "journal-ref": "Motion, Interaction and Games, Oct 2019, Newcastle, United Kingdom", "doi": "10.1145/3359566.3360067", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method leveraging the naturally time-related expressivity of our\nvoice to control an animation composed of a set of short events. The user\nrecords itself mimicking onomatopoeia sounds such as \"Tick\", \"Pop\", or \"Chhh\"\nwhich are associated with specific animation events. The recorded soundtrack is\nautomatically analyzed to extract every instant and types of sounds. We finally\nsynthesize an animation where each event type and timing correspond with the\nsoundtrack. In addition to being a natural way to control animation timing, we\ndemonstrate that multiple stories can be efficiently generated by recording\ndifferent voice sequences. Also, the use of more than one soundtrack allows us\nto control different characters with overlapping actions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:15:09 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Nivaggioli", "Adrien", "", "LIX"], ["Rohmer", "Damien", "", "LIX"]]}, {"id": "1910.08470", "submitter": "Dimitrios Sakkos", "authors": "Dimitrios Sakkos, Hubert P. H. Shum and Edmond S. L. Ho", "title": "Illumination-Based Data Augmentation for Robust Background Subtraction", "comments": "SKIMA 2019 - Best Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core challenge in background subtraction (BGS) is handling videos with\nsudden illumination changes in consecutive frames. In this paper, we tackle the\nproblem from a data point-of-view using data augmentation. Our method performs\ndata augmentation that not only creates endless data on the fly, but also\nfeatures semantic transformations of illumination which enhance the\ngeneralisation of the model. It successfully simulates flashes and shadows by\napplying the Euclidean distance transform over a binary mask that is randomly\ngenerated. Such data allows us to effectively train an illumination-invariant\ndeep learning model for BGS. Experimental results demonstrate the contribution\nof the synthetics in the ability of the models to perform BGS even when\nsignificant illumination changes take place. The source code of the project is\nmade publicly available at\nhttps://github.com/dksakkos/illumination_augmentation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:28:59 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sakkos", "Dimitrios", ""], ["Shum", "Hubert P. H.", ""], ["Ho", "Edmond S. L.", ""]]}, {"id": "1910.08537", "submitter": "Jun Zhou", "authors": "Jun Zhou, Hua Huang, Bin Liu, Xiuping Liu", "title": "Normal Estimation for 3D Point Clouds via Local Plane Constraint and\n  Multi-scale Selection", "comments": "arXiv admin note: text overlap with arXiv:1710.04954,\n  arXiv:1904.07172, arXiv:1812.00709 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a normal estimation method for unstructured 3D\npoint clouds. In this method, a feature constraint mechanism called Local Plane\nFeatures Constraint (LPFC) is used and then a multi-scale selection strategy is\nintroduced. The LPEC can be used in a single-scale point network architecture\nfor a more stable normal estimation of the unstructured 3D point clouds. In\nparticular, it can partly overcome the influence of noise on a large sampling\nscale compared to the other methods which only use regression loss for normal\nestimation. For more details, a subnetwork is built after point-wise features\nextracted layers of the network and it gives more constraints to each point of\nthe local patch via a binary classifier in the end. Then we use multi-task\noptimization to train the normal estimation and local plane classification\ntasks simultaneously.Also, to integrate the advantages of multi-scale results,\na scale selection strategy is adopted, which is a data-driven approach for\nselecting the optimal scale around each point and encourages subnetwork\nspecialization. Specifically, we employed a subnetwork called Scale Estimation\nNetwork to extract scale weight information from multi-scale features. More\nanalysis is given about the relations between noise levels, local boundary, and\nscales in the experiment. These relationships can be a better guide to choosing\nparticular scales for a particular model. Besides, the experimental result\nshows that our network can distinguish the points on the fitting plane\naccurately and this can be used to guide the normal estimation and our\nmulti-scale method can improve the results well. Compared to some\nstate-of-the-art surface normal estimators, our method is robust to noise and\ncan achieve competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 00:17:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Jun", ""], ["Huang", "Hua", ""], ["Liu", "Bin", ""], ["Liu", "Xiuping", ""]]}, {"id": "1910.08685", "submitter": "Deepali Aneja", "authors": "Deepali Aneja and Wilmot Li", "title": "Real-Time Lip Sync for Live 2D Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of commercial tools for real-time performance-based 2D\nanimation has enabled 2D characters to appear on live broadcasts and streaming\nplatforms. A key requirement for live animation is fast and accurate lip sync\nthat allows characters to respond naturally to other actors or the audience\nthrough the voice of a human performer. In this work, we present a deep\nlearning based interactive system that automatically generates live lip sync\nfor layered 2D characters using a Long Short Term Memory (LSTM) model. Our\nsystem takes streaming audio as input and produces viseme sequences with less\nthan 200ms of latency (including processing time). Our contributions include\nspecific design decisions for our feature definition and LSTM configuration\nthat provide a small but useful amount of lookahead to produce accurate lip\nsync. We also describe a data augmentation procedure that allows us to achieve\ngood results with a very small amount of hand-animated training data (13-20\nminutes). Extensive human judgement experiments show that our results are\npreferred over several competing methods, including those that only support\noffline (non-live) processing. Video summary and supplementary results at\nGitHub link: https://github.com/deepalianeja/CharacterLipSync2D\n", "versions": [{"version": "v1", "created": "Sat, 19 Oct 2019 03:12:26 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Aneja", "Deepali", ""], ["Li", "Wilmot", ""]]}, {"id": "1910.08865", "submitter": "Yang Wang", "authors": "Yang Wang", "title": "Deck.gl: Large-scale Web-based Visual Analytics Made Easy", "comments": "The IEEE Workshop on Visualization in Practice, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deck.gl, an open-source project born out of\ndata-heavy visual analytics applications, has grown into the robust\nvisualization framework it is today. We begin by explaining why we built\nanother data visualization framework in the first place. Then, we summarize our\ndesign goals (distilled from our interactions with users) and discuss how they\nguided the development of the framework's main features. We use two real-world\napplications of deck.gl to showcase how it can be applied to simplify the\ncreation of data-heavy visualizations. We also discuss our lessons learned as\nwe continue to improve the framework for the larger visualization community.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 01:01:42 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Wang", "Yang", ""]]}, {"id": "1910.08954", "submitter": "Zhiyang Dou", "authors": "Zhiyang Dou, Shiqing Xin, Rui Xu, Jian Xu, Yuanfeng Zhou, Shuangmin\n  Chen, Wenping Wang, Xiuyang Zhao, Changhe Tu", "title": "Top-Down Shape Abstraction Based on Greedy Pole Selection", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2995495", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivated by the fact that the medial axis transform is able to encode nearly\nthe complete shape, we propose to use as few medial balls as possible to\napproximate the original enclosed volume by the boundary surface. We\nprogressively select new medial balls, in a top-down style, to enlarge the\nregion spanned by the existing medial balls. The key spirit of the selection\nstrategy is to encourage large medial balls while imposing given geometric\nconstraints. We further propose a speedup technique based on a provable\nobservation that the intersection of medial balls implies the adjacency of\npower cells (in the sense of the power crust). We further elaborate the\nselection rules in combination with two closely related applications. One\napplication is to develop an easy-to-use ball-stick modeling system that helps\nnon-professional users to quickly build a shape with only balls and wires, but\nany penetration between two medial balls must be suppressed. The other\napplication is to generate porous structures with convex, compact (with a high\nisoperimetric quotient) and shape-aware pores where two adjacent spherical\npores may have penetration as long as the mechanical rigidity can be well\npreserved.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 11:48:41 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 12:15:22 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 02:08:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Dou", "Zhiyang", ""], ["Xin", "Shiqing", ""], ["Xu", "Rui", ""], ["Xu", "Jian", ""], ["Zhou", "Yuanfeng", ""], ["Chen", "Shuangmin", ""], ["Wang", "Wenping", ""], ["Zhao", "Xiuyang", ""], ["Tu", "Changhe", ""]]}, {"id": "1910.09166", "submitter": "Xiaopei Liu", "authors": "Kai Bai, Wei Li, Mathieu Desbrun, Xiaopei Liu", "title": "Dynamic Upsampling of Smoke through Dictionary-based Learning", "comments": "17 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating turbulent smoke flows is computationally intensive due to their\nintrinsic multiscale behavior, thus requiring relatively high resolution grids\nto fully capture their complexity. For iterative editing or simply faster\ngeneration of smoke flows, dynamic upsampling of an input low-resolution\nnumerical simulation is an attractive, yet currently unattainable goal. In this\npaper, we propose a novel dictionary-based learning approach to the dynamic\nupsampling of smoke flows. For each frame of an input coarse animation, we seek\na sparse representation of small, local velocity patches of the flow based on\nan over-complete dictionary, and use the resulting sparse coefficients to\ngenerate a high-resolution smoke animation sequence. We propose a novel\ndictionary-based neural network which learns both a fast evaluation of sparse\npatch encoding and a dictionary of corresponding coarse and fine patches from a\nsequence of example simulations computed with any numerical solver. Our\nupsampling network then injects into coarse input sequences physics-driven fine\ndetails, unlike most previous approaches that only employed fast procedural\nmodels to add high frequency to the input. We present a variety of upsampling\nresults for smoke flows and offer comparisons to their corresponding\nhigh-resolution simulations to demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 06:37:41 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Bai", "Kai", ""], ["Li", "Wei", ""], ["Desbrun", "Mathieu", ""], ["Liu", "Xiaopei", ""]]}, {"id": "1910.09399", "submitter": "Haicheng Tao", "authors": "Jorge Agnese, Jonathan Herrera, Haicheng Tao, Xingquan Zhu", "title": "A Survey and Taxonomy of Adversarial Neural Networks for Text-to-Image\n  Synthesis", "comments": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery.\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-to-image synthesis refers to computational methods which translate human\nwritten textual descriptions, in the form of keywords or sentences, into images\nwith similar semantic meaning to the text. In earlier research, image synthesis\nrelied mainly on word to image correlation analysis combined with supervised\nmethods to find best alignment of the visual content matching to the text.\nRecent progress in deep learning (DL) has brought a new set of unsupervised\ndeep learning methods, particularly deep generative models which are able to\ngenerate realistic visual images using suitably trained neural network models.\nIn this paper, we review the most recent development in the text-to-image\nsynthesis research domain. Our survey first introduces image synthesis and its\nchallenges, and then reviews key concepts such as generative adversarial\nnetworks (GANs) and deep convolutional encoder-decoder neural networks (DCNN).\nAfter that, we propose a taxonomy to summarize GAN based text-to-image\nsynthesis into four major categories: Semantic Enhancement GANs, Resolution\nEnhancement GANs, Diversity Enhancement GANS, and Motion Enhancement GANs. We\nelaborate the main objective of each group, and further review typical GAN\narchitectures in each group. The taxonomy and the review outline the techniques\nand the evolution of different approaches, and eventually provide a clear\nroadmap to summarize the list of contemporaneous solutions that utilize GANs\nand DCNNs to generate enthralling results in categories such as human faces,\nbirds, flowers, room interiors, object reconstruction from edge maps (games)\netc. The survey will conclude with a comparison of the proposed solutions,\nchallenges that remain unresolved, and future developments in the text-to-image\nsynthesis domain.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 14:23:14 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Agnese", "Jorge", ""], ["Herrera", "Jonathan", ""], ["Tao", "Haicheng", ""], ["Zhu", "Xingquan", ""]]}, {"id": "1910.10836", "submitter": "Willemijn Elkhuizen", "authors": "Willemijn Elkhuizen, Tessa Essers, Yu Song, Jo Geraedts, Clemens\n  Weijkamp, Joris Dik, Sylvia Pont", "title": "Gloss, Color and Topography Scanning for Reproducing a Painting's\n  Appearance using 3D printing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High fidelity reproductions of paintings provide new opportunities to museums\nin preserving and providing access to cultural heritage. This paper presents an\nintegrated system which is able to capture and fabricate color, topography and\ngloss of a painting, of which gloss capturing forms the most important\ncontribution. A 3D imaging system, using fringe-encoded stereo imaging, is\nextended to capture spatially-varying gloss, utilizing specular reflectance\npolarization. The gloss is measured by sampling the specular reflection around\nBrewster's angle, where these reflections are effectively polarized, and can be\nseparated from the unpolarized, diffuse reflectance. Off-center gloss\nmeasurements are calibrated relative to the center measurement. Off-specular\ngloss measurements, following from local variation of the surface normal, are\nmasked based on the height map and corrected. Shadowed regions, caused by the\n3D relief, are treated similarly. The area of a single capture is approximately\n180x90mm at a resolution of 25x25micron. Aligned color, height, and gloss tiles\nare stitched together, registering overlapping color areas. These maps are\ninputs for a 3D printer. Two paintings were reproduced to verify the\neffectiveness and efficiency of the proposed system. One painting was scanned\nfour times, consecutively rotated by 90 degrees, to evaluate the influence of\nthe scanning system geometric configuration on the gloss measurement.\nExperimental results show that the method is sufficiently fast for practical\napplication. The results can well be used for the purpose of physical\nreproduction and other applications needing first-order estimates of the\nappearance. Our method to extend appearance scanning with gloss measurements is\na valuable addition in the quest for realistic reproductions, in terms of its\npractical applicability and its perceptual added value, when added to color and\ntopography.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 23:21:19 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Elkhuizen", "Willemijn", ""], ["Essers", "Tessa", ""], ["Song", "Yu", ""], ["Geraedts", "Jo", ""], ["Weijkamp", "Clemens", ""], ["Dik", "Joris", ""], ["Pont", "Sylvia", ""]]}, {"id": "1910.11211", "submitter": "Mohamed Hamidi", "authors": "Mohamed Hamidi and Aladine Chetouani and Mohamed El Haziti1 and\n  Mohammed El Hassouni and Hocine Cherifi", "title": "A Robust Blind 3-D Mesh Watermarking technique based on SCS quantization\n  and mesh Saliency for Copyright Protection", "comments": "10 pages, 11 figures, 5th International Conference on Mobile, Secure\n  and Programmable Networking (MSPN'2019)", "journal-ref": null, "doi": "10.1007/978-3-030-22885-9_19", "report-no": null, "categories": "cs.CR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent demand of 3-D meshes in a wide range of applications such\nas video games, medical imaging, film special effect making, computer-aided\ndesign (CAD), among others, the necessity of implementing 3-D mesh watermarking\nschemes aiming to protect copyright has increased in the last decade. Nowadays,\nthe majority of robust 3-D watermarking approaches have mainly focused on the\nrobustness against attacks while the imperceptibility of these techniques is\nstill a serious challenge. In this context, a blind robust 3-D mesh\nwatermarking method based on mesh saliency and scalar Costa scheme (SCS) for\nCopyright protection is proposed. The watermark is embedded by quantifying the\nvertex norms of the 3-D mesh by SCS scheme using the vertex normal norms as\nsynchronizing primitives. The choice of these vertices is based on 3-D mesh\nsaliency to achieve watermark robustness while ensuring high imperceptibility.\nThe experimental results show that in comparison with the alternative methods,\nthe proposed work can achieve a high imperceptibility performance while\nensuring a good robustness against several common attacks including similarity\ntransformations, noise addition, quantization, smoothing, elements reordering,\netc.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:16:10 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Hamidi", "Mohamed", ""], ["Chetouani", "Aladine", ""], ["Haziti1", "Mohamed El", ""], ["Hassouni", "Mohammed El", ""], ["Cherifi", "Hocine", ""]]}, {"id": "1910.11336", "submitter": "Orly Liba", "authors": "Orly Liba, Kiran Murthy, Yun-Ta Tsai, Tim Brooks, Tianfan Xue, Nikhil\n  Karnad, Qiurui He, Jonathan T. Barron, Dillon Sharlet, Ryan Geiss, Samuel W.\n  Hasinoff, Yael Pritch, Marc Levoy", "title": "Handheld Mobile Photography in Very Low Light", "comments": "22 pages, 27 figures", "journal-ref": "ACM Trans. Graph.38, 6, Article 164 (November 2019)", "doi": "10.1145/3355089.3356508", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Taking photographs in low light using a mobile phone is challenging and\nrarely produces pleasing results. Aside from the physical limits imposed by\nread noise and photon shot noise, these cameras are typically handheld, have\nsmall apertures and sensors, use mass-produced analog electronics that cannot\neasily be cooled, and are commonly used to photograph subjects that move, like\nchildren and pets. In this paper we describe a system for capturing clean,\nsharp, colorful photographs in light as low as 0.3~lux, where human vision\nbecomes monochromatic and indistinct. To permit handheld photography without\nflash illumination, we capture, align, and combine multiple frames. Our system\nemploys \"motion metering\", which uses an estimate of motion magnitudes (whether\ndue to handshake or moving objects) to identify the number of frames and the\nper-frame exposure times that together minimize both noise and motion blur in a\ncaptured burst. We combine these frames using robust alignment and merging\ntechniques that are specialized for high-noise imagery. To ensure accurate\ncolors in such low light, we employ a learning-based auto white balancing\nalgorithm. To prevent the photographs from looking like they were shot in\ndaylight, we use tone mapping techniques inspired by illusionistic painting:\nincreasing contrast, crushing shadows to black, and surrounding the scene with\ndarkness. All of these processes are performed using the limited computational\nresources of a mobile device. Our system can be used by novice photographers to\nproduce shareable pictures in a few seconds based on a single shutter press,\neven in environments so dim that humans cannot see clearly.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:21:08 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Liba", "Orly", ""], ["Murthy", "Kiran", ""], ["Tsai", "Yun-Ta", ""], ["Brooks", "Tim", ""], ["Xue", "Tianfan", ""], ["Karnad", "Nikhil", ""], ["He", "Qiurui", ""], ["Barron", "Jonathan T.", ""], ["Sharlet", "Dillon", ""], ["Geiss", "Ryan", ""], ["Hasinoff", "Samuel W.", ""], ["Pritch", "Yael", ""], ["Levoy", "Marc", ""]]}, {"id": "1910.11626", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik\n  Strobelt, Bolei Zhou, Antonio Torralba", "title": "Seeing What a GAN Cannot Generate", "comments": "ICCV 2019 oral; http://ganseeing.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs), mode collapse\nremains a serious issue during GAN training. To date, little work has focused\non understanding and quantifying which modes have been dropped by a model. In\nthis work, we visualize mode collapse at both the distribution level and the\ninstance level. First, we deploy a semantic segmentation network to compare the\ndistribution of segmented objects in the generated images with the target\ndistribution in the training set. Differences in statistics reveal object\nclasses that are omitted by a GAN. Second, given the identified omitted object\nclasses, we visualize the GAN's omissions directly. In particular, we compare\nspecific differences between individual photos and their approximate inversions\nby a GAN. To this end, we relax the problem of inversion and solve the\ntractable problem of inverting a GAN layer instead of the entire generator.\nFinally, we use this framework to analyze several recent GANs trained on\nmultiple datasets and identify their typical failure cases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 17:56:04 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Wulff", "Jonas", ""], ["Peebles", "William", ""], ["Strobelt", "Hendrik", ""], ["Zhou", "Bolei", ""], ["Torralba", "Antonio", ""]]}, {"id": "1910.12056", "submitter": "Zhijie Wu", "authors": "Chunjin Song, Zhijie Wu, Yang Zhou, Minglun Gong, Hui Huang", "title": "ETNet: Error Transition Network for Arbitrary Style Transfer", "comments": "Accepted by NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous valuable efforts have been devoted to achieving arbitrary style\ntransfer since the seminal work of Gatys et al. However, existing\nstate-of-the-art approaches often generate insufficiently stylized results\nunder challenging cases. We believe a fundamental reason is that these\napproaches try to generate the stylized result in a single shot and hence fail\nto fully satisfy the constraints on semantic structures in the content images\nand style patterns in the style images. Inspired by the works on\nerror-correction, instead, we propose a self-correcting model to predict what\nis wrong with the current stylization and refine it accordingly in an iterative\nmanner. For each refinement, we transit the error features across both the\nspatial and scale domain and invert the processed features into a residual\nimage, with a network we call Error Transition Network (ETNet). The proposed\nmodel improves over the state-of-the-art methods with better semantic\nstructures and more adaptive style pattern details. Various qualitative and\nquantitative experiments show that the key concept of both progressive strategy\nand error-correction leads to better results. Code and models are available at\nhttps://github.com/zhijieW94/ETNet.\n", "versions": [{"version": "v1", "created": "Sat, 26 Oct 2019 12:49:00 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 17:04:01 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Song", "Chunjin", ""], ["Wu", "Zhijie", ""], ["Zhou", "Yang", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1910.12591", "submitter": "David Solans", "authors": "David Solans, Christopher Tauchmann, Aideen Farrell, Karolin Kappler,\n  Hans-Hendrik Huber, Carlos Castillo, Kristian Kersting", "title": "Conflict and Cooperation: AI Research and Development in terms of the\n  Economy of Conventions", "comments": "Accepted at ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) and its relation with societies is increasingly\nbecoming an interesting object of study from the perspective of sociology and\nother disciplines. Theories such as the Economy of Conventions (EC) are usually\napplied in the context of interpersonal relations but there is still a clear\nlack of studies around how this and other theories can shed light on\ninteractions between human an autonomous systems. This work is focused into\nstudying a preliminary step that is a key enabler for the subsequent\ninteraction between machines and humans: how the processes of researching,\ndesigning and developing AI related systems reflect different moral registers,\nrepresented by conventions within the EC. Having a better understanding of\nthose conventions guiding the advances in AI is considered as the first and\nrequired advance to understand the conventions afterwards reflected by those\nautonomous systems in the interactions with societies. For this purpose, we\ndevelop an iterative tool based on active learning to label a data set from the\nfield of AI and Machine Learning (ML) research and present preliminary results\nof a supervised classifier trained on these conventions. To further demonstrate\nthe feasibility of the approach, the results are contrasted with a classifier\ntrained on software conventions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 19:38:12 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 19:25:28 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 08:15:24 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 11:40:25 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Solans", "David", ""], ["Tauchmann", "Christopher", ""], ["Farrell", "Aideen", ""], ["Kappler", "Karolin", ""], ["Huber", "Hans-Hendrik", ""], ["Castillo", "Carlos", ""], ["Kersting", "Kristian", ""]]}, {"id": "1910.12713", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, Bryan\n  Catanzaro", "title": "Few-shot Video-to-Video Synthesis", "comments": "In NeurIPS, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-to-video synthesis (vid2vid) aims at converting an input semantic\nvideo, such as videos of human poses or segmentation masks, to an output\nphotorealistic video. While the state-of-the-art of vid2vid has advanced\nsignificantly, existing approaches share two major limitations. First, they are\ndata-hungry. Numerous images of a target human subject or a scene are required\nfor training. Second, a learned model has limited generalization capability. A\npose-to-human vid2vid model can only synthesize poses of the single person in\nthe training set. It does not generalize to other humans that are not in the\ntraining set. To address the limitations, we propose a few-shot vid2vid\nframework, which learns to synthesize videos of previously unseen subjects or\nscenes by leveraging few example images of the target at test time. Our model\nachieves this few-shot generalization capability via a novel network weight\ngeneration module utilizing an attention mechanism. We conduct extensive\nexperimental validations with comparisons to strong baselines using several\nlarge-scale video datasets including human-dancing videos, talking-head videos,\nand street-scene videos. The experimental results verify the effectiveness of\nthe proposed framework in addressing the two limitations of existing vid2vid\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 14:33:09 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Tao", "Andrew", ""], ["Liu", "Guilin", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1910.13688", "submitter": "Qing Zhang", "authors": "Qing Zhang, Yongwei Nie, Wei-Shi Zheng", "title": "Dual Illumination Estimation for Robust Exposure Correction", "comments": "Computer Graphics Forum (Proceedings of Pacific Graphics 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure correction is one of the fundamental tasks in image processing and\ncomputational photography. While various methods have been proposed, they\neither fail to produce visually pleasing results, or only work well for limited\ntypes of image (e.g., underexposed images). In this paper, we present a novel\nautomatic exposure correction method, which is able to robustly produce\nhigh-quality results for images of various exposure conditions (e.g.,\nunderexposed, overexposed, and partially under- and over-exposed). At the core\nof our approach is the proposed dual illumination estimation, where we\nseparately cast the under- and over-exposure correction as trivial illumination\nestimation of the input image and the inverted input image. By performing dual\nillumination estimation, we obtain two intermediate exposure correction results\nfor the input image, with one fixes the underexposed regions and the other one\nrestores the overexposed regions. A multi-exposure image fusion technique is\nthen employed to adaptively blend the visually best exposed parts in the two\nintermediate exposure correction images and the input image into a globally\nwell-exposed image. Experiments on a number of challenging images demonstrate\nthe effectiveness of the proposed approach and its superiority over the\nstate-of-the-art methods and popular automatic exposure correction tools.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 05:59:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Zhang", "Qing", ""], ["Nie", "Yongwei", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1910.13921", "submitter": "Mojtaba Bemana", "authors": "Mojtaba Bemana, Karol Myszkowski, Hans-Peter Seidel, Tobias Ritschel", "title": "Neural View-Interpolation for Sparse Light Field Video", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest representing light field (LF) videos as \"one-off\" neural networks\n(NN), i.e., a learned mapping from view-plus-time coordinates to\nhigh-resolution color values, trained on sparse views. Initially, this sounds\nlike a bad idea for three main reasons: First, a NN LF will likely have less\nquality than a same-sized pixel basis representation. Second, only few training\ndata, e.g., 9 exemplars per frame are available for sparse LF videos. Third,\nthere is no generalization across LFs, but across view and time instead.\nConsequently, a network needs to be trained for each LF video. Surprisingly,\nthese problems can turn into substantial advantages: Other than the linear\npixel basis, a NN has to come up with a compact, non-linear i.e., more\nintelligent, explanation of color, conditioned on the sparse view and time\ncoordinates. As observed for many NN however, this representation now is\ninterpolatable: if the image output for sparse view coordinates is plausible,\nit is for all intermediate, continuous coordinates as well. Our specific\nnetwork architecture involves a differentiable occlusion-aware warping step,\nwhich leads to a compact set of trainable parameters and consequently fast\nlearning and fast execution.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 15:18:37 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 16:38:40 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 10:27:50 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Bemana", "Mojtaba", ""], ["Myszkowski", "Karol", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1910.14063", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Lin Gao, Jie Yang, Paul L. Rosin, Yu-Kun Lai, and Xilin\n  Chen", "title": "LaplacianNet: Learning on 3D Meshes with Laplacian Encoding and Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D models are commonly used in computer vision and graphics. With the wider\navailability of mesh data, an efficient and intrinsic deep learning approach to\nprocessing 3D meshes is in great need. Unlike images, 3D meshes have irregular\nconnectivity, requiring careful design to capture relations in the data. To\nutilize the topology information while staying robust under different\ntriangulation, we propose to encode mesh connectivity using Laplacian spectral\nanalysis, along with Mesh Pooling Blocks (MPBs) that can split the surface\ndomain into local pooling patches and aggregate global information among them.\nWe build a mesh hierarchy from fine to coarse using Laplacian spectral\nclustering, which is flexible under isometric transformation. Inside the MPBs\nthere are pooling layers to collect local information and multi-layer\nperceptrons to compute vertex features with increasing complexity. To obtain\nthe relationships among different clusters, we introduce a Correlation Net to\ncompute a correlation matrix, which can aggregate the features globally by\nmatrix multiplication with cluster features. Our network architecture is\nflexible enough to be used on meshes with different numbers of vertices. We\nconduct several experiments including shape segmentation and classification,\nand our LaplacianNet outperforms state-of-the-art algorithms for these tasks on\nShapeNet and COSEG datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 18:02:23 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Rosin", "Paul L.", ""], ["Lai", "Yu-Kun", ""], ["Chen", "Xilin", ""]]}]