[{"id": "1909.00032", "submitter": "Shingo Kagami", "authors": "Shingo Kagami and Koichi Hashimoto", "title": "Animated Stickies: Fast Video Projection Mapping onto a Markerless Plane\n  through a Direct Closed-Loop Alignment", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  (ISMAR 2019 Special Issue)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2932248", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast projection mapping method for moving image content\nprojected onto a markerless planar surface using a low-latency Digital\nMicromirror Device (DMD) projector. By adopting a closed-loop alignment\napproach, in which not only the surface texture but also the projected image is\ntracked by a camera, the proposed method is free from a calibration or position\nadjustment between the camera and projector. We designed fiducial patterns to\nbe inserted into a fast flapping sequence of binary frames of the DMD\nprojector, which allows the simultaneous tracking of the surface texture and a\nfiducial geometry separate from a single image captured by the camera. The\nproposed method implemented on a CPU runs at 400 fps and enables arbitrary\nvideo contents to be \"stuck\" onto a variety of textured surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:37:19 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kagami", "Shingo", ""], ["Hashimoto", "Koichi", ""]]}, {"id": "1909.00302", "submitter": "Akshay Gadi Patil", "authors": "Akshay Gadi Patil, Omri Ben-Eliezer, Or Perel, Hadar Averbuch-Elor", "title": "READ: Recursive Autoencoders for Document Layout Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layout is a fundamental component of any graphic design. Creating large\nvarieties of plausible document layouts can be a tedious task, requiring\nnumerous constraints to be satisfied, including local ones relating different\nsemantic elements and global constraints on the general appearance and spacing.\nIn this paper, we present a novel framework, coined READ, for REcursive\nAutoencoders for Document layout generation, to generate plausible 2D layouts\nof documents in large quantities and varieties. First, we devise an exploratory\nrecursive method to extract a structural decomposition of a single document.\nLeveraging a dataset of documents annotated with labeled bounding boxes, our\nrecursive neural network learns to map the structural representation, given in\nthe form of a simple hierarchy, to a compact code, the space of which is\napproximated by a Gaussian distribution. Novel hierarchies can be sampled from\nthis space, obtaining new document layouts. Moreover, we introduce a\ncombinatorial metric to measure structural similarity among document layouts.\nWe deploy it to show that our method is able to generate highly variable and\nrealistic layouts. We further demonstrate the utility of our generated layouts\nin the context of standard detection tasks on documents, showing that detection\nperformance improves when the training data is augmented with generated\ndocuments whose layouts are produced by READ.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 01:58:31 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 17:43:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 22:38:52 GMT"}, {"version": "v4", "created": "Thu, 16 Apr 2020 23:26:17 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Patil", "Akshay Gadi", ""], ["Ben-Eliezer", "Omri", ""], ["Perel", "Or", ""], ["Averbuch-Elor", "Hadar", ""]]}, {"id": "1909.00470", "submitter": "Bailin Deng", "authors": "Juyong Zhang, Yue Peng, Wenqing Ouyang, Bailin Deng", "title": "Accelerating ADMM for Efficient Simulation and Optimization", "comments": "SIGGRAPH Asia 2019 Technical Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alternating direction method of multipliers (ADMM) is a popular approach\nfor solving optimization problems that are potentially non-smooth and with hard\nconstraints. It has been applied to various computer graphics applications,\nincluding physical simulation, geometry processing, and image processing.\nHowever, ADMM can take a long time to converge to a solution of high accuracy.\nMoreover, many computer graphics tasks involve non-convex optimization, and\nthere is often no convergence guarantee for ADMM on such problems since it was\noriginally designed for convex optimization. In this paper, we propose a method\nto speed up ADMM using Anderson acceleration, an established technique for\naccelerating fixed-point iterations. We show that in the general case, ADMM is\na fixed-point iteration of the second primal variable and the dual variable,\nand Anderson acceleration can be directly applied. Additionally, when the\nproblem has a separable target function and satisfies certain conditions, ADMM\nbecomes a fixed-point iteration of only one variable, which further reduces the\ncomputational overhead of Anderson acceleration. Moreover, we analyze a\nparticular non-convex problem structure that is common in computer graphics,\nand prove the convergence of ADMM on such problems under mild assumptions. We\napply our acceleration technique on a variety of optimization problems in\ncomputer graphics, with notable improvement on their convergence speed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 20:36:33 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Juyong", ""], ["Peng", "Yue", ""], ["Ouyang", "Wenqing", ""], ["Deng", "Bailin", ""]]}, {"id": "1909.00551", "submitter": "Yusuf Hamza", "authors": "Yusuf Fatihu Hamza, Hongwei Lin, Zihao Li", "title": "Implicit Progressive-Iterative Approximation for Curve and Surface\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit curve and surface reconstruction attracts the attention of many\nresearchers and gains a wide range of applications, due to its ability to\ndescribe objects with complicated geometry and topology. However, extra\nzero-level sets or spurious sheets arise in the reconstruction process makes\nthe reconstruction result challenging to be interpreted and damage the final\nresult. In this paper, we proposed an implicit curve and surface reconstruction\nmethod based on the progressive-iterative approximation method, named implicit\nprogressive-iterative approximation (I-PIA). The proposed method elegantly\neliminates the spurious sheets naturally without requiring any explicit\nminimization procedure, thus reducing the computational cost greatly and\nproviding high-quality reconstruction results. Numerical examples are provided\nto demonstrate the efficiency and effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 05:33:50 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hamza", "Yusuf Fatihu", ""], ["Lin", "Hongwei", ""], ["Li", "Zihao", ""]]}, {"id": "1909.00573", "submitter": "Johannes Jendersie", "authors": "Johannes Jendersie", "title": "Next Event Backtracking", "comments": "25 pages, octree source code included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In light transport simulation, challenging situations are caused by the\nvariety of materials and the relative length of path segments. Path Tracing can\nhandle many situations and scales well to parallel hardware. However, it is not\nable to produce paths which have a smooth surface in connection with a small\nlight source. Here, photon transports perform superior, which can be\nineffective if the smooth object is small compared to the scene size.\n  We propose to use the last segment of a Path Tracer path as the first segment\nof a photon path. As a result, the strengths of next event estimation are\ninherited by the photon transport and photons are guided toward the regions\nwhere they are most useful. To that end, we developed a lock-free sparse\noctree, which we use for fast and robust density estimates. Summarizing, the\nnew method can outperform state of the art algorithms like Vertex Connection\nand Merging in certain scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 07:16:53 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jendersie", "Johannes", ""]]}, {"id": "1909.01456", "submitter": "Paul Rosen", "authors": "Junyi Tu, Paul Rosen", "title": "Topologically-Guided Color Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement is an important step in post-processing digital images for\npersonal use, in medical imaging, and for object recognition. Most existing\nmanual techniques rely on region selection, similarity, and/or thresholding for\nediting, never really considering the topological structure of the image. In\nthis paper, we leverage the contour tree to extract a hierarchical\nrepresentation of the topology of an image. We propose 4 topology-aware\ntransfer functions for editing features of the image using local topological\nproperties, instead of global image properties. Finally, we evaluate our\napproach with grayscale and color images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:15:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tu", "Junyi", ""], ["Rosen", "Paul", ""]]}, {"id": "1909.01513", "submitter": "Paul Rosen", "authors": "Junyi Tu, Mustafa Hajij, Paul Rosen", "title": "Propagate and Pair: A Single-Pass Approach to Critical Point Pairing in\n  Reeb Graphs", "comments": null, "journal-ref": "Advances in Visual Computing. ISVC 2019. Lecture Notes in Computer\n  Science, vol 11844. Springer, Cham", "doi": "10.1007/978-3-030-33720-9_8", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularization of Topological Data Analysis, the Reeb graph has\nfound new applications as a summarization technique in the analysis and\nvisualization of large and complex data, whose usefulness extends beyond just\nthe graph itself. Pairing critical points enables forming topological\nfingerprints, known as persistence diagrams, that provides insights into the\nstructure and noise in data. Although the body of work addressing the efficient\ncalculation of Reeb graphs is large, the literature on pairing is limited. In\nthis paper, we discuss two algorithmic approaches for pairing critical points\nin Reeb graphs, first a multipass approach, followed by a new single-pass\nalgorithm, called Propagate and Pair.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 01:22:23 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 15:14:36 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Tu", "Junyi", ""], ["Hajij", "Mustafa", ""], ["Rosen", "Paul", ""]]}, {"id": "1909.01815", "submitter": "Bernhard Egger", "authors": "Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer,\n  Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam\n  Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, Thomas Vetter", "title": "3D Morphable Face Models -- Past, Present and Future", "comments": "ACM Transactions on Graphics (TOG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a detailed survey of 3D Morphable Face Models over\nthe 20 years since they were first proposed. The challenges in building and\napplying these models, namely capture, modeling, image formation, and image\nanalysis, are still active research topics, and we review the state-of-the-art\nin each of these areas. We also look ahead, identifying unsolved challenges,\nproposing directions for future research and highlighting the broad range of\ncurrent and future applications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:49:53 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 13:56:31 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Egger", "Bernhard", ""], ["Smith", "William A. P.", ""], ["Tewari", "Ayush", ""], ["Wuhrer", "Stefanie", ""], ["Zollhoefer", "Michael", ""], ["Beeler", "Thabo", ""], ["Bernard", "Florian", ""], ["Bolkart", "Timo", ""], ["Kortylewski", "Adam", ""], ["Romdhani", "Sami", ""], ["Theobalt", "Christian", ""], ["Blanz", "Volker", ""], ["Vetter", "Thomas", ""]]}, {"id": "1909.01875", "submitter": "Yuanmin Deng", "authors": "Bin Wang, Yuanmin Deng, Paul Kry, Uri Ascher, Hui Huang and Baoquan\n  Chen", "title": "Learning Elastic Constitutive Material and Damping Models", "comments": "11 pages, 14 figures. arXiv admin note: text overlap with\n  arXiv:1808.04931", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used linear and nonlinear constitutive material models in\ndeformation simulation contain many simplifications and only cover a tiny part\nof possible material behavior. In this work we propose a framework for learning\ncustomized models of deformable materials from example surface trajectories.\nThe key idea is to iteratively improve a correction to a nominal model of the\nelastic and damping properties of the object, which allows new forward\nsimulations with the learned correction to more accurately predict the behavior\nof a given soft object. Space-time optimization is employed to identify gentle\ncontrol forces with which we extract necessary data for model inference and to\nfinally encapsulate the material correction into a compact parametric form.\nFurthermore, a patch based position constraint is proposed to tackle the\nchallenge of handling incomplete and noisy observations arising in real-world\nexamples. We demonstrate the effectiveness of our method with a set of\nsynthetic examples, as well with data captured from real world homogeneous\nelastic objects.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:29:11 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 14:09:12 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Bin", ""], ["Deng", "Yuanmin", ""], ["Kry", "Paul", ""], ["Ascher", "Uri", ""], ["Huang", "Hui", ""], ["Chen", "Baoquan", ""]]}, {"id": "1909.02165", "submitter": "Nilesh Pandey", "authors": "Nilesh Pandey and Andreas Savakis", "title": "Poly-GAN: Multi-Conditioned GAN for Fashion Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Poly-GAN, a novel conditional GAN architecture that is motivated\nby Fashion Synthesis, an application where garments are automatically placed on\nimages of human models at an arbitrary pose. Poly-GAN allows conditioning on\nmultiple inputs and is suitable for many tasks, including image alignment,\nimage stitching, and inpainting. Existing methods have a similar pipeline where\nthree different networks are used to first align garments with the human pose,\nthen perform stitching of the aligned garment and finally refine the results.\nPoly-GAN is the first instance where a common architecture is used to perform\nall three tasks. Our novel architecture enforces the conditions at all layers\nof the encoder and utilizes skip connections from the coarse layers of the\nencoder to the respective layers of the decoder. Poly-GAN is able to perform a\nspatial transformation of the garment based on the RGB skeleton of the model at\nan arbitrary pose. Additionally, Poly-GAN can perform image stitching,\nregardless of the garment orientation, and inpainting on the garment mask when\nit contains irregular holes. Our system achieves state-of-the-art quantitative\nresults on Structural Similarity Index metric and Inception Score metric using\nthe DeepFashion dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 00:29:39 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Pandey", "Nilesh", ""], ["Savakis", "Andreas", ""]]}, {"id": "1909.02215", "submitter": "Baris Gecer", "authors": "Baris Gecer, Alexander Lattas, Stylianos Ploumpis, Jiankang Deng,\n  Athanasios Papaioannou, Stylianos Moschoglou, Stefanos Zafeiriou", "title": "Synthesizing Coupled 3D Face Modalities by Trunk-Branch Generative\n  Adversarial Networks", "comments": "Check project page: https://github.com/barisgecer/TBGAN for the full\n  resolution results and the accompanying video", "journal-ref": null, "doi": "10.1007/978-3-030-58526-6_25", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating realistic 3D faces is of high importance for computer graphics and\ncomputer vision applications. Generally, research on 3D face generation\nrevolves around linear statistical models of the facial surface. Nevertheless,\nthese models cannot represent faithfully either the facial texture or the\nnormals of the face, which are very crucial for photo-realistic face synthesis.\nRecently, it was demonstrated that Generative Adversarial Networks (GANs) can\nbe used for generating high-quality textures of faces. Nevertheless, the\ngeneration process either omits the geometry and normals, or independent\nprocesses are used to produce 3D shape information. In this paper, we present\nthe first methodology that generates high-quality texture, shape, and normals\njointly, which can be used for photo-realistic synthesis. To do so, we propose\na novel GAN that can generate data from different modalities while exploiting\ntheir correlations. Furthermore, we demonstrate how we can condition the\ngeneration on the expression and create faces with various facial expressions.\nThe qualitative results shown in this paper are compressed due to size\nlimitations, full-resolution results and the accompanying video can be found in\nthe supplementary documents. The code and models are available at the project\npage: https://github.com/barisgecer/TBGAN.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:33:50 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 18:25:34 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2020 01:12:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gecer", "Baris", ""], ["Lattas", "Alexander", ""], ["Ploumpis", "Stylianos", ""], ["Deng", "Jiankang", ""], ["Papaioannou", "Athanasios", ""], ["Moschoglou", "Stylianos", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1909.02518", "submitter": "Christian Richardt", "authors": "Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh\\\"ofer, Hans-Peter\n  Seidel, Thabo Beeler, Christian Richardt, Christian Theobalt", "title": "Neural Style-Preserving Visual Dubbing", "comments": "SIGGRAPH Asia 2019", "journal-ref": null, "doi": "10.1145/3355089.3356500", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dubbing is a technique for translating video content from one language to\nanother. However, state-of-the-art visual dubbing techniques directly copy\nfacial expressions from source to target actors without considering\nidentity-specific idiosyncrasies such as a unique type of smile. We present a\nstyle-preserving visual dubbing approach from single video inputs, which\nmaintains the signature style of target actors when modifying facial\nexpressions, including mouth motions, to match foreign languages. At the heart\nof our approach is the concept of motion style, in particular for facial\nexpressions, i.e., the person-specific expression change that is yet another\nessential factor beyond visual accuracy in face editing applications. Our\nmethod is based on a recurrent generative adversarial network that captures the\nspatiotemporal co-activation of facial expressions, and enables generating and\nmodifying the facial expressions of the target actor while preserving their\nstyle. We train our model with unsynchronized source and target videos in an\nunsupervised manner using cycle-consistency and mouth expression losses, and\nsynthesize photorealistic video frames using a layered neural face renderer.\nOur approach generates temporally coherent results, and handles dynamic\nbackgrounds. Our results show that our dubbing approach maintains the\nidiosyncratic style of the target actor better than previous approaches, even\nfor widely differing source and target actors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:41:20 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 10:53:40 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kim", "Hyeongwoo", ""], ["Elgharib", "Mohamed", ""], ["Zollh\u00f6fer", "Michael", ""], ["Seidel", "Hans-Peter", ""], ["Beeler", "Thabo", ""], ["Richardt", "Christian", ""], ["Theobalt", "Christian", ""]]}, {"id": "1909.02533", "submitter": "David Novotn\\'y", "authors": "David Novotny, Nikhila Ravi, Benjamin Graham, Natalia Neverova, Andrea\n  Vedaldi", "title": "C3DPO: Canonical 3D Pose Networks for Non-Rigid Structure From Motion", "comments": "Added a link to the source code into the abstract", "journal-ref": "IEEE/CVF International Conference on Computer Vision 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose C3DPO, a method for extracting 3D models of deformable objects\nfrom 2D keypoint annotations in unconstrained images. We do so by learning a\ndeep network that reconstructs a 3D object from a single view at a time,\naccounting for partial occlusions, and explicitly factoring the effects of\nviewpoint changes and object deformations. In order to achieve this\nfactorization, we introduce a novel regularization technique. We first show\nthat the factorization is successful if, and only if, there exists a certain\ncanonicalization function of the reconstructed shapes. Then, we learn the\ncanonicalization function together with the reconstruction one, which\nconstrains the result to be consistent. We demonstrate state-of-the-art\nreconstruction results for methods that do not use ground-truth 3D supervision\nfor a number of benchmarks, including Up3D and PASCAL3D+. Source code has been\nmade available at https://github.com/facebookresearch/c3dpo_nrsfm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:16:15 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 16:39:18 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Novotny", "David", ""], ["Ravi", "Nikhila", ""], ["Graham", "Benjamin", ""], ["Neverova", "Natalia", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1909.02641", "submitter": "Jinsoo Choi", "authors": "Jinsoo Choi, In So Kweon", "title": "Deep Iterative Frame Interpolation for Full-frame Video Stabilization", "comments": "Accepted to ACM Transactions on Graphics, To be presented at SIGGRAPH\n  Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video stabilization is a fundamental and important technique for higher\nquality videos. Prior works have extensively explored video stabilization, but\nmost of them involve cropping of the frame boundaries and introduce moderate\nlevels of distortion. We present a novel deep approach to video stabilization\nwhich can generate video frames without cropping and low distortion. The\nproposed framework utilizes frame interpolation techniques to generate in\nbetween frames, leading to reduced inter-frame jitter. Once applied in an\niterative fashion, the stabilization effect becomes stronger. A major advantage\nis that our framework is end-to-end trainable in an unsupervised manner. In\naddition, our method is able to run in near real-time (15 fps). To the best of\nour knowledge, this is the first work to propose an unsupervised deep approach\nto full-frame video stabilization. We show the advantages of our method through\nquantitative and qualitative evaluations comparing to the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:38:45 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Choi", "Jinsoo", ""], ["Kweon", "In So", ""]]}, {"id": "1909.02807", "submitter": "Fabrizio Corda", "authors": "Fabrizio Corda, Jean-Marc Thiery, Marco Livesu, Enrico Puppo, Tamy\n  Boubekeur, Riccardo Scateni", "title": "Real-time Deformation with Coupled Cages and Skeletons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based and cage-based deformation techniques represent the two most\npopular approaches to control real-time deformations of digital shapes and are,\nto a vast extent, complementary to one another. Despite their complementary\nroles, high-end modeling packages do not allow for seamless integration of such\ncontrol structures, thus inducing a considerable burden on the user to maintain\nthem synchronized. In this paper, we propose a framework that seamlessly\ncombines rigging skeletons and deformation cages, granting artists with a\nreal-time deformation system that operates using any smooth combination of the\ntwo approaches. By coupling the deformation spaces of cages and skeletons, we\naccess a much larger space, containing poses that are impossible to obtain by\nacting solely on a skeleton or a cage. Our method is oblivious to the specific\ntechniques used to perform skinning and cage-based deformation, securing it\ncompatible with pre-existing tools. We demonstrate the usefulness of our hybrid\napproach on a variety of examples.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:30:39 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Corda", "Fabrizio", ""], ["Thiery", "Jean-Marc", ""], ["Livesu", "Marco", ""], ["Puppo", "Enrico", ""], ["Boubekeur", "Tamy", ""], ["Scateni", "Riccardo", ""]]}, {"id": "1909.02986", "submitter": "Aryaman Gupta", "authors": "Aryaman Gupta, Ulrik G\\\"unther, Pietro Incardona, Ata Deniz Aydin,\n  Raimund Dachselt, Stefan Gumhold, Ivo F. Sbalzarini", "title": "A Proposed Framework for Interactive Virtual Reality In Situ\n  Visualization of Parallel Numerical Simulations", "comments": "2 pages, 2 figures, accepted at IEEE Large Data Analysis and\n  Visualization (LDAV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computer simulations progress to increasingly complex, non-linear, and\nthree-dimensional systems and phenomena, intuitive and immediate visualization\nof their results is becoming crucial. While Virtual Reality (VR) and Natural\nUser Interfaces (NUIs) have been shown to improve understanding of complex 3D\ndata, their application to live in situ visualization and computational\nsteering is hampered by performance requirements. Here, we present the design\nof a software framework for interactive VR in situ visualization of parallel\nnumerical simulations, as well as a working prototype implementation. Our\ndesign is targeted towards meeting the performance requirements for VR, and our\nwork is packaged in a framework that allows for easy instrumentation of\nsimulations. Our preliminary results inform about the technical feasibility of\nthe architecture, as well as the challenges that remain.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:03:12 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gupta", "Aryaman", ""], ["G\u00fcnther", "Ulrik", ""], ["Incardona", "Pietro", ""], ["Aydin", "Ata Deniz", ""], ["Dachselt", "Raimund", ""], ["Gumhold", "Stefan", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1909.03669", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Gaofeng Meng, Jiwen Lu, Shiming Xiang,\n  Chunhong Pan", "title": "DensePoint: Learning Densely Contextual Representation for Efficient\n  Point Cloud Processing", "comments": "Accepted to ICCV 2019. 15 pages, 8 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud processing is very challenging, as the diverse shapes formed by\nirregular points are often indistinguishable. A thorough grasp of the elusive\nshape requires sufficiently contextual semantic information, yet few works\ndevote to this. Here we propose DensePoint, a general architecture to learn\ndensely contextual representation for point cloud processing. Technically, it\nextends regular grid CNN to irregular point configuration by generalizing a\nconvolution operator, which holds the permutation invariance of points, and\nachieves efficient inductive learning of local patterns. Architecturally, it\nfinds inspiration from dense connection mode, to repeatedly aggregate\nmulti-level and multi-scale semantics in a deep hierarchy. As a result, densely\ncontextual information along with rich semantics, can be acquired by DensePoint\nin an organic manner, making it highly effective. Extensive experiments on\nchallenging benchmarks across four tasks, as well as thorough model analysis,\nverify DensePoint achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 07:18:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Meng", "Gaofeng", ""], ["Lu", "Jiwen", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1909.03993", "submitter": "Fabiano Petronetto", "authors": "Luis Gustavo Nonato, Fabiano Petronetto e Claudio Silva", "title": "GLoG: Laplacian of Gaussian for Spatial Pattern Detection in\n  Spatio-Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary detection has long been a fundamental tool for image processing and\ncomputer vision, supporting the analysis of static and time-varying data. In\nthis work, we built upon the theory of Graph Signal Processing to propose a\nnovel boundary detection filter in the context of graphs, having as main\napplication scenario the visual analysis of spatio-temporal data. More\nspecifically, we propose the equivalent for graphs of the so-called Laplacian\nof Gaussian edge detection filter, which is widely used in image processing.\nThe proposed filter is able to reveal interesting spatial patterns while still\nenabling the definition of entropy of time slices. The entropy reveals the\ndegree of randomness of a time slice, helping users to identify expected and\nunexpected phenomena over time. The effectiveness of our approach appears in\napplications involving synthetic and real data sets, which show that the\nproposed methodology is able to uncover interesting spatial and temporal\nphenomena. The provided examples and case studies make clear the usefulness of\nour approach as a mechanism to support visual analytic tasks involving\nspatio-temporal data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:04:48 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Nonato", "Luis Gustavo", ""], ["Silva", "Fabiano Petronetto e Claudio", ""]]}, {"id": "1909.04610", "submitter": "Bedrich Benes", "authors": "Suren Deepak Rajasekaran, Hao Kang, Bedrich Benes, Martin \\v{C}ad\\'ik,\n  Eric Galin, Eric Gu\\'erin, Adrien Peytavie, Pavel Slav\\'ik", "title": "PTRM: Perceived Terrain Realism Metrics", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrains are visually important and commonly used in computer graphics. While\nmany algorithms for their generation exist, it is difficult to assess the\nrealism of a generated terrain. This paper presents a first step in the\ndirection of perceptual evaluation of terrain models. We gathered and\ncategorized several classes of real terrains and we generated synthetic\nterrains by using methods from computer graphics. We then conducted two large\nstudies ranking the terrains perceptually and showing that the synthetic\nterrains are perceived as lacking realism as compared to the real ones. Then we\nprovide insight into the features that affect the perceived realism by a\nquantitative evaluation based on localized geomorphology-based landform\nfeatures (geomorphons) that categorize terrain structures such as valleys,\nridges, hollows, etc. We show that the presence or absence of certain features\nhave a significant perceptual effect. We then introduce Perceived Terrain\nRealism Metrics (PTRM); a perceptual metrics that estimates perceived realism\nof a terrain represented as a digital elevation map by relating distribution of\nterrain features with their perceived realism. We validated PTRM on real and\nsynthetic data and compared it to the perceptual studies. To confirm the\nimportance of the presence of these features, we used a generative deep neural\nnetwork to transfer them between real terrains and synthetic ones and we\nperformed another perceptual experiment that further confirmed their importance\nfor perceived realism.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:35:53 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Rajasekaran", "Suren Deepak", ""], ["Kang", "Hao", ""], ["Benes", "Bedrich", ""], ["\u010cad\u00edk", "Martin", ""], ["Galin", "Eric", ""], ["Gu\u00e9rin", "Eric", ""], ["Peytavie", "Adrien", ""], ["Slav\u00edk", "Pavel", ""]]}, {"id": "1909.04835", "submitter": "Bei Wang", "authors": "Ingrid Hotz, Roxana Bujack, Christoph Garth, Bei Wang", "title": "Mathematical Foundations in Visualization", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by participants of the Dagstuhl Seminar 18041 (\"Foundations of Data\n  Visualization\"). The book is expected to be published by Springer. The final\n  book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical concepts and tools have shaped the field of visualization in\nfundamental ways and played a key role in the development of a large variety of\nvisualization techniques. In this chapter, we sample the visualization\nliterature to provide a taxonomy of the usage of mathematics in visualization,\nand to identify a fundamental set of mathematics that should be taught to\nstudents as part of an introduction to contemporary visualization research.\nWithin the scope of this chapter, we are unable to provide a full review of all\nmathematical foundations of visualization; rather, we identify a number of\nconcepts that are useful in visualization, explain their significance, and\nprovide references for further reading.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:10:40 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Hotz", "Ingrid", ""], ["Bujack", "Roxana", ""], ["Garth", "Christoph", ""], ["Wang", "Bei", ""]]}, {"id": "1909.04974", "submitter": "Richard Jiang", "authors": "Khan Faraz, Ahmed Bouridane, Richard Jiang, Tiancheng Xia, Paul\n  Chazot, Abdel Ennaceur", "title": "Computer-Aided Automated Detection of Gene-Controlled Social Actions of\n  Drosophila", "comments": "published on International Conference on Smart Cities at Cambridge\n  2018", "journal-ref": "International Conference on Smart Cities at Cambridge 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene expression of social actions in Drosophilae has been attracting wide\ninterest from biologists, medical scientists and psychologists. Gene-edited\nDrosophilae have been used as a test platform for experimental investigation.\nFor example, Parkinson's genes can be embedded into a group of newly bred\nDrosophilae for research purpose. However, human observation of numerous tiny\nDrosophilae for a long term is an arduous work, and the dependence on human's\nacute perception is highly unreliable. As a result, an automated system of\nsocial action detection using machine learning has been highly demanded. In\nthis study, we propose to automate the detection and classification of two\ninnate aggressive actions demonstrated by Drosophilae. Robust keypoint\ndetection is achieved using selective spatio-temporal interest points (sSTIP)\nwhich are then described using the 3D Scale Invariant Feature Transform\n(3D-SIFT) descriptors. Dimensionality reduction is performed using Spectral\nRegression Kernel Discriminant Analysis (SR-KDA) and classification is done\nusing the nearest centre rule. The classification accuracy shown demonstrates\nthe feasibility of the proposed system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:15:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Faraz", "Khan", ""], ["Bouridane", "Ahmed", ""], ["Jiang", "Richard", ""], ["Xia", "Tiancheng", ""], ["Chazot", "Paul", ""], ["Ennaceur", "Abdel", ""]]}, {"id": "1909.05295", "submitter": "Fabian Bolte", "authors": "Fabian Bolte and Stefan Bruckner", "title": "Measures in Visualization Space", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by participants of the Dagstuhl Seminar 18041 (\"Foundations of Data\n  Visualization\") and that is expected to be published by Springer. The final\n  book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement is an integral part of modern science, providing the fundamental\nmeans for evaluation, comparison, and prediction. In the context of\nvisualization, several different types of measures have been proposed, ranging\nfrom approaches that evaluate particular aspects of individual visualization\ntechniques, their perceptual characteristics, and even economic factors.\nFurthermore, there are approaches that attempt to provide means for measuring\ngeneral properties of the visualization process as a whole. Measures can be\nquantitative or qualitative, and one of the primary goals is to provide\nobjective means for reasoning about visualizations and their effectiveness. As\nsuch, they play a central role in the development of scientific theories for\nvisualization. In this chapter, we provide an overview of the current state of\nthe art, survey and classify different types of visualization measures,\ncharacterize their strengths and drawbacks, and provide an outline of open\nchallenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 18:38:06 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bolte", "Fabian", ""], ["Bruckner", "Stefan", ""]]}, {"id": "1909.05483", "submitter": "Simon Niklaus", "authors": "Simon Niklaus, Long Mai, Jimei Yang, Feng Liu", "title": "3D Ken Burns Effect from a Single Image", "comments": "TOG 2019, http://sniklaus.com/kenburns", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ken Burns effect allows animating still images with a virtual camera scan\nand zoom. Adding parallax, which results in the 3D Ken Burns effect, enables\nsignificantly more compelling results. Creating such effects manually is\ntime-consuming and demands sophisticated editing skills. Existing automatic\nmethods, however, require multiple input images from varying viewpoints. In\nthis paper, we introduce a framework that synthesizes the 3D Ken Burns effect\nfrom a single image, supporting both a fully automatic mode and an interactive\nmode with the user controlling the camera. Our framework first leverages a\ndepth prediction pipeline, which estimates scene depth that is suitable for\nview synthesis tasks. To address the limitations of existing depth estimation\nmethods such as geometric distortions, semantic distortions, and inaccurate\ndepth boundaries, we develop a semantic-aware neural network for depth\nprediction, couple its estimate with a segmentation-based depth adjustment\nprocess, and employ a refinement neural network that facilitates accurate depth\npredictions at object boundaries. According to this depth estimate, our\nframework then maps the input image to a point cloud and synthesizes the\nresulting video frames by rendering the point cloud from the corresponding\ncamera positions. To address disocclusions while maintaining geometrically and\ntemporally coherent synthesis results, we utilize context-aware color- and\ndepth-inpainting to fill in the missing information in the extreme views of the\ncamera path, thus extending the scene geometry of the point cloud. Experiments\nwith a wide variety of image content show that our method enables realistic\nsynthesis results. Our study demonstrates that our system allows users to\nachieve better results while requiring little effort compared to existing\nsolutions for the 3D Ken Burns effect creation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:55:07 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Yang", "Jimei", ""], ["Liu", "Feng", ""]]}, {"id": "1909.05511", "submitter": "Alireza Amiraghdam", "authors": "Alireza Amiraghdam (1), Alexandra Diehl (1), Renato Pajarola (1) ((1)\n  Department of Informatics University of Zurich)", "title": "LOCALIS: Locally-adaptive Line Simplification for GPU-based Geographic\n  Vector Data Visualization", "comments": null, "journal-ref": "Computer Graphics Forum 39 (2020) 443-453", "doi": "10.1111/cgf.13993", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of large vector line data is a core task in geographic and\ncartographic systems. Vector maps are often displayed at different cartographic\ngeneralization levels, traditionally by using several discrete levels-of-detail\n(LODs). This limits the generalization levels to a fixed and predefined set of\nLODs, and generally does not support smooth LOD transitions. However, fast GPUs\nand novel line rendering techniques can be exploited to integrate dynamic\nvector map LOD management into GPU-based algorithms for locally-adaptive line\nsimplification and real-time rendering. We propose a new technique that\ninteractively visualizes large line vector datasets at variable LODs. It is\nbased on the Douglas-Peucker line simplification principle, generating an\nexhaustive set of line segments whose specific subsets represent the lines at\nany variable LOD. At run time, an appropriate and view-dependent error metric\nsupports screen-space adaptive LOD levels and the display of the correct subset\nof line segments accordingly. Our implementation shows that we can simplify and\ndisplay large line datasets interactively. We can successfully apply line style\npatterns, dynamic LOD selection lenses, and anti-aliasing techniques to our\nline rendering.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 08:58:13 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 20:57:32 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Amiraghdam", "Alireza", ""], ["Diehl", "Alexandra", ""], ["Pajarola", "Renato", ""]]}, {"id": "1909.05719", "submitter": "Paul Zikas", "authors": "Paul Zikas, George Papagiannakis, Nick Lydatakis, Steve Kateros,\n  Stavroula Ntoa, Ilia Adami, Constantine Stephanidis", "title": "Scenior: An Immersive Visual Scripting system based on VR Software\n  Design Patterns for Experiential Training", "comments": null, "journal-ref": "The Visual Computer volume 36, pages 1965-1977 (2020)", "doi": "10.1007/s00371-020-01919-0", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer\nproduct, and training on simulators is rapidly becoming standard in many\nindustrial sectors. However, the available systems are either focusing on\ngaming context, featuring limited capabilities or they support only content\ncreation of virtual environments without any rapid prototyping and\nmodification. In this project, we propose a code-free, visual scripting\nplatform to replicate gamified training scenarios through rapid prototyping and\nVR software design patterns. We implemented and compared two authoring tools:\na) visual scripting and b) VR editor for the rapid reconstruction of VR\ntraining scenarios. Our visual scripting module is capable to generate training\napplications utilizing a node-based scripting system whereas the VR editor\ngives user/developer the ability to customize and populate new VR training\nscenarios directly from the virtual environment. We also introduce action\nprototypes, a new software design pattern suitable to replicate behavioral\ntasks for VR experiences. In addition, we present the training scenegraph\narchitecture as the main model to represent training scenarios on a modular,\ndynamic and highly adaptive acyclic graph based on a structured educational\ncurriculum. Finally, a user-based evaluation of the proposed solution indicated\nthat users - regardless of their programming expertise - can effectively use\nthe tools to create and modify training scenarios in VR.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:37:13 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 23:07:27 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zikas", "Paul", ""], ["Papagiannakis", "George", ""], ["Lydatakis", "Nick", ""], ["Kateros", "Steve", ""], ["Ntoa", "Stavroula", ""], ["Adami", "Ilia", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "1909.05736", "submitter": "Boyang Deng", "authors": "Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey\n  Hinton, Andrea Tagliasacchi", "title": "CvxNet: Learnable Convex Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any solid object can be decomposed into a collection of convex polytopes (in\nshort, convexes). When a small number of convexes are used, such a\ndecomposition can be thought of as a piece-wise approximation of the geometry.\nThis decomposition is fundamental in computer graphics, where it provides one\nof the most common ways to approximate geometry, for example, in real-time\nphysics simulation. A convex object also has the property of being\nsimultaneously an explicit and implicit representation: one can interpret it\nexplicitly as a mesh derived by computing the vertices of a convex hull, or\nimplicitly as the collection of half-space constraints or support functions.\nTheir implicit representation makes them particularly well suited for neural\nnetwork training, as they abstract away from the topology of the geometry they\nneed to represent. However, at testing time, convexes can also generate\nexplicit representations -- polygonal meshes -- which can then be used in any\ndownstream application. We introduce a network architecture to represent a low\ndimensional family of convexes. This family is automatically derived via an\nauto-encoding process. We investigate the applications of this architecture\nincluding automatic convex decomposition, image to 3D reconstruction, and\npart-based shape retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:59:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 15:59:46 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 19:18:09 GMT"}, {"version": "v4", "created": "Sun, 12 Apr 2020 23:43:12 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Deng", "Boyang", ""], ["Genova", "Kyle", ""], ["Yazdani", "Soroosh", ""], ["Bouaziz", "Sofien", ""], ["Hinton", "Geoffrey", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1909.08766", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Daniel McDuff, Shital Shah", "title": "A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression\n  Capabilities", "comments": "International Conference on Multimodal Interaction (ICMI 2019)", "journal-ref": null, "doi": "10.1145/3340555.3353744", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied avatars as virtual agents have many applications and provide\nbenefits over disembodied agents, allowing non-verbal social and interactional\ncues to be leveraged, in a similar manner to how humans interact with each\nother. We present an open embodied avatar built upon the Unreal Engine that can\nbe controlled via a simple python programming interface. The avatar has lip\nsyncing (phoneme control), head gesture and facial expression (using either\nfacial action units or cardinal emotion categories) capabilities. We release\ncode and models to illustrate how the avatar can be controlled like a puppet or\nused to create a simple conversational agent using public application\nprogramming interfaces (APIs). GITHUB link:\nhttps://github.com/danmcduff/AvatarSim\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:39:39 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 05:10:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Aneja", "Deepali", ""], ["McDuff", "Daniel", ""], ["Shah", "Shital", ""]]}, {"id": "1909.09059", "submitter": "Titus Leistner", "authors": "Titus Leistner, Hendrik Schilling, Radek Mackowiak, Stefan Gumhold,\n  Carsten Rother", "title": "Learning to Think Outside the Box: Wide-Baseline Light Field Depth\n  Estimation with EPI-Shift", "comments": "Published at International Conference on 3D Vision (3DV) 2019", "journal-ref": null, "doi": "10.1109/3DV.2019.00036", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for depth estimation from light field data, based on a\nfully convolutional neural network architecture. Our goal is to design a\npipeline which achieves highly accurate results for small- and wide-baseline\nlight fields. Since light field training data is scarce, all learning-based\napproaches use a small receptive field and operate on small disparity ranges.\nIn order to work with wide-baseline light fields, we introduce the idea of\nEPI-Shift: To virtually shift the light field stack which enables to retain a\nsmall receptive field, independent of the disparity range. In this way, our\napproach \"learns to think outside the box of the receptive field\". Our network\nperforms joint classification of integer disparities and regression of\ndisparity-offsets. A U-Net component provides excellent long-range smoothing.\nEPI-Shift considerably outperforms the state-of-the-art learning-based\napproaches and is on par with hand-crafted methods. We demonstrate this on a\npublicly available, synthetic, small-baseline benchmark and on large-baseline\nreal-world recordings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:57:17 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Leistner", "Titus", ""], ["Schilling", "Hendrik", ""], ["Mackowiak", "Radek", ""], ["Gumhold", "Stefan", ""], ["Rother", "Carsten", ""]]}, {"id": "1909.09351", "submitter": "Patrick Cheong-Iao Pang", "authors": "Patrick Cheong-Iao Pang, Robert P. Biuk-Aghai, Simon Fong, Yain-Whar\n  Si", "title": "An Experimental Comparison of Map-like Visualisations and Treemaps", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treemaps have been used in information visualisation for over two decades.\nThey make use of nested filled areas to represent information hierarchies such\nas file systems, library catalogues, etc. Recent years have witnessed the\nemergence of visualisations that resemble geographic maps. In this paper we\npresent a study that compares the performance of one such map-like\nvisualisation with the original two forms of the treemap, namely nested and\nnon-nested treemaps. Our study employed a mixed-method evaluation of accuracy,\nspeed and usability (such as the ease-of-use and helpfulness of understanding\nthe information). We found that accuracy was highest for the map-like\nvisualisations, followed by nested treemaps and lastly non-nested treemaps.\nTask performance was fastest for nested treemaps, followed by non-nested\ntreemaps, and then map-like visualisations. For usability, nested treemaps was\nconsidered slightly more helpful than map-like visualisations while non-nested\nperformed poorly. We conclude that the results regarding accuracy are promising\nfor the use of map-like visualisations in tasks involving the visualisation of\nhierarchical information, while non-nested treemap are favoured in tasks\nrequiring speed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:26:11 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Pang", "Patrick Cheong-Iao", ""], ["Biuk-Aghai", "Robert P.", ""], ["Fong", "Simon", ""], ["Si", "Yain-Whar", ""]]}, {"id": "1909.09429", "submitter": "Paul Zikas", "authors": "Efstratios Geronikolakis, Paul Zikas, Steve Kateros, Nick Lydatakis,\n  Stelios Georgiou, Mike Kentros, George Papagiannakis", "title": "A True AR Authoring Tool for Interactive Virtual Museums", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by \"Visual Computing in Cultural Heritage\" and that is expected to be\n  published by Springer. The final book chapter will differ from this preprint", "journal-ref": null, "doi": "10.1007/978-3-030-37191-3_12", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new and innovative way of spatial computing that appeared\nrecently in the bibliography called True Augmented Reality (AR), is employed in\ncultural heritage preservation. This innovation could be adapted by the Virtual\nMuseums of the future to enhance the quality of experience. It emphasises, the\nfact that a visitor will not be able to tell, at a first glance, if the\nartefact that he/she is looking at is real or not and it is expected to draw\nthe visitors' interest. True AR is not limited to artefacts but extends even to\nbuildings or life-sized character simulations of statues. It provides the best\nvisual quality possible so that the users will not be able to tell the real\nobjects from the augmented ones. Such applications can be beneficial for future\nmuseums, as with True AR, 3D models of various exhibits, monuments, statues,\ncharacters and buildings can be reconstructed and presented to the visitors in\na realistic and innovative way. We also propose our Virtual Reality Sample\napplication, a True AR playground featuring basic components and tools for\ngenerating interactive Virtual Museum applications, alongside a 3D\nreconstructed character (the priest of Asinou church) facilitating the\nstoryteller of the augmented experience.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:10:23 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 12:39:51 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 05:29:31 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 11:09:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Geronikolakis", "Efstratios", ""], ["Zikas", "Paul", ""], ["Kateros", "Steve", ""], ["Lydatakis", "Nick", ""], ["Georgiou", "Stelios", ""], ["Kentros", "Mike", ""], ["Papagiannakis", "George", ""]]}, {"id": "1909.10337", "submitter": "Esmitt Ram\\'irez", "authors": "Luis Ortegano and Esmitt Ramirez", "title": "Serious Educational Reinforcement Game in Preschool", "comments": "9 pages, written in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.GR cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Education, is constant the searching of techniques to strengthen and\nextend the educational strategies in order to achieve that students get\nknowledge received in classroom, training personal courses and workshops.\nSerious games are part of that set of educational strategies to reinforce or\nextend the knowledge using a set of rules and policies in an interactive and\namuse way. Nowadays, using technology, it is possible to add images and sounds\ninto serious games under a digital platform. In this paper, we present a\nsolution based on serious games to reinforce the mathematical knowledge in the\npreschool education following the study plan of the Bolivarian Republic of\nVenezuela. Thus, we offer a tool with expectation of improving skills and\nknowledge of children on a specific area, also allowing follow the student\ntrack getting information of the activities performed. This compilation is made\nperforming reports which will be available for teachers and facilitators.\nReports allow adjust orientation and organization of topics imparted in\nclassroom, and to know the general overview of a preschool educational course.\nTest performed determine the computational performance of solution, also the\nimpact on children and teachers. The wide attention of children was verified,\naccomplishing the educational tasks in an indirect way.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:41:16 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 14:42:18 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ortegano", "Luis", ""], ["Ramirez", "Esmitt", ""]]}, {"id": "1909.11620", "submitter": "Erva Ulu", "authors": "Erva Ulu, Nurcan Gecer Ulu, Walter Hsiao, Saigopal Nelaturi", "title": "Manufacturability Oriented Model Correction and Build Direction\n  Optimization for Additive Manufacturing", "comments": "Accepted to Journal of Mechanical Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to analyze and modify a shape to make it manufacturable\nfor a given additive manufacturing (AM) process. Different AM technologies,\nprocess parameters or materials introduce geometric constraints on what is\nmanufacturable or not. Given an input 3D model and minimum printable feature\nsize dictated by the manufacturing process characteristics and parameters, our\nalgorithm generates a corrected geometry that is printable with the intended AM\nprocess. A key issue in model correction for manufacturability is the\nidentification of critical features that are affected by the printing process.\nTo address this challenge, we propose a topology aware approach to construct\nthe allowable space for a print head to traverse during the 3D printing\nprocess. Combined with our build orientation optimization algorithm, the amount\nof modifications performed on the shape is kept at minimum while providing an\naccurate approximation of the as-manufactured part. We demonstrate our method\non a variety of 3D models and validate it by 3D printing the results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 20:44:37 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ulu", "Erva", ""], ["Ulu", "Nurcan Gecer", ""], ["Hsiao", "Walter", ""], ["Nelaturi", "Saigopal", ""]]}, {"id": "1909.11622", "submitter": "K\\'aroly Zsolnai-Feh\\'er", "authors": "K\\'aroly Zsolnai-Feh\\'er, Peter Wonka, Michael Wimmer", "title": "Photorealistic Material Editing Through Direct Image Manipulation", "comments": "The high-resolution paper, supplementary materials, video and source\n  code are available here:\n  https://users.cg.tuwien.ac.at/zsolnai/gfx/photorealistic-material-editing/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Creating photorealistic materials for light transport algorithms requires\ncarefully fine-tuning a set of material properties to achieve a desired\nartistic effect. This is typically a lengthy process that involves a trained\nartist with specialized knowledge. In this work, we present a technique that\naims to empower novice and intermediate-level users to synthesize high-quality\nphotorealistic materials by only requiring basic image processing knowledge. In\nthe proposed workflow, the user starts with an input image and applies a few\nintuitive transforms (e.g., colorization, image inpainting) within a 2D image\neditor of their choice, and in the next step, our technique produces a\nphotorealistic result that approximates this target image. Our method combines\nthe advantages of a neural network-augmented optimizer and an encoder neural\nnetwork to produce high-quality output results within 30 seconds. We also\ndemonstrate that it is resilient against poorly-edited target images and\npropose a simple extension to predict image sequences with a strict time budget\nof 1-2 seconds per image.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:17:49 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Zsolnai-Feh\u00e9r", "K\u00e1roly", ""], ["Wonka", "Peter", ""], ["Wimmer", "Michael", ""]]}, {"id": "1909.12354", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Zherong Pan, Lin Gao, Dinesh Manocha", "title": "Realtime Simulation of Thin-Shell Deformable Materials using CNN-Based\n  Mesh Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of accelerating thin-shell deformable object\nsimulations by dimension reduction. We present a new algorithm to embed a\nhigh-dimensional configuration space of deformable objects in a low-dimensional\nfeature space, where the configurations of objects and feature points have\napproximate one-to-one mapping. Our key technique is a graph-based\nconvolutional neural network (CNN) defined on meshes with arbitrary topologies\nand a new mesh embedding approach based on physics-inspired loss term. We have\napplied our approach to accelerate high-resolution thin shell simulations\ncorresponding to cloth-like materials, where the configuration space has tens\nof thousands of degrees of freedom. We show that our physics-inspired embedding\napproach leads to higher accuracy compared with prior mesh embedding methods.\nFinally, we show that the temporal evolution of the mesh in the feature space\ncan also be learned using a recurrent neural network (RNN) leading to fully\nlearnable physics simulators. After training our learned simulator runs\n$500-10000\\times$ faster and the accuracy is high enough for robot manipulation\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:38:58 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 14:28:37 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2019 04:16:33 GMT"}, {"version": "v4", "created": "Fri, 28 Feb 2020 04:45:12 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tan", "Qingyang", ""], ["Pan", "Zherong", ""], ["Gao", "Lin", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1909.12583", "submitter": "Jan Morovic", "authors": "Jan Morovic", "title": "Color continuity along the journey from ideas to objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human endeavor has involved making choices about color and looking for ways\nto color objects since the dawn of civilization. While it has been the\nexclusive domain of artists and craftspeople for millennia, the last century\nhas seen the introduction of a scientific basis to color communication. The\nultimate goal of this development is for color communication to happen\nseamlessly and in a transparent way. There are however two categories of\nchallenges here: first, understanding and quantifying color needs and\nexpectation and second, developing control mechanisms that deliver the desired\ncolor. In this paper a review will be presented of the color needs in\nend-to-end color journeys, from initial concept to final colored object and an\noverview of recent developments in color printing will follow. Topics like\nimaging pipelines (including the recently-introduced HP Pixel Control), the\nease of use of color workflows (including HP Smart Color Tools), the handling\nof brand or corporate identity colors (via HP Professional PANTONE Emulation)\nand the measurement of color difference under specific viewing arrangements\n(i.e., the dENS metric for viewing samples without separation) will be\naddressed. Finally, a series of challenges for the future will be set out, so\nthat their solution can be approached by both academic and industrial\ncommunities.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 09:50:20 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Morovic", "Jan", ""]]}, {"id": "1909.13066", "submitter": "Chai Shuangming", "authors": "Shuangming Chai, Xiao-Ming Fu, Ligang Liu", "title": "Voting for Distortion Points in Geometric Processing", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2947420", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low isometric distortion is often required for mesh parameterizations. A\nconfiguration of some vertices, where the distortion is concentrated, provides\na way to mitigate isometric distortion, but determining the number and\nplacement of these vertices is non-trivial. We call these vertices distortion\npoints. We present a novel and automatic method to detect distortion points\nusing a voting strategy. Our method integrates two components: candidate\ngeneration and candidate voting. Given a closed triangular mesh, we generate\ncandidate distortion points by executing a three-step procedure repeatedly: (1)\nrandomly cut an input to a disk topology; (2) compute a low conformal\ndistortion parameterization; and (3) detect the distortion points. Finally, we\ncount the candidate points and generate the final distortion points by voting.\nWe demonstrate that our algorithm succeeds when employed on various closed\nmeshes with a genus of zero or higher. The distortion points generated by our\nmethod are utilized in three applications, including planar parameterization,\nsemi-automatic landmark correspondence, and isotropic remeshing. Compared to\nother state-of-the-art methods, our method demonstrates stronger practical\nrobustness in distortion point detection.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 10:39:22 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 15:14:58 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 18:16:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chai", "Shuangming", ""], ["Fu", "Xiao-Ming", ""], ["Liu", "Ligang", ""]]}, {"id": "1909.13200", "submitter": "Omri Azencot", "authors": "Omri Azencot and Rongjie Lai", "title": "Shape Analysis via Functional Map Construction and Bases Pursuit", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to simultaneously compute scalar basis functions with an\nassociated functional map for a given pair of triangle meshes. Unlike previous\ntechniques that put emphasis on smoothness with respect to the\nLaplace--Beltrami operator and thus favor low-frequency eigenfunctions, we aim\nfor a spectrum that allows for better feature matching. This change of\nperspective introduces many degrees of freedom into the problem which we\nexploit to improve the accuracy of our computed correspondences. To effectively\nsearch in this high dimensional space of solutions, we incorporate into our\nminimization state-of-the-art regularizers. We solve the resulting highly\nnon-linear and non-convex problem using an iterative scheme via the Alternating\nDirection Method of Multipliers. At each step, our optimization involves simple\nto solve linear or Sylvester-type equations. In practice, our method performs\nwell in terms of convergence, and we additionally show that it is similar to a\nprovably convergent problem. We show the advantages of our approach by\nextensively testing it on multiple datasets in a few applications including\nshape matching, consistent quadrangulation and scalar function transfer.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 04:16:13 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Azencot", "Omri", ""], ["Lai", "Rongjie", ""]]}, {"id": "1909.13501", "submitter": "Guang-Yuan Hao", "authors": "Guang-Yuan Hao, Hong-Xing Yu, Wei-Shi Zheng", "title": "DSRGAN: Explicitly Learning Disentangled Representation of Underlying\n  Structure and Rendering for Image Generation without Tuple Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on explicitly learning disentangled representation for natural image\ngeneration, where the underlying spatial structure and the rendering on the\nstructure can be independently controlled respectively, yet using no tuple\nsupervision. The setting is significant since tuple supervision is costly and\nsometimes even unavailable. However, the task is highly unconstrained and thus\nill-posed. To address this problem, we propose to introduce an auxiliary domain\nwhich shares a common underlying-structure space with the target domain, and we\nmake a partially shared latent space assumption. The key idea is to encourage\nthe partially shared latent variable to represent the similar underlying\nspatial structures in both domains, while the two domain-specific latent\nvariables will be unavoidably arranged to present renderings of two domains\nrespectively. This is achieved by designing two parallel generative networks\nwith a common Progressive Rendering Architecture (PRA), which constrains both\ngenerative networks' behaviors to model shared underlying structure and to\nmodel spatially dependent relation between rendering and underlying structure.\nThus, we propose DSRGAN (GANs for Disentangling Underlying Structure and\nRendering) to instantiate our method. We also propose a quantitative criterion\n(the Normalized Disentanglability) to quantify disentanglability. Comparison to\nthe state-of-the-art methods shows that DSRGAN can significantly outperform\nthem in disentanglability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:07:11 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hao", "Guang-Yuan", ""], ["Yu", "Hong-Xing", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1909.13646", "submitter": "Tao Wen", "authors": "Tao Wen, Danilo Pelusi, Yong Deng", "title": "Vital Spreaders Identification in Complex Networks with Multi-Local\n  Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The important nodes identification has been an interesting problem in this\nissue. Several centrality measures have been proposed to solve this problem,\nbut most of previous methods have their own limitations. To address this\nproblem more effectively, multi-local dimension (MLD) which is based on the\nfractal property is proposed to identify the vital spreaders in this paper.\nThis proposed method considers the information contained in the box and $q$\nplays a weighting coefficient for this partition information. MLD would have\ndifferent expressions with different value of $q$, and it would degenerate to\nlocal information dimension and variant of local dimension when $q = 1$ when $q\n= 0$ respectively, both of which have been effective identification method for\ninfluential nodes. Thus, MLD would be a more general method which can\ndegenerate to some exiting centrality measures. In addition, different with\nclassical methods, the node with low MLD would be more important in the\nnetwork. Some comparison methods and real-world complex networks are applied in\nthis paper to show the effectiveness and reasonableness of this proposed\nmethod. The experiment results show the superiority of this proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 12:48:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wen", "Tao", ""], ["Pelusi", "Danilo", ""], ["Deng", "Yong", ""]]}, {"id": "1909.13875", "submitter": "Tiago Ribeiro", "authors": "Tiago Ribeiro and Ana Paiva", "title": "Expressive Inverse Kinematics Solving in Real-time for Virtual and\n  Robotic Interactive Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With new advancements in interaction techniques, character animation also\nrequires new methods, to support fields such as robotics, and VR/AR.\nInteractive characters in such fields are becoming driven by AI which opens up\nthe possibility of non-linear and open-ended narratives that may even include\ninteraction with the real, physical world. This paper presents and describes\nERIK, an expressive inverse kinematics technique aimed at such applications.\nOur technique allows an arbitrary kinematic chain, such as an arm, snake, or\nrobotic manipulator, to exhibit an expressive posture while aiming its\nend-point towards a given target orientation. The technique runs in\ninteractive-time and does not require any pre-processing step such as e.g.\ntraining in machine learning techniques, in order to support new embodiments or\nnew postures. That allows it to be integrated in an artist-friendly workflow,\nbringing artists closer to the development of such AI-driven expressive\ncharacters, by allowing them to use their typical animation tools of choice,\nand to properly pre-visualize the animation during design-time, even on a real\nrobot. The full algorithmic specification is presented and described so that it\ncan be implemented and used throughout the communities of the various fields we\naddress. We demonstrate ERIK on different virtual kinematic structures, and\nalso on a low-fidelity robot that was crafted using wood and hobby-grade\nservos, to show how well the technique performs even on a low-grade robot. Our\nevaluation shows how well the technique performs, i.e., how well the character\nis able to point at the target orientation, while minimally disrupting its\ntarget expressive posture, and respecting its mechanical rotation limits.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:56:24 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 12:58:34 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ribeiro", "Tiago", ""], ["Paiva", "Ana", ""]]}]