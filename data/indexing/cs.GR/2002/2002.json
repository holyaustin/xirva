[{"id": "2002.00241", "submitter": "James Damon", "authors": "James Damon", "title": "Rigidity Properties of the Blum Medial Axis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Blum medial axis of a region in $\\mathbb R^n$ with piecewise\nsmooth boundary and examine its \"rigidity properties\", by which we mean\nproperties preserved under diffeomorphisms of the regions preserving the medial\naxis. There are several possible versions of rigidity depending on what\nfeatures of the Blum medial axis we wish to retain. We use a form of the cross\nratio from projective geometry to show that in the case of four smooth sheets\nof the medial axis meeting along a branching submanifold, the cross ratio\ndefines a function on the branching sheet which must be preserved under any\ndiffeomorphism of the medial axis with another. Second, we show in the generic\ncase, along a Y-branching submanifold that there are three cross ratios\ninvolving the three limiting tangent planes of the three smooth sheets and each\nof the hyperplanes defined by one of the radial lines and the tangent space to\nthe Y-branching submanifold at the point, which again must be preserved.\nMoreover, the triple of cross ratios then locally uniquely determines the\nangles between the smooth sheets. Third, we observe that for a diffeomorphism\nof the region preserving the Blum medial axis and the infinitesimal directions\nof the radial lines, the second derivative of the diffeomorphism at points of\nthe medial axis must satisfy a condition relating the radial shape operators\nand hence the differential geometry of the boundaries at corresponding boundary\npoints.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 17:11:00 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Damon", "James", ""]]}, {"id": "2002.00328", "submitter": "Shao-Kui Zhang", "authors": "Song-Hai Zhang, Shao-Kui Zhang, Wei-Yu Xie, Cheng-Yang Luo, Hong-Bo Fu", "title": "Fast 3D Indoor Scene Synthesis with Discrete and Exact Layout Pattern\n  Extraction", "comments": "We currently received our first valuable comments from reviewers. We\n  will continuing modify our paper accordingly, so this paper will be modified\n  frequently", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast framework for indoor scene synthesis, given a room geometry\nand a list of objects with learnt priors. Unlike existing data-driven\nsolutions, which often extract priors by co-occurrence analysis and statistical\nmodel fitting, our method measures the strengths of spatial relations by tests\nfor complete spatial randomness (CSR), and extracts complex priors based on\nsamples with the ability to accurately represent discrete layout patterns. With\nthe extracted priors, our method achieves both acceleration and plausibility by\npartitioning input objects into disjoint groups, followed by layout\noptimization based on the Hausdorff metric. Extensive experiments show that our\nframework is capable of measuring more reasonable relations among objects and\nsimultaneously generating varied arrangements in seconds.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 05:09:09 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 09:14:29 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Song-Hai", ""], ["Zhang", "Shao-Kui", ""], ["Xie", "Wei-Yu", ""], ["Luo", "Cheng-Yang", ""], ["Fu", "Hong-Bo", ""]]}, {"id": "2002.00369", "submitter": "R\\'emi Coulon", "authors": "R\\'emi Coulon, Elisabetta A. Matsumoto, Henry Segerman, Steve Trettel", "title": "Non-Euclidean Virtual Reality IV: Sol", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.GR math.GT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents virtual reality software designed to explore the Sol\ngeometry. The simulation is available on 3-dimensional.space/sol.html\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 11:32:30 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Coulon", "R\u00e9mi", ""], ["Matsumoto", "Elisabetta A.", ""], ["Segerman", "Henry", ""], ["Trettel", "Steve", ""]]}, {"id": "2002.02159", "submitter": "Daisuke Iwai", "authors": "Daiki Tone, Daisuke Iwai, Shinsaku Hiura, Kosuke Sato", "title": "FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers\n  in Dynamic Projection Mapping", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973444", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel active marker for dynamic projection mapping (PM)\nthat emits a temporal blinking pattern of infrared (IR) light representing its\nID. We used a multi-material three dimensional (3D) printer to fabricate a\nprojection object with optical fibers that can guide IR light from LEDs\nattached on the bottom of the object. The aperture of an optical fiber is\ntypically very small; thus, it is unnoticeable to human observers under\nprojection and can be placed on a strongly curved part of a projection surface.\nIn addition, the working range of our system can be larger than previous\nmarker-based methods as the blinking patterns can theoretically be recognized\nby a camera placed at a wide range of distances from markers. We propose an\nautomatic marker placement algorithm to spread multiple active markers over the\nsurface of a projection object such that its pose can be robustly estimated\nusing captured images from arbitrary directions. We also propose an\noptimization framework for determining the routes of the optical fibers in such\na way that collisions of the fibers can be avoided while minimizing the loss of\nlight intensity in the fibers. Through experiments conducted using three\nfabricated objects containing strongly curved surfaces, we confirmed that the\nproposed method can achieve accurate dynamic PMs in a significantly wide\nworking range.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 08:56:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tone", "Daiki", ""], ["Iwai", "Daisuke", ""], ["Hiura", "Shinsaku", ""], ["Sato", "Kosuke", ""]]}, {"id": "2002.02167", "submitter": "Daisuke Iwai", "authors": "Tatsuyuki Ueda, Daisuke Iwai, Takefumi Hiraki, Kosuke Sato", "title": "IlluminatedFocus: Vision Augmentation using Spatial Defocusing via Focal\n  Sweep Eyeglasses and High-Speed Projector", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": "10.1109/TVCG.2020.2973496", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at realizing novel vision augmentation experiences, this paper\nproposes the IlluminatedFocus technique, which spatially defocuses real-world\nappearances regardless of the distance from the user's eyes to observed real\nobjects. With the proposed technique, a part of a real object in an image\nappears blurred, while the fine details of the other part at the same distance\nremain visible. We apply Electrically Focus-Tunable Lenses (ETL) as eyeglasses\nand a synchronized high-speed projector as illumination for a real scene. We\nperiodically modulate the focal lengths of the glasses (focal sweep) at more\nthan 60 Hz so that a wearer cannot perceive the modulation. A part of the scene\nto appear focused is illuminated by the projector when it is in focus of the\nuser's eyes, while another part to appear blurred is illuminated when it is out\nof the focus. As the basis of our spatial focus control, we build mathematical\nmodels to predict the range of distance from the ETL within which real objects\nbecome blurred on the retina of a user. Based on the blur range, we discuss a\ndesign guideline for effective illumination timing and focal sweep range. We\nalso model the apparent size of a real scene altered by the focal length\nmodulation. This leads to an undesirable visible seam between focused and\nblurred areas. We solve this unique problem by gradually blending the two\nareas. Finally, we demonstrate the feasibility of our proposal by implementing\nvarious vision augmentation applications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 09:16:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ueda", "Tatsuyuki", ""], ["Iwai", "Daisuke", ""], ["Hiraki", "Takefumi", ""], ["Sato", "Kosuke", ""]]}, {"id": "2002.02358", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-VIBS, IHMTEK), Anton Andreev\n  (GIPSA-Services), Cesar Mendoza (IHMTEK), Marco Congedo (GIPSA-VIBS)", "title": "A comparison of mobile VR display running on an ordinary smartphone with\n  standard PC display for P300-BCI stimulus presentation", "comments": "IEEE Transactions on Games, Institute of Electrical and Electronics\n  Engineers, In press", "journal-ref": null, "doi": "10.1109/TG.2019.2957963", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) based on electroencephalography (EEG) is a\npromising technology for enhancing virtual reality (VR) applications-in\nparticular, for gaming. We focus on the so-called P300-BCI, a stable and\naccurate BCI paradigm relying on the recognition of a positive event-related\npotential (ERP) occurring in the EEG about 300 ms post-stimulation. We\nimplemented a basic version of such a BCI displayed on an ordinary and\naffordable smartphone-based head-mounted VR device: that is, a mobile and\npassive VR system (with no electronic components beyond the smartphone). The\nmobile phone performed the stimuli presentation, EEG synchronization (tagging)\nand feedback display. We compared the ERPs and the accuracy of the BCI on the\nVR device with a traditional BCI running on a personal computer (PC). We also\nevaluated the impact of subjective factors on the accuracy. The study was\nwithin-subjects, with 21 participants and one session in each modality. No\nsignificant difference in BCI accuracy was found between the PC and VR systems,\nalthough the P200 ERP was significantly wider and larger in the VR system as\ncompared to the PC system.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:04:17 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-VIBS, IHMTEK"], ["Andreev", "Anton", "", "GIPSA-Services"], ["Mendoza", "Cesar", "", "IHMTEK"], ["Congedo", "Marco", "", "GIPSA-VIBS"]]}, {"id": "2002.02671", "submitter": "Ali Asadipour", "authors": "Efstratios Doukakis, Kurt Debattista, Thomas Bashford-Rogers, Amar\n  Dhokia, Ali Asadipour, Alan Chalmers and Carlo Harvey", "title": "Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual\n  Environments", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2898823", "report-no": null, "categories": "cs.GR cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Environments (VEs) provide the opportunity to simulate a wide range\nof applications, from training to entertainment, in a safe and controlled\nmanner. For applications which require realistic representations of real world\nenvironments, the VEs need to provide multiple, physically accurate sensory\nstimuli. However, simulating all the senses that comprise the human sensory\nsystem (HSS) is a task that requires significant computational resources. Since\nit is intractable to deliver all senses at the highest quality, we propose a\nresource distribution scheme in order to achieve an optimal perceptual\nexperience within the given computational budgets. This paper investigates\nresource balancing for multi-modal scenarios composed of aural, visual and\nolfactory stimuli. Three experimental studies were conducted. The first\nexperiment identified perceptual boundaries for olfactory computation. In the\nsecond experiment, participants (N=25) were asked, across a fixed number of\nbudgets (M=5), to identify what they perceived to be the best visual, acoustic\nand olfactory stimulus quality for a given computational budget. Results\ndemonstrate that participants tend to prioritise visual quality compared to\nother sensory stimuli. However, as the budget size is increased, users prefer a\nbalanced distribution of resources with an increased preference for having\nsmell impulses in the VE. Based on the collected data, a quality prediction\nmodel is proposed and its accuracy is validated against previously unused\nbudgets and an untested scenario in a third and final experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 08:59:41 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Doukakis", "Efstratios", ""], ["Debattista", "Kurt", ""], ["Bashford-Rogers", "Thomas", ""], ["Dhokia", "Amar", ""], ["Asadipour", "Ali", ""], ["Chalmers", "Alan", ""], ["Harvey", "Carlo", ""]]}, {"id": "2002.02792", "submitter": "Prerana Mukherjee", "authors": "Laxman Kumarapu and Prerana Mukherjee", "title": "AnimePose: Multi-person 3D pose estimation and animation", "comments": "arXiv admin note: text overlap with arXiv:1907.11346 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D animation of humans in action is quite challenging as it involves using a\nhuge setup with several motion trackers all over the person's body to track the\nmovements of every limb. This is time-consuming and may cause the person\ndiscomfort in wearing exoskeleton body suits with motion sensors. In this work,\nwe present a trivial yet effective solution to generate 3D animation of\nmultiple persons from a 2D video using deep learning. Although significant\nimprovement has been achieved recently in 3D human pose estimation, most of the\nprior works work well in case of single person pose estimation and multi-person\npose estimation is still a challenging problem. In this work, we firstly\npropose a supervised multi-person 3D pose estimation and animation framework\nnamely AnimePose for a given input RGB video sequence. The pipeline of the\nproposed system consists of various modules: i) Person detection and\nsegmentation, ii) Depth Map estimation, iii) Lifting 2D to 3D information for\nperson localization iv) Person trajectory prediction and human pose tracking.\nOur proposed system produces comparable results on previous state-of-the-art 3D\nmulti-person pose estimation methods on publicly available datasets MuCo-3DHP\nand MuPoTS-3D datasets and it also outperforms previous state-of-the-art human\npose tracking methods by a significant margin of 11.7% performance gain on MOTA\nscore on Posetrack 2018 dataset.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 11:11:56 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Kumarapu", "Laxman", ""], ["Mukherjee", "Prerana", ""]]}, {"id": "2002.03165", "submitter": "Chandra Sekhar Ravuri", "authors": "Chandra Sekhar Ravuri (1), Rajesh Sureddi (2), Sathya Veera Reddy\n  Dendi (2), Shanmuganathan Raman (1), Sumohana S. Channappayya (2) ((1)\n  Department of Electrical Engineering, Indian Institute of Technology\n  Gandhinagar, India., (2) Department of Electrical Engineering, Indian\n  Institute of Technology Hyderabad, India.)", "title": "Deep No-reference Tone Mapped Image Quality Assessment", "comments": "5 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of rendering high dynamic range (HDR) images to be viewed on\nconventional displays is called tone mapping. However, tone mapping introduces\ndistortions in the final image which may lead to visual displeasure. To\nquantify these distortions, we introduce a novel no-reference quality\nassessment technique for these tone mapped images. This technique is composed\nof two stages. In the first stage, we employ a convolutional neural network\n(CNN) to generate quality aware maps (also known as distortion maps) from tone\nmapped images by training it with the ground truth distortion maps. In the\nsecond stage, we model the normalized image and distortion maps using an\nAsymmetric Generalized Gaussian Distribution (AGGD). The parameters of the AGGD\nmodel are then used to estimate the quality score using support vector\nregression (SVR). We show that the proposed technique delivers competitive\nperformance relative to the state-of-the-art techniques. The novelty of this\nwork is its ability to visualize various distortions as quality maps\n(distortion maps), especially in the no-reference setting, and to use these\nmaps as features to estimate the quality score of tone mapped images.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 13:41:18 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ravuri", "Chandra Sekhar", ""], ["Sureddi", "Rajesh", ""], ["Dendi", "Sathya Veera Reddy", ""], ["Raman", "Shanmuganathan", ""], ["Channappayya", "Sumohana S.", ""]]}, {"id": "2002.03196", "submitter": "Benjamin Cecchetto T", "authors": "Benjamin T. Cecchetto", "title": "Correction of Chromatic Aberration from a Single Image Using Keypoints", "comments": "Originally this paper was a project for a course in 2009 and has not\n  been published. It has been cited multiple times since then. The LaTeX code\n  was lost, so it has been revised in February 2020 to post on ArXiV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method to correct for chromatic aberration in a\nsingle photograph. Our method replicates what a user would do in a photo\nediting program to account for this defect. We find matching keypoints in each\ncolour channel then align them as a user would.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 16:36:30 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Cecchetto", "Benjamin T.", ""]]}, {"id": "2002.03575", "submitter": "Hongmin Zhu", "authors": "Hongmin Zhu, Fuli Feng, Xiangnan He, Xiang Wang, Yan Li, Kai Zheng,\n  Yongdong Zhang", "title": "Bilinear Graph Neural Network with Neighbor Interactions", "comments": "Accepted by IJCAI 2020. SOLE copyright holder is IJCAI (International\n  Joint Conferences on Artificial Intelligence), all rights reserved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Network (GNN) is a powerful model to learn representations and\nmake predictions on graph data. Existing efforts on GNN have largely defined\nthe graph convolution as a weighted sum of the features of the connected nodes\nto form the representation of the target node. Nevertheless, the operation of\nweighted sum assumes the neighbor nodes are independent of each other, and\nignores the possible interactions between them. When such interactions exist,\nsuch as the co-occurrence of two neighbor nodes is a strong signal of the\ntarget node's characteristics, existing GNN models may fail to capture the\nsignal. In this work, we argue the importance of modeling the interactions\nbetween neighbor nodes in GNN. We propose a new graph convolution operator,\nwhich augments the weighted sum with pairwise interactions of the\nrepresentations of neighbor nodes. We term this framework as Bilinear Graph\nNeural Network (BGNN), which improves GNN representation ability with bilinear\ninteractions between neighbor nodes. In particular, we specify two BGNN models\nnamed BGCN and BGAT, based on the well-known GCN and GAT, respectively.\nEmpirical results on three public benchmarks of semi-supervised node\nclassification verify the effectiveness of BGNN -- BGCN (BGAT) outperforms GCN\n(GAT) by 1.6% (1.5%) in classification accuracy.Codes are available at:\nhttps://github.com/zhuhm1996/bgnn.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 06:43:38 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 08:20:11 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 01:50:32 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 03:51:46 GMT"}, {"version": "v5", "created": "Sat, 30 May 2020 02:41:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhu", "Hongmin", ""], ["Feng", "Fuli", ""], ["He", "Xiangnan", ""], ["Wang", "Xiang", ""], ["Li", "Yan", ""], ["Zheng", "Kai", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2002.03891", "submitter": "Fabian Bolte", "authors": "Fabian Bolte, Mahsan Nourani, Eric D. Ragan and Stefan Bruckner", "title": "SplitStreams: A Visual Metaphor for Evolving Hierarchies", "comments": "Will be published at IEEE Transactions on Visualization & Computer\n  Graphics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of hierarchically structured data over time is an ongoing\nchallenge and several approaches exist trying to solve it. Techniques such as\nanimated or juxtaposed tree visualizations are not capable of providing a good\noverview of the time series and lack expressiveness in conveying changes over\ntime. Nested streamgraphs provide a better understanding of the data evolution,\nbut lack the clear outline of hierarchical structures at a given timestep.\nFurthermore, these approaches are often limited to static hierarchies or\nexclude complex hierarchical changes in the data, limiting their use cases. We\npropose a novel visual metaphor capable of providing a static overview of all\nhierarchical changes over time, as well as clearly outlining the hierarchical\nstructure at each individual time step. Our method allows for smooth\ntransitions between tree maps and nested streamgraphs, enabling the exploration\nof the trade-off between dynamic behavior and hierarchical structure. As our\ntechnique handles topological changes of all types, it is suitable for a wide\nrange of applications. We demonstrate the utility of our method on several use\ncases, evaluate it with a user study, and provide its full source code.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 16:00:01 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Bolte", "Fabian", ""], ["Nourani", "Mahsan", ""], ["Ragan", "Eric D.", ""], ["Bruckner", "Stefan", ""]]}, {"id": "2002.04439", "submitter": "Maurice Quach", "authors": "Maurice Quach, Giuseppe Valenzise and Frederic Dufaux", "title": "Folding-based compression of point cloud attributes", "comments": "Published in ICIP 2020. The source code can be found at\n  https://github.com/mauriceqch/pcc_attr_folding", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to compress point cloud attributes leverage either\ngeometric or video-based compression tools. We explore a radically different\napproach inspired by recent advances in point cloud representation learning.\nPoint clouds can be interpreted as 2D manifolds in 3D space. Specifically, we\nfold a 2D grid onto a point cloud and we map attributes from the point cloud\nonto the folded 2D grid using a novel optimized mapping method. This mapping\nresults in an image, which opens a way to apply existing image processing\ntechniques on point cloud attributes. However, as this mapping process is lossy\nin nature, we propose several strategies to refine it so that attributes can be\nmapped to the 2D grid with minimal distortion. Moreover, this approach can be\nflexibly applied to point cloud patches in order to better adapt to local\ngeometric complexity. In this work, we consider point cloud attribute\ncompression; thus, we compress this image with a conventional 2D image codec.\nOur preliminary results show that the proposed folding-based coding scheme can\nalready reach performance similar to the latest MPEG Geometry-based PCC (G-PCC)\ncodec.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:55:58 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 09:04:51 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 07:17:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Quach", "Maurice", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""]]}, {"id": "2002.04509", "submitter": "Charles G. Gunn", "authors": "Charles G. Gunn", "title": "Course notes Geometric Algebra for Computer Graphics, SIGGRAPH 2019", "comments": "56 pages, 14 figures, 6 tables, Course notes for SIGGRAPH 2019 short\n  course July 30, 2019 taught with Steven De Keninck. arXiv admin note:\n  substantial text overlap with arXiv:1901.05873", "journal-ref": null, "doi": "10.1145/3305366.3328099", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  What is the best representation for doing euclidean geometry on computers?\nThese notes from a SIGGRAPH 2019 short course entitled \"Geometric algebra for\ncomputer graphics\" introduce projective geometric algebra (PGA) as a modern\nframework for this task. PGA features: uniform representation of points, lines,\nand planes; robust, parallel-safe join and meet operations; compact,\npolymorphic syntax for euclidean formulas and constructions; a single intuitive\nsandwich form for isometries; native support for automatic differentiation; and\ntight integration of kinematics and rigid body mechanics. PGA includes vector,\nquaternion, dual quaternion, and exterior algebras as sub-algebras, simplifying\nthe learning curve and transition path for experienced practitioners. On the\npractical side, it can be efficiently implemented, while its rich syntax\nenhances programming productivity. The basic ideas are introduced in the 2D\ncontext and developed selectively for 3D. Advantages to traditional approaches\nare collected in a table at the end. The article aims to be a self-contained\nintroduction for practitioners of euclidean geometry and includes numerous\nexamples, formulas, figures, and tables.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:11:16 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:35:04 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Gunn", "Charles G.", ""]]}, {"id": "2002.05234", "submitter": "David Lowry-Duda", "authors": "David Lowry-Duda", "title": "Visualizing modular forms", "comments": "20 pages, many figures, after first major set of revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine several currently used techniques for visualizing complex-valued\nfunctions applied to modular forms. We plot several examples and study the\nbenefits and limitations of each technique. We then introduce a method of\nvisualization that can take advantage of colormaps in Python's matplotlib\nlibrary, describe an implementation, and give more examples. Much of this\ndiscussion applies to general visualizations of complex-valued functions in the\nplane.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 21:01:00 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 17:57:31 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Lowry-Duda", "David", ""]]}, {"id": "2002.05282", "submitter": "Min Chen", "authors": "Min Chen, Mateu Sbert, Alfie Abdul-Rahman, and Deborah Silver", "title": "A Bounded Measure for Estimating the Benefit of Visualization", "comments": "Comment on version 2: This revised version, which includes a new\n  formal proof, many additions, and a detailed revision report, was submitted\n  to SciVis 2020. Unexpectedly, our revision effort did not have much influence\n  on the SciVis 2020 reviewers who gave an outright rejection with lower scores\n  than EuroVis reviews. We will share these reviews after we have completed our\n  feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory can be used to analyze the cost-benefit of visualization\nprocesses. However, the current measure of benefit contains an unbounded term\nthat is neither easy to estimate nor intuitive to interpret. In this work, we\npropose to revise the existing cost-benefit measure by replacing the unbounded\nterm with a bounded one. We examine a number of bounded measures that include\nthe Jenson-Shannon divergence and a new divergence measure formulated as part\nof this work. We use visual analysis to support the multi-criteria comparison,\nnarrowing the search down to those options with better mathematical properties.\nWe apply those remaining options to two visualization case studies to\ninstantiate their uses in practical scenarios, while the collected real world\ndata further informs the selection of a bounded measure, which can be used to\nestimate the benefit of visualization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 23:39:07 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 20:33:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chen", "Min", ""], ["Sbert", "Mateu", ""], ["Abdul-Rahman", "Alfie", ""], ["Silver", "Deborah", ""]]}, {"id": "2002.05409", "submitter": "David Baum", "authors": "David Baum, Pascal Kovacs, Ulrich Eisenecker, Richard M\\\"uller", "title": "A User-centered Approach for Optimizing Information Visualizations", "comments": null, "journal-ref": "WSCG2016", "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimization of information visualizations is time consuming and\nexpensive. To reduce this we propose an improvement of existing optimization\napproaches based on user-centered design, focusing on readability,\ncomprehensibility, and user satisfaction as optimization goals. The changes\ncomprise (1) a separate optimization of user interface and representation, (2)\na fully automated evaluation of the representation, and (3) qualitative user\nstudies for simultaneously creating and evaluating interface variants. On the\nbasis of these results we are able to find a local optimum of an information\nvisualization in an efficient way.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 09:50:34 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Baum", "David", ""], ["Kovacs", "Pascal", ""], ["Eisenecker", "Ulrich", ""], ["M\u00fcller", "Richard", ""]]}, {"id": "2002.05509", "submitter": "Andrey Ignatov", "authors": "Andrey Ignatov, Luc Van Gool, Radu Timofte", "title": "Replacing Mobile Camera ISP with a Single Deep Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the popularity of mobile photography is growing constantly, lots of\nefforts are being invested now into building complex hand-crafted camera ISP\nsolutions. In this work, we demonstrate that even the most sophisticated ISP\npipelines can be replaced with a single end-to-end deep learning model trained\nwithout any prior knowledge about the sensor and optics used in a particular\ndevice. For this, we present PyNET, a novel pyramidal CNN architecture designed\nfor fine-grained image restoration that implicitly learns to perform all ISP\nsteps such as image demosaicing, denoising, white balancing, color and contrast\ncorrection, demoireing, etc. The model is trained to convert RAW Bayer data\nobtained directly from mobile camera sensor into photos captured with a\nprofessional high-end DSLR camera, making the solution independent of any\nparticular mobile ISP implementation. To validate the proposed approach on the\nreal data, we collected a large-scale dataset consisting of 10 thousand\nfull-resolution RAW-RGB image pairs captured in the wild with the Huawei P20\ncameraphone (12.3 MP Sony Exmor IMX380 sensor) and Canon 5D Mark IV DSLR. The\nexperiments demonstrate that the proposed solution can easily get to the level\nof the embedded P20's ISP pipeline that, unlike our approach, is combining the\ndata from two (RGB + B/W) camera sensors. The dataset, pre-trained models and\ncodes used in this paper are available on the project website.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:22:39 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ignatov", "Andrey", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2002.05721", "submitter": "Baptiste Wojtkowski", "authors": "Baptiste Wojtkowski (Heudiasyc), Pedro Castillo (Heudiasyc), Indira\n  Thouvenin (Heudiasyc)", "title": "A New Exocentric Metaphor for Complex Path Following to Control a UAV\n  Using Mixed Reality", "comments": null, "journal-ref": "Journ\\'ees Fran\\c{c}aise de l'Informatique graphique et de\n  R\\'ealit\\'e Virtuelle, Nov 2019, Marseille, France", "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teleoperation of Unmanned Aerial Vehicles (UAVs) has recently become an\nnoteworthly research topic in the field of human robot interaction. Each year,\na variety of devices is being studied to design adapted interface for diverse\npurpose such as view taking, search and rescue operation or suveillance. New\ninterfaces have to be precise, simple and intuitive even for complex path\nplanning. Moreover, when teleoperation involves long distance control, user\nneeds to get proper feedbacks and avoid motion sickness. In order to overcome\nall these challenges, a new interaction metaphor named DrEAM (Drone Exocentric\nAdvanced Metaphor) was designed. User can see the UAV he is controlling in a\nvirtual environment mapped to the real world. He can interact with it as a\nsimple object in a classical virtual world. An experiment was lead in order to\nevaluate the perfomances of this metaphor, comparing performance of novice user\nusing either a direct-view joystick control or using DrEAM.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 11:02:33 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Wojtkowski", "Baptiste", "", "Heudiasyc"], ["Castillo", "Pedro", "", "Heudiasyc"], ["Thouvenin", "Indira", "", "Heudiasyc"]]}, {"id": "2002.05968", "submitter": "Dongbo Zhang", "authors": "Dongbo Zhang, Xuequan Lu, Hong Qin and Ying He", "title": "Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud filtering is a fundamental problem in geometry modeling and\nprocessing. Despite of significant advancement in recent years, the existing\nmethods still suffer from two issues: 1) they are either designed without\npreserving sharp features or less robust in feature preservation; and 2) they\nusually have many parameters and require tedious parameter tuning. In this\npaper, we propose a novel deep learning approach that automatically and\nrobustly filters point clouds by removing noise and preserving their sharp\nfeatures. Our point-wise learning architecture consists of an encoder and a\ndecoder. The encoder directly takes points (a point and its neighbors) as\ninput, and learns a latent representation vector which goes through the decoder\nto relate the ground-truth position with a displacement vector. The trained\nneural network can automatically generate a set of clean points from a noisy\ninput. Extensive experiments show that our approach outperforms the\nstate-of-the-art deep learning techniques in terms of both visual quality and\nquantitative error metrics. The source code and dataset can be found at\nhttps://github.com/dongbo-BUAA-VR/Pointfilter.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:06:44 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 08:10:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Zhang", "Dongbo", ""], ["Lu", "Xuequan", ""], ["Qin", "Hong", ""], ["He", "Ying", ""]]}, {"id": "2002.06260", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Why Do Line Drawings Work? A Realism Hypothesis", "comments": "Accepted to Perception", "journal-ref": "Perception. 49:4 (2020) 439-451", "doi": "10.1177/0301006620908207", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why is it that we can recognize object identity and 3D shape from line\ndrawings, even though they do not exist in the natural world? This paper\nhypothesizes that the human visual system perceives line drawings as if they\nwere approximately realistic images. Moreover, the techniques of line drawing\nare chosen to accurately convey shape to a human observer. Several implications\nand variants of this hypothesis are explored.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 21:41:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "2002.06597", "submitter": "Kui Jia", "authors": "Jiabao Lei and Kui Jia", "title": "Analytic Marching: An Analytic Meshing Solution from Deep Implicit\n  Surface Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a problem of learning surface mesh via implicit functions\nin an emerging field of deep learning surface reconstruction, where implicit\nfunctions are popularly implemented as multi-layer perceptrons (MLPs) with\nrectified linear units (ReLU). To achieve meshing from learned implicit\nfunctions, existing methods adopt the de-facto standard algorithm of marching\ncubes; while promising, they suffer from loss of precision learned in the MLPs,\ndue to the discretization nature of marching cubes. Motivated by the knowledge\nthat a ReLU based MLP partitions its input space into a number of linear\nregions, we identify from these regions analytic cells and analytic faces that\nare associated with zero-level isosurface of the implicit function, and\ncharacterize the theoretical conditions under which the identified analytic\nfaces are guaranteed to connect and form a closed, piecewise planar surface.\nBased on our theorem, we propose a naturally parallelizable algorithm of\nanalytic marching, which marches among analytic cells to exactly recover the\nmesh captured by a learned MLP. Experiments on deep learning mesh\nreconstruction verify the advantages of our algorithm over existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 15:36:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Lei", "Jiabao", ""], ["Jia", "Kui", ""]]}, {"id": "2002.06827", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki and Eric Zimmermann", "title": "Large-Scale Evaluation of Shape-Aware Neighborhood Weights and\n  Neighborhood Sizes", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-20", "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define and evaluate a weighting scheme for neighborhoods in\npoint sets. Our weighting takes the shape of the geometry, i.e. the normal\ninformation, into account. This causes the obtained neighborhoods to be more\nreliable in the sense that connectivity also depends on the orientation of the\npoint set. We utilize a sigmoid to define the weights based on the normal\nvariation. For an evaluation of the weighting scheme, we turn to a Shannon\nentropy model for feature separation and rigorously prove its non-degeneracy\nfor our family of weights. Based on this model, we evaluate our weighting terms\non a large scale of both clean and real-world models. This evaluation provides\nresults regarding the choice of optimal parameters within our weighting scheme.\nFurthermore, the large-scale evaluation also reveals that neighborhood sizes\nshould not be fixed globally when processing models. This is in contrast to\ncurrent general practice in the field of geometry processing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:25:26 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 11:18:14 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Skrodzki", "Martin", ""], ["Zimmermann", "Eric", ""]]}, {"id": "2002.07002", "submitter": "Alexandros Keros", "authors": "Alexandros D. Keros, Divakaran Divakaran and Kartic Subr", "title": "Jittering Samples using a kd-Tree Stratification", "comments": "24 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo sampling techniques are used to estimate high-dimensional\nintegrals that model the physics of light transport in virtual scenes for\ncomputer graphics applications. These methods rely on the law of large numbers\nto estimate expectations via simulation, typically resulting in slow\nconvergence. Their errors usually manifest as undesirable grain in the pictures\ngenerated by image synthesis algorithms. It is well known that these errors\ndiminish when the samples are chosen appropriately. A well known technique for\nreducing error operates by subdividing the integration domain, estimating\nintegrals in each \\emph{stratum} and aggregating these values into a stratified\nsampling estimate. Na\\\"{i}ve methods for stratification, based on a lattice\n(grid) are known to improve the convergence rate of Monte Carlo, but require\nsamples that grow exponentially with the dimensionality of the domain.\n  We propose a simple stratification scheme for $d$ dimensional hypercubes\nusing the kd-tree data structure. Our scheme enables the generation of an\narbitrary number of equal volume partitions of the rectangular domain, and $n$\nsamples can be generated in $O(n)$ time. Since we do not always need to\nexplicitly build a kd-tree, we provide a simple procedure that allows the\nsample set to be drawn fully in parallel without any precomputation or storage,\nspeeding up sampling to $O(\\log n)$ time per sample when executed on $n$ cores.\nIf the tree is implicitly precomputed ($O(n)$ storage) the parallelised run\ntime reduces to $O(1)$ on $n$ cores. In addition to these benefits, we provide\nan upper bound on the worst case star-discrepancy for $n$ samples matching that\nof lattice-based sampling strategies, which occur as a special case of our\nproposed method. We use a number of quantitative and qualitative tests to\ncompare our method against state of the art samplers for image synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:27:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Keros", "Alexandros D.", ""], ["Divakaran", "Divakaran", ""], ["Subr", "Kartic", ""]]}, {"id": "2002.07481", "submitter": "Eduardo Faccin Vernier", "authors": "E. F. Vernier and R. Garcia and I. P. da Silva and J. L. D. Comba and\n  A. C. Telea", "title": "Quantitative Evaluation of Time-Dependent Multidimensional Projection\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dimensionality reduction methods are an essential tool for multidimensional\ndata analysis, and many interesting processes can be studied as time-dependent\nmultivariate datasets. There are, however, few studies and proposals that\nleverage on the concise power of expression of projections in the context of\ndynamic/temporal data. In this paper, we aim at providing an approach to assess\nprojection techniques for dynamic data and understand the relationship between\nvisual quality and stability. Our approach relies on an experimental setup that\nconsists of existing techniques designed for time-dependent data and new\nvariations of static methods. To support the evaluation of these techniques, we\nprovide a collection of datasets that has a wide variety of traits that encode\ndynamic patterns, as well as a set of spatial and temporal stability metrics\nthat assess the quality of the layouts. We present an evaluation of 11 methods,\n10 datasets, and 12 quality metrics, and elect the best-suited methods for\nprojecting time-dependent multivariate data, exploring the design choices and\ncharacteristics of each method. All our results are documented and made\navailable in a public repository to allow reproducibility of results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:40:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Vernier", "E. F.", ""], ["Garcia", "R.", ""], ["da Silva", "I. P.", ""], ["Comba", "J. L. D.", ""], ["Telea", "A. C.", ""]]}, {"id": "2002.07995", "submitter": "Yun-Peng Xiao", "authors": "Yun-Peng Xiao, Yu-Kun Lai, Fang-Lue Zhang, Chunpeng Li, Lin Gao", "title": "A Survey on Deep Geometry Learning: From a Representation Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers have now achieved great success on dealing with 2D images using\ndeep learning. In recent years, 3D computer vision and Geometry Deep Learning\ngain more and more attention. Many advanced techniques for 3D shapes have been\nproposed for different applications. Unlike 2D images, which can be uniformly\nrepresented by regular grids of pixels, 3D shapes have various representations,\nsuch as depth and multi-view images, voxel-based representation, point-based\nrepresentation, mesh-based representation, implicit surface representation,\netc. However, the performance for different applications largely depends on the\nrepresentation used, and there is no unique representation that works well for\nall applications. Therefore, in this survey, we review recent development in\ndeep learning for 3D geometry from a representation perspective, summarizing\nthe advantages and disadvantages of different representations in different\napplications. We also present existing datasets in these representations and\nfurther discuss future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 03:59:56 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 15:13:52 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Xiao", "Yun-Peng", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Fang-Lue", ""], ["Li", "Chunpeng", ""], ["Gao", "Lin", ""]]}, {"id": "2002.08657", "submitter": "Yuki Koyama", "authors": "Yuki Koyama and Takeo Igarashi", "title": "Computational Design with Crowds", "comments": "This book chapter was originally published in Computational\n  Interaction edited by Antti Oulasvirta, Per Ola Kristensson, Xiaojun Bi, and\n  Andrew Howes", "journal-ref": "Computational Interaction (Antti Oulasvirta, Per Ola Kristensson,\n  Xiaojun Bi, and Andrew Howes (Eds.)), chapter 6, pages 153-184. Oxford\n  University Press, 2018", "doi": "10.1093/oso/9780198799603.001.0001", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational design is aimed at supporting or automating design processes\nusing computational techniques. However, some classes of design tasks involve\ncriteria that are difficult to handle only with computers. For example, visual\ndesign tasks seeking to fulfill aesthetic goals are difficult to handle purely\nwith computers. One promising approach is to leverage human computation; that\nis, to incorporate human input into the computation process. Crowdsourcing\nplatforms provide a convenient way to integrate such human computation into a\nworking system.\n  In this chapter, we discuss such computational design with crowds in the\ndomain of parameter tweaking tasks in visual design. Parameter tweaking is\noften performed to maximize the aesthetic quality of designed objects.\nComputational design powered by crowds can solve this maximization problem by\nleveraging human computation. We discuss the opportunities and challenges of\ncomputational design with crowds with two illustrative examples: (1) estimating\nthe objective function (specifically, preference learning from crowds' pairwise\ncomparisons) to facilitate interactive design exploration by a designer and (2)\ndirectly searching for the optimal parameter setting that maximizes the\nobjective function (specifically, crowds-in-the-loop Bayesian optimization).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 10:40:13 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Koyama", "Yuki", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2002.08897", "submitter": "Manish Tiwari Mr", "authors": "Manish Tiwari", "title": "STW and SPIHT Wavelet compression using MATLAB wavelet Tool for Color\n  Image", "comments": "3", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images can be represented by mathematical function using wavelets. Wavelet\ncan be manipulated (shrink/expand) by applying some values to its function. It\nhelps to localize the signals. Application of wavelet in images processing has\nlarger scope as proved. Image compression is one of the dimension. There are\nvarious wavelet image compression techniques. This research paper focused on\ncomparison of only two techniques i.e. STW and SPIHT for color JPEG images.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 05:35:54 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Tiwari", "Manish", ""]]}, {"id": "2002.09533", "submitter": "Eryk Kopczy\\'nski", "authors": "Eryk Kopczy\\'nski and Dorota Celi\\'nska-Kopczy\\'nska", "title": "Real-Time Visualization in Non-Isotropic Geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-isotropic geometries are of interest to low-dimensional topologists,\nphysicists and cosmologists. However, they are challenging to comprehend and\nvisualize. We present novel methods of computing real-time native geodesic\nrendering of non-isotropic geometries. Our methods can be applied not only to\nvisualization, but also are essential for potential applications in machine\nlearning and video games.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:51:54 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 15:26:48 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kopczy\u0144ski", "Eryk", ""], ["Celi\u0144ska-Kopczy\u0144ska", "Dorota", ""]]}, {"id": "2002.10099", "submitter": "Amos Gropp", "authors": "Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, Yaron Lipman", "title": "Implicit Geometric Regularization for Learning Shapes", "comments": "37th International Conference on Machine Learning, Vienna, Austria,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing shapes as level sets of neural networks has been recently proved\nto be useful for different shape analysis and reconstruction tasks. So far,\nsuch representations were computed using either: (i) pre-computed implicit\nshape representations; or (ii) loss functions explicitly defined over the\nneural level sets. In this paper we offer a new paradigm for computing high\nfidelity implicit neural representations directly from raw data (i.e., point\nclouds, with or without normal information). We observe that a rather simple\nloss function, encouraging the neural network to vanish on the input point\ncloud and to have a unit norm gradient, possesses an implicit geometric\nregularization property that favors smooth and natural zero level set surfaces,\navoiding bad zero-loss solutions. We provide a theoretical analysis of this\nproperty for the linear case, and show that, in practice, our method leads to\nstate of the art implicit neural representations with higher level-of-details\nand fidelity compared to previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:36:32 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 12:32:45 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Gropp", "Amos", ""], ["Yariv", "Lior", ""], ["Haim", "Niv", ""], ["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "2002.10137", "submitter": "Ran Yi", "authors": "Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, Yong-Jin Liu", "title": "Audio-driven Talking Face Video Generation with Learning-based\n  Personalized Head Pose", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world talking faces often accompany with natural head movement. However,\nmost existing talking face video generation methods only consider facial\nanimation with fixed head pose. In this paper, we address this problem by\nproposing a deep neural network model that takes an audio signal A of a source\nperson and a very short video V of a target person as input, and outputs a\nsynthesized high-quality talking face video with personalized head pose (making\nuse of the visual information in V), expression and lip synchronization (by\nconsidering both A and V). The most challenging issue in our work is that\nnatural poses often cause in-plane and out-of-plane head rotations, which makes\nsynthesized talking face video far from realistic. To address this challenge,\nwe reconstruct 3D face animation and re-render it into synthesized frames. To\nfine tune these frames into realistic ones with smooth background transition,\nwe propose a novel memory-augmented GAN module. By first training a general\nmapping based on a publicly available dataset and fine-tuning the mapping using\nthe input short video of target person, we develop an effective strategy that\nonly requires a small number of frames (about 300 frames) to learn personalized\ntalking behavior including head pose. Extensive experiments and two user\nstudies show that our method can generate high-quality (i.e., personalized head\nmovements, expressions and good lip synchronization) talking face videos, which\nare naturally looking with more distinguishing head movement effects than the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:02:10 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:06:22 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Yi", "Ran", ""], ["Ye", "Zipeng", ""], ["Zhang", "Juyong", ""], ["Bao", "Hujun", ""], ["Liu", "Yong-Jin", ""]]}, {"id": "2002.10880", "submitter": "Charlie Nash", "authors": "Charlie Nash, Yaroslav Ganin, S. M. Ali Eslami, Peter W. Battaglia", "title": "PolyGen: An Autoregressive Generative Model of 3D Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygon meshes are an efficient representation of 3D geometry, and are of\ncentral importance in computer graphics, robotics and games development.\nExisting learning-based approaches have avoided the challenges of working with\n3D meshes, instead using alternative object representations that are more\ncompatible with neural architectures and training approaches. We present an\napproach which models the mesh directly, predicting mesh vertices and faces\nsequentially using a Transformer-based architecture. Our model can condition on\na range of inputs, including object classes, voxels, and images, and because\nthe model is probabilistic it can produce samples that capture uncertainty in\nambiguous scenarios. We show that the model is capable of producing\nhigh-quality, usable meshes, and establish log-likelihood benchmarks for the\nmesh-modelling task. We also evaluate the conditional models on surface\nreconstruction metrics against alternative methods, and demonstrate competitive\nperformance despite not training directly on this task.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 17:16:34 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Nash", "Charlie", ""], ["Ganin", "Yaroslav", ""], ["Eslami", "S. M. Ali", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "2002.10945", "submitter": "Ignacio Garcia Dorado", "authors": "Ignacio Garcia-Dorado, Pascal Getreuer, Bartlomiej Wronski, Peyman\n  Milanfar", "title": "Image Stylization: From Predefined to Personalized", "comments": "14 pages, 22 figures. arXiv admin note: text overlap with\n  arXiv:1712.06654", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a framework for interactive design of new image stylizations using\na wide range of predefined filter blocks. Both novel and off-the-shelf image\nfiltering and rendering techniques are extended and combined to allow the user\nto unleash their creativity to intuitively invent, modify, and tune new styles\nfrom a given set of filters. In parallel to this manual design, we propose a\nnovel procedural approach that automatically assembles sequences of filters,\nleading to unique and novel styles. An important aim of our framework is to\nallow for interactive exploration and design, as well as to enable videos and\ncamera streams to be stylized on the fly. In order to achieve this real-time\nperformance, we use the \\textit{Best Linear Adaptive Enhancement} (BLADE)\nframework -- an interpretable shallow machine learning method that simulates\ncomplex filter blocks in real time. Our representative results include over a\ndozen styles designed using our interactive tool, a set of styles created\nprocedurally, and new filters trained with our BLADE approach.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 06:48:28 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Garcia-Dorado", "Ignacio", ""], ["Getreuer", "Pascal", ""], ["Wronski", "Bartlomiej", ""], ["Milanfar", "Peyman", ""]]}, {"id": "2002.11109", "submitter": "P\\'eter Salvi", "authors": "P\\'eter Salvi", "title": "$G^1$ hole filling with S-patches made easy", "comments": null, "journal-ref": "Proceedings of the 12th Conference of the Hungarian Association\n  for Image Processing and Pattern Recognition, #1, 2019", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  S-patches have been around for 30 years, but they are seldom used, and are\nconsidered more of a mathematical curiosity than a practical surface\nrepresentation. In this article a method is presented for automatically\ncreating S-patches of any degree or any number of sides, suitable for inclusion\nin a curve network with tangential continuity to the adjacent surfaces. The\npresentation aims at making the implementation straightforward; a few examples\nconclude the paper.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:29:25 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Salvi", "P\u00e9ter", ""]]}, {"id": "2002.11111", "submitter": "P\\'eter Salvi", "authors": "P\\'eter Salvi", "title": "On the CAD-compatible conversion of S-patches", "comments": null, "journal-ref": "Proceedings of the Workshop on the Advances of Information\n  Technology, pp. 72-76, 2019", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  S-patches have many nice mathematical properties. It is known since their\nfirst appearance, that any regular S-patch can be exactly converted into a\ntrimmed rational B\\'ezier surface. This is a big advantage compared to other\nmulti-sided surface representations that have to be approximated for exporting\nthem into CAD/CAM systems. The actual conversion process, however, remained at\na theoretical level, with bits and pieces scattered in multiple publications.\nIn this paper we review the entirety of the algorithm, and investigate it from\na practical aspect.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 15:09:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Salvi", "P\u00e9ter", ""]]}, {"id": "2002.11212", "submitter": "P\\'eter Salvi", "authors": "P\\'eter Salvi, Istv\\'an Kov\\'acs and Tam\\'as V\\'arady", "title": "Computationally efficient transfinite patches with fullness control", "comments": null, "journal-ref": "Proceedings of the Workshop on the Advances of Information\n  Technology, pp. 96-100, 2017", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfinite patches provide a simple and elegant solution to the problem of\nrepresenting non-four-sided continuous surfaces, which are useful in a variety\nof applications, such as curve network based design. Real-time responsiveness\nis essential in this context, and thus reducing the computation cost is an\nimportant concern. The Midpoint Coons (MC) patch presented in this paper is a\nfusion of two previous transfinite schemes, combining the speed of one with the\nsuperior control mechanism of the other. This is achieved using a new\nconstrained parameterization based on generalized barycentric coordinates and\ntransfinite blending functions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 22:55:41 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Salvi", "P\u00e9ter", ""], ["Kov\u00e1cs", "Istv\u00e1n", ""], ["V\u00e1rady", "Tam\u00e1s", ""]]}, {"id": "2002.11347", "submitter": "P\\'eter Salvi", "authors": "P\\'eter Salvi", "title": "A multi-sided generalization of the $C^0$ Coons patch", "comments": null, "journal-ref": "Proceedings of the Workshop on the Advances of Information\n  Technology, pp. 110-111, 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most multi-sided transfinite surfaces require cross-derivatives at the\nboundaries. Here we show a general $n$-sided patch that interpolates all\nboundaries based on only positional information. The surface is a weighted sum\nof $n$ Coons patches, using a parameterization based on Wachspress coordinates.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 08:14:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Salvi", "P\u00e9ter", ""]]}, {"id": "2002.11812", "submitter": "Qingyuan Zheng", "authors": "Qingyuan Zheng, Zhuoru Li and Adam Bargteil", "title": "Learning to Shadow Hand-drawn Sketches", "comments": "To appear in CVPR 2020 (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic method to generate detailed and accurate\nartistic shadows from pairs of line drawing sketches and lighting directions.\nWe also contribute a new dataset of one thousand examples of pairs of line\ndrawings and shadows that are tagged with lighting directions. Remarkably, the\ngenerated shadows quickly communicate the underlying 3D structure of the\nsketched scene. Consequently, the shadows generated by our approach can be used\ndirectly or as an excellent starting point for artists. We demonstrate that the\ndeep learning network we propose takes a hand-drawn sketch, builds a 3D model\nin latent space, and renders the resulting shadows. The generated shadows\nrespect the hand-drawn lines and underlying 3D space and contain sophisticated\nand accurate details, such as self-shadowing effects. Moreover, the generated\nshadows contain artistic effects, such as rim lighting or halos appearing from\nback lighting, that would be achievable with traditional 3D rendering methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:57:17 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 23:12:21 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zheng", "Qingyuan", ""], ["Li", "Zhuoru", ""], ["Bargteil", "Adam", ""]]}, {"id": "2002.12106", "submitter": "Avinash Paliwal", "authors": "Avinash Paliwal and Nima Khademi Kalantari", "title": "Deep Slow Motion Video Reconstruction with Hybrid Imaging System", "comments": "IEEE TPAMI and ICCP 2020. Project page containing code and video at\n  http://faculty.cs.tamu.edu/nimak/Papers/ICCP2020_Slomo", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2987316", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow motion videos are becoming increasingly popular, but capturing\nhigh-resolution videos at extremely high frame rates requires professional\nhigh-speed cameras. To mitigate this problem, current techniques increase the\nframe rate of standard videos through frame interpolation by assuming linear\nobject motion which is not valid in challenging cases. In this paper, we\naddress this problem using two video streams as input; an auxiliary video with\nhigh frame rate and low spatial resolution, providing temporal information, in\naddition to the standard main video with low frame rate and high spatial\nresolution. We propose a two-stage deep learning system consisting of alignment\nand appearance estimation that reconstructs high resolution slow motion video\nfrom the hybrid video input. For alignment, we propose to compute flows between\nthe missing frame and two existing frames of the main video by utilizing the\ncontent of the auxiliary video frames. For appearance estimation, we propose to\ncombine the warped and auxiliary frames using a context and occlusion aware\nnetwork. We train our model on synthetically generated hybrid videos and show\nhigh-quality results on a variety of test scenes. To demonstrate practicality,\nwe show the performance of our system on two real dual camera setups with small\nbaseline.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:18:12 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 11:05:44 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Paliwal", "Avinash", ""], ["Kalantari", "Nima Khademi", ""]]}, {"id": "2002.12228", "submitter": "James Rondinelli", "authors": "M. J. Waters, J. M. Walker, C. T. Nelson, D. Joester, and J. M.\n  Rondinelli", "title": "Exploiting Colorimetry for Fidelity in Data Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cond-mat.mtrl-sci cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in multimodal characterization methods fuel a generation of\nincreasing immense hyper-dimensional datasets. Color mapping is employed for\nconveying higher dimensional data in two-dimensional (2D) representations for\nhuman consumption without relying on multiple projections. How one constructs\nthese color maps, however, critically affects how accurately one perceives\ndata. For simple scalar fields, perceptually uniform color maps and color\nselection have been shown to improve data readability and interpretation across\nresearch fields. Here we review core concepts underlying the design of\nperceptually uniform color map and extend the concepts from scalar fields to\ntwo-dimensional vector fields and three-component composition fields frequently\nfound in materials-chemistry research to enable high-fidelity visualization. We\ndevelop the software tools PAPUC and CMPUC to enable researchers to utilize\nthese colorimetry principles and employ perceptually uniform color spaces for\nrigorously meaningful color mapping of higher dimensional data representations.\nLast, we demonstrate how these approaches deliver immediate improvements in\ndata readability and interpretation in microscopies and spectroscopies\nroutinely used in discerning materials structure, chemistry, and properties.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:17:23 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Waters", "M. J.", ""], ["Walker", "J. M.", ""], ["Nelson", "C. T.", ""], ["Joester", "D.", ""], ["Rondinelli", "J. M.", ""]]}, {"id": "2002.12623", "submitter": "Florian Bernard", "authors": "Florian Bernard, Zeeshan Khan Suri, Christian Theobalt", "title": "MINA: Convex Mixed-Integer Programming for Non-Rigid Shape Alignment", "comments": "to appear at CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convex mixed-integer programming formulation for non-rigid shape\nmatching. To this end, we propose a novel shape deformation model based on an\nefficient low-dimensional discrete model, so that finding a globally optimal\nsolution is tractable in (most) practical cases. Our approach combines several\nfavourable properties: it is independent of the initialisation, it is much more\nefficient to solve to global optimality compared to analogous quadratic\nassignment problem formulations, and it is highly flexible in terms of the\nvariants of matching problems it can handle. Experimentally we demonstrate that\nour approach outperforms existing methods for sparse shape matching, that it\ncan be used for initialising dense shape matching methods, and we showcase its\nflexibility on several examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:54:06 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Bernard", "Florian", ""], ["Suri", "Zeeshan Khan", ""], ["Theobalt", "Christian", ""]]}]