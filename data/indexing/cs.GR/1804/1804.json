[{"id": "1804.00863", "submitter": "Maxim Maximov", "authors": "Maxim Maximov, Laura Leal-Taix\\'e, Mario Fritz, Tobias Ritschel", "title": "Deep Appearance Maps", "comments": null, "journal-ref": "ICCV 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep representation of appearance, i. e., the relation of color,\nsurface orientation, viewer position, material and illumination. Previous\napproaches have useddeep learning to extract classic appearance\nrepresentationsrelating to reflectance model parameters (e. g., Phong)\norillumination (e. g., HDR environment maps). We suggest todirectly represent\nappearance itself as a network we call aDeep Appearance Map (DAM). This is a 4D\ngeneralizationover 2D reflectance maps, which held the view direction fixed.\nFirst, we show how a DAM can be learned from images or video frames and later\nbe used to synthesize appearance, given new surface orientations and viewer\npositions. Second, we demonstrate how another network can be used to map from\nan image or video frames to a DAM network to reproduce this appearance, without\nusing a lengthy optimization such as stochastic gradient descent\n(learning-to-learn). Finally, we show the example of an appearance\nestimation-and-segmentation task, mapping from an image showingmultiple\nmaterials to multiple deep appearance maps.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 08:17:38 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 13:15:36 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 06:31:33 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Maximov", "Maxim", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Fritz", "Mario", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1804.01225", "submitter": "Jianchao Tan", "authors": "Jianchao Tan, Jose Echevarria, Yotam Gingold", "title": "Palette-based image decomposition, harmonization, and color transfer", "comments": "17 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a palette-based framework for color composition for visual\napplications. Color composition is a critical aspect of visual applications in\nart, design, and visualization. The color wheel is often used to explain\npleasing color combinations in geometric terms, and, in digital design, to\nprovide a user interface to visualize and manipulate colors. We abstract\nrelationships between palette colors as a compact set of axes describing\nharmonic templates over perceptually uniform color wheels. Our framework\nprovides a basis for a variety of color-aware image operations, such as color\nharmonization and color transfer, and can be applied to videos. To enable our\napproach, we introduce an extremely scalable and efficient yet simple\npalette-based image decomposition algorithm. Our approach is based on the\ngeometry of images in RGBXY-space. This new geometric approach is orders of\nmagnitude more efficient than previous work and requires no numerical\noptimization. We demonstrate a real-time layer decomposition tool. After\npreprocessing, our algorithm can decompose 6 MP images into layers in 20\nmilliseconds. We also conducted three large-scale, wide-ranging perceptual\nstudies on the perception of harmonic colors and harmonization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 03:22:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 03:51:26 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 22:34:31 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Tan", "Jianchao", ""], ["Echevarria", "Jose", ""], ["Gingold", "Yotam", ""]]}, {"id": "1804.01253", "submitter": "Yoichi Ochiai Prof.", "authors": "Yoichi Ochiai", "title": "How could we ignore the lens and pupils of eyeballs: Metamaterial optics\n  for retinal projection", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retinal projection is required for xR applications that can deliver immersive\nvisual experience throughout the day. If general-purpose retinal projection\nmethods can be realized at a low cost, not only could the image be displayed on\nthe retina using less energy, but there is also the possibility of cutting off\nthe weight of projection unit itself from the AR goggles. Several retinal\nprojection methods have been previously proposed; however, as the lenses and\niris of the eyeball are in front of the retina, which is a limitation of the\neyeball, the proposal of retinal projection is generally fraught with narrow\nviewing angles and small eyebox problems. In this short technical report, we\nintroduce ideas and samples of an optical system for solving the common\nproblems of retinal projection by using the metamaterial mirror (plane\nsymmetric transfer optical system). Using this projection method, the designing\nof retinal projection can becomes easy, and if appropriate optics are\navailable, it would be possible to construct an optical system that allows the\nquick follow-up of retinal projection hardware.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 06:35:09 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 03:13:00 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Ochiai", "Yoichi", ""]]}, {"id": "1804.02684", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Ronnachai Jaroensri, Changil Kim, Mohamed Elgharib,\n  Fr\\'edo Durand, William T. Freeman, Wojciech Matusik", "title": "Learning-based Video Motion Magnification", "comments": "Accepted as ECCV 2018 Oral. The 1st and 2nd authors equally\n  contributed. Video result: https://youtu.be/GrMLeEcSNzY , Project page:\n  http://people.csail.mit.edu/tiam/deepmag/ Some bibliography information was\n  fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video motion magnification techniques allow us to see small motions\npreviously invisible to the naked eyes, such as those of vibrating airplane\nwings, or swaying buildings under the influence of the wind. Because the motion\nis small, the magnification results are prone to noise or excessive blurring.\nThe state of the art relies on hand-designed filters to extract representations\nthat may not be optimal. In this paper, we seek to learn the filters directly\nfrom examples using deep convolutional neural networks. To make training\ntractable, we carefully design a synthetic dataset that captures small motion\nwell, and use two-frame input for training. We show that the learned filters\nachieve high-quality results on real videos, with less ringing artifacts and\nbetter noise characteristics than previous methods. While our model is not\ntrained with temporal filters, we found that the temporal filters can be used\nwith our extracted representations up to a moderate magnification, enabling a\nfrequency-based motion selection. Finally, we analyze the learned filters and\nshow that they behave similarly to the derivative filters used in previous\nworks. Our code, trained model, and datasets will be available online.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 12:57:23 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 17:45:57 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 03:26:46 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Jaroensri", "Ronnachai", ""], ["Kim", "Changil", ""], ["Elgharib", "Mohamed", ""], ["Durand", "Fr\u00e9do", ""], ["Freeman", "William T.", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1804.02717", "submitter": "Xue Bin Peng", "authors": "Xue Bin Peng, Pieter Abbeel, Sergey Levine, Michiel van de Panne", "title": "DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based\n  Character Skills", "comments": null, "journal-ref": null, "doi": "10.1145/3197517.3201311", "report-no": null, "categories": "cs.GR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A longstanding goal in character animation is to combine data-driven\nspecification of behavior with a system that can execute a similar behavior in\na physical simulation, thus enabling realistic responses to perturbations and\nenvironmental variation. We show that well-known reinforcement learning (RL)\nmethods can be adapted to learn robust control policies capable of imitating a\nbroad range of example motion clips, while also learning complex recoveries,\nadapting to changes in morphology, and accomplishing user-specified goals. Our\nmethod handles keyframed motions, highly-dynamic actions such as\nmotion-captured flips and spins, and retargeted motions. By combining a\nmotion-imitation objective with a task objective, we can train characters that\nreact intelligently in interactive settings, e.g., by walking in a desired\ndirection or throwing a ball at a user-specified target. This approach thus\ncombines the convenience and motion quality of using motion clips to define the\ndesired style and appearance, with the flexibility and generality afforded by\nRL methods and physics-based animation. We further explore a number of methods\nfor integrating multiple clips into the learning process to develop\nmulti-skilled agents capable of performing a rich repertoire of diverse skills.\nWe demonstrate results using multiple characters (human, Atlas robot, bipedal\ndinosaur, dragon) and a large variety of skills, including locomotion,\nacrobatics, and martial arts.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 17:04:58 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 20:48:52 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 03:44:10 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Peng", "Xue Bin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["van de Panne", "Michiel", ""]]}, {"id": "1804.03189", "submitter": "Fujun Luan", "authors": "Fujun Luan, Sylvain Paris, Eli Shechtman, Kavita Bala", "title": "Deep Painterly Harmonization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copying an element from a photo and pasting it into a painting is a\nchallenging task. Applying photo compositing techniques in this context yields\nsubpar results that look like a collage --- and existing painterly stylization\nalgorithms, which are global, perform poorly when applied locally. We address\nthese issues with a dedicated algorithm that carefully determines the local\nstatistics to be transferred. We ensure both spatial and inter-scale\nstatistical consistency and demonstrate that both aspects are key to generating\nquality results. To cope with the diversity of abstraction levels and types of\npaintings, we introduce a technique to adjust the parameters of the transfer\ndepending on the painting. We show that our algorithm produces significantly\nbetter results than photo compositing or global stylization techniques and that\nit enables creative painterly edits that would be otherwise difficult to\nachieve.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:09:27 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 05:03:02 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 18:11:29 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 20:15:53 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Luan", "Fujun", ""], ["Paris", "Sylvain", ""], ["Shechtman", "Eli", ""], ["Bala", "Kavita", ""]]}, {"id": "1804.03245", "submitter": "Teseo Schneider", "authors": "Teseo Schneider, Jeremie Dumas, Xifeng Gao, Mario Botsch, Daniele\n  Panozzo, Denis Zorin", "title": "Poly-Spline Finite Element Method", "comments": "FORTHCOMING in TOG", "journal-ref": null, "doi": "10.1145/3313797", "report-no": null, "categories": "cs.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an integrated meshing and finite element method pipeline\nenabling black-box solution of partial differential equations in the volume\nenclosed by a boundary representation. We construct a hybrid\nhexahedral-dominant mesh, which contains a small number of star-shaped\npolyhedra, and build a set of high-order basis on its elements, combining\ntriquadratic B-splines, triquadratic hexahedra (27 degrees of freedom), and\nharmonic elements. We demonstrate that our approach converges cubically under\nrefinement, while requiring around 50% of the degrees of freedom than a\nsimilarly dense hexahedral mesh composed of triquadratic hexahedra. We validate\nour approach solving Poisson's equation on a large collection of models, which\nare automatically processed by our algorithm, only requiring the user to\nprovide boundary conditions on their surface.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 21:27:01 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 20:14:03 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Schneider", "Teseo", ""], ["Dumas", "Jeremie", ""], ["Gao", "Xifeng", ""], ["Botsch", "Mario", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""]]}, {"id": "1804.03407", "submitter": "Manish Sreenivasa", "authors": "Manish Sreenivasa and Monika Harant", "title": "ModelFactory: A Matlab/Octave based toolbox to create human body models", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.1137656", "report-no": null, "categories": "cs.RO cs.GR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Model-based analysis of movements can help better understand\nhuman motor control. Here, the models represent the human body as an\narticulated multi-body system that reflects the characteristics of the human\nbeing studied.\n  Results: We present an open-source toolbox that allows for the creation of\nhuman models with easy-to-setup, customizable configurations. The toolbox\nscripts are written in Matlab/Octave and provide a command-based interface as\nwell as a graphical interface to construct, visualize and export models.\nBuilt-in software modules provide functionalities such as automatic scaling of\nmodels based on subject height and weight, custom scaling of segment lengths,\nmass and inertia, addition of body landmarks, and addition of motion capture\nmarkers. Users can set up custom definitions of joints, segments and other body\nproperties using the many included examples as templates. In addition to the\nhuman, any number of objects (e.g. exoskeletons, orthoses, prostheses, boxes)\ncan be added to the modeling environment.\n  Conclusions: The ModelFactory toolbox is published as open-source software\nunder the permissive zLib license. The toolbox fulfills an important function\nby making it easier to create human models, and should be of interest to human\nmovement researchers.\n  This document is the author's version of this article.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 09:07:41 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 03:58:54 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Sreenivasa", "Manish", ""], ["Harant", "Monika", ""]]}, {"id": "1804.03447", "submitter": "Ryota Natsume", "authors": "Ryota Natsume, Tatsuya Yatagawa, Shigeo Morishima", "title": "RSGAN: Face Swapping and Editing using Face and Hair Representation in\n  Latent Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an integrated system for automatically generating\nand editing face images through face swapping, attribute-based editing, and\nrandom face parts synthesis. The proposed system is based on a deep neural\nnetwork that variationally learns the face and hair regions with large-scale\nface image datasets. Different from conventional variational methods, the\nproposed network represents the latent spaces individually for faces and hairs.\nWe refer to the proposed network as region-separative generative adversarial\nnetwork (RSGAN). The proposed network independently handles face and hair\nappearances in the latent spaces, and then, face swapping is achieved by\nreplacing the latent-space representations of the faces, and reconstruct the\nentire face image with them. This approach in the latent space robustly\nperforms face swapping even for images which the previous methods result in\nfailure due to inappropriate fitting or the 3D morphable models. In addition,\nthe proposed system can further edit face-swapped images with the same network\nby manipulating visual attributes or by composing them with randomly generated\nface or hair parts.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:54:34 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 06:44:06 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Natsume", "Ryota", ""], ["Yatagawa", "Tatsuya", ""], ["Morishima", "Shigeo", ""]]}, {"id": "1804.03977", "submitter": "Silvia Biasotti", "authors": "Elia Moscoso Thompson and Silvia Biasotti", "title": "Edge-based LBP description of surfaces with colorimetric patterns", "comments": "Eurographics Workshop on 3D Object Retrieval 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we target the problem of the retrieval of colour patterns over\nsurfaces. We generalize to surface tessellations the well known Local Binary\nPattern (LBP) descriptor for images. The key concept of the LBP is to code the\nvariability of the colour values around each pixel. In the case of a surface\ntessellation we adopt rings around vertices that are obtained with a\nsphere-mesh intersection driven by the edges of the mesh; for this reason, we\nname our method edgeLBP. Experimental results are provided to show how this\ndescription performs well for pattern retrieval, also when patterns come from\ndegraded and corrupted archaeological fragments.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:34:31 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Thompson", "Elia Moscoso", ""], ["Biasotti", "Silvia", ""]]}, {"id": "1804.03979", "submitter": "Silvia Biasotti", "authors": "Silvia Biasotti and Elia Moscoso Thompson and Michela Spagnuolo", "title": "Experimental similarity assessment for a collection of fragmented\n  artifacts", "comments": "Eurographics Workshop on 3D Object Retrieval 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Visual Heritage domain, search engines are expected to support\narchaeologists and curators to address cross-correlation and searching across\nmultiple collections. Archaeological excavations return artifacts that often\nare damaged with parts that are fragmented in more pieces or totally missing.\nThe notion of similarity among fragments cannot simply base on the geometric\nshape but style, material, color, decorations, etc. are all important factors\nthat concur to this concept. In this work, we discuss to which extent the\nexisting techniques for 3D similarity matching are able to approach fragment\nsimilarity, what is missing and what is necessary to be further developed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:42:55 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Biasotti", "Silvia", ""], ["Thompson", "Elia Moscoso", ""], ["Spagnuolo", "Michela", ""]]}, {"id": "1804.04001", "submitter": "Stephane Guinard", "authors": "Stephane Guinard and Bruno Vallet", "title": "Weighted simplicial complex reconstruction from mobile laser scanning\n  using sensor topology", "comments": "8 pages, 11 figures, CFPT 2018. arXiv admin note: substantial text\n  overlap with arXiv:1802.07487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for the reconstruction of simplicial complexes\n(combining points, edges and triangles) from 3D point clouds from Mobile Laser\nScanning (MLS). Our method uses the inherent topology of the MLS sensor to\ndefine a spatial adjacency relationship between points. We then investigate\neach possible connexion between adjacent points, weighted according to its\ndistance to the sensor, and filter them by searching collinear structures in\nthe scene, or structures perpendicular to the laser beams. Next, we create and\nfilter triangles for each triplet of self-connected edges and according to\ntheir local planarity. We compare our results to an unweighted simplicial\ncomplex reconstruction.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 10:07:27 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 15:06:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Guinard", "Stephane", ""], ["Vallet", "Bruno", ""]]}, {"id": "1804.04013", "submitter": "Oliver Glauser", "authors": "Oliver Glauser, Daniele Panozzo, Otmar Hilliges, Olga Sorkine-Hornung", "title": "Deformation Capture via Soft and Stretchable Sensor Arrays", "comments": null, "journal-ref": "ACM Transactions on Graphics (TOG) , Volume 38 Issue 2, March\n  2019, Article No. 16", "doi": "10.1145/3311972", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hardware and software pipeline to fabricate flexible wearable\nsensors and use them to capture deformations without line of sight. Our first\ncontribution is a low-cost fabrication pipeline to embed multiple aligned\nconductive layers with complex geometries into silicone compounds. Overlapping\nconductive areas from separate layers form local capacitors that measure dense\narea changes. Contrary to existing fabrication methods, the proposed technique\nonly requires hardware that is readily available in modern fablabs. While area\nmeasurements alone are not enough to reconstruct the full 3D deformation of a\nsurface, they become sufficient when paired with a data-driven prior. A novel\nsemi-automatic tracking algorithm, based on an elastic surface geometry\ndeformation, allows to capture ground-truth data with an optical mocap system,\neven under heavy occlusions or partially unobservable markers. The resulting\ndataset is used to train a regressor based on deep neural networks, directly\nmapping the area readings to global positions of surface vertices. We\ndemonstrate the flexibility and accuracy of the proposed hardware and software\nin a series of controlled experiments, and design a prototype of wearable\nwrist, elbow and biceps sensors, which do not require line-of-sight and can be\nworn below regular clothing.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:22:15 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 16:54:13 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 15:24:07 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Glauser", "Oliver", ""], ["Panozzo", "Daniele", ""], ["Hilliges", "Otmar", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1804.04111", "submitter": "Yongbin Sun", "authors": "Jonathan Dyssel Stets, Yongbin Sun, Wiley Corning, Scott Greenwald", "title": "Visualization and Labeling of Point Clouds in Virtual Reality", "comments": "2 pages, 3 figures", "journal-ref": "SA '17 SIGGRAPH Asia 2017 Posters Article No. 31", "doi": "10.1145/3145690.3145729", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Virtual Reality (VR) application for labeling and handling point\ncloud data sets. A series of room-scale point clouds are recorded as a video\nsequence using a Microsoft Kinect. The data can be played and paused, and\nframes can be skipped just like in a video player. The user can walk around and\ninspect the data while it is playing or paused. Using the tracked hand-held\ncontroller, the user can select and label individual parts of the point cloud.\nThe points are highlighted with a color when they are labeled. With a tracking\nalgorithm, the labeled points can be tracked from frame to frame to ease the\nlabeling process. Our sample data is an RGB point cloud recording of two people\njuggling with pins. Here, the user can select and label, for example, the\njuggler pins as shown in Figure 1. Each juggler pin is labeled with various\ncolors to indicate di erent labels.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:35:26 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Stets", "Jonathan Dyssel", ""], ["Sun", "Yongbin", ""], ["Corning", "Wiley", ""], ["Greenwald", "Scott", ""]]}, {"id": "1804.04371", "submitter": "Yibing Song", "authors": "Xin Yang, Ke Xu, Yibing Song, Qiang Zhang, Xiaopeng Wei, Rynson Lau", "title": "Image Correction via Deep Reciprocating HDR Transformation", "comments": "in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image correction aims to adjust an input image into a visually pleasing one.\nExisting approaches are proposed mainly from the perspective of image pixel\nmanipulation. They are not effective to recover the details in the under/over\nexposed regions. In this paper, we revisit the image formation procedure and\nnotice that the missing details in these regions exist in the corresponding\nhigh dynamic range (HDR) data. These details are well perceived by the human\neyes but diminished in the low dynamic range (LDR) domain because of the tone\nmapping process. Therefore, we formulate the image correction task as an HDR\ntransformation process and propose a novel approach called Deep Reciprocating\nHDR Transformation (DRHT). Given an input LDR image, we first reconstruct the\nmissing details in the HDR domain. We then perform tone mapping on the\npredicted HDR data to generate the output LDR image with the recovered details.\nTo this end, we propose a united framework consisting of two CNNs for HDR\nreconstruction and tone mapping. They are integrated end-to-end for joint\ntraining and prediction. Experiments on the standard benchmarks demonstrate\nthat the proposed method performs favorably against state-of-the-art image\ncorrection methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 08:23:04 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Yang", "Xin", ""], ["Xu", "Ke", ""], ["Song", "Yibing", ""], ["Zhang", "Qiang", ""], ["Wei", "Xiaopeng", ""], ["Lau", "Rynson", ""]]}, {"id": "1804.04619", "submitter": "Seungjae Lee", "authors": "Seungjae Lee, Youngjin Jo, Dongheon Yoo, Jaebum Cho, Dukho Lee, and\n  Byoungho Lee", "title": "TomoReal: Tomographic Displays", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the history of display technologies began, people have dreamed an\nultimate 3D display system. In order to get close to the dream, 3D displays\nshould provide both of psychological and physiological cues for recognition of\ndepth information. However, it is challenging to satisfy the essential features\nwithout sacrifice in conventional technical values including resolution, frame\nrate, and eye-box. Here, we present a new type of 3D displays: tomographic\ndisplays. We claim that tomographic displays may support extremely wide depth\nof field, quasi-continuous accommodation, omni-directional motion parallax,\npreserved resolution, full frame, and moderate field of view within enough\neye-box. Tomographic displays consist of focus-tunable optics, 2D display\npanel, and fast spatially adjustable backlight. The synchronization of the\nfocus-tunable optics and the backlight enables the 2D display panel to express\nthe depth information. Tomographic displays have various applications including\ntabletop 3D displays, head-up displays, and near-eye stereoscopes. In this\nstudy, we implement a near-eye display named TomoReal, which is one of the most\npromising application of tomographic displays. We conclude with the detailed\nanalysis and thorough discussion for tomographic displays, which would open a\nnew research field.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 05:46:27 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Lee", "Seungjae", ""], ["Jo", "Youngjin", ""], ["Yoo", "Dongheon", ""], ["Cho", "Jaebum", ""], ["Lee", "Dukho", ""], ["Lee", "Byoungho", ""]]}, {"id": "1804.05261", "submitter": "Garoe Dorta", "authors": "Garoe Dorta, Luca Benedetti, Dmitry Kit, Yong-Liang Yang", "title": "Physics-driven Fire Modeling from Multi-view Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fire effects are widely used in various computer graphics applications such\nas visual effects and video games. Modeling the shape and appearance of fire\nphenomenon is challenging as the underlying effects are driven by complex laws\nof physics. State-of-the-art fire modeling techniques rely on sophisticated\nphysical simulations which require intensive parameter tuning, or use\nsimplifications which produce physically invalid results. In this paper, we\npresent a novel method of reconstructing physically valid fire models from\nmulti-view stereo images. Our method, for the first time, provides plausible\nestimation of physical properties (e.g., temperature, density) of a fire volume\nusing RGB cameras. This allows for a number of novel phenomena such as global\nfire illumination effects. The effectiveness and usefulness of our method are\ntested by generating fire models from a variety of input data, and applying the\nreconstructed fire models for realistic illumination of virtual scenes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 18:28:51 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Dorta", "Garoe", ""], ["Benedetti", "Luca", ""], ["Kit", "Dmitry", ""], ["Yang", "Yong-Liang", ""]]}, {"id": "1804.05541", "submitter": "Peng Gao", "authors": "Yan Zhang, Peng Gao, Xiao-Qing Li", "title": "A Novel Parallel Ray-Casting Algorithm", "comments": "Accepted by ICCWAMTIP 2016", "journal-ref": null, "doi": "10.1109/ICCWAMTIP.2016.8079804", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ray-Casting algorithm is an important method for fast real-time surface\ndisplay from 3D medical images. Based on the Ray-Casting algorithm, a novel\nparallel Ray-Casting algorithm is proposed in this paper. A novel operation is\nintroduced and defined as a star operation, and star operations can be computed\nin parallel in the proposed algorithm compared with the serial chain of star\noperations in the Ray-Casting algorithm. The computation complexity of the\nproposed algorithm is reduced from $O(n)$ to $O(\\log^n_2)$.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 08:15:38 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 13:40:00 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhang", "Yan", ""], ["Gao", "Peng", ""], ["Li", "Xiao-Qing", ""]]}, {"id": "1804.06092", "submitter": "Zhongping Ji", "authors": "Zhongping Ji, Xianfang Sun, Weiyin Ma", "title": "Normal Image Manipulation for Bas-relief Generation with Hybrid Styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a normal-based bas-relief generation and stylization method\nwhich is motivated by the recent advancement in this topic. Creating bas-relief\nfrom normal images has successfully facilitated bas-relief modeling in image\nspace. However, the use of normal images in previous work is often restricted\nto certain type of operations only. This paper is intended to extend\nnormal-based methods and construct bas-reliefs from normal images in a\nversatile way. Our method can not only generate a new normal image by combining\nvarious frequencies of existing normal images and details transferring, but\nalso build bas-reliefs from a single RGB image and its edge-based sketch image.\nIn addition, we introduce an auxiliary function to represent a smooth base\nsurface and generate a layered global shape. To integrate above considerations\ninto our framework, we formulate the bas- relief generation as a variational\nproblem which can be solved by a screened Poisson equation. Some advantages of\nour method are that it expands the bas-relief shape space and generates\ndiversified styles of results, and that it is capable of transferring details\nfrom one region to other regions. Our method is easy to implement, and produces\ngood-quality bas-relief models. We experiment our method on a range of normal\nimages and it compares favorably to other popular classic and state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 08:00:52 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Ji", "Zhongping", ""], ["Sun", "Xianfang", ""], ["Ma", "Weiyin", ""]]}, {"id": "1804.06579", "submitter": "Fenggen Yu", "authors": "Fenggen Yu, Yan Zhang, Kai Xu, Ali Mahdavi-Amiri, Hao Zhang", "title": "Semi-Supervised Co-Analysis of 3D Shape Styles from Projected Lines", "comments": "17 pages, 25 figures", "journal-ref": "ACM Transactions on Graphics 37(2). February 2018", "doi": "10.1145/3182158", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-supervised co-analysis method for learning 3D shape styles\nfrom projected feature lines, achieving style patch localization with only weak\nsupervision. Given a collection of 3D shapes spanning multiple object\ncategories and styles, we perform style co-analysis over projected feature\nlines of each 3D shape and then backproject the learned style features onto the\n3D shapes. Our core analysis pipeline starts with mid-level patch sampling and\npre-selection of candidate style patches. Projective features are then encoded\nvia patch convolution. Multi-view feature integration and style clustering are\ncarried out under the framework of partially shared latent factor (PSLF)\nlearning, a multi-view feature learning scheme. PSLF achieves effective\nmulti-view feature fusion by distilling and exploiting consistent and\ncomplementary feature information from multiple views, while also selecting\nstyle patches from the candidates. Our style analysis approach supports both\nunsupervised and semi-supervised analysis. For the latter, our method accepts\nboth user-specified shape labels and style-ranked triplets as clustering\nconstraints.We demonstrate results from 3D shape style analysis and patch\nlocalization as well as improvements over state-of-the-art methods. We also\npresent several applications enabled by our style analysis.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 07:23:18 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Yu", "Fenggen", ""], ["Zhang", "Yan", ""], ["Xu", "Kai", ""], ["Mahdavi-Amiri", "Ali", ""], ["Zhang", "Hao", ""]]}, {"id": "1804.06662", "submitter": "Zuzana Majdisova", "authors": "Zuzana Majdisova, Vaclav Skala", "title": "A New Radial Basis Function Approximation with Reproduction", "comments": null, "journal-ref": "Proceedings of IHCI 2016; GET 2016; and CGVCVIP 2016, Portugal,\n  pp.215-222, ISBN 978-989-8533-52-4, IADIS Press, 2016", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation of scattered geometric data is often a task in many engineering\nproblems. The Radial Basis Function (RBF) approximation is appropriate for\nlarge scattered (unordered) datasets in d-dimensional space. This method is\nuseful for a higher dimension d>=2, because the other methods require a\nconversion of a scattered dataset to a semi-regular mesh using some\ntessellation techniques, which is computationally expensive. The RBF\napproximation is non-separable, as it is based on a distance of two points. It\nleads to a solution of overdetermined Linear System of Equations (LSE). In this\npaper a new RBF approximation method is derived and presented. The presented\napproach is applicable for d dimensional cases in general.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 11:40:59 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Majdisova", "Zuzana", ""], ["Skala", "Vaclav", ""]]}, {"id": "1804.06996", "submitter": "Gaurav Bharaj", "authors": "Gaurav Bharaj, Danny Kaufman, Etienne Vouga, Hanspeter Pfister", "title": "Metamorphs: Bistable Planar Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme deformation can drastically morph a structure from one structural\nform into another. Programming such deformation properties into the structure\nis often challenging and in many cases an impossible task. The morphed forms do\nnot hold and usually relapse to the original form, where the structure is in\nits lowest energy state. For example, a stick, when bent, resists its bent form\nand tends to go back to its initial straight form, where it holds the least\namount of potential energy.\n  In this project, we present a computational design method which can create\nfabricable planar structure that can morph into two different bistable forms.\nOnce the user provides the initial desired forms, the method automatically\ncreates support structures (internal springs), such that, the structure can not\nonly morph, but also hold the respective forms under external force\napplication. We achieve this through an iterative nonlinear optimization\nstrategy for shaping the potential energy of the structure in the two forms\nsimultaneously. Our approach guarantees first and second-order stability with\nrespect to the potential energy of the bistable structure.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 05:15:03 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bharaj", "Gaurav", ""], ["Kaufman", "Danny", ""], ["Vouga", "Etienne", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1804.07006", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Ke Song, Chao Li, Fei Wang, Liyi Xiao", "title": "Large Margin Structured Convolution Operator for Thermal Infrared Object\n  Tracking", "comments": "Accepted as contributed paper in ICPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with visible object tracking, thermal infrared (TIR) object tracking\ncan track an arbitrary target in total darkness since it cannot be influenced\nby illumination variations. However, there are many unwanted attributes that\nconstrain the potentials of TIR tracking, such as the absence of visual color\npatterns and low resolutions. Recently, structured output support vector\nmachine (SOSVM) and discriminative correlation filter (DCF) have been\nsuccessfully applied to visible object tracking, respectively. Motivated by\nthese, in this paper, we propose a large margin structured convolution operator\n(LMSCO) to achieve efficient TIR object tracking. To improve the tracking\nperformance, we employ the spatial regularization and implicit interpolation to\nobtain continuous deep feature maps, including deep appearance features and\ndeep motion features, of the TIR targets. Finally, a collaborative optimization\nstrategy is exploited to significantly update the operators. Our approach not\nonly inherits the advantage of the strong discriminative capability of SOSVM\nbut also achieves accurate and robust tracking with higher-dimensional features\nand more dense samples. To the best of our knowledge, we are the first to\nincorporate the advantages of DCF and SOSVM for TIR object tracking.\nComprehensive evaluations on two thermal infrared tracking benchmarks, i.e.\nVOT-TIR2015 and VOT-TIR2016, clearly demonstrate that our LMSCO tracker\nachieves impressive results and outperforms most state-of-the-art trackers in\nterms of accuracy and robustness with sufficient frame rate.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 06:12:02 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:21:20 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Song", "Ke", ""], ["Li", "Chao", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""]]}, {"id": "1804.07459", "submitter": "Peng Gao", "authors": "Peng Gao, Yipeng Ma, Chao Li, Ke Song, Fei Wang, Liyi Xiao", "title": "A Complementary Tracking Model with Multiple Features", "comments": "Accepted by IVPAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative Correlation Filters based tracking algorithms exploiting\nconventional handcrafted features have achieved impressive results both in\nterms of accuracy and robustness. Template handcrafted features have shown\nexcellent performance, but they perform poorly when the appearance of target\nchanges rapidly such as fast motions and fast deformations. In contrast,\nstatistical handcrafted features are insensitive to fast states changes, but\nthey yield inferior performance in the scenarios of illumination variations and\nbackground clutters. In this work, to achieve an efficient tracking\nperformance, we propose a novel visual tracking algorithm, named MFCMT, based\non a complementary ensemble model with multiple features, including Histogram\nof Oriented Gradients (HOGs), Color Names (CNs) and Color Histograms (CHs).\nAdditionally, to improve tracking results and prevent targets drift, we\nintroduce an effective fusion method by exploiting relative entropy to coalesce\nall basic response maps and get an optimal response. Furthermore, we suggest a\nsimple but efficient update strategy to boost tracking performance.\nComprehensive evaluations are conducted on two tracking benchmarks demonstrate\nand the experimental results demonstrate that our method is competitive with\nnumerous state-of-the-art trackers. Our tracker achieves impressive performance\nwith faster speed on these benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 06:23:37 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 08:32:38 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 08:47:44 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Gao", "Peng", ""], ["Ma", "Yipeng", ""], ["Li", "Chao", ""], ["Song", "Ke", ""], ["Wang", "Fei", ""], ["Xiao", "Liyi", ""]]}, {"id": "1804.08197", "submitter": "Stanislav Pidhorskyi", "authors": "Stanislav Pidhorskyi, Michael Morehead, Quinn Jones, George Spirou,\n  Gianfranco Doretto", "title": "syGlass: Interactive Exploration of Multidimensional Images Using\n  Virtual Reality Head-mounted Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quest for deeper understanding of biological systems has driven the\nacquisition of increasingly larger multidimensional image datasets. Inspecting\nand manipulating data of this complexity is very challenging in traditional\nvisualization systems. We developed syGlass, a software package capable of\nvisualizing large scale volumetric data with inexpensive virtual reality\nhead-mounted display technology. This allows leveraging stereoscopic vision to\nsignificantly improve perception of complex 3D structures, and provides\nimmersive interaction with data directly in 3D. We accomplished this by\ndeveloping highly optimized data flow and volume rendering pipelines, tested on\ndatasets up to 16TB in size, as well as tools available in a virtual reality\nGUI to support advanced data exploration, annotation, and cataloguing.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 00:04:54 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 03:58:42 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 04:00:45 GMT"}, {"version": "v4", "created": "Wed, 22 Aug 2018 01:48:32 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Pidhorskyi", "Stanislav", ""], ["Morehead", "Michael", ""], ["Jones", "Quinn", ""], ["Spirou", "George", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "1804.08497", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Noa Fish, Zhenhua Wang, Raja Giryes, Shachar Fleishman\n  and Daniel Cohen-Or", "title": "ALIGNet: Partial-Shape Agnostic Alignment via Unsupervised Learning", "comments": "To be presented at SIGGRAPH Asia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of aligning a pair of shapes is a fundamental operation in\ncomputer graphics. Traditional approaches rely heavily on matching\ncorresponding points or features to guide the alignment, a paradigm that\nfalters when significant shape portions are missing. These techniques generally\ndo not incorporate prior knowledge about expected shape characteristics, which\ncan help compensate for any misleading cues left by inaccuracies exhibited in\nthe input shapes. We present an approach based on a deep neural network,\nleveraging shape datasets to learn a shape-aware prior for source-to-target\nalignment that is robust to shape incompleteness. In the absence of ground\ntruth alignments for supervision, we train a network on the task of shape\nalignment using incomplete shapes generated from full shapes for\nself-supervision. Our network, called ALIGNet, is trained to warp complete\nsource shapes to incomplete targets, as if the target shapes were complete,\nthus essentially rendering the alignment partial-shape agnostic. We aim for the\nnetwork to develop specialized expertise over the common characteristics of the\nshapes in each dataset, thereby achieving a higher-level understanding of the\nexpected shape space to which a local approach would be oblivious. We constrain\nALIGNet through an anisotropic total variation identity regularization to\npromote piecewise smooth deformation fields, facilitating both partial-shape\nagnosticism and post-deformation applications. We demonstrate that ALIGNet\nlearns to align geometrically distinct shapes, and is able to infer plausible\nmappings even when the target shape is significantly incomplete. We show that\nour network learns the common expected characteristics of shape collections,\nwithout over-fitting or memorization, enabling it to produce plausible\ndeformations on unseen data during test time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 15:17:26 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 22:26:39 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hanocka", "Rana", ""], ["Fish", "Noa", ""], ["Wang", "Zhenhua", ""], ["Giryes", "Raja", ""], ["Fleishman", "Shachar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1804.08968", "submitter": "J\\'er\\'emie Dumas", "authors": "Zhen Chen, Daniele Panozzo, Jeremie Dumas", "title": "Half-Space Power Diagrams and Discrete Surface Offsets", "comments": "15 pages, 20 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient, trivially parallelizable algorithm to compute offset\nsurfaces of shapes discretized using a dexel data structure. Our algorithm is\nbased on a two-stage sweeping procedure that is simple to implement and\nefficient, entirely avoiding volumetric distance field computations typical of\nexisting methods. Our construction is based on properties of half-space power\ndiagrams, where each seed is only visible by a half-space, which were never\nused before for the computation of surface offsets. The primary application of\nour method is interactive modeling for digital fabrication. Our technique\nenables a user to interactively process high-resolution models. It is also\nuseful in a plethora of other geometry processing tasks requiring fast,\napproximate offsets, such as topology optimization, collision detection, and\nskeleton extraction. We present experimental timings, comparisons with previous\napproaches, and provide a reference implementation in the supplemental\nmaterial.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 11:50:05 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:53:53 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chen", "Zhen", ""], ["Panozzo", "Daniele", ""], ["Dumas", "Jeremie", ""]]}, {"id": "1804.08972", "submitter": "Tiziano Portenier", "authors": "Tiziano Portenier, Qiyang Hu, Attila Szab\\'o, Siavash Arjomand\n  Bigdeli, Paolo Favaro, Matthias Zwicker", "title": "FaceShop: Deep Sketch-based Face Image Editing", "comments": "13 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel system for sketch-based face image editing, enabling users\nto edit images intuitively by sketching a few strokes on a region of interest.\nOur interface features tools to express a desired image manipulation by\nproviding both geometry and color constraints as user-drawn strokes. As an\nalternative to the direct user input, our proposed system naturally supports a\ncopy-paste mode, which allows users to edit a given image region by using parts\nof another exemplar image without the need of hand-drawn sketching at all. The\nproposed interface runs in real-time and facilitates an interactive and\niterative workflow to quickly express the intended edits. Our system is based\non a novel sketch domain and a convolutional neural network trained end-to-end\nto automatically learn to render image regions corresponding to the input\nstrokes. To achieve high quality and semantically consistent results we train\nour neural network on two simultaneous tasks, namely image completion and image\ntranslation. To the best of our knowledge, we are the first to combine these\ntwo tasks in a unified framework for interactive image editing. Our results\nshow that the proposed sketch domain, network architecture, and training\nprocedure generalize well to real user input and enable high quality synthesis\nresults without additional post-processing.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:03:45 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 13:28:54 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Portenier", "Tiziano", ""], ["Hu", "Qiyang", ""], ["Szab\u00f3", "Attila", ""], ["Bigdeli", "Siavash Arjomand", ""], ["Favaro", "Paolo", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1804.09152", "submitter": "Rhaleb Zayer", "authors": "Rhaleb Zayer, Daniel Mlakar, Markus Steinberger, Hans-Peter Seidel", "title": "Layered Fields for Natural Tessellations on Surfaces", "comments": "Natural tessellations, surface fields, Voronoi diagrams, Lloyd's\n  algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mimicking natural tessellation patterns is a fascinating multi-disciplinary\nproblem. Geometric methods aiming at reproducing such partitions on surface\nmeshes are commonly based on the Voronoi model and its variants, and are often\nfaced with challenging issues such as metric estimation, geometric, topological\ncomplications, and most critically parallelization. In this paper, we introduce\nan alternate model which may be of value for resolving these issues. We drop\nthe assumption that regions need to be separated by lines. Instead, we regard\nregion boundaries as narrow bands and we model the partition as a set of smooth\nfunctions layered over the surface. Given an initial set of seeds or regions,\nthe partition emerges as the solution of a time dependent set of partial\ndifferential equations describing concurrently evolving fronts on the surface.\nOur solution does not require geodesic estimation, elaborate numerical solvers,\nor complicated bookkeeping data structures. The cost per time-iteration is\ndominated by the multiplication and addition of two sparse matrices. Extension\nof our approach in a Lloyd's algorithm fashion can be easily achieved and the\nextraction of the dual mesh can be conveniently preformed in parallel through\nmatrix algebra. As our approach relies mainly on basic linear algebra kernels,\nit lends itself to efficient implementation on modern graphics hardware.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 17:20:30 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zayer", "Rhaleb", ""], ["Mlakar", "Daniel", ""], ["Steinberger", "Markus", ""], ["Seidel", "Hans-Peter", ""]]}, {"id": "1804.09293", "submitter": "Yuanming Hu", "authors": "Yuanming Hu", "title": "Taichi: An Open-Source Computer Graphics Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ideal software system in computer graphics should be a combination of\ninnovative ideas, solid software engineering and rapid development. However, in\nreality these requirements are seldom met simultaneously. In this paper, we\npresent early results on an open-source library named Taichi\n(http://taichi.graphics), which alleviates this practical issue by providing an\naccessible, portable, extensible, and high-performance infrastructure that is\nreusable and tailored for computer graphics. As a case study, we share our\nexperience in building a novel physical simulation system using Taichi.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 23:31:55 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Hu", "Yuanming", ""]]}, {"id": "1804.09404", "submitter": "Fumio Okura", "authors": "Takahiro Isokane, Fumio Okura, Ayaka Ide, Yasuyuki Matsushita, Yasushi\n  Yagi", "title": "Probabilistic Plant Modeling via Multi-View Image-to-Image Translation", "comments": "To appear in CVPR2018. The first two authors contributed equally.\n  Project website:\n  http://www.am.sanken.osaka-u.ac.jp/~okura/project/cvpr2018_plant.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for inferring three-dimensional (3D) plant\nbranch structures that are hidden under leaves from multi-view observations.\nUnlike previous geometric approaches that heavily rely on the visibility of the\nbranches or use parametric branching models, our method makes statistical\ninferences of branch structures in a probabilistic framework. By inferring the\nprobability of branch existence using a Bayesian extension of image-to-image\ntranslation applied to each of multi-view images, our method generates a\nprobabilistic plant 3D model, which represents the 3D branching pattern that\ncannot be directly observed. Experiments demonstrate the usefulness of the\nproposed approach in generating convincing branch structures in comparison to\nprior approaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 07:44:52 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Isokane", "Takahiro", ""], ["Okura", "Fumio", ""], ["Ide", "Ayaka", ""], ["Matsushita", "Yasuyuki", ""], ["Yagi", "Yasushi", ""]]}, {"id": "1804.10992", "submitter": "Qifeng Chen", "authors": "Xiaojuan Qi, Qifeng Chen, Jiaya Jia, and Vladlen Koltun", "title": "Semi-parametric Image Synthesis", "comments": "Published at the Conference on Computer Vision and Pattern\n  Recognition (CVPR 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semi-parametric approach to photographic image synthesis from\nsemantic layouts. The approach combines the complementary strengths of\nparametric and nonparametric techniques. The nonparametric component is a\nmemory bank of image segments constructed from a training set of images. Given\na novel semantic layout at test time, the memory bank is used to retrieve\nphotographic references that are provided as source material to a deep network.\nThe synthesis is performed by a deep network that draws on the provided\nphotographic material. Experiments on multiple semantic segmentation datasets\nshow that the presented approach yields considerably more realistic images than\nrecent purely parametric techniques. The results are shown in the supplementary\nvideo at https://youtu.be/U4Q98lenGLQ\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 21:20:43 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Qi", "Xiaojuan", ""], ["Chen", "Qifeng", ""], ["Jia", "Jiaya", ""], ["Koltun", "Vladlen", ""]]}]