[{"id": "1808.00328", "submitter": "Anthony Savidis", "authors": "Anthony Savidis", "title": "There is more to PCG than Meets the Eye: NPC AI, Dynamic Camera, PVS and\n  Lightmaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural content generation (PCG) concerns all sorts of algorithms and\ntools which automatically produce game content, without requiring manual\nauthoring by game artists. Besides generating com-plex static meshes, the PCG\ncore usually encompasses geometrical information about the game world that can\nbe useful in supporting other critical subsystems of the game engine. We\ndiscuss our experi-ence from the development of the iOS game title named\n\"Fallen God: Escape Underworld\", and show how our PCG produced extra metadata\nregarding the game world, in particular: (i) an annotated dun-geon graph to\nsupport path finding for NPC AI to attack or avoid the player (working for\nbipeds, birds, insects and serpents); (ii) a quantized voxel space to allow\ndiscrete A* for the dynamic camera system to work in the continuous 3d space;\n(iii) dungeon portals to support a dynamic PVS; and (iv) procedural ambient\nocclusion and tessellation of a separate set of simplified meshes to support\nvery-fast and high-quality light mapping.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 14:10:44 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Savidis", "Anthony", ""]]}, {"id": "1808.00362", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Jason Saragih, Tomas Simon, Yaser Sheikh", "title": "Deep Appearance Models for Face Rendering", "comments": "Accepted to SIGGRAPH 2018", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2018) 37, 4, Article 68", "doi": "10.1145/3197517.3201401", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep appearance model for rendering the human face. Inspired\nby Active Appearance Models, we develop a data-driven rendering pipeline that\nlearns a joint representation of facial geometry and appearance from a\nmultiview capture setup. Vertex positions and view-specific textures are\nmodeled using a deep variational autoencoder that captures complex nonlinear\neffects while producing a smooth and compact latent representation.\nView-specific texture enables the modeling of view-dependent effects such as\nspecularity. In addition, it can also correct for imperfect geometry stemming\nfrom biased or low resolution estimates. This is a significant departure from\nthe traditional graphics pipeline, which requires highly accurate geometry as\nwell as all elements of the shading model to achieve realism through\nphysically-inspired light transport. Acquiring such a high level of accuracy is\ndifficult in practice, especially for complex and intricate parts of the face,\nsuch as eyelashes and the oral cavity. These are handled naturally by our\napproach, which does not rely on precise estimates of geometry. Instead, the\nshading model accommodates deficiencies in geometry though the flexibility\nafforded by the neural network employed. At inference time, we condition the\ndecoding network on the viewpoint of the camera in order to generate the\nappropriate texture for rendering. The resulting system can be implemented\nsimply using existing rendering engines through dynamic textures with flat\nlighting. This representation, together with a novel unsupervised technique for\nmapping images to facial states, results in a system that is naturally suited\nto real-time interactive settings such as Virtual Reality (VR).\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:13:48 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lombardi", "Stephen", ""], ["Saragih", "Jason", ""], ["Simon", "Tomas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1808.00607", "submitter": "Rui Li", "authors": "Rui Li and Jian Chen", "title": "Toward A Deep Understanding of What Makes a Scientific Visualization\n  Memorable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report results from a preliminary study exploring the memorability of\nspatial scientific visualizations, the goal of which is to understand the\nvisual features that contribute to memorability. The evaluation metrics include\nthree objective measures (entropy, feature congestion, the number of edges),\nfour subjective ratings (clutter, the number of distinct colors, familiarity,\nand realism), and two sentiment ratings (interestingness and happiness). We\ncurate 1142 scientific visualization (SciVis) images from the original 2231\nimages in published IEEE SciVis papers from 2008 to 2017 and compute\nmemorability scores of 228 SciVis images from data collected on Amazon\nMechanical Turk (MTurk). Results showed that the memorability of SciVis images\nis mostly correlated with clutter and the number of distinct colors. We further\ninvestigate the differences between scientific visualization and infographics\nas a means to understand memorability differences by data attributes.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 00:15:39 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 21:46:16 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Rui", ""], ["Chen", "Jian", ""]]}, {"id": "1808.00703", "submitter": "Hammad Haleem", "authors": "Hammad Haleem, Yong Wang, Abishek Puri, Sahil Wadhwa and Huamin Qu", "title": "Evaluating the Readability of Force Directed Graph Layouts: A Deep\n  Learning Approach", "comments": "This work has been accepted at IEEE CG&A", "journal-ref": null, "doi": "10.1109/MCG.2018.2881501", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing graph layout algorithms are usually not able to optimize all the\naesthetic properties desired in a graph layout. To evaluate how well the\ndesired visual features are reflected in a graph layout, many readability\nmetrics have been proposed in the past decades. However, the calculation of\nthese readability metrics often requires access to the node and edge\ncoordinates and is usually computationally inefficient, especially for dense\ngraphs. Importantly, when the node and edge coordinates are not accessible, it\nbecomes impossible to evaluate the graph layouts quantitatively. In this paper,\nwe present a novel deep learning-based approach to evaluate the readability of\ngraph layouts by directly using graph images. A convolutional neural network\narchitecture is proposed and trained on a benchmark dataset of graph images,\nwhich is composed of synthetically-generated graphs and graphs created by\nsampling from real large networks. Multiple representative readability metrics\n(including edge crossing, node spread, and group overlap) are considered in the\nproposed approach. We quantitatively compare our approach to traditional\nmethods and qualitatively evaluate our approach using a case study and\nvisualizing convolutional layers. This work is a first step towards using deep\nlearning based methods to evaluate images from the visualization field\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:57:59 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 23:20:45 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Haleem", "Hammad", ""], ["Wang", "Yong", ""], ["Puri", "Abishek", ""], ["Wadhwa", "Sahil", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.01308", "submitter": "Hui Zhao", "authors": "Hui Zhao and Kehua Su and Ming Ma and Na Lei and Li Cui and Xianfeng\n  Gu", "title": "The Normal Map Based on Area-Preserving Parameterization", "comments": "we need update it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach to enhance and improve the current\nnormal map rendering technique. Our algorithm is based on semi-discrete Optimal\nMass Transportation (OMT) theory and has a solid theoretical base. The key\ndifference from previous normal map method is that we preserve the local area\nwhen we unwrap a disk-like 3D surface onto 2D plane. Compared to the currently\nused techniques which is based on conformal parameterization, our method does\nnot need to cut a surface into many small pieces to avoid the large area\ndistortion. The following charts packing step is also unnecessary in our\nframework. Our method is practical and makes the normal map technique more\nrobust and efficient.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 13:19:31 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 17:25:18 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Zhao", "Hui", ""], ["Su", "Kehua", ""], ["Ma", "Ming", ""], ["Lei", "Na", ""], ["Cui", "Li", ""], ["Gu", "Xianfeng", ""]]}, {"id": "1808.01427", "submitter": "Elena Balashova", "authors": "Elena Balashova, Vivek Singh, Jiangping Wang, Brian Teixeira, Terrence\n  Chen, Thomas Funkhouser", "title": "Structure-Aware Shape Synthesis", "comments": "Accepted to 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new procedure to guide training of a data-driven shape\ngenerative model using a structure-aware loss function. Complex 3D shapes often\ncan be summarized using a coarsely defined structure which is consistent and\nrobust across variety of observations. However, existing synthesis techniques\ndo not account for structure during training, and thus often generate\nimplausible and structurally unrealistic shapes. During training, we enforce\nstructural constraints in order to enforce consistency and structure across the\nentire manifold. We propose a novel methodology for training 3D generative\nmodels that incorporates structural information into an end-to-end training\npipeline.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 05:15:49 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Balashova", "Elena", ""], ["Singh", "Vivek", ""], ["Wang", "Jiangping", ""], ["Teixeira", "Brian", ""], ["Chen", "Terrence", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1808.02651", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai,\n  Alec Jacobson", "title": "Beyond Pixel Norm-Balls: Parametric Adversaries using an Analytically\n  Differentiable Renderer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning image classifiers are vulnerable to adversarial\nattacks, inputs with perturbations designed to intentionally trigger\nmisclassification. Current adversarial methods directly alter pixel colors and\nevaluate against pixel norm-balls: pixel perturbations smaller than a specified\nmagnitude, according to a measurement norm. This evaluation, however, has\nlimited practical utility since perturbations in the pixel space do not\ncorrespond to underlying real-world phenomena of image formation that lead to\nthem and has no security motivation attached. Pixels in natural images are\nmeasurements of light that has interacted with the geometry of a physical\nscene. As such, we propose the direct perturbation of physical parameters that\nunderly image formation: lighting and geometry. As such, we propose a novel\nevaluation measure, parametric norm-balls, by directly perturbing physical\nparameters that underly image formation. One enabling contribution we present\nis a physically-based differentiable renderer that allows us to propagate pixel\ngradients to the parametric space of lighting and geometry. Our approach\nenables physically-based adversarial attacks, and our differentiable renderer\nleverages models from the interactive rendering literature to balance the\nperformance and accuracy trade-offs necessary for a memory-efficient and\nscalable adversarial data augmentation workflow.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:01:18 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:12:01 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Tao", "Michael", ""], ["Li", "Chun-Liang", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "1808.02860", "submitter": "Kalina Borkiewicz", "authors": "Kalina Borkiewicz, J.P. Naiman, and Haoming Lai", "title": "Cinematic Visualization of Multiresolution Data: Ytini for Adaptive Mesh\n  Refinement in Houdini", "comments": "24 pages, 14 figures", "journal-ref": null, "doi": "10.3847/1538-3881/ab1f6f", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have entered the era of large multidimensional datasets represented by\nincreasingly complex data structures. Current tools for scientific\nvisualization are not optimized to efficiently and intuitively create cinematic\nproduction quality, time-evolving representations of numerical data for broad\nimpact science communication via film, media, or journalism. To present such\ndata in a cinematic environment, it is advantageous to develop methods that\nintegrate these complex data structures into industry standard visual effects\nsoftware packages, which provide a myriad of control features otherwise\nunavailable in traditional scientific visualization software. In this paper, we\npresent the general methodology for the import and visualization of nested\nmultiresolution datasets into commercially available visual effects software.\nWe further provide a specific example of importing Adaptive Mesh Refinement\ndata into the software Houdini. This paper builds on our previous work, which\ndescribes a method for using Houdini to visualize uniform Cartesian datasets.\nWe summarize a tutorial available on the website www.ytini.com, which includes\nsample data downloads, Python code, and various other resources to simplify the\nprocess of importing and rendering multiresolution data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 17:00:06 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 23:06:13 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Borkiewicz", "Kalina", ""], ["Naiman", "J. P.", ""], ["Lai", "Haoming", ""]]}, {"id": "1808.03338", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Daniel McDuff", "title": "DeepMag: Source Specific Motion Magnification Using Gradient Ascent", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important physical phenomena involve subtle signals that are difficult\nto observe with the unaided eye, yet visualizing them can be very informative.\nCurrent motion magnification techniques can reveal these small temporal\nvariations in video, but require precise prior knowledge about the target\nsignal, and cannot deal with interference motions at a similar frequency. We\npresent DeepMag an end-to-end deep neural video-processing framework based on\ngradient ascent that enables automated magnification of subtle color and motion\nsignals from a specific source, even in the presence of large motions of\nvarious velocities. While the approach is generalizable, the advantages of\nDeepMag are highlighted via the task of video-based physiological\nvisualization. Through systematic quantitative and qualitative evaluation of\nthe approach on videos with different levels of head motion, we compare the\nmagnification of pulse and respiration to existing state-of-the-art methods.\nOur method produces magnified videos with substantially fewer artifacts and\nblurring whilst magnifying the physiological changes by a similar degree.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:36:57 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Chen", "Weixuan", ""], ["McDuff", "Daniel", ""]]}, {"id": "1808.03823", "submitter": "Kai Xu", "authors": "Biao Leng, Cheng Zhang, Xiaocheng Zhou, Cheng Xu, Kai Xu", "title": "Learning Discriminative 3D Shape Representations by View Discerning\n  Networks", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics.\n  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view-based 3D shape recognition, extracting discriminative visual\nrepresentation of 3D shapes from projected images is considered the core\nproblem. Projections with low discriminative ability can adversely influence\nthe final 3D shape representation. Especially under the real situations with\nbackground clutter and object occlusion, the adverse effect is even more\nsevere. To resolve this problem, we propose a novel deep neural network, View\nDiscerning Network, which learns to judge the quality of views and adjust their\ncontributions to the representation of shapes. In this network, a Score\nGeneration Unit is devised to evaluate the quality of each projected image with\nscore vectors. These score vectors are used to weight the image features and\nthe weighted features perform much better than original features in 3D shape\nrecognition task. In particular, we introduce two structures of Score\nGeneration Unit, Channel-wise Score Unit and Part-wise Score Unit, to assess\nthe quality of feature maps from different perspectives. Our network aggregates\nfeatures and scores in an end-to-end framework, so that final shape descriptors\nare directly obtained from its output. Our experiments on ModelNet and ShapeNet\nCore55 show that View Discerning Network outperforms the state-of-the-arts in\nterms of the retrieval task, with excellent robustness against background\nclutter and object occlusion.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:09:45 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 21:22:36 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Leng", "Biao", ""], ["Zhang", "Cheng", ""], ["Zhou", "Xiaocheng", ""], ["Xu", "Cheng", ""], ["Xu", "Kai", ""]]}, {"id": "1808.03856", "submitter": "Thomas M\\\"uller", "authors": "Thomas M\\\"uller, Brian McWilliams, Fabrice Rousselle, Markus Gross,\n  Jan Nov\\'ak", "title": "Neural Importance Sampling", "comments": "19 pages, 15 figures. Accepted for publication in ACM Transactions on\n  Graphics; presented at SIGGRAPH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use deep neural networks for generating samples in Monte Carlo\nintegration. Our work is based on non-linear independent components estimation\n(NICE), which we extend in numerous ways to improve performance and enable its\napplication to integration problems. First, we introduce piecewise-polynomial\ncoupling transforms that greatly increase the modeling power of individual\ncoupling layers. Second, we propose to preprocess the inputs of neural networks\nusing one-blob encoding, which stimulates localization of computation and\nimproves inference. Third, we derive a gradient-descent-based optimization for\nthe KL and the $\\chi^2$ divergence for the specific application of Monte Carlo\nintegration with unnormalized stochastic estimates of the target distribution.\nOur approach enables fast and accurate inference and efficient sample\ngeneration independently of the dimensionality of the integration domain. We\nshow its benefits on generating natural images and in two applications to\nlight-transport simulation: first, we demonstrate learning of joint\npath-sampling densities in the primary sample space and importance sampling of\nmulti-dimensional path prefixes thereof. Second, we use our technique to\nextract conditional directional densities driven by the product of incident\nillumination and the BSDF in the rendering equation, and we leverage the\ndensities for path guiding. In all applications, our approach yields on-par or\nhigher performance than competing techniques at equal sample count.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 20:12:49 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 15:16:55 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 06:59:09 GMT"}, {"version": "v4", "created": "Mon, 27 May 2019 07:57:34 GMT"}, {"version": "v5", "created": "Tue, 3 Sep 2019 07:18:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["M\u00fcller", "Thomas", ""], ["McWilliams", "Brian", ""], ["Rousselle", "Fabrice", ""], ["Gross", "Markus", ""], ["Nov\u00e1k", "Jan", ""]]}, {"id": "1808.03981", "submitter": "Zhijie Wu", "authors": "Zhijie Wu and Xiang Wang and Di Lin and Dani Lischinski and Daniel\n  Cohen-Or and Hui Huang", "title": "SAGNet:Structure-aware Generative Network for 3D-Shape Modeling", "comments": "Accepted by SIGGRAPH 2019", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 91", "doi": "10.1145/3306346.3322956", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SAGNet, a structure-aware generative model for 3D shapes. Given a\nset of segmented objects of a certain class, the geometry of their parts and\nthe pairwise relationships between them (the structure) are jointly learned and\nembedded in a latent space by an autoencoder. The encoder intertwines the\ngeometry and structure features into a single latent code, while the decoder\ndisentangles the features and reconstructs the geometry and structure of the 3D\nmodel. Our autoencoder consists of two branches, one for the structure and one\nfor the geometry. The key idea is that during the analysis, the two branches\nexchange information between them, thereby learning the dependencies between\nstructure and geometry and encoding two augmented features, which are then\nfused into a single latent code. This explicit intertwining of information\nenables separately controlling the geometry and the structure of the generated\nmodels. We evaluate the performance of our method and conduct an ablation\nstudy. We explicitly show that encoding of shapes accounts for both\nsimilarities in structure and geometry. A variety of quality results generated\nby SAGNet are presented. The data and code are at\nhttps://github.com/zhijieW-94/SAGNet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 18:31:06 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 10:44:42 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 10:35:05 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2019 15:35:16 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Wu", "Zhijie", ""], ["Wang", "Xiang", ""], ["Lin", "Di", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "1808.04121", "submitter": "Hao Yan", "authors": "Song Yuheng and Yan Hao", "title": "Image Inpainting Based on a Novel Criminisi Algorithm", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the problem of image inpainting error continuation and the\ndeviation of finding best match block, an improved Criminisi algorithm is\nproposed. The improvement was mainly embodied in two aspects. In the repairing\norder aspect, we redefine the calculation formula of the priority. In order to\nsolve the problem of error continuation caused by local confidence item\nupdating, the mean value of Manhattan distance is used for replace the\nconfidence item. In the matching strategy aspect, finding the best match block\nnot only depend on the difference of the two pixels, but also consider the\nmatching region. Therefore, Euclidean distance is introduced. Experiments\nconfirm that the improved algorithm can overcome the insufficiencies of the\noriginal algorithm. The repairing effect has been improved, and the results\nhave a better visual appearance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 09:34:06 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Yuheng", "Song", ""], ["Hao", "Yan", ""]]}, {"id": "1808.04545", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli\n  Shechtman, Sunil Hadap, Ersin Yumer, Honglak Lee", "title": "MT-VAE: Learning Motion Transformations to Generate Multimodal Human\n  Dynamics", "comments": "Published at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term human motion can be represented as a series of motion\nmodes---motion sequences that capture short-term temporal dynamics---with\ntransitions between them. We leverage this structure and present a novel Motion\nTransformation Variational Auto-Encoders (MT-VAE) for learning motion sequence\ngeneration. Our model jointly learns a feature embedding for motion modes (that\nthe motion sequence can be reconstructed from) and a feature transformation\nthat represents the transition of one motion mode to the next motion mode. Our\nmodel is able to generate multiple diverse and plausible motion sequences in\nthe future from the same input. We apply our approach to both facial and full\nbody motion, and demonstrate applications like analogy-based motion transfer\nand video synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 06:21:03 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Rastogi", "Akash", ""], ["Villegas", "Ruben", ""], ["Sunkavalli", "Kalyan", ""], ["Shechtman", "Eli", ""], ["Hadap", "Sunil", ""], ["Yumer", "Ersin", ""], ["Lee", "Honglak", ""]]}, {"id": "1808.04931", "submitter": "Paul Kry", "authors": "Bin Wang, Paul Kry, Yuanmin Deng, Uri Ascher, Hui Huang, Baoquan Chen", "title": "Neural Material: Learning Elastic Constitutive Material and Damping\n  Models from Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy and fidelity of deformation simulations are highly dependent\nupon the underlying constitutive material model. Commonly used linear or\nnonlinear constitutive material models only cover a tiny part of possible\nmaterial behavior. In this work we propose a unified framework for modeling\ndeformable material. The key idea is to use a neural network to correct a\nnominal model of the elastic and damping properties of the object. The neural\nnetwork encapsulates a complex function that is hard to explicitly model. It\ninjects force corrections that help the forward simulation to more accurately\npredict the true behavior of a given soft object, which includes non-linear\nelastic forces and damping. Attempting to satisfy the requirement from real\nmaterial interference and animation design scenarios, we learn material models\nfrom examples of dynamic behavior of a deformable object's surface. The\nchallenge is that such data is sparse as it is consistently given only on part\nof the surface. Sparse reduced space-time optimization is employed to gradually\ngenerate increasingly accurate training data, which further refines and\nenhances the neural network. We evaluate our choice of network architecture and\nshow evidence that the modest amount of training data we use is suitable for\nthe problem tackled. Our method is demonstrated with a set of synthetic\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 00:55:30 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Wang", "Bin", ""], ["Kry", "Paul", ""], ["Deng", "Yuanmin", ""], ["Ascher", "Uri", ""], ["Huang", "Hui", ""], ["Chen", "Baoquan", ""]]}, {"id": "1808.04952", "submitter": "Hao Pan", "authors": "Yuqi Yang, Shilin Liu, Hao Pan, Yang Liu, Xin Tong", "title": "PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel\n  Frames", "comments": "15 pages, 18 figures. CVPR 2020. Project page:\n  https://haopan.github.io/surfacecnn.html", "journal-ref": "The IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 13578-13587", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface meshes are widely used shape representations and capture finer\ngeometry data than point clouds or volumetric grids, but are challenging to\napply CNNs directly due to their non-Euclidean structure. We use parallel\nframes on surface to define PFCNNs that enable effective feature learning on\nsurface meshes by mimicking standard convolutions faithfully. In particular,\nthe convolution of PFCNN not only maps local surface patches onto flat tangent\nplanes, but also aligns the tangent planes such that they locally form a flat\nEuclidean structure, thus enabling recovery of standard convolutions. The\nalignment is achieved by the tool of locally flat connections borrowed from\ndiscrete differential geometry, which can be efficiently encoded and computed\nby parallel frame fields. In addition, the lack of canonical axis on surface is\nhandled by sampling with the frame directions. Experiments show that for tasks\nincluding classification, segmentation and registration on deformable geometric\ndomains, as well as semantic scene segmentation on rigid domains, PFCNNs\nachieve robust and superior performances without using sophisticated input\nfeatures than state-of-the-art surface based CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 02:39:35 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 05:58:56 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Yang", "Yuqi", ""], ["Liu", "Shilin", ""], ["Pan", "Hao", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1808.05174", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Shugao Ma, Deva Ramanan, Yaser Sheikh", "title": "Recycle-GAN: Unsupervised Video Retargeting", "comments": "ECCV 2018; Please refer to project webpage for videos -\n  http://www.cs.cmu.edu/~aayushb/Recycle-GAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a data-driven approach for unsupervised video retargeting that\ntranslates content from one domain to another while preserving the style native\nto a domain, i.e., if contents of John Oliver's speech were to be transferred\nto Stephen Colbert, then the generated content/speech should be in Stephen\nColbert's style. Our approach combines both spatial and temporal information\nalong with adversarial losses for content translation and style preservation.\nIn this work, we first study the advantages of using spatiotemporal constraints\nover spatial constraints for effective retargeting. We then demonstrate the\nproposed approach for the problems where information in both space and time\nmatters such as face-to-face translation, flower-to-flower, wind and cloud\nsynthesis, sunrise and sunset.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 16:34:08 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Bansal", "Aayush", ""], ["Ma", "Shugao", ""], ["Ramanan", "Deva", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1808.05323", "submitter": "Yudong Guo", "authors": "Yudong Guo, Lin Cai, Juyong Zhang", "title": "3D Face From X: Learning Face Shape from Diverse Sources", "comments": "Accepted by IEEE Transactions on Image Processing, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method to jointly learn a 3D face parametric model and 3D\nface reconstruction from diverse sources. Previous methods usually learn 3D\nface modeling from one kind of source, such as scanned data or in-the-wild\nimages. Although 3D scanned data contain accurate geometric information of face\nshapes, the capture system is expensive and such datasets usually contain a\nsmall number of subjects. On the other hand, in-the-wild face images are easily\nobtained and there are a large number of facial images. However, facial images\ndo not contain explicit geometric information. In this paper, we propose a\nmethod to learn a unified face model from diverse sources. Besides scanned face\ndata and face images, we also utilize a large number of RGB-D images captured\nwith an iPhone X to bridge the gap between the two sources. Experimental\nresults demonstrate that with training data from more sources, we can learn a\nmore powerful face model.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 01:59:15 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 00:57:16 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 09:20:30 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Guo", "Yudong", ""], ["Cai", "Lin", ""], ["Zhang", "Juyong", ""]]}, {"id": "1808.05870", "submitter": "Maxime Soler", "authors": "Maxime Soler, M\\'elanie Plainchault, Bruno Conche, Julien Tierny", "title": "Lifted Wasserstein Matcher for Fast and Robust Topology Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust and efficient method for tracking topological\nfeatures in time-varying scalar data. Structures are tracked based on the\noptimal matching between persistence diagrams with respect to the Wasserstein\nmetric. This fundamentally relies on solving the assignment problem, a special\ncase of optimal transport, for all consecutive timesteps. Our approach relies\non two main contributions. First, we revisit the seminal assignment algorithm\nby Kuhn and Munkres which we specifically adapt to the problem of matching\npersistence diagrams in an efficient way. Second, we propose an extension of\nthe Wasserstein metric that significantly improves the geometrical stability of\nthe matching of domain-embedded persistence pairs. We show that this\ngeometrical lifting has the additional positive side-effect of improving the\nassignment matrix sparsity and therefore computing time. The global framework\nimplements a coarse-grained parallelism by computing persistence diagrams and\nfinding optimal matchings in parallel for every couple of consecutive\ntimesteps. Critical trajectories are constructed by associating successively\nmatched persistence pairs over time. Merging and splitting events are detected\nwith a geometrical threshold in a post-processing stage. Extensive experiments\non real-life datasets show that our matching approach is an order of magnitude\nfaster than the seminal Munkres algorithm. Moreover, compared to a modern\napproximation method, our method provides competitive runtimes while yielding\nexact results. We demonstrate the utility of our global framework by extracting\ncritical point trajectories from various simulated time-varying datasets and\ncompare it to the existing methods based on associated overlaps of volumes.\nRobustness to noise and temporal resolution downsampling is empirically\ndemonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 13:54:05 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 11:17:11 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 09:23:15 GMT"}, {"version": "v4", "created": "Wed, 2 Jan 2019 08:02:12 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Soler", "Maxime", ""], ["Plainchault", "M\u00e9lanie", ""], ["Conche", "Bruno", ""], ["Tierny", "Julien", ""]]}, {"id": "1808.06060", "submitter": "Rushan Ziatdinov", "authors": "Rushan Ziatdinov, Valerijan G. Muftejev, Rustam I. Akhmetshin,\n  Alexander P. Zelev, Rifkat I. Nabiyev, Albert R. Mardanov", "title": "Universal software platform for visualizing class F curves,\n  log-aesthetic curves and development of applied CAD systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the capabilities of a universal software platform for\nvisualizing class F curves and developing specialized applications for CAD\nsystems based on Microsoft Excel VBA, the software complex FairCurveModeler,\nand computer algebra systems. Additionally, it demonstrates the use of a\nsoftware platform for visualizing functional and log-aesthetic curves\nintegrated with CAD Fusion360. The value of the curves is evident in\nvisualizing the qualitative geometry of the product shape in industrial design.\nMoreover, the requirements for the characteristics of class F curves are\nemphasized to form a visual purity of shape in industrial design and to provide\na positive emotional perception of the visual image of the product by a person.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 09:01:24 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Ziatdinov", "Rushan", ""], ["Muftejev", "Valerijan G.", ""], ["Akhmetshin", "Rustam I.", ""], ["Zelev", "Alexander P.", ""], ["Nabiyev", "Rifkat I.", ""], ["Mardanov", "Albert R.", ""]]}, {"id": "1808.06201", "submitter": "Amin Babadi", "authors": "Amin Babadi, Kourosh Naderi, Perttu H\\\"am\\\"al\\\"ainen", "title": "Intelligent Middle-Level Game Control", "comments": "2018 IEEE Conference on Computational Intelligence and Games (IEEE\n  CIG 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the concept of intelligent middle-level game control, which lies\non a continuum of control abstraction levels between the following two dual\nopposites: 1) high-level control that translates player's simple commands into\ncomplex actions (such as pressing Space key for jumping), and 2) low-level\ncontrol which simulates real-life complexities by directly manipulating, e.g.,\njoint rotations of the character as it is done in the runner game QWOP. We\nposit that various novel control abstractions can be explored using recent\nadvances in movement intelligence of game characters. We demonstrate this\nthrough design and evaluation of a novel 2-player martial arts game prototype.\nIn this game, each player guides a simulated humanoid character by clicking and\ndragging body parts. This defines the cost function for an online continuous\ncontrol algorithm that executes the requested movement. Our control algorithm\nuses Covariance Matrix Adaptation Evolution Strategy (CMA-ES) in a rolling\nhorizon manner with custom population seeding techniques. Our playtesting data\nindicates that intelligent middle-level control results in producing novel and\ninnovative gameplay without frustrating interface complexities.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 11:53:50 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Babadi", "Amin", ""], ["Naderi", "Kourosh", ""], ["H\u00e4m\u00e4l\u00e4inen", "Perttu", ""]]}, {"id": "1808.06601", "submitter": "Ting-Chun Wang", "authors": "Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan\n  Kautz, Bryan Catanzaro", "title": "Video-to-Video Synthesis", "comments": "In NeurIPS, 2018. Code, models, and more results are available at\n  https://github.com/NVIDIA/vid2vid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of video-to-video synthesis, whose goal is to learn a\nmapping function from an input source video (e.g., a sequence of semantic\nsegmentation masks) to an output photorealistic video that precisely depicts\nthe content of the source video. While its image counterpart, the\nimage-to-image synthesis problem, is a popular topic, the video-to-video\nsynthesis problem is less explored in the literature. Without understanding\ntemporal dynamics, directly applying existing image synthesis approaches to an\ninput video often results in temporally incoherent videos of low visual\nquality. In this paper, we propose a novel video-to-video synthesis approach\nunder the generative adversarial learning framework. Through carefully-designed\ngenerator and discriminator architectures, coupled with a spatio-temporal\nadversarial objective, we achieve high-resolution, photorealistic, temporally\ncoherent video results on a diverse set of input formats including segmentation\nmasks, sketches, and poses. Experiments on multiple benchmarks show the\nadvantage of our method compared to strong baselines. In particular, our model\nis capable of synthesizing 2K resolution videos of street scenes up to 30\nseconds long, which significantly advances the state-of-the-art of video\nsynthesis. Finally, we apply our approach to future video prediction,\noutperforming several state-of-the-art competing systems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 17:58:42 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 15:12:44 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Wang", "Ting-Chun", ""], ["Liu", "Ming-Yu", ""], ["Zhu", "Jun-Yan", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Kautz", "Jan", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "1808.06698", "submitter": "Kai Xu", "authors": "Songle Chen, Lintao Zheng, Yan Zhang, Zhixin Sun, Kai Xu", "title": "VERAM: View-Enhanced Recurrent Attention Model for 3D Shape\n  Classification", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics.\n  Corresponding Author: Kai Xu (kevin.kai.xu@gmail.com)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view deep neural network is perhaps the most successful approach in 3D\nshape classification. However, the fusion of multi-view features based on max\nor average pooling lacks a view selection mechanism, limiting its application\nin, e.g., multi-view active object recognition by a robot. This paper presents\nVERAM, a recurrent attention model capable of actively selecting a sequence of\nviews for highly accurate 3D shape classification. VERAM addresses an important\nissue commonly found in existing attention-based models, i.e., the unbalanced\ntraining of the subnetworks corresponding to next view estimation and shape\nclassification. The classification subnetwork is easily overfitted while the\nview estimation one is usually poorly trained, leading to a suboptimal\nclassification performance. This is surmounted by three essential\nview-enhancement strategies: 1) enhancing the information flow of gradient\nbackpropagation for the view estimation subnetwork, 2) devising a highly\ninformative reward function for the reinforcement training of view estimation\nand 3) formulating a novel loss function that explicitly circumvents view\nduplication. Taking grayscale image as input and AlexNet as CNN architecture,\nVERAM with 9 views achieves instance-level and class-level accuracy of 95:5%\nand 95:3% on ModelNet10, 93:7% and 92:1% on ModelNet40, both are the\nstate-of-the-art performance under the same number of views.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 21:08:02 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Chen", "Songle", ""], ["Zheng", "Lintao", ""], ["Zhang", "Yan", ""], ["Sun", "Zhixin", ""], ["Xu", "Kai", ""]]}, {"id": "1808.06715", "submitter": "Alejandro Sztrajman", "authors": "Alejandro Sztrajman, Jaroslav Krivanek, Alexander Wilkie, Tim Weyrich", "title": "Image-based remapping of spatially-varying material appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BRDF models are ubiquitous tools for the representation of material\nappearance. However, there is now an astonishingly large number of different\nmodels in practical use. Both a lack of BRDF model standardisation across\nimplementations found in different renderers, as well as the often semantically\ndifferent capabilities of various models, have grown to be a major hindrance to\nthe interchange of production assets between different rendering systems.\nCurrent attempts to solve this problem rely on manually finding visual\nsimilarities between models, or mathematical ones between their functional\nshapes, which requires access to the shader implementation, usually unavailable\nin commercial renderers. We present a method for automatic translation of\nmaterial appearance between different BRDF models, which uses an image-based\nmetric for appearance comparison, and that delegates the interaction with the\nmodel to the renderer. We analyse the performance of the method, both with\nrespect to robustness and visual differences of the fits for multiple\ncombinations of BRDF models. While it is effective for individual BRDFs, the\ncomputational cost does not scale well for spatially-varying BRDFs. Therefore,\nwe further present a parametric regression scheme that approximates the shape\nof the transformation function and generates a reduced representation which\nevaluates instantly and without further interaction with the renderer. We\npresent respective visual comparisons of the remapped SVBRDF models for\ncommonly used renderers and shading models, and show that our approach is able\nto extrapolate transformed BRDF parameters better than other complex regression\nschemes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 22:54:53 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Sztrajman", "Alejandro", ""], ["Krivanek", "Jaroslav", ""], ["Wilkie", "Alexander", ""], ["Weyrich", "Tim", ""]]}, {"id": "1808.06847", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Mingyi Shi, Jing Liao, Dani Lischinski, Baoquan Chen,\n  Daniel Cohen-Or", "title": "Deep Video-Based Performance Cloning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new video-based performance cloning technique. After training a\ndeep generative network using a reference video capturing the appearance and\ndynamics of a target actor, we are able to generate videos where this actor\nreenacts other performances. All of the training data and the driving\nperformances are provided as ordinary video segments, without motion capture or\ndepth information. Our generative model is realized as a deep neural network\nwith two branches, both of which train the same space-time conditional\ngenerator, using shared weights. One branch, responsible for learning to\ngenerate the appearance of the target actor in various poses, uses\n\\emph{paired} training data, self-generated from the reference video. The\nsecond branch uses unpaired data to improve generation of temporally coherent\nvideo renditions of unseen pose sequences. We demonstrate a variety of\npromising results, where our method is able to generate temporally coherent\nvideos, for challenging scenarios where the reference and driving videos\nconsist of very different dance performances. Supplementary video:\nhttps://youtu.be/JpwsEeqNhhA.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 11:20:49 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Aberman", "Kfir", ""], ["Shi", "Mingyi", ""], ["Liao", "Jing", ""], ["Lischinski", "Dani", ""], ["Chen", "Baoquan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1808.06884", "submitter": "Donghui Yan", "authors": "Donghui Yan and Gary E. Davis", "title": "The Turtleback Diagram for Conditional Probability", "comments": "23 pages, 14 figures", "journal-ref": "The Open Journal of Statistics, Vol 8(4), 684-705, 2018", "doi": "10.4236/ojs.2018.84045", "report-no": null, "categories": "stat.AP cs.GR stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We elaborate on an alternative representation of conditional probability to\nthe usual tree diagram. We term the representation `turtleback diagram' for its\nresemblance to the pattern on turtle shells. Adopting the set theoretic view of\nevents and the sample space, the turtleback diagram uses elements from Venn\ndiagrams---set intersection, complement and partition---for conditioning, with\nthe additional notion that the area of a set indicates probability whereas the\nratio of areas for conditional probability. Once parts of the diagram are drawn\nand properly labeled, the calculation of conditional probability involves only\nsimple arithmetic on the area of relevant sets. We discuss turtleback diagrams\nin relation to other visual representations of conditional probability, and\ndetail several scenarios in which turtleback diagrams prove useful. By the\nequivalence of recursive space partition and the tree, the turtleback diagram\nis seen to be equally expressive as the tree diagram for representing abstract\nconcepts. We also provide empirical data on the use of turtleback diagrams with\nundergraduate students in elementary statistics or probability courses.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 13:05:13 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Yan", "Donghui", ""], ["Davis", "Gary E.", ""]]}, {"id": "1808.07371", "submitter": "Caroline Chan", "authors": "Caroline Chan, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros", "title": "Everybody Dance Now", "comments": "In ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple method for \"do as I do\" motion transfer: given a\nsource video of a person dancing, we can transfer that performance to a novel\n(amateur) target after only a few minutes of the target subject performing\nstandard moves. We approach this problem as video-to-video translation using\npose as an intermediate representation. To transfer the motion, we extract\nposes from the source subject and apply the learned pose-to-appearance mapping\nto generate the target subject. We predict two consecutive frames for\ntemporally coherent video results and introduce a separate pipeline for\nrealistic face synthesis. Although our method is quite simple, it produces\nsurprisingly compelling results (see video). This motivates us to also provide\na forensics tool for reliable synthetic content detection, which is able to\ndistinguish videos synthesized by our system from real data. In addition, we\nrelease a first-of-its-kind open-source dataset of videos that can be legally\nused for training and motion transfer.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:58:36 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 21:10:54 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Chan", "Caroline", ""], ["Ginosar", "Shiry", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1808.07560", "submitter": "Konstantinos Gavriil", "authors": "Konstantinos Gavriil, Alexander Schiftner, Helmut Pottmann", "title": "Optimizing B-spline surfaces for developability and paneling\n  architectural freeform surfaces", "comments": null, "journal-ref": "Computer-Aided Design 111 (2019) 29-43", "doi": "10.1016/j.cad.2019.01.006", "report-no": null, "categories": "cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in architecture and design, we present a novel\nmethod for increasing the developability of a B-spline surface. We use the\nproperty that the Gauss image of a developable surface is 1-dimensional and can\nbe locally well approximated by circles. This is cast into an algorithm for\nthinning the Gauss image by increasing the planarity of the Gauss images of\nappropriate neighborhoods. A variation of the main method allows us to tackle\nthe problem of paneling a freeform architectural surface with developable\npanels, in particular enforcing rotational cylindrical, rotational conical and\nplanar panels, which are the main preferred types of developable panels in\narchitecture due to the reduced cost of manufacturing.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 20:53:08 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 12:13:13 GMT"}, {"version": "v3", "created": "Tue, 26 Feb 2019 09:04:34 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gavriil", "Konstantinos", ""], ["Schiftner", "Alexander", ""], ["Pottmann", "Helmut", ""]]}, {"id": "1808.07757", "submitter": "Xian Wu", "authors": "Xian Wu, Rui-Long Li, Fang-Lue Zhang, Jian-Cheng Liu, Jue Wang, Ariel\n  Shamir, Shi-Min Hu", "title": "Deep Portrait Image Completion and Extrapolation", "comments": "accepted by Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2945866", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General image completion and extrapolation methods often fail on portrait\nimages where parts of the human body need to be recovered - a task that\nrequires accurate human body structure and appearance synthesis. We present a\ntwo-stage deep learning framework for tacking this problem. In the first stage,\ngiven a portrait image with an incomplete human body, we extract a complete,\ncoherent human body structure through a human parsing network, which focuses on\nstructure recovery inside the unknown region with the help of pose estimation.\nIn the second stage, we use an image completion network to fill the unknown\nregion, guided by the structure map recovered in the first stage. For realistic\nsynthesis the completion network is trained with both perceptual loss and\nconditional adversarial loss. We evaluate our method on public portrait image\ndatasets, and show that it outperforms other state-of-art general image\ncompletion methods. Our method enables new portrait image editing applications\nsuch as occlusion removal and portrait extrapolation. We further show that the\nproposed general learning framework can be applied to other types of images,\ne.g. animal images.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 13:52:51 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 15:32:55 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Wu", "Xian", ""], ["Li", "Rui-Long", ""], ["Zhang", "Fang-Lue", ""], ["Liu", "Jian-Cheng", ""], ["Wang", "Jue", ""], ["Shamir", "Ariel", ""], ["Hu", "Shi-Min", ""]]}, {"id": "1808.07778", "submitter": "Stefan Ohrhallinger", "authors": "Stefan Ohrhallinger and Michael Wimmer", "title": "StretchDenoise: Parametric Curve Reconstruction with Guarantees by\n  Separating Connectivity from Residual Uncertainty of Samples", "comments": "Extended version of accepted short paper: 10 pages, 9 figures, 2\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We reconstruct a closed denoised curve from an unstructured and highly noisy\n2D point cloud. Our proposed method uses a two- pass approach: Previously\nrecovered manifold connectivity is used for ordering noisy samples along this\nmanifold and express these as residuals in order to enable parametric\ndenoising. This separates recovering low-frequency features from denoising high\nfrequencies, which avoids over-smoothing. The noise probability density\nfunctions (PDFs) at samples are either taken from sensor noise models or from\nestimates of the connectivity recovered in the first pass. The output curve\nbalances the signed distances (inside/outside) to the samples. Additionally,\nthe angles between edges of the polygon representing the connectivity become\nminimized in the least-square sense. The movement of the polygon's vertices is\nrestricted to their noise extent, i.e., a cut-off distance corresponding to a\nmaximum variance of the PDFs. We approximate the resulting optimization model,\nwhich consists of higher-order functions, by a linear model with good\ncorrespondence. Our algorithm is parameter-free and operates fast on the local\nneighborhoods determined by the connectivity. We augment a least-squares solver\nconstrained by a linear system to also handle bounds. This enables us to\nguarantee stochastic error bounds for sampled curves corrupted by noise, e.g.,\nsilhouettes from sensed data, and we improve on the reconstruction error from\nground truth. Open source to reproduce figures and tables in this paper is\navailable at: https://github.com/stefango74/stretchdenoise\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 14:35:33 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Ohrhallinger", "Stefan", ""], ["Wimmer", "Michael", ""]]}, {"id": "1808.07840", "submitter": "Quan Zheng", "authors": "Quan Zheng and Matthias Zwicker", "title": "Learning to Importance Sample in Primary Sample Space", "comments": "Submitted to SIGGRAPH ASIA'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is one of the most widely used variance reduction\nstrategies in Monte Carlo rendering. In this paper, we propose a novel\nimportance sampling technique that uses a neural network to learn how to sample\nfrom a desired density represented by a set of samples. Our approach considers\nan existing Monte Carlo rendering algorithm as a black box. During a\nscene-dependent training phase, we learn to generate samples with a desired\ndensity in the primary sample space of the rendering algorithm using maximum\nlikelihood estimation. We leverage a recent neural network architecture that\nwas designed to represent real-valued non-volume preserving ('Real NVP')\ntransformations in high dimensional spaces. We use Real NVP to non-linearly\nwarp primary sample space and obtain desired densities. In addition, Real NVP\nefficiently computes the determinant of the Jacobian of the warp, which is\nrequired to implement the change of integration variables implied by the warp.\nA main advantage of our approach is that it is agnostic of underlying light\ntransport effects, and can be combined with many existing rendering techniques\nby treating them as a black box. We show that our approach leads to effective\nvariance reduction in several practical scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 16:55:53 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Zheng", "Quan", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1808.08718", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Yuchen Fan, Jianchao Yang, Ning Xu, Zhaowen Wang, Xinchao\n  Wang, Thomas Huang", "title": "Wide Activation for Efficient and Accurate Image Super-Resolution", "comments": "tech report and factsheet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we demonstrate that with same parameters and computational\nbudgets, models with wider features before ReLU activation have significantly\nbetter performance for single image super-resolution (SISR). The resulted SR\nresidual network has a slim identity mapping pathway with wider (\\(2\\times\\) to\n\\(4\\times\\)) channels before activation in each residual block. To further\nwiden activation (\\(6\\times\\) to \\(9\\times\\)) without computational overhead,\nwe introduce linear low-rank convolution into SR networks and achieve even\nbetter accuracy-efficiency tradeoffs. In addition, compared with batch\nnormalization or no normalization, we find training with weight normalization\nleads to better accuracy for deep super-resolution networks. Our proposed SR\nnetwork \\textit{WDSR} achieves better results on large-scale DIV2K image\nsuper-resolution benchmark in terms of PSNR with same or lower computational\ncomplexity. Based on WDSR, our method also won 1st places in NTIRE 2018\nChallenge on Single Image Super-Resolution in all three realistic tracks.\nExperiments and ablation studies support the importance of wide activation for\nimage super-resolution. Code is released at:\nhttps://github.com/JiahuiYu/wdsr_ntire2018\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:48:21 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 02:42:59 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Yu", "Jiahui", ""], ["Fan", "Yuchen", ""], ["Yang", "Jianchao", ""], ["Xu", "Ning", ""], ["Wang", "Zhaowen", ""], ["Wang", "Xinchao", ""], ["Huang", "Thomas", ""]]}, {"id": "1808.09351", "submitter": "Jun-Yan Zhu", "authors": "Shunyu Yao, Tzu Ming Harry Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio\n  Torralba, William T. Freeman, Joshua B. Tenenbaum", "title": "3D-Aware Scene Manipulation via Inverse Graphics", "comments": "NeurIPS 2018. Code: https://github.com/ysymyth/3D-SDN Website:\n  http://3dsdn.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to obtain an interpretable, expressive, and disentangled scene\nrepresentation that contains comprehensive structural and textural information\nfor each object. Previous scene representations learned by neural networks are\noften uninterpretable, limited to a single object, or lacking 3D knowledge. In\nthis work, we propose 3D scene de-rendering networks (3D-SDN) to address the\nabove issues by integrating disentangled representations for semantics,\ngeometry, and appearance into a deep generative model. Our scene encoder\nperforms inverse graphics, translating a scene into a structured object-wise\nrepresentation. Our decoder has two components: a differentiable shape renderer\nand a neural texture generator. The disentanglement of semantics, geometry, and\nappearance supports 3D-aware scene manipulation, e.g., rotating and moving\nobjects freely while keeping the consistent shape and texture, and changing the\nobject appearance without affecting its shape. Experiments demonstrate that our\nediting scheme based on 3D-SDN is superior to its 2D counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 15:16:07 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 17:58:48 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 17:34:14 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 18:57:20 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Yao", "Shunyu", ""], ["Hsu", "Tzu Ming Harry", ""], ["Zhu", "Jun-Yan", ""], ["Wu", "Jiajun", ""], ["Torralba", "Antonio", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1808.10083", "submitter": "He Zhang", "authors": "He Zhang, Hanlin Mo, You Hao, Qi Li, and Hua Li", "title": "Differential and integral invariants under Mobius transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging problems in the domain of 2-D image or 3-D shape\nis to handle the non-rigid deformation. From the perspective of transformation\ngroups, the conformal transformation is a key part of the diffeomorphism.\nAccording to the Liouville Theorem, an important part of the conformal\ntransformation is the Mobius transformation, so we focus on Mobius\ntransformation and propose two differential expressions that are invariable\nunder 2-D and 3-D Mobius transformation respectively. Next, we analyze the\nabsoluteness and relativity of invariance on them and their components. After\nthat, we propose integral invariants under Mobius transformation based on the\ntwo differential expressions. Finally, we propose a conjecture about the\nstructure of differential invariants under conformal transformation according\nto our observation on the composition of the above two differential invariants.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 01:45:57 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Zhang", "He", ""], ["Mo", "Hanlin", ""], ["Hao", "You", ""], ["Li", "Qi", ""], ["Li", "Hua", ""]]}, {"id": "1808.10654", "submitter": "Amir Zamir", "authors": "Fei Xia, Amir Zamir, Zhi-Yang He, Alexander Sax, Jitendra Malik,\n  Silvio Savarese", "title": "Gibson Env: Real-World Perception for Embodied Agents", "comments": "Access the code, dataset, and project website at\n  http://gibsonenv.vision/ . CVPR 2018", "journal-ref": "CVPR 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing visual perception models for active agents and sensorimotor\ncontrol are cumbersome to be done in the physical world, as existing algorithms\nare too slow to efficiently learn in real-time and robots are fragile and\ncostly. This has given rise to learning-in-simulation which consequently casts\na question on whether the results transfer to real-world. In this paper, we are\nconcerned with the problem of developing real-world perception for active\nagents, propose Gibson Virtual Environment for this purpose, and showcase\nsample perceptual tasks learned therein. Gibson is based on virtualizing real\nspaces, rather than using artificially designed ones, and currently includes\nover 1400 floor spaces from 572 full buildings. The main characteristics of\nGibson are: I. being from the real-world and reflecting its semantic\ncomplexity, II. having an internal synthesis mechanism, \"Goggles\", enabling\ndeploying the trained models in real-world without needing further domain\nadaptation, III. embodiment of agents and making them subject to constraints of\nphysics and space.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:56:43 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Xia", "Fei", ""], ["Zamir", "Amir", ""], ["He", "Zhi-Yang", ""], ["Sax", "Alexander", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}]