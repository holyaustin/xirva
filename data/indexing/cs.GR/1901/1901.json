[{"id": "1901.00238", "submitter": "Ran Ling", "authors": "Gang Xu, Ran Ling, Jessica Zhang, Zhoufang Xiao, Zhongping Ji and\n  Timon Rabczuk", "title": "Singularity Structure Simplification of Hexahedral Mesh via Weighted\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improved singularity structure simplification\nmethod for hexahedral (hex) meshes using a weighted ranking approach. In\nprevious work, the selection of to-be-collapsed base complex sheets/chords is\nonly based on their thickness, which will introduce a few closed-loops and\ncause an early termination of simplification and a slow convergence rate. In\nthis paper, a new weighted ranking function is proposed by combining the\nvalence prediction function of local singularity structure, shape quality\nmetric of elements and the width of base complex sheets/chords together.\nAdaptive refinement and local optimization are also introduced to improve the\nuniformity and aspect ratio of mesh elements. Compared to thickness ranking\nmethods, our weighted ranking approach can yield a simpler singularity\nstructure with fewer base-complex components, while achieving comparable\nHausdorff distance ratio and better mesh quality. Comparisons on a hex-mesh\ndataset are performed to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:54:47 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 07:44:23 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Xu", "Gang", ""], ["Ling", "Ran", ""], ["Zhang", "Jessica", ""], ["Xiao", "Zhoufang", ""], ["Ji", "Zhongping", ""], ["Rabczuk", "Timon", ""]]}, {"id": "1901.00992", "submitter": "Julian Marcon", "authors": "Julian Marcon, Michael Turner, Joaquim Peir\\'o, David Moxey, Claire R.\n  Pollard, Henry Bucklow, Mark Gammon", "title": "High-order curvilinear hybrid mesh generation for CFD simulations", "comments": "Pre-print accepted to the 2018 AIAA Aerospace Sciences Meeting", "journal-ref": "AIAA SciTech Forum (AIAA 2018-1403)", "doi": "10.2514/6.2018-1403", "report-no": null, "categories": "cs.GR cs.CG physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a semi-structured method for the generation of high-order hybrid\nmeshes suited for the simulation of high Reynolds number flows. This is\nachieved through the use of highly stretched elements in the viscous boundary\nlayers near the wall surfaces. CADfix is used to first repair any possible\ndefects in the CAD geometry and then generate a medial object based\ndecomposition of the domain that wraps the wall boundaries with partitions\nsuitable for the generation of either prismatic or hexahedral elements. The\nlatter is a novel distinctive feature of the method that permits to obtain\nwell-shaped hexahedral meshes at corners or junctions in the boundary layer.\nThe medial object approach allows greater control on the \"thickness\" of the\nboundary-layer mesh than is generally achievable with advancing layer\ntechniques. CADfix subsequently generates a hybrid straight sided mesh of\nprismatic and hexahedral elements in the near-field region modelling the\nboundary layer, and tetrahedral elements in the far-field region covering the\nrest of the domain. The mesh in the near-field region provides a framework that\nfacilitates the generation, via an isoparametric technique, of layers of highly\nstretched elements with a distribution of points in the direction normal to the\nwall tailored to efficiently and accurately capture the flow in the boundary\nlayer. The final step is the generation of a high-order mesh using NekMesh, a\nhigh-order mesh generator within the Nektar++ framework. NekMesh uses the\nCADfix API as a geometry engine that handles all the geometrical queries to the\nCAD geometry required during the high-order mesh generation process. We will\ndescribe in some detail the methodology using a simple geometry, a NACA wing\ntip, for illustrative purposes. Finally, we will present two examples of\napplication to reasonably complex geometries proposed by NASA as CFD validation\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 06:31:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Marcon", "Julian", ""], ["Turner", "Michael", ""], ["Peir\u00f3", "Joaquim", ""], ["Moxey", "David", ""], ["Pollard", "Claire R.", ""], ["Bucklow", "Henry", ""], ["Gammon", "Mark", ""]]}, {"id": "1901.01060", "submitter": "Marie-Julie Rakotosaona", "authors": "Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J.\n  Mitra, Maks Ovsjanikov", "title": "PointCleanNet: Learning to Denoise and Remove Outliers from Dense Point\n  Clouds", "comments": null, "journal-ref": "Computer Graphics Forum, 2020", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds obtained with 3D scanners or by image-based reconstruction\ntechniques are often corrupted with significant amount of noise and outliers.\nTraditional methods for point cloud denoising largely rely on local surface\nfitting (e.g., jets or MLS surfaces), local or non-local averaging, or on\nstatistical assumptions about the underlying noise model. In contrast, we\ndevelop a simple data-driven method for removing outliers and reducing noise in\nunordered point clouds. We base our approach on a deep learning architecture\nadapted from PCPNet, which was recently proposed for estimating local 3D shape\nproperties in point clouds. Our method first classifies and discards outlier\nsamples, and then estimates correction vectors that project noisy points onto\nthe original clean surfaces. The approach is efficient and robust to varying\namounts of noise and outliers, while being able to handle large densely-sampled\npoint clouds. In our extensive evaluation, both on synthesic and real data, we\nshow an increased robustness to strong noise levels compared to various\nstate-of-the-art methods, enabling accurate surface reconstruction from\nextremely noisy real data obtained by range scans. Finally, the simplicity and\nuniversality of our approach makes it very easy to integrate in any existing\ngeometry processing pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 11:28:26 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 16:29:25 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 15:22:52 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rakotosaona", "Marie-Julie", ""], ["La Barbera", "Vittorio", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1901.01255", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Benjamin Busam and Nassir Navab and Slobodan Ilic and\n  Peter Sturm", "title": "Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric\n  Fits", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). arXiv admin note: substantial text overlap with\n  arXiv:1803.07191", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and effective method for detecting 3D primitives in\ncluttered, unorganized point clouds, without axillary segmentation or type\nspecification. We consider the quadric surfaces for encapsulating the basic\nbuilding blocks of our environments - planes, spheres, ellipsoids, cones or\ncylinders, in a unified fashion. Moreover, quadrics allow us to model higher\ndegree of freedom shapes, such as hyperboloids or paraboloids that could be\nused in non-rigid settings.\n  We begin by contributing two novel quadric fits targeting 3D point sets that\nare endowed with tangent space information. Based upon the idea of aligning the\nquadric gradients with the surface normals, our first formulation is exact and\nrequires as low as four oriented points. The second fit approximates the first,\nand reduces the computational effort. We theoretically analyze these fits with\nrigor, and give algebraic and geometric arguments. Next, by re-parameterizing\nthe solution, we devise a new local Hough voting scheme on the null-space\ncoefficients that is combined with RANSAC, reducing the complexity from\n$O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the\nfirst method capable of performing a generic cross-type multi-object primitive\ndetection in difficult scenes without segmentation. Our extensive qualitative\nand quantitative results show that our method is efficient and flexible, as\nwell as being accurate.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:09:50 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Birdal", "Tolga", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Sturm", "Peter", ""]]}, {"id": "1901.02037", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane and Aniket Bera and Emily Kubin and Kurt Gray and\n  Dinesh Manocha", "title": "Modeling Data-Driven Dominance Traits for Virtual Characters using Gait\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven algorithm for generating gaits of virtual characters\nwith varying dominance traits. Our formulation utilizes a user study to\nestablish a data-driven dominance mapping between gaits and dominance labels.\nWe use our dominance mapping to generate walking gaits for virtual characters\nthat exhibit a variety of dominance traits while interacting with the user.\nFurthermore, we extract gait features based on known criteria in visual\nperception and psychology literature that can be used to identify the dominance\nlevels of any walking gait. We validate our mapping and the perceived dominance\ntraits by a second user study in an immersive virtual environment. Our gait\ndominance classification algorithm can classify the dominance traits of gaits\nwith ~73% accuracy. We also present an application of our approach that\nsimulates interpersonal relationships between virtual characters. To the best\nof our knowledge, ours is the first practical approach to classifying gait\ndominance and generate dominance traits in virtual characters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 19:51:43 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Kubin", "Emily", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1901.02508", "submitter": "Fereshteh Sadat Bashiri", "authors": "Fereshteh S. Bashiri, Reihaneh Rostami, Peggy Peissig, Roshan M.\n  D'Souza, Zeyun Yu", "title": "An Application of Manifold Learning in Global Shape Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid expansion of applied 3D computational vision, shape\ndescriptors have become increasingly important for a wide variety of\napplications and objects from molecules to planets. Appropriate shape\ndescriptors are critical for accurate (and efficient) shape retrieval and 3D\nmodel classification. Several spectral-based shape descriptors have been\nintroduced by solving various physical equations over a 3D surface model. In\nthis paper, for the first time, we incorporate a specific group of techniques\nin statistics and machine learning, known as manifold learning, to develop a\nglobal shape descriptor in the computer graphics domain. The proposed\ndescriptor utilizes the Laplacian Eigenmap technique in which the Laplacian\neigenvalue problem is discretized using an exponential weighting scheme. As a\nresult, our descriptor eliminates the limitations tied to the existing spectral\ndescriptors, namely dependency on triangular mesh representation and high\nintra-class quality of 3D models. We also present a straightforward\nnormalization method to obtain a scale-invariant descriptor. The extensive\nexperiments performed in this study show that the present contribution provides\na highly discriminative and robust shape descriptor under the presence of a\nhigh level of noise, random scale variations, and low sampling rate, in\naddition to the known isometric-invariance property of the Laplace-Beltrami\noperator. The proposed method significantly outperforms state-of-the-art\nalgorithms on several non-rigid shape retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:41:49 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bashiri", "Fereshteh S.", ""], ["Rostami", "Reihaneh", ""], ["Peissig", "Peggy", ""], ["D'Souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1901.02629", "submitter": "Hunmin Park", "authors": "Hunmin Park, Sung-Eui Yoon", "title": "Collaborative 3D modeling system based on blockchain", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a collaborative 3D modeling system, which is based on the\nblockchain technology. Our approach uses the blockchain to communicate with\nmodeling tools and to provide them a decentralized database of the mesh\nmodification history. This approach also provides a server-less version control\nsystem: users can commit their modifications to the blockchain and checkout\nothers' modifications from the blockchain. As a result, our system enables\nusers to do collaborative modeling without any central server.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 08:09:01 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Park", "Hunmin", ""], ["Yoon", "Sung-Eui", ""]]}, {"id": "1901.02823", "submitter": "Jozsef Molnar", "authors": "Jozsef Molnar, Michael Barbier, Winnok H. De Vos, Peter Horvath", "title": "An Elastic Energy Minimization Framework for Mean Contour Calculation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a contour mean calculation and interpolation method\ndesigned for averaging manual delineations of objects performed by experts and\ninterpolate 3D layer stack images. The proposed method retains all visible\ninformation of the input contour set: the relative positions, orientations and\nsize, but allows invisible quantities - parameterization and the centroid - to\nbe changed. The chosen representation space - the position vector rescaled by\nsquare root velocity - is a real valued vector space on which the imposed L2\nmetric is used to define the distance function. With respect to this\nrepresentation the re-parameterization group acts by isometries and the\ndistance has well defined meaning: the sum of the central second moments of the\ncoordinate functions. To identify the optimal re-parameterization system and\nproper centroid we use double energy minimization realized in a variational\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 16:51:30 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Molnar", "Jozsef", ""], ["Barbier", "Michael", ""], ["De Vos", "Winnok H.", ""], ["Horvath", "Peter", ""]]}, {"id": "1901.02875", "submitter": "Jiajun Wu", "authors": "Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T.\n  Freeman, Joshua B. Tenenbaum, Jiajun Wu", "title": "Learning to Infer and Execute 3D Shape Programs", "comments": "ICLR 2019. Project page: http://shape2prog.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human perception of 3D shapes goes beyond reconstructing them as a set of\npoints or a composition of geometric primitives: we also effortlessly\nunderstand higher-level shape structure such as the repetition and reflective\nsymmetry of object parts. In contrast, recent advances in 3D shape sensing\nfocus more on low-level geometry but less on these higher-level relationships.\nIn this paper, we propose 3D shape programs, integrating bottom-up recognition\nsystems with top-down, symbolic program structure to capture both low-level\ngeometry and high-level structural priors for 3D shapes. Because there are no\nannotations of shape programs for real shapes, we develop neural modules that\nnot only learn to infer 3D shape programs from raw, unannotated shapes, but\nalso to execute these programs for shape reconstruction. After initial\nbootstrapping, our end-to-end differentiable model learns 3D shape programs by\nreconstructing shapes in a self-supervised manner. Experiments demonstrate that\nour model accurately infers and executes 3D shape programs for highly complex\nshapes from various categories. It can also be integrated with an\nimage-to-shape module to infer 3D shape programs directly from an RGB image,\nleading to 3D shape reconstructions that are both more accurate and more\nphysically plausible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 18:55:03 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 19:37:26 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 23:07:36 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Tian", "Yonglong", ""], ["Luo", "Andrew", ""], ["Sun", "Xingyuan", ""], ["Ellis", "Kevin", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Wu", "Jiajun", ""]]}, {"id": "1901.03427", "submitter": "Kurmanbek Kaiyrbekov", "authors": "Kurmanbek Kaiyrbekov, Metin Sezgin", "title": "Stroke-based sketched symbol reconstruction and segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand-drawn objects usually consist of multiple semantically meaningful parts.\nFor example, a stick figure consists of a head, a torso, and pairs of legs and\narms. Efficient and accurate identification of these subparts promises to\nsignificantly improve algorithms for stylization, deformation, morphing and\nanimation of 2D drawings. In this paper, we propose a neural network model that\nsegments symbols into stroke-level components. Our segmentation framework has\ntwo main elements: a fixed feature extractor and a Multilayer Perceptron (MLP)\nnetwork that identifies a component based on the feature. As the feature\nextractor we utilize an encoder of a stroke-rnn, which is our newly proposed\ngenerative Variational Auto-Encoder (VAE) model that reconstructs symbols on a\nstroke by stroke basis. Experiments show that a single encoder could be reused\nfor segmenting multiple categories of sketched symbols with negligible effects\non segmentation accuracies. Our segmentation scores surpass existing\nmethodologies on an available small state of the art dataset. Moreover,\nextensive evaluations on our newly annotated big dataset demonstrate that our\nframework obtains significantly better accuracies as compared to baseline\nmodels. We release the dataset to the community.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 23:04:46 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 07:32:09 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kaiyrbekov", "Kurmanbek", ""], ["Sezgin", "Metin", ""]]}, {"id": "1901.03968", "submitter": "Sahar Yousefi", "authors": "Sahar Yousefi, M. T. Manzuri Shalmani, Antoni B. Chan", "title": "A Fully Bayesian Infinite Generative Model for Dynamic Texture\n  Segmentation", "comments": "38 pages; 15 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Generative dynamic texture models (GDTMs) are widely used for dynamic texture\n(DT) segmentation in the video sequences. GDTMs represent DTs as a set of\nlinear dynamical systems (LDSs). A major limitation of these models concerns\nthe automatic selection of a proper number of DTs. Dirichlet process mixture\n(DPM) models which have appeared recently as the cornerstone of the\nnon-parametric Bayesian statistics, is an optimistic candidate toward resolving\nthis issue. Under this motivation to resolve the aforementioned drawback, we\npropose a novel non-parametric fully Bayesian approach for DT segmentation,\nformulated on the basis of a joint DPM and GDTM construction. This interaction\ncauses the algorithm to overcome the problem of automatic segmentation\nproperly. We derive the Variational Bayesian Expectation-Maximization (VBEM)\ninference for the proposed model. Moreover, in the E-step of inference, we\napply Rauch-Tung-Striebel smoother (RTSS) algorithm on Variational Bayesian\nLDSs. Ultimately, experiments on different video sequences are performed.\nExperiment results indicate that the proposed algorithm outperforms the\nprevious methods in efficiency and accuracy noticeably.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 12:15:11 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Yousefi", "Sahar", ""], ["Shalmani", "M. T. Manzuri", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1901.04161", "submitter": "Chengzhou Tang", "authors": "Chengzhou Tang, Oliver Wang, Feng Liu, Ping Tan", "title": "Joint Stabilization and Direction of 360\\deg Videos", "comments": "Accepted to ACM Transactions on Graphics, To be presented at SIGGRAPH\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  360{\\deg} video provides an immersive experience for viewers, allowing them\nto freely explore the world by turning their head. However, creating\nhigh-quality 360{\\deg} video content can be challenging, as viewers may miss\nimportant events by looking in the wrong direction, or they may see things that\nruin the immersion, such as stitching artifacts and the film crew. We take\nadvantage of the fact that not all directions are equally likely to be\nobserved; most viewers are more likely to see content located at ``true\nnorth'', i.e. in front of them, due to ergonomic constraints. We therefore\npropose 360{\\deg} video direction, where the video is jointly optimized to\norient important events to the front of the viewer and visual clutter behind\nthem, while producing smooth camera motion. Unlike traditional video, viewers\ncan still explore the space as desired, but with the knowledge that the most\nimportant content is likely to be in front of them. Constraints can be user\nguided, either added directly on the equirectangular projection or by recording\n``guidance'' viewing directions while watching the video in a VR headset, or\nautomatically computed, such as via visual saliency or forward motion\ndirection. To accomplish this, we propose a new motion estimation technique\nspecifically designed for 360{\\deg} video which outperforms the commonly used\n5-point algorithm on wide angle video. We additionally formulate the direction\nproblem as an optimization where a novel parametrization of spherical warping\nallows us to correct for some degree of parallax effects. We compare our\napproach to recent methods that address stabilization-only and converting\n360{\\deg} video to narrow field-of-view video.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 07:13:46 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Tang", "Chengzhou", ""], ["Wang", "Oliver", ""], ["Liu", "Feng", ""], ["Tan", "Ping", ""]]}, {"id": "1901.04544", "submitter": "Matan Shoef", "authors": "Matan Shoef, Sharon Fogel and Daniel Cohen-Or", "title": "PointWise: An Unsupervised Point-wise Feature Learning Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to learning a point-wise, meaningful embedding\nfor point-clouds in an unsupervised manner, through the use of neural-networks.\nThe domain of point-cloud processing via neural-networks is rapidly evolving,\nwith novel architectures and applications frequently emerging. Within this\nfield of research, the availability and plethora of unlabeled point-clouds as\nwell as their possible applications make finding ways of characterizing this\ntype of data appealing. Though significant advancement was achieved in the\nrealm of unsupervised learning, its adaptation to the point-cloud\nrepresentation is not trivial. Previous research focuses on the embedding of\nentire point-clouds representing an object in a meaningful manner. We present a\ndeep learning framework to learn point-wise description from a set of shapes\nwithout supervision. Our approach leverages self-supervision to define a\nrelevant loss function to learn rich per-point features. We train a\nneural-network with objectives based on context derived directly from the raw\ndata, with no added annotation. We use local structures of point-clouds to\nincorporate geometric information into each point's latent representation. In\naddition to using local geometric information, we encourage adjacent points to\nhave similar representations and vice-versa, creating a smoother, more\ndescriptive representation. We demonstrate the ability of our method to capture\nmeaningful point-wise features through three applications. By clustering the\nlearned embedding space, we perform unsupervised part-segmentation on point\nclouds. By calculating euclidean distance in the latent space we derive\nsemantic point-analogies. Finally, by retrieving nearest-neighbors in our\nlearned latent space we present meaningful point-correspondence within and\namong point-clouds.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:06:28 GMT"}, {"version": "v2", "created": "Sun, 10 Mar 2019 14:18:11 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Shoef", "Matan", ""], ["Fogel", "Sharon", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1901.04686", "submitter": "Somnuk Phon-Amnuaisuk", "authors": "Somnuk Phon-Amnuaisuk", "title": "Image Synthesis and Style Transfer", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affine transformation, layer blending, and artistic filters are popular\nprocesses that graphic designers employ to transform pixels of an image to\ncreate a desired effect. Here, we examine various approaches that synthesize\nnew images: pixel-based compositing models and in particular, distributed\nrepresentations of deep neural network models. This paper focuses on\nsynthesizing new images from a learned representation model obtained from the\nVGG network. This approach offers an interesting creative process from its\ndistributed representation of information in hidden layers of a deep VGG\nnetwork i.e., information such as contour, shape, etc. are effectively captured\nin hidden layers of neural networks. Conceptually, if $\\Phi$ is the function\nthat transforms input pixels into distributed representations of VGG layers\n${\\bf h}$, a new synthesized image $X$ can be generated from its inverse\nfunction, $X = \\Phi^{-1}({\\bf h})$. We describe the concept behind the\napproach, present some representative synthesized images and style-transferred\nimage examples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 07:21:52 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Phon-Amnuaisuk", "Somnuk", ""]]}, {"id": "1901.04944", "submitter": "Jean-Emmanuel Deschaud", "authors": "Hassan Bouchiba, Simon Santoso, Jean-Emmanuel Deschaud, Luisa\n  Rocha-Da-Silva, Fran\\c{c}ois Goulette, Thierry Coupez", "title": "Computational Fluid Dynamics on 3D Point Set Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CE cs.CG physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational fluid dynamics (CFD) in many cases requires designing 3D models\nmanually, which is a tedious task that requires specific skills. In this paper,\nwe present a novel method for performing CFD directly on scanned 3D point\nclouds. The proposed method builds an anisotropic volumetric tetrahedral mesh\nadapted around a point-sampled surface, without an explicit surface\nreconstruction step. The surface is represented by a new extended implicit\nmoving least squares (EIMLS) scalar representation that extends the definition\nof the function to the entire computational domain, which makes it possible for\nuse in immersed boundary flow simulations. The workflow we present allows us to\ncompute flows around point-sampled geometries automatically. It also gives a\nbetter control of the precision around the surface with a limited number of\ncomputational nodes, which is a critical issue in CFD.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 13:14:19 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Bouchiba", "Hassan", ""], ["Santoso", "Simon", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Rocha-Da-Silva", "Luisa", ""], ["Goulette", "Fran\u00e7ois", ""], ["Coupez", "Thierry", ""]]}, {"id": "1901.05064", "submitter": "Wang Guangjun", "authors": "Guangjun Wang", "title": "A novel 3D display based on micro-volumetric scanning and real time\n  reconstruction of holograms principle", "comments": "arXiv admin note: substantial text overlap with arXiv:1706.03231", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes a novel 3D display contains a micro-volumetric\nscanning system (MVS) and a real time reconstruction hologram system (RTRH).\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 13:06:49 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Wang", "Guangjun", ""]]}, {"id": "1901.05423", "submitter": "Alexander Keller", "authors": "Nikolaus Binder and Alexander Keller", "title": "Massively Parallel Construction of Radix Tree Forests for the Efficient\n  Sampling of Discrete Probability Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare different methods for sampling from discrete probability\ndistributions and introduce a new algorithm which is especially efficient on\nmassively parallel processors, such as GPUs. The scheme preserves the\ndistribution properties of the input sequence, exposes constant time complexity\non the average, and significantly lowers the average number of operations for\ncertain distributions when sampling is performed in a parallel algorithm that\nrequires synchronization afterwards. Avoiding load balancing issues of na\\\"ive\napproaches, a very efficient massively parallel construction algorithm for the\nrequired auxiliary data structure is complemented.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 13:17:06 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 14:28:09 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Binder", "Nikolaus", ""], ["Keller", "Alexander", ""]]}, {"id": "1901.05637", "submitter": "Caigui Jiang", "authors": "Caigui Jiang, Chengcheng Tang, Hans-Peter Seidel, Renjie Chen, Peter\n  Wonka", "title": "Computational Design of Lightweight Trusses", "comments": "15 pages,34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trusses are load-carrying light-weight structures consisting of bars\nconnected at joints ubiquitously applied in a variety of engineering scenarios.\nDesigning optimal trusses that satisfy functional specifications with a minimal\namount of material has interested both theoreticians and practitioners for more\nthan a century. In this paper, we introduce two main ideas to improve upon the\nstate of the art. First, we formulate an alternating linear programming problem\nfor geometry optimization. Second, we introduce two sets of complementary\ntopological operations, including a novel subdivision scheme for global\ntopology refinement inspired by Michell's famed theoretical study. Based on\nthese two ideas, we build an efficient computational framework for the design\nof lightweight trusses. \\AD{We illustrate our framework with a variety of\nfunctional specifications and extensions. We show that our method achieves\ntrusses with smaller volumes and is over two orders of magnitude faster\ncompared with recent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 06:08:33 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Jiang", "Caigui", ""], ["Tang", "Chengcheng", ""], ["Seidel", "Hans-Peter", ""], ["Chen", "Renjie", ""], ["Wonka", "Peter", ""]]}, {"id": "1901.06034", "submitter": "Si Lu", "authors": "Si Lu", "title": "High-speed Video from Asynchronous Camera Array", "comments": "10 pages, 82 figures, Published at IEEE WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for capturing high-speed video using an\nasynchronous camera array. Our method sequentially fires each sensor in a\ncamera array with a small time offset and assembles captured frames into a\nhigh-speed video according to the time stamps. The resulting video, however,\nsuffers from parallax jittering caused by the viewpoint difference among\nsensors in the camera array. To address this problem, we develop a dedicated\nnovel view synthesis algorithm that transforms the video frames as if they were\ncaptured by a single reference sensor. Specifically, for any frame from a\nnon-reference sensor, we find the two temporally neighboring frames captured by\nthe reference sensor. Using these three frames, we render a new frame with the\nsame time stamp as the non-reference frame but from the viewpoint of the\nreference sensor. Specifically, we segment these frames into super-pixels and\nthen apply local content-preserving warping to warp them to form the new frame.\nWe employ a multi-label Markov Random Field method to blend these warped\nframes. Our experiments show that our method can produce high-quality and\nhigh-speed video of a wide variety of scenes with large parallax, scene\ndynamics, and camera motion and outperforms several baseline and\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 23:26:55 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Lu", "Si", ""]]}, {"id": "1901.06046", "submitter": "Si Lu", "authors": "Si Lu", "title": "Good Similar Patches for Image Denoising", "comments": "10 pages, 13 figures, 6 tables, IEEE WACV 2019", "journal-ref": null, "doi": "10.1109/WACV.2019.00205", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patch-based denoising algorithms like BM3D have achieved outstanding\nperformance. An important idea for the success of these methods is to exploit\nthe recurrence of similar patches in an input image to estimate the underlying\nimage structures. However, in these algorithms, the similar patches used for\ndenoising are obtained via Nearest Neighbour Search (NNS) and are sometimes not\noptimal. First, due to the existence of noise, NNS can select similar patches\nwith similar noise patterns to the reference patch. Second, the unreliable\nnoisy pixels in digital images can bring a bias to the patch searching process\nand result in a loss of color fidelity in the final denoising result. We\nobserve that given a set of good similar patches, their distribution is not\nnecessarily centered at the noisy reference patch and can be approximated by a\nGaussian component. Based on this observation, we present a patch searching\nmethod that clusters similar patch candidates into patch groups using Gaussian\nMixture Model-based clustering, and selects the patch group that contains the\nreference patch as the final patches for denoising. We also use an unreliable\npixel estimation algorithm to pre-process the input noisy images to further\nimprove the patch searching. Our experiments show that our approach can better\ncapture the underlying patch structures and can consistently enable the\nstate-of-the-art patch-based denoising algorithms, such as BM3D, LPCA and PLOW,\nto better denoise images by providing them with patches found by our approach\nwhile without modifying these algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 01:04:01 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Lu", "Si", ""]]}, {"id": "1901.06408", "submitter": "Wenshan Cai", "authors": "Shoufeng Lan, Xueyue Zhang, Mohammad Taghinejad, Sean Rodrigues,\n  Kyu-Tae Lee, Zhaocheng Liu, and Wenshan Cai", "title": "Metasurfaces for near-eye augmented reality", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) has the potential to revolutionize the way in which\ninformation is presented by overlaying virtual information onto a person's\ndirect view of their real-time surroundings. By placing the display on the\nsurface of the eye, a contact lens display (CLD) provides a versatile solution\nfor compact AR. However, an unaided human eye cannot visualize patterns on the\nCLD simply because of the limited accommodation of the eye. Here, we introduce\na holographic display technology that casts virtual information directly to the\nretina so that the eye sees it while maintaining the visualization of the\nreal-world intact. The key to our design is to introduce metasurfaces to create\na phase distribution that projects virtual information in a pixel-by-pixel\nmanner. Unlike conventional holographic techniques, our metasurface-based\ntechnique is able to display arbitrary patterns using a single passive\nhologram. With a small form-factor, the designed metasurface empowers near-eye\nAR excluding the need of extra optical elements, such as a spatial light\nmodulator, for dynamic image control.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 20:02:35 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Lan", "Shoufeng", ""], ["Zhang", "Xueyue", ""], ["Taghinejad", "Mohammad", ""], ["Rodrigues", "Sean", ""], ["Lee", "Kyu-Tae", ""], ["Liu", "Zhaocheng", ""], ["Cai", "Wenshan", ""]]}, {"id": "1901.06487", "submitter": "Sebastian Ochmann", "authors": "Sebastian Ochmann and Reinhard Klein", "title": "Automatic normal orientation in point clouds of building interiors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orienting surface normals correctly and consistently is a fundamental problem\nin geometry processing. Applications such as visualization, feature detection,\nand geometry reconstruction often rely on the availability of correctly\noriented normals. Many existing approaches for automatic orientation of normals\non meshes or point clouds make severe assumptions on the input data or the\ntopology of the underlying object which are not applicable to real-world\nmeasurements of urban scenes. In contrast, our approach is specifically\ntailored to the challenging case of unstructured indoor point cloud scans of\nmulti-story, multi-room buildings. We evaluate the correctness and speed of our\napproach on multiple real-world point cloud datasets.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 09:21:57 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 10:27:13 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Ochmann", "Sebastian", ""], ["Klein", "Reinhard", ""]]}, {"id": "1901.06931", "submitter": "Marina Alterman", "authors": "Chen Bar, Marina Alterman, Ioannis Gkioulekas, Anat Levin", "title": "A Monte Carlo Framework for Rendering Speckle Statistics in Scattering\n  Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Monte Carlo rendering framework for the physically-accurate\nsimulation of speckle patterns arising from volumetric scattering of coherent\nwaves. These noise-like patterns are characterized by strong statistical\nproperties, such as the so-called memory effect, which are at the core of\nimaging techniques for applications as diverse as tissue imaging, motion\ntracking, and non-line-of-sight imaging. Our framework allows for these\nproperties to be replicated computationally, in a way that is orders of\nmagnitude more efficient than alternatives based on directly solving the wave\nequations. At the core of our framework is a path-space formulation for the\ncovariance of speckle patterns arising from a scattering volume, which we\nderive from first principles. We use this formulation to develop two Monte\nCarlo rendering algorithms, for computing speckle covariance as well as\ndirectly speckle fields. While approaches based on wave equation solvers\nrequire knowing the microscopic position of wavelength-sized scatterers, our\napproach takes as input only bulk parameters describing the statistical\ndistribution of these scatterers inside a volume. We validate the accuracy of\nour framework by comparing against speckle patterns simulated using wave\nequation solvers, use it to simulate memory effect observations that were\npreviously only possible through lab measurements, and demonstrate its\napplicability for computational imaging tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:36:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Bar", "Chen", ""], ["Alterman", "Marina", ""], ["Gkioulekas", "Ioannis", ""], ["Levin", "Anat", ""]]}, {"id": "1901.07165", "submitter": "Kentaro Fukamizu", "authors": "Kentaro Fukamizu, Masaaki Kondo, Ryuichi Sakamoto", "title": "Generation High resolution 3D model from natural language by Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of generating high resolution 3D shapes from natural\nlanguage descriptions. To achieve this goal, we propose two steps that\ngenerating low resolution shapes which roughly reflect texts and generating\nhigh resolution shapes which reflect the detail of texts. In a previous paper,\nthe authors have shown a method of generating low resolution shapes. We improve\nit to generate 3D shapes more faithful to natural language and test the\neffectiveness of the method. To generate high resolution 3D shapes, we use the\nframework of Conditional Wasserstein GAN. We propose two roles of Critic\nseparately, which calculate the Wasserstein distance between two probability\ndistribution, so that we achieve generating high quality shapes or acceleration\nof learning speed of model. To evaluate our approach, we performed quantitive\nevaluation with several numerical metrics for Critic models. Our method is\nfirst to realize the generation of high quality model by propagating text\nembedding information to high resolution task when generating 3D model.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 03:50:59 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Fukamizu", "Kentaro", ""], ["Kondo", "Masaaki", ""], ["Sakamoto", "Ryuichi", ""]]}, {"id": "1901.08397", "submitter": "Zhiyong Yuan", "authors": "Xuejie Mai, Zhiyong Yuan, Qianqian Tong, Tianchen Yuan, and Jianhui\n  Zhao", "title": "Periodic-corrected data driven coupling of blood flow and vessel wall\n  for virtual surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and realistic coupling of blood flow and vessel wall is of great\nimportance to virtual surgery. In this paper, we propose a novel data-driven\ncoupling method that formulates physics-based blood flow simulation as a\nregression problem, using an improved periodic-corrected neural network\n(PcNet), estimating the acceleration of every particle at each frame to obtain\nfast, stable and realistic simulation. We design a particle state feature\nvector based on smoothed particle hydrodynamics (SPH), modeling the mixed\ncontribution of neighboring proxy particles on the blood vessel wall and\nneighboring blood particles, giving the extrapolation ability to deal with more\ncomplex couplings. We present a semi-supervised training strategy to improve\nthe traditional BP neural network, which corrects the error periodically to\nensure long term stability. Experimental results demonstrate that our method is\nable to implement stable and vivid coupling of blood flow and vessel wall while\ngreatly improving computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 13:32:03 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Mai", "Xuejie", ""], ["Yuan", "Zhiyong", ""], ["Tong", "Qianqian", ""], ["Yuan", "Tianchen", ""], ["Zhao", "Jianhui", ""]]}, {"id": "1901.08419", "submitter": "Michal Mackiewicz", "authors": "Michal Mackiewicz, Hans Jakob Rivertz, Graham D. Finlayson", "title": "Spherical sampling methods for the calculation of metamer mismatch\n  volumes", "comments": "One print or electronic copy may be made for personal use only.\n  Systematic reproduction and distribution, duplication of any material in this\n  paper for a fee or for commercial purposes, or modifications of this paper\n  are prohibited. Optical Society of America", "journal-ref": "Vol. 36, No. 1 / Jan 2019 / Journal of the Optical Society of\n  America A", "doi": "10.1364/JOSAA.36.000096", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two methods of calculating theoretically maximal\nmetamer mismatch volumes. Unlike prior art techniques, our methods do not make\nany assumptions on the shape of spectra on the boundary of the mismatch\nvolumes. Both methods utilize a spherical sampling approach, but they calculate\nmismatch volumes in two different ways. The first method uses a linear\nprogramming optimization, while the second is a computational geometry approach\nbased on half-space intersection. We show that under certain conditions the\ntheoretically maximal metamer mismatch volume is significantly larger than the\none approximated using a prior art method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 18:33:05 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Mackiewicz", "Michal", ""], ["Rivertz", "Hans Jakob", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "1901.10024", "submitter": "Ben Usman", "authors": "Ben Usman, Nick Dufour, Kate Saenko, Chris Bregler", "title": "Cross-Domain Image Manipulation by Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a model that can manipulate individual visual\nattributes of objects in a real scene using examples of how respective\nattribute manipulations affect the output of a simulation. As an example, we\ntrain our model to manipulate the expression of a human face using\nnonphotorealistic 3D renders of a face with varied expression. Our model\nmanages to preserve all other visual attributes of a real face, such as head\norientation, even though this and other attributes are not labeled in either\nreal or synthetic domain. Since our model learns to manipulate a specific\nproperty in isolation using only \"synthetic demonstrations\" of such\nmanipulations without explicitly provided labels, it can be applied to shape,\ntexture, lighting, and other properties that are difficult to measure or\nrepresent as real-valued vectors. We measure the degree to which our model\npreserves other attributes of a real image when a single specific attribute is\nmanipulated. We use digit datasets to analyze how discrepancy in attribute\ndistributions affects the performance of our model, and demonstrate results in\na far more difficult setting: learning to manipulate real human faces using\nnonphotorealistic 3D renders.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 22:49:05 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 15:28:10 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Usman", "Ben", ""], ["Dufour", "Nick", ""], ["Saenko", "Kate", ""], ["Bregler", "Chris", ""]]}]