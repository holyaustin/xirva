[{"id": "1704.00090", "submitter": "Marc-Andr\\'e Gardner", "authors": "Marc-Andr\\'e Gardner, Kalyan Sunkavalli, Ersin Yumer, Xiaohui Shen,\n  Emiliano Gambaretto, Christian Gagn\\'e, Jean-Fran\\c{c}ois Lalonde", "title": "Learning to Predict Indoor Illumination from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automatic method to infer high dynamic range illumination from\na single, limited field-of-view, low dynamic range photograph of an indoor\nscene. In contrast to previous work that relies on specialized image capture,\nuser input, and/or simple scene models, we train an end-to-end deep neural\nnetwork that directly regresses a limited field-of-view photo to HDR\nillumination, without strong assumptions on scene geometry, material\nproperties, or lighting. We show that this can be accomplished in a three step\nprocess: 1) we train a robust lighting classifier to automatically annotate the\nlocation of light sources in a large dataset of LDR environment maps, 2) we use\nthese annotations to train a deep neural network that predicts the location of\nlights in a scene from a single limited field-of-view photo, and 3) we\nfine-tune this network using a small dataset of HDR environment maps to predict\nlight intensities. This allows us to automatically recover high-quality HDR\nillumination estimates that significantly outperform previous state-of-the-art\nmethods. Consequently, using our illumination estimates for applications like\n3D object insertion, we can achieve results that are photo-realistic, which is\nvalidated via a perceptual user study.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 00:50:12 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 19:20:01 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 08:32:24 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Gardner", "Marc-Andr\u00e9", ""], ["Sunkavalli", "Kalyan", ""], ["Yumer", "Ersin", ""], ["Shen", "Xiaohui", ""], ["Gambaretto", "Emiliano", ""], ["Gagn\u00e9", "Christian", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1704.02525", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Chris H. Rycroft", "title": "Density-equalizing maps for simply-connected open surfaces", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 11, 1134-1178 (2018)", "doi": "10.1137/17M1124796", "report-no": null, "categories": "cs.CG cs.GR math.DG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the problem of creating flattening maps\nof simply-connected open surfaces in $\\mathbb{R}^3$. Using a natural principle\nof density diffusion in physics, we propose an effective algorithm for\ncomputing density-equalizing flattening maps with any prescribed density\ndistribution. By varying the initial density distribution, a large variety of\nmappings with different properties can be achieved. For instance,\narea-preserving parameterizations of simply-connected open surfaces can be\neasily computed. Experimental results are presented to demonstrate the\neffectiveness of our proposed method. Applications to data visualization and\nsurface remeshing are explored.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 19:08:08 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Rycroft", "Chris H.", ""]]}, {"id": "1704.02724", "submitter": "Byungmoon Kim Dr", "authors": "Yeojin Kim, Byungmoon Kim, Jiyang Kim, Young J. Kim", "title": "CanvoX: High-resolution VR Painting in Large Volumetric Canvas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  With virtual reality, digital painting on 2D canvases is now being extended\nto 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as\ntools that pave the way to a new form of art - 3D emmersive painting. Current\n3D painting systems are only a start, emitting textured triangular geometries.\nIn this paper, we advance this new art of 3D painting to 3D volumetric painting\nthat enables an artist to draw a huge scene with full control of spatial color\nfields. Inspired by the fact that 2D paintings often use vast space to paint\nbackground and small but detailed space for foreground, we claim that\nsupporting a large canvas in varying detail is essential for 3D painting. In\norder to help artists focus and audiences to navigate the large canvas space,\nwe provide small artist-defined areas, called rooms, that serve as beacons for\nartist-suggested scales, spaces, locations for intended appreciation view of\nthe painting. Artists and audiences can easily transport themselves between\ndifferent rooms. Technically, our canvas is represented as an array of deep\noctrees of depth 24 or higher, built on CPU for volume painting and on GPU for\nvolume rendering using accurate ray casting. In CPU side, we design an\nefficient iterative algorithm to refine or coarsen octree, as a result of\nvolumetric painting strokes, at highly interactive rates, and update the\ncorresponding GPU textures. Then we use GPU-based ray casting algorithms to\nrender the volumetric painting result. We explore precision issues stemming\nfrom ray-casting the octree of high depth, and provide a new analysis and\nverification. From our experimental results as well as the positive feedback\nfrom the participating artists, we strongly believe that our new 3D volume\npainting system can open up a new possibility for VR-driven digital art medium\nto professional artists as well as to novice users.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 06:40:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Kim", "Yeojin", ""], ["Kim", "Byungmoon", ""], ["Kim", "Jiyang", ""], ["Kim", "Young J.", ""]]}, {"id": "1704.02897", "submitter": "Daisuke Iwai", "authors": "Daisuke Iwai", "title": "Projection Mapping Technologies for AR", "comments": "3 pages, 1 figure, 23rd International Display Workshops in\n  conjunction with Asia Display 2016. arXiv admin note: substantial text\n  overlap with arXiv:1510.02710", "journal-ref": "Proceedings of International Display Workshops (IDW), pp.\n  1076-1078, 2016", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This invited talk will present recent projection mapping technologies for\naugmented reality. First, fundamental technologies are briefly explained, which\nhave been proposed to overcome the technical limitations of ordinary\nprojectors. Second, augmented reality (AR) applications using projection\nmapping technologies are introduced.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:13:40 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Iwai", "Daisuke", ""]]}, {"id": "1704.02906", "submitter": "Viveka Kulharia", "authors": "Arnab Ghosh and Viveka Kulharia and Vinay Namboodiri and Philip H. S.\n  Torr and Puneet K. Dokania", "title": "Multi-Agent Diverse Generative Adversarial Networks", "comments": "This is an updated version of our CVPR'18 paper with the same title.\n  In this version, we also introduce MAD-GAN-Sim in Appendix B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MAD-GAN, an intuitive generalization to the Generative Adversarial\nNetworks (GANs) and its conditional variants to address the well known problem\nof mode collapse. First, MAD-GAN is a multi-agent GAN architecture\nincorporating multiple generators and one discriminator. Second, to enforce\nthat different generators capture diverse high probability modes, the\ndiscriminator of MAD-GAN is designed such that along with finding the real and\nfake samples, it is also required to identify the generator that generated the\ngiven fake sample. Intuitively, to succeed in this task, the discriminator must\nlearn to push different generators towards different identifiable modes. We\nperform extensive experiments on synthetic and real datasets and compare\nMAD-GAN with different variants of GAN. We show high quality diverse sample\ngenerations for challenging tasks such as image-to-image translation and face\ngeneration. In addition, we also show that MAD-GAN is able to disentangle\ndifferent modalities when trained using highly challenging diverse-class\ndataset (e.g. dataset with images of forests, icebergs, and bedrooms). In the\nend, we show its efficacy on the unsupervised feature representation task. In\nAppendix, we introduce a similarity based competing objective (MAD-GAN-Sim)\nwhich encourages different generators to generate diverse samples based on a\nuser defined similarity metric. We show its performance on the image-to-image\ntranslation, and also show its effectiveness on the unsupervised feature\nrepresentation task.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 23:29:16 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 16:21:52 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Arnab", ""], ["Kulharia", "Viveka", ""], ["Namboodiri", "Vinay", ""], ["Torr", "Philip H. S.", ""], ["Dokania", "Puneet K.", ""]]}, {"id": "1704.03140", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Yu Hin Lai, Lok Ming Lui", "title": "Restoration of Atmospheric Turbulence-distorted Images via RPCA and\n  Quasiconformal Maps", "comments": "21 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of restoring a high-quality image from an observed\nimage sequence strongly distorted by atmospheric turbulence. A novel algorithm\nis proposed in this paper to reduce geometric distortion as well as\nspace-and-time-varying blur due to strong turbulence. By considering a suitable\nenergy functional, our algorithm first obtains a sharp reference image and a\nsubsampled image sequence containing sharp and mildly distorted image frames\nwith respect to the reference image. The subsampled image sequence is then\nstabilized by applying the Robust Principal Component Analysis (RPCA) on the\ndeformation fields between image frames and warping the image frames by a\nquasiconformal map associated with the low-rank part of the deformation matrix.\nAfter image frames are registered to the reference image, the low-rank part of\nthem are deblurred via a blind deconvolution, and the deblurred frames are then\nfused with the enhanced sparse part. Experiments have been carried out on both\nsynthetic and real turbulence-distorted video. Results demonstrate that our\nmethod is effective in alleviating distortions and blur, restoring image\ndetails and enhancing visual quality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 04:24:44 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 03:40:28 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Lau", "Chun Pong", ""], ["Lai", "Yu Hin", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1704.03375", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Reconstruction of~3-D Rigid Smooth Curves Moving Free when Two Traceable\n  Points Only are Available", "comments": null, "journal-ref": "Preliminaru version of the paper M.A. K{\\l}opotek: Reconstruction\n  of 3-D rigid smooth curves moving free when two traceable points only are\n  available. Machine Graphics \\& Vision 1(1992)1-2, pp. 392-405", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends previous research in that sense that for orthogonal\nprojections of rigid smooth (true-3D) curves moving totally free it reduces the\nnumber of required traceable points to two only (the best results known so far\nto the author are 3 points from free motion and 2 for motion restricted to\nrotation around a fixed direction and and 2 for motion restricted to influence\nof a homogeneous force field). The method used is exploitation of information\non tangential projections. It discusses also possibility of simplification of\nreconstruction of flat curves moving free for prospective projections.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 15:48:56 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1704.04038", "submitter": "Man Kit Lau", "authors": "Siu-Wing Cheng and Man-Kit Lau", "title": "Denoising a Point Cloud for Surface Reconstruction", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface reconstruction from an unorganized point cloud is an important\nproblem due to its widespread applications. White noise, possibly clustered\noutliers, and noisy perturbation may be generated when a point cloud is sampled\nfrom a surface. Most existing methods handle limited amount of noise. We\ndevelop a method to denoise a point cloud so that the users can run their\nsurface reconstruction codes or perform other analyses afterwards. Our\nexperiments demonstrate that our method is computationally efficient and it has\nsignificantly better noise handling ability than several existing surface\nreconstruction codes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:03:28 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 06:39:53 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Cheng", "Siu-Wing", ""], ["Lau", "Man-Kit", ""]]}, {"id": "1704.04456", "submitter": "Kiwon Um", "authors": "Kiwon Um, Xiangyu Hu, Nils Thuerey", "title": "Liquid Splash Modeling with Neural Networks", "comments": "to appear in Computer Graphics Forum, more information:\n  https://ge.in.tum.de/publications/2018-mlflip-um/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new data-driven approach to model detailed splashes for\nliquid simulations with neural networks. Our model learns to generate\nsmall-scale splash detail for the fluid-implicit-particle method using training\ndata acquired from physically parametrized, high resolution simulations. We use\nneural networks to model the regression of splash formation using a classifier\ntogether with a velocity modifier. For the velocity modification, we employ a\nheteroscedastic model. We evaluate our method for different spatial scales,\nsimulation setups, and solvers. Our simulation results demonstrate that our\nmodel significantly improves visual fidelity with a large amount of realistic\ndroplet formation and yields splash detail much more efficiently than finer\ndiscretizations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:28:37 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 17:19:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Um", "Kiwon", ""], ["Hu", "Xiangyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1704.04610", "submitter": "Raj Gupta", "authors": "Raj Kumar Gupta and Alex Yong-Sang Chia and Deepu Rajan and Huang\n  Zhiyong", "title": "A learning-based approach for automatic image and video colorization", "comments": "Computer Graphics International - 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a color transfer algorithm to colorize a broad\nrange of gray images without any user intervention. The algorithm uses a\nmachine learning-based approach to automatically colorize grayscale images. The\nalgorithm uses the superpixel representation of the reference color images to\nlearn the relationship between different image features and their corresponding\ncolor values. We use this learned information to predict the color value of\neach grayscale image superpixel. As compared to processing individual image\npixels, our use of superpixels helps us to achieve a much higher degree of\nspatial consistency as well as speeds up the colorization process. The\npredicted color values of the gray-scale image superpixels are used to provide\na 'micro-scribble' at the centroid of the superpixels. These color scribbles\nare refined by using a voting based approach. To generate the final\ncolorization result, we use an optimization-based approach to smoothly spread\nthe color scribble across all pixels within a superpixel. Experimental results\non a broad range of images and the comparison with existing state-of-the-art\ncolorization methods demonstrate the greater effectiveness of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 09:21:57 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Gupta", "Raj Kumar", ""], ["Chia", "Alex Yong-Sang", ""], ["Rajan", "Deepu", ""], ["Zhiyong", "Huang", ""]]}, {"id": "1704.05368", "submitter": "Jan Egger", "authors": "Jan Egger, Dieter Schmalstieg, Xiaojun Chen, Wolfram G. Zoller,\n  Alexander Hann", "title": "Interactive Outlining of Pancreatic Cancer Liver Metastases in\n  Ultrasound Images", "comments": "15 pages, 16 figures, 2 tables, 58 references", "journal-ref": "Sci. Rep. 7, 892, 2017", "doi": "10.1038/s41598-017-00940-z", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is the most commonly used liver imaging modality worldwide.\nDue to its low cost, it is increasingly used in the follow-up of cancer\npatients with metastases localized in the liver. In this contribution, we\npresent the results of an interactive segmentation approach for liver\nmetastases in US acquisitions. A (semi-) automatic segmentation is still very\nchallenging because of the low image quality and the low contrast between the\nmetastasis and the surrounding liver tissue. Thus, the state of the art in\nclinical practice is still manual measurement and outlining of the metastases\nin the US images. We tackle the problem by providing an interactive\nsegmentation approach providing real-time feedback of the segmentation results.\nThe approach has been evaluated with typical US acquisitions from the clinical\nroutine, and the datasets consisted of pancreatic cancer metastases. Even for\ndifficult cases, satisfying segmentations results could be achieved because of\nthe interactive real-time behavior of the approach. In total, 40 clinical\nimages have been evaluated with our method by comparing the results against\nmanual ground truth segmentations. This evaluation yielded to an average Dice\nScore of 85% and an average Hausdorff Distance of 13 pixels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 14:45:20 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Egger", "Jan", ""], ["Schmalstieg", "Dieter", ""], ["Chen", "Xiaojun", ""], ["Zoller", "Wolfram G.", ""], ["Hann", "Alexander", ""]]}, {"id": "1704.06192", "submitter": "Daniel Reiter Horn", "authors": "Daniel Reiter Horn, Ken Elkabany, Chris Lesniewski-Laas, Keith\n  Winstein", "title": "The Design, Implementation, and Deployment of a System to Transparently\n  Compress Hundreds of Petabytes of Image Files for a File-Storage Service", "comments": "12 pages", "journal-ref": "Proc. NSDI 2017, Boston. p1-15", "doi": null, "report-no": null, "categories": "cs.MM cs.DC cs.GR cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We report the design, implementation, and deployment of Lepton, a\nfault-tolerant system that losslessly compresses JPEG images to 77% of their\noriginal size on average. Lepton replaces the lowest layer of baseline JPEG\ncompression-a Huffman code-with a parallelized arithmetic code, so that the\nexact bytes of the original JPEG file can be recovered quickly. Lepton matches\nthe compression efficiency of the best prior work, while decoding more than\nnine times faster and in a streaming manner. Lepton has been released as\nopen-source software and has been deployed for a year on the Dropbox\nfile-storage backend. As of February 2017, it had compressed more than 203 PiB\nof user JPEG files, saving more than 46 PiB.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 00:38:30 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Horn", "Daniel Reiter", ""], ["Elkabany", "Ken", ""], ["Lesniewski-Laas", "Chris", ""], ["Winstein", "Keith", ""]]}, {"id": "1704.06835", "submitter": "Benedikt Bitterli", "authors": "Benedikt Bitterli and Wenzel Jakob and Jan Nov\\'ak and Wojciech Jarosz", "title": "Reversible Jump Metropolis Light Transport using Inverse Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Markov Chain Monte Carlo (MCMC) methods operating in primary sample\nspace and their interactions with multiple sampling techniques. We observe that\nincorporating the sampling technique into the state of the Markov Chain, as\ndone in Multiplexed Metropolis Light Transport (MMLT), impedes the ability of\nthe chain to properly explore the path space, as transitions between sampling\ntechniques lead to disruptive alterations of path samples. To address this\nissue, we reformulate Multiplexed MLT in the Reversible Jump MCMC framework\n(RJMCMC) and introduce inverse sampling techniques that turn light paths into\nthe random numbers that would produce them. This allows us to formulate a novel\nperturbation that can locally transition between sampling techniques without\nchanging the geometry of the path, and we derive the correct acceptance\nprobability using RJMCMC. We investigate how to generalize this concept to\nnon-invertible sampling techniques commonly found in practice, and introduce\nprobabilistic inverses that extend our perturbation to cover most sampling\nmethods found in light transport simulations. Our theory reconciles the\ninverses with RJMCMC yielding an unbiased algorithm, which we call Reversible\nJump MLT (RJMLT). We verify the correctness of our implementation in canonical\nand practical scenarios and demonstrate improved temporal coherence, decrease\nin structured artifacts, and faster convergence on a wide variety of scenes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 18:57:23 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bitterli", "Benedikt", ""], ["Jakob", "Wenzel", ""], ["Nov\u00e1k", "Jan", ""], ["Jarosz", "Wojciech", ""]]}, {"id": "1704.06873", "submitter": "Keenan Crane", "authors": "Rohan Sawhney and Keenan Crane", "title": "Boundary First Flattening", "comments": "13 pages", "journal-ref": "ACM Trans. Graph. 37 (1), 2017", "doi": "10.1145/3132705", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conformal flattening maps a curved surface to the plane without distorting\nangles---such maps have become a fundamental building block for problems in\ngeometry processing, numerical simulation, and computational design. Yet\nexisting methods provide little direct control over the shape of the flattened\ndomain, or else demand expensive nonlinear optimization. Boundary first\nflattening (BFF) is a linear method for conformal parameterization which is\nfaster than traditional linear methods, yet provides control and quality\ncomparable to sophisticated nonlinear schemes. The key insight is that the\nboundary data for many conformal mapping problems can be efficiently\nconstructed via the Cherrier formula together with a pair of Poincare-Steklov\noperators; once the boundary is known, the map can be easily extended over the\nrest of the domain. Since computation demands only a single factorization of\nthe real Laplace matrix, the amortized cost is about 50x less than any\npreviously published technique for boundary-controlled conformal flattening. As\na result, BFF opens the door to real-time editing or fast optimization of\nhigh-resolution maps, with direct control over boundary length or angle. We\nshow how this method can be used to construct maps with sharp corners, cone\nsingularities, minimal area distortion, and uniformization over the unit disk;\nwe also demonstrate for the first time how a surface can be conformally\nflattened directly onto any given target shape.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 02:43:33 GMT"}, {"version": "v2", "created": "Sat, 27 Jan 2018 15:39:25 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sawhney", "Rohan", ""], ["Crane", "Keenan", ""]]}, {"id": "1704.07528", "submitter": "Yeong Won Kim", "authors": "Yeong Won Kim, Dae-Yong Jo, Chang-Ryeol Lee, Hyeok-Jae Choi, Yong Hoon\n  Kwon and Kuk-Jin Yoon", "title": "Automatic Content-aware Projection for 360{\\deg} Videos", "comments": "Accepted to International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To watch 360{\\deg} videos on normal 2D displays, we need to project the\nselected part of the 360{\\deg} image onto the 2D display plane. In this paper,\nwe propose a fully-automated framework for generating content-aware 2D\nnormal-view perspective videos from 360{\\deg} videos. Especially, we focus on\nthe projection step preserving important image contents and reducing image\ndistortion. Basically, our projection method is based on Pannini projection\nmodel. At first, the salient contents such as linear structures and salient\nregions in the image are preserved by optimizing the single Panini projection\nmodel. Then, the multiple Panini projection models at salient regions are\ninterpolated to suppress image distortion globally. Finally, the temporal\nconsistency for image projection is enforced for producing temporally stable\nnormal-view videos. Our proposed projection method does not require any\nuser-interaction and is much faster than previous content-preserving methods.\nIt can be applied to not only images but also videos taking the temporal\nconsistency of projection into account. Experiments on various 360{\\deg} videos\nshow the superiority of the proposed projection method quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 03:44:29 GMT"}, {"version": "v2", "created": "Sun, 10 Sep 2017 14:49:15 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Kim", "Yeong Won", ""], ["Jo", "Dae-Yong", ""], ["Lee", "Chang-Ryeol", ""], ["Choi", "Hyeok-Jae", ""], ["Kwon", "Yong Hoon", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1704.07854", "submitter": "Lukas Prantl", "authors": "Lukas Prantl, Boris Bonev, Nils Thuerey", "title": "Generating Liquid Simulations with Deformation-aware Neural Networks", "comments": "ICLR 2019, further information and videos at\n  https://ge.in.tum.de/publications/2017-prantl-defonn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for deformation-aware neural networks that learn\nthe weighting and synthesis of dense volumetric deformation fields. Our method\nspecifically targets the space-time representation of physical surfaces from\nliquid simulations. Liquids exhibit highly complex, non-linear behavior under\nchanging simulation conditions such as different initial conditions. Our\nalgorithm captures these complex phenomena in two stages: a first neural\nnetwork computes a weighting function for a set of pre-computed deformations,\nwhile a second network directly generates a deformation field for refining the\nsurface. Key for successful training runs in this setting is a suitable loss\nfunction that encodes the effect of the deformations, and a robust calculation\nof the corresponding gradients. To demonstrate the effectiveness of our\napproach, we showcase our method with several complex examples of flowing\nliquids with topology changes. Our representation makes it possible to rapidly\ngenerate the desired implicit surfaces. We have implemented a mobile\napplication to demonstrate that real-time interactions with complex liquid\neffects are possible with our approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:21:42 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 14:54:16 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 15:05:01 GMT"}, {"version": "v4", "created": "Wed, 20 Feb 2019 13:27:28 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Prantl", "Lukas", ""], ["Bonev", "Boris", ""], ["Thuerey", "Nils", ""]]}, {"id": "1704.08049", "submitter": "Daniela Cabiddu", "authors": "Daniela Cabiddu and Marco Attene", "title": "Epsilon-shapes: characterizing, detecting and thickening thin features\n  in geometric models", "comments": null, "journal-ref": "In Computers & Graphics, Volume 66, 2017, Pages 143-153, ISSN\n  0097-8493", "doi": "10.1016/j.cag.2017.05.014", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the analysis of planar shapes and solid objects having thin\nfeatures and propose a new mathematical model to characterize them. Based on\nour model, that we call an epsilon-shape, we show how thin parts can be\neffectively and efficiently detected by an algorithm, and propose a novel\napproach to thicken these features while leaving all the other parts of the\nshape unchanged. When compared with state-of-the-art solutions, our proposal\nproves to be particularly flexible, efficient and stable, and does not require\nany unintuitive parameter to fine-tune the process. Furthermore, our method is\nable to detect thin features both in the object and in its complement, thus\nproviding a useful tool to detect thin cavities and narrow channels. We discuss\nthe importance of this kind of analysis in the design of robust structures and\nin the creation of geometry to be fabricated with modern additive manufacturing\ntechnology.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:33:15 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 13:46:41 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Cabiddu", "Daniela", ""], ["Attene", "Marco", ""]]}, {"id": "1704.08657", "submitter": "David Barina", "authors": "David Barina and Michal Kula and Michal Matysek and Pavel Zemcik", "title": "Accelerating Discrete Wavelet Transforms on Parallel Architectures", "comments": "submitted on WSCG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2-D discrete wavelet transform (DWT) can be found in the heart of many\nimage-processing algorithms. Until recently, several studies have compared the\nperformance of such transform on various shared-memory parallel architectures,\nespecially on graphics processing units (GPUs). All these studies, however,\nconsidered only separable calculation schemes. We show that corresponding\nseparable parts can be merged into non-separable units, which halves the number\nof steps. In addition, we introduce an optional optimization approach leading\nto a reduction in the number of arithmetic operations. The discussed schemes\nwere adapted on the OpenCL framework and pixel shaders, and then evaluated\nusing GPUs of two biggest vendors. We demonstrate the performance of the\nproposed non-separable methods by comparison with existing separable schemes.\nThe non-separable schemes outperform their separable counterparts on numerous\nsetups, especially considering the pixel shaders.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 17:01:07 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 17:39:27 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Barina", "David", ""], ["Kula", "Michal", ""], ["Matysek", "Michal", ""], ["Zemcik", "Pavel", ""]]}]