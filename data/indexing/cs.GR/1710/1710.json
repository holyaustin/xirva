[{"id": "1710.00546", "submitter": "Alan Brunton", "authors": "Philipp Urban, Tejas Madan Tanksale, Alan Brunton, Bui Minh Vu,\n  Shigeki Nakauchi", "title": "Redefining A in RGBA: Towards a Standard for Graphical 3D Printing", "comments": "20 pages (incl. appendices), 20 figures. Version with higher quality\n  images: https://cloud-ext.igd.fraunhofer.de/s/pAMH67XjstaNcrF (main article)\n  and https://cloud-ext.igd.fraunhofer.de/s/4rR5bH3FMfNsS5q (appendix).\n  Supplemental material including code:\n  https://cloud-ext.igd.fraunhofer.de/s/9BrZaj5Uh5d0cOU/download", "journal-ref": "ACM Transactions on Graphics, 38(3): Article 21, May 2019", "doi": "10.1145/3319910", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in multimaterial 3D printing have the potential to reproduce various\nvisual appearance attributes of an object in addition to its shape. Since many\nexisting 3D file formats encode color and translucency by RGBA textures mapped\nto 3D shapes, RGBA information is particularly important for practical\napplications. In contrast to color (encoded by RGB), which is specified by the\nobject's reflectance, selected viewing conditions and a standard observer,\ntranslucency (encoded by A) is neither linked to any measurable physical nor\nperceptual quantity. Thus, reproducing translucency encoded by A is open for\ninterpretation.\n  In this paper, we propose a rigorous definition for A suitable for use in\ngraphical 3D printing, which is independent of the 3D printing hardware and\nsoftware, and which links both optical material properties and perceptual\nuniformity for human observers. By deriving our definition from the absorption\nand scattering coefficients of virtual homogeneous reference materials with an\nisotropic phase function, we achieve two important properties. First, a simple\nadjustment of A is possible, which preserves the translucency appearance if an\nobject is re-scaled for printing. Second, determining the value of A for a real\n(potentially non-homogeneous) material, can be achieved by minimizing a\ndistance function between light transport measurements of this material and\nsimulated measurements of the reference materials. Such measurements can be\nconducted by commercial spectrophotometers used in graphic arts.\n  Finally, we conduct visual experiments employing the method of constant\nstimuli, and derive from them an embedding of A into a nearly perceptually\nuniform scale of translucency for the reference materials.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 09:10:25 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 09:23:54 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Urban", "Philipp", ""], ["Tanksale", "Tejas Madan", ""], ["Brunton", "Alan", ""], ["Vu", "Bui Minh", ""], ["Nakauchi", "Shigeki", ""]]}, {"id": "1710.01052", "submitter": "Martin Hahner", "authors": "Martin Hahner and Orestis Varesis and Panagiotis Bountouris", "title": "Simulating Structure-from-Motion", "comments": "This paper was written as part of a group project in the \"3D Vision\"\n  course at ETH Z\\\"urich in Spring semester 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of a Structure-from-Motion (SfM) pipeline from a\nsynthetically generated scene as well as the investigation of the faithfulness\nof diverse reconstructions is the subject of this project. A series of\ndifferent SfM reconstructions are implemented and their camera pose estimations\nare being contrasted with their respective ground truth locations. Finally,\ninjection of ground truth location data into the rendered images in order to\nreduce the estimation error of the camera poses is studied as well.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 09:34:38 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hahner", "Martin", ""], ["Varesis", "Orestis", ""], ["Bountouris", "Panagiotis", ""]]}, {"id": "1710.01802", "submitter": "Wenbin Li", "authors": "Rui Tang and Yuhan Wang and Darren Cosker and Wenbin Li", "title": "Automatic Structural Scene Digitalization", "comments": "paper submitted to PloS One", "journal-ref": null, "doi": "10.1371/journal.pone.0187513", "report-no": null, "categories": "cs.GR cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present an automatic system for the analysis and labeling\nof structural scenes, floor plan drawings in Computer-aided Design (CAD)\nformat. The proposed system applies a fusion strategy to detect and recognize\nvarious components of CAD floor plans, such as walls, doors, windows and other\nambiguous assets. Technically, a general rule-based filter parsing method is\nfist adopted to extract effective information from the original floor plan.\nThen, an image-processing based recovery method is employed to correct\ninformation extracted in the first step. Our proposed method is fully automatic\nand real-time. Such analysis system provides high accuracy and is also\nevaluated on a public website that, on average, archives more than ten\nthousands effective uses per day and reaches a relatively high satisfaction\nrate.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 22:57:31 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Tang", "Rui", ""], ["Wang", "Yuhan", ""], ["Cosker", "Darren", ""], ["Li", "Wenbin", ""]]}, {"id": "1710.02173", "submitter": "Cagatay Demiralp", "authors": "\\c{C}a\\u{g}atay Demiralp", "title": "Clustrophile: A Tool for Visual Clustering Analysis", "comments": "KDD IDEA'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While clustering is one of the most popular methods for data mining, analysts\nlack adequate tools for quick, iterative clustering analysis, which is\nessential for hypothesis generation and data reasoning. We introduce\nClustrophile, an interactive tool for iteratively computing discrete and\ncontinuous data clusters, rapidly exploring different choices of clustering\nparameters, and reasoning about clustering instances in relation to data\ndimensions. Clustrophile combines three basic visualizations -- a table of raw\ndatasets, a scatter plot of planar projections, and a matrix diagram (heatmap)\nof discrete clusterings -- through interaction and intermediate visual\nencoding. Clustrophile also contributes two spatial interaction techniques,\n$\\textit{forward projection}$ and $\\textit{backward projection}$, and a\nvisualization method, $\\textit{prolines}$, for reasoning about two-dimensional\nprojections obtained through dimensionality reductions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 18:27:56 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1710.02630", "submitter": "Xiaopei Liu", "authors": "Yulong Guo, Xiaopei Liu, Chi Xiong, Xuemiao Xu and Chi-Wing Fu", "title": "Towards High-quality Visualization of Superfluid Vortices", "comments": "14 pages, 15 figures, accepted by IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2017.2719684", "report-no": null, "categories": "physics.flu-dyn cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superfluidity is a special state of matter exhibiting macroscopic quantum\nphenomena and acting like a fluid with zero viscosity. In such a state,\nsuperfluid vortices exist as phase singularities of the model equation with\nunique distributions. This paper presents novel techniques to aid the visual\nunderstanding of superfluid vortices based on the state-of-the-art non-linear\nKlein-Gordon equation, which evolves a complex scalar field, giving rise to\nspecial vortex lattice/ring structures with dynamic vortex formation,\nreconnection, and Kelvin waves, etc. By formulating a numerical model with\ntheoretical physicists in superfluid research, we obtain high-quality\nsuperfluid flow data sets without noise-like waves, suitable for vortex\nvisualization. By further exploring superfluid vortex properties, we develop a\nnew vortex identification and visualization method: a novel mechanism with\nvelocity circulation to overcome phase singularity and an orthogonal-plane\nstrategy to avoid ambiguity. Hence, our visualizations can help reveal various\nsuperfluid vortex structures and enable domain experts for related visual\nanalysis, such as the steady vortex lattice/ring structures, dynamic vortex\nstring interactions with reconnections and energy radiations, where the famous\nKelvin waves and decaying vortex tangle were clearly observed. These\nvisualizations have assisted physicists to verify the superfluid model, and\nfurther explore its dynamic behavior more intuitively.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 03:58:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Guo", "Yulong", ""], ["Liu", "Xiaopei", ""], ["Xiong", "Chi", ""], ["Xu", "Xuemiao", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "1710.02754", "submitter": "Jos\\'e Silva Neto", "authors": "Jos\\'e F. S. Neto, Waldson P. N. Leandro, Matheus A. Gadelha, Tiago S.\n  Santos, Bruno M. Carvalho, Edgar Gardu\\~no", "title": "Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image segmentation is the process of assigning distinct labels to\ndifferent objects in a digital image, and the fuzzy segmentation algorithm has\nbeen successfully used in the segmentation of images from a wide variety of\nsources. However, the traditional fuzzy segmentation algorithm fails to segment\nobjects that are characterized by textures whose patterns cannot be\nsuccessfully described by simple statistics computed over a very restricted\narea. In this paper, we propose an extension of the fuzzy segmentation\nalgorithm that uses adaptive textural affinity functions to perform the\nsegmentation of such objects on bidimensional images. The adaptive affinity\nfunctions compute their appropriate neighborhood size as they compute the\ntexture descriptors surrounding the seed spels (spatial elements), according to\nthe characteristics of the texture being processed. The algorithm then segments\nthe image with an appropriate neighborhood for each object. We performed\nexperiments on mosaic images that were composed using images from the Brodatz\ndatabase, and compared our results with the ones produced by a recently\npublished texture segmentation algorithm, showing the applicability of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 22:10:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Neto", "Jos\u00e9 F. S.", ""], ["Leandro", "Waldson P. N.", ""], ["Gadelha", "Matheus A.", ""], ["Santos", "Tiago S.", ""], ["Carvalho", "Bruno M.", ""], ["Gardu\u00f1o", "Edgar", ""]]}, {"id": "1710.02862", "submitter": "Mahsa Mirzargar", "authors": "Mahsa Mirzargar and Ross T. Whitaker and Robert M. Kirby", "title": "Exploration of Heterogeneous Data Using Robust Similarity", "comments": "Presented at Visualization in Data Science (VDS at IEEE VIS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous data pose serious challenges to data analysis tasks, including\nexploration and visualization. Current techniques often utilize dimensionality\nreductions, aggregation, or conversion to numerical values to analyze\nheterogeneous data. However, the effectiveness of such techniques to find\nsubtle structures such as the presence of multiple modes or detection of\noutliers is hindered by the challenge to find the proper subspaces or prior\nknowledge to reveal the structures. In this paper, we propose a generic\nsimilarity-based exploration technique that is applicable to a wide variety of\ndatatypes and their combinations, including heterogeneous ensembles. The\nproposed concept of similarity has a close connection to statistical analysis\nand can be deployed for summarization, revealing fine structures such as the\npresence of multiple modes, and detection of anomalies or outliers. We then\npropose a visual encoding framework that enables the exploration of a\nheterogeneous dataset in different levels of detail and provides insightful\ninformation about both global and local structures. We demonstrate the utility\nof the proposed technique using various real datasets, including ensemble data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 17:43:23 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Mirzargar", "Mahsa", ""], ["Whitaker", "Ross T.", ""], ["Kirby", "Robert M.", ""]]}, {"id": "1710.02984", "submitter": "Jan Egger", "authors": "Alexander Hann, Lucas Bettac, Mark M. Haenle, Tilmann Graeter, Andreas\n  W. Berger, Jens Dreyhaupt, Dieter Schmalstieg, Wolfram G. Zoller, Jan Egger", "title": "Algorithm guided outlining of 105 pancreatic cancer liver metastases in\n  Ultrasound", "comments": "7 pages, 3 Figures, 3 Tables, 46 References", "journal-ref": "Sci Rep. 2017 Oct 6;7(1):12779", "doi": "10.1038/s41598-017-12925-z", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual segmentation of hepatic metastases in ultrasound images acquired from\npatients suffering from pancreatic cancer is common practice. Semiautomatic\nmeasurements promising assistance in this process are often assessed using a\nsmall number of lesions performed by examiners who already know the algorithm.\nIn this work, we present the application of an algorithm for the segmentation\nof liver metastases due to pancreatic cancer using a set of 105 different\nimages of metastases. The algorithm and the two examiners had never assessed\nthe images before. The examiners first performed a manual segmentation and,\nafter five weeks, a semiautomatic segmentation using the algorithm. They were\nsatisfied in up to 90% of the cases with the semiautomatic segmentation\nresults. Using the algorithm was significantly faster and resulted in a median\nDice similarity score of over 80%. Estimation of the inter-operator variability\nby using the intra class correlation coefficient was good with 0.8. In\nconclusion, the algorithm facilitates fast and accurate segmentation of liver\nmetastases, comparable to the current gold standard of manual segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 08:22:28 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Hann", "Alexander", ""], ["Bettac", "Lucas", ""], ["Haenle", "Mark M.", ""], ["Graeter", "Tilmann", ""], ["Berger", "Andreas W.", ""], ["Dreyhaupt", "Jens", ""], ["Schmalstieg", "Dieter", ""], ["Zoller", "Wolfram G.", ""], ["Egger", "Jan", ""]]}, {"id": "1710.03337", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth", "title": "Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs", "comments": "Follow up for previous adversarial stop sign paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 22:20:59 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 21:53:58 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Lu", "Jiajun", ""], ["Sibai", "Hussein", ""], ["Fabry", "Evan", ""], ["Forsyth", "David", ""]]}, {"id": "1710.04034", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Chun Pang Yung and Lok Ming Lui", "title": "Image retargeting via Beltrami representation", "comments": "13pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image retargeting aims to resize an image to one with a prescribed aspect\nratio. Simple scaling inevitably introduces unnatural geometric distortions on\nthe important content of the image. In this paper, we propose a simple and yet\neffective method to resize an image, which preserves the geometry of the\nimportant content, using the Beltrami representation. Our algorithm allows\nusers to interactively label content regions as well as line structures. Image\nresizing can then be achieved by warping the image by an orientation-preserving\nbijective warping map with controlled distortion. The warping map is\nrepresented by its Beltrami representation, which captures the local geometric\ndistortion of the map. By carefully prescribing the values of the Beltrami\nrepresentation, images with different complexity can be effectively resized.\nOur method does not require solving any optimization problems and tuning\nparameters throughout the process. This results in a simple and efficient\nalgorithm to solve the image retargeting problem. Extensive experiments have\nbeen carried out, which demonstrate the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:20:20 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Lau", "Chun Pong", ""], ["Yung", "Chun Pang", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1710.04328", "submitter": "Oh-Hyun Kwon", "authors": "Oh-Hyun Kwon, Tarik Crnovrsanin, Kwan-Liu Ma", "title": "What Would a Graph Look Like in This Layout? A Machine Learning Approach\n  to Large Graph Visualization", "comments": "Presented at IEEE InfoVis 2017. To appear in IEEE Transactions on\n  Visualization and Computer Graphics", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (2018)", "doi": "10.1109/TVCG.2017.2743858", "report-no": null, "categories": "cs.SI cs.CG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using different methods for laying out a graph can lead to very different\nvisual appearances, with which the viewer perceives different information.\nSelecting a \"good\" layout method is thus important for visualizing a graph. The\nselection can be highly subjective and dependent on the given task. A common\napproach to selecting a good layout is to use aesthetic criteria and visual\ninspection. However, fully calculating various layouts and their associated\naesthetic metrics is computationally expensive. In this paper, we present a\nmachine learning approach to large graph visualization based on computing the\ntopological similarity of graphs using graph kernels. For a given graph, our\napproach can show what the graph would look like in different layouts and\nestimate their corresponding aesthetic metrics. An important contribution of\nour work is the development of a new framework to design graph kernels. Our\nexperimental study shows that our estimation calculation is considerably faster\nthan computing the actual layouts and their aesthetic metrics. Also, our graph\nkernels outperform the state-of-the-art ones in both time and accuracy. In\naddition, we conducted a user study to demonstrate that the topological\nsimilarity computed with our graph kernel matches perceptual similarity\nassessed by human users.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 23:00:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kwon", "Oh-Hyun", ""], ["Crnovrsanin", "Tarik", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1710.04867", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Volker Rasche, Timo Ropinski, Tobias Ritschel", "title": "Single-image Tomography: 3D Volumes from 2D Cranial X-Rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As many different 3D volumes could produce the same 2D x-ray image, inverting\nthis process is challenging. We show that recent deep learning-based\nconvolutional neural networks can solve this task. As the main challenge in\nlearning is the sheer amount of data created when extending the 2D image into a\n3D volume, we suggest firstly to learn a coarse, fixed-resolution volume which\nis then fused in a second step with the input x-ray into a high-resolution\nvolume. To train and validate our approach we introduce a new dataset that\ncomprises of close to half a million computer-simulated 2D x-ray images of 3D\nvolumes scanned from 175 mammalian species. Applications of our approach\ninclude stereoscopic rendering of legacy x-ray images, re-rendering of x-rays\nincluding changes of illumination, view pose or geometry. Our evaluation\nincludes comparison to previous tomography work, previous learning methods\nusing our data, a user study and application to a set of real x-rays.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 10:34:34 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 08:38:34 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 14:59:23 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Henzler", "Philipp", ""], ["Rasche", "Volker", ""], ["Ropinski", "Timo", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1710.05189", "submitter": "Nicolas Rougier", "authors": "Nicolas P. Rougier", "title": "A graphical, scalable and intuitive method for the placement and the\n  connection of biological cells", "comments": "Corresponding code at https://github.com/rougier/spatial-computation", "journal-ref": null, "doi": "10.3389/fninf.2018.00012", "report-no": null, "categories": "cs.NE cs.GR q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a graphical method originating from the computer graphics domain\nthat is used for the arbitrary and intuitive placement of cells over a\ntwo-dimensional manifold. Using a bitmap image as input, where the color\nindicates the identity of the different structures and the alpha channel\nindicates the local cell density, this method guarantees a discrete\ndistribution of cell position respecting the local density function. This\nmethod scales to any number of cells, allows to specify several different\nstructures at once with arbitrary shapes and provides a scalable and versatile\nalternative to the more classical assumption of a uniform non-spatial\ndistribution. Furthermore, several connection schemes can be derived from the\npaired distances between cells using either an automatic mapping or a\nuser-defined local reference frame, providing new computational properties for\nthe underlying model. The method is illustrated on a discrete homogeneous\nneural field, on the distribution of cones and rods in the retina and on a\ncoronal view of the basal ganglia.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:10:39 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Rougier", "Nicolas P.", ""]]}, {"id": "1710.05592", "submitter": "Yanir Kleiman", "authors": "Yanir Kleiman and Maks Ovsjanikov", "title": "Robust Structure-based Shape Correspondence", "comments": "Accepted to Computer Graphics Forum, February 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust method to find region-level correspondences between\nshapes, which are invariant to changes in geometry and applicable across\nmultiple shape representations. We generate simplified shape graphs by jointly\ndecomposing the shapes, and devise an adapted graph-matching technique, from\nwhich we infer correspondences between shape regions. The simplified shape\ngraphs are designed to primarily capture the overall structure of the shapes,\nwithout reflecting precise information about the geometry of each region, which\nenables us to find correspondences between shapes that might have significant\ngeometric differences. Moreover, due to the special care we take to ensure the\nrobustness of each part of our pipeline, our method can find correspondences\nbetween shapes with different representations, such as triangular meshes and\npoint clouds. We demonstrate that the region-wise matching that we obtain can\nbe used to find correspondences between feature points, reveal the intrinsic\nself-similarities of each shape, and even construct point-to-point maps across\nshapes. Our method is both time and space efficient, leading to a pipeline that\nis significantly faster than comparable approaches. We demonstrate the\nperformance of our approach through an extensive quantitative and qualitative\nevaluation on several benchmarks where we achieve comparable or superior\nperformance to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 09:39:46 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 21:45:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Kleiman", "Yanir", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1710.06364", "submitter": "Scott Burns", "authors": "Scott Allen Burns", "title": "Subtractive Color Mixture Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modeling subtractive color mixture (e.g., the way that paints mix) is\ndifficult when working with colors described only by three-dimensional color\nspace values, such as RGB. Although RGB values are sufficient to describe a\nspecific color sensation, they do not contain enough information to predict the\nRGB color that would result from a subtractive mixture of two specified RGB\ncolors. Methods do exist for accurately modeling subtractive mixture, such as\nthe Kubelka-Munk equations, but require extensive spectrophotometric\nmeasurements of the mixed components, making them unsuitable for many computer\ngraphics applications. This paper presents a strategy for modeling subtractive\ncolor mixture given only the RGB information of the colors being mixed, written\nfor a general audience. The RGB colors are first transformed to generic,\nrepresentative spectral distributions, and then this spectral information is\nused to perform the subtractive mixture, using the weighted\narithmetic-geometric mean. This strategy provides reasonable, representative\nsubtractive mixture colors with only modest computational effort and no\nexperimental measurements. As such, it provides a useful way to model\nsubtractive color mixture in computer graphics applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 16:18:04 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Burns", "Scott Allen", ""]]}, {"id": "1710.06368", "submitter": "Zhiyu Sun", "authors": "Zhiyu Sun and Yusen He and Andrey Gritsenko and Amaury Lendasse and\n  Stephen Baek", "title": "Embedded Spectral Descriptors: Learning the point-wise correspondence\n  metric via Siamese neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and informative local shape descriptor plays an important role in\nmesh registration. In this regard, spectral descriptors that are based on the\nspectrum of the Laplace-Beltrami operator have been a popular subject of\nresearch for the last decade due to their advantageous properties, such as\nisometry invariance. Despite such, however, spectral descriptors often fail to\ngive a correct similarity measure for non-isometric cases where the metric\ndistortion between the models is large. Hence, they are not reliable for\ncorrespondence matching problems when the models are not isometric. In this\npaper, it is proposed a method to improve the similarity metric of spectral\ndescriptors for correspondence matching problems. We embed a spectral shape\ndescriptor into a different metric space where the Euclidean distance between\nthe elements directly indicates the geometric dissimilarity. We design and\ntrain a Siamese neural network to find such an embedding, where the embedded\ndescriptors are promoted to rearrange based on the geometric similarity. We\ndemonstrate our approach can significantly enhance the performance of the\nconventional spectral descriptors by the simple augmentation achieved via the\nSiamese neural network in comparison to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 16:26:04 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 21:30:24 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 07:37:30 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sun", "Zhiyu", ""], ["He", "Yusen", ""], ["Gritsenko", "Andrey", ""], ["Lendasse", "Amaury", ""], ["Baek", "Stephen", ""]]}, {"id": "1710.06615", "submitter": "Christian Tominski", "authors": "Davide Ceneda, Theresia Gschwandtner, Thorsten May, Silvia Miksch,\n  Hans-J\\\"org Schulz, Marc Streit, Christian Tominski", "title": "Amending the Characterization of Guidance in Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At VAST 2016, a characterization of guidance has been presented. It includes\na definition of guidance and a model of guidance based on van Wijk's model of\nvisualization. This note amends the original characterization of guidance in\ntwo aspects. First, we provide a clarification of what guidance actually is\n(and is not). Second, we insert into the model a conceptually relevant link\nthat was missing in the original version.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:24:07 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Ceneda", "Davide", ""], ["Gschwandtner", "Theresia", ""], ["May", "Thorsten", ""], ["Miksch", "Silvia", ""], ["Schulz", "Hans-J\u00f6rg", ""], ["Streit", "Marc", ""], ["Tominski", "Christian", ""]]}, {"id": "1710.06815", "submitter": "Mohammad Raji", "authors": "Mohammad Raji, Alok Hota, Robert Sisneros, Peter Messmer, Jian Huang", "title": "Photo-Guided Exploration of Volume Data Features", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we pose the question of whether, by considering qualitative\ninformation such as a sample target image as input, one can produce a rendered\nimage of scientific data that is similar to the target. The algorithm resulting\nfrom our research allows one to ask the question of whether features like those\nin the target image exists in a given dataset. In that way, our method is one\nof imagery query or reverse engineering, as opposed to manual parameter\ntweaking of the full visualization pipeline. For target images, we can use\nreal-world photographs of physical phenomena. Our method leverages deep neural\nnetworks and evolutionary optimization. Using a trained similarity function\nthat measures the difference between renderings of a phenomenon and real-world\nphotographs, our method optimizes rendering parameters. We demonstrate the\nefficacy of our method using a superstorm simulation dataset and images found\nonline. We also discuss a parallel implementation of our method, which was run\non NCSA's Blue Waters.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 16:28:08 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Raji", "Mohammad", ""], ["Hota", "Alok", ""], ["Sisneros", "Robert", ""], ["Messmer", "Peter", ""], ["Huang", "Jian", ""]]}, {"id": "1710.07421", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Frank Neumann, Tobias Friedrich", "title": "Quasi-random Agents for Image Transition and Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-random walks show similar features as standard random walks, but with\nmuch less randomness. We utilize this established model from discrete\nmathematics and show how agents carrying out quasi-random walks can be used for\nimage transition and animation. The key idea is to generalize the notion of\nquasi-random walks and let a set of autonomous agents perform quasi-random\nwalks painting an image. Each agent has one particular target image that they\npaint when following a sequence of directions for their quasi-random walk. The\nsequence can easily be chosen by an artist and allows them to produce a wide\nrange of different transition patterns and animations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 06:16:44 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Neumann", "Aneta", ""], ["Neumann", "Frank", ""], ["Friedrich", "Tobias", ""]]}, {"id": "1710.07480", "submitter": "Gabriel Eilertsen", "authors": "Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa{\\l} K. Mantiuk,\n  Jonas Unger", "title": "HDR image reconstruction from a single exposure using deep CNNs", "comments": "15 pages, 19 figures, Siggraph Asia 2017. Project webpage located at\n  http://hdrv.org/hdrcnn/ where paper with high quality images is available, as\n  well as supplementary material (document, images, video and source code)", "journal-ref": "ACM Trans. Graph. 36, 6, Article 178 (2017)", "doi": "10.1145/3130800.3130816", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera sensors can only capture a limited range of luminance simultaneously,\nand in order to create high dynamic range (HDR) images a set of different\nexposures are typically combined. In this paper we address the problem of\npredicting information that have been lost in saturated image areas, in order\nto enable HDR reconstruction from a single exposure. We show that this problem\nis well-suited for deep learning algorithms, and propose a deep convolutional\nneural network (CNN) that is specifically designed taking into account the\nchallenges in predicting HDR values. To train the CNN we gather a large dataset\nof HDR images, which we augment by simulating sensor saturation for a range of\ncameras. To further boost robustness, we pre-train the CNN on a simulated HDR\ndataset created from a subset of the MIT Places database. We demonstrate that\nour approach can reconstruct high-resolution visually convincing HDR results in\na wide range of situations, and that it generalizes well to reconstruction of\nimages captured with arbitrary and low-end cameras that use unknown camera\nresponse functions and post-processing. Furthermore, we compare to existing\nmethods for HDR expansion, and show high quality results also for image based\nlighting. Finally, we evaluate the results in a subjective experiment performed\non an HDR display. This shows that the reconstructed HDR images are visually\nconvincing, with large improvements as compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 10:48:22 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Eilertsen", "Gabriel", ""], ["Kronander", "Joel", ""], ["Denes", "Gyorgy", ""], ["Mantiuk", "Rafa\u0142 K.", ""], ["Unger", "Jonas", ""]]}, {"id": "1710.08313", "submitter": "Tuanfeng Wang", "authors": "Tuanfeng Y. Wang, Tobias Ritschel, Niloy J. Mitra", "title": "Joint Material and Illumination Estimation from Photo Sets in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faithful manipulation of shape, material, and illumination in 2D Internet\nimages would greatly benefit from a reliable factorization of appearance into\nmaterial (i.e., diffuse and specular) and illumination (i.e., environment\nmaps). On the one hand, current methods that produce very high fidelity\nresults, typically require controlled settings, expensive devices, or\nsignificant manual effort. To the other hand, methods that are automatic and\nwork on 'in the wild' Internet images, often extract only low-frequency\nlighting or diffuse materials. In this work, we propose to make use of a set of\nphotographs in order to jointly estimate the non-diffuse materials and sharp\nlighting in an uncontrolled setting. Our key observation is that seeing\nmultiple instances of the same material under different illumination (i.e.,\nenvironment), and different materials under the same illumination provide\nvaluable constraints that can be exploited to yield a high-quality solution\n(i.e., specular materials and environment illumination) for all the observed\nmaterials and environments. Similar constraints also arise when observing\nmultiple materials in a single environment, or a single material across\nmultiple environments. The core of this approach is an optimization procedure\nthat uses two neural networks that are trained on synthetic images to predict\ngood gradients in parametric space given observation of reflected light. We\nevaluate our method on a range of synthetic and real examples to generate\nhigh-quality estimates, qualitatively compare our results against\nstate-of-the-art alternatives via a user study, and demonstrate\nphoto-consistent image manipulation that is otherwise very challenging to\nachieve.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 14:48:23 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:40:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wang", "Tuanfeng Y.", ""], ["Ritschel", "Tobias", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1710.09192", "submitter": "David Brander", "authors": "David Brander, J. Andreas B{\\ae}rentzen, Ann-Sofie Fisker and Jens\n  Gravesen", "title": "B\\'ezier curves that are close to elastica", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying those cubic B\\'ezier curves that are\nclose in the L2 norm to planar elastic curves. The problem arises in design\nsituations where the manufacturing process produces elastic curves; these are\ndifficult to work with in a digital environment. We seek a sub-class of special\nB\\'ezier curves as a proxy. We identify an easily computable quantity, which we\ncall the lambda-residual, that accurately predicts a small L2 distance. We then\nidentify geometric criteria on the control polygon that guarantee that a\nB\\'ezier curve has lambda-residual below 0.4, which effectively implies that\nthe curve is within 1 percent of its arc-length to an elastic curve in the L2\nnorm. Finally we give two projection algorithms that take an input B\\'ezier\ncurve and adjust its length and shape, whilst keeping the end-points and\nend-tangent angles fixed, until it is close to an elastic curve.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 12:24:52 GMT"}, {"version": "v2", "created": "Sat, 19 May 2018 11:59:20 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Brander", "David", ""], ["B\u00e6rentzen", "J. Andreas", ""], ["Fisker", "Ann-Sofie", ""], ["Gravesen", "Jens", ""]]}, {"id": "1710.09545", "submitter": "Joshua Levine", "authors": "Matthew Berger, Jixian Li, Joshua A. Levine", "title": "A Generative Model for Volume Rendering", "comments": null, "journal-ref": "IEEE Trans. Vis. Comput. Graph. 25(4) (2019) 1636-1650", "doi": "10.1109/TVCG.2018.2816059", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to synthesize and analyze volume-rendered images using\ngenerative models. We use the Generative Adversarial Network (GAN) framework to\ncompute a model from a large collection of volume renderings, conditioned on\n(1) viewpoint and (2) transfer functions for opacity and color. Our approach\nfacilitates tasks for volume analysis that are challenging to achieve using\nexisting rendering techniques such as ray casting or texture-based methods. We\nshow how to guide the user in transfer function editing by quantifying expected\nchange in the output image. Additionally, the generative model transforms\ntransfer functions into a view-invariant latent space specifically designed to\nsynthesize volume-rendered images. We use this space directly for rendering,\nenabling the user to explore the space of volume-rendered images. As our model\nis independent of the choice of volume rendering process, we show how to\nanalyze volume-rendered images produced by direct and global illumination\nlighting, for a variety of volume datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 05:20:05 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:50:24 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Berger", "Matthew", ""], ["Li", "Jixian", ""], ["Levine", "Joshua A.", ""]]}, {"id": "1710.09834", "submitter": "Manu Mathew Thomas", "authors": "Manu Mathew Thomas, Angus G. Forbes", "title": "Deep Illumination: Approximating Dynamic Global Illumination with\n  Generative Adversarial Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Illumination, a novel machine learning technique for\napproximating global illumination (GI) in real-time applications using a\nConditional Generative Adversarial Network. Our primary focus is on generating\nindirect illumination and soft shadows with offline rendering quality at\ninteractive rates. Inspired from recent advancement in image-to-image\ntranslation problems using deep generative convolutional networks, we introduce\na variant of this network that learns a mapping from Gbuffers (depth map,\nnormal map, and diffuse map) and direct illumination to any global illumination\nsolution. Our primary contribution is showing that a generative model can be\nused to learn a density estimation from screen space buffers to an advanced\nillumination model for a 3D environment. Once trained, our network can\napproximate global illumination for scene configurations it has never\nencountered before within the environment it was trained on. We evaluate Deep\nIllumination through a comparison with both a state of the art real-time GI\ntechnique (VXGI) and an offline rendering GI technique (path tracing). We show\nthat our method produces effective GI approximations and is also\ncomputationally cheaper than existing GI techniques. Our technique has the\npotential to replace existing precomputed and screen-space techniques for\nproducing global illumination effects in dynamic scenes with physically-based\nrendering quality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:56:00 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 23:16:53 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Thomas", "Manu Mathew", ""], ["Forbes", "Angus G.", ""]]}]