[{"id": "2102.01029", "submitter": "Filippo Andrea Fanni", "authors": "Filippo Andrea Fanni, Fabio Pellacini, Riccardo Scateni, Andrea\n  Giachetti", "title": "PAVEL: Decorative Patterns with Packed Volumetric Elements", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world hand-crafted objects are decorated with elements that are\npacked onto the object's surface and deformed to cover it as much as possible.\nExamples are artisanal ceramics and metal jewelry. Inspired by these objects,\nwe present a method to enrich surfaces with packed volumetric decorations. Our\nalgorithm works by first determining the locations in which to add the\ndecorative elements and then removing the non-physical overlap between them\nwhile preserving the decoration volume. For the placement, we support several\nstrategies depending on the desired overall motif. To remove the overlap, we\nuse an approach based on implicit deformable models creating the qualitative\neffect of plastic warping while avoiding expensive and hard-to-control physical\nsimulations. Our decorative elements can be used to enhance virtual surfaces,\nas well as 3D-printed pieces, by assembling the decorations onto real-surfaces\nto obtain tangible reproductions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:03:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Fanni", "Filippo Andrea", ""], ["Pellacini", "Fabio", ""], ["Scateni", "Riccardo", ""], ["Giachetti", "Andrea", ""]]}, {"id": "2102.01330", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Yun Wang, Xinhuan Shu, Dominik Moritz, Weiwei Cui, Haidong\n  Zhang, Dongmei Zhang, Huamin Qu", "title": "AI4VIS: Survey on Artificial Intelligence Approaches for Data\n  Visualization", "comments": "Accepted to IEEE TVCG. 20 pages, 12 figures and 8 tables. The\n  associated website is https://ai4vis.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualizations themselves have become a data format. Akin to other data\nformats such as text and images, visualizations are increasingly created,\nstored, shared, and (re-)used with artificial intelligence (AI) techniques. In\nthis survey, we probe the underlying vision of formalizing visualizations as an\nemerging data format and review the recent advance in applying AI techniques to\nvisualization data (AI4VIS). We define visualization data as the digital\nrepresentations of visualizations in computers and focus on data visualization\n(e.g., charts and infographics). We build our survey upon a corpus spanning ten\ndifferent fields in computer science with an eye toward identifying important\ncommon interests. Our resulting taxonomy is organized around WHAT is\nvisualization data and its representation, WHY and HOW to apply AI to\nvisualization data. We highlight a set of common tasks that researchers apply\nto the visualization data and present a detailed discussion of AI approaches\ndeveloped to accomplish those tasks. Drawing upon our literature review, we\ndiscuss several important research questions surrounding the management and\nexploitation of visualization data, as well as the role of AI in support of\nthose processes. We make the list of surveyed papers and related material\navailable online at ai4vis.github.io.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 06:14:51 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 16:56:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wu", "Aoyu", ""], ["Wang", "Yun", ""], ["Shu", "Xinhuan", ""], ["Moritz", "Dominik", ""], ["Cui", "Weiwei", ""], ["Zhang", "Haidong", ""], ["Zhang", "Dongmei", ""], ["Qu", "Huamin", ""]]}, {"id": "2102.01747", "submitter": "Vin\\'icius da Silva", "authors": "Vin\\'icius da Silva, Tiago Novello, H\\'elio Lopes, Luiz Velho", "title": "Real-time rendering of complex fractals", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter describes how to use intersection and closest-hit shaders to\nimplement real-time visualizations of complex fractals using distance\nfunctions. The Mandelbulb and Julia Sets are used as examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 20:44:59 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["da Silva", "Vin\u00edcius", ""], ["Novello", "Tiago", ""], ["Lopes", "H\u00e9lio", ""], ["Velho", "Luiz", ""]]}, {"id": "2102.01895", "submitter": "Barak Or", "authors": "Barak Or and Liam Hazan", "title": "Length Learning for Planar Euclidean Curves", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we used deep neural networks (DNNs) to solve a fundamental\nproblem in differential geometry. One can find many closed-form expressions for\ncalculating curvature, length, and other geometric properties in the\nliterature. As we know these concepts, we are highly motivated to reconstruct\nthem by using deep neural networks. In this framework, our goal is to learn\ngeometric properties from examples. The simplest geometric object is a curve.\nTherefore, this work focuses on learning the length of planar sampled curves\ncreated by a sine waves dataset. For this reason, the fundamental length axioms\nwere reconstructed using a supervised learning approach. Following these axioms\na simplified DNN model, we call ArcLengthNet, was established. The robustness\nto additive noise and discretization errors were tested.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 06:30:03 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Or", "Barak", ""], ["Hazan", "Liam", ""]]}, {"id": "2102.02141", "submitter": "Tim Dykes", "authors": "Tim Dykes, Claudio Gheller, B\\\"arbel S. Koribalski, Klaus Dolag, Mel\n  Krokos", "title": "A New View of Observed Galaxies through 3D Modelling and Visualisation", "comments": "18 pages, 12 figures, accepted for publication in Astronomy and\n  Computing", "journal-ref": null, "doi": "10.1016/j.ascom.2021.100448", "report-no": null, "categories": "astro-ph.IM cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Observational astronomers survey the sky in great detail to gain a better\nunderstanding of many types of astronomical phenomena. In particular, the\nformation and evolution of galaxies, including our own, is a wide field of\nresearch. Three dimensional (spatial 3D) scientific visualisation is typically\nlimited to simulated galaxies, due to the inherently two dimensional spatial\nresolution of Earth-based observations. However, with appropriate means of\nreconstruction, such visualisation can also be used to bring out the inherent\n3D structure that exists in 2D observations of known galaxies, providing new\nviews of these galaxies and visually illustrating the spatial relationships\nwithin galaxy groups that are not obvious in 2D. We present a novel approach to\nreconstruct and visualise 3D representations of nearby galaxies based on\nobservational data using the scientific visualisation software Splotch. We\napply our approach to a case study of the nearby barred spiral galaxy known as\nM83, presenting a new perspective of the M83 local group and highlighting the\nsimilarities between our reconstructed views of M83 and other known galaxies of\nsimilar inclinations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 16:55:08 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:56:30 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Dykes", "Tim", ""], ["Gheller", "Claudio", ""], ["Koribalski", "B\u00e4rbel S.", ""], ["Dolag", "Klaus", ""], ["Krokos", "Mel", ""]]}, {"id": "2102.02332", "submitter": "Jon McCormack", "authors": "Jon McCormack, Camilo Cruz Gambardella and Andy Lomas", "title": "The Enigma of Complexity", "comments": "Preprint of paper accepted for EvoMUSART 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we examine the concept of complexity as it applies to\ngenerative art and design. Complexity has many different, discipline specific\ndefinitions, such as complexity in physical systems (entropy), algorithmic\nmeasures of information complexity and the field of \"complex systems\". We apply\na series of different complexity measures to three different generative art\ndatasets and look at the correlations between complexity and individual\naesthetic judgement by the artist (in the case of two datasets) or the\nphysically measured complexity of 3D forms. Our results show that the degree of\ncorrelation is different for each set and measure, indicating that there is no\noverall \"better\" measure. However, specific measures do perform well on\nindividual datasets, indicating that careful choice can increase the value of\nusing such measures. We conclude by discussing the value of direct measures in\ngenerative and evolutionary art, reinforcing recent findings from neuroimaging\nand psychology which suggest human aesthetic judgement is informed by many\nextrinsic factors beyond the measurable properties of the object being judged.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 23:26:49 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["McCormack", "Jon", ""], ["Gambardella", "Camilo Cruz", ""], ["Lomas", "Andy", ""]]}, {"id": "2102.02783", "submitter": "Lia Morra", "authors": "F. Gabriele Prattic\\`o, Fabrizio Lamberti, Alberto Cannav\\`o, Lia\n  Morra, Paolo Montuschi", "title": "Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for\n  Autonomous Vehicle-to-Pedestrian Communication", "comments": "Accepted for publication in IEEE Transactions on Vehicular Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing pedestrians and other vulnerable road users with a clear indication\nabout a fully autonomous vehicle status and intentions is crucial to make them\ncoexist. In the last few years, a variety of external interfaces have been\nproposed, leveraging different paradigms and technologies including\nvehicle-mounted devices (like LED panels), short-range on-road projections, and\nroad infrastructure interfaces (e.g., special asphalts with embedded displays).\nThese designs were experimented in different settings, using mockups, specially\nprepared vehicles, or virtual environments, with heterogeneous evaluation\nmetrics. Promising interfaces based on Augmented Reality (AR) have been\nproposed too, but their usability and effectiveness have not been tested yet.\nThis paper aims to complement such body of literature by presenting a\ncomparison of state-of-the-art interfaces and new designs under common\nconditions. To this aim, an immersive Virtual Reality-based simulation was\ndeveloped, recreating a well-known scenario represented by pedestrians crossing\nin urban environments under non-regulated conditions. A user study was then\nperformed to investigate the various dimensions of vehicle-to-pedestrian\ninteraction leveraging objective and subjective metrics. Even though no\ninterface clearly stood out over all the considered dimensions, one of the AR\ndesigns achieved state-of-the-art results in terms of safety and trust, at the\ncost of higher cognitive effort and lower intuitiveness compared to LED panels\nshowing anthropomorphic features. Together with rankings on the various\ndimensions, indications about advantages and drawbacks of the various\nalternatives that emerged from this study could provide important information\nfor next developments in the field.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:03:06 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Prattic\u00f2", "F. Gabriele", ""], ["Lamberti", "Fabrizio", ""], ["Cannav\u00f2", "Alberto", ""], ["Morra", "Lia", ""], ["Montuschi", "Paolo", ""]]}, {"id": "2102.02798", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Michael Gharbi, Michal Lukac, Niloy J. Mitra", "title": "Im2Vec: Synthesizing Vector Graphics without Vector Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector graphics are widely used to represent fonts, logos, digital artworks,\nand graphic designs. But, while a vast body of work has focused on generative\nalgorithms for raster images, only a handful of options exists for vector\ngraphics. One can always rasterize the input graphic and resort to image-based\ngenerative approaches, but this negates the advantages of the vector\nrepresentation. The current alternative is to use specialized models that\nrequire explicit supervision on the vector graphics representation at training\ntime. This is not ideal because large-scale high quality vector-graphics\ndatasets are difficult to obtain. Furthermore, the vector representation for a\ngiven design is not unique, so models that supervise on the vector\nrepresentation are unnecessarily constrained. Instead, we propose a new neural\nnetwork that can generate complex vector graphics with varying topologies, and\nonly requires indirect supervision from readily-available raster training\nimages (i.e., with no vector counterparts). To enable this, we use a\ndifferentiable rasterization pipeline that renders the generated vector shapes\nand composites them together onto a raster canvas. We demonstrate our method on\na range of datasets, and provide comparison with state-of-the-art SVG-VAE and\nDeepSVG, both of which require explicit vector graphics supervision. Finally,\nwe also demonstrate our approach on the MNIST dataset, for which no groundtruth\nvector representation is available. Source code, datasets, and more results are\navailable at geometry.cs.ucl.ac.uk/projects/2021/Im2Vec/\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 18:39:45 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:34:00 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 10:48:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Gharbi", "Michael", ""], ["Lukac", "Michal", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2102.03011", "submitter": "Oliver Wang", "authors": "Felix Klose and Oliver Wang and Jean-Charles Bazin and Marcus Magnor\n  and Alexander Sorkine-Hornung", "title": "Sampling Based Scene-Space Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many compelling video processing effects can be achieved if per-pixel depth\ninformation and 3D camera calibrations are known. However, the success of such\nmethods is highly dependent on the accuracy of this \"scene-space\" information.\nWe present a novel, sampling-based framework for processing video that enables\nhigh-quality scene-space video effects in the presence of inevitable errors in\ndepth and camera pose estimation. Instead of trying to improve the explicit 3D\nscene representation, the key idea of our method is to exploit the high\nredundancy of approximate scene information that arises due to most scene\npoints being visible multiple times across many frames of video. Based on this\nobservation, we propose a novel pixel gathering and filtering approach. The\ngathering step is general and collects pixel samples in scene-space, while the\nfiltering step is application-specific and computes a desired output video from\nthe gathered sample sets. Our approach is easily parallelizable and has been\nimplemented on GPU, allowing us to take full advantage of large volumes of\nvideo data and facilitating practical runtimes on HD video using a standard\ndesktop computer. Our generic scene-space formulation is able to\ncomprehensively describe a multitude of video processing applications such as\ndenoising, deblurring, super resolution, object removal, computational shutter\nfunctions, and other scene-space camera effects. We present results for various\ncasually captured, hand-held, moving, compressed, monocular videos depicting\nchallenging scenes recorded in uncontrolled environments.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 05:55:04 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Klose", "Felix", ""], ["Wang", "Oliver", ""], ["Bazin", "Jean-Charles", ""], ["Magnor", "Marcus", ""], ["Sorkine-Hornung", "Alexander", ""]]}, {"id": "2102.04072", "submitter": "Christian van Onzenoodt", "authors": "Christian van Onzenoodt, Gurprit Singh, Timo Ropinski, Tobias Ritschel", "title": "Blue Noise Plots", "comments": "9 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Blue Noise Plots, two-dimensional dot plots that depict data\npoints of univariate data sets. While often one-dimensional strip plots are\nused to depict such data, one of their main problems is visual clutter which\nresults from overlap. To reduce this overlap, jitter plots were introduced,\nwhereby an additional, non-encoding plot dimension is introduced, along which\nthe data point representing dots are randomly perturbed. Unfortunately, this\nrandomness can suggest non-existent clusters, and often leads to visually\nunappealing plots, in which overlap might still occur. To overcome these\nshortcomings, we introduce BlueNoise Plots where random jitter along the\nnon-encoding plot dimension is replaced by optimizing all dots to keep a\nminimum distance in 2D i. e., Blue Noise. We evaluate the effectiveness as well\nas the aesthetics of Blue Noise Plots through both, a quantitative and a\nqualitative user study. The Python implementation of Blue Noise Plots is\navailable here.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 09:16:51 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 09:20:34 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 09:42:04 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["van Onzenoodt", "Christian", ""], ["Singh", "Gurprit", ""], ["Ropinski", "Timo", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2102.04317", "submitter": "Shuquan Ye", "authors": "Shuquan Ye, Dongdong Chen, Songfang Han, Ziyu Wan, Jing Liao", "title": "Meta-PU: An Arbitrary-Scale Upsampling Network for Point Cloud", "comments": "To appear at TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud upsampling is vital for the quality of the mesh in\nthree-dimensional reconstruction. Recent research on point cloud upsampling has\nachieved great success due to the development of deep learning. However, the\nexisting methods regard point cloud upsampling of different scale factors as\nindependent tasks. Thus, the methods need to train a specific model for each\nscale factor, which is both inefficient and impractical for storage and\ncomputation in real applications. To address this limitation, in this work, we\npropose a novel method called ``Meta-PU\" to firstly support point cloud\nupsampling of arbitrary scale factors with a single model. In the Meta-PU\nmethod, besides the backbone network consisting of residual graph convolution\n(RGC) blocks, a meta-subnetwork is learned to adjust the weights of the RGC\nblocks dynamically, and a farthest sampling block is adopted to sample\ndifferent numbers of points. Together, these two blocks enable our Meta-PU to\ncontinuously upsample the point cloud with arbitrary scale factors by using\nonly a single model. In addition, the experiments reveal that training on\nmultiple scales simultaneously is beneficial to each other. Thus, Meta-PU even\noutperforms the existing methods trained for a specific scale factor only.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 16:21:48 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Ye", "Shuquan", ""], ["Chen", "Dongdong", ""], ["Han", "Songfang", ""], ["Wan", "Ziyu", ""], ["Liao", "Jing", ""]]}, {"id": "2102.04533", "submitter": "Yuting Yang", "authors": "Yuting Yang, Connelly Barnes, Adam Finkelstein", "title": "Learning from Shader Program Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks for image processing typically learn from RGB pixels. This\npaper proposes instead to learn from program traces, the intermediate values\ncomputed during program execution. We study this idea in the context of\npixel~shaders -- programs that generate images, typically running in parallel\n(for each pixel) on GPU hardware. The intermediate values computed at each\npixel during program execution form the input to the learned model. In a\nvariety of applications, models learned from program traces outperform baseline\nmodels learned from RGB, even when augmented with hand-picked shader-specific\nfeatures. We also investigate strategies for selecting a subset of trace\nfeatures for learning; using just a small subset of the trace still outperforms\nthe baselines.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 21:08:14 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Yang", "Yuting", ""], ["Barnes", "Connelly", ""], ["Finkelstein", "Adam", ""]]}, {"id": "2102.04942", "submitter": "F\\'elix G. Harvey", "authors": "F\\'elix G. Harvey, Mike Yurick, Derek Nowrouzezahrai, Christopher Pal", "title": "Robust Motion In-betweening", "comments": "Published at SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392480", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work we present a novel, robust transition generation technique that\ncan serve as a new tool for 3D animators, based on adversarial recurrent neural\nnetworks. The system synthesizes high-quality motions that use\ntemporally-sparse keyframes as animation constraints. This is reminiscent of\nthe job of in-betweening in traditional animation pipelines, in which an\nanimator draws motion frames between provided keyframes. We first show that a\nstate-of-the-art motion prediction model cannot be easily converted into a\nrobust transition generator when only adding conditioning information about\nfuture keyframes. To solve this problem, we then propose two novel additive\nembedding modifiers that are applied at each timestep to latent representations\nencoded inside the network's architecture. One modifier is a time-to-arrival\nembedding that allows variations of the transition length with a single model.\nThe other is a scheduled target noise vector that allows the system to be\nrobust to target distortions and to sample different transitions given fixed\nkeyframes. To qualitatively evaluate our method, we present a custom\nMotionBuilder plugin that uses our trained model to perform in-betweening in\nproduction scenarios. To quantitatively evaluate performance on transitions and\ngeneralizations to longer time horizons, we present well-defined in-betweening\nbenchmarks on a subset of the widely used Human3.6M dataset and on LaFAN1, a\nnovel high quality motion capture dataset that is more appropriate for\ntransition generation. We are releasing this new dataset along with this work,\nwith accompanying code for reproducing our baseline results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 16:52:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Harvey", "F\u00e9lix G.", ""], ["Yurick", "Mike", ""], ["Nowrouzezahrai", "Derek", ""], ["Pal", "Christopher", ""]]}, {"id": "2102.05361", "submitter": "Stefan Krumpen", "authors": "Stefan Krumpen, Reinhard Klein and Michael Weinmann", "title": "Towards Tangible Cultural Heritage Experiences -- Enriching VR-Based\n  Object Inspection with Haptic Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VR/AR technology is a key enabler for new ways of immersively experiencing\ncultural heritage artifacts based on their virtual counterparts obtained from a\ndigitization process. In this paper, we focus on enriching VR-based object\ninspection by additional haptic feedback, thereby creating tangible cultural\nheritage experiences. For this purpose, we present an approach for interactive\nand collaborative VR-based object inspection and annotation. Our system\nsupports high-quality 3D models with accurate reflectance characteristics while\nadditionally providing haptic feedback regarding the object shape features\nbased on a 3D printed replica. The digital object model in terms of a printable\nrepresentation of the geometry as well as reflectance characteristics are\nstored in a compact and streamable representation on a central server, which\nstreams the data to remotely connected users/clients. The latter can jointly\nperform an interactive inspection of the object in VR with additional haptic\nfeedback through the 3D printed replica. Evaluations regarding system\nperformance, visual quality of the considered models as well as insights from a\nuser study indicate an improved interaction, assessment and experience of the\nconsidered objects.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 10:01:21 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Krumpen", "Stefan", ""], ["Klein", "Reinhard", ""], ["Weinmann", "Michael", ""]]}, {"id": "2102.05462", "submitter": "Katja Wolff", "authors": "Katja Wolff, Philipp Herholz, Verena Ziegler, Frauke Link, Nico\n  Br\\\"ugel and Olga Sorkine-Hornung", "title": "3D Custom Fit Garment Design with Body Movement", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The standardized sizes used in the garment industry do not cover the range of\nindividual differences in body shape for most people, leading to ill-fitting\nclothes, high return rates and overproduction. Recent research efforts in both\nindustry and academia therefore focus on on-demand fabrication of individually\nfitting garments. We propose an interactive design tool for creating custom-fit\ngarments based on 3D body scans of the intended wearer. Our method explicitly\nincorporates transitions between various body poses to ensure a better fit and\nfreedom of movement. The core of our method focuses on tools to create a 3D\ngarment shape directly on an avatar without an underlying sewing pattern, and\non the adjustment of that garment's rest shape while interpolating and moving\nthrough the different input poses. We alternate between cloth simulation steps\nand rest shape adjustment steps based on stretch to achieve the final shape of\nthe garment. At any step in the real-time process, we allow for interactive\nchanges to the garment. Once the garment shape is finalized for production,\nestablished techniques can be used to parameterize it into a 2D sewing pattern\nor transform it into a knitting pattern.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 14:33:47 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wolff", "Katja", ""], ["Herholz", "Philipp", ""], ["Ziegler", "Verena", ""], ["Link", "Frauke", ""], ["Br\u00fcgel", "Nico", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2102.05791", "submitter": "Junior Rojas", "authors": "Junior Rojas, Eftychios Sifakis, Ladislav Kavan", "title": "Differentiable Implicit Soft-Body Physics", "comments": "Fixed Figure 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a differentiable soft-body physics simulator that can be composed\nwith neural networks as a differentiable layer. In contrast to other\ndifferentiable physics approaches that use explicit forward models to define\nstate transitions, we focus on implicit state transitions defined via function\nminimization. Implicit state transitions appear in implicit numerical\nintegration methods, which offer the benefits of large time steps and excellent\nnumerical stability, but require a special treatment to achieve\ndifferentiability due to the absence of an explicit differentiable forward\npass. In contrast to other implicit differentiation approaches that require\nexplicit formulas for the force function and the force Jacobian matrix, we\npresent an energy-based approach that allows us to compute these derivatives\nautomatically and in a matrix-free fashion via reverse-mode automatic\ndifferentiation. This allows for more flexibility and productivity when\ndefining physical models and is particularly important in the context of neural\nnetwork training, which often relies on reverse-mode automatic differentiation\n(backpropagation). We demonstrate the effectiveness of our differentiable\nsimulator in policy optimization for locomotion tasks and show that it achieves\nbetter sample efficiency than model-free reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 01:06:54 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 00:42:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Rojas", "Junior", ""], ["Sifakis", "Eftychios", ""], ["Kavan", "Ladislav", ""]]}, {"id": "2102.05921", "submitter": "Enrico Puppo", "authors": "Claudio Mancinelli, Giacomo Nazzaro, Fabio Pellacini, Enrico Puppo", "title": "b/Surf: Interactive B\\'ezier Splines on Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  B\\'ezier curves provide the basic building blocks of graphic design in 2D. In\nthis paper, we port B\\'ezier curves to manifolds. We support the interactive\ndrawing and editing of B\\'ezier splines on manifold meshes with millions of\ntriangles, by relying on just repeated manifold averages. We show that direct\nextensions of the De Casteljau and Bernstein evaluation algorithms to the\nmanifold setting are fragile, and prone to discontinuities when control\npolygons become large. Conversely, approaches based on subdivision are robust\nand can be implemented efficiently. We define B\\'ezier curves on manifolds, by\nextending both the recursive De Casteljau bisection and a new open-uniform\nLane-Riesenfeld subdivision scheme, which provide curves with different degrees\nof smoothness. For both schemes, we present algorithms for curve tracing, point\nevaluation, and point insertion. We test our algorithms for robustness and\nperformance on all watertight, manifold, models from the Thingi10k repository,\nwithout any pre-processing and with random control points. For interactive\nediting, we port all the basic user interface interactions found in 2D tools\ndirectly to the mesh. We also support mapping complex SVG drawings to the mesh\nand their interactive editing.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 10:15:57 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Mancinelli", "Claudio", ""], ["Nazzaro", "Giacomo", ""], ["Pellacini", "Fabio", ""], ["Puppo", "Enrico", ""]]}, {"id": "2102.05963", "submitter": "Alejandro Sztrajman", "authors": "Alejandro Sztrajman, Gilles Rainer, Tobias Ritschel, Tim Weyrich", "title": "Neural BRDF Representation and Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlled capture of real-world material appearance yields tabulated sets of\nhighly realistic reflectance data. In practice, however, its high memory\nfootprint requires compressing into a representation that can be used\nefficiently in rendering while remaining faithful to the original. Previous\nworks in appearance encoding often prioritised one of these requirements at the\nexpense of the other, by either applying high-fidelity array compression\nstrategies not suited for efficient queries during rendering, or by fitting a\ncompact analytic model that lacks expressiveness. We present a compact neural\nnetwork-based representation of BRDF data that combines high-accuracy\nreconstruction with efficient practical rendering via built-in interpolation of\nreflectance. We encode BRDFs as lightweight networks, and propose a training\nscheme with adaptive angular sampling, critical for the accurate reconstruction\nof specular highlights. Additionally, we propose a novel approach to make our\nrepresentation amenable to importance sampling: rather than inverting the\ntrained networks, we learn to encode them in a more compact embedding that can\nbe mapped to parameters of an analytic BRDF for which importance sampling is\nknown. We evaluate encoding results on isotropic and anisotropic BRDFs from\nmultiple real-world datasets, and importance sampling performance for isotropic\nBRDFs mapped to two different analytic models.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 12:00:24 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 12:38:18 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 21:57:47 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sztrajman", "Alejandro", ""], ["Rainer", "Gilles", ""], ["Ritschel", "Tobias", ""], ["Weyrich", "Tim", ""]]}, {"id": "2102.06199", "submitter": "Shih-Yang Su", "authors": "Shih-Yang Su, Frank Yu, Michael Zollhoefer and Helge Rhodin", "title": "A-NeRF: Surface-free Human 3D Pose Refinement via Neural Rendering", "comments": "Project website:\n  https://lemonatsu.github.io/ANeRF-Surface-free-Pose-Refinement/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has reshaped the classical motion capture pipeline,\ngenerative, analysis-by-synthesis elements are still in use to recover fine\ndetails if a high-quality 3D model of the user is available. Unfortunately,\nobtaining such a model for every user a priori is challenging, time-consuming,\nand limits the application scenarios. We propose a novel test-time optimization\napproach for monocular motion capture that learns a volumetric body model of\nthe user in a self-supervised manner. To this end, our approach combines the\nadvantages of neural radiance fields with an articulated skeleton\nrepresentation. Our proposed skeleton embedding serves as a common reference\nthat links constraints across time, thereby reducing the number of required\ncamera views from traditionally dozens of calibrated cameras, down to a single\nuncalibrated one. As a starting point, we employ the output of an off-the-shelf\nmodel that predicts the 3D skeleton pose. The volumetric body shape and\nappearance is then learned from scratch, while jointly refining the initial\npose estimate. Our approach is self-supervised and does not require any\nadditional ground truth labels for appearance, pose, or 3D shape. We\ndemonstrate that our novel combination of a discriminative pose estimation\ntechnique with surface-free analysis-by-synthesis outperforms purely\ndiscriminative monocular pose estimation approaches and generalizes well to\nmultiple views.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 18:58:31 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Su", "Shih-Yang", ""], ["Yu", "Frank", ""], ["Zollhoefer", "Michael", ""], ["Rhodin", "Helge", ""]]}, {"id": "2102.07343", "submitter": "He Chen", "authors": "He Chen, Hyojoon Park, Kutay Macit, Ladislav Kavan", "title": "Capturing Detailed Deformations of Moving Human Bodies", "comments": "20 pages, 25 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new method to capture detailed human motion, sampling more than\n1000 unique points on the body. Our method outputs highly accurate 4D\n(spatio-temporal) point coordinates and, crucially, automatically assigns a\nunique label to each of the points. The locations and unique labels of the\npoints are inferred from individual 2D input images only, without relying on\ntemporal tracking or any human body shape or skeletal kinematics models.\nTherefore, our captured point trajectories contain all of the details from the\ninput images, including motion due to breathing, muscle contractions and flesh\ndeformation, and are well suited to be used as training data to fit advanced\nmodels of the human body and its motion. The key idea behind our system is a\nnew type of motion capture suit which contains a special pattern with\ncheckerboard-like corners and two-letter codes. The images from our\nmulti-camera system are processed by a sequence of neural networks which are\ntrained to localize the corners and recognize the codes, while being robust to\nsuit stretching and self-occlusions of the body. Our system relies only on\nstandard RGB or monochrome sensors and fully passive lighting and the passive\nsuit, making our method easy to replicate, deploy and use. Our experiments\ndemonstrate highly accurate captures of a wide variety of human poses,\nincluding challenging motions such as yoga, gymnastics, or rolling on the\nground.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 04:58:13 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:21:08 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 23:04:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "He", ""], ["Park", "Hyojoon", ""], ["Macit", "Kutay", ""], ["Kavan", "Ladislav", ""]]}, {"id": "2102.07499", "submitter": "Manos Kamarianakis", "authors": "Manos Kamarianakis, George Papagiannakis", "title": "An All-In-One Geometric Algorithm for Cutting, Tearing, and Drilling\n  Deformable Models", "comments": "25 pages pages, 20 figues, extended version of arXiv:2007.04464v2 ,\n  accepted in Advances in Applied Clifford Algebras (AACA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal Geometric Algebra (CGA) is a framework that allows the\nrepresentation of objects, such as points, planes and spheres, and\ndeformations, such as translations, rotations and dilations as uniform vectors,\ncalled multivectors. In this work, we demonstrate the merits of multivector\nusage with a novel, integrated rigged character simulation framework based on\nCGA. In such a framework, and for the first time, one may perform real-time\ncuts and tears as well as drill holes on a rigged 3D model. These operations\ncan be performed before and/or after model animation, while maintaining\ndeformation topology. Moreover, our framework permits generation of\nintermediate keyframes on-the-fly based on user input, apart from the frames\nprovided in the model data. We are motivated to use CGA as it is the\nlowest-dimension extension of dual-quaternion algebra that amends the\nshortcomings of the majority of existing animation and deformation techniques.\nSpecifically, we no longer need to maintain objects of multiple algebras and\nconstantly transmute between them, such as matrices, quaternions and\ndual-quaternions, and we can effortlessly apply dilations. Using such an\nall-in-one geometric framework allows for better maintenance and optimization\nand enables easier interpolation and application of all native deformations.\nFurthermore, we present these three novel algorithms in a single CGA\nrepresentation which enables cutting, tearing and drilling of the input rigged\nmodel, where the output model can be further re-deformed in interactive frame\nrates. These close to real-time cut,tear and drill algorithms can enable a new\nsuite of applications, especially under the scope of a medical VR simulation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:04:03 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 19:15:29 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kamarianakis", "Manos", ""], ["Papagiannakis", "George", ""]]}, {"id": "2102.08860", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas, Ricardo Martin-Brualla, Vittorio Ferrari", "title": "ShaRF: Shape-conditioned Radiance Fields from a Single View", "comments": "Project page: http://www.krematas.com/sharf/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating neural scenes representations of objects\ngiven only a single image. The core of our method is the estimation of a\ngeometric scaffold for the object and its use as a guide for the reconstruction\nof the underlying radiance field. Our formulation is based on a generative\nprocess that first maps a latent code to a voxelized shape, and then renders it\nto an image, with the object appearance being controlled by a second latent\ncode. During inference, we optimize both the latent codes and the networks to\nfit a test image of a new object. The explicit disentanglement of shape and\nappearance allows our model to be fine-tuned given a single image. We can then\nrender new views in a geometrically consistent manner and they represent\nfaithfully the input object. Additionally, our method is able to generalize to\nimages outside of the training domain (more realistic renderings and even real\nphotographs). Finally, the inferred geometric scaffold is itself an accurate\nestimate of the object's 3D shape. We demonstrate in several experiments the\neffectiveness of our approach in both synthetic and real images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 16:40:28 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 09:53:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Martin-Brualla", "Ricardo", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "2102.09105", "submitter": "Minghua Liu", "authors": "Minghua Liu, Minhyuk Sung, Radomir Mech, Hao Su", "title": "DeepMetaHandles: Learning Deformation Meta-Handles of 3D Meshes with\n  Biharmonic Coordinates", "comments": "CVPR2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose DeepMetaHandles, a 3D conditional generative model based on mesh\ndeformation. Given a collection of 3D meshes of a category and their\ndeformation handles (control points), our method learns a set of meta-handles\nfor each shape, which are represented as combinations of the given handles. The\ndisentangled meta-handles factorize all the plausible deformations of the\nshape, while each of them corresponds to an intuitive deformation. A new\ndeformation can then be generated by sampling the coefficients of the\nmeta-handles in a specific range. We employ biharmonic coordinates as the\ndeformation function, which can smoothly propagate the control points'\ntranslations to the entire mesh. To avoid learning zero deformation as\nmeta-handles, we incorporate a target-fitting module which deforms the input\nmesh to match a random target. To enhance deformations' plausibility, we employ\na soft-rasterizer-based discriminator that projects the meshes to a 2D space.\nOur experiments demonstrate the superiority of the generated deformations as\nwell as the interpretability and consistency of the learned meta-handles.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 01:31:26 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 18:22:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Minghua", ""], ["Sung", "Minhyuk", ""], ["Mech", "Radomir", ""], ["Su", "Hao", ""]]}, {"id": "2102.09438", "submitter": "Dan Reznik", "authors": "Mark Helman, Dominique Laurain, Ronaldo Garcia, Dan Reznik", "title": "Invariant Center Power and Elliptic Loci of Poncelet Triangles", "comments": "25 pages, 16 figures, 6 tables, 8 video links, 7 live app links", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.GR cs.RO math.CV math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study center power with respect to circles derived from Poncelet\n3-periodics (triangles) in a generic pair of ellipses as well as loci of their\ntriangle centers. We show that (i) for any concentric pair, the power of the\ncenter with respect to either circumcircle or Euler's circle is invariant, and\n(ii) if a triangle center of a 3-periodic in a generic nested pair is a fixed\naffine combination of barycenter and circumcenter, its locus over the family is\nan ellipse.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:57:22 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 13:12:17 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 22:01:20 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 20:34:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Helman", "Mark", ""], ["Laurain", "Dominique", ""], ["Garcia", "Ronaldo", ""], ["Reznik", "Dan", ""]]}, {"id": "2102.10013", "submitter": "Erva Ulu", "authors": "Erva Ulu, Nurcan Gecer Ulu, Jiahao Li, Walter Hsiao", "title": "Curvy: An Interactive Design Tool for Varying Density Support Structures", "comments": "Submitted to Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Curvy-an interactive design tool to generate varying density\nsupport structures for 3D printing. Support structures are essential for\nprinting models with extreme overhangs. Yet, they often cause defects on\ncontact areas, resulting in poor surface quality. Low-level design of support\nstructures may alleviate such negative effects. However, it is tedious and\nunintuitive for novice users as it is hard to predict the impact of changes to\nthe support structure on the final printed part. Curvy allows users to define\ntheir high-level preferences on the surface quality directly on the target\nobject rather than explicitly designing the supports. These preferences are\nthen automatically translated into low-level design parameters to generate the\nsupport structure. Underlying novel curvy zigzag toolpathing algorithm uses\nthese instructions to generate varying density supports by altering the spacing\nbetween individual paths in order to achieve prescribed quality. Combined with\nthe build orientation optimization, Curvy provides a practical solution to the\ndesign of support structures with minimal perceptual or functional impact on\nthe target part to be printed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 16:27:17 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Ulu", "Erva", ""], ["Ulu", "Nurcan Gecer", ""], ["Li", "Jiahao", ""], ["Hsiao", "Walter", ""]]}, {"id": "2102.10294", "submitter": "Jacopo Pantaleoni", "authors": "Markus Kettunen, Eugene d'Eon, Jacopo Pantaleoni, Jan Novak", "title": "An unbiased ray-marching transmittance estimator", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an in-depth analysis of the sources of variance in\nstate-of-the-art unbiased volumetric transmittance estimators, and propose\nseveral new methods for improving their efficiency. These combine to produce a\nsingle estimator that is universally optimal relative to prior work, with up to\nseveral orders of magnitude lower variance at the same cost, and has zero\nvariance for any ray with non-varying extinction. We first reduce the variance\nof truncated power-series estimators using a novel efficient application of\nU-statistics. We then greatly reduce the average expansion order of the power\nseries and redistribute density evaluations to filter the optical depth\nestimates with an equidistant sampling comb. Combined with the use of an online\ncontrol variate built from a sampled mean density estimate, the resulting\nestimator effectively performs ray marching most of the time while using\nrarely-sampled higher order terms to correct the bias.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:08:11 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kettunen", "Markus", ""], ["d'Eon", "Eugene", ""], ["Pantaleoni", "Jacopo", ""], ["Novak", "Jan", ""]]}, {"id": "2102.10320", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Mohammad Rahmani-Asl", "title": "GenFloor: Interactive Generative Space Layout System via Encoded Tree\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated floorplanning or space layout planning has been a long-standing\nNP-hard problem in the field of computer-aided design, with applications in\nintegrated circuits, architecture, urbanism, and operational research. In this\npaper, we introduce GenFloor, an interactive design system that takes\ngeometrical, topological, and performance goals and constraints as input and\nprovides optimized spatial design solutions as output. As part of our work, we\npropose three novel permutation methods for existing space layout graph\nrepresentations, namely O-Tree and B*-Tree representations. We implement our\nproposed floorplanning methods as a package for Dynamo, a visual programming\ntool, with a custom GUI and additional evaluation functionalities to facilitate\ndesigners in their generative design workflow. Furthermore, we illustrate the\nperformance of GenFloor in two sets of case-study experiments for residential\nfloorplanning tasks by (a) measuring the ability of the proposed system to find\na known optimal solution, and (b) observing how the system can generate diverse\nfloorplans while addressing given a constant residential design problem. Our\nresults indicate convergence to the global optimum is achieved while offering a\ndiverse set of solutions of a residential floorplan corresponding to the\nPareto-optimums of the solution landscape.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 12:03:15 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Rahmani-Asl", "Mohammad", ""]]}, {"id": "2102.10375", "submitter": "Chaochao Li", "authors": "Chaochao Li, Mingliang Xu", "title": "Hybrid-driven Trajectory Prediction Based on Group Emotion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid-driven trajectory prediction method based on group\nemotion. The data driven and model driven methods are combined to make a\ncompromise between the controllability, generality, and efficiency of the\nmethod on the basis of simulating more real crowd movements. A hybrid driven\nmethod is proposed to improve the reliability of the calculation results based\non real crowd data, and ensure the controllability of the model. It reduces the\ndependence of our model on real data and realizes the complementary advantages\nof these two kinds of methods. In addition, we divide crowd into groups based\non human relations in society. So our method can calculate the movements in\ndifferent scales. We predict individual movement trajectories according to the\ntrajectories of group and fully consider the influence of the group movement\nstate on the individual movements. Besides we also propose a group emotion\ncalculation method and our method also considers the effect of group emotion on\ncrowd movements.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 15:52:39 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Li", "Chaochao", ""], ["Xu", "Mingliang", ""]]}, {"id": "2102.11026", "submitter": "Siyuan Shen", "authors": "Siyuan Shen, Yang Yin, Tianjia Shao, He Wang, Chenfanfu Jiang, Lei\n  Lan, Kun Zhou", "title": "High-order Differentiable Autoencoder for Nonlinear Model Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a new avenue for exploiting deep neural networks to\nimprove physics-based simulation. Specifically, we integrate the classic\nLagrangian mechanics with a deep autoencoder to accelerate elastic simulation\nof deformable solids. Due to the inertia effect, the dynamic equilibrium cannot\nbe established without evaluating the second-order derivatives of the deep\nautoencoder network. This is beyond the capability of off-the-shelf automatic\ndifferentiation packages and algorithms, which mainly focus on the gradient\nevaluation. Solving the nonlinear force equilibrium is even more challenging if\nthe standard Newton's method is to be used. This is because we need to compute\na third-order derivative of the network to obtain the variational Hessian. We\nattack those difficulties by exploiting complex-step finite difference, coupled\nwith reverse automatic differentiation. This strategy allows us to enjoy the\nconvenience and accuracy of complex-step finite difference and in the meantime,\nto deploy complex-value perturbations as collectively as possible to save\nexcessive network passes. With a GPU-based implementation, we are able to wield\ndeep autoencoders (e.g., $10+$ layers) with a relatively high-dimension latent\nspace in real-time. Along this pipeline, we also design a sampling network and\na weighting network to enable \\emph{weight-varying} Cubature integration in\norder to incorporate nonlinearity in the model reduction. We believe this work\nwill inspire and benefit future research efforts in nonlinearly reduced\nphysical simulation problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 02:30:14 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Shen", "Siyuan", ""], ["Yin", "Yang", ""], ["Shao", "Tianjia", ""], ["Wang", "He", ""], ["Jiang", "Chenfanfu", ""], ["Lan", "Lei", ""], ["Zhou", "Kun", ""]]}, {"id": "2102.11175", "submitter": "Hessam Djavaherpour", "authors": "Hessam Djavaherpour, Faramarz Samavati, Ali Mahdavi-Amiri, Fatemeh\n  Yazdanbakhsh, Samuel Huron, Richard Levy, Yvonne Jansen, Lora Oehlberg", "title": "Data to Physicalization: A Survey of the Physical Rendering Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical representations of data offer physical and spatial ways of looking\nat, navigating, and interacting with data. While digital fabrication has\nfacilitated the creation of objects with data-driven geometry, rendering data\nas a physically fabricated object is still a daunting leap for many\nphysicalization designers. Rendering in the scope of this research refers to\nthe back-and-forth process from digital design to digital fabrication and its\nspecific challenges. We developed a corpus of example data physicalizations\nfrom research literature and physicalization practice. This survey then unpacks\nthe \"rendering\" phase of the extended InfoVis pipeline in greater detail\nthrough these examples, with the aim of identifying ways that researchers,\nartists, and industry practitioners \"render\" physicalizations using digital\ndesign and fabrication tools.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:53:48 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Djavaherpour", "Hessam", ""], ["Samavati", "Faramarz", ""], ["Mahdavi-Amiri", "Ali", ""], ["Yazdanbakhsh", "Fatemeh", ""], ["Huron", "Samuel", ""], ["Levy", "Richard", ""], ["Jansen", "Yvonne", ""], ["Oehlberg", "Lora", ""]]}, {"id": "2102.11541", "submitter": "Lan Chen", "authors": "Lan Chen, Lin Gao, Jie Yang, Shibiao Xu, Juntao Ye, Xiaopeng Zhang,\n  Yu-Kun Lai", "title": "Deep Deformation Detail Synthesis for Thin Shell Models", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In physics-based cloth animation, rich folds and detailed wrinkles are\nachieved at the cost of expensive computational resources and huge labor\ntuning. Data-driven techniques make efforts to reduce the computation\nsignificantly by a database. One type of methods relies on human poses to\nsynthesize fitted garments which cannot be applied to general cloth. Another\ntype of methods adds details to the coarse meshes without such restrictions.\nHowever, existing works usually utilize coordinate-based representations which\ncannot cope with large-scale deformation, and requires dense vertex\ncorrespondences between coarse and fine meshes. Moreover, as such methods only\nadd details, they require coarse meshes to be close to fine meshes, which can\nbe either impossible, or require unrealistic constraints when generating fine\nmeshes. To address these challenges, we develop a temporally and spatially\nas-consistent-as-possible deformation representation (named TS-ACAP) and a\nDeformTransformer network to learn the mapping from low-resolution meshes to\ndetailed ones. This TS-ACAP representation is designed to ensure both spatial\nand temporal consistency for sequential large-scale deformations from cloth\nanimations. With this representation, our DeformTransformer network first\nutilizes two mesh-based encoders to extract the coarse and fine features,\nrespectively. To transduct the coarse features to the fine ones, we leverage\nthe Transformer network that consists of frame-level attention mechanisms to\nensure temporal coherence of the prediction. Experimental results show that our\nmethod is able to produce reliable and realistic animations in various datasets\nat high frame rates: 10 ~ 35 times faster than physics-based simulation, with\nsuperior detail synthesis abilities than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 08:09:11 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Chen", "Lan", ""], ["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Xu", "Shibiao", ""], ["Ye", "Juntao", ""], ["Zhang", "Xiaopeng", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "2102.11617", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and\n  Gustav Eje Henter", "title": "A large, crowdsourced evaluation of gesture generation systems on common\n  data: The GENEA Challenge 2020", "comments": "Accepted for publication at the 26th International Conference on\n  Intelligent User Interfaces (IUI'21). 11 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3397481.3450692", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-speech gestures, gestures that accompany speech, play an important role in\nhuman communication. Automatic co-speech gesture generation is thus a key\nenabling technology for embodied conversational agents (ECAs), since humans\nexpect ECAs to be capable of multi-modal communication. Research into gesture\ngeneration is rapidly gravitating towards data-driven methods. Unfortunately,\nindividual research efforts in the field are difficult to compare: there are no\nestablished benchmarks, and each study tends to use its own dataset, motion\nvisualisation, and evaluation methodology. To address this situation, we\nlaunched the GENEA Challenge, a gesture-generation challenge wherein\nparticipating teams built automatic gesture-generation systems on a common\ndataset, and the resulting systems were evaluated in parallel in a large,\ncrowdsourced user study using the same motion-rendering pipeline. Since\ndifferences in evaluation outcomes between systems now are solely attributable\nto differences between the motion-generation methods, this enables benchmarking\nrecent approaches against one another in order to get a better impression of\nthe state of the art in the field. This paper reports on the purpose, design,\nresults, and implications of our challenge.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:54:58 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Kucherenko", "Taras", ""], ["Jonell", "Patrik", ""], ["Yoon", "Youngwoo", ""], ["Wolfert", "Pieter", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2102.11861", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Valentin Deschaintre, Niloy J. Mitra, Tobias Ritschel", "title": "Generative Modelling of BRDF Textures from Flash Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn a latent space for easy capture, semantic editing, consistent\ninterpolation, and efficient reproduction of visual material appearance. When\nusers provide a photo of a stationary natural material captured under flash\nlight illumination, it is converted in milliseconds into a latent material\ncode. In a second step, conditioned on the material code, our method, again in\nmilliseconds, produces an infinite and diverse spatial field of BRDF model\nparameters (diffuse albedo, specular albedo, roughness, normals) that allows\nrendering in complex scenes and illuminations, matching the appearance of the\ninput picture. Technically, we jointly embed all flash images into a latent\nspace using a convolutional encoder, and -- conditioned on these latent codes\n-- convert random spatial fields into fields of BRDF parameters using a\nconvolutional neural network (CNN). We condition these BRDF parameters to match\nthe visual characteristics (statistics and spectra of visual features) of the\ninput under matching light. A user study confirms that the semantics of the\nlatent material space agree with user expectations and compares our approach\nfavorably to previous work.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 18:45:18 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Henzler", "Philipp", ""], ["Deschaintre", "Valentin", ""], ["Mitra", "Niloy J.", ""], ["Ritschel", "Tobias", ""]]}, {"id": "2102.12285", "submitter": "Jie Guo Dr.", "authors": "Jie Guo, Bingyang Hu, Yanjun Chen, Yuanqi Li, Yanwen Guo, Ling-Qi Yan", "title": "Rendering Discrete Participating Media with Geometrical Optics\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the scattering of light in participating media composed of\nsparsely and randomly distributed discrete particles. The particle size is\nexpected to range from the scale of the wavelength to the scale several orders\nof magnitude greater than the wavelength, and the appearance shows distinct\ngraininess as opposed to the smooth appearance of continuous media. One\nfundamental issue in physically-based synthesizing this appearance is to\ndetermine necessary optical properties in every local region. Since these\noptical properties vary spatially, we resort to geometrical optics\napproximation (GOA), a highly efficient alternative to rigorous Lorenz-Mie\ntheory, to quantitatively represent the scattering of a single particle. This\nenables us to quickly compute bulk optical properties according to any particle\nsize distribution. Then, we propose a practical Monte Carlo rendering solution\nto solve the transfer of energy in discrete participating media. Results show\nthat for the first time our proposed framework can simulate a wide range of\ndiscrete participating media with different levels of graininess and converges\nto continuous media as the particle concentration increases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 13:48:13 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Guo", "Jie", ""], ["Hu", "Bingyang", ""], ["Chen", "Yanjun", ""], ["Li", "Yuanqi", ""], ["Guo", "Yanwen", ""], ["Yan", "Ling-Qi", ""]]}, {"id": "2102.12302", "submitter": "Rajmund Nagy", "authors": "Rajmund Nagy, Taras Kucherenko, Birger Moell, Andr\\'e Pereira, Hedvig\n  Kjellstr\\\"om and Ulysses Bernardet", "title": "A Framework for Integrating Gesture Generation Models into Interactive\n  Conversational Agents", "comments": "Rajmund Nagy and Taras Kucherenko contributed equally to this work.\n  To be published in the Proceedings of the 20th International Conference on\n  Autonomous Agents and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021,\n  IFAA-MAS, 3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied conversational agents (ECAs) benefit from non-verbal behavior for\nnatural and efficient interaction with users. Gesticulation - hand and arm\nmovements accompanying speech - is an essential part of non-verbal behavior.\nGesture generation models have been developed for several decades: starting\nwith rule-based and ending with mainly data-driven methods. To date, recent\nend-to-end gesture generation methods have not been evaluated in a real-time\ninteraction with users. We present a proof-of-concept framework, which is\nintended to facilitate evaluation of modern gesture generation models in\ninteraction.\n  We demonstrate an extensible open-source framework that contains three\ncomponents: 1) a 3D interactive agent; 2) a chatbot backend; 3) a gesticulating\nsystem. Each component can be replaced, making the proposed framework\napplicable for investigating the effect of different gesturing models in\nreal-time interactions with different communication modalities, chatbot\nbackends, or different agent appearances. The code and video are available at\nthe project page https://nagyrajmund.github.io/project/gesturebot.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 14:31:21 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Nagy", "Rajmund", ""], ["Kucherenko", "Taras", ""], ["Moell", "Birger", ""], ["Pereira", "Andr\u00e9", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Bernardet", "Ulysses", ""]]}, {"id": "2102.12682", "submitter": "Jakub Maksymilian Fober", "authors": "Jakub Maksymilian Fober", "title": "Pantomorphic Perspective for Immersive Imagery", "comments": "8 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A wide choice of cinematic lenses enables motion-picture creators to adapt\nimage visual-appearance to their creative vision. Such choice does not exist in\nthe realm of real-time computer graphics, where only one type of perspective\nprojection is widely used. This work provides a perspective imaging model that\nin an artistically convincing manner resembles anamorphic photography lens\nvariety and more. It presents an asymmetrical-anamorphic azimuthal projection\nmap with natural vignetting and realistic chromatic aberration. The\nmathematical model for this projection has been chosen such that its parameters\nreflect the psycho-physiological aspects of visual perception. That enables its\nuse in artistic and professional environments, where specific aspects of the\nphotographed space are to be presented.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 04:46:53 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 03:18:57 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 21:09:24 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 21:47:34 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Fober", "Jakub Maksymilian", ""]]}, {"id": "2102.13191", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Xie Li, Yang Hu, Huwan Peng, Michael Taylor, Shuaiwen\n  Leon Song", "title": "Q-VR: System-Level Design for Future Mobile Collaborative Virtual\n  Reality", "comments": null, "journal-ref": null, "doi": "10.1145/3445814.3446715", "report-no": null, "categories": "cs.AR cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High Quality Mobile Virtual Reality (VR) is what the incoming graphics\ntechnology era demands: users around the world, regardless of their hardware\nand network conditions, can all enjoy the immersive virtual experience.\nHowever, the state-of-the-art software-based mobile VR designs cannot fully\nsatisfy the realtime performance requirements due to the highly interactive\nnature of user's actions and complex environmental constraints during VR\nexecution. Inspired by the unique human visual system effects and the strong\ncorrelation between VR motion features and realtime hardware-level information,\nwe propose Q-VR, a novel dynamic collaborative rendering solution via\nsoftware-hardware co-design for enabling future low-latency high-quality mobile\nVR. At software-level, Q-VR provides flexible high-level tuning interface to\nreduce network latency while maintaining user perception. At hardware-level,\nQ-VR accommodates a wide spectrum of hardware and network conditions across\nusers by effectively leveraging the computing capability of the increasingly\npowerful VR hardware. Extensive evaluation on real-world games demonstrates\nthat Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to\n6.7x) over the traditional local rendering design in commercial VR devices, and\na 4.1x frame rate improvement over the state-of-the-art static collaborative\nrendering.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:56:05 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xie", "Chenhao", ""], ["Li", "Xie", ""], ["Hu", "Yang", ""], ["Peng", "Huwan", ""], ["Taylor", "Michael", ""], ["Song", "Shuaiwen Leon", ""]]}]