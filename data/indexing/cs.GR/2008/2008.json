[{"id": "2008.00151", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Jian Zhao, Francine Chen, Kwan-Liu Ma", "title": "A Visual Analytics Framework for Contrastive Network Analysis", "comments": "To appear in IEEE Conference on Visual Analytics Science and\n  Technology (VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common network analysis task is comparison of two networks to identify\nunique characteristics in one network with respect to the other. For example,\nwhen comparing protein interaction networks derived from normal and cancer\ntissues, one essential task is to discover protein-protein interactions unique\nto cancer tissues. However, this task is challenging when the networks contain\ncomplex structural (and semantic) relations. To address this problem, we design\nContraNA, a visual analytics framework leveraging both the power of machine\nlearning for uncovering unique characteristics in networks and also the\neffectiveness of visualization for understanding such uniqueness. The basis of\nContraNA is cNRL, which integrates two machine learning schemes, network\nrepresentation learning (NRL) and contrastive learning (CL), to generate a\nlow-dimensional embedding that reveals the uniqueness of one network when\ncompared to another. ContraNA provides an interactive visualization interface\nto help analyze the uniqueness by relating embedding results and network\nstructures as well as explaining the learned features by cNRL. We demonstrate\nthe usefulness of ContraNA with two case studies using real-world datasets. We\nalso evaluate through a controlled user study with 12 participants on network\ncomparison tasks. The results show that participants were able to both\neffectively identify unique characteristics from complex networks and interpret\nthe results obtained from cNRL.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 02:18:10 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 01:46:51 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Zhao", "Jian", ""], ["Chen", "Francine", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2008.00305", "submitter": "Omid Poursaeed", "authors": "Omid Poursaeed, Tianxing Jiang, Han Qiao, Nayun Xu, Vladimir G. Kim", "title": "Self-supervised Learning of Point Clouds via Orientation Estimation", "comments": "3DV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds provide a compact and efficient representation of 3D shapes.\nWhile deep neural networks have achieved impressive results on point cloud\nlearning tasks, they require massive amounts of manually labeled data, which\ncan be costly and time-consuming to collect. In this paper, we leverage 3D\nself-supervision for learning downstream tasks on point clouds with fewer\nlabels. A point cloud can be rotated in infinitely many ways, which provides a\nrich label-free source for self-supervision. We consider the auxiliary task of\npredicting rotations that in turn leads to useful features for other tasks such\nas shape classification and 3D keypoint prediction. Using experiments on\nShapeNet and ModelNet, we demonstrate that our approach outperforms the\nstate-of-the-art. Moreover, features learned by our model are complementary to\nother self-supervised methods and combining them leads to further performance\nimprovement.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:49:45 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 01:46:11 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Poursaeed", "Omid", ""], ["Jiang", "Tianxing", ""], ["Qiao", "Han", ""], ["Xu", "Nayun", ""], ["Kim", "Vladimir G.", ""]]}, {"id": "2008.00409", "submitter": "Min Tang", "authors": "Cheng Li and Min Tang and Ruofeng Tong and Ming Cai and Jieyi Zhao and\n  Dinesh Manocha", "title": "P-Cloth: Interactive Complex Cloth Simulation on Multi-GPU Systems using\n  Dynamic Matrix Assembly and Pipelined Implicit Integrators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel parallel algorithm for cloth simulation that exploits\nmultiple GPUs for fast computation and the handling of very high resolution\nmeshes. To accelerate implicit integration, we describe new parallel algorithms\nfor sparse matrix-vector multiplication (SpMV) and for dynamic matrix assembly\non a multi-GPU workstation. Our algorithms use a novel work queue generation\nscheme for a fat-tree GPU interconnect topology. Furthermore, we present a\nnovel collision handling scheme that uses spatial hashing for discrete and\ncontinuous collision detection along with a non-linear impact zone solver. Our\nparallel schemes can distribute the computation and storage overhead among\nmultiple GPUs and enable us to perform almost interactive simulation on complex\ncloth meshes, which can hardly be handled on a single GPU due to memory\nlimitations. We have evaluated the performance with two multi-GPU workstations\n(with 4 and 8 GPUs, respectively) on cloth meshes with 0.5-1.65M triangles. Our\napproach can reliably handle the collisions and generate vivid wrinkles and\nfolds at 2-5 fps, which is significantly faster than prior cloth simulation\nsystems. We observe almost linear speedups with respect to the number of GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 06:34:37 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 13:58:47 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Li", "Cheng", ""], ["Tang", "Min", ""], ["Tong", "Ruofeng", ""], ["Cai", "Ming", ""], ["Zhao", "Jieyi", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2008.00485", "submitter": "Yifei Shi", "authors": "Yifei Shi, Junwen Huang, Hongjia Zhang, Xin Xu, Szymon Rusinkiewicz,\n  Kai Xu", "title": "SymmetryNet: Learning to Predict Reflectional and Rotational Symmetries\n  of 3D Shapes from Single-View RGB-D Images", "comments": "15 pages", "journal-ref": "ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of symmetry detection of 3D shapes from single-view\nRGB-D images, where severely missing data renders geometric detection approach\ninfeasible. We propose an end-to-end deep neural network which is able to\npredict both reflectional and rotational symmetries of 3D objects present in\nthe input RGB-D image. Directly training a deep model for symmetry prediction,\nhowever, can quickly run into the issue of overfitting. We adopt a multi-task\nlearning approach. Aside from symmetry axis prediction, our network is also\ntrained to predict symmetry correspondences. In particular, given the 3D points\npresent in the RGB-D image, our network outputs for each 3D point its symmetric\ncounterpart corresponding to a specific predicted symmetry. In addition, our\nnetwork is able to detect for a given shape multiple symmetries of different\ntypes. We also contribute a benchmark of 3D symmetry detection based on\nsingle-view RGB-D images. Extensive evaluation on the benchmark demonstrates\nthe strong generalization ability of our method, in terms of high accuracy of\nboth symmetry axis prediction and counterpart estimation. In particular, our\nmethod is robust in handling unseen object instances with large variation in\nshape, multi-symmetry composition, as well as novel object categories.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 14:10:09 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 07:54:41 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 23:56:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Shi", "Yifei", ""], ["Huang", "Junwen", ""], ["Zhang", "Hongjia", ""], ["Xu", "Xin", ""], ["Rusinkiewicz", "Szymon", ""], ["Xu", "Kai", ""]]}, {"id": "2008.00579", "submitter": "Jernej Barbic", "authors": "Bohan Wang, George Matcuk, Jernej Barbic", "title": "Modeling of Personalized Anatomy using Plastic Strains", "comments": "18 pages, 24 figures. Rejected from ACM SIGGRAPH 2020 and ACM\n  SIGGRAPH Asia 2020. Resubmission is under preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a method for modeling solid objects undergoing large spatially\nvarying and/or anisotropic strains, and use it to reconstruct human anatomy\nfrom medical images. Our novel shape deformation method uses plastic strains\nand the Finite Element Method to successfully model shapes undergoing large\nand/or anisotropic strains, specified by sparse point constraints on the\nboundary of the object. We extensively compare our method to standard\nsecond-order shape deformation methods, variational methods and surface-based\nmethods and demonstrate that our method avoids the spikiness, wiggliness and\nother artefacts of previous methods. We demonstrate how to perform such shape\ndeformation both for attached and un-attached (\"free flying\") objects, using a\nnovel method to solve linear systems with singular matrices with a known\nnullspace. While our method is applicable to general large-strain shape\ndeformation modeling, we use it to create personalized 3D triangle and\nvolumetric meshes of human organs, based on MRI or CT scans. Given a medically\naccurate anatomy template of a generic individual, we optimize the geometry of\nthe organ to match the MRI or CT scan of a specific individual. Our examples\ninclude human hand muscles, a liver, a hip bone, and a gluteus medius muscle\n(\"hip abductor\").\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 22:46:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Bohan", ""], ["Matcuk", "George", ""], ["Barbic", "Jernej", ""]]}, {"id": "2008.00666", "submitter": "Jiacheng Pan", "authors": "Jiacheng Pan, Wei Chen, Xiaodong Zhao, Shuyue Zhou, Wei Zeng, Minfeng\n  Zhu, Jian Chen, Siwei Fu, Yingcai Wu", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and evaluate a novel layout fine-tuning technique for node-link\ndiagrams that facilitates exemplar-based adjustment of a group of substructures\nin batching mode. The key idea is to transfer user modifications on a local\nsubstructure to other substructures in the whole graph that are topologically\nsimilar to the exemplar. We first precompute a canonical representation for\neach substructure with node embedding techniques and then use it for on-the-fly\nsubstructure retrieval. We design and develop a light-weight interactive system\nto enable intuitive adjustment, modification transfer, and visual graph\nexploration. We also report some results of quantitative comparisons, three\ncase studies, and a within-participant user study.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 06:36:05 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:32:39 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 05:10:57 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 02:33:29 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2020 10:23:02 GMT"}, {"version": "v6", "created": "Sun, 6 Sep 2020 10:38:57 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Pan", "Jiacheng", ""], ["Chen", "Wei", ""], ["Zhao", "Xiaodong", ""], ["Zhou", "Shuyue", ""], ["Zeng", "Wei", ""], ["Zhu", "Minfeng", ""], ["Chen", "Jian", ""], ["Fu", "Siwei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2008.00823", "submitter": "Yibing Song", "authors": "Yinglong Wang, Yibing Song, Chao Ma, and Bing Zeng", "title": "Rethinking Image Deraining via Rain Streaks and Vapors", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image deraining regards an input image as a fusion of a background\nimage, a transmission map, rain streaks, and atmosphere light. While advanced\nmodels are proposed for image restoration (i.e., background image generation),\nthey regard rain streaks with the same properties as background rather than\ntransmission medium. As vapors (i.e., rain streaks accumulation or fog-like\nrain) are conveyed in the transmission map to model the veiling effect, the\nfusion of rain streaks and vapors do not naturally reflect the rain image\nformation. In this work, we reformulate rain streaks as transmission medium\ntogether with vapors to model rain imaging. We propose an encoder-decoder CNN\nnamed as SNet to learn the transmission map of rain streaks. As rain streaks\nappear with various shapes and directions, we use ShuffleNet units within SNet\nto capture their anisotropic representations. As vapors are brought by rain\nstreaks, we propose a VNet containing spatial pyramid pooling (SSP) to predict\nthe transmission map of vapors in multi-scales based on that of rain streaks.\nMeanwhile, we use an encoder CNN named ANet to estimate atmosphere light. The\nSNet, VNet, and ANet are jointly trained to predict transmission maps and\natmosphere light for rain image restoration. Extensive experiments on the\nbenchmark datasets demonstrate the effectiveness of the proposed visual model\nto predict rain streaks and vapors. The proposed deraining method performs\nfavorably against state-of-the-art deraining approaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:15:07 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Yinglong", ""], ["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Zeng", "Bing", ""]]}, {"id": "2008.01068", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang, Yu-Qi Yang, Qian-Fang Zou, Zhirong Wu, Yang Liu, Xin\n  Tong", "title": "Unsupervised 3D Learning for Shape Analysis via Multiresolution Instance\n  Discrimination", "comments": "Accepted by AAAI 2021. Code:\n  https://github.com/microsoft/O-CNN/blob/master/docs/unsupervised.md", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although unsupervised feature learning has demonstrated its advantages to\nreducing the workload of data labeling and network design in many fields,\nexisting unsupervised 3D learning methods still cannot offer a generic network\nfor various shape analysis tasks with competitive performance to supervised\nmethods. In this paper, we propose an unsupervised method for learning a\ngeneric and efficient shape encoding network for different shape analysis\ntasks. The key idea of our method is to jointly encode and learn shape and\npoint features from unlabeled 3D point clouds. For this purpose, we adapt\nHR-Net to octree-based convolutional neural networks for jointly encoding shape\nand point features with fused multiresolution subnetworks and design a\nsimple-yet-efficient Multiresolution Instance Discrimination (MID) loss for\njointly learning the shape and point features. Our network takes a 3D point\ncloud as input and output both shape and point features. After training, the\nnetwork is concatenated with simple task-specific back-end layers and\nfine-tuned for different shape analysis tasks. We evaluate the efficacy and\ngenerality of our method and validate our network and loss design with a set of\nshape analysis tasks, including shape classification, semantic shape\nsegmentation, as well as shape registration tasks. With simple back-ends, our\nnetwork demonstrates the best performance among all unsupervised methods and\nachieves competitive performance to supervised methods, especially in tasks\nwith a small labeled dataset. For fine-grained shape segmentation, our method\neven surpasses existing supervised methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:58:46 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 02:49:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Yang", "Yu-Qi", ""], ["Zou", "Qian-Fang", ""], ["Wu", "Zhirong", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "2008.01174", "submitter": "Naresh Kumar Mallenahalli Prof. Dr.", "authors": "K. Seshadri and M. Naresh Kumar", "title": "Mesh Processing Strategies and Fractals for Three Dimensional\n  Morphological Analysis of a Granitic Terrain using IRS LISS IV and Carto DEM", "comments": "3 pages, 2 figures, ESRI User Conference, September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Reality (VR) enabled applications are becoming very important to\nvisualize the terrain features in 3D. In general 3D datasets generated from\nhigh-resolution satellites and DEM occupy large volumes of data. However,\nlightweight datasets are required to create better user experiences on VR\nplatforms. So, the present study develops a methodology to generate datasets\ncompatible with VR using Indian Remote Sensing satellite (IRS) sensors. A\nLinear Imaging Self-Scanning System - IV (LISS IV) with 5.8 m spatial\nresolution and Carto DEM are used for generating the 3D view using the Arc\nenvironment and then converted into virtual reality modeling language (VRML)\nformat. In order to reduce the volume of the VRML dataset a quadratic edge\ncollapse decimation method is applied which reduces the number of faces in the\nmesh while preserving the boundary and/or normal. A granitic terrain in the\nsouth-west part of Hyderabad comprising of dyke intrusion is considered for the\ngeneration of 3D VR dataset, as it has high elevation differences thus\nrendering it most suitable for the present study. Further, the enhanced\ngeomorphological features such as hills and valleys, geological structures such\nas fractures, intrusive (dykes) are studied and found suitable for better\ninterpretation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:51:31 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Seshadri", "K.", ""], ["Kumar", "M. Naresh", ""]]}, {"id": "2008.01286", "submitter": "Manush Bhatt", "authors": "Manush Bhatt, Rajesh Kalyanam, Gen Nishida, Liu He, Christopher May,\n  Dev Niyogi, Daniel Aliaga", "title": "Design and Deployment of Photo2Building: A Cloud-based Procedural\n  Modeling Tool as a Service", "comments": "7 pages, 7 figures, PEARC '20: Practice and Experience in Advanced\n  Research Computing, July 26--30, 2020, Portland, OR, USA", "journal-ref": "ACM, PEARC 2020", "doi": "10.1145/3311790.3396670", "report-no": null, "categories": "cs.DC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Photo2Building tool to create a plausible 3D model of a building\nfrom only a single photograph. Our tool is based on a prior desktop version\nwhich, as described in this paper, is converted into a client-server model,\nwith job queuing, web-page support, and support of concurrent usage. The\nreported cloud-based web-accessible tool can reconstruct a building in 40\nseconds on average and costing only 0.60 USD with current pricing. This\nprovides for an extremely scalable and possibly widespread tool for creating\nbuilding models for use in urban design and planning applications. With the\ngrowing impact of rapid urbanization on weather and climate and resource\navailability, access to such a service is expected to help a wide variety of\nusers such as city planners, urban meteorologists worldwide in the quest to\nimproved prediction of urban weather and designing climate-resilient cities of\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:43:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bhatt", "Manush", ""], ["Kalyanam", "Rajesh", ""], ["Nishida", "Gen", ""], ["He", "Liu", ""], ["May", "Christopher", ""], ["Niyogi", "Dev", ""], ["Aliaga", "Daniel", ""]]}, {"id": "2008.01332", "submitter": "Eloise Berson", "authors": "Elo\\\"ise Berson, Catherine Soladi\\'e, Nicolas Stoiber", "title": "Real-Time Cleaning and Refinement of Facial Animation Signals", "comments": "ICGSP 2020: Proceedings of the 2020 The 4th International Conference\n  on Graphics and Signal Processing", "journal-ref": null, "doi": "10.1145/3406971.3406985", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for real-time animated 3D content in the\nentertainment industry and beyond, performance-based animation has garnered\ninterest among both academic and industrial communities. While recent solutions\nfor motion-capture animation have achieved impressive results, handmade\npost-processing is often needed, as the generated animations often contain\nartifacts. Existing real-time motion capture solutions have opted for standard\nsignal processing methods to strengthen temporal coherence of the resulting\nanimations and remove inaccuracies. While these methods produce smooth results,\nthey inherently filter-out part of the dynamics of facial motion, such as high\nfrequency transient movements. In this work, we propose a real-time animation\nrefining system that preserves -- or even restores -- the natural dynamics of\nfacial motions. To do so, we leverage an off-the-shelf recurrent neural network\narchitecture that learns proper facial dynamics patterns on clean animation\ndata. We parametrize our system using the temporal derivatives of the signal,\nenabling our network to process animations at any framerate. Qualitative\nresults show that our system is able to retrieve natural motion signals from\nnoisy or degraded input animation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 05:21:02 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Berson", "Elo\u00efse", ""], ["Soladi\u00e9", "Catherine", ""], ["Stoiber", "Nicolas", ""]]}, {"id": "2008.01358", "submitter": "Wei Pan", "authors": "Chaofan Dai, Wei Pan and Xuequan Lu", "title": "Segmentation Based Mesh Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Feature-preserving mesh denoising has received noticeable attention recently.\nMany methods often design great weighting for anisotropic surfaces and small\nweighting for isotropic surfaces, to preserve sharp features. However, they\noften disregard the fact that small weights still pose negative impacts to the\ndenoising outcomes. Furthermore, it may increase the difficulty in parameter\ntuning, especially for users without any background knowledge. In this paper,\nwe propose a novel clustering method for mesh denoising, which can avoid the\ndisturbance of anisotropic information and be easily embedded into\ncommonly-used mesh denoising frameworks. Extensive experiments have been\nconducted to validate our method, and demonstrate that it can enhance the\ndenoising results of some existing methods remarkably both visually and\nquantitatively. It also largely relaxes the parameter tuning procedure for\nusers, in terms of increasing stability for existing mesh denoising methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 06:31:02 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 10:03:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Dai", "Chaofan", ""], ["Pan", "Wei", ""], ["Lu", "Xuequan", ""]]}, {"id": "2008.01363", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki", "title": "Illustrations of non-Euclidean geometry in virtual reality", "comments": "Submitted for publication in the Yearbook of Moving Image Studies,\n  this is the reviewed version", "journal-ref": null, "doi": null, "report-no": "RIKEN-iTHEMS-Report-20", "categories": "math.HO cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical objects are generally abstract and not very approachable.\nIllustrations and interactive visualizations help both students and\nprofessionals to comprehend mathematical material and to work with it. This\napproach lends itself particularly well to geometrical objects. An example for\nthis category of mathematical objects are hyperbolic geometric spaces. When\nEuclid lay down the foundations of mathematics, his formulation of geometry\nreflected the surrounding space, as humans perceive it. For about two\nmillennia, it remained unclear whether there are alternative geometric spaces\nthat carry their own, unique mathematical properties and that do not reflect\nhuman every-day perceptions. Finally, in the early 19th century, several\nmathematicians described such geometries, which do not follow Euclid's rules\nand which were at first interesting solely from a pure mathematical point of\nview. These descriptions were not very accessible as mathematicians approached\nthe geometries via complicated collections of formulae. Within the following\ndecades, visualization aided the new concepts and two-dimensional versions of\nthese illustrations even appeared in artistic works. Furthermore, certain\naspects of Einstein's theory of relativity provided applications for\nnon-Euclidean geometric spaces. With the rise of computer graphics towards the\nend of the twentieth century, three-dimensional illustrations became available\nto explore these geometries and their non-intuitive properties. However, just\nas the canvas confines the two-dimensional depictions, the computer monitor\nconfines these three-dimensional visualizations. Only virtual reality recently\nmade it possible to present immersive experiences of non-Euclidean geometries.\nIn virtual reality, users have completely new opportunities to encounter\ngeometric properties and effects that are not present in their surrounding\nEuclidean world.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 06:39:11 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 11:11:54 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Skrodzki", "Martin", ""]]}, {"id": "2008.01541", "submitter": "Qisi Wang", "authors": "Qisi Wang, Yutian Tao, Eric Brandt, Court Cutting, and Eftychios\n  Sifakis", "title": "Optimized Processing of Localized Collisions in Projective Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for the efficient processing of contact and collision in\nvolumetric elastic models simulated using the Projective Dynamics paradigm. Our\napproach enables interactive simulation of tetrahedral meshes with more than\nhalf a million elements, provided that the model satisfies two fundamental\nproperties: the region of the model's surface that is susceptible to collision\nevents needs to be known in advance, and the simulation degrees of freedom\nassociated with that surface region should be limited to a small fraction (e.g.\n5\\%) of the total simulation nodes. Despite this conscious delineation of\nscope, our hypotheses hold true for common animation subjects, such as\nsimulated models of the human face and parts of the body. In such scenarios, a\npartial Cholesky factorization can abstract away the behavior of the\ncollision-safe subset of the face into the Schur Complement matrix with respect\nto the collision-prone region. We demonstrate how fast and accurate updates of\npenalty-based collision terms can be incorporated into this representation, and\nsolved with high efficiency on the GPU. We also demonstrate the opportunity to\niterate a partial update of the element rotations, akin to a selective\napplication of the local step, specifically on the smaller collision-prone\nregion without explicitly paying the cost associated with the rest of the\nsimulation mesh. We demonstrate efficient and robust interactive simulation in\ndetailed models from animation and medical applications.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 23:09:01 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Wang", "Qisi", ""], ["Tao", "Yutian", ""], ["Brandt", "Eric", ""], ["Cutting", "Court", ""], ["Sifakis", "Eftychios", ""]]}, {"id": "2008.01639", "submitter": "Edgar Tretschk", "authors": "Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael Zollh\\\"ofer,\n  Carsten Stoll, Christian Theobalt", "title": "PatchNets: Patch-Based Generalizable Deep Implicit 3D Shape\n  Representations", "comments": "25 pages, including supplementary material. Code:\n  https://github.com/edgar-tr/patchnets Project page:\n  https://gvv.mpi-inf.mpg.de/projects/PatchNets/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit surface representations, such as signed-distance functions, combined\nwith deep learning have led to impressive models which can represent detailed\nshapes of objects with arbitrary topology. Since a continuous function is\nlearned, the reconstructions can also be extracted at any arbitrary resolution.\nHowever, large datasets such as ShapeNet are required to train such models. In\nthis paper, we present a new mid-level patch-based surface representation. At\nthe level of patches, objects across different categories share similarities,\nwhich leads to more generalizable models. We then introduce a novel method to\nlearn this patch-based representation in a canonical space, such that it is as\nobject-agnostic as possible. We show that our representation trained on one\ncategory of objects from ShapeNet can also well represent detailed shapes from\nany other category. In addition, it can be trained using much fewer shapes,\ncompared to existing approaches. We show several applications of our new\nrepresentation, including shape interpolation and partial point cloud\ncompletion. Due to explicit control over positions, orientations and scales of\npatches, our representation is also more controllable compared to object-level\nrepresentations, which enables us to deform encoded shapes non-rigidly.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:34:46 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 13:32:02 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tretschk", "Edgar", ""], ["Tewari", "Ayush", ""], ["Golyanik", "Vladislav", ""], ["Zollh\u00f6fer", "Michael", ""], ["Stoll", "Carsten", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.01645", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Shilpika, Naohisa Sakamoto, Jorji Nonaka, Keiji\n  Yamamoto, and Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data\n  with Dimensionality Reduction", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2020 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven problem solving in many real-world applications involves analysis\nof time-dependent multivariate data, for which dimensionality reduction (DR)\nmethods are often used to uncover the intrinsic structure and features of the\ndata. However, DR is usually applied to a subset of data that is either\nsingle-time-point multivariate or univariate time-series, resulting in the need\nto manually examine and correlate the DR results out of different data subsets.\nWhen the number of dimensions is large either in terms of the number of time\npoints or attributes, this manual task becomes too tedious and infeasible. In\nthis paper, we present MulTiDR, a new DR framework that enables processing of\ntime-dependent multivariate data as a whole to provide a comprehensive overview\nof the data. With the framework, we employ DR in two steps. When treating the\ninstances, time points, and attributes of the data as a 3D array, the first DR\nstep reduces the three axes of the array to two, and the second DR step\nvisualizes the data in a lower-dimensional space. In addition, by coupling with\na contrastive learning method and interactive visualizations, our framework\nenhances analysts' ability to interpret DR results. We demonstrate the\neffectiveness of our framework with four case studies using real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:22:43 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 00:37:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Shilpika", "", ""], ["Sakamoto", "Naohisa", ""], ["Nonaka", "Jorji", ""], ["Yamamoto", "Keiji", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2008.01815", "submitter": "Kai-En Lin", "authors": "Kai-En Lin, Zexiang Xu, Ben Mildenhall, Pratul P. Srinivasan, Yannick\n  Hold-Geoffroy, Stephen DiVerdi, Qi Sun, Kalyan Sunkavalli, and Ravi\n  Ramamoorthi", "title": "Deep Multi Depth Panoramas for View Synthesis", "comments": "Published at the European Conference on Computer Vision, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based approach for novel view synthesis for\nmulti-camera 360$^{\\circ}$ panorama capture rigs. Previous work constructs RGBD\npanoramas from such data, allowing for view synthesis with small amounts of\ntranslation, but cannot handle the disocclusions and view-dependent effects\nthat are caused by large translations. To address this issue, we present a\nnovel scene representation - Multi Depth Panorama (MDP) - that consists of\nmultiple RGBD$\\alpha$ panoramas that represent both scene geometry and\nappearance. We demonstrate a deep neural network-based method to reconstruct\nMDPs from multi-camera 360$^{\\circ}$ images. MDPs are more compact than\nprevious 3D scene representations and enable high-quality, efficient new view\nrendering. We demonstrate this via experiments on both synthetic and real data\nand comparisons with previous state-of-the-art methods spanning both\nlearning-based approaches and classical RGBD-based methods.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 20:29:15 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Lin", "Kai-En", ""], ["Xu", "Zexiang", ""], ["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Hold-Geoffroy", "Yannick", ""], ["DiVerdi", "Stephen", ""], ["Sun", "Qi", ""], ["Sunkavalli", "Kalyan", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2008.01936", "submitter": "Kangxue Yin", "authors": "Kangxue Yin, Zhiqin Chen, Siddhartha Chaudhuri, Matthew Fisher,\n  Vladimir G. Kim, Hao Zhang", "title": "COALESCE: Component Assembly by Learning to Synthesize Connections", "comments": "20 pages: paper + supplementary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce COALESCE, the first data-driven framework for component-based\nshape assembly which employs deep learning to synthesize part connections. To\nhandle geometric and topological mismatches between parts, we remove the\nmismatched portions via erosion, and rely on a joint synthesis step, which is\nlearned from data, to fill the gap and arrive at a natural and plausible part\njoint. Given a set of input parts extracted from different objects, COALESCE\nautomatically aligns them and synthesizes plausible joints to connect the parts\ninto a coherent 3D object represented by a mesh. The joint synthesis network,\ndesigned to focus on joint regions, reconstructs the surface between the parts\nby predicting an implicit shape representation that agrees with existing parts,\nwhile generating a smooth and topologically meaningful connection. We employ\ntest-time optimization to further ensure that the synthesized joint region\nclosely aligns with the input parts to create realistic component assemblies\nfrom diverse input parts. We demonstrate that our method significantly\noutperforms prior approaches including baseline deep models for 3D shape\nsynthesis, as well as state-of-the-art methods for shape completion.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:12:06 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 08:11:55 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yin", "Kangxue", ""], ["Chen", "Zhiqin", ""], ["Chaudhuri", "Siddhartha", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Zhang", "Hao", ""]]}, {"id": "2008.02268", "submitter": "Daniel Duckworth", "authors": "Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Sajjadi, Jonathan T.\n  Barron, Alexey Dosovitskiy, Daniel Duckworth", "title": "NeRF in the Wild: Neural Radiance Fields for Unconstrained Photo\n  Collections", "comments": "Project website: https://nerf-w.github.io. Ricardo Martin-Brualla,\n  Noha Radwan, and Mehdi S. M. Sajjadi contributed equally to this work.\n  Updated with results for three additional scenes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for synthesizing novel views of complex\nscenes using only unstructured collections of in-the-wild photographs. We build\non Neural Radiance Fields (NeRF), which uses the weights of a multilayer\nperceptron to model the density and color of a scene as a function of 3D\ncoordinates. While NeRF works well on images of static subjects captured under\ncontrolled settings, it is incapable of modeling many ubiquitous, real-world\nphenomena in uncontrolled images, such as variable illumination or transient\noccluders. We introduce a series of extensions to NeRF to address these issues,\nthereby enabling accurate reconstructions from unstructured image collections\ntaken from the internet. We apply our system, dubbed NeRF-W, to internet photo\ncollections of famous landmarks, and demonstrate temporally consistent novel\nview renderings that are significantly closer to photorealism than the prior\nstate of the art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:51:16 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 11:02:36 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 13:45:14 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Radwan", "Noha", ""], ["Sajjadi", "Mehdi S. M.", ""], ["Barron", "Jonathan T.", ""], ["Dosovitskiy", "Alexey", ""], ["Duckworth", "Daniel", ""]]}, {"id": "2008.02396", "submitter": "Chloe LeGendre", "authors": "Chloe LeGendre, Wan-Chun Ma, Rohit Pandey, Sean Fanello, Christoph\n  Rhemann, Jason Dourgarian, Jay Busch, Paul Debevec", "title": "Learning Illumination from Diverse Portraits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based technique for estimating high dynamic range\n(HDR), omnidirectional illumination from a single low dynamic range (LDR)\nportrait image captured under arbitrary indoor or outdoor lighting conditions.\nWe train our model using portrait photos paired with their ground truth\nenvironmental illumination. We generate a rich set of such photos by using a\nlight stage to record the reflectance field and alpha matte of 70 diverse\nsubjects in various expressions. We then relight the subjects using image-based\nrelighting with a database of one million HDR lighting environments,\ncompositing the relit subjects onto paired high-resolution background imagery\nrecorded during the lighting acquisition. We train the lighting estimation\nmodel using rendering-based loss functions and add a multi-scale adversarial\nloss to estimate plausible high frequency lighting detail. We show that our\ntechnique outperforms the state-of-the-art technique for portrait-based\nlighting estimation, and we also show that our method reliably handles the\ninherent ambiguity between overall lighting strength and surface albedo,\nrecovering a similar scale of illumination for subjects with diverse skin\ntones. We demonstrate that our method allows virtual objects and digital\ncharacters to be added to a portrait photograph with consistent illumination.\nOur lighting inference runs in real-time on a smartphone, enabling realistic\nrendering and compositing of virtual objects into live video for augmented\nreality applications.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 23:41:23 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["LeGendre", "Chloe", ""], ["Ma", "Wan-Chun", ""], ["Pandey", "Rohit", ""], ["Fanello", "Sean", ""], ["Rhemann", "Christoph", ""], ["Dourgarian", "Jason", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""]]}, {"id": "2008.02401", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Peihao Zhu, Niloy Mitra, Peter Wonka", "title": "StyleFlow: Attribute-conditioned Exploration of StyleGAN-Generated\n  Images using Conditional Continuous Normalizing Flows", "comments": "\"Project Page https://rameenabdal.github.io/StyleFlow Video:\n  https://youtu.be/LRAUJUn3EqQ \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-quality, diverse, and photorealistic images can now be generated by\nunconditional GANs (e.g., StyleGAN). However, limited options exist to control\nthe generation process using (semantic) attributes, while still preserving the\nquality of the output. Further, due to the entangled nature of the GAN latent\nspace, performing edits along one attribute can easily result in unwanted\nchanges along other attributes. In this paper, in the context of conditional\nexploration of entangled latent spaces, we investigate the two sub-problems of\nattribute-conditioned sampling and attribute-controlled editing. We present\nStyleFlow as a simple, effective, and robust solution to both the sub-problems\nby formulating conditional exploration as an instance of conditional continuous\nnormalizing flows in the GAN latent space conditioned by attribute features. We\nevaluate our method using the face and the car latent space of StyleGAN, and\ndemonstrate fine-grained disentangled edits along various attributes on both\nreal photographs and StyleGAN generated images. For example, for faces, we vary\ncamera pose, illumination variation, expression, facial hair, gender, and age.\nFinally, via extensive qualitative and quantitative comparisons, we demonstrate\nthe superiority of StyleFlow to other concurrent works.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 00:10:03 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 15:39:46 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Abdal", "Rameen", ""], ["Zhu", "Peihao", ""], ["Mitra", "Niloy", ""], ["Wonka", "Peter", ""]]}, {"id": "2008.02787", "submitter": "Ye Yuan", "authors": "Mariko Isogawa, Dorian Chan, Ye Yuan, Kris Kitani, Matthew O'Toole", "title": "Efficient Non-Line-of-Sight Imaging from Transient Sinograms", "comments": "ECCV 2020. Project page:\n  https://marikoisogawa.github.io/project/c2nlos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV eess.SP physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-line-of-sight (NLOS) imaging techniques use light that diffusely reflects\noff of visible surfaces (e.g., walls) to see around corners. One approach\ninvolves using pulsed lasers and ultrafast sensors to measure the travel time\nof multiply scattered light. Unlike existing NLOS techniques that generally\nrequire densely raster scanning points across the entirety of a relay wall, we\nexplore a more efficient form of NLOS scanning that reduces both acquisition\ntimes and computational requirements. We propose a circular and confocal\nnon-line-of-sight (C2NLOS) scan that involves illuminating and imaging a common\npoint, and scanning this point in a circular path along a wall. We observe that\n(1) these C2NLOS measurements consist of a superposition of sinusoids, which we\nrefer to as a transient sinogram, (2) there exists computationally efficient\nreconstruction procedures that transform these sinusoidal measurements into 3D\npositions of hidden scatterers or NLOS images of hidden objects, and (3)\ndespite operating on an order of magnitude fewer measurements than previous\napproaches, these C2NLOS scans provide sufficient information about the hidden\nscene to solve these different NLOS imaging tasks. We show results from both\nsimulated and real C2NLOS scans.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:50:50 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Isogawa", "Mariko", ""], ["Chan", "Dorian", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris", ""], ["O'Toole", "Matthew", ""]]}, {"id": "2008.02796", "submitter": "Andrew Liu", "authors": "Andrew Liu, Shiry Ginosar, Tinghui Zhou, Alexei A. Efros, Noah Snavely", "title": "Learning to Factorize and Relight a City", "comments": "ECCV 2020 (Spotlight). Supplemental Material attached", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based framework for disentangling outdoor scenes into\ntemporally-varying illumination and permanent scene factors. Inspired by the\nclassic intrinsic image decomposition, our learning signal builds upon two\ninsights: 1) combining the disentangled factors should reconstruct the original\nimage, and 2) the permanent factors should stay constant across multiple\ntemporal samples of the same scene. To facilitate training, we assemble a\ncity-scale dataset of outdoor timelapse imagery from Google Street View, where\nthe same locations are captured repeatedly through time. This data represents\nan unprecedented scale of spatio-temporal outdoor imagery. We show that our\nlearned disentangled factors can be used to manipulate novel images in\nrealistic ways, such as changing lighting effects and scene geometry. Please\nvisit factorize-a-city.github.io for animated results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:59:54 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Liu", "Andrew", ""], ["Ginosar", "Shiry", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""], ["Snavely", "Noah", ""]]}, {"id": "2008.02912", "submitter": "Aaron Hertzmann", "authors": "Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter O'Donovan, Aaron\n  Hertzmann, Zoya Bylinskii", "title": "Predicting Visual Importance Across Graphic Design Types", "comments": null, "journal-ref": "Proceedings of UIST 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Unified Model of Saliency and Importance (UMSI),\nwhich learns to predict visual importance in input graphic designs, and\nsaliency in natural images, along with a new dataset and applications. Previous\nmethods for predicting saliency or visual importance are trained individually\non specialized datasets, making them limited in application and leading to poor\ngeneralization on novel image classes, while requiring a user to know which\nmodel to apply to which input. UMSI is a deep learning-based model\nsimultaneously trained on images from different design classes, including\nposters, infographics, mobile UIs, as well as natural images, and includes an\nautomatic classification module to classify the input. This allows the model to\nwork more effectively without requiring a user to label the input. We also\nintroduce Imp1k, a new dataset of designs annotated with importance\ninformation. We demonstrate two new design interfaces that use importance\nprediction, including a tool for adjusting the relative importance of design\nelements, and a tool for reflowing designs to new aspect ratios while\npreserving visual importance. The model, code, and importance dataset are\navailable at https://predimportance.mit.edu .\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:12:18 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Fosco", "Camilo", ""], ["Casser", "Vincent", ""], ["Bedi", "Amish Kumar", ""], ["O'Donovan", "Peter", ""], ["Hertzmann", "Aaron", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "2008.03085", "submitter": "Aritra Banerjee", "authors": "Aritra Banerjee", "title": "SimPatch: A Nearest Neighbor Similarity Match between Image Patches", "comments": "12 pages, 13 figures, Submitted to International Journal of\n  Artificial Intelligence and Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the similarity between patches in images is a fundamental building\nblock in various tasks. Naturally, the patch-size has a major impact on the\nmatching quality, and on the consequent application performance. We try to use\nlarge patches instead of relatively small patches so that each patch contains\nmore information. We use different feature extraction mechanisms to extract the\nfeatures of each individual image patches which forms a feature matrix and find\nout the nearest neighbor patches in the image. The nearest patches are\ncalculated using two different nearest neighbor algorithms in this paper for a\nquery patch for a given image and the results have been demonstrated in this\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:51:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Banerjee", "Aritra", ""]]}, {"id": "2008.03560", "submitter": "Cihan \\\"Ong\\\"un", "authors": "Cihan \\\"Ong\\\"un, Alptekin Temizel", "title": "LPMNet: Latent Part Modification and Generation for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on latent modification and generation of 3D point\ncloud object models with respect to their semantic parts. Different to the\nexisting methods which use separate networks for part generation and assembly,\nwe propose a single end-to-end Autoencoder model that can handle generation and\nmodification of both semantic parts, and global shapes. The proposed method\nsupports part exchange between 3D point cloud models and composition by\ndifferent parts to form new models by directly editing latent representations.\nThis holistic approach does not need part-based training to learn part\nrepresentations and does not introduce any extra loss besides the standard\nreconstruction loss. The experiments demonstrate the robustness of the proposed\nmethod with different object categories and varying number of points. The\nmethod can generate new models by integration of generative models such as GANs\nand VAEs and can work with unannotated point clouds by integration of a\nsegmentation module.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 17:24:37 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:41:37 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 15:44:08 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["\u00d6ng\u00fcn", "Cihan", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2008.03669", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Sebastian Feld and Thomy Phan and Pierre-Alain\n  Fayolle", "title": "Accelerating Evolutionary Construction Tree Extraction via Graph\n  Partitioning", "comments": "The 26th International Conference in Central Europe on Computer\n  Graphics, Visualization and Computer Vision 2016 in co-operation with\n  EUROGRAPHICS: University of West Bohemia, Plzen, Czech Republic May 28 - June\n  1 2018, p. 29-37", "journal-ref": null, "doi": "10.24132/CSRN.2018.2802.5", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting a Construction Tree from potentially noisy point clouds is an\nimportant aspect of Reverse Engineering tasks in Computer Aided Design.\nSolutions based on algorithmic geometry impose constraints on usable model\nrepresentations (e.g. quadric surfaces only) and noise robustness.\nRe-formulating the problem as a combinatorial optimization problem and solving\nit with an Evolutionary Algorithm can mitigate some of these constraints at the\ncost of increased computational complexity. This paper proposes a graph-based\nsearch space partitioning scheme that is able to accelerate Evolutionary\nConstruction Tree extraction while exploiting parallelization capabilities of\nmodern CPUs. The evaluation indicates a speed-up up to a factor of $46.6$\ncompared to the baseline approach while resulting tree sizes increased by\n$25.2\\%$ to $88.6\\%$.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 06:11:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Friedrich", "Markus", ""], ["Feld", "Sebastian", ""], ["Phan", "Thomy", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2008.03674", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Christoph Roch and Sebastian Feld and Carsten\n  Hahn and Pierre-Alain Fayolle", "title": "A Flexible Pipeline for the Optimization of CSG Trees", "comments": "The 28th International Conference in Central Europe on Computer\n  Graphics, Visualization and Computer Vision (WSCG) 2020, p. 79-88. Update:\n  Enlarge some of the figures and move some of the figures to the end", "journal-ref": null, "doi": "10.24132/CSRN.2020.3001.10", "report-no": null, "categories": "cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CSG trees are an intuitive, yet powerful technique for the representation of\ngeometry using a combination of Boolean set-operations and geometric\nprimitives. In general, there exists an infinite number of trees all describing\nthe same 3D solid. However, some trees are optimal regarding the number of used\noperations, their shape or other attributes, like their suitability for\nintuitive, human-controlled editing. In this paper, we present a systematic\ncomparison of newly developed and existing tree optimization methods and\npropose a flexible processing pipeline with a focus on tree editability. The\npipeline uses a redundancy removal and decomposition stage for complexity\nreduction and different (meta-)heuristics for remaining tree optimization. We\nalso introduce a new quantitative measure for CSG tree editability and show how\nit can be used as a constraint in the optimization process.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 06:45:10 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 05:43:03 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Friedrich", "Markus", ""], ["Roch", "Christoph", ""], ["Feld", "Sebastian", ""], ["Hahn", "Carsten", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2008.03806", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Sean Fanello, Yun-Ta Tsai, Tiancheng Sun, Tianfan Xue,\n  Rohit Pandey, Sergio Orts-Escolano, Philip Davidson, Christoph Rhemann, Paul\n  Debevec, Jonathan T. Barron, Ravi Ramamoorthi, William T. Freeman", "title": "Neural Light Transport for Relighting and View Synthesis", "comments": "Camera-ready version for TOG 2021. Project Page:\n  http://nlt.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3446328", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The light transport (LT) of a scene describes how it appears under different\nlighting and viewing directions, and complete knowledge of a scene's LT enables\nthe synthesis of novel views under arbitrary lighting. In this paper, we focus\non image-based LT acquisition, primarily for human bodies within a light stage\nsetup. We propose a semi-parametric approach to learn a neural representation\nof LT that is embedded in the space of a texture atlas of known geometric\nproperties, and model all non-diffuse and global LT as residuals added to a\nphysically-accurate diffuse base rendering. In particular, we show how to fuse\npreviously seen observations of illuminants and views to synthesize a new image\nof the same scene under a desired lighting condition from a chosen viewpoint.\nThis strategy allows the network to learn complex material effects (such as\nsubsurface scattering) and global illumination, while guaranteeing the physical\ncorrectness of the diffuse LT (such as hard shadows). With this learned LT, one\ncan relight the scene photorealistically with a directional light or an HDRI\nmap, synthesize novel views with view-dependent effects, or do both\nsimultaneously, all in a unified framework using a set of sparse, previously\nseen observations. Qualitative and quantitative experiments demonstrate that\nour neural LT (NLT) outperforms state-of-the-art solutions for relighting and\nview synthesis, without separate treatment for both problems that prior work\nrequires.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 20:13:15 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 16:32:01 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 15:45:52 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Zhang", "Xiuming", ""], ["Fanello", "Sean", ""], ["Tsai", "Yun-Ta", ""], ["Sun", "Tiancheng", ""], ["Xue", "Tianfan", ""], ["Pandey", "Rohit", ""], ["Orts-Escolano", "Sergio", ""], ["Davidson", "Philip", ""], ["Rhemann", "Christoph", ""], ["Debevec", "Paul", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""], ["Freeman", "William T.", ""]]}, {"id": "2008.03824", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan\n  Sunkavalli, Milo\\v{s} Ha\\v{s}an, Yannick Hold-Geoffroy, David Kriegman, Ravi\n  Ramamoorthi", "title": "Neural Reflectance Fields for Appearance Acquisition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Neural Reflectance Fields, a novel deep scene representation that\nencodes volume density, normal and reflectance properties at any 3D point in a\nscene using a fully-connected neural network. We combine this representation\nwith a physically-based differentiable ray marching framework that can render\nimages from a neural reflectance field under any viewpoint and light. We\ndemonstrate that neural reflectance fields can be estimated from images\ncaptured with a simple collocated camera-light setup, and accurately model the\nappearance of real-world scenes with complex geometry and reflectance. Once\nestimated, they can be used to render photo-realistic images under novel\nviewpoint and (non-collocated) lighting conditions and accurately reproduce\nchallenging effects like specularities, shadows and occlusions. This allows us\nto perform high-quality view synthesis and relighting that is significantly\nbetter than previous methods. We also demonstrate that we can compose the\nestimated neural reflectance field of a real scene with traditional scene\nmodels and render them using standard Monte Carlo rendering engines. Our work\nthus enables a complete pipeline from high-quality and practical appearance\nacquisition to 3D scene composition and rendering.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 22:04:36 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 08:39:07 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Srinivasan", "Pratul", ""], ["Mildenhall", "Ben", ""], ["Sunkavalli", "Kalyan", ""], ["Ha\u0161an", "Milo\u0161", ""], ["Hold-Geoffroy", "Yannick", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2008.03834", "submitter": "Jichao Zhang", "authors": "Jichao Zhang, Jingjing Chen, Hao Tang, Wei Wang, Yan Yan, Enver\n  Sangineto, Nicu Sebe", "title": "Dual In-painting Model for Unsupervised Gaze Correction and Animation in\n  the Wild", "comments": "Accepted By ACMMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of unsupervised gaze correction in the\nwild, presenting a solution that works without the need for precise annotations\nof the gaze angle and the head pose. We have created a new dataset called\nCelebAGaze, which consists of two domains X, Y, where the eyes are either\nstaring at the camera or somewhere else. Our method consists of three novel\nmodules: the Gaze Correction module (GCM), the Gaze Animation module (GAM), and\nthe Pretrained Autoencoder module (PAM). Specifically, GCM and GAM separately\ntrain a dual in-painting network using data from the domain $X$ for gaze\ncorrection and data from the domain $Y$ for gaze animation. Additionally, a\nSynthesis-As-Training method is proposed when training GAM to encourage the\nfeatures encoded from the eye region to be correlated with the angle\ninformation, resulting in a gaze animation which can be achieved by\ninterpolation in the latent space. To further preserve the identity\ninformation~(e.g., eye shape, iris color), we propose the PAM with an\nAutoencoder, which is based on Self-Supervised mirror learning where the\nbottleneck features are angle-invariant and which works as an extra input to\nthe dual in-painting models. Extensive experiments validate the effectiveness\nof the proposed method for gaze correction and gaze animation in the wild and\ndemonstrate the superiority of our approach in producing more compelling\nresults than state-of-the-art baselines. Our code, the pretrained models and\nthe supplementary material are available at:\nhttps://github.com/zhangqianhui/GazeAnimation.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 23:14:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhang", "Jichao", ""], ["Chen", "Jingjing", ""], ["Tang", "Hao", ""], ["Wang", "Wei", ""], ["Yan", "Yan", ""], ["Sangineto", "Enver", ""], ["Sebe", "Nicu", ""]]}, {"id": "2008.03875", "submitter": "Juncheng Liu Dr", "authors": "Juncheng Liu, Steven Mills, Brendan McCane", "title": "RocNet: Recursive Octree Network for Efficient 3D Deep Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep recursive octree network for the compression of 3D voxel\ndata. Our network compresses a voxel grid of any size down to a very small\nlatent space in an autoencoder-like network. We show results for compressing\n32, 64 and 128 grids down to just 80 floats in the latent space. We demonstrate\nthe effectiveness and efficiency of our proposed method on several publicly\navailable datasets with three experiments: 3D shape classification, 3D shape\nreconstruction, and shape generation. Experimental results show that our\nalgorithm maintains accuracy while consuming less memory with shorter training\ntimes compared to existing methods, especially in 3D reconstruction tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 03:02:10 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Liu", "Juncheng", ""], ["Mills", "Steven", ""], ["McCane", "Brendan", ""]]}, {"id": "2008.04367", "submitter": "Meng Zhang", "authors": "Meng Zhang, Tuanfeng Wang, Duygu Ceylan, Niloy J. Mitra", "title": "Deep Detail Enhancement for Any Garment", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating fine garment details requires significant efforts and huge\ncomputational resources. In contrast, a coarse shape may be easy to acquire in\nmany scenarios (e.g., via low-resolution physically-based simulation, linear\nblend skinning driven by skeletal motion, portable scanners). In this paper, we\nshow how to enhance, in a data-driven manner, rich yet plausible details\nstarting from a coarse garment geometry. Once the parameterization of the\ngarment is given, we formulate the task as a style transfer problem over the\nspace of associated normal maps. In order to facilitate generalization across\ngarment types and character motions, we introduce a patch-based formulation,\nthat produces high-resolution details by matching a Gram matrix based style\nloss, to hallucinate geometric details (i.e., wrinkle density and shape). We\nextensively evaluate our method on a variety of production scenarios and show\nthat our method is simple, light-weight, efficient, and generalizes across\nunderlying garment types, sewing patterns, and body motion.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:00:32 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhang", "Meng", ""], ["Wang", "Tuanfeng", ""], ["Ceylan", "Duygu", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2008.04411", "submitter": "Giuseppe Patane'", "authors": "Giuseppe Patan\\`e", "title": "Meshless Approximation and Helmholtz-Hodge Decomposition of Vector\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of vector fields is crucial for the understanding of several\nphysical phenomena, such as natural events (e.g., analysis of waves), diffusive\nprocesses, electric and electromagnetic fields. While previous work has been\nfocused mainly on the analysis of 2D or 3D vector fields on volumes or\nsurfaces, we address the meshless analysis of a vector field defined on an\narbitrary domain, without assumptions on its dimension and discretisation. The\nmeshless approximation of the Helmholtz-Hodge decomposition of a vector field\nis achieved by expressing the potential of its components as a linear\ncombination of radial basis functions and by computing the corresponding\nconservative, irrotational, and harmonic components as solution to a\nleast-squares or to a differential problem. To this end, we identify the\nconditions on the kernel of the radial basis functions that guarantee the\nexistence of their derivatives. Finally, we demonstrate our approach on 2D and\n3D vector fields measured by sensors or generated through simulation.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 20:58:47 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Patan\u00e8", "Giuseppe", ""]]}, {"id": "2008.04524", "submitter": "Haotian Zhang", "authors": "Haotian Zhang, Cristobal Sciutto, Maneesh Agrawala, Kayvon Fatahalian", "title": "Vid2Player: Controllable Video Sprites that Behave and Appear like\n  Professional Tennis Players", "comments": "16 pages, Latex; added player shadows in Sec 8.1; added visual\n  quality evaluation in Sec 9.3; website:\n  https://cs.stanford.edu/~haotianz/research/vid2player/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that converts annotated broadcast video of tennis matches\ninto interactively controllable video sprites that behave and appear like\nprofessional tennis players. Our approach is based on controllable video\ntextures, and utilizes domain knowledge of the cyclic structure of tennis\nrallies to place clip transitions and accept control inputs at key\ndecision-making moments of point play. Most importantly, we use points from the\nvideo collection to model a player's court positioning and shot selection\ndecisions during points. We use these behavioral models to select video clips\nthat reflect actions the real-life player is likely to take in a given match\nplay situation, yielding sprites that behave realistically at the macro level\nof full points, not just individual tennis motions. Our system can generate\nnovel points between professional tennis players that resemble Wimbledon\nbroadcasts, enabling new experiences such as the creation of matchups between\nplayers that have not competed in real life, or interactive control of players\nin the Wimbledon final. According to expert tennis players, the rallies\ngenerated using our approach are significantly more realistic in terms of\nplayer behavior than video sprite methods that only consider the quality of\nmotion transitions during video synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 05:37:05 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 18:11:39 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Haotian", ""], ["Sciutto", "Cristobal", ""], ["Agrawala", "Maneesh", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2008.04852", "submitter": "Ricardo Martin Brualla", "authors": "Ricardo Martin-Brualla, Rohit Pandey, Sofien Bouaziz, Matthew Brown,\n  Dan B Goldman", "title": "GeLaTO: Generative Latent Textured Objects", "comments": "ECCV 2020 Spotlight. Project website: https://gelato-paper.github.io", "journal-ref": "European Conference on Computer Vision 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate modeling of 3D objects exhibiting transparency, reflections and thin\nstructures is an extremely challenging problem. Inspired by billboards and\ngeometric proxies used in computer graphics, this paper proposes Generative\nLatent Textured Objects (GeLaTO), a compact representation that combines a set\nof coarse shape proxies defining low frequency geometry with learned neural\ntextures, to encode both medium and fine scale geometry as well as\nview-dependent appearance. To generate the proxies' textures, we learn a joint\nlatent space allowing category-level appearance and geometry interpolation. The\nproxies are independently rasterized with their corresponding neural texture\nand composited using a U-Net, which generates an output photorealistic image\nincluding an alpha map. We demonstrate the effectiveness of our approach by\nreconstructing complex objects from a sparse set of views. We show results on a\ndataset of real images of eyeglasses frames, which are particularly challenging\nto reconstruct using classical methods. We also demonstrate that these coarse\nproxies can be handcrafted when the underlying object geometry is easy to\nmodel, like eyeglasses, or generated using a neural network for more complex\ncategories, such as cars.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:55:26 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Martin-Brualla", "Ricardo", ""], ["Pandey", "Rohit", ""], ["Bouaziz", "Sofien", ""], ["Brown", "Matthew", ""], ["Goldman", "Dan B", ""]]}, {"id": "2008.05110", "submitter": "Keyu Chen", "authors": "Juyong Zhang, Keyu Chen, Jianmin Zheng", "title": "Facial Expression Retargeting from Human to Avatar Made Easy", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression retargeting from humans to virtual characters is a useful\ntechnique in computer graphics and animation. Traditional methods use markers\nor blendshapes to construct a mapping between the human and avatar faces.\nHowever, these approaches require a tedious 3D modeling process, and the\nperformance relies on the modelers' experience. In this paper, we propose a\nbrand-new solution to this cross-domain expression transfer problem via\nnonlinear expression embedding and expression domain translation. We first\nbuild low-dimensional latent spaces for the human and avatar facial expressions\nwith variational autoencoder. Then we construct correspondences between the two\nlatent spaces guided by geometric and perceptual constraints. Specifically, we\ndesign geometric correspondences to reflect geometric matching and utilize a\ntriplet data structure to express users' perceptual preference of avatar\nexpressions. A user-friendly method is proposed to automatically generate\ntriplets for a system allowing users to easily and efficiently annotate the\ncorrespondences. Using both geometric and perceptual correspondences, we\ntrained a network for expression domain translation from human to avatar.\nExtensive experimental results and user studies demonstrate that even\nnonprofessional users can apply our method to generate high-quality facial\nexpression retargeting results with less time and effort.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:55:54 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhang", "Juyong", ""], ["Chen", "Keyu", ""], ["Zheng", "Jianmin", ""]]}, {"id": "2008.05157", "submitter": "Di Qiu", "authors": "Di Qiu, Jin Zeng, Zhanghan Ke, Wenxiu Sun, Chengxi Yang", "title": "Towards Geometry Guided Neural Relighting with Flash Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous image based relighting methods require capturing multiple images to\nacquire high frequency lighting effect under different lighting conditions,\nwhich needs nontrivial effort and may be unrealistic in certain practical use\nscenarios. While such approaches rely entirely on cleverly sampling the color\nimages under different lighting conditions, little has been done to utilize\ngeometric information that crucially influences the high-frequency features in\nthe images, such as glossy highlight and cast shadow. We therefore propose a\nframework for image relighting from a single flash photograph with its\ncorresponding depth map using deep learning. By incorporating the depth map,\nour approach is able to extrapolate realistic high-frequency effects under\nnovel lighting via geometry guided image decomposition from the flashlight\nimage, and predict the cast shadow map from the shadow-encoding transformed\ndepth map. Moreover, the single-image based setup greatly simplifies the data\ncapture process. We experimentally validate the advantage of our geometry\nguided approach over state-of-the-art image-based approaches in intrinsic image\ndecomposition and image relighting, and also demonstrate our performance on\nreal mobile phone photo examples.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:03:28 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Qiu", "Di", ""], ["Zeng", "Jin", ""], ["Ke", "Zhanghan", ""], ["Sun", "Wenxiu", ""], ["Yang", "Chengxi", ""]]}, {"id": "2008.05262", "submitter": "Alexandros Haridis (Charidis)", "authors": "Alexandros Haridis", "title": "The Topology of Shapes Made with Points", "comments": "12 pages, 5 figures. Keywords: Shape with Points, Finite Order\n  Topology, T0-space, Structural Description, Mathematics of Shapes. Preprint\n  of journal article. Article first published online: February 11, 2019.\n  Environment and Planning B: Urban Analytics and City Science (2019)", "journal-ref": "Environment and Planning B: Urban Analytics and City Science 47\n  (7), 2020", "doi": "10.1177/2399808319827015", "report-no": null, "categories": "cs.GR math.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In architecture, city planning, visual arts, and other design areas, shapes\nare often made with points, or with structural representations based on\npoint-sets. Shapes made with points can be understood more generally as finite\narrangements formed with elements (i.e. points) of the algebra of shapes $U_i$,\nfor $i = 0$. This paper examines the kind of topology that is applicable to\nsuch shapes. From a mathematical standpoint, any \"shape made with points\" is\nequivalent to a finite space, so that topology on a shape made with points is\nno different than topology on a finite space: the study of topological\nstructure naturally coincides with the study of preorder relations on the\npoints of the shape. After establishing this fact, some connections between the\ntopology of shapes made with points and the topology of \"point-free\" pictorial\nshapes (when $i > 0$) are discussed and the main differences between the two\nare summarized.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 12:15:41 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Haridis", "Alexandros", ""]]}, {"id": "2008.05440", "submitter": "Jie Yang", "authors": "Jie Yang, Kaichun Mo, Yu-Kun Lai, Leonidas J. Guibas, Lin Gao", "title": "DSG-Net: Learning Disentangled Structure and Geometry for 3D Shape\n  Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  D shape generation is a fundamental operation in computer graphics. While\nsignificant progress has been made, especially with recent deep generative\nmodels, it remains a challenge to synthesize high-quality shapes with rich\ngeometric details and complex structure, in a controllable manner. To tackle\nthis, we introduce DSG-Net, a deep neural network that learns a disentangled\nstructured and geometric mesh representation for 3D shapes, where two key\naspects of shapes, geometry, and structure, are encoded in a synergistic manner\nto ensure plausibility of the generated shapes, while also being disentangled\nas much as possible. This supports a range of novel shape generation\napplications with disentangled control, such as interpolation of structure\n(geometry) while keeping geometry (structure) unchanged. To achieve this, we\nsimultaneously learn structure and geometry through variational autoencoders\n(VAEs) in a hierarchical manner for both, with bijective mappings at each\nlevel. In this manner, we effectively encode geometry and structure in separate\nlatent spaces, while ensuring their compatibility: the structure is used to\nguide the geometry and vice versa. At the leaf level, the part geometry is\nrepresented using a conditional part VAE, to encode high-quality geometric\ndetails, guided by the structure context as the condition. Our method not only\nsupports controllable generation applications but also produces high-quality\nsynthesized shapes, outperforming state-of-the-art methods. The code has been\nreleased at https://github.com/IGLICT/DSG-Net.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:06:51 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:38:45 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 14:45:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yang", "Jie", ""], ["Mo", "Kaichun", ""], ["Lai", "Yu-Kun", ""], ["Guibas", "Leonidas J.", ""], ["Gao", "Lin", ""]]}, {"id": "2008.05567", "submitter": "Soren Pirk", "authors": "Till Niese, S\\\"oren Pirk, Matthias Albrecht, Bedrich Benes, Oliver\n  Deussen", "title": "Procedural Urban Forestry", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The placement of vegetation plays a central role in the realism of virtual\nscenes. We introduce procedural placement models (PPMs) for vegetation in urban\nlayouts. PPMs are environmentally sensitive to city geometry and allow\nidentifying plausible plant positions based on structural and functional zones\nin an urban layout. PPMs can either be directly used by defining their\nparameters or can be learned from satellite images and land register data.\nTogether with approaches for generating buildings and trees, this allows us to\npopulate urban landscapes with complex 3D vegetation. The effectiveness of our\nframework is shown through examples of large-scale city scenes and close-ups of\nindividually grown tree models; we also validate it by a perceptual user study.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:44:56 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 00:35:44 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Niese", "Till", ""], ["Pirk", "S\u00f6ren", ""], ["Albrecht", "Matthias", ""], ["Benes", "Bedrich", ""], ["Deussen", "Oliver", ""]]}, {"id": "2008.05872", "submitter": "Peter Kan", "authors": "Anna Sebernegg, Peter K\\'an, Hannes Kaufmann", "title": "Motion Similarity Modeling -- A State of the Art Report", "comments": null, "journal-ref": null, "doi": null, "report-no": "VR-TR-001", "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of human motion opens up a wide range of possibilities, such as\nrealistic training simulations or authentic motions in robotics or animation.\nOne of the problems underlying motion analysis is the meaningful comparison of\nactions based on similarity measures. Since the motion analysis is\napplication-dependent, it is essential to find the appropriate motion\nsimilarity method for the particular use case. This state of the art report\nprovides an overview of human motion analysis and different similarity modeling\nmethods, while mainly focusing on approaches that work with 3D motion data. The\nsurvey summarizes various similarity aspects and features of motion and\ndescribes approaches to measuring the similarity between two actions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:08:30 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sebernegg", "Anna", ""], ["K\u00e1n", "Peter", ""], ["Kaufmann", "Hannes", ""]]}, {"id": "2008.05955", "submitter": "Mona Jalal", "authors": "Mona Jalal, Josef Spjut, Ben Boudaoud, Margrit Betke", "title": "SIDOD: A Synthetic Image Dataset for 3D Object Pose Recognition with\n  Distractors", "comments": "3 pages, 4 figures, 1 table, Accepted at CVPR 2019 Workshop", "journal-ref": null, "doi": "10.1109/CVPRW.2019.00063", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, publicly-available image dataset generated by the NVIDIA\nDeep Learning Data Synthesizer intended for use in object detection, pose\nestimation, and tracking applications. This dataset contains 144k stereo image\npairs that synthetically combine 18 camera viewpoints of three photorealistic\nvirtual environments with up to 10 objects (chosen randomly from the 21 object\nmodels of the YCB dataset [1]) and flying distractors. Object and camera pose,\nscene lighting, and quantity of objects and distractors were randomized. Each\nprovided view includes RGB, depth, segmentation, and surface normal images, all\npixel level. We describe our approach for domain randomization and provide\ninsight into the decisions that produced the dataset.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 00:14:19 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Jalal", "Mona", ""], ["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Betke", "Margrit", ""]]}, {"id": "2008.06134", "submitter": "Dening Luo", "authors": "Dening Luo", "title": "Interactive volume illumination of slice-based ray casting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volume rendering always plays an important role in the field of medical\nimaging and industrial design. In recent years, the realistic and interactive\nvolume rendering of the global illumination can improve the perception of shape\nand depth of volumetric datasets. In this paper, a novel and flexible\nperformance method of slice-based ray casting is proposed to implement the\nvolume illumination effects, such as volume shadow and other scattering\neffects. This benefits from the slice-based illumination attenuation buffers of\nthe whole geometry slices at the viewpoint of the light source and the\nhigh-efficiency shadow or scattering coefficient calculation per sample in ray\ncasting. These tests show the method can obtain much better volume illumination\neffects and more scalable performance in contrast to the local volume\nillumination in ray casting volume rendering or other similar slice-based\nglobal volume illumination.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:32:04 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Luo", "Dening", ""]]}, {"id": "2008.06471", "submitter": "Gal Metzer", "authors": "Gal Metzer, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Self-Sampling for Neural Point Cloud Consolidation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel technique for neural point cloud consolidation which\nlearns from only the input point cloud. Unlike other point upsampling methods\nwhich analyze shapes via local patches, in this work, we learn from global\nsubsets. We repeatedly self-sample the input point cloud with global subsets\nthat are used to train a deep neural network. Specifically, we define source\nand target subsets according to the desired consolidation criteria (e.g.,\ngenerating sharp points or points in sparse regions). The network learns a\nmapping from source to target subsets, and implicitly learns to consolidate the\npoint cloud. During inference, the network is fed with random subsets of points\nfrom the input, which it displaces to synthesize a consolidated point set. We\nleverage the inductive bias of neural networks to eliminate noise and outliers,\na notoriously difficult problem in point cloud consolidation. The shared\nweights of the network are optimized over the entire shape, learning non-local\nstatistics and exploiting the recurrence of local-scale geometries.\nSpecifically, the network encodes the distribution of the underlying shape\nsurface within a fixed set of local kernels, which results in the best\nexplanation of the underlying shape surface. We demonstrate the ability to\nconsolidate point sets from a variety of shapes, while eliminating outliers and\nnoise.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:16:02 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 17:09:31 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Metzer", "Gal", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2008.06678", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Wai Tong, Tim Dwyer, Bongshin Lee, Petra Isenberg, Huamin Qu", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones\n  Leveraging an Explainable Reinforcement Learning Framework", "comments": "Accepted at IEEE VIS 2020 (Info VIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute MobileVisFixer, a new method to make visualizations more\nmobile-friendly. Although mobile devices have become the primary means of\naccessing information on the web, many existing visualizations are not\noptimized for small screens and can lead to a frustrating user experience.\nCurrently, practitioners and researchers have to engage in a tedious and\ntime-consuming process to ensure that their designs scale to screens of\ndifferent sizes, and existing toolkits and libraries provide little support in\ndiagnosing and repairing issues. To address this challenge, MobileVisFixer\nautomates a mobile-friendly visualization re-design process with a novel\nreinforcement learning framework. To inform the design of MobileVisFixer, we\nfirst collected and analyzed SVG-based visualizations on the web, and\nidentified five common mobile-friendly issues. MobileVisFixer addresses four of\nthese issues on single-view Cartesian visualizations with linear or discrete\nscales by a Markov Decision Process model that is both generalizable across\nvarious visualizations and fully explainable. MobileVisFixer deconstructs\ncharts into declarative formats, and uses a greedy heuristic based on Policy\nGradient methods to find solutions to this difficult, multi-criteria\noptimization problem in reasonable time. In addition, MobileVisFixer can be\neasily extended with the incorporation of optimization algorithms for data\nvisualizations. Quantitative evaluation on two real-world datasets demonstrates\nthe effectiveness and generalizability of our method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 08:24:45 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Aoyu", ""], ["Tong", "Wai", ""], ["Dwyer", "Tim", ""], ["Lee", "Bongshin", ""], ["Isenberg", "Petra", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.06722", "submitter": "Miguel Crespo", "authors": "Miguel Crespo, Felix Bernal, Adrian Jarabo, Adolfo Mu\\~noz", "title": "Primary-Space Adaptive Control Variates using Piecewise-Polynomial\n  Approximations", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unbiased numerical integration algorithm that handles both\nlow-frequency regions and high frequency details of multidimensional integrals.\nIt combines quadrature and Monte Carlo integration, by using a quadrature-base\napproximation as a control variate of the signal. We adaptively build the\ncontrol variate constructed as a piecewise polynomial, which can be\nanalytically integrated, and accurately reconstructs the low frequency regions\nof the integrand. We then recover the high-frequency details missed by the\ncontrol variate by using Monte Carlo integration of the residual. Our work\nleverages importance sampling techniques by working in primary space, allowing\nthe combination of multiple mappings; this enables multiple importance sampling\nin quadrature-based integration. Our algorithm is generic, and can be applied\nto any complex multidimensional integral. We demonstrate its effectiveness with\nfour applications with low dimensionality: transmittance estimation in\nheterogeneous participating media, low-order scattering in homogeneous media,\ndirect illumination computation, and rendering of distributed effects. Finally,\nwe show how our technique is extensible to integrands of higher dimensionality,\nby computing the control variate on Monte Carlo estimates of the\nhigh-dimensional signal, and accounting for such additional dimensionality on\nthe residual as well. In all cases, we show accurate results and faster\nconvergence compared to previous approaches.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:08:26 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Crespo", "Miguel", ""], ["Bernal", "Felix", ""], ["Jarabo", "Adrian", ""], ["Mu\u00f1oz", "Adolfo", ""]]}, {"id": "2008.07689", "submitter": "Yaorui Zhang", "authors": "Yitong Deng, Yaorui Zhang, Xingzhe He, Shuqi Yang, Yunjin Tong,\n  Michael Zhang, Daniel DiPietro, Bo Zhu", "title": "Soft Multicopter Control using Neural Dynamics Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic control of a soft-body robot to deliver complex behaviors with\nlow-dimensional actuation inputs is challenging. In this paper, we present a\ncomputational approach to automatically generate versatile, underactuated\ncontrol policies that drives soft-bodied machines with complicated structures\nand nonlinear dynamics. Our target application is focused on the autonomous\ncontrol of a soft multicopter, featured by its elastic material components,\nnon-conventional shapes, and asymmetric rotor layouts, to precisely deliver\ncompliant deformation and agile locomotion. The central piece of our approach\nlies in a lightweight neural surrogate model to identify and predict the\ntemporal evolution of a set of geometric variables characterizing an elastic\nsoft body. This physics-based learning model is further integrated into a\nLinear Quadratic Regulator (LQR) control loop enhanced by a novel online\nfixed-point relinearization scheme to accommodate the dynamic body balance,\nallowing an aggressive reduction of the computational overhead caused by the\nconventional full-scale sensing-simulation-control workflow. We demonstrate the\nefficacy of our approach by generating controllers for a broad spectrum of\ncustomized soft multicopter designs and testing them in a high-fidelity physics\nsimulation environment. The control algorithm enables the multicopters to\nperform a variety of tasks, including hovering, trajectory tracking, cruising\nand active deforming.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 01:38:18 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 19:37:18 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 09:44:15 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 09:11:02 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Deng", "Yitong", ""], ["Zhang", "Yaorui", ""], ["He", "Xingzhe", ""], ["Yang", "Shuqi", ""], ["Tong", "Yunjin", ""], ["Zhang", "Michael", ""], ["DiPietro", "Daniel", ""], ["Zhu", "Bo", ""]]}, {"id": "2008.07944", "submitter": "Yalong Yang", "authors": "Vahan Yoghourdjian, Yalong Yang, Tim Dwyer, Lee Lawrence, Michael\n  Wybrow, Kim Marriott", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "comments": "To be presented at IEEE Conference on Information Visualization\n  (InfoVis 2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030459", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node-link diagrams are widely used to visualise networks. However, even the\nbest network layout algorithms ultimately result in 'hairball' visualisations\nwhen the graph reaches a certain degree of complexity, requiring simplification\nthrough aggregation or interaction (such as filtering) to remain usable. Until\nnow, there has been little data to indicate at what level of complexity\nnode-link diagrams become ineffective or how visual complexity affects\ncognitive load. To this end, we conducted a controlled study to understand\nworkload limits for a task that requires a detailed understanding of the\nnetwork topology---finding the shortest path between two nodes. We tested\nperformance on graphs with 25 to 175 nodes with varying density. We collected\nperformance measures (accuracy and response time), subjective feedback, and\nphysiological measures (EEG, pupil dilation, and heart rate variability). To\nthe best of our knowledge, this is the first network visualisation study to\ninclude physiological measures. Our results show that people have significant\ndifficulty finding the shortest path in high-density node-link diagrams with\nmore than 50 nodes and even low-density graphs with more than 100 nodes. From\nour collected EEG data we observe functional differences in brain activity\nbetween hard and easy tasks. We found that cognitive load increased up to a\ncertain level of difficulty after which it decreased, likely because\nparticipants had given up. We also explored the effects of global network\nlayout features such as size or number of crossings, and features of the\nshortest path such as length or straightness on task difficulty. We found that\nglobal features generally had a greater impact than those of the shortest path.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:20:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yoghourdjian", "Vahan", ""], ["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Lawrence", "Lee", ""], ["Wybrow", "Michael", ""], ["Marriott", "Kim", ""]]}, {"id": "2008.08171", "submitter": "Jiaman Li", "authors": "Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler,\n  Hao Li", "title": "Learning to Generate Diverse Dance Motions with Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ongoing pandemic, virtual concerts and live events using digitized\nperformances of musicians are getting traction on massive multiplayer online\nworlds. However, well choreographed dance movements are extremely complex to\nanimate and would involve an expensive and tedious production process. In\naddition to the use of complex motion capture systems, it typically requires a\ncollaborative effort between animators, dancers, and choreographers. We\nintroduce a complete system for dance motion synthesis, which can generate\ncomplex and highly diverse dance sequences given an input music sequence. As\nmotion capture data is limited for the range of dance motions and styles, we\nintroduce a massive dance motion data set that is created from YouTube videos.\nWe also present a novel two-stream motion transformer generative model, which\ncan generate motion sequences with high flexibility. We also introduce new\nevaluation metrics for the quality of synthesized dance motions, and\ndemonstrate that our system can outperform state-of-the-art methods. Our system\nprovides high-quality animations suitable for large crowds for virtual concerts\nand can also be used as reference for professional animation pipelines. Most\nimportantly, we show that vast online videos can be effective in training dance\nmotion models.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:29:40 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Li", "Jiaman", ""], ["Yin", "Yihang", ""], ["Chu", "Hang", ""], ["Zhou", "Yi", ""], ["Wang", "Tingwu", ""], ["Fidler", "Sanja", ""], ["Li", "Hao", ""]]}, {"id": "2008.08345", "submitter": "Dominik Engel", "authors": "Dominik Engel, Timo Ropinski", "title": "Deep Volumetric Ambient Occlusion", "comments": "IEEE VIS SciVis 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030344", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning based technique for volumetric ambient\nocclusion in the context of direct volume rendering. Our proposed Deep\nVolumetric Ambient Occlusion (DVAO) approach can predict per-voxel ambient\nocclusion in volumetric data sets, while considering global information\nprovided through the transfer function. The proposed neural network only needs\nto be executed upon change of this global information, and thus supports\nreal-time volume interaction. Accordingly, we demonstrate DVAOs ability to\npredict volumetric ambient occlusion, such that it can be applied interactively\nwithin direct volume rendering. To achieve the best possible results, we\npropose and analyze a variety of transfer function representations and\ninjection strategies for deep neural networks. Based on the obtained results we\nalso give recommendations applicable in similar volume learning scenarios.\nLastly, we show that DVAO generalizes to a variety of modalities, despite being\ntrained on computed tomography data only.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 09:19:08 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 06:13:14 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Engel", "Dominik", ""], ["Ropinski", "Timo", ""]]}, {"id": "2008.08424", "submitter": "Harkirat Behl", "authors": "Harkirat Singh Behl, At{\\i}l{\\i}m G\\\"une\\c{s} Baydin, Ran Gal, Philip\n  H.S. Torr, Vibhav Vineet", "title": "AutoSimulate: (Quickly) Learning Synthetic Data Generation", "comments": "ECCV 2020", "journal-ref": "European Conference on Computer Vision (ECCV) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation is increasingly being used for generating large labelled datasets\nin many machine learning problems. Recent methods have focused on adjusting\nsimulator parameters with the goal of maximising accuracy on a validation task,\nusually relying on REINFORCE-like gradient estimators. However these approaches\nare very expensive as they treat the entire data generation, model training,\nand validation pipeline as a black-box and require multiple costly objective\nevaluations at each iteration. We propose an efficient alternative for optimal\nsynthetic data generation, based on a novel differentiable approximation of the\nobjective. This allows us to optimize the simulator, which may be\nnon-differentiable, requiring only one objective evaluation at each iteration\nwith a little overhead. We demonstrate on a state-of-the-art photorealistic\nrenderer that the proposed method finds the optimal data distribution faster\n(up to $50\\times$), with significantly reduced training data generation (up to\n$30\\times$) and better accuracy ($+8.7\\%$) on real-world test datasets than\nprevious methods.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:36:11 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Behl", "Harkirat Singh", ""], ["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Gal", "Ran", ""], ["Torr", "Philip H. S.", ""], ["Vineet", "Vibhav", ""]]}, {"id": "2008.08688", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li,\n  Daniel Leithinger", "title": "RealitySketch: Embedding Responsive Graphics and Visualizations in AR\n  through Dynamic Sketching", "comments": "UIST 2020", "journal-ref": null, "doi": "10.1145/3379337.3415892", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RealitySketch, an augmented reality interface for sketching\ninteractive graphics and visualizations. In recent years, an increasing number\nof AR sketching tools enable users to draw and embed sketches in the real\nworld. However, with the current tools, sketched contents are inherently\nstatic, floating in mid air without responding to the real world. This paper\nintroduces a new way to embed dynamic and responsive graphics in the real\nworld. In RealitySketch, the user draws graphical elements on a mobile AR\nscreen and binds them with physical objects in real-time and improvisational\nways, so that the sketched elements dynamically move with the corresponding\nphysical motion. The user can also quickly visualize and analyze real-world\nphenomena through responsive graph plots or interactive visualizations. This\npaper contributes to a set of interaction techniques that enable capturing,\nparameterizing, and visualizing real-world motion without pre-defined programs\nand configurations. Finally, we demonstrate our tool with several application\nscenarios, including physics education, sports training, and in-situ tangible\ninterfaces.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 22:16:19 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Suzuki", "Ryo", ""], ["Kazi", "Rubaiat Habib", ""], ["Wei", "Li-Yi", ""], ["DiVerdi", "Stephen", ""], ["Li", "Wilmot", ""], ["Leithinger", "Daniel", ""]]}, {"id": "2008.08823", "submitter": "Georgios Albanis", "authors": "Georgios Albanis, Nikolaos Zioulis, Anastasios Dimou, Dimitrios\n  Zarpalas, Petros Daras", "title": "DronePose: Photorealistic UAV-Assistant Dataset Synthesis for 3D Pose\n  Estimation via a Smooth Silhouette Loss", "comments": "Accepted in ECCVW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we consider UAVs as cooperative agents supporting human users in\ntheir operations. In this context, the 3D localisation of the UAV assistant is\nan important task that can facilitate the exchange of spatial information\nbetween the user and the UAV. To address this in a data-driven manner, we\ndesign a data synthesis pipeline to create a realistic multimodal dataset that\nincludes both the exocentric user view, and the egocentric UAV view. We then\nexploit the joint availability of photorealistic and synthesized inputs to\ntrain a single-shot monocular pose estimation model. During training we\nleverage differentiable rendering to supplement a state-of-the-art direct\nregression objective with a novel smooth silhouette loss. Our results\ndemonstrate its qualitative and quantitative performance gains over traditional\nsilhouette objectives. Our data and code are available at\nhttps://vcl3d.github.io/DronePose\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:54:56 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 06:14:34 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Albanis", "Georgios", ""], ["Zioulis", "Nikolaos", ""], ["Dimou", "Anastasios", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2008.08880", "submitter": "Soshi Shimada", "authors": "Soshi Shimada, Vladislav Golyanik, Weipeng Xu, Christian Theobalt", "title": "PhysCap: Physically Plausible Monocular 3D Motion Capture in Real Time", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marker-less 3D human motion capture from a single colour camera has seen\nsignificant progress. However, it is a very challenging and severely ill-posed\nproblem. In consequence, even the most accurate state-of-the-art approaches\nhave significant limitations. Purely kinematic formulations on the basis of\nindividual joints or skeletons, and the frequent frame-wise reconstruction in\nstate-of-the-art methods greatly limit 3D accuracy and temporal stability\ncompared to multi-view or marker-based motion capture. Further, captured 3D\nposes are often physically incorrect and biomechanically implausible, or\nexhibit implausible environment interactions (floor penetration, foot skating,\nunnatural body leaning and strong shifting in depth), which is problematic for\nany use case in computer graphics. We, therefore, present PhysCap, the first\nalgorithm for physically plausible, real-time and marker-less human 3D motion\ncapture with a single colour camera at 25 fps. Our algorithm first captures 3D\nhuman poses purely kinematically. To this end, a CNN infers 2D and 3D joint\npositions, and subsequently, an inverse kinematics step finds space-time\ncoherent joint angles and global 3D pose. Next, these kinematic reconstructions\nare used as constraints in a real-time physics-based pose optimiser that\naccounts for environment constraints (e.g., collision handling and floor\nplacement), gravity, and biophysical plausibility of human postures. Our\napproach employs a combination of ground reaction force and residual force for\nplausible root control, and uses a trained neural network to detect foot\ncontact events in images. Our method captures physically plausible and\ntemporally stable global 3D human motion, without physically implausible\npostures, floor penetrations or foot skating, from video in real time and in\ngeneral scenes. The video is available at\nhttp://gvv.mpi-inf.mpg.de/projects/PhysCap\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 10:46:32 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 14:18:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Xu", "Weipeng", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.08950", "submitter": "Dan Reznik", "authors": "Liliana Gabriela Gheorghe and Dan Reznik", "title": "A Special Conic Associated with the Reuleaux Negative Pedal Curve", "comments": "17 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.GR math.CV math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Negative Pedal Curve of the Reuleaux Triangle w.r. to a point $M$ on its\nboundary consists of two elliptic arcs and a point $P_0$. Interestingly, the\nconic passing through the four arc endpoints and by $P_0$ has a remarkable\nproperty: one of its foci is $M$. We provide a synthetic proof based on\nPoncelet's polar duality and inversive techniques. Additional intriguing\nproperties of Reuleaux negative pedal are proved using straightforward\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 13:15:31 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 12:37:09 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Gheorghe", "Liliana Gabriela", ""], ["Reznik", "Dan", ""]]}, {"id": "2008.08999", "submitter": "Qian Zheng", "authors": "Qian Zheng, Weikai Wu, Hanting Pan, Niloy Mitra, Daniel Cohen-Or, Hui\n  Huang", "title": "Object Properties Inferring from and Transfer for Human Interaction\n  Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans regularly interact with their surrounding objects. Such interactions\noften result in strongly correlated motion between humans and the interacting\nobjects. We thus ask: \"Is it possible to infer object properties from skeletal\nmotion alone, even without seeing the interacting object itself?\" In this\npaper, we present a fine-grained action recognition method that learns to infer\nsuch latent object properties from human interaction motion alone. This\ninference allows us to disentangle the motion from the object property and\ntransfer object properties to a given motion. We collected a large number of\nvideos and 3D skeletal motions of the performing actors using an inertial\nmotion capture device. We analyze similar actions and learn subtle differences\namong them to reveal latent properties of the interacting objects. In\nparticular, we learn to identify the interacting object, by estimating its\nweight, or its fragility or delicacy. Our results clearly demonstrate that the\ninteraction motions and interacting objects are highly correlated and indeed\nrelative object latent properties can be inferred from the 3D skeleton\nsequences alone, leading to new synthesis possibilities for human interaction\nmotions. Dataset will be available at http://vcc.szu.edu.cn/research/2020/IT.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:36:34 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zheng", "Qian", ""], ["Wu", "Weikai", ""], ["Pan", "Hanting", ""], ["Mitra", "Niloy", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "2008.09062", "submitter": "Vasileios Choutas", "authors": "Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dimitrios Tzionas,\n  Michael J. Black", "title": "Monocular Expressive Body Regression through Body-Driven Attention", "comments": "Accepted in ECCV'20. Project page: http://expose.is.tue.mpg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand how people look, interact, or perform tasks, we need to quickly\nand accurately capture their 3D body, face, and hands together from an RGB\nimage. Most existing methods focus only on parts of the body. A few recent\napproaches reconstruct full expressive 3D humans from images using 3D body\nmodels that include the face and hands. These methods are optimization-based\nand thus slow, prone to local optima, and require 2D keypoints as input. We\naddress these limitations by introducing ExPose (EXpressive POse and Shape\nrEgression), which directly regresses the body, face, and hands, in SMPL-X\nformat, from an RGB image. This is a hard problem due to the high\ndimensionality of the body and the lack of expressive training data.\nAdditionally, hands and faces are much smaller than the body, occupying very\nfew image pixels. This makes hand and face estimation hard when body images are\ndownscaled for neural networks. We make three main contributions. First, we\naccount for the lack of training data by curating a dataset of SMPL-X fits on\nin-the-wild images. Second, we observe that body estimation localizes the face\nand hands reasonably well. We introduce body-driven attention for face and hand\nregions in the original image to extract higher-resolution crops that are fed\nto dedicated refinement modules. Third, these modules exploit part-specific\nknowledge from existing face- and hand-only datasets. ExPose estimates\nexpressive 3D humans more accurately than existing optimization methods at a\nsmall fraction of the computational cost. Our data, model and code are\navailable for research at https://expose.is.tue.mpg.de .\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:33:47 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Choutas", "Vasileios", ""], ["Pavlakos", "Georgios", ""], ["Bolkart", "Timo", ""], ["Tzionas", "Dimitrios", ""], ["Black", "Michael J.", ""]]}, {"id": "2008.09092", "submitter": "Amlan Kar", "authors": "Jeevan Devaranjan, Amlan Kar, Sanja Fidler", "title": "Meta-Sim2: Unsupervised Learning of Scene Structure for Synthetic Data\n  Generation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural models are being widely used to synthesize scenes for graphics,\ngaming, and to create (labeled) synthetic datasets for ML. In order to produce\nrealistic and diverse scenes, a number of parameters governing the procedural\nmodels have to be carefully tuned by experts. These parameters control both the\nstructure of scenes being generated (e.g. how many cars in the scene), as well\nas parameters which place objects in valid configurations. Meta-Sim aimed at\nautomatically tuning parameters given a target collection of real images in an\nunsupervised way. In Meta-Sim2, we aim to learn the scene structure in addition\nto parameters, which is a challenging problem due to its discrete nature.\nMeta-Sim2 proceeds by learning to sequentially sample rule expansions from a\ngiven probabilistic scene grammar. Due to the discrete nature of the problem,\nwe use Reinforcement Learning to train our model, and design a feature space\ndivergence between our synthesized and target images that is key to successful\ntraining. Experiments on a real driving dataset show that, without any\nsupervision, we can successfully learn to generate data that captures discrete\nstructural statistics of objects, such as their frequency, in real images. We\nalso show that this leads to downstream improvement in the performance of an\nobject detector trained on our generated dataset as opposed to other baseline\nsimulation methods. Project page:\nhttps://nv-tlabs.github.io/meta-sim-structure/.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:28:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Devaranjan", "Jeevan", ""], ["Kar", "Amlan", ""], ["Fidler", "Sanja", ""]]}, {"id": "2008.09367", "submitter": "Martin N\\\"ollenburg", "authors": "Ben Jacobsen, Markus Wallinger, Stephen Kobourov and Martin\n  N\\\"ollenburg", "title": "MetroSets: Visualizing Sets as Metro Maps", "comments": "19 pages; accepted for IEEE INFOVIS 2020; for associated live system,\n  see http://metrosets.ac.tuwien.ac.at", "journal-ref": "IEEE TVCG 27(2):1257-1267 (2021)", "doi": "10.1109/TVCG.2020.3030475", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MetroSets, a new, flexible online tool for visualizing set systems\nusing the metro map metaphor. We model a given set system as a hypergraph\n$\\mathcal{H} = (V, \\mathcal{S})$, consisting of a set $V$ of vertices and a set\n$\\mathcal{S}$, which contains subsets of $V$ called hyperedges. Our system then\ncomputes a metro map representation of $\\mathcal{H}$, where each hyperedge $E$\nin $\\mathcal{S}$ corresponds to a metro line and each vertex corresponds to a\nmetro station. Vertices that appear in two or more hyperedges are drawn as\ninterchanges in the metro map, connecting the different sets. MetroSets is\nbased on a modular 4-step pipeline which constructs and optimizes a path-based\nhypergraph support, which is then drawn and schematized using metro map layout\nalgorithms. We propose and implement multiple algorithms for each step of the\nMetroSet pipeline and provide a functional prototype with easy-to-use preset\nconfigurations. Furthermore, using several real-world datasets, we perform an\nextensive quantitative evaluation of the impact of different pipeline stages on\ndesirable properties of the generated maps, such as octolinearity,\nmonotonicity, and edge uniformity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:22:09 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:58:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jacobsen", "Ben", ""], ["Wallinger", "Markus", ""], ["Kobourov", "Stephen", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2008.09544", "submitter": "Tobias Rapp", "authors": "Tobias Rapp, Christoph Peters, Carsten Dachsbacher", "title": "Visual Analysis of Large Multivariate Scattered Data using Clustering\n  and Probabilistic Summaries", "comments": "Accepted to IEEE SciVis 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030379", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly growing data sizes of scientific simulations pose significant\nchallenges for interactive visualization and analysis techniques. In this work,\nwe propose a compact probabilistic representation to interactively visualize\nlarge scattered datasets. In contrast to previous approaches that represent\nblocks of volumetric data using probability distributions, we model clusters of\narbitrarily structured multivariate data. In detail, we discuss how to\nefficiently represent and store a high-dimensional distribution for each\ncluster. We observe that it suffices to consider low-dimensional marginal\ndistributions for two or three data dimensions at a time to employ common\nvisual analysis techniques. Based on this observation, we represent\nhigh-dimensional distributions by combinations of low-dimensional Gaussian\nmixture models. We discuss the application of common interactive visual\nanalysis techniques to this representation. In particular, we investigate\nseveral frequency-based views, such as density plots in 1D and 2D,\ndensity-based parallel coordinates, and a time histogram. We visualize the\nuncertainty introduced by the representation, discuss a level-of-detail\nmechanism, and explicitly visualize outliers. Furthermore, we propose a spatial\nvisualization by splatting anisotropic 3D Gaussians for which we derive a\nclosed-form solution. Lastly, we describe the application of brushing and\nlinking to this clustered representation. Our evaluation on several large,\nreal-world datasets demonstrates the scaling of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 15:38:30 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 08:32:49 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Rapp", "Tobias", ""], ["Peters", "Christoph", ""], ["Dachsbacher", "Carsten", ""]]}, {"id": "2008.09655", "submitter": "Elizaveta Logacheva", "authors": "Elizaveta Logacheva, Roman Suvorov, Oleg Khomenko, Anton Mashikhin and\n  Victor Lempitsky", "title": "DeepLandscape: Adversarial Modeling of Landscape Video", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a new model of landscape videos that can be trained on a mixture of\nstatic landscape images as well as landscape animations. Our architecture\nextends StyleGAN model by augmenting it with parts that allow to model dynamic\nchanges in a scene. Once trained, our model can be used to generate realistic\ntime-lapse landscape videos with moving objects and time-of-the-day changes.\nFurthermore, by fitting the learned models to a static landscape image, the\nlatter can be reenacted in a realistic way. We propose simple but necessary\nmodifications to StyleGAN inversion procedure, which lead to in-domain latent\ncodes and allow to manipulate real images. Quantitative comparisons and user\nstudies suggest that our model produces more compelling animations of given\nphotographs than previously proposed methods. The results of our approach\nincluding comparisons with prior art can be seen in supplementary materials and\non the project page https://saic-mdal.github.io/deep-landscape\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 19:14:19 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Logacheva", "Elizaveta", ""], ["Suvorov", "Roman", ""], ["Khomenko", "Oleg", ""], ["Mashikhin", "Anton", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2008.09688", "submitter": "Xi Wang", "authors": "Xi Wang, Zoya Bylinskii, Aaron Hertzmann, Robert Pepperell", "title": "Toward Quantifying Ambiguities in Artistic Images", "comments": null, "journal-ref": "ACM Trans. Applied Perception, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been hypothesized that perceptual ambiguities play an important\nrole in aesthetic experience: a work with some ambiguity engages a viewer more\nthan one that does not. However, current frameworks for testing this theory are\nlimited by the availability of stimuli and data collection methods. This paper\npresents an approach to measuring the perceptual ambiguity of a collection of\nimages. Crowdworkers are asked to describe image content, after different\nviewing durations. Experiments are performed using images created with\nGenerative Adversarial Networks, using the Artbreeder website. We show that\ntext processing of viewer responses can provide a fine-grained way to measure\nand describe image ambiguities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:40:16 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Xi", ""], ["Bylinskii", "Zoya", ""], ["Hertzmann", "Aaron", ""], ["Pepperell", "Robert", ""]]}, {"id": "2008.09941", "submitter": "Yalong Yang", "authors": "Yalong Yang, Maxime Cordeil, Johanna Beyer, Tim Dwyer, Kim Marriott,\n  Hanspeter Pfister", "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is\n  Overview+Detail or Zooming Better for 3D Scatterplots?", "comments": "To be presented at IEEE Conference on Information Visualization\n  (InfoVis 2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030427", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract data has no natural scale and so interactive data visualizations\nmust provide techniques to allow the user to choose their viewpoint and scale.\nSuch techniques are well established in desktop visualization tools. The two\nmost common techniques are zoom+pan and overview+detail. However, how best to\nenable the analyst to navigate and view abstract data at different levels of\nscale in immersive environments has not previously been studied. We report the\nfindings of the first systematic study of immersive navigation techniques for\n3D scatterplots. We tested four conditions that represent our best attempt to\nadapt standard 2D navigation techniques to data visualization in an immersive\nenvironment while still providing standard immersive navigation techniques\nthrough physical movement and teleportation. We compared room-sized\nvisualization versus a zooming interface, each with and without an overview. We\nfind significant differences in participants' response times and accuracy for a\nnumber of standard visual analysis tasks. Both zoom and overview provide\nbenefits over standard locomotion support alone (i.e., physical movement and\npointer teleportation). However, which variation is superior, depends on the\ntask. We obtain a more nuanced understanding of the results by analyzing them\nin terms of a time-cost model for the different components of navigation:\nway-finding, travel, number of travel steps, and context switching.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 01:41:14 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yang", "Yalong", ""], ["Cordeil", "Maxime", ""], ["Beyer", "Johanna", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2008.10004", "submitter": "Kun Li", "authors": "Jingying Liu, Binyuan Hui, Kun Li, Yunke Liu, Yu-Kun Lai, Yuxiang\n  Zhang, Yebin Liu and Jingyu Yang", "title": "Geometry-guided Dense Perspective Network for Speech-Driven Facial\n  Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic speech-driven 3D facial animation is a challenging problem due to\nthe complex relationship between speech and face. In this paper, we propose a\ndeep architecture, called Geometry-guided Dense Perspective Network (GDPnet),\nto achieve speaker-independent realistic 3D facial animation. The encoder is\ndesigned with dense connections to strengthen feature propagation and encourage\nthe re-use of audio features, and the decoder is integrated with an attention\nmechanism to adaptively recalibrate point-wise feature responses by explicitly\nmodeling interdependencies between different neuron units. We also introduce a\nnon-linear face reconstruction representation as a guidance of latent space to\nobtain more accurate deformation, which helps solve the geometry-related\ndeformation and is good for generalization across subjects. Huber and HSIC\n(Hilbert-Schmidt Independence Criterion) constraints are adopted to promote the\nrobustness of our model and to better exploit the non-linear and high-order\ncorrelations. Experimental results on the public dataset and real scanned\ndataset validate the superiority of our proposed GDPnet compared with\nstate-of-the-art model.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 09:48:09 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Liu", "Jingying", ""], ["Hui", "Binyuan", ""], ["Li", "Kun", ""], ["Liu", "Yunke", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Yuxiang", ""], ["Liu", "Yebin", ""], ["Yang", "Jingyu", ""]]}, {"id": "2008.10174", "submitter": "Egor Zakharov", "authors": "Egor Zakharov, Aleksei Ivakhnenko, Aliaksandra Shysheya, Victor\n  Lempitsky", "title": "Fast Bi-layer Neural Synthesis of One-Shot Realistic Head Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural rendering-based system that creates head avatars from a\nsingle photograph. Our approach models a person's appearance by decomposing it\ninto two layers. The first layer is a pose-dependent coarse image that is\nsynthesized by a small neural network. The second layer is defined by a\npose-independent texture image that contains high-frequency details. The\ntexture image is generated offline, warped and added to the coarse image to\nensure a high effective resolution of synthesized head views. We compare our\nsystem to analogous state-of-the-art systems in terms of visual quality and\nspeed. The experiments show significant inference speedup over previous neural\nhead avatar models for a given visual quality. We also report on a real-time\nsmartphone-based implementation of our system.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:23:59 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zakharov", "Egor", ""], ["Ivakhnenko", "Aleksei", ""], ["Shysheya", "Aliaksandra", ""], ["Lempitsky", "Victor", ""]]}, {"id": "2008.10247", "submitter": "Mallikarjun Byrasandra Ramalinga Reddy", "authors": "Mallikarjun B R. (1), Ayush Tewari (1), Tae-Hyun Oh (2), Tim Weyrich\n  (3), Bernd Bickel (4), Hans-Peter Seidel (1), Hanspeter Pfister (5), Wojciech\n  Matusik (6), Mohamed Elgharib (1), Christian Theobalt (1) ((1) Max Planck\n  Institute for Informatics, Saarland Informatics Campus, (2) POSTECH, (3)\n  University College London, (4) IST Austria, (5) Harvard University, (6) MIT\n  CSAIL)", "title": "Monocular Reconstruction of Neural Face Reflectance Fields", "comments": "Project page -\n  http://gvv.mpi-inf.mpg.de/projects/FaceReflectanceFields/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reflectance field of a face describes the reflectance properties\nresponsible for complex lighting effects including diffuse, specular,\ninter-reflection and self shadowing. Most existing methods for estimating the\nface reflectance from a monocular image assume faces to be diffuse with very\nfew approaches adding a specular component. This still leaves out important\nperceptual aspects of reflectance as higher-order global illumination effects\nand self-shadowing are not modeled. We present a new neural representation for\nface reflectance where we can estimate all components of the reflectance\nresponsible for the final appearance from a single monocular image. Instead of\nmodeling each component of the reflectance separately using parametric models,\nour neural representation allows us to generate a basis set of faces in a\ngeometric deformation-invariant space, parameterized by the input light\ndirection, viewpoint and face geometry. We learn to reconstruct this\nreflectance field of a face just from a monocular image, which can be used to\nrender the face from any viewpoint in any light condition. Our method is\ntrained on a light-stage training dataset, which captures 300 people\nilluminated with 150 light conditions from 8 viewpoints. We show that our\nmethod outperforms existing monocular reflectance reconstruction methods, in\nterms of photorealism due to better capturing of physical premitives, such as\nsub-surface scattering, specularities, self-shadows and other higher-order\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 08:19:05 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["R.", "Mallikarjun B", ""], ["Tewari", "Ayush", ""], ["Oh", "Tae-Hyun", ""], ["Weyrich", "Tim", ""], ["Bickel", "Bernd", ""], ["Seidel", "Hans-Peter", ""], ["Pfister", "Hanspeter", ""], ["Matusik", "Wojciech", ""], ["Elgharib", "Mohamed", ""], ["Theobalt", "Christian", ""]]}, {"id": "2008.10599", "submitter": "William Peebles", "authors": "William Peebles, John Peebles, Jun-Yan Zhu, Alexei Efros, Antonio\n  Torralba", "title": "The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement", "comments": "ECCV 2020 (Spotlight). Code available at\n  https://github.com/wpeebles/hessian_penalty . Project page and videos\n  available at https://www.wpeebles.com/hessian-penalty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing disentanglement methods for deep generative models rely on\nhand-picked priors and complex encoder-based architectures. In this paper, we\npropose the Hessian Penalty, a simple regularization term that encourages the\nHessian of a generative model with respect to its input to be diagonal. We\nintroduce a model-agnostic, unbiased stochastic approximation of this term\nbased on Hutchinson's estimator to compute it efficiently during training. Our\nmethod can be applied to a wide range of deep generators with just a few lines\nof code. We show that training with the Hessian Penalty often causes\naxis-aligned disentanglement to emerge in latent space when applied to ProGAN\non several datasets. Additionally, we use our regularization term to identify\ninterpretable directions in BigGAN's latent space in an unsupervised fashion.\nFinally, we provide empirical evidence that the Hessian Penalty encourages\nsubstantial shrinkage when applied to over-parameterized latent spaces.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 17:59:56 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Peebles", "William", ""], ["Peebles", "John", ""], ["Zhu", "Jun-Yan", ""], ["Efros", "Alexei", ""], ["Torralba", "Antonio", ""]]}, {"id": "2008.10824", "submitter": "Varuna De Silva D", "authors": "Varuna De Silva", "title": "A Critical Analysis of Patch Similarity Based Image Denoising Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image denoising is a classical signal processing problem that has received\nsignificant interest within the image processing community during the past two\ndecades. Most of the algorithms for image denoising has focused on the paradigm\nof non-local similarity, where image blocks in the neighborhood that are\nsimilar, are collected to build a basis for reconstruction. Through rigorous\nexperimentation, this paper reviews multiple aspects of image denoising\nalgorithm development based on non-local similarity. Firstly, the concept of\nnon-local similarity as a foundational quality that exists in natural images\nhas not received adequate attention. Secondly, the image denoising algorithms\nthat are developed are a combination of multiple building blocks, making\ncomparison among them a tedious task. Finally, most of the work surrounding\nimage denoising presents performance results based on Peak-Signal-to-Noise\nRatio (PSNR) between a denoised image and a reference image (which is perturbed\nwith Additive White Gaussian Noise). This paper starts with a statistical\nanalysis on non-local similarity and its effectiveness under various noise\nlevels, followed by a theoretical comparison of different state-of-the-art\nimage denoising algorithms. Finally, we argue for a methodological overhaul to\nincorporate no-reference image quality measures and unprocessed images (raw)\nduring performance evaluation of image denoising algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 05:30:37 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["De Silva", "Varuna", ""]]}, {"id": "2008.11256", "submitter": "Gilbert Bernstein", "authors": "Gilbert Bernstein and Michael Mara and Tzu-Mao Li and Dougal Maclaurin\n  and Jonathan Ragan-Kelley", "title": "Differentiating a Tensor Language", "comments": "In-progress Draft; unsubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How does one compile derivatives of tensor programs, such that the resulting\ncode is purely functional (hence easier to optimize and parallelize) and\nprovably efficient relative to the original program? We show that naively\ndifferentiating tensor code---as done in popular systems like Tensorflow and\nPyTorch---can cause asymptotic slowdowns in pathological cases, violating the\nCheap Gradients Principle. However, all existing automatic differentiation\nmethods that guarantee this principle (for variable size data) do so by relying\non += mutation through aliases/pointers---which complicates downstream\noptimization. We provide the first purely functional, provably efficient,\nadjoint/reverse-mode derivatives of array/tensor code by explicitly accounting\nfor sparsity. We do this by focusing on the indicator function from Iverson's\nAPL. We also introduce a new \"Tensor SSA\" normal form and a new derivation of\nreverse-mode automatic differentiation based on the universal property of\ninner-products.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:30:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bernstein", "Gilbert", ""], ["Mara", "Michael", ""], ["Li", "Tzu-Mao", ""], ["Maclaurin", "Dougal", ""], ["Ragan-Kelley", "Jonathan", ""]]}, {"id": "2008.11657", "submitter": "Christian Freude", "authors": "Elias Brugger, Christian Freude, Michael Wimmer", "title": "Test Scene Design for Physically Based Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physically based rendering is a discipline in computer graphics which aims at\nreproducing certain light and material appearances that occur in the real\nworld. Complex scenes can be difficult to compute for rendering algorithms.\nThis paper introduces a new comprehensive test database of scenes that treat\ndifferent light setups in conjunction with diverse materials and discusses its\ndesign principles. A lot of research is focused on the development of new\nalgorithms that can deal with difficult light conditions and materials\nefficiently. This database delivers a comprehensive foundation for evaluating\nexisting and newly developed rendering techniques. A final evaluation compares\ndifferent results of different rendering algorithms for all scenes.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:33:42 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 08:09:00 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Brugger", "Elias", ""], ["Freude", "Christian", ""], ["Wimmer", "Michael", ""]]}, {"id": "2008.11989", "submitter": "Dongming Han", "authors": "Dongming Han, Wei Chen, Rusheng Pan, Yijing Liu, Jiehui Zhou, Ying Xu,\n  Tianye Zhang, Changjie Fan, Jianrong Tao and Xiaolong (Luke) Zhang", "title": "GraphFederator: Federated Visual Analysis for Multi-party Graphs", "comments": "12 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GraphFederator, a novel approach to construct joint\nrepresentations of multi-party graphs and supports privacy-preserving visual\nanalysis of graphs. Inspired by the concept of federated learning, we\nreformulate the analysis of multi-party graphs into a decentralization process.\nThe new federation framework consists of a shared module that is responsible\nfor joint modeling and analysis, and a set of local modules that run on\nrespective graph data. Specifically, we propose a federated graph\nrepresentation model (FGRM) that is learned from encrypted characteristics of\nmulti-party graphs in local modules. We also design multiple visualization\nviews for joint visualization, exploration, and analysis of multi-party graphs.\nExperimental results with two datasets demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:36:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Han", "Dongming", "", "Luke"], ["Chen", "Wei", "", "Luke"], ["Pan", "Rusheng", "", "Luke"], ["Liu", "Yijing", "", "Luke"], ["Zhou", "Jiehui", "", "Luke"], ["Xu", "Ying", "", "Luke"], ["Zhang", "Tianye", "", "Luke"], ["Fan", "Changjie", "", "Luke"], ["Tao", "Jianrong", "", "Luke"], ["Xiaolong", "", "", "Luke"], ["Zhang", "", ""]]}, {"id": "2008.12298", "submitter": "Johannes Kopf", "authors": "Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge,\n  Yangming Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu,\n  Peizhao Zhang, Zijian He, Peter Vajda, Ayush Saraf, Michael Cohen", "title": "One Shot 3D Photography", "comments": "Project page:\n  https://facebookresearch.github.io/one_shot_3d_photography/ Code:\n  https://github.com/facebookresearch/one_shot_3d_photography", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH 2020),\n  Volume 39, Number 4, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D photography is a new medium that allows viewers to more fully experience a\ncaptured moment. In this work, we refer to a 3D photo as one that displays\nparallax induced by moving the viewpoint (as opposed to a stereo pair with a\nfixed viewpoint). 3D photos are static in time, like traditional photos, but\nare displayed with interactive parallax on mobile or desktop screens, as well\nas on Virtual Reality devices, where viewing it also includes stereo. We\npresent an end-to-end system for creating and viewing 3D photos, and the\nalgorithmic and design choices therein. Our 3D photos are captured in a single\nshot and processed directly on a mobile device. The method starts by estimating\ndepth from the 2D input image using a new monocular depth estimation network\nthat is optimized for mobile devices. It performs competitively to the\nstate-of-the-art, but has lower latency and peak memory consumption and uses an\norder of magnitude fewer parameters. The resulting depth is lifted to a layered\ndepth image, and new geometry is synthesized in parallax regions. We synthesize\ncolor texture and structures in the parallax regions as well, using an\ninpainting network, also optimized for mobile devices, on the LDI directly.\nFinally, we convert the result into a mesh-based representation that can be\nefficiently transmitted and rendered even on low-end devices and over poor\nnetwork connections. Altogether, the processing takes just a few seconds on a\nmobile device, and the result can be instantly viewed and shared. We perform\nextensive quantitative evaluation to validate our system and compare its new\ncomponents against the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:59:31 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 14:52:55 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Kopf", "Johannes", ""], ["Matzen", "Kevin", ""], ["Alsisan", "Suhib", ""], ["Quigley", "Ocean", ""], ["Ge", "Francis", ""], ["Chong", "Yangming", ""], ["Patterson", "Josh", ""], ["Frahm", "Jan-Michael", ""], ["Wu", "Shu", ""], ["Yu", "Matthew", ""], ["Zhang", "Peizhao", ""], ["He", "Zijian", ""], ["Vajda", "Peter", ""], ["Saraf", "Ayush", ""], ["Cohen", "Michael", ""]]}, {"id": "2008.12933", "submitter": "I-Chao Shen", "authors": "I-Chao Shen, Kuan-Hung Liu, Li-Wen Su, Yu-Ting Wu, Bing-Yu Chen", "title": "ClipFlip : Multi-view Clipart Design", "comments": "12 pages, accepted in computer graphics forum (CGF)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an assistive system for clipart design by providing visual\nscaffolds from the unseen viewpoints. Inspired by the artists' creation\nprocess, our system constructs the visual scaffold by first synthesizing the\nreference 3D shape of the input clipart and rendering it from the desired\nviewpoint. The critical challenge of constructing this visual scaffold is to\ngenerate a reference 3Dshape that matches the user's expectation in terms of\nobject sizing and positioning while preserving the geometric style of the input\nclipart. To address this challenge, we propose a user-assisted curve extrusion\nmethod to obtain the reference 3D shape.We render the synthesized reference 3D\nshape with consistent style into the visual scaffold. By following the\ngenerated visual scaffold, the users can efficiently design clipart with their\ndesired viewpoints. The user study conducted by an intuitive user interface and\nour generated visual scaffold suggests that the users are able to design\nclipart from different viewpoints while preserving the original geometric style\nwithout losing its original shape.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 08:19:45 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 07:46:05 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 07:24:19 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Shen", "I-Chao", ""], ["Liu", "Kuan-Hung", ""], ["Su", "Li-Wen", ""], ["Wu", "Yu-Ting", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "2008.13150", "submitter": "Barbora Kozlikova", "authors": "Mar\\'ia Virginia Sabando, Pavol Ulbrich, Mat\\'ias Selzer, Jan\n  By\\v{s}ka, Jan Mi\\v{c}an, Ignacio Ponzoni, Axel J. Soto, Mar\\'ia Luj\\'an\n  Ganuza, Barbora Kozl\\'ikov\\'a", "title": "ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in\n  Virtual Screening", "comments": "Accepted for the IEEE VIS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern drug discovery process, medicinal chemists deal with the\ncomplexity of analysis of large ensembles of candidate molecules. Computational\ntools, such as dimensionality reduction (DR) and classification, are commonly\nused to efficiently process the multidimensional space of features. These\nunderlying calculations often hinder interpretability of results and prevent\nexperts from assessing the impact of individual molecular features on the\nresulting representations. To provide a solution for scrutinizing such complex\ndata, we introduce ChemVA, an interactive application for the visual\nexploration of large molecular ensembles and their features. Our tool consists\nof multiple coordinated views: Hexagonal view, Detail view, 3D view, Table\nview, and a newly proposed Difference view designed for the comparison of DR\nprojections. These views display DR projections combined with biological\nactivity, selected molecular features, and confidence scores for each of these\nprojections. This conjunction of views allows the user to drill down through\nthe dataset and to efficiently select candidate compounds. Our approach was\nevaluated on two case studies of finding structurally similar ligands with\nsimilar binding affinity to a target protein, as well as on an external\nqualitative evaluation. The results suggest that our system allows effective\nvisual inspection and comparison of different high-dimensional molecular\nrepresentations. Furthermore, ChemVA assists in the identification of candidate\ncompounds while providing information on the certainty behind different\nmolecular representations.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 12:17:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Sabando", "Mar\u00eda Virginia", ""], ["Ulbrich", "Pavol", ""], ["Selzer", "Mat\u00edas", ""], ["By\u0161ka", "Jan", ""], ["Mi\u010dan", "Jan", ""], ["Ponzoni", "Ignacio", ""], ["Soto", "Axel J.", ""], ["Ganuza", "Mar\u00eda Luj\u00e1n", ""], ["Kozl\u00edkov\u00e1", "Barbora", ""]]}, {"id": "2008.13236", "submitter": "Amir Vaxman", "authors": "Christian M\\\"uller, Amir Vaxman", "title": "Discrete Curvature and Torsion from Cross-Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.GR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a M\\\"obius invariant subdivision scheme for polygons, we study a\ncurvature notion for discrete curves where the cross-ratio plays an important\nrole in all our key definitions. Using a particular M\\\"obius invariant\npoint-insertion-rule, comparable to the classical four-point-scheme, we\nconstruct circles along discrete curves. Asymptotic analysis shows that these\ncircles defined on a sampled curve converge to the smooth curvature circles as\nthe sampling density increases. We express our discrete torsion for space\ncurves, which is not a M\\\"obius invariant notion, using the cross-ratio and\nshow its asymptotic behavior in analogy to the curvature.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 18:25:35 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["M\u00fcller", "Christian", ""], ["Vaxman", "Amir", ""]]}, {"id": "2008.13306", "submitter": "Subhashis Hazarika", "authors": "Subhashis Hazarika, Ayan Biswas, Phillip J. Wolfram, Earl Lawrence,\n  Nathan Urban", "title": "Relationship-aware Multivariate Sampling Strategy for Scientific\n  Simulation Data", "comments": "To appear as IEEE Vis 2020 Shortpaper", "journal-ref": null, "doi": "10.1109/VIS47514.2020.00015", "report-no": "2020 IEEE Visualization Conference (VIS)", "categories": "cs.LG cs.GR cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computational power of current supercomputers, the size\nof data produced by scientific simulations is rapidly growing. To reduce the\nstorage footprint and facilitate scalable post-hoc analyses of such scientific\ndata sets, various data reduction/summarization methods have been proposed over\nthe years. Different flavors of sampling algorithms exist to sample the\nhigh-resolution scientific data, while preserving important data properties\nrequired for subsequent analyses. However, most of these sampling algorithms\nare designed for univariate data and cater to post-hoc analyses of single\nvariables. In this work, we propose a multivariate sampling strategy which\npreserves the original variable relationships and enables different\nmultivariate analyses directly on the sampled data. Our proposed strategy\nutilizes principal component analysis to capture the variance of multivariate\ndata and can be built on top of any existing state-of-the-art sampling\nalgorithms for single variables. In addition, we also propose variants of\ndifferent data partitioning schemes (regular and irregular) to efficiently\nmodel the local multivariate relationships. Using two real-world multivariate\ndata sets, we demonstrate the efficacy of our proposed multivariate sampling\nstrategy with respect to its data reduction capabilities as well as the ease of\nperforming efficient post-hoc multivariate analyses.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 00:52:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hazarika", "Subhashis", ""], ["Biswas", "Ayan", ""], ["Wolfram", "Phillip J.", ""], ["Lawrence", "Earl", ""], ["Urban", "Nathan", ""]]}, {"id": "2008.13576", "submitter": "Tushar Athawale", "authors": "Tushar Athawale, Bo Ma, Elham Sakhaee, Chris R. Johnson, and Alireza\n  Entezari", "title": "Direct Volume Rendering with Nonparametric Models of Uncertainty", "comments": "11 pages,13 figures, accepted at the IEEE VIS 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric statistical framework for the quantification,\nanalysis, and propagation of data uncertainty in direct volume rendering (DVR).\nThe state-of-the-art statistical DVR framework allows for preserving the\ntransfer function (TF) of the ground truth function when visualizing uncertain\ndata; however, the existing framework is restricted to parametric models of\nuncertainty. In this paper, we address the limitations of the existing DVR\nframework by extending the DVR framework for nonparametric distributions. We\nexploit the quantile interpolation technique to derive probability\ndistributions representing uncertainty in viewing-ray sample intensities in\nclosed form, which allows for accurate and efficient computation. We evaluate\nour proposed nonparametric statistical models through qualitative and\nquantitative comparisons with the mean-field and parametric statistical models,\nsuch as uniform and Gaussian, as well as Gaussian mixtures. In addition, we\npresent an extension of the state-of-the-art rendering parametric framework to\n2D TFs for improved DVR classifications. We show the applicability of our\nuncertainty quantification framework to ensemble, downsampled, and bivariate\nversions of scalar field datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 12:59:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Athawale", "Tushar", ""], ["Ma", "Bo", ""], ["Sakhaee", "Elham", ""], ["Johnson", "Chris R.", ""], ["Entezari", "Alireza", ""]]}, {"id": "2008.13670", "submitter": "Matthew Petroff", "authors": "Matthew A. Petroff", "title": "A Square Equal-area Map Projection", "comments": "15 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel square equal-area map projection is proposed. The projection combines\nclosed-form forward and inverse solutions with relatively low angular\ndistortion and minimal cusps, a combination of properties not manifested by any\npreviously published square equal-area projection. Thus, the new projection has\nlower angular distortion than any previously published square equal-area\nprojection with a closed-form solution. Utilizing a quincuncial arrangement,\nthe new projection places the north pole at the center of the square and\ndivides the south pole between its four corners; the projection can be\nseamlessly tiled. The existence of closed-form solutions makes the projection\nsuitable for real-time visualization applications, both in cartography and in\nother areas, such as for the display of panoramic images.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 15:20:55 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Petroff", "Matthew A.", ""]]}]