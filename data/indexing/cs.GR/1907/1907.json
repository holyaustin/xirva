[{"id": "1907.00377", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Aniket Bera, Kyra Kapsaskis, Kurt Gray, Dinesh\n  Manocha", "title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement\n  Characteristics", "comments": "To appear in ISMAR 2019 Special Issue of TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for improving the friendliness and warmth of a\nvirtual agent in an AR environment by generating appropriate movement\ncharacteristics. Our algorithm is based on a novel data-driven friendliness\nmodel that is computed using a user-study and psychological characteristics. We\nuse our model to control the movements corresponding to the gaits, gestures,\nand gazing of friendly virtual agents (FVAs) as they interact with the user's\navatar and other agents in the environment. We have integrated FVA agents with\nan AR environment using with a Microsoft HoloLens. Our algorithm can generate\nplausible movements at interactive rates to increase the social presence. We\nalso investigate the perception of a user in an AR setting and observe that an\nFVA has a statistically significant improvement in terms of the perceived\nfriendliness and social presence of a user compared to an agent without the\nfriendliness modeling. We observe an increment of 5.71% in the mean responses\nto a friendliness measure and an improvement of 4.03% in the mean responses to\na social presence measure.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 13:04:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Kapsaskis", "Kyra", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1907.00523", "submitter": "Minjing Yu", "authors": "Zipeng Ye, Ran Yi, Minjing Yu, Yong-Jin Liu and Ying He", "title": "Geodesic Centroidal Voronoi Tessellations: Theories, Algorithms and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, big data of digital media (including images, videos and 3D\ngraphical models) are frequently modeled as low-dimensional manifold meshes\nembedded in a high-dimensional feature space. In this paper, we summarized our\nrecent work on geodesic centroidal Voronoi tessellations(GCVTs), which are\nintrinsic geometric structures on manifold meshes. We show that GCVT can find a\nwidely range of interesting applications in computer vision and graphics, due\nto the efficiency of search, location and indexing inherent in these intrinsic\ngeometric structures. Then we present the challenging issues of how to build\nthe combinatorial structures of GCVTs and establish their time and space\ncomplexities, including both theoretical and algorithmic results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 03:07:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ye", "Zipeng", ""], ["Yi", "Ran", ""], ["Yu", "Minjing", ""], ["Liu", "Yong-Jin", ""], ["He", "Ying", ""]]}, {"id": "1907.00559", "submitter": "Evgeny Burnaev", "authors": "Maria Taktasheva and Albert Matveev and Alexey Artemov and Evgeny\n  Burnaev", "title": "Learning to Approximate Directional Fields Defined over 2D Planes", "comments": "7 pages, 5 figures", "journal-ref": "Proc. of AIST, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of directional fields is a need in many geometry processing\ntasks, such as image tracing, extraction of 3D geometric features, and finding\nprincipal surface directions. A common approach to the construction of\ndirectional fields from data relies on complex optimization procedures, which\nare usually poorly formalizable, require a considerable computational effort,\nand do not transfer across applications. In this work, we propose a deep\nlearning-based approach and study the expressive power and generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 05:58:46 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Taktasheva", "Maria", ""], ["Matveev", "Albert", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1907.00631", "submitter": "Sebastian Ochmann", "authors": "Sebastian Ochmann, Richard Vock, Reinhard Klein", "title": "Automatic reconstruction of fully volumetric 3D building models from\n  point clouds", "comments": "Preprint submitted to ISPRS Journal of Photogrammetry and Remote\n  Sensing. Final version available at\n  https://doi.org/10.1016/j.isprsjprs.2019.03.017", "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, Volume 151,\n  May 2019, Pages 251-262", "doi": "10.1016/j.isprsjprs.2019.03.017", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for reconstructing parametric, volumetric,\nmulti-story building models from unstructured, unfiltered indoor point clouds\nby means of solving an integer linear optimization problem. Our approach\novercomes limitations of previous methods in several ways: First, we drop\nassumptions about the input data such as the availability of separate scans as\nan initial room segmentation. Instead, a fully automatic room segmentation and\noutlier removal is performed on the unstructured point clouds. Second,\nrestricting the solution space of our optimization approach to arrangements of\nvolumetric wall entities representing the structure of a building enforces a\nconsistent model of volumetric, interconnected walls fitted to the observed\ndata instead of unconnected, paper-thin surfaces. Third, we formulate the\noptimization as an integer linear programming problem which allows for an exact\nsolution instead of the approximations achieved with most previous techniques.\nLastly, our optimization approach is designed to incorporate hard constraints\nwhich were difficult or even impossible to integrate before. We evaluate and\ndemonstrate the capabilities of our proposed approach on a variety of complex\nreal-world point clouds.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 09:51:04 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ochmann", "Sebastian", ""], ["Vock", "Richard", ""], ["Klein", "Reinhard", ""]]}, {"id": "1907.00837", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu,\n  Mohamed Elgharib, Pascal Fua, Hans-Peter Seidel, Helge Rhodin, Gerard\n  Pons-Moll, Christian Theobalt", "title": "XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera", "comments": "To appear in ACM Transactions on Graphics (SIGGRAPH) 2020", "journal-ref": null, "doi": "10.1145/3386569.3392410", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time approach for multi-person 3D motion capture at over 30\nfps using a single RGB camera. It operates successfully in generic scenes which\nmay contain occlusions by objects and by other people. Our method operates in\nsubsequent stages. The first stage is a convolutional neural network (CNN) that\nestimates 2D and 3D pose features along with identity assignments for all\nvisible joints of all individuals.We contribute a new architecture for this\nCNN, called SelecSLS Net, that uses novel selective long and short range skip\nconnections to improve the information flow allowing for a drastically faster\nnetwork without compromising accuracy. In the second stage, a fully connected\nneural network turns the possibly partial (on account of occlusion) 2Dpose and\n3Dpose features for each subject into a complete 3Dpose estimate per\nindividual. The third stage applies space-time skeletal model fitting to the\npredicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose,\nand enforce temporal coherence. Our method returns the full skeletal pose in\njoint angles for each subject. This is a further key distinction from previous\nwork that do not produce joint angle results of a coherent skeleton in real\ntime for multi-person scenes. The proposed system runs on consumer hardware at\na previously unseen speed of more than 30 fps given 512x320 images as input\nwhile achieving state-of-the-art accuracy, which we will demonstrate on a range\nof challenging real-world scenes.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:59:02 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 09:55:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mehta", "Dushyant", ""], ["Sotnychenko", "Oleksandr", ""], ["Mueller", "Franziska", ""], ["Xu", "Weipeng", ""], ["Elgharib", "Mohamed", ""], ["Fua", "Pascal", ""], ["Seidel", "Hans-Peter", ""], ["Rhodin", "Helge", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "1907.00893", "submitter": "Yin Yang", "authors": "Xudong Feng, Jiafeng Liu, Huamin Wang, Yin Yang, Hujun Bao, Bernd\n  Bickel, Weiwei Xu", "title": "Computational Design of Skinned Quad-Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational design system that assists users to model,\noptimize, and fabricate quad-robots with soft skins.Our system addresses the\nchallenging task of predicting their physical behavior by fully integrating the\nmultibody dynamics of the mechanical skeleton and the elastic behavior of the\nsoft skin. The developed motion control strategy uses an alternating\noptimization scheme to avoid expensive full space time-optimization,\ninterleaving space-time optimization for the skeleton and frame-by-frame\noptimization for the full dynamics. The output are motor torques to drive the\nrobot to achieve a user prescribed motion trajectory.We also provide a\ncollection of convenient engineering tools and empirical manufacturing guidance\nto support the fabrication of the designed quad-robot. We validate the\nfeasibility of designs generated with our system through physics simulations\nand with a physically-fabricated prototype.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 16:10:13 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Feng", "Xudong", ""], ["Liu", "Jiafeng", ""], ["Wang", "Huamin", ""], ["Yang", "Yin", ""], ["Bao", "Hujun", ""], ["Bickel", "Bernd", ""], ["Xu", "Weiwei", ""]]}, {"id": "1907.00960", "submitter": "Eric-Tuan Le", "authors": "Eric-Tuan Le, Iasonas Kokkinos, Niloy J. Mitra", "title": "Going Deeper with Lean Point Networks", "comments": "16 pages, 11 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce Lean Point Networks (LPNs) to train deeper and more\naccurate point processing networks by relying on three novel point processing\nblocks that improve memory consumption, inference time, and accuracy: a\nconvolution-type block for point sets that blends neighborhood information in a\nmemory-efficient manner; a crosslink block that efficiently shares information\nacross low- and high-resolution processing branches; and a multiresolution\npoint cloud processing block for faster diffusion of information. By combining\nthese blocks, we design wider and deeper point-based architectures. We report\nsystematic accuracy and memory consumption improvements on multiple publicly\navailable segmentation tasks by using our generic modules as drop-in\nreplacements for the blocks of multiple architectures (PointNet++, DGCNN,\nSpiderNet, PointCNN).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:53:20 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 17:01:43 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Le", "Eric-Tuan", ""], ["Kokkinos", "Iasonas", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1907.01361", "submitter": "Matias Tassano", "authors": "Matias Tassano, Julie Delon, Thomas Veit", "title": "FastDVDnet: Towards Real-Time Deep Video Denoising Without Flow\n  Estimation", "comments": "Code for this algorithm and results can be found in\n  https://github.com/m-tassano/fastdvdnet. arXiv admin note: text overlap with\n  arXiv:1906.11890", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a state-of-the-art video denoising algorithm based\non a convolutional neural network architecture. Until recently, video denoising\nwith neural networks had been a largely under explored domain, and existing\nmethods could not compete with the performance of the best patch-based methods.\nThe approach we introduce in this paper, called FastDVDnet, shows similar or\nbetter performance than other state-of-the-art competitors with significantly\nlower computing times. In contrast to other existing neural network denoisers,\nour algorithm exhibits several desirable properties such as fast runtimes, and\nthe ability to handle a wide range of noise levels with a single network model.\nThe characteristics of its architecture make it possible to avoid using a\ncostly motion compensation stage while achieving excellent performance. The\ncombination between its denoising performance and lower computational load\nmakes this algorithm attractive for practical denoising applications. We\ncompare our method with different state-of-art algorithms, both visually and\nwith respect to objective quality metrics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:10:34 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 18:29:49 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Tassano", "Matias", ""], ["Delon", "Julie", ""], ["Veit", "Thomas", ""]]}, {"id": "1907.01652", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Luisa Caldas, Luis Santos", "title": "RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool", "comments": "Accepted to Automation in Construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces RadVR, a virtual reality tool for daylighting analysis\nthat simultaneously combines qualitative assessments through immersive\nreal-time renderings with quantitative physically correct daylighting\nsimulations in a 6DOF virtual environment. By taking a 3D building model with\nmaterial properties as input, RadVR allows users to (1) perform\nphysically-based daylighting simulations via Radiance, (2) study sunlight in\ndifferent hours-of-the-year, (3) interact with a 9-point-in-time matrix for the\nmost representative times of the year, and (4) visualize, compare, and analyze\ndaylighting simulation results. With an end-to-end workflow, RadVR integrates\nwith 3D modeling software that is commonly used by building designers.\nAdditionally, by conducting user experiments we compare the proposed system\nwith DIVA for Rhino, a Radiance-based tool that uses conventional 2D-displays.\nThe results show that RadVR can provide promising assistance in spatial\nunderstanding tasks, navigation, and sun position analysis in virtual reality.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:15:02 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 06:19:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Caldas", "Luisa", ""], ["Santos", "Luis", ""]]}, {"id": "1907.02102", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Aniket Bera, Kyra Kapsaskis, Rahul Sheth, Kurt\n  Gray, Dinesh Manocha", "title": "EVA: Generating Emotional Behavior of Virtual Agents using Expressive\n  Features of Gait and Gaze", "comments": "In Proceedings of ACM Symposium on Applied Perception 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, real-time algorithm, EVA, for generating virtual agents\nwith various perceived emotions. Our approach is based on using Expressive\nFeatures of gaze and gait to convey emotions corresponding to happy, sad,\nangry, or neutral. We precompute a data-driven mapping between gaits and their\nperceived emotions. EVA uses this gait emotion association at runtime to\ngenerate appropriate walking styles in terms of gaits and gaze. Using the EVA\nalgorithm, we can simulate gaits and gazing behaviors of hundreds of virtual\nagents in real-time with known emotional characteristics. We have evaluated the\nbenefits in different multi-agent VR simulation environments. Our studies\nsuggest that the use of expressive features corresponding to gait and gaze can\nconsiderably increase the sense of presence in scenarios with multiple virtual\nagents.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:54:57 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Kapsaskis", "Kyra", ""], ["Sheth", "Rahul", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1907.03107", "submitter": "Lin Wang", "authors": "Lin Wang and Kuk-Jin Yoon", "title": "CoAug-MR: An MR-based Interactive Office Workstation Design System via\n  Augmented Multi-Person Collaboration", "comments": "10 pages, Paper in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital prototyping and evaluation using 3D modeling and digital human models\nare becoming more practical for customizing products to the preference of a\nuser. However, the 3D modeling is less accessible to casual users, and digital\nhuman models suffer from insufficient body data and less intuitive illustration\non how people use the product or how it accommodates to their body. Recently,\nVR-supported 'Do It Yourself' design has achieved real-time ergonomic\nevaluation with users themselves by capturing their poses, however, it lacks\nreliability and quality of design. In this paper, we explore a multi-person\ninteractive design approach that enables designers, users, and even ergonomists\nto collaborate to achieve effective and reliable design and prototyping tasks.\nMixed Reality that utilizes Hololens and motion tracking devices had been\ndeveloped to provide instant design feedback and evaluation and to experience\nprototyping in physical space. We evaluate the system based on the usability\nstudy, where casual users and designers are engaged in the interactive process\nof designing items with respect to the body information, the preference, and\nthe environment.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 10:19:04 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 12:07:11 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 08:21:41 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wang", "Lin", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1907.03118", "submitter": "Jie An", "authors": "Jie An, Haoyi Xiong, Jiebo Luo, Jun Huan, Jinwen Ma", "title": "Fast Universal Style Transfer for Artistic and Photorealistic Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Universal style transfer is an image editing task that renders an input\ncontent image using the visual style of arbitrary reference images, including\nboth artistic and photorealistic stylization. Given a pair of images as the\nsource of content and the reference of style, existing solutions usually first\ntrain an auto-encoder (AE) to reconstruct the image using deep features and\nthen embeds pre-defined style transfer modules into the AE reconstruction\nprocedure to transfer the style of the reconstructed image through modifying\nthe deep features. While existing methods typically need multiple rounds of\ntime-consuming AE reconstruction for better stylization, our work intends to\ndesign novel neural network architectures on top of AE for fast style transfer\nwith fewer artifacts and distortions all in one pass of end-to-end inference.\nTo this end, we propose two network architectures named ArtNet and PhotoNet to\nimprove artistic and photo-realistic stylization, respectively. Extensive\nexperiments demonstrate that ArtNet generates images with fewer artifacts and\ndistortions against the state-of-the-art artistic transfer algorithms, while\nPhotoNet improves the photorealistic stylization results by creating sharp\nimages faithfully preserving rich details of the input content. Moreover,\nArtNet and PhotoNet can achieve 3X to 100X speed-up over the state-of-the-art\nalgorithms, which is a major advantage for large content images.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 11:57:40 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["An", "Jie", ""], ["Xiong", "Haoyi", ""], ["Luo", "Jiebo", ""], ["Huan", "Jun", ""], ["Ma", "Jinwen", ""]]}, {"id": "1907.03387", "submitter": "Fangqiao Hu", "authors": "Fangqiao Hu, Jin Zhao, Yong Huang, Hui Li", "title": "Learning Structural Graph Layouts and 3D Shapes for Long Span Bridges 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A learning-based 3D reconstruction method for long-span bridges is proposed\nin this paper. 3D reconstruction generates a 3D computer model of a real object\nor scene from images, it involves many stages and open problems. Existing\npoint-based methods focus on generating 3D point clouds and their reconstructed\npolygonal mesh or fitting-based geometrical models in urban scenes civil\nstructures reconstruction within Manhattan world constrains and have made great\nachievements. Difficulties arise when an attempt is made to transfer these\nsystems to structures with complex topology and part relations like steel\ntrusses and long-span bridges, this could be attributed to point clouds are\noften unevenly distributed with noise and suffer from occlusions and\nincompletion, recovering a satisfactory 3D model from these highly unstructured\npoint clouds in a bottom-up pattern while preserving the geometrical and\ntopological properties makes enormous challenge to existing algorithms.\nConsidering the prior human knowledge that these structures are in conformity\nto regular spatial layouts in terms of components, a learning-based\ntopology-aware 3D reconstruction method which can obtain high-level structural\ngraph layouts and low-level 3D shapes from images is proposed in this paper. We\ndemonstrate the feasibility of this method by testing on two real long-span\nsteel truss cable-stayed bridges.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:39:04 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 12:42:03 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hu", "Fangqiao", ""], ["Zhao", "Jin", ""], ["Huang", "Yong", ""], ["Li", "Hui", ""]]}, {"id": "1907.03842", "submitter": "Anastasia Zvezdakova", "authors": "Anastasia Zvezdakova, Dmitriy Kulikov, Denis Kondranin, Dmitriy\n  Vatolin", "title": "Barriers towards no-reference metrics application to compressed video\n  quality analysis: on the example of no-reference metric NIQE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses the application of no-reference metric NIQE to the task\nof video-codec comparison. A number of issues in the metric behaviour on videos\nwas detected and described. The metric has outlying scores on black and\nsolid-coloured frames. The proposed averaging technique for metric quality\nscores helped to improve the results in some cases. Also, NIQE has low-quality\nscores for videos with detailed textures and higher scores for videos of lower\nbitrates due to the blurring of these textures after compression. Although NIQE\nshowed natural results for many tested videos, it is not universal and\ncurrently can not be used for video-codec comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 20:07:16 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 14:30:22 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Kulikov", "Dmitriy", ""], ["Kondranin", "Denis", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.03953", "submitter": "Young Jin Oh Mr.", "authors": "Tae Min Lee, Young Jin Oh, In-Kwon Lee", "title": "Efficient Cloth Simulation using Miniature Cloth and Upscaling Deep\n  Neural Networks", "comments": "11 pages, 15 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloth simulation requires a fast and stable method for interactively and\nrealistically visualizing fabric materials using computer graphics. We propose\nan efficient cloth simulation method using miniature cloth simulation and\nupscaling Deep Neural Networks (DNN). The upscaling DNNs generate the target\ncloth simulation from the results of physically-based simulations of a\nminiature cloth that has similar physical properties to those of the target\ncloth. We have verified the utility of the proposed method through experiments,\nand the results demonstrate that it is possible to generate fast and stable\ncloth simulations under various conditions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:13:58 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Lee", "Tae Min", ""], ["Oh", "Young Jin", ""], ["Lee", "In-Kwon", ""]]}, {"id": "1907.04099", "submitter": "Markus Kluge", "authors": "Markus Kluge, Tim Weyrich, Andreas Kolb", "title": "Progressive Refinement Imaging", "comments": "This article has been published in final form at\n  https://doi.org/10.1111/cgf.13808", "journal-ref": null, "doi": "10.1111/cgf.13808", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel technique for progressive online integration of\nuncalibrated image sequences with substantial geometric and/or photometric\ndiscrepancies into a single, geometrically and photometrically consistent\nimage. Our approach can handle large sets of images, acquired from a nearly\nplanar or infinitely distant scene at different resolutions in object domain\nand under variable local or global illumination conditions. It allows for\nefficient user guidance as its progressive nature provides a valid and\nconsistent reconstruction at any moment during the online refinement process.\nOur approach avoids global optimization techniques, as commonly used in the\nfield of image refinement, and progressively incorporates new imagery into a\ndynamically extendable and memory-efficient Laplacian pyramid. Our image\nregistration process includes a coarse homography and a local refinement stage\nusing optical flow. Photometric consistency is achieved by retaining the\nphotometric intensities given in a reference image, while it is being refined.\nGlobally blurred imagery and local geometric inconsistencies due to, e.g.\nmotion are detected and removed prior to image fusion. We demonstrate the\nquality and robustness of our approach using several image and video sequences,\nincluding handheld acquisition with mobile phones and zooming sequences with\nconsumer cameras.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:49:15 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 15:07:02 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Kluge", "Markus", ""], ["Weyrich", "Tim", ""], ["Kolb", "Andreas", ""]]}, {"id": "1907.04353", "submitter": "Jonghyun Kim", "authors": "Jui-Yi Wu and Jonghyun Kim", "title": "Prescription AR: A Fully-Customized Prescription-Embedded Augmented\n  Reality Display", "comments": "17 pages, 16 figures, Optica", "journal-ref": null, "doi": "10.1364/OE.380945", "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a fully-customized AR display design that considers\nthe user's prescription, interpupillary distance, and taste of fashion. A\nfree-form image combiner embedded inside the prescription lens provides\naugmented images onto the vision-corrected real world. We establish a\nprescription-embedded AR display optical design method as well as the\ncustomization method for individual users. Our design can cover myopia,\nhyperopia, astigmatism, and presbyopia, and allows the eye-contact interaction\nwith privacy protection. A 169$g$ dynamic prototype showed a 40$^\\circ$\n$\\times$ 20 $^\\circ$ virtual image with a 23 cpd resolution at center field and\n6 mm $\\times$ 4 mm eye box, with the vision-correction and varifocal (0.5-3$m$)\ncapability.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 18:21:50 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wu", "Jui-Yi", ""], ["Kim", "Jonghyun", ""]]}, {"id": "1907.04435", "submitter": "Fabio Miranda", "authors": "Fabio Miranda, Harish Doraiswamy, Marcos Lage, Luc Wilson, Mondrian\n  Hsieh, Claudio T. Silva", "title": "Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over\n  Time", "comments": "Video: https://www.youtube.com/watch?v=LsZv23d1LyM, Data:\n  https://github.com/ViDA-NYU/shadow-accrual-maps", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (Volume:\n  25, Issue: 3, Mar. 2019)", "doi": "10.1109/TVCG.2018.2802945", "report-no": null, "categories": "cs.GR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale shadows from buildings in a city play an important role in\ndetermining the environmental quality of public spaces. They can be both\nbeneficial, such as for pedestrians during summer, and detrimental, by\nimpacting vegetation and by blocking direct sunlight. Determining the effects\nof shadows requires the accumulation of shadows over time across different\nperiods in a year. In this paper, we propose a simple yet efficient class of\napproach that uses the properties of sun movement to track the changing\nposition of shadows within a fixed time interval. We use this approach to\nextend two commonly used shadowing techniques, shadow maps and ray tracing, and\ndemonstrate the efficiency of our approach. Our technique is used to develop an\ninteractive visual analysis system, Shadow Profiler, targeted at city planners\nand architects that allows them to test the impact of shadows for different\ndevelopment scenarios. We validate the usefulness of this system through case\nstudies set in Manhattan, a dense borough of New York City.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:03:40 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Miranda", "Fabio", ""], ["Doraiswamy", "Harish", ""], ["Lage", "Marcos", ""], ["Wilson", "Luc", ""], ["Hsieh", "Mondrian", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1907.04565", "submitter": "Jules Vidal", "authors": "Jules Vidal, Joseph Budin, and Julien Tierny", "title": "Progressive Wasserstein Barycenters of Persistence Diagrams", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934256", "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient algorithm for the progressive approximation\nof Wasserstein barycenters of persistence diagrams, with applications to the\nvisual analysis of ensemble data. Given a set of scalar fields, our approach\nenables the computation of a persistence diagram which is representative of the\nset, and which visually conveys the number, data ranges and saliences of the\nmain features of interest found in the set. Such representative diagrams are\nobtained by computing explicitly the discrete Wasserstein barycenter of the set\nof persistence diagrams, a notoriously computationally intensive task. In\nparticular, we revisit efficient algorithms for Wasserstein distance\napproximation [12,51] to extend previous work on barycenter estimation [94]. We\npresent a new fast algorithm, which progressively approximates the barycenter\nby iteratively increasing the computation accuracy as well as the number of\npersistent features in the output diagram. Such a progressivity drastically\nimproves convergence in practice and allows to design an interruptible\nalgorithm, capable of respecting computation time constraints. This enables the\napproximation of Wasserstein barycenters within interactive times. We present\nan application to ensemble clustering where we revisit the k-means algorithm to\nexploit our barycenters and compute, within execution time constraints,\nmeaningful clusters of ensemble data along with their barycenter diagram.\nExtensive experiments on synthetic and real-life data sets report that our\nalgorithm converges to barycenters that are qualitatively meaningful with\nregard to the applications, and quantitatively comparable to previous\ntechniques, while offering an order of magnitude speedup when run until\nconvergence (without time constraint). Our algorithm can be trivially\nparallelized to provide additional speedups in practice on standard\nworkstations. [...]\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 08:24:11 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:36:24 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Vidal", "Jules", ""], ["Budin", "Joseph", ""], ["Tierny", "Julien", ""]]}, {"id": "1907.04587", "submitter": "Miles Macklin", "authors": "Miles Macklin, Kenny Erleben, Matthias M\\\"uller, Nuttapong Chentanez,\n  Stefan Jeschke, Viktor Makoviychuk", "title": "Non-Smooth Newton Methods for Deformable Multi-Body Dynamics", "comments": "20 pages, ACM Transactions on Graphics", "journal-ref": null, "doi": "10.1145/3338695", "report-no": null, "categories": "cs.RO cs.CE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for the simulation of rigid and deformable bodies in\nthe presence of contact and friction. Our method is based on a non-smooth\nNewton iteration that solves the underlying nonlinear complementarity problems\n(NCPs) directly. This approach allows us to support nonlinear dynamics models,\nincluding hyperelastic deformable bodies and articulated rigid mechanisms,\ncoupled through a smooth isotropic friction model. The fixed-point nature of\nour method means it requires only the solution of a symmetric linear system as\na building block. We propose a new complementarity preconditioner for NCP\nfunctions that improves convergence, and we develop an efficient GPU-based\nsolver based on the conjugate residual (CR) method that is suitable for\ninteractive simulations. We show how to improve robustness using a new\ngeometric stiffness approximation and evaluate our method's performance on a\nnumber of robotics simulation scenarios, including dexterous manipulation and\ntraining using reinforcement learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 09:27:44 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Macklin", "Miles", ""], ["Erleben", "Kenny", ""], ["M\u00fcller", "Matthias", ""], ["Chentanez", "Nuttapong", ""], ["Jeschke", "Stefan", ""], ["Makoviychuk", "Viktor", ""]]}, {"id": "1907.04807", "submitter": "Anastasia Antsiferova", "authors": "Anastasia Zvezdakova, Sergey Zvezdakov, Dmitriy Kulikov, Dmitriy\n  Vatolin", "title": "Hacking VMAF with Video Color and Contrast Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video quality measurement takes an important role in many applications.\nFull-reference quality metrics which are usually used in video codecs\ncomparisons are expected to reflect any changes in videos. In this article, we\nconsider different color corrections of compressed videos which increase the\nvalues of full-reference metric VMAF and almost don't decrease other\nwidely-used metric SSIM. The proposed video contrast enhancement approach shows\nthe metric inapplicability in some cases for video codecs comparisons, as it\nmay be used for cheating in the comparisons via tuning to improve this metric\nvalues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:56:33 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 13:35:31 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zvezdakova", "Anastasia", ""], ["Zvezdakov", "Sergey", ""], ["Kulikov", "Dmitriy", ""], ["Vatolin", "Dmitriy", ""]]}, {"id": "1907.05073", "submitter": "Tobias Rapp", "authors": "Tobias Rapp, Christoph Peters, Carsten Dachsbacher", "title": "Void-and-Cluster Sampling of Large Scattered Data and Trajectories", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  as a special issue from the proceedings of VIS 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934335", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data reduction technique for scattered data based on statistical\nsampling. Our void-and-cluster sampling technique finds a representative subset\nthat is optimally distributed in the spatial domain with respect to the blue\nnoise property. In addition, it can adapt to a given density function, which we\nuse to sample regions of high complexity in the multivariate value domain more\ndensely. Moreover, our sampling technique implicitly defines an ordering on the\nsamples that enables progressive data loading and a continuous level-of-detail\nrepresentation. We extend our technique to sample time-dependent trajectories,\nfor example pathlines in a time interval, using an efficient and iterative\napproach. Furthermore, we introduce a local and continuous error measure to\nquantify how well a set of samples represents the original dataset. We apply\nthis error measure during sampling to guide the number of samples that are\ntaken. Finally, we use this error measure and other quantities to evaluate the\nquality, performance, and scalability of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:29:39 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 09:36:52 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rapp", "Tobias", ""], ["Peters", "Christoph", ""], ["Dachsbacher", "Carsten", ""]]}, {"id": "1907.05279", "submitter": "Lukas Prantl", "authors": "Lukas Prantl, Nuttapong Chentanez, Stefan Jeschke, and Nils Thuerey", "title": "Tranquil Clouds: Neural Networks for Learning Temporally Coherent\n  Features in Point Clouds", "comments": "Further information and videos at\n  https://ge.in.tum.de/publications/2020-iclr-prantl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds, as a form of Lagrangian representation, allow for powerful and\nflexible applications in a large number of computational disciplines. We\npropose a novel deep-learning method to learn stable and temporally coherent\nfeature spaces for points clouds that change over time. We identify a set of\ninherent problems with these approaches: without knowledge of the time\ndimension, the inferred solutions can exhibit strong flickering, and easy\nsolutions to suppress this flickering can result in undesirable local minima\nthat manifest themselves as halo structures. We propose a novel temporal loss\nfunction that takes into account higher time derivatives of the point\npositions, and encourages mingling, i.e., to prevent the aforementioned halos.\nWe combine these techniques in a super-resolution method with a truncation\napproach to flexibly adapt the size of the generated positions. We show that\nour method works for large, deforming point sets from different sources to\ndemonstrate the flexibility of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:54:02 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 10:55:16 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Prantl", "Lukas", ""], ["Chentanez", "Nuttapong", ""], ["Jeschke", "Stefan", ""], ["Thuerey", "Nils", ""]]}, {"id": "1907.05280", "submitter": "Maximilian Bachl", "authors": "Maximilian Bachl and Daniel C. Ferreira", "title": "City-GAN: Learning architectural styles using a custom Conditional GAN\n  architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are a well-known technique that is\ntrained on samples (e.g. pictures of fruits) and which after training is able\nto generate realistic new samples. Conditional GANs (CGANs) additionally\nprovide label information for subclasses (e.g. apple, orange, pear) which\nenables the GAN to learn more easily and increase the quality of its output\nsamples. We use GANs to learn architectural features of major cities and to\ngenerate images of buildings which do not exist. We show that currently\navailable GAN and CGAN architectures are unsuited for this task and propose a\ncustom architecture and demonstrate that our architecture has superior\nperformance for this task and verify its capabilities with extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 11:43:36 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 20:19:30 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Bachl", "Maximilian", ""], ["Ferreira", "Daniel C.", ""]]}, {"id": "1907.05552", "submitter": "Usman Nazir", "authors": "Usman Nazir, Numan Khurshid, Muhammad Ahmed Bhimra, Murtaza Taj", "title": "Tiny-Inception-ResNet-v2: Using Deep Learning for Eliminating Bonded\n  Labors of Brick Kilns in South Asia", "comments": null, "journal-ref": "CVPR 2019 workshop", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposes to employ a Inception-ResNet inspired deep learning\narchitecture called Tiny-Inception-ResNet-v2 to eliminate bonded labor by\nidentifying brick kilns within \"Brick-Kiln-Belt\" of South Asia. The framework\nis developed by training a network on the satellite imagery consisting of 11\ndifferent classes of South Asian region. The dataset developed during the\nprocess includes the geo-referenced images of brick kilns, houses, roads,\ntennis courts, farms, sparse trees, dense trees, orchards, parking lots, parks\nand barren lands. The dataset is made publicly available for further research.\nOur proposed network architecture with very fewer learning parameters\noutperforms all state-of-the-art architectures employed for recognition of\nbrick kilns. Our proposed solution would enable regional monitoring and\nevaluation mechanisms for the Sustainable Development Goals.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 07:43:42 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Nazir", "Usman", ""], ["Khurshid", "Numan", ""], ["Bhimra", "Muhammad Ahmed", ""], ["Taj", "Murtaza", ""]]}, {"id": "1907.05783", "submitter": "Daniel Alcaide", "authors": "Daniel Alcaide, and Jan Aerts", "title": "Improving the Projection of Global Structures in Data through Spanning\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The connection of edges in a graph generates a structure that is independent\nof a coordinate system. This visual metaphor allows creating a more flexible\nrepresentation of data than a two-dimensional scatterplot. In this work, we\npresent STAD (Spanning Trees as Approximation of Data), a dimensionality\nreduction method to approximate the high-dimensional structure into a graph\nwith or without formulating prior hypotheses. STAD generates an abstract\nrepresentation of high-dimensional data by giving each data point a location in\na graph which preserves the distances in the original high-dimensional space.\nThe STAD graph is built upon the Minimum Spanning Tree (MST) to which new edges\nare added until the correlation between the distances from the graph and the\noriginal dataset is maximized. Additionally, STAD supports the inclusion of\nadditional functions to focus the exploration and allow the analysis of data\nfrom new perspectives, emphasizing traits in data which otherwise would remain\nhidden. We demonstrate the effectiveness of our method by applying it to two\nreal-world datasets: traffic density in Barcelona and temporal measurements of\nair quality in Castile and Le\\'on in Spain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:11:19 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Alcaide", "Daniel", ""], ["Aerts", "Jan", ""]]}, {"id": "1907.06790", "submitter": "Zeshi Yang", "authors": "Li-Ke Ma, Zeshi Yang, Baining Guo, KangKang Yin", "title": "Towards Robust Direction Invariance in Character Animation", "comments": null, "journal-ref": "Computer Graphics Forum, 2019, 38(7): 235-242", "doi": "10.1111/cgf.13832", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In character animation, direction invariance is a desirable property. That\nis, a pose facing north and the same pose facing south are considered the same;\na character that can walk to the north is expected to be able to walk to the\nsouth in a similar style. To achieve such direction invariance, the current\npractice is to remove the facing direction's rotation around the vertical axis\nbefore further processing. Such a scheme, however, is not robust for rotational\nbehaviors in the sagittal plane. In search of a smooth scheme to achieve\ndirection invariance, we prove that in general a singularity free scheme does\nnot exist. We further connect the problem with the hairy ball theorem, which is\nbetter-known to the graphics community. Due to the nonexistence of a\nsingularity free scheme, a general solution does not exist and we propose a\nremedy by using a properly-chosen motion direction that can avoid singularities\nfor specific motions at hand. We perform comparative studies using two\ndeep-learning based methods, one builds kinematic motion representations and\nthe other learns physics-based controls. The results show that with our robust\ndirection invariant features, both methods can achieve better results in terms\nof learning speed and/or final quality. We hope this paper can not only boost\nperformance for character animation methods, but also help related communities\ncurrently not fully aware of the direction invariance problem to achieve more\nrobust results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 23:49:57 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 22:44:11 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 02:05:50 GMT"}, {"version": "v4", "created": "Thu, 29 Aug 2019 01:07:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ma", "Li-Ke", ""], ["Yang", "Zeshi", ""], ["Guo", "Baining", ""], ["Yin", "KangKang", ""]]}, {"id": "1907.07198", "submitter": "Avik Pal", "authors": "Avik Pal", "title": "RayTracer.jl: A Differentiable Renderer that supports Parameter\n  Optimization for Scene Reconstruction", "comments": "Proceedings of the JuliaCon Conferences 2019", "journal-ref": null, "doi": "10.5281/zenodo.1442780", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present RayTracer.jl, a renderer in Julia that is fully\ndifferentiable using source-to-source Automatic Differentiation (AD). This\nmeans that RayTracer not only renders 2D images from 3D scene parameters, but\nit can be used to optimize for model parameters that generate a target image in\na Differentiable Programming (DP) pipeline. We interface our renderer with the\ndeep learning library Flux for use in combination with neural networks. We\ndemonstrate the use of this differentiable renderer in rendering tasks and in\nsolving inverse graphics problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 18:01:44 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 19:43:00 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:06:21 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Pal", "Avik", ""]]}, {"id": "1907.07224", "submitter": "Ashok Jallepalli", "authors": "Ashok Jallepalli, Joshua A. Levine, and Robert M. Kirby", "title": "The Effect of Data Transformations on Scalar Field Topological Analysis\n  of High-Order FEM Solutions", "comments": "11 pages, Accepted to IEEEVIS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order finite element methods (HO-FEM) are gaining popularity in the\nsimulation community due to their success in solving complex flow dynamics.\nThere is an increasing need to analyze the data produced as output by these\nsimulations. Simultaneously, topological analysis tools are emerging as\npowerful methods for investigating simulation data. However, most of the\ncurrent approaches to topological analysis have had limited application to\nHO-FEM simulation data for two reasons. First, the current topological tools\nare designed for linear data (polynomial degree one), but the polynomial degree\nof the data output by these simulations is typically higher (routinely up to\npolynomial degree six). Second, the simulation data and derived quantities of\nthe simulation data have discontinuities at element boundaries, and these\ndiscontinuities do not match the input requirements for the topological tools.\nOne solution to both issues is to transform the high-order data to achieve\nlow-order, continuous inputs for topological analysis. Nevertheless, there has\nbeen little work evaluating the possible transformation choices and their\ndownstream effect on the topological analysis. We perform an empirical study to\nevaluate two commonly used data transformation methodologies along with the\nrecently introduced L-SIAC filter for processing high-order simulation data.\nOur results show diverse behaviors are possible. We offer some guidance about\nhow best to consider a pipeline of topological analysis of HO-FEM simulations\nwith the currently available implementations of topological analysis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:27:42 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jallepalli", "Ashok", ""], ["Levine", "Joshua A.", ""], ["Kirby", "Robert M.", ""]]}, {"id": "1907.08553", "submitter": "Andreas Walch", "authors": "Andreas Walch, Michael Schw\\\"arzler, Christian Luksch, Elmar Eisemann,\n  Theresia Gschwandtner", "title": "LightGuider: Guiding Interactive Lighting Design using Suggestions,\n  Provenance, and Quality Visualization", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934658", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LightGuider is a novel guidance-based approach to interactive lighting\ndesign, which typically consists of interleaved 3D modeling operations and\nlight transport simulations. Rather than having designers use a trial-and-error\napproach to match their illumination constraints and aesthetic goals,\nLightGuider supports the process by simulating potential next modeling steps\nthat can deliver the most significant improvements. LightGuider takes\npredefined quality criteria and the current focus of the designer into account\nto visualize suggestions for lighting-design improvements via a specialized\nprovenance tree. This provenance tree integrates snapshot visualizations of how\nwell a design meets the given quality criteria weighted by the designer's\npreferences. This integration facilitates the analysis of quality improvements\nover the course of a modeling workflow as well as the comparison of alternative\ndesign solutions. We evaluate our approach with three lighting designers to\nillustrate its usefulness.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:49:57 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 09:45:36 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Walch", "Andreas", ""], ["Schw\u00e4rzler", "Michael", ""], ["Luksch", "Christian", ""], ["Eisemann", "Elmar", ""], ["Gschwandtner", "Theresia", ""]]}, {"id": "1907.08884", "submitter": "Minkesh Asati", "authors": "Asati Minkesh, Kraittipong Worranitta, Miyachi Taizo", "title": "Human Extraction and Scene Transition utilizing Mask R-CNN", "comments": "6 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a trendy branch of computer vision, especially on human\nrecognition and pedestrian detection. Recognizing the complete body of a person\nhas always been a difficult problem. Over the years, researchers proposed\nvarious methods, and recently, Mask R-CNN has made a breakthrough for instance\nsegmentation. Based on Faster R-CNN, Mask R-CNN has been able to generate a\nsegmentation mask for each instance. We propose an application to extracts\nmultiple persons from images and videos for pleasant life scenes to grouping\nhappy moments of people such as family or friends and a community for QOL\n(Quality Of Life). We likewise propose a methodology to put extracted images of\npersons into the new background. This enables a user to make a pleasant\ncollection of happy facial expressions and actions of his/her family and\nfriends in his/her life. Mask R-CNN detects all types of object masks from\nimages. Then our algorithm considers only the target person and extracts a\nperson only without obstacles, such as dogs in front of the person, and the\nuser also can select multiple persons as their expectations. Our algorithm is\neffective for both an image and a video irrespective of the length of it. Our\nalgorithm does not add any overhead to Mask R-CNN, running at 5 fps. We show\nexamples of yoga-person in an image and a dancer in a dance-video frame. We\nhope our simple and effective approach would serve as a baseline for replacing\nthe image background and help ease future research.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 23:57:46 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 14:31:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Minkesh", "Asati", ""], ["Worranitta", "Kraittipong", ""], ["Taizo", "Miyachi", ""]]}, {"id": "1907.09146", "submitter": "Gromit Yeuk-Yin Chan", "authors": "Gromit Yeuk-Yin Chan, Luis Gustavo Nonato, Alice Chu, Preeti Raghavan,\n  Viswanath Aluru, Claudio T. Silva", "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb\n  Movement Under Obstetrical Brachial Plexus Injuries", "comments": "IEEE Transactions on Visualization and Computer Graphics (VAST 2019,\n  to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brachial plexus is a complex network of peripheral nerves that enables\nsensing from and control of the movements of the arms and hand. Nowadays, the\ncoordination between the muscles to generate simple movements is still not well\nunderstood, hindering the knowledge of how to best treat patients with this\ntype of peripheral nerve injury. To acquire enough information for medical data\nanalysis, physicians conduct motion analysis assessments with patients to\nproduce a rich dataset of electromyographic signals from multiple muscles\nrecorded with joint movements during real-world tasks. However, tools for the\nanalysis and visualization of the data in a succinct and interpretable manner\nare currently not available. Without the ability to integrate, compare, and\ncompute multiple data sources in one platform, physicians can only compute\nsimple statistical values to describe patient's behavior vaguely, which limits\nthe possibility to answer clinical questions and generate hypotheses for\nresearch. To address this challenge, we have developed \\systemname, an\ninteractive visual analytics system which provides an efficient framework to\nextract and compare muscle activity patterns from the patient's limbs and\ncoordinated views to help users analyze muscle signals, motion data, and video\ninformation to address different tasks. The system was developed as a result of\na collaborative endeavor between computer scientists and orthopedic surgery and\nrehabilitation physicians. We present case studies showing physicians can\nutilize the information displayed to understand how individuals coordinate\ntheir muscles to initiate appropriate treatment and generate new hypotheses for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 06:15:54 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chan", "Gromit Yeuk-Yin", ""], ["Nonato", "Luis Gustavo", ""], ["Chu", "Alice", ""], ["Raghavan", "Preeti", ""], ["Aluru", "Viswanath", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1907.09375", "submitter": "Yifan Wang", "authors": "Yifan Wang, Zichun Zhong, and Jing Hua", "title": "DeepOrganNet: On-the-Fly Reconstruction and Visualization of 3D / 4D\n  Lung Models from Single-View Projections by Deep Deformation Network", "comments": "11 pages, 11 figures, proceeding to IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) (IEEE SciVis 2019), October, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a deep neural network based method, i.e., DeepOrganNet,\nto generate and visualize high-fidelity 3D / 4D organ geometric models from\nsingle-view medical image in real time. Traditional 3D / 4D medical image\nreconstruction requires near hundreds of projections, which cost insufferable\ncomputational time and deliver undesirable high imaging / radiation dose to\nhuman subjects. Moreover, it always needs further notorious processes to\nextract the accurate 3D organ models subsequently. To our knowledge, there is\nno method directly and explicitly reconstructing multiple 3D organ meshes from\na single 2D medical grayscale image on the fly. Given single-view 2D medical\nimages, e.g., 3D / 4D-CT projections or X-ray images, our end-to-end\nDeepOrganNet framework can efficiently and effectively reconstruct 3D / 4D lung\nmodels with a variety of geometric shapes by learning the smooth deformation\nfields from multiple templates based on a trivariate tensor-product deformation\ntechnique, leveraging an informative latent descriptor extracted from input 2D\nimages. The proposed method can guarantee to generate high-quality and\nhigh-fidelity manifold meshes for 3D / 4D lung models. The major contributions\nof this work are to accurately reconstruct the 3D organ shapes from 2D\nsingle-view projection, significantly improve the procedure time to allow\non-the-fly visualization, and dramatically reduce the imaging dose for human\nsubjects. Experimental results are evaluated and compared with the traditional\nreconstruction method and the state-of-the-art in deep learning, by using\nextensive 3D and 4D examples from synthetic phantom and real patient datasets.\nThe proposed method only needs several milliseconds to generate organ meshes\nwith 10K vertices, which has a great potential to be used in real-time image\nguided radiation therapy (IGRT).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:42:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wang", "Yifan", ""], ["Zhong", "Zichun", ""], ["Hua", "Jing", ""]]}, {"id": "1907.09642", "submitter": "Wei Liu", "authors": "Wei Liu, Pingping Zhang, Yinjie Lei, Xiaolin Huang, Jie Yang and Ian\n  Reid", "title": "A Generalized Framework for Edge-preserving and Structure-preserving\n  Image Smoothing", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image smoothing is a fundamental procedure in applications of both computer\nvision and graphics. The required smoothing properties can be different or even\ncontradictive among different tasks. Nevertheless, the inherent smoothing\nnature of one smoothing operator is usually fixed and thus cannot meet the\nvarious requirements of different applications. In this paper, a non-convex\nnon-smooth optimization framework is proposed to achieve diverse smoothing\nnatures where even contradictive smoothing behaviors can be achieved. To this\nend, we first introduce the truncated Huber penalty function which has seldom\nbeen used in image smoothing. A robust framework is then proposed. When\ncombined with the strong flexibility of the truncated Huber penalty function,\nour framework is capable of a range of applications and can outperform the\nstate-of-the-art approaches in several tasks. In addition, an efficient\nnumerical solution is provided and its convergence is theoretically guaranteed\neven the optimization framework is non-convex and non-smooth. The effectiveness\nand superior performance of our approach are validated through comprehensive\nexperimental results in a range of applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 00:51:21 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 07:06:25 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 07:34:08 GMT"}, {"version": "v4", "created": "Wed, 27 Nov 2019 04:58:19 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Lei", "Yinjie", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Reid", "Ian", ""]]}, {"id": "1907.10163", "submitter": "Alec Jacobson", "authors": "Rinat Abdrashitov and Alec Jacobson and Karan Singh", "title": "A system for efficient 3D printed stop-motion face animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer animation in conjunction with 3D printing has the potential to\npositively impact traditional stop-motion animation. As 3D printing every frame\nof a computer animation is prohibitively slow and expensive, 3D printed\nstop-motion can only be viable if animations can be faithfully reproduced using\na compact library of 3D printed and efficiently assemblable parts. We thus\npresent the first system for processing computer animation sequences (typically\nfaces) to produce an optimal set of replacement parts for use in 3D printed\nstop-motion animation. Given an input animation sequence of topology invariant\ndeforming meshes, our problem is to output a library of replacement parts and\nper-animation-frame assignment of the parts, such that we maximally approximate\nthe input animation, while minimizing the amount of 3D printing and assembly.\nInspired by current stop-motion workflows, a user manually indicates which\nparts of the model are preferred for segmentation; then, we find curves with\nminimal deformation along which to segment the mesh. We then present a novel\nalgorithm to zero out deformations along the segment boundaries, so that\nreplacement sets for each part can be interchangeably and seamlessly assembled\ntogether. The part boundaries are designed to ease 3D printing and\ninstrumentation for assembly. Each part is then independently optimized using a\ngraph-cut technique to find a set of replacements, whose size can be user\ndefined, or automatically computed to adhere to a printing budget or allowed\ndeviation from the original animation. Our evaluation is threefold: we show\nresults on a variety of facial animations, both digital and 3D printed,\ncritiqued by a professional animator; we show the impact of various algorithmic\nparameters; and compare our results to naive solutions. Our approach can reduce\nthe printing time and cost significantly for stop-motion animated films.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 22:38:10 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Abdrashitov", "Rinat", ""], ["Jacobson", "Alec", ""], ["Singh", "Karan", ""]]}, {"id": "1907.10208", "submitter": "Liang Zhou", "authors": "Liang Zhou, Rudolf Netzel, Daniel Weiskopf, Chris Johnson", "title": "Spectral Visualization Sharpening", "comments": "Symposium of Applied Perception'19", "journal-ref": null, "doi": "10.1145/3343036.3343133", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a perceptually-guided visualization sharpening\ntechnique. We analyze the spectral behavior of an established comprehensive\nperceptual model to arrive at our approximated model based on an adapted\nweighting of the bandpass images from a Gaussian pyramid. The main benefit of\nthis approximated model is its controllability and predictability for\nsharpening color-mapped visualizations. Our method can be integrated into any\nvisualization tool as it adopts generic image-based post-processing, and it is\nintuitive and easy to use as viewing distance is the only parameter. Using\nhighly diverse datasets, we show the usefulness of our method across a wide\nrange of typical visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 02:24:57 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Zhou", "Liang", ""], ["Netzel", "Rudolf", ""], ["Weiskopf", "Daniel", ""], ["Johnson", "Chris", ""]]}, {"id": "1907.10402", "submitter": "Yeara Kozlov", "authors": "Yeara Kozlov, Hongyi Xu, Moritz B\\\"acher, Derek Bradley, Markus Gross\n  and Thabo Beeler", "title": "Data-Driven Physical Face Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial animation is one of the most challenging problems in computer\ngraphics, and it is often solved using linear heuristics like blend-shape\nrigging. More expressive approaches like physical simulation have emerged, but\nthese methods are very difficult to tune, especially when simulating a real\nactor's face. We propose to use a simple finite element simulation approach for\nface animation, and present a novel method for recovering the required\nsimulation parameters in order to best match a real actor's face motion. Our\nmethod involves reconstructing a very small number of head poses of the actor\nin 3D, where the head poses span different configurations of force directions\ndue to gravity. Our algorithm can then automatically recover both the\ngravity-free rest shape of the face as well as the spatially-varying physical\nmaterial stiffness such that a forward simulation will match the captured\ntargets as closely as possible. As a result, our system can produce\nactor-specific, physical parameters that can be immediately used in recent\nphysical simulation methods for faces. Furthermore, as the simulation results\ndepend heavily on the chosen spatial layout of material clusters, we analyze\nand compare different spatial layouts.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 12:44:54 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Kozlov", "Yeara", ""], ["Xu", "Hongyi", ""], ["B\u00e4cher", "Moritz", ""], ["Bradley", "Derek", ""], ["Gross", "Markus", ""], ["Beeler", "Thabo", ""]]}, {"id": "1907.10699", "submitter": "Ravi Chugh", "authors": "Brian Hempel and Justin Lubin and Ravi Chugh", "title": "Sketch-n-Sketch: Output-Directed Programming for SVG", "comments": "UIST 2019 Paper + Appendix", "journal-ref": null, "doi": "10.1145/3332165.3347925", "report-no": null, "categories": "cs.HC cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For creative tasks, programmers face a choice: Use a GUI and sacrifice\nflexibility, or write code and sacrifice ergonomics?\n  To obtain both flexibility and ease of use, a number of systems have explored\na workflow that we call output-directed programming. In this paradigm, direct\nmanipulation of the program's graphical output corresponds to writing code in a\ngeneral-purpose programming language, and edits not possible with the mouse can\nstill be enacted through ordinary text edits to the program. Such capabilities\nprovide hope for integrating graphical user interfaces into what are currently\ntext-centric programming environments.\n  To further advance this vision, we present a variety of new output-directed\ntechniques that extend the expressive power of Sketch-n-Sketch, an\noutput-directed programming system for creating programs that generate vector\ngraphics. To enable output-directed interaction at more stages of program\nconstruction, we expose intermediate execution products for manipulation and we\npresent a mechanism for contextual drawing. Looking forward to output-directed\nprogramming beyond vector graphics, we also offer generic refactorings through\nthe GUI, and our techniques employ a domain-agnostic provenance tracing scheme.\n  To demonstrate the improved expressiveness, we implement a dozen new\nparametric designs in Sketch-n-Sketch without text-based edits. Among these is\nthe first demonstration of building a recursive function in an output-directed\nprogramming setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:16:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 02:10:58 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 02:20:28 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 21:07:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Hempel", "Brian", ""], ["Lubin", "Justin", ""], ["Chugh", "Ravi", ""]]}, {"id": "1907.10707", "submitter": "Haoyin Zhou", "authors": "Haoyin Zhou, Eva Gombos, Mehra Golshan, Jayender Jagadeesan", "title": "Real-time Deformation of Soft Tissue Internal Structure with Surface\n  Profile Variations using Particle System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intraoperative observation of tissue internal structure is often difficult.\nHence, real-time soft tissue deformation is essential for the localization of\ntumor and other internal structures. We propose a method to simulate the\ninternal structural deformations in a soft tissue with surface profile\nvariations. The deformation simulation utilizes virtual physical particles that\nreceive interaction forces from the surface and other particles and adjust\ntheir positions accordingly. The proposed method involves two stages. In the\ninitialization stage, the three-dimensional internal structure of the surface\nmesh is uniformly sampled using the particle expansion and attracting-repelling\nforce models whilst simultaneously building the internal particle connections.\nIn the simulation stage, under surface profile variations, we simulate the\ninternal structural deformation based on a deformation force model that uses\nthe internal particle connections. The main advantage of this method is that it\ngreatly reduces the computational burden as it only involves simplified\ncalculations and also does not require generating three-dimensional meshes.\nPreliminary experimental results show that the proposed method can handle up to\n10,000 particles in 0.3s.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:38:45 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Zhou", "Haoyin", ""], ["Gombos", "Eva", ""], ["Golshan", "Mehra", ""], ["Jagadeesan", "Jayender", ""]]}, {"id": "1907.11308", "submitter": "Yang Zhou", "authors": "Yang Zhou, Zachary While, Evangelos Kalogerakis", "title": "SceneGraphNet: Neural Message Passing for 3D Indoor Scene Augmentation", "comments": "8 pages, 8 figures, to appear in ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a neural message passing approach to augment an\ninput 3D indoor scene with new objects matching their surroundings. Given an\ninput, potentially incomplete, 3D scene and a query location, our method\npredicts a probability distribution over object types that fit well in that\nlocation. Our distribution is predicted though passing learned messages in a\ndense graph whose nodes represent objects in the input scene and edges\nrepresent spatial and structural relationships. By weighting messages through\nan attention mechanism, our method learns to focus on the most relevant\nsurrounding scene context to predict new scene objects. We found that our\nmethod significantly outperforms state-of-the-art approaches in terms of\ncorrectly predicting objects missing in a scene based on our experiments in the\nSUNCG dataset. We also demonstrate other applications of our method, including\ncontext-based 3D object recognition and iterative scene generation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:03:15 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhou", "Yang", ""], ["While", "Zachary", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "1907.11721", "submitter": "Tong Fu", "authors": "Tong Fu, Rapha\\\"elle Chaine and Julie Digne", "title": "FAKIR: An algorithm for revealing the anatomy and pose of statues from\n  raw point sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D acquisition of archaeological artefacts has become an essential part of\ncultural heritage research for preservation or restoration purpose. Statues, in\nparticular, have been at the center of many projects. In this paper, we\nintroduce a way to improve the understanding of acquired statues representing\nreal or imaginary creatures by registering a simple and pliable articulated\nmodel to the raw point set data. Our approach performs a Forward And bacKward\nIterative Registration (FAKIR) which proceeds joint by joint, needing only a\nfew iterations to converge. We are thus able to detect the pose and elementary\nanatomy of sculptures, with possibly non realistic body proportions. By\nadapting our simple skeleton, our method can work on animals and imaginary\ncreatures.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 17:37:42 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 12:04:15 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 14:04:14 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Fu", "Tong", ""], ["Chaine", "Rapha\u00eblle", ""], ["Digne", "Julie", ""]]}, {"id": "1907.11762", "submitter": "Soumya Dutta", "authors": "Soumya Dutta and Ayan Biswas and James Ahrens", "title": "Multivariate Pointwise Information-Driven Data Sampling and\n  Visualization", "comments": "25 pages", "journal-ref": "Entropy, Volume 21, Issue 7, Year 2019", "doi": "10.3390/e21070699", "report-no": null, "categories": "cs.HC cs.GR cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing computing capabilities of modern supercomputers, the size of\nthe data generated from the scientific simulations is growing rapidly. As a\nresult, application scientists need effective data summarization techniques\nthat can reduce large-scale multivariate spatiotemporal data sets while\npreserving the important data properties so that the reduced data can answer\ndomain-specific queries involving multiple variables with sufficient accuracy.\nWhile analyzing complex scientific events, domain experts often analyze and\nvisualize two or more variables together to obtain a better understanding of\nthe characteristics of the data features. Therefore, data summarization\ntechniques are required to analyze multi-variable relationships in detail and\nthen perform data reduction such that the important features involving multiple\nvariables are preserved in the reduced data. To achieve this, in this work, we\npropose a data sub-sampling algorithm for performing statistical data\nsummarization that leverages pointwise information theoretic measures to\nquantify the statistical association of data points considering multiple\nvariables and generates a sub-sampled data that preserves the statistical\nassociation among multi-variables. Using such reduced sampled data, we show\nthat multivariate feature query and analysis can be done effectively. The\nefficacy of the proposed multivariate association driven sampling algorithm is\npresented by applying it on several scientific data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:32:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dutta", "Soumya", ""], ["Biswas", "Ayan", ""], ["Ahrens", "James", ""]]}, {"id": "1907.11842", "submitter": "Amin Babadi", "authors": "Amin Babadi, Kourosh Naderi, Perttu H\\\"am\\\"al\\\"ainen", "title": "Self-Imitation Learning of Locomotion Movements through Termination\n  Curriculum", "comments": "2019 ACM SIGGRAPH Conference on Motion, Interaction and Games (MIG\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animation and machine learning research have shown great advancements in the\npast decade, leading to robust and powerful methods for learning complex\nphysically-based animations. However, learning can take hours or days,\nespecially if no reference movement data is available. In this paper, we\npropose and evaluate a novel combination of techniques for accelerating the\nlearning of stable locomotion movements through self-imitation learning of\nsynthetic animations. First, we produce synthetic and cyclic reference movement\nusing a recent online tree search approach that can discover stable walking\ngaits in a few minutes. This allows us to use reinforcement learning with\nReference State Initialization (RSI) to find a neural network controller for\nimitating the synthesized reference motion. We further accelerate the learning\nusing a novel curriculum learning approach called Termination Curriculum (TC),\nthat adapts the episode termination threshold over time. The combination of the\nRSI and TC ensures that simulation budget is not wasted in regions of the state\nspace not visited by the final policy. As a result, our agents can learn\nlocomotion skills in just a few hours on a modest 4-core computer. We\ndemonstrate this by producing locomotion movements for a variety of characters.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 04:08:30 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 03:41:49 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Babadi", "Amin", ""], ["Naderi", "Kourosh", ""], ["H\u00e4m\u00e4l\u00e4inen", "Perttu", ""]]}, {"id": "1907.11922", "submitter": "Ziwei Liu", "authors": "Cheng-Han Lee, Ziwei Liu, Lingyun Wu, Ping Luo", "title": "MaskGAN: Towards Diverse and Interactive Facial Image Manipulation", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020. The code, models and dataset are available at:\n  https://github.com/switchablenorms/CelebAMask-HQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial image manipulation has achieved great progress in recent years.\nHowever, previous methods either operate on a predefined set of face attributes\nor leave users little freedom to interactively manipulate images. To overcome\nthese drawbacks, we propose a novel framework termed MaskGAN, enabling diverse\nand interactive face manipulation. Our key insight is that semantic masks serve\nas a suitable intermediate representation for flexible face manipulation with\nfidelity preservation. MaskGAN has two main components: 1) Dense Mapping\nNetwork (DMN) and 2) Editing Behavior Simulated Training (EBST). Specifically,\nDMN learns style mapping between a free-form user modified mask and a target\nimage, enabling diverse generation results. EBST models the user editing\nbehavior on the source mask, making the overall framework more robust to\nvarious manipulated inputs. Specifically, it introduces dual-editing\nconsistency as the auxiliary supervision signal. To facilitate extensive\nstudies, we construct a large-scale high-resolution face dataset with\nfine-grained mask annotations named CelebAMask-HQ. MaskGAN is comprehensively\nevaluated on two challenging tasks: attribute transfer and style copy,\ndemonstrating superior performance over other state-of-the-art methods. The\ncode, models, and dataset are available at\nhttps://github.com/switchablenorms/CelebAMask-HQ.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 14:23:19 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 05:34:29 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Lee", "Cheng-Han", ""], ["Liu", "Ziwei", ""], ["Wu", "Lingyun", ""], ["Luo", "Ping", ""]]}, {"id": "1907.12343", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Blue-Noise Dithered QMC Hierarchical Russian Roulette", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to efficiently sample specular-diffuse-glossy and\nglossy-diffuse-glossy transport phenomena, Tokuyoshi and Harada introduced\nhierarchical Russian roulette, a smart algorithm that allows to compute the\nminimum of the random numbers associated to leaves of a tree at each internal\nnode. The algorithm is used to efficiently cull the connections between the\nproduct set of eye and light vertices belonging to large caches of eye and\nlight subpaths produced through bidirectional path tracing. The original\nversion of the algorithm is entirely based on the generation of semi-stratified\npseudo-random numbers. Our paper proposes a novel variant based on\ndeterministic blue-noise dithered Quasi Monte Carlo samples.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:38:05 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:22:01 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1907.12352", "submitter": "Tobias Isenberg", "authors": "Sarkis Halladjian, Haichao Miao, David Kou\\v{r}il, M. Eduard\n  Gr\\\"oller, Ivan Viola, Tobias Isenberg", "title": "ScaleTrotter: Illustrative Visual Travels Across Negative Scales", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934334", "report-no": null, "categories": "cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ScaleTrotter, a conceptual framework for an interactive,\nmulti-scale visualization of biological mesoscale data and, specifically,\ngenome data. ScaleTrotter allows viewers to smoothly transition from the\nnucleus of a cell to the atomistic composition of the DNA, while bridging\nseveral orders of magnitude in scale. The challenges in creating an interactive\nvisualization of genome data are fundamentally different in several ways from\nthose in other domains like astronomy that require a multi-scale representation\nas well. First, genome data has intertwined scale levels---the DNA is an\nextremely long, connected molecule that manifests itself at all scale levels.\nSecond, elements of the DNA do not disappear as one zooms out---instead the\nscale levels at which they are observed group these elements differently.\nThird, we have detailed information and thus geometry for the entire dataset\nand for all scale levels, posing a challenge for interactive visual\nexploration. Finally, the conceptual scale levels for genome data are close in\nscale space, requiring us to find ways to visually embed a smaller scale into a\ncoarser one. We address these challenges by creating a new multi-scale\nvisualization concept. We use a scale-dependent camera model that controls the\nvisual embedding of the scales into their respective parents, the rendering of\na subset of the scale hierarchy, and the location, size, and scope of the view.\nIn traversing the scales, ScaleTrotter is roaming between 2D and 3D visual\nrepresentations that are depicted in integrated visuals. We discuss,\nspecifically, how this form of multi-scale visualization follows from the\nspecific characteristics of the genome data and describe its implementation.\nFinally, we discuss the implications of our work to the general illustrative\ndepiction of multi-scale data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:01:54 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Halladjian", "Sarkis", ""], ["Miao", "Haichao", ""], ["Kou\u0159il", "David", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Viola", "Ivan", ""], ["Isenberg", "Tobias", ""]]}, {"id": "1907.12627", "submitter": "Anjul Tyagi", "authors": "Anjul Tyagi, Zhen Cao, Tyler Estro, Erez Zadok, Klaus Mueller", "title": "ICE: An Interactive Configuration Explorer for High Dimensional\n  Categorical Parameter Spaces", "comments": "10 pages, Published by IEEE at VIS 2019 (Vancouver, BC, Canada)", "journal-ref": "2019 IEEE Conference on Visual Analytics Science and Technology\n  (VAST), Vancouver, BC, Canada, 2019, pp. 23-34", "doi": "10.1109/VAST47406.2019.8986923", "report-no": null, "categories": "cs.GR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications where users seek to explore the impact of the\nsettings of several categorical variables with respect to one dependent\nnumerical variable. For example, a computer systems analyst might want to study\nhow the type of file system or storage device affects system performance. A\nusual choice is the method of Parallel Sets designed to visualize multivariate\ncategorical variables. However, we found that the magnitude of the parameter\nimpacts on the numerical variable cannot be easily observed here. We also\nattempted a dimension reduction approach based on Multiple Correspondence\nAnalysis but found that the SVD-generated 2D layout resulted in a loss of\ninformation. We hence propose a novel approach, the Interactive Configuration\nExplorer (ICE), which directly addresses the need of analysts to learn how the\ndependent numerical variable is affected by the parameter settings given\nmultiple optimization objectives. No information is lost as ICE shows the\ncomplete distribution and statistics of the dependent variable in context with\neach categorical variable. Analysts can interactively filter the variables to\noptimize for certain goals such as achieving a system with maximum performance,\nlow variance, etc. Our system was developed in tight collaboration with a group\nof systems performance researchers and its final effectiveness was evaluated\nwith expert interviews, a comparative user study, and two case studies.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:23:27 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 23:49:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tyagi", "Anjul", ""], ["Cao", "Zhen", ""], ["Estro", "Tyler", ""], ["Zadok", "Erez", ""], ["Mueller", "Klaus", ""]]}, {"id": "1907.12845", "submitter": "Tanja Munz", "authors": "Tanja Munz, Michael Burch, Toon van Benthem, Yoeri Poels, Fabian Beck\n  and Daniel Weiskopf", "title": "Overlap-free Drawing of Generalized Pythagoras Trees for Hierarchy\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Pythagoras trees were developed for visualizing hierarchical\ndata, producing organic, fractal-like representations. However, the drawback of\nthe original layout algorithm is visual overlap of tree branches. To avoid such\noverlap, we introduce an adapted drawing algorithm using ellipses instead of\ncircles to recursively place tree nodes representing the subhierarchies. Our\ntechnique is demonstrated by resolving overlap in diverse real-world and\ngenerated datasets, while comparing the results to the original approach.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 11:53:49 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Munz", "Tanja", ""], ["Burch", "Michael", ""], ["van Benthem", "Toon", ""], ["Poels", "Yoeri", ""], ["Beck", "Fabian", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "1907.12879", "submitter": "Nicolas Holliman Professor", "authors": "Nicolas S. Holliman, Arzu Coltekin, Sara J. Fernstad, Michael D.\n  Simpson, Kevin J. Wilson, Andrew J. Woods", "title": "Visual Entropy and the Visualization of Uncertainty", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: It is possible to find many different visual representations of\ndata values in visualizations, it is less common to see visual representations\nthat include uncertainty, especially in visualizations intended for\nnon-technical audiences. Objective: our aim is to rigorously define and\nevaluate the novel use of visual entropy as a measure of shape that allows us\nto construct an ordered scale of glyphs for use in representing both\nuncertainty and value in 2D and 3D environments. Method: We use sample entropy\nas a numerical measure of visual entropy to construct a set of glyphs using R\nand Blender which vary in their complexity. Results: A Bradley-Terry analysis\nof a pairwise comparison of the glyphs shows participants (n=19) ordered the\nglyphs as predicted by the visual entropy score (linear regression R2 >0.97,\np<0.001). We also evaluate whether the glyphs can effectively represent\nuncertainty using a signal detection method, participants (n=15) were able to\nsearch for glyphs representing uncertainty with high sensitivity and low error\nrates. Conclusion: visual entropy is a novel cue for representing ordered data\nand provides a channel that allows the uncertainty of a measure to be presented\nalongside its mean value.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:18:27 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Holliman", "Nicolas S.", ""], ["Coltekin", "Arzu", ""], ["Fernstad", "Sara J.", ""], ["Simpson", "Michael D.", ""], ["Wilson", "Kevin J.", ""], ["Woods", "Andrew J.", ""]]}, {"id": "1907.13178", "submitter": "Seth Johnson", "authors": "Seth Johnson, Francesca Samsel, Gregory Abram, Daniel Olson, Andrew J.\n  Solis, Bridger Herman, Phillip J. Wolfram, Christophe Lenglet, Daniel F.\n  Keefe", "title": "Artifact-Based Rendering: Harnessing Natural and Traditional Visual\n  Media for More Expressive and Engaging 3D Visualizations", "comments": "Published in IEEE VIS 2019, 9 pages of content with 2 pages of\n  references, 12 figures", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934260", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Artifact-Based Rendering (ABR), a framework of tools,\nalgorithms, and processes that makes it possible to produce real, data-driven\n3D scientific visualizations with a visual language derived entirely from\ncolors, lines, textures, and forms created using traditional physical media or\nfound in nature. A theory and process for ABR is presented to address three\ncurrent needs: (i) designing better visualizations by making it possible for\nnon-programmers to rapidly design and critique many alternative data-to-visual\nmappings; (ii) expanding the visual vocabulary used in scientific\nvisualizations to depict increasingly complex multivariate data; (iii) bringing\na more engaging, natural, and human-relatable handcrafted aesthetic to data\nvisualization. New tools and algorithms to support ABR include front-end\napplets for constructing artifact-based colormaps, optimizing 3D scanned meshes\nfor use in data visualization, and synthesizing textures from artifacts. These\nare complemented by an interactive rendering engine with custom algorithms and\ninterfaces that demonstrate multiple new visual styles for depicting point,\nline, surface, and volume data. A within-the-research-team design study\nprovides early evidence of the shift in visualization design processes that ABR\nis believed to enable when compared to traditional scientific visualization\nsystems. Qualitative user feedback on applications to climate science and brain\nimaging support the utility of ABR for scientific discovery and public\ncommunication.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 18:51:27 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 15:03:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Johnson", "Seth", ""], ["Samsel", "Francesca", ""], ["Abram", "Gregory", ""], ["Olson", "Daniel", ""], ["Solis", "Andrew J.", ""], ["Herman", "Bridger", ""], ["Wolfram", "Phillip J.", ""], ["Lenglet", "Christophe", ""], ["Keefe", "Daniel F.", ""]]}, {"id": "1907.13534", "submitter": "Manuela Waldner", "authors": "Manuela Waldner, Alexandra Diehl, Denis Gracanin, Rainer Splechtna,\n  Claudio Delrieux, Kresimir Matkovic", "title": "A Comparison of Radial and Linear Charts for Visualizing Daily Pattern", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934784", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radial charts are generally considered less effective than linear charts.\nPerhaps the only exception is in visualizing periodical time-dependent data,\nwhich is believed to be naturally supported by the radial layout. It has been\ndemonstrated that the drawbacks of radial charts outweigh the benefits of this\nnatural mapping. Visualization of daily patterns, as a special case, has not\nbeen systematically evaluated using radial charts. In contrast to yearly or\nweekly recurrent trends, the analysis of daily patterns on a radial chart may\nbenefit from our trained skill on reading radial clocks that are ubiquitous in\nour culture. In a crowd-sourced experiment with 92 non-expert users, we\nevaluated the accuracy, efficiency, and subjective ratings of radial and linear\ncharts for visualizing daily traffic accident patterns. We systematically\ncompared juxtaposed 12-hours variants and single 24-hours variants for both\nlayouts in four low-level tasks and one high-level interpretation task. Our\nresults show that over all tasks, the most elementary 24-hours linear bar chart\nis most accurate and efficient and is also preferred by the users. This\nprovides strong evidence for the use of linear layouts - even for visualizing\nperiodical daily patterns.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:44:42 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Waldner", "Manuela", ""], ["Diehl", "Alexandra", ""], ["Gracanin", "Denis", ""], ["Splechtna", "Rainer", ""], ["Delrieux", "Claudio", ""], ["Matkovic", "Kresimir", ""]]}, {"id": "1907.13538", "submitter": "Zhutian Chen", "authors": "Zhutian Chen and Wei Zeng and Zhiguang Yang and Lingyun Yu and\n  Chi-Wing Fu and Huamin Qu", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "comments": "10 pages", "journal-ref": "TVCG2019", "doi": "10.1109/TVCG.2019.2934332", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection is a fundamental task in exploratory analysis and visualization of\n3D point clouds. Prior researches on selection methods were developed mainly\nbased on heuristics such as local point density, thus limiting their\napplicability in general data. Specific challenges root in the great\nvariabilities implied by point clouds (e.g., dense vs. sparse), viewpoint\n(e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this\nwork, we introduce LassoNet, a new deep neural network for lasso selection of\n3D point clouds, attempting to learn a latent mapping from viewpoint and lasso\nto point cloud regions. To achieve this, we couple user-target points with\nviewpoint and lasso information through 3D coordinate transform and naive\nselection, and improve the method scalability via an intention filtering and\nfarthest point sampling. A hierarchical network is trained using a dataset with\nover 30K lasso-selection records on two different point cloud data. We conduct\na formal user study to compare LassoNet with two state-of-the-art\nlasso-selection methods. The evaluations confirm that our approach improves the\nselection effectiveness and efficiency across different combinations of 3D\npoint clouds, viewpoints, and lasso selections. Project Website:\nhttps://lassonet.github.io\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:51:53 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 11:45:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Zhutian", ""], ["Zeng", "Wei", ""], ["Yang", "Zhiguang", ""], ["Yu", "Lingyun", ""], ["Fu", "Chi-Wing", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13550", "submitter": "Zhutian Chen", "authors": "Zhutian Chen, Yun Wang, Qianwen Wang, Yong Wang, and Huamin Qu", "title": "Towards Automated Infographic Design: Deep Learning-based\n  Auto-Extraction of Extensible Timeline", "comments": "10 pages, Automated Infographic Design, Deep Learning-based Approach,\n  Timeline Infographics, Multi-task Model", "journal-ref": "TVCG2019", "doi": "10.1109/TVCG.2019.2934810", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designers need to consider not only perceptual effectiveness but also visual\nstyles when creating an infographic. This process can be difficult and time\nconsuming for professional designers, not to mention non-expert users, leading\nto the demand for automated infographics design. As a first step, we focus on\ntimeline infographics, which have been widely used for centuries. We contribute\nan end-to-end approach that automatically extracts an extensible timeline\ntemplate from a bitmap image. Our approach adopts a deconstruction and\nreconstruction paradigm. At the deconstruction stage, we propose a multi-task\ndeep neural network that simultaneously parses two kinds of information from a\nbitmap timeline: 1) the global information, i.e., the representation, scale,\nlayout, and orientation of the timeline, and 2) the local information, i.e.,\nthe location, category, and pixels of each visual element on the timeline. At\nthe reconstruction stage, we propose a pipeline with three techniques, i.e.,\nNon-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an\nextensible template from the infographic, by utilizing the deconstruction\nresults. To evaluate the effectiveness of our approach, we synthesize a\ntimeline dataset (4296 images) and collect a real-world timeline dataset (393\nimages) from the Internet. We first report quantitative evaluation results of\nour approach over the two datasets. Then, we present examples of automatically\nextracted templates and timelines automatically generated based on these\ntemplates to qualitatively demonstrate the performance. The results confirm\nthat our approach can effectively extract extensible templates from real-world\ntimeline infographics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:20:33 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 12:12:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Zhutian", ""], ["Wang", "Yun", ""], ["Wang", "Qianwen", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13615", "submitter": "Qianli Ma", "authors": "Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard\n  Pons-Moll, Siyu Tang, Michael J. Black", "title": "Learning to Dress 3D People in Generative Clothing", "comments": "CVPR-2020 camera ready. Code and data are available at\n  https://cape.is.tue.mpg.de", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional human body models are widely used in the analysis of human\npose and motion. Existing models, however, are learned from minimally-clothed\n3D scans and thus do not generalize to the complexity of dressed people in\ncommon images and videos. Additionally, current models lack the expressive\npower needed to represent the complex non-linear geometry of pose-dependent\nclothing shapes. To address this, we learn a generative 3D mesh model of\nclothed people from 3D scans with varying pose and clothing. Specifically, we\ntrain a conditional Mesh-VAE-GAN to learn the clothing deformation from the\nSMPL body model, making clothing an additional term in SMPL. Our model is\nconditioned on both pose and clothing type, giving the ability to draw samples\nof clothing to dress different body shapes in a variety of styles and poses. To\npreserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to\n3D meshes. Our model, named CAPE, represents global shape and fine local\nstructure, effectively extending the SMPL body model to clothing. To our\nknowledge, this is the first generative model that directly dresses 3D human\nbody meshes and generalizes to different poses. The model, code and data are\navailable for research purposes at https://cape.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:30:54 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 18:50:07 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 17:21:49 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Ma", "Qianli", ""], ["Yang", "Jinlong", ""], ["Ranjan", "Anurag", ""], ["Pujades", "Sergi", ""], ["Pons-Moll", "Gerard", ""], ["Tang", "Siyu", ""], ["Black", "Michael J.", ""]]}]