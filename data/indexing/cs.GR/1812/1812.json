[{"id": "1812.00003", "submitter": "Yu-Ki Lee", "authors": "Yu-Ki Lee, Zhonghua Xi, Young-Joo Lee, Yun-Hyeong Kim, Yue Hao,\n  Young-Chang Joo, Changsoon Kim, Jyh-Ming Lien, In-Suk Choi", "title": "Computational paper wrapping transforms non-stretchable 2D devices into\n  wearable and conformable 3D devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cond-mat.soft cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study starts from the counter-intuitive question of how we can render a\nconventional stiff, non-stretchable and even brittle material conformable so\nthat it can fully wrap around a curved surface, such as a sphere, without\nfailure. Here, we answer this conundrum by extending geometrical design in\ncomputational kirigami (paper cutting and folding) to paper wrapping. Our\ncomputational paper wrapping-based approach provides the more robust and\nreliable fabrication of conformal devices than paper folding approaches. This\nin turn leads to a significant increase in the applicability of computational\nkirigami to real-world fabrication. This new computer-aided design transforms\n2D-based conventional materials, such as Si and copper, into a variety of\ntargeted conformal structures that can fully wrap the desired 3D structure\nwithout plastic deformation or fracture. We further demonstrated that our novel\napproach enables a pluripotent design platform to transform conventional\nnon-stretchable 2D-based devices, such as electroluminescent lighting and a\npaper battery, into wearable and conformable 3D curved devices.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 05:13:50 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 13:56:18 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Lee", "Yu-Ki", ""], ["Xi", "Zhonghua", ""], ["Lee", "Young-Joo", ""], ["Kim", "Yun-Hyeong", ""], ["Hao", "Yue", ""], ["Joo", "Young-Chang", ""], ["Kim", "Changsoon", ""], ["Lien", "Jyh-Ming", ""], ["Choi", "In-Suk", ""]]}, {"id": "1812.00110", "submitter": "Swapneel Mehta", "authors": "Swapneel Mehta, Chirag Raman, Nitin Ayer, Sameer Sahasrabudhe", "title": "Auto-Grading for 3D Modeling Assignments in MOOCs", "comments": null, "journal-ref": null, "doi": "10.1109/ICALT.2018.00012", "report-no": null, "categories": "cs.CY cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottlenecks such as the latency in correcting assignments and providing a\ngrade for Massive Open Online Courses (MOOCs) could impact the levels of\ninterest among learners. In this proposal for an auto-grading system, we\npresent a method to simplify grading for an online course that focuses on 3D\nModeling, thus addressing a critical component of the MOOC ecosystem that\naffects. Our approach involves a live auto-grader that is capable of attaching\ndescriptive labels to assignments which will be deployed for evaluating\nsubmissions. This paper presents a brief overview of this auto-grading system\nand the reasoning behind its inception. Preliminary internal tests show that\nour system presents results comparable to human graders.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 00:50:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Mehta", "Swapneel", ""], ["Raman", "Chirag", ""], ["Ayer", "Nitin", ""], ["Sahasrabudhe", "Sameer", ""]]}, {"id": "1812.00232", "submitter": "Jung-Hyun Byun", "authors": "JungHyun Byun, SeungHo Chae, and TackDon Han", "title": "Accurate control of a pan-tilt system based on parameterization of\n  rotational motion", "comments": "Presented at EUROGRAPHICS 2018 as Short Paper", "journal-ref": null, "doi": "10.2312/egs.20181044", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pan-tilt camera system has been adopted by a variety of fields since it can\ncover a wide range of region compared to a single fixated camera setup. Yet\nmany studies rely on factory-assembled and calibrated platforms and assume an\nideal rotation where rotation axes are perfectly aligned with the optical axis\nof the local camera. However, in a user-created setup where a pan-tilting\nmechanism is arbitrarily assembled, the kinematic configurations may be\ninaccurate or unknown, violating ideal rotation. These discrepancies in the\nmodel with the real physics result in erroneous servo manipulation of the\npan-tilting system. In this paper, we propose an accurate control mechanism for\narbitrarily-assembled pan-tilt camera systems. The proposed method formulates\npan-tilt rotations as motion along great circle trajectories and calibrates its\nmodel parameters, such as positions and vectors of rotation axes, in 3D space.\nThen, one can accurately servo pan-tilt rotations with pose estimation from\ninverse kinematics of their transformation. The comparative experiment\ndemonstrates out-performance of the proposed method, in terms of accurately\nlocalizing target points in world coordinates, after being rotated from their\ncaptured camera frames.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 17:53:57 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Byun", "JungHyun", ""], ["Chae", "SeungHo", ""], ["Han", "TackDon", ""]]}, {"id": "1812.00233", "submitter": "Jung-Hyun Byun", "authors": "JungHyun Byun, SeungHo Chae, YoonSik Yang and TackDon Han", "title": "AIR: Anywhere Immersive Reality with User-Perspective Projection", "comments": "Presented at EUROGRAPHICS 2017 as Short Paper", "journal-ref": null, "doi": "10.2312/egsh.20171001", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection-based augmented reality (AR) has much potential, but is limited in\nthat it requires burdensome installations and prone to geometric distortions on\ndisplay surface. To overcome these limitations, we propose AIR. It can be\ncarried and placed anywhere to project AR using pan/tilting motors, while\nproviding the user with distortion-free projection of a correct 3D view.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 17:54:36 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Byun", "JungHyun", ""], ["Chae", "SeungHo", ""], ["Yang", "YoonSik", ""], ["Han", "TackDon", ""]]}, {"id": "1812.00240", "submitter": "Jung-Hyun Byun", "authors": "Jung-Hyun Byun and Tack-Don Han", "title": "Fast and Accurate Reconstruction of Pan-Tilt RGB-D Scans via Axis Bound\n  Registration", "comments": "in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast and accurate algorithm is presented for registering scans from an\nRGB-D camera on a pan-tilt platform. The pan-tilt RGB-D camera rotates and\nscans the entire scene in an automated fashion. The proposed algorithm exploits\nthe movement of the camera that is bound by the two rotation axes of the servo\nmotors so as to realize fast and accurate registration of acquired point\nclouds. The rotation parameters, including the rotation axes, pan-tilt\ntransformations and the servo control mechanism, are calibrated beforehand.\nSubsequently, fast global registration can be performed during online operation\nwith transformation matrices formed by the calibrated rotation axes and angles.\nIn local registration, features are extracted and matched between two scenes.\nFalse-positive correspondences, whose distances to the rotation trajectories\nexceed a threshold, are rejected. Then, a more accurate registration can be\nachieved by minimizing the residual distances between corresponding points,\nwhile transformations are bound to the rotation axes. Finally, the preliminary\nalignment result is input to the iterative closed point algorithm to compute\nthe final transformation. Results of comparative experiments validate that the\nproposed method outperforms state-of-the-art algorithms of various approaches\nbased on camera calibration, global registration, and\nsimultaneous-localization-and-mapping in terms of root-mean-square error and\ncomputation time.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 18:56:02 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 12:48:14 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 05:46:03 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Byun", "Jung-Hyun", ""], ["Han", "Tack-Don", ""]]}, {"id": "1812.00307", "submitter": "Jiaping Ren", "authors": "Jiaping Ren, Wei Xiang, Yangxi Xiao, Ruigang Yang, Dinesh Manocha,\n  Xiaogang Jin", "title": "Heter-Sim: Heterogeneous multi-agent systems simulation by interactive\n  data-driven optimization", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive multi-agent simulation algorithms are used to compute the\ntrajectories and behaviors of different entities in virtual reality scenarios.\nHowever, current methods involve considerable parameter tweaking to generate\nplausible behaviors. We introduce a novel approach (Heter-Sim) that combines\nphysics-based simulation methods with data-driven techniques using an\noptimization-based formulation. Our approach is general and can simulate\nheterogeneous agents corresponding to human crowds, traffic, vehicles, or\ncombinations of different agents with varying dynamics. We estimate motion\nstates from real-world datasets that include information about position,\nvelocity, and control direction. Our optimization algorithm considers several\nconstraints, including velocity continuity, collision avoidance, attraction,\nand direction control. To accelerate the computations, we reduce the search\nspace for both collision avoidance and optimal solution computation. Heter-Sim\ncan simulate tens or hundreds of agents at interactive rates and we compare its\naccuracy with real-world datasets and prior algorithms. We also perform user\nstudies that evaluate the plausible behaviors generated by our algorithm and a\nuser study that evaluates the plausibility of our algorithm via VR.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 02:36:00 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ren", "Jiaping", ""], ["Xiang", "Wei", ""], ["Xiao", "Yangxi", ""], ["Yang", "Ruigang", ""], ["Manocha", "Dinesh", ""], ["Jin", "Xiaogang", ""]]}, {"id": "1812.00606", "submitter": "Chenming Wu", "authors": "Chenming Wu, Chengkai Dai, Guoxin Fang, Yong-Jin Liu, Charlie C.L.\n  Wang", "title": "General Support-Effective Decomposition for Multi-Directional 3D\n  Printing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for fabricating general models with multi-directional 3D\nprinting systems by printing different model regions along with different\ndirections. The core of our method is a support-effective volume decomposition\nalgorithm that minimizes the area of the regions with large overhangs. A\nbeam-guided searching algorithm with manufacturing constraints determines the\noptimal volume decomposition, which is represented by a sequence of clipping\nplanes. While current approaches require manually assembling separate\ncomponents into a final model, our algorithm allows for directly printing the\nfinal model in a single pass. It can also be applied to models with loops and\nhandles. A supplementary algorithm generates special supporting structures for\nmodels where supporting structures for large overhangs cannot be eliminated. We\nverify the effectiveness of our method using two hardware systems: a\nCartesian-motion based system and an angular-motion based system. A variety of\n3D models have been successfully fabricated on these systems.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 08:53:33 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:05:44 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 00:45:24 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 03:32:28 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Wu", "Chenming", ""], ["Dai", "Chengkai", ""], ["Fang", "Guoxin", ""], ["Liu", "Yong-Jin", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "1812.01387", "submitter": "Zelin Zhao", "authors": "Zelin Zhao, Gao Peng, Haoyu Wang, Hao-Shu Fang, Chengkun Li, Cewu Lu", "title": "Estimating 6D Pose From Localizing Designated Surface Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an accurate yet effective solution for 6D pose\nestimation from an RGB image. The core of our approach is that we first\ndesignate a set of surface points on target object model as keypoints and then\ntrain a keypoint detector (KPD) to localize them. Finally a PnP algorithm can\nrecover the 6D pose according to the 2D-3D relationship of keypoints. Different\nfrom recent state-of-the-art CNN-based approaches that rely on a time-consuming\npost-processing procedure, our method can achieve competitive accuracy without\nany refinement after pose prediction. Meanwhile, we obtain a 30% relative\nimprovement in terms of ADD accuracy among methods without using refinement.\nMoreover, we succeed in handling heavy occlusion by selecting the most\nconfident keypoints to recover the 6D pose. For the sake of reproducibility, we\nwill make our code and models publicly available soon.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:55:06 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhao", "Zelin", ""], ["Peng", "Gao", ""], ["Wang", "Haoyu", ""], ["Fang", "Hao-Shu", ""], ["Li", "Chengkun", ""], ["Lu", "Cewu", ""]]}, {"id": "1812.01525", "submitter": "Hang Chu", "authors": "Hang Chu, Daiqing Li, Sanja Fidler", "title": "A Face-to-Face Neural Conversation Model", "comments": "Published at CVPR 2018", "journal-ref": "CVPR (2018) 7113-7121", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have recently become good at engaging in dialog. However,\ncurrent approaches are based solely on verbal text, lacking the richness of a\nreal face-to-face conversation. We propose a neural conversation model that\naims to read and generate facial gestures alongside with text. This allows our\nmodel to adapt its response based on the \"mood\" of the conversation. In\nparticular, we introduce an RNN encoder-decoder that exploits the movement of\nfacial muscles, as well as the verbal conversation. The decoder consists of two\nlayers, where the lower layer aims at generating the verbal response and coarse\nfacial expressions, while the second layer fills in the subtle gestures, making\nthe generated output more smooth and natural. We train our neural network by\nhaving it \"watch\" 250 movies. We showcase our joint face-text model in\ngenerating more natural conversations through automatic metrics and a human\nstudy. We demonstrate an example application with a face-to-face chatting\navatar.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:55:25 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Chu", "Hang", ""], ["Li", "Daiqing", ""], ["Fidler", "Sanja", ""]]}, {"id": "1812.01598", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Hanbyul Joo, Yaser Sheikh", "title": "Monocular Total Capture: Posing Face, Body, and Hands in the Wild", "comments": "17 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first method to capture the 3D total motion of a target person\nfrom a monocular view input. Given an image or a monocular video, our method\nreconstructs the motion from body, face, and fingers represented by a 3D\ndeformable mesh model. We use an efficient representation called 3D Part\nOrientation Fields (POFs), to encode the 3D orientations of all body parts in\nthe common 2D image space. POFs are predicted by a Fully Convolutional Network\n(FCN), along with the joint confidence maps. To train our network, we collect a\nnew 3D human motion dataset capturing diverse total body motion of 40 subjects\nin a multiview system. We leverage a 3D deformable human model to reconstruct\ntotal body pose from the CNN outputs by exploiting the pose and shape prior in\nthe model. We also present a texture-based tracking method to obtain temporally\ncoherent motion capture output. We perform thorough quantitative evaluations\nincluding comparison with the existing body-specific and hand-specific methods,\nand performance analysis on camera viewpoint and human pose changes. Finally,\nwe demonstrate the results of our total body motion capture on various\nchallenging in-the-wild videos. Our code and newly collected human motion\ndataset will be publicly shared.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 18:55:33 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Xiang", "Donglai", ""], ["Joo", "Hanbyul", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1812.01608", "submitter": "Jacob Menick", "authors": "Jacob Menick, Nal Kalchbrenner", "title": "Generating High Fidelity Images with Subscale Pixel Networks and\n  Multidimensional Upscaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unconditional generation of high fidelity images is a longstanding\nbenchmark for testing the performance of image decoders. Autoregressive image\nmodels have been able to generate small images unconditionally, but the\nextension of these methods to large images where fidelity can be more readily\nassessed has remained an open problem. Among the major challenges are the\ncapacity to encode the vast previous context and the sheer difficulty of\nlearning a distribution that preserves both global semantic coherence and\nexactness of detail. To address the former challenge, we propose the Subscale\nPixel Network (SPN), a conditional decoder architecture that generates an image\nas a sequence of sub-images of equal size. The SPN compactly captures\nimage-wide spatial dependencies and requires a fraction of the memory and the\ncomputation required by other fully autoregressive models. To address the\nlatter challenge, we propose to use Multidimensional Upscaling to grow an image\nin both size and depth via intermediate stages utilising distinct SPNs. We\nevaluate SPNs on the unconditional generation of CelebAHQ of size 256 and of\nImageNet from size 32 to 256. We achieve state-of-the-art likelihood results in\nmultiple settings, set up new benchmark results in previously unexplored\nsettings and are able to generate very high fidelity large scale samples on the\nbasis of both datasets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 15:47:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Menick", "Jacob", ""], ["Kalchbrenner", "Nal", ""]]}, {"id": "1812.01677", "submitter": "Ning Jin", "authors": "Ning Jin, Yilin Zhu, Zhenglin Geng and Ronald Fedkiw", "title": "A Pixel-Based Framework for Data-Driven Clothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aim of creating virtual cloth deformations more similar to real\nworld clothing, we propose a new computational framework that recasts three\ndimensional cloth deformation as an RGB image in a two dimensional pattern\nspace. Then a three dimensional animation of cloth is equivalent to a sequence\nof two dimensional RGB images, which in turn are driven/choreographed via\nanimation parameters such as joint angles. This allows us to leverage popular\nCNNs to learn cloth deformations in image space. The two dimensional cloth\npixels are extended into the real world via standard body skinning techniques,\nafter which the RGB values are interpreted as texture offsets and displacement\nmaps. Notably, we illustrate that our approach does not require accurate\nunclothed body shapes or robust skinning techniques. Additionally, we discuss\nhow standard image based techniques such as image partitioning for higher\nresolution, GANs for merging partitioned image regions back together, etc., can\nreadily be incorporated into our framework.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 04:52:10 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Jin", "Ning", ""], ["Zhu", "Yilin", ""], ["Geng", "Zhenglin", ""], ["Fedkiw", "Ronald", ""]]}, {"id": "1812.02246", "submitter": "Chung-Yi Weng", "authors": "Chung-Yi Weng, Brian Curless, Ira Kemelmacher-Shlizerman", "title": "Photo Wake-Up: 3D Character Animation from a Single Photo", "comments": "The project page is at\n  https://grail.cs.washington.edu/projects/wakeup/, and the supplementary video\n  is at https://youtu.be/G63goXc5MyU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method and application for animating a human subject from a\nsingle photo. E.g., the character can walk out, run, sit, or jump in 3D. The\nkey contributions of this paper are: 1) an application of viewing and animating\nhumans in single photos in 3D, 2) a novel 2D warping method to deform a posable\ntemplate body model to fit the person's complex silhouette to create an\nanimatable mesh, and 3) a method for handling partial self occlusions. We\ncompare to state-of-the-art related methods and evaluate results with human\nstudies. Further, we present an interactive interface that allows re-posing the\nperson in 3D, and an augmented reality setup where the animated 3D person can\nemerge from the photo into the real world. We demonstrate the method on photos,\nposters, and art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 22:09:52 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Weng", "Chung-Yi", ""], ["Curless", "Brian", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1812.02552", "submitter": "Mojtaba Bemana", "authors": "Mojtaba Bemana, Joachim Keinert, Karol Myszkowski, Michel B\\\"atz,\n  Matthias Ziegler, Hans-Peter Seidel, Tobias Ritschel", "title": "Learning to Predict Image-based Rendering Artifacts with Respect to a\n  Hidden Reference Image", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image metrics predict the perceived per-pixel difference between a reference\nimage and its degraded (e. g., re-rendered) version. In several important\napplications, the reference image is not available and image metrics cannot be\napplied. We devise a neural network architecture and training procedure that\nallows predicting the MSE, SSIM or VGG16 image difference from the distorted\nimage alone while the reference is not observed. This is enabled by two\ninsights: The first is to inject sufficiently many un-distorted natural image\npatches, which can be found in arbitrary amounts and are known to have no\nperceivable difference to themselves. This avoids false positives. The second\nis to balance the learning, where it is carefully made sure that all image\nerrors are equally likely, avoiding false negatives. Surprisingly, we observe,\nthat the resulting no-reference metric, subjectively, can even perform better\nthan the reference-based one, as it had to become robust against\nmis-alignments. We evaluate the effectiveness of our approach in an image-based\nrendering context, both quantitatively and qualitatively. Finally, we\ndemonstrate two applications which reduce light field capture time and provide\nguidance for interactive depth adjustment.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 14:35:54 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 13:47:10 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Bemana", "Mojtaba", ""], ["Keinert", "Joachim", ""], ["Myszkowski", "Karol", ""], ["B\u00e4tz", "Michel", ""], ["Ziegler", "Matthias", ""], ["Seidel", "Hans-Peter", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1812.02725", "submitter": "Jun-Yan Zhu", "authors": "Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu, Antonio\n  Torralba, Joshua B. Tenenbaum, William T. Freeman", "title": "Visual Object Networks: Image Generation with Disentangled 3D\n  Representation", "comments": "NeurIPS 2018. Code: https://github.com/junyanz/VON Website:\n  http://von.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep generative models has led to tremendous breakthroughs\nin image generation. However, while existing models can synthesize\nphotorealistic images, they lack an understanding of our underlying 3D world.\nWe present a new generative model, Visual Object Networks (VON), synthesizing\nnatural images of objects with a disentangled 3D representation. Inspired by\nclassic graphics rendering pipelines, we unravel our image formation process\ninto three conditionally independent factors---shape, viewpoint, and\ntexture---and present an end-to-end adversarial learning framework that jointly\nmodels 3D shapes and 2D images. Our model first learns to synthesize 3D shapes\nthat are indistinguishable from real shapes. It then renders the object's 2.5D\nsketches (i.e., silhouette and depth map) from its shape under a sampled\nviewpoint. Finally, it learns to add realistic texture to these 2.5D sketches\nto generate natural images. The VON not only generates images that are more\nrealistic than state-of-the-art 2D image synthesis methods, but also enables\nmany 3D operations such as changing the viewpoint of a generated image, editing\nof shape and texture, linear interpolation in texture and shape space, and\ntransferring appearance across different objects and viewpoints.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:58:34 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Zhu", "Jun-Yan", ""], ["Zhang", "Zhoutong", ""], ["Zhang", "Chengkai", ""], ["Wu", "Jiajun", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""]]}, {"id": "1812.02822", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen and Hao Zhang", "title": "Learning Implicit Fields for Generative Shape Modeling", "comments": "Accepted to CVPR 2019. Code:\n  https://github.com/czq142857/implicit-decoder Project page:\n  https://www.sfu.ca/~zhiqinc/imgan/Readme.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate the use of implicit fields for learning generative models of\nshapes and introduce an implicit field decoder, called IM-NET, for shape\ngeneration, aimed at improving the visual quality of the generated shapes. An\nimplicit field assigns a value to each point in 3D space, so that a shape can\nbe extracted as an iso-surface. IM-NET is trained to perform this assignment by\nmeans of a binary classifier. Specifically, it takes a point coordinate, along\nwith a feature vector encoding a shape, and outputs a value which indicates\nwhether the point is outside the shape or not. By replacing conventional\ndecoders by our implicit decoder for representation learning (via IM-AE) and\nshape generation (via IM-GAN), we demonstrate superior results for tasks such\nas generative shape modeling, interpolation, and single-view 3D reconstruction,\nparticularly in terms of visual quality. Code and supplementary material are\navailable at https://github.com/czq142857/implicit-decoder.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 21:52:33 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 04:36:31 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 04:43:34 GMT"}, {"version": "v4", "created": "Mon, 3 Jun 2019 22:25:57 GMT"}, {"version": "v5", "created": "Mon, 16 Sep 2019 20:29:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chen", "Zhiqin", ""], ["Zhang", "Hao", ""]]}, {"id": "1812.03066", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-Services, IHMTEK), Anton Andreev\n  (GIPSA-Services, CNRS), Bastien Maureille (IHMTEK), Marco Congedo\n  (GIPSA-Services)", "title": "Analysis of tagging latency when comparing event-related potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-related potentials (ERPs) are very small voltage produced by the brain\nin response to external stimulation. In order to detect and evaluate an ERP in\nan ongoing electroencephalogram (EEG), it is necessary to tag the EEG with the\nexact onset time of the stimulus. We define the latency as the delay between\nthe time the tagging command is sent and the detection of the stimulus on the\nscreen. Failing to control sequencing in the tagging pipeline causes problems\nwhen interpreting latency, in particular when comparing ERPs generated from\nstimuli displayed by different systems. In this work, we present number of\ntechnical aspects which can influence latency such as the refresh rate of the\nscreen or the display of a stimulus at different screen location. A few\npropositions are suggested to estimate and correct this latency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 15:19:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-Services, IHMTEK"], ["Andreev", "Anton", "", "GIPSA-Services, CNRS"], ["Maureille", "Bastien", "", "IHMTEK"], ["Congedo", "Marco", "", "GIPSA-Services"]]}, {"id": "1812.03794", "submitter": "Abhishek Sharma", "authors": "Jean-Michel Roufosse, Abhishek Sharma, Maks Ovsjanikov", "title": "Unsupervised Deep Learning for Structured Shape Matching", "comments": "Oral Presentation at ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for computing correspondences across 3D shapes\nusing unsupervised learning. Our method computes a non-linear transformation of\ngiven descriptor functions, while optimizing for global structural properties\nof the resulting maps, such as their bijectivity or approximate isometry. To\nthis end, we use the functional maps framework, and build upon the recent FMNet\narchitecture for descriptor learning. Unlike that approach, however, we show\nthat learning can be done in a purely \\emph{unsupervised setting}, without\nhaving access to any ground truth correspondences. This results in a very\ngeneral shape matching method that we call SURFMNet for Spectral Unsupervised\nFMNet, and which can be used to establish correspondences within 3D shape\ncollections without any prior information. We demonstrate on a wide range of\nchallenging benchmarks, that our approach leads to state-of-the-art results\ncompared to the existing unsupervised methods and achieves results that are\ncomparable even to the supervised learning techniques. Moreover, our framework\nis an order of magnitude faster, and does not rely on geodesic distance\ncomputation or expensive post-processing.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:50:34 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:07:31 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 10:53:58 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Roufosse", "Jean-Michel", ""], ["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1812.04223", "submitter": "Rushan Ziatdinov", "authors": "Rushan Ziatdinov, Valerijan Muftejev, Rifkat Nabiyev, Albert Mardanov,\n  Rustam Akhmetshin", "title": "Techniques for modeling a high-quality B-spline curves by S-polygons in\n  a float format", "comments": "4 pages, 3 figures", "journal-ref": "GraphiCon 2018 Conference Proceedings, Tomsk, Russia", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a technique for the geometrically stable modeling of\nhigh-degree B-spline curves based on S-polygon in a float format, which will\nallow the accurate positioning of the end points of curves and the direction of\nthe tangent vectors. The method of shape approximation is described with the\npurpose of providing geometrical proximity between the original and\napproximating curve. The content of the notion of a harmonious, regular form of\nB-spline curve's S-polygon in a float format is revealed as a factor in\nachieving a high-quality of fit for the generated curve. The expediency of the\nshape modeling method based on S-polygon in a float format at the end portions\nof the curve for quality control of curve modeling and editing is\nsubstantiated. The results of a comparative test are presented, demonstrating\nthe superlative efficacy of using the Mineur-Farin configuration for\nconstructing constant and monotone curvature curves based on an S-polygon in a\nfloat format. The findings presented in this article confirm that it is\npreferable to employ the principle of \"constructing a control polygon of a\nharmonious form (or the Mineur-Farin configuration) of a parametric polynomial\"\nto a B-spline curve's S-polygon in a float format, and not to a B-polygon of\nthe Bezier curve. Recommendations are given for prospective studies in the\nfield of applying the technique of constructing a high-quality B-spline curves\nto the approximation of log-aesthetic curves, Ziatdinov's superspirals, etc.\nThe authors of the article developed a technique for constructing smooth\nconnections of B-spline curves with ensuring a high order of smoothness of the\ncomposite curve. The proposed techniques are implemented in the\nFairCurveModeler program as a plug-in to engineering CAD systems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 05:23:37 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Ziatdinov", "Rushan", ""], ["Muftejev", "Valerijan", ""], ["Nabiyev", "Rifkat", ""], ["Mardanov", "Albert", ""], ["Akhmetshin", "Rustam", ""]]}, {"id": "1812.04233", "submitter": "Chiranjoy Chattopadhyay", "authors": "Pratik Kalshetti, Parag Rahangdale, Dinesh Jangra, Manas Bundele,\n  Chiranjoy Chattopadhyay", "title": "Antara: An Interactive 3D Volume Rendering and Visualization Framework", "comments": "10 pages, 13 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of 3D visualization is to provide the user with an intuitive\ninterface which enables him to explore the 3D data in an interactive manner.\nThe aim of the exploration is to identify and analyze anomalies or to give\nproof of the non-anomaly of the visualized organic structures. For 3D Medical\nData, Magnetic Resonance Images (MRI) has been used. To create the 3D model, we\nused the Direct Volume Rendering technique. In the input 3D data, we have $x,\ny$ and $z$ coordinates and an intensity value for each voxel. The 3D data is\nused by Volume Ray Casting to compute 2D projections from 3D volumetric data\nsets. In ray casting, a ray of light is made to pass through the volume data.\nThe interaction of each voxel with this ray is used to assign RGB and alpha\nvalues for every voxel in the volume. As a result, we are able to generate the\n3D model of the region of interest using the 3D data. The 3D model is\ninteractive, thus enabling us to visualize the different layers of the 3D\nvolume by adjusting the transfer function.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 06:47:49 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kalshetti", "Pratik", ""], ["Rahangdale", "Parag", ""], ["Jangra", "Dinesh", ""], ["Bundele", "Manas", ""], ["Chattopadhyay", "Chiranjoy", ""]]}, {"id": "1812.04303", "submitter": "Enrico Grisan", "authors": "Marco Virgulin and Marco Castellaro and Enrico Grisan and Fabio\n  Marcuzzi", "title": "Analytic heuristics for a fast DSC-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we propose a deterministic approach for the reconstruction of\nDynamic Susceptibility Contrast magnetic resonance imaging data and compare it\nwith the compressed sensing solution existing in the literature for the same\nproblem. Our study is based on the mathematical analysis of the problem, which\nis computationally intractable because of its non polynomial complexity, but\nsuggests simple heuristics that perform quite well. We give results on real\nimages and on artificial phantoms with added noise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 09:46:45 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Virgulin", "Marco", ""], ["Castellaro", "Marco", ""], ["Grisan", "Enrico", ""], ["Marcuzzi", "Fabio", ""]]}, {"id": "1812.06060", "submitter": "Bailin Deng", "authors": "Jiong Tao, Juyong Zhang, Bailin Deng, Zheng Fang, Yue Peng, Ying He", "title": "Parallel and Scalable Heat Methods for Geodesic Distance Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a parallel and scalable approach for geodesic\ndistance computation on triangle meshes. Our key observation is that the\nrecovery of geodesic distance with the heat method from [Crane et al. 2013] can\nbe reformulated as optimization of its gradients subject to integrability,\nwhich can be solved using an efficient first-order method that requires no\nlinear system solving and converges quickly. Afterward, the geodesic distance\nis efficiently recovered by parallel integration of the optimized gradients in\nbreadth-first order. Moreover, we employ a similar breadth-first strategy to\nderive a parallel Gauss-Seidel solver for the diffusion step in the heat\nmethod. To further lower the memory consumption from gradient optimization on\nfaces, we also propose a formulation that optimizes the projected gradients on\nedges, which reduces the memory footprint by about 50%. Our approach is\ntrivially parallelizable, with a low memory footprint that grows linearly with\nrespect to the model size. This makes it particularly suitable for handling\nlarge models. Experimental results show that it can efficiently compute\ngeodesic distance on meshes with more than 200 million vertices on a desktop PC\nwith 128GB RAM, outperforming the original heat method and other\nstate-of-the-art geodesic distance solvers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:15:44 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 16:04:29 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 16:39:05 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Tao", "Jiong", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""], ["Fang", "Zheng", ""], ["Peng", "Yue", ""], ["He", "Ying", ""]]}, {"id": "1812.06216", "submitter": "Sebastian Koch", "authors": "Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams,\n  Alexey Artemov, Evgeny Burnaev, Marc Alexa, Denis Zorin, Daniele Panozzo", "title": "ABC: A Big CAD Model Dataset For Geometric Deep Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ABC-Dataset, a collection of one million Computer-Aided Design\n(CAD) models for research of geometric deep learning methods and applications.\nEach model is a collection of explicitly parametrized curves and surfaces,\nproviding ground truth for differential quantities, patch segmentation,\ngeometric feature detection, and shape reconstruction. Sampling the parametric\ndescriptions of surfaces and curves allows generating data in different formats\nand resolutions, enabling fair comparisons for a wide range of geometric\nlearning algorithms. As a use case for our dataset, we perform a large-scale\nbenchmark for estimation of surface normals, comparing existing data driven\nmethods and evaluating their performance against both the ground truth and\ntraditional normal estimation methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 01:21:48 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 07:18:44 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Koch", "Sebastian", ""], ["Matveev", "Albert", ""], ["Jiang", "Zhongshi", ""], ["Williams", "Francis", ""], ["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""], ["Alexa", "Marc", ""], ["Zorin", "Denis", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1812.07122", "submitter": "Wei Liu", "authors": "Wei Liu, Pingping Zhang, Xiaogang Chen, Chunhua Shen, Xiaolin Huang\n  and Jie Yang", "title": "Embedding Bilateral Filter in Least Squares for Efficient\n  Edge-preserving Image Smoothing", "comments": "accepted by TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-preserving smoothing is a fundamental procedure for many computer vision\nand graphic applications. This can be achieved with either local methods or\nglobal methods. In most cases, global methods can yield superior performance\nover local ones. However, local methods usually run much faster than global\nones. In this paper, we propose a new global method that embeds the bilateral\nfilter in the least squares model for efficient edge-preserving smoothing. The\nproposed method can show comparable performance with the state-of-the-art\nglobal method. Meanwhile, since the proposed method can take advantages of the\nefficiency of the bilateral filter and least squares model, it runs much\nfaster. In addition, we show the flexibility of our method which can be easily\nextended by replacing the bilateral filter with its variants. They can be\nfurther modified to handle more applications. We validate the effectiveness and\nefficiency of the proposed method through comprehensive experiments in a range\nof applications.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:00:28 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Chen", "Xiaogang", ""], ["Shen", "Chunhua", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""]]}, {"id": "1812.07273", "submitter": "Magdalena Schwarzl", "authors": "M. Schwarzl, L. Autin, G. Johnson, T. Torsney-Weir and T. M\\\"oller", "title": "cellPACKexplorer: Interactive Model Building for Volumetric Data of\n  Complex Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an algorithm the quality of the output largely depends on a proper\nspecification of the input parameters. A lot of work has been done to analyze\ntasks related to using a fixed model [25] and finding a good set of inputs. In\nthis paper we present a different scenario, model building. In contrast to\nmodel usage the underlying algorithm, i.e. the underlying model, changes and\ntherefore the associated parameters also change. Developing a new algorithm\nrequires a particular set of parameters that, on the one hand, give access to\nan expected range of outputs and, on the other hand, are still interpretable.\nAs the model is developed and parameters are added, deleted, or changed\ndifferent features of the outputs are of interest. Therefore it is important to\nfind objective measures that quantify these features. In a model building\nprocess these features are prone to change and need to be adaptable as the\nmodel changes. We discuss these problems in the application of cellPACK, a tool\nthat generates virtual 3D cells. Our analysis is based on an output set\ngenerated by sampling the input parameter space. Hence we also present\ntechniques and metrics to analyze an ensemble of probabilistic volumes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:18:06 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Schwarzl", "M.", ""], ["Autin", "L.", ""], ["Johnson", "G.", ""], ["Torsney-Weir", "T.", ""], ["M\u00f6ller", "T.", ""]]}, {"id": "1812.07524", "submitter": "Lukasz Romaszko", "authors": "Lukasz Romaszko, Christopher K.I. Williams, John Winn", "title": "Learning Direct Optimization for Scene Understanding", "comments": null, "journal-ref": "Pattern Recognition, Volume 105, 2020, 107369", "doi": "10.1016/j.patcog.2020.107369", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Learning Direct Optimization (LiDO) method for the refinement of\na latent variable model that describes input image x. Our goal is to explain a\nsingle image x with an interpretable 3D computer graphics model having scene\ngraph latent variables z (such as object appearance, camera position). Given a\ncurrent estimate of z we can render a prediction of the image g(z), which can\nbe compared to the image x. The standard way to proceed is then to measure the\nerror E(x, g(z)) between the two, and use an optimizer to minimize the error.\nHowever, it is unknown which error measure E would be most effective for\nsimultaneously addressing issues such as misaligned objects, occlusions,\ntextures, etc. In contrast, the LiDO approach trains a Prediction Network to\npredict an update directly to correct z, rather than minimizing the error with\nrespect to z. Experiments show that our LiDO method converges rapidly as it\ndoes not need to perform a search on the error landscape, produces better\nsolutions than error-based competitors, and is able to handle the mismatch\nbetween the data and the fitted scene model. We apply LiDO to a realistic\nsynthetic dataset, and show that the method also transfers to work well with\nreal images.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:46:13 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 13:43:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Romaszko", "Lukasz", ""], ["Williams", "Christopher K. I.", ""], ["Winn", "John", ""]]}, {"id": "1812.07710", "submitter": "Holly Grimm", "authors": "Holly Grimm", "title": "Training on Art Composition Attributes to Influence CycleGAN Art\n  Generation", "comments": "Poster at Neural Information Processing Systems 2018 Workshop on\n  Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I consider how to influence CycleGAN, image-to-image translation, by using\nadditional constraints from a neural network trained on art composition\nattributes. I show how I trained the the Art Composition Attributes Network\n(ACAN) by incorporating domain knowledge based on the rules of art evaluation\nand the result of applying each art composition attribute to apple2orange image\ntranslation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 00:36:10 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Grimm", "Holly", ""]]}, {"id": "1812.08861", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, St\\'ephane Lathuili\\`ere, Sergey Tulyakov, Elisa\n  Ricci and Nicu Sebe", "title": "Animating Arbitrary Objects via Deep Motion Transfer", "comments": "CVPR-2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel deep learning framework for image animation.\nGiven an input image with a target object and a driving video sequence\ndepicting a moving object, our framework generates a video in which the target\nobject is animated according to the driving sequence. This is achieved through\na deep architecture that decouples appearance and motion information. Our\nframework consists of three main modules: (i) a Keypoint Detector unsupervisely\ntrained to extract object keypoints, (ii) a Dense Motion prediction network for\ngenerating dense heatmaps from sparse keypoints, in order to better encode\nmotion information and (iii) a Motion Transfer Network, which uses the motion\nheatmaps and appearance information extracted from the input image to\nsynthesize the output frames. We demonstrate the effectiveness of our method on\nseveral benchmark datasets, spanning a wide variety of object appearances, and\nshow that our approach outperforms state-of-the-art image animation and video\ngeneration methods. Our source code is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:45:56 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 08:01:58 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 23:48:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Tulyakov", "Sergey", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1812.09569", "submitter": "Sergey Belim", "authors": "S.V. Belim, S.B. Larionov", "title": "The algorithm of formation of a training set for an artificial neural\n  network for image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article suggests an algorithm of formation a training set for artificial\nneural network in case of image segmentation. The distinctive feature of this\nalgorithm is that it using only one image for segmentation. The segmentation\nperforms using three-layer perceptron. The main method of the segmentation is a\nmethod of region growing. Neural network is using for get a decision to include\npixel into an area or not. Impulse noise is using for generation of a training\nset. Pixels damaged by noise are not related to the same region. Suggested\nmethod has been tested with help of computer experiment in automatic and\ninteractive modes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 17:21:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Belim", "S. V.", ""], ["Larionov", "S. B.", ""]]}, {"id": "1812.09874", "submitter": "Oleg Voinov", "authors": "Oleg Voynov, Alexey Artemov, Vage Egiazarian, Alexander Notchenko,\n  Gleb Bobrovskikh, Denis Zorin, Evgeny Burnaev", "title": "Perceptual deep depth super-resolution", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGBD images, combining high-resolution color and lower-resolution depth from\nvarious types of depth sensors, are increasingly common. One can significantly\nimprove the resolution of depth maps by taking advantage of color information;\ndeep learning methods make combining color and depth information particularly\neasy. However, fusing these two sources of data may lead to a variety of\nartifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual\nreality applications, the visual quality of upsampled images is particularly\nimportant. The main idea of our approach is to measure the quality of depth map\nupsampling using renderings of resulting 3D surfaces. We demonstrate that a\nsimple visual appearance-based loss, when used with either a trained CNN or\nsimply a deep prior, yields significantly improved 3D shapes, as measured by a\nnumber of existing perceptual metrics. We compare this approach with a number\nof existing optimization and learning-based techniques.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 09:43:25 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 02:20:13 GMT"}, {"version": "v3", "created": "Mon, 9 Sep 2019 14:12:39 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Voynov", "Oleg", ""], ["Artemov", "Alexey", ""], ["Egiazarian", "Vage", ""], ["Notchenko", "Alexander", ""], ["Bobrovskikh", "Gleb", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1812.10098", "submitter": "Sergey Belim", "authors": "S.V. Belim, S.B. Larionov", "title": "The algorithm of the impulse noise filtration in images based on an\n  algorithm of community detection in graphs", "comments": null, "journal-ref": "2017 Dynamics of Systems, Mechanisms and Machines (Dynamics),\n  Omsk, Russia, pp. 1-5", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article suggests an algorithm of impulse noise filtration, based on the\ncommunity detection in graphs. The image is representing as non-oriented\nweighted graph. Each pixel of an image is corresponding to a vertex of the\ngraph. Community detection algorithm is running on the given graph. Assumed\nthat communities that contain only one pixel are corresponding to noised pixels\nof an image. Suggested method was tested with help of computer experiment. This\nexperiment was conducted on grayscale, and on colored images, on artificial\nimages and on photos. It is shown that the suggested method is better than\nmedian filter by 20% regardless of noise percent. Higher efficiency is\njustified by the fact that most of filters are changing all of image pixels,\nbut suggested method is finding and restoring only noised pixels. The\ndependence of the effectiveness of the proposed method on the percentage of\nnoise in the image is shown.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 12:40:12 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Belim", "S. V.", ""], ["Larionov", "S. B.", ""]]}, {"id": "1812.10111", "submitter": "Hamid Laga", "authors": "Hamid Laga", "title": "A Survey on Non-rigid 3D Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape is an important physical property of natural and manmade 3D objects\nthat characterizes their external appearances. Understanding differences\nbetween shapes and modeling the variability within and across shape classes,\nhereinafter referred to as \\emph{shape analysis}, are fundamental problems to\nmany applications, ranging from computer vision and computer graphics to\nbiology and medicine. This chapter provides an overview of some of the recent\ntechniques that studied the shape of 3D objects that undergo non-rigid\ndeformations including bending and stretching. Recent surveys that covered some\naspects such classification, retrieval, recognition, and rigid or nonrigid\nregistration, focused on methods that use shape descriptors. Descriptors,\nhowever, provide abstract representations that do not enable the exploration of\nshape variability. In this chapter, we focus on recent techniques that treated\nthe shape of 3D objects as points in some high dimensional space where paths\ndescribe deformations. Equipping the space with a suitable metric enables the\nquantification of the range of deformations of a given shape, which in turn\nenables (1) comparing and classifying 3D objects based on their shape, (2)\ncomputing smooth deformations, i.e. geodesics, between pairs of objects, and\n(3) modeling and exploring continuous shape variability in a collection of 3D\nmodels. This article surveys and classifies recent developments in this field,\noutlines fundamental issues, discusses their potential applications in computer\nvision and graphics, and highlights opportunities for future research. Our\nprimary goal is to bridge the gap between various techniques that have been\noften independently proposed by different communities including mathematics and\nstatistics, computer vision and graphics, and medical image analysis.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 14:33:42 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Laga", "Hamid", ""]]}, {"id": "1812.10515", "submitter": "Dmitrij Gendler", "authors": "Dmitrij Gendler, Christian Eisele, Dirk Seiffer, Norbert Wendelstein", "title": "Derivation of an Algorithm for Calculation of the Intersection Area of a\n  Circle with a Grid with Finite Fill Factor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem deals with an exact calculation of the intersection area of a\ncircle arbitrary placed on a grid of square shaped elements with gaps between\nthem (finite fill factor). Usually an approximation is used for the calculation\nof the intersection area of the circle and the squares of the grid. We analyze\nthe geometry of the problem and derive an algorithm for the exact computation\nof the intersection areas. The results of the analysis are summarized in the\ntally sheet. In a real world example this might be a CCD or CMOS chip, or the\ntile structure of a floor.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 18:15:13 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Gendler", "Dmitrij", ""], ["Eisele", "Christian", ""], ["Seiffer", "Dirk", ""], ["Wendelstein", "Norbert", ""]]}, {"id": "1812.10592", "submitter": "Robert Ravier", "authors": "Robert J. Ravier", "title": "Eyes on the Prize: Improved Biological Surface Registration via Forward\n  Propagation", "comments": "20 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for surface registration risk producing significant errors if\nsurfaces are significantly nonisometric. Manifold learning has been shown to be\neffective at improving registration quality, using information from an entire\ncollection of surfaces to correct issues present in pairwise registrations.\nThese methods, however, are not robust to changes in the collection of\nsurfaces, or do not produce accurate registrations at a resolution high enough\nfor subsequent downstream analysis. We propose a novel algorithm for\nefficiently registering such collections given initial correspondences with\nvarying degrees of accuracy. By combining the initial information with recent\ndevelopments in manifold learning, we employ a simple metric condition to\nconstruct a measure on the space of correspondences between any pair of shapes\nin our collection, which we then use to distill soft correspondences. We\ndemonstrate that this measure can improve correspondence accuracy between\nfeature points compared to currently employed, less robust methods on a diverse\ndataset of surfaces from evolutionary biology. We then show how our methods can\nbe used, in combination with recent sampling and interpolation methods, to\ncompute accurate and consistent homeomorphisms between surfaces.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 02:11:36 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 17:55:18 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Ravier", "Robert J.", ""]]}, {"id": "1812.10650", "submitter": "Wonbong Jang", "authors": "Wonbong Jang", "title": "Sampling Using Neural Networks for colorizing the grayscale images", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of this paper is to explore the possibilities of generating\nsamples from the neural networks, mostly focusing on the colorization of the\ngrey-scale images. I will compare the existing methods for colorization and\nexplore the possibilities of using new generative modeling to the task of\ncolorization. The contributions of this paper are to compare the existing\nstructures with similar generating structures(Decoders) and to apply the novel\nstructures including Conditional VAE(CVAE), Conditional Wasserstein GAN with\nGradient Penalty(CWGAN-GP), CWGAN-GP with L1 reconstruction loss, Adversarial\nGenerative Encoders(AGE) and Introspective VAE(IVAE). I trained these models\nusing CIFAR-10 images. To measure the performance, I use Inception Score(IS)\nwhich measures how distinctive each image is and how diverse overall samples\nare as well as human eyes for CIFAR-10 images. It turns out that CVAE with L1\nreconstruction loss and IVAE achieve the highest score in IS. CWGAN-GP with L1\ntends to learn faster than CWGAN-GP, but IS does not increase from CWGAN-GP.\nCWGAN-GP tends to generate more diverse images than other models using\nreconstruction loss. Also, I figured out that the proper regularization plays a\nvital role in generative modeling.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 07:51:45 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jang", "Wonbong", ""]]}, {"id": "1812.11339", "submitter": "Nieto Gregoire", "authors": "Gr\\'egoire Nieto (LJK), Fr\\'ed\\'eric Devernay (PRIMA), James Crowley\n  (PERVASIVE)", "title": "Rendu bas\\'e image avec contraintes sur les gradients", "comments": "in French. Traitement du Signal, Lavoisier, A para\\^itre", "journal-ref": null, "doi": "10.3166/HSP.x.1-26", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view image-based rendering consists in generating a novel view of a\nscene from a set of source views. In general, this works by first doing a\ncoarse 3D reconstruction of the scene, and then using this reconstruction to\nestablish correspondences between source and target views, followed by blending\nthe warped views to get the final image. Unfortunately, discontinuities in the\nblending weights, due to scene geometry or camera placement, result in\nartifacts in the target view. In this paper, we show how to avoid these\nartifacts by imposing additional constraints on the image gradients of the\nnovel view. We propose a variational framework in which an energy functional is\nderived and optimized by iteratively solving a linear system. We demonstrate\nthis method on several structured and unstructured multi-view datasets, and\nshow that it numerically outperforms state-of-the-art methods, and eliminates\nartifacts that result from visibility discontinuities\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 11:16:39 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Nieto", "Gr\u00e9goire", "", "LJK"], ["Devernay", "Fr\u00e9d\u00e9ric", "", "PRIMA"], ["Crowley", "James", "", "PERVASIVE"]]}]