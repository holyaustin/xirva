[{"id": "2005.00074", "submitter": "Alec Jacobson", "authors": "Sarah Kushner, Risa Ulinski, Karan Singh, David I.W. Levin, Alec\n  Jacobson", "title": "Levitating Rigid Objects with Hidden Rods and Wires", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm to efficiently generate hidden structures to\nsupport arrangements of floating rigid objects. Our optimization finds a small\nset of rods and wires between objects and each other or a supporting surface\n(e.g., wall or ceiling) that hold all objects in force and torque equilibrium.\nOur objective function includes a sparsity inducing total volume term and a\nlinear visibility term based on efficiently pre-computed Monte-Carlo\nintegration, to encourage solutions that are as-hidden-as-possible. The\nresulting optimization is convex and the global optimum can be efficiently\nrecovered via a linear program. Our representation allows for a\nuser-controllable mixture of tension-, compression-, and shear-resistant rods\nor tension-only wires. We explore applications to theatre set design, museum\nexhibit curation, and other artistic endeavours.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:48:57 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 00:25:05 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kushner", "Sarah", ""], ["Ulinski", "Risa", ""], ["Singh", "Karan", ""], ["Levin", "David I. W.", ""], ["Jacobson", "Alec", ""]]}, {"id": "2005.00202", "submitter": "John Evans", "authors": "Corey Wetterer-Nelson, Kenneth E. Jansen, John A. Evans", "title": "Interactive Geometry Modification of High Performance Finite Element\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of high performance finite element analysis, the cost of\niteratively modifying a computational domain via re-meshing and restarting the\nanalysis becomes time prohibitive as the size of simulations increases. In this\npaper, we demonstrate a new interactive simulation pipeline targeting high\nperformance finite element simulations where the computational domain is\nmodifiable in situ, that is, while the simulation is ongoing. This pipeline is\ndesigned to be modular so that it may interface with any existing finite\nelement simulation framework. A server-client architecture is employed to\nmanage simulation mesh data existing on a high performance computing resource\nwhile user-prescribed freeform geometric modifications take place on a separate\nworkstation. We employ existing in situ visualization techniques to rapidly\ninform the user of simulation progression, enabling computational steering. By\nexpressing the simulation domain in a reduced fashion on the client\napplication, this pipeline manages highly refined finite element simulation\ndomains on the server while maintaining good performance on the client\napplication.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:53:26 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 17:23:36 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wetterer-Nelson", "Corey", ""], ["Jansen", "Kenneth E.", ""], ["Evans", "John A.", ""]]}, {"id": "2005.00387", "submitter": "Ulrik G\\\"unther", "authors": "Ulrik G\\\"unther, Kyle I.S. Harrington, Raimund Dachselt, Ivo F.\n  Sbalzarini", "title": "Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual\n  Reality", "comments": "22 pages, 10 figures. Accepted at BioImageComputing workshop at ECCV\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present Bionic Tracking, a novel method for solving biological cell\ntracking problems with eye tracking in virtual reality using commodity\nhardware. Using gaze data, and especially smooth pursuit eye movements, we are\nable to track cells in time series of 3D volumetric datasets. The problem of\ntracking cells is ubiquitous in developmental biology, where large volumetric\nmicroscopy datasets are acquired on a daily basis, often comprising hundreds or\nthousands of time points that span hours or days. The image data, however, is\nonly a means to an end, and scientists are often interested in the\nreconstruction of cell trajectories and cell lineage trees. Reliably tracking\ncells in crowded three-dimensional space over many timepoints remains an open\nproblem, and many current approaches rely on tedious manual annotation and\ncuration. In our Bionic Tracking approach, we substitute the usual 2D\npoint-and-click annotation to track cells with eye tracking in a virtual\nreality headset, where users simply have to follow a cell with their eyes in 3D\nspace in order to track it. We detail the interaction design of our approach\nand explain the graph-based algorithm used to connect different time points,\nalso taking occlusion and user distraction into account. We demonstrate our\ncell tracking method using the example of two different biological datasets.\nFinally, we report on a user study with seven cell tracking experts,\ndemonstrating the benefits of our approach over manual point-and-click\ntracking.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:08:40 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 12:00:03 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["G\u00fcnther", "Ulrik", ""], ["Harrington", "Kyle I. S.", ""], ["Dachselt", "Raimund", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "2005.00554", "submitter": "David Coeurjolly", "authors": "Nicolas Bonneel and David Coeurjolly and Julie Digne and Nicolas\n  Mellado", "title": "Code Replicability in Computer Graphics", "comments": "8 pages. ACM Trans. on Graphics (Proceedings of SIGGRAPH 2020)", "journal-ref": null, "doi": "10.1145/3386569.3392413", "report-no": null, "categories": "cs.DL cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to duplicate published research results is an important process of\nconducting research whether to build upon these findings or to compare with\nthem. This process is called \"replicability\" when using the original authors'\nartifacts (e.g., code), or \"reproducibility\" otherwise (e.g., re-implementing\nalgorithms). Reproducibility and replicability of research results have gained\na lot of interest recently with assessment studies being led in various fields,\nand they are often seen as a trigger for better result diffusion and\ntransparency. In this work, we assess replicability in Computer Graphics, by\nevaluating whether the code is available and whether it works properly. As a\nproxy for this field we compiled, ran and analyzed 151 codes out of 374 papers\nfrom 2014, 2016 and 2018 SIGGRAPH conferences. This analysis shows a clear\nincrease in the number of papers with available and operational research codes\nwith a dependency on the subfields, and indicates a correlation between code\nreplicability and citation count. We further provide an interactive tool to\nexplore our results and evaluation data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:03:13 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 11:44:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Bonneel", "Nicolas", ""], ["Coeurjolly", "David", ""], ["Digne", "Julie", ""], ["Mellado", "Nicolas", ""]]}, {"id": "2005.00559", "submitter": "Zhan Xu", "authors": "Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Chris Landreth and Karan\n  Singh", "title": "RigNet: Neural Rigging for Articulated Characters", "comments": "SIGGRAPH 2020. Project page https://zhan-xu.github.io/rig-net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RigNet, an end-to-end automated method for producing animation\nrigs from input character models. Given an input 3D model representing an\narticulated character, RigNet predicts a skeleton that matches the animator\nexpectations in joint placement and topology. It also estimates surface skin\nweights based on the predicted skeleton. Our method is based on a deep\narchitecture that directly operates on the mesh representation without making\nassumptions on shape class and structure. The architecture is trained on a\nlarge and diverse collection of rigged models, including their mesh, skeletons\nand corresponding skin weights. Our evaluation is three-fold: we show better\nresults than prior art when quantitatively compared to animator rigs;\nqualitatively we show that our rigs can be expressively posed and animated at\nmultiple levels of detail; and finally, we evaluate the impact of various\nalgorithm choices on our output rigs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 18:12:44 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 19:38:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Xu", "Zhan", ""], ["Zhou", "Yang", ""], ["Kalogerakis", "Evangelos", ""], ["Landreth", "Chris", ""], ["Singh", "Karan", ""]]}, {"id": "2005.00803", "submitter": "Byungsoo Kim", "authors": "Byungsoo Kim, Vinicius C. Azevedo, Markus Gross, Barbara Solenthaler", "title": "Lagrangian Neural Style Transfer for Fluids", "comments": "ACM Transaction on Graphics (SIGGRAPH 2020), additional materials:\n  http://www.byungsoo.me/project/lnst/index.html", "journal-ref": "ACM Trans. Graph. 39, 4, Article 1 (July 2020), 10 pages", "doi": "10.1145/3386569.3392473", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistically controlling the shape, motion and appearance of fluid\nsimulations pose major challenges in visual effects production. In this paper,\nwe present a neural style transfer approach from images to 3D fluids formulated\nin a Lagrangian viewpoint. Using particles for style transfer has unique\nbenefits compared to grid-based techniques. Attributes are stored on the\nparticles and hence are trivially transported by the particle motion. This\nintrinsically ensures temporal consistency of the optimized stylized structure\nand notably improves the resulting quality. Simultaneously, the expensive,\nrecursive alignment of stylization velocity fields of grid approaches is\nunnecessary, reducing the computation time to less than an hour and rendering\nneural flow stylization practical in production settings. Moreover, the\nLagrangian representation improves artistic control as it allows for\nmulti-fluid stylization and consistent color transfer from images, and the\ngenerality of the method enables stylization of smoke and liquids likewise.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 11:53:05 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Gross", "Markus", ""], ["Solenthaler", "Barbara", ""]]}, {"id": "2005.01003", "submitter": "Martin Skrodzki", "authors": "Martin Skrodzki and Eric Zimmermann and Konrad Polthier", "title": "Variational Shape Approximation of Point Set Surfaces", "comments": "Corrected two formulae in the \"merge\" process, fixed dated that the\n  preprint was submitted", "journal-ref": "Computer Aided Geometric Design Volume 80, June 2020, 101875", "doi": "10.1016/j.cagd.2020.101875", "report-no": "RIKEN-iTHEMS-Report-20", "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a translation of the complete pipeline for\nvariational shape approximation (VSA) to the setting of point sets. First, we\ndescribe an explicit example for the theoretically known non-convergence of the\ncurrently available VSA approaches. The example motivates us to introduce an\nalternate version of VSA based on a switch operation for which we prove\nconvergence. Second, we discuss how two operations - split and merge - can be\nincluded in a fully automatic pipeline that is in turn independent of the\nplacement and number of initial seeds. Third and finally, we present two\napproaches how to obtain a simplified mesh from the output of the VSA\nprocedure. This simplification is either based on simple plane intersection or\nbased on a variational optimization problem. Several qualitative and\nquantitative results prove the relevance of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 06:44:27 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 05:58:57 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Skrodzki", "Martin", ""], ["Zimmermann", "Eric", ""], ["Polthier", "Konrad", ""]]}, {"id": "2005.01819", "submitter": "Hsueh-Ti Derek Liu", "authors": "Hsueh-Ti Derek Liu, Vladimir G. Kim, Siddhartha Chaudhuri, Noam\n  Aigerman, Alec Jacobson", "title": "Neural Subdivision", "comments": "16 pages", "journal-ref": "ACM Trans. Graph. 39, 4, (July 2020)", "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Neural Subdivision, a novel framework for data-driven\ncoarse-to-fine geometry modeling. During inference, our method takes a coarse\ntriangle mesh as input and recursively subdivides it to a finer geometry by\napplying the fixed topological updates of Loop Subdivision, but predicting\nvertex positions using a neural network conditioned on the local geometry of a\npatch. This approach enables us to learn complex non-linear subdivision\nschemes, beyond simple linear averaging used in classical techniques. One of\nour key contributions is a novel self-supervised training setup that only\nrequires a set of high-resolution meshes for learning network weights. For any\ntraining shape, we stochastically generate diverse low-resolution\ndiscretizations of coarse counterparts, while maintaining a bijective mapping\nthat prescribes the exact target position of every new vertex during the\nsubdivision process. This leads to a very efficient and accurate loss function\nfor conditional mesh generation, and enables us to train a method that\ngeneralizes across discretizations and favors preserving the manifold structure\nof the output. During training we optimize for the same set of network weights\nacross all local mesh patches, thus providing an architecture that is not\nconstrained to a specific input mesh, fixed genus, or category. Our network\nencodes patch geometry in a local frame in a rotation- and\ntranslation-invariant manner. Jointly, these design choices enable our method\nto generalize well, and we demonstrate that even when trained on a single\nhigh-resolution mesh our method generates reasonable subdivisions for novel\nshapes.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:03:21 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Liu", "Hsueh-Ti Derek", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Aigerman", "Noam", ""], ["Jacobson", "Alec", ""]]}, {"id": "2005.01878", "submitter": "Sorour Mohajerani", "authors": "Sorour Mohajerani, Mark S. Drew, Parvaneh Saeedi", "title": "Illumination-Invariant Image from 4-Channel Images: The Effect of\n  Near-Infrared Data in Shadow Removal", "comments": "Accepted for oral presentation in London Imaging Meeting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing the effect of illumination variation in images has been proved to be\nbeneficial in many computer vision applications such as object recognition and\nsemantic segmentation. Although generating illumination-invariant images has\nbeen studied in the literature before, it has not been investigated on real\n4-channel (4D) data. In this study, we examine the quality of\nillumination-invariant images generated from red, green, blue, and\nnear-infrared (RGBN) data. Our experiments show that the near-infrared channel\nsubstantively contributes toward removing illumination. As shown in our\nnumerical and visual results, the illumination-invariant image obtained by RGBN\ndata is superior compared to that obtained by RGB alone.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 22:51:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Mohajerani", "Sorour", ""], ["Drew", "Mark S.", ""], ["Saeedi", "Parvaneh", ""]]}, {"id": "2005.02152", "submitter": "Jaya Sreevalsan-Nair", "authors": "Jaya Sreevalsan-Nair and Pragyan Mohapatra", "title": "Augmented Semantic Signatures of Airborne LiDAR Point Clouds for\n  Comparison", "comments": "18 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  LiDAR point clouds provide rich geometric information, which is particularly\nuseful for the analysis of complex scenes of urban regions. Finding structural\nand semantic differences between two different three-dimensional point clouds,\nsay, of the same region but acquired at different time instances is an\nimportant problem. A comparison of point clouds involves computationally\nexpensive registration and segmentation. We are interested in capturing the\nrelative differences in the geometric uncertainty and semantic content of the\npoint cloud without the registration process. Hence, we propose an\norientation-invariant geometric signature of the point cloud, which integrates\nits probabilistic geometric and semantic classifications. We study different\nproperties of the geometric signature, which are an image-based encoding of\ngeometric uncertainty and semantic content. We explore different metrics to\ndetermine differences between these signatures, which in turn compare point\nclouds without performing point-to-point registration. Our results show that\nthe differences in the signatures corroborate with the geometric and semantic\ndifferences of the point clouds.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:27:07 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 04:32:43 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sreevalsan-Nair", "Jaya", ""], ["Mohapatra", "Pragyan", ""]]}, {"id": "2005.02186", "submitter": "Jingyi Shen", "authors": "Jingyi Shen, Han-Wei Shen", "title": "An Information-theoretic Visual Analysis Framework for Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the great success of Convolutional Neural Networks (CNNs) in Computer\nVision and Natural Language Processing, the working mechanism behind CNNs is\nstill under extensive discussions and research. Driven by a strong demand for\nthe theoretical explanation of neural networks, some researchers utilize\ninformation theory to provide insight into the black box model. However, to the\nbest of our knowledge, employing information theory to quantitatively analyze\nand qualitatively visualize neural networks has not been extensively studied in\nthe visualization community. In this paper, we combine information entropies\nand visualization techniques to shed light on how CNN works. Specifically, we\nfirst introduce a data model to organize the data that can be extracted from\nCNN models. Then we propose two ways to calculate entropy under different\ncircumstances. To provide a fundamental understanding of the basic building\nblocks of CNNs (e.g., convolutional layers, pooling layers, normalization\nlayers) from an information-theoretic perspective, we develop a visual analysis\nsystem, CNNSlicer. CNNSlicer allows users to interactively explore the amount\nof information changes inside the model. With case studies on the widely used\nbenchmark datasets (MNIST and CIFAR-10), we demonstrate the effectiveness of\nour system in opening the blackbox of CNNs.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 21:36:50 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Shen", "Jingyi", ""], ["Shen", "Han-Wei", ""]]}, {"id": "2005.02189", "submitter": "Saad Alqithami", "authors": "Saad Alqithami, Musaad Alzahrani, Abdulkareem Alzahrani and Ahmed\n  Mustafa", "title": "AR-Therapist: Design and Simulation of an AR-Game Environment as a CBT\n  for Patients with ADHD", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.01003", "journal-ref": "Healthcare 2019, 7, 146", "doi": "10.3390/healthcare7040146", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention Deficit Hyperactivity Disorder is one of the most common\nneurodevelopmental disorders in which patients have difficulties related to\ninattention, hyperactivity, and impulsivity. Those patients are in need of a\npsychological therapy use Cognitive Behavioral Therapy (CBT) to enhance the way\nthey think and behave. This type of therapy is mostly common in treating\npatients with anxiety and depression but also is useful in treating autism,\nobsessive compulsive disorder and post-traumatic stress disorder. A major\nlimitation of traditional CBT is that therapists may face difficulty in\noptimizing patients' neuropsychological stimulus following a specified\ntreatment plan. Other limitations include availability, accessibility and\nlevel-of-experience of the therapists. Hence, this paper aims to design and\nsimulate a generic cognitive model that can be used as an appropriate\nalternative treatment to traditional CBT, we term as \"AR-Therapist.\" This model\ntakes advantage of the current developments of augmented reality to engage\npatients in both real and virtual game-based environments.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:51:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Alqithami", "Saad", ""], ["Alzahrani", "Musaad", ""], ["Alzahrani", "Abdulkareem", ""], ["Mustafa", "Ahmed", ""]]}, {"id": "2005.03011", "submitter": "Mohamed ElHelw", "authors": "Mohamed A. ElHelw", "title": "Overview of Surgical Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the current demand of clinical governance, surgical simulation\nis now a well-established modality for basic skills training and assessment.\nThe practical deployment of the technique is a multi-disciplinary venture\nencompassing areas in engineering, medicine and psychology. This paper provides\nan overview of the key topics involved in surgical simulation and associated\ntechnical challenges. The paper discusses the clinical motivation for surgical\nsimulation, the use of virtual environments for surgical training, model\nacquisition and simplification, deformable models, collision detection, tissue\nproperty measurement, haptic rendering and image synthesis. Additional topics\ninclude surgical skill training and assessment metrics as well as challenges\nfacing the incorporation of surgical simulation into medical education\ncurricula.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 11:51:31 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["ElHelw", "Mohamed A.", ""]]}, {"id": "2005.03288", "submitter": "Trista Chen", "authors": "Ying-Sheng Luo (1), Jonathan Hans Soeseno (1), Trista Pei-Chun Chen\n  (1), Wei-Chao Chen (1, 2) ((1) Inventec Corp. (2) Skywatch Innovation Inc.)", "title": "CARL: Controllable Agent with Reinforcement Learning for Quadruped\n  Locomotion", "comments": "Project page available at\n  https://inventec-ai-center.github.io/projects/CARL/index.html", "journal-ref": "ACM Transactions on Graphics (2020), Volume 39, Issue 4, Article\n  38", "doi": "10.1145/3386569.3392433", "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion synthesis in a dynamic environment has been a long-standing problem\nfor character animation. Methods using motion capture data tend to scale poorly\nin complex environments because of their larger capturing and labeling\nrequirement. Physics-based controllers are effective in this regard, albeit\nless controllable. In this paper, we present CARL, a quadruped agent that can\nbe controlled with high-level directives and react naturally to dynamic\nenvironments. Starting with an agent that can imitate individual animation\nclips, we use Generative Adversarial Networks to adapt high-level controls,\nsuch as speed and heading, to action distributions that correspond to the\noriginal animations. Further fine-tuning through the deep reinforcement\nlearning enables the agent to recover from unseen external perturbations while\nproducing smooth transitions. It then becomes straightforward to create\nautonomous agents in dynamic environments by adding navigation modules over the\nentire process. We evaluate our approach by measuring the agent's ability to\nfollow user control and provide a visual analysis of the generated motion to\nshow its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:18:57 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 03:20:42 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 05:10:27 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Luo", "Ying-Sheng", "", "Inventec Corp"], ["Soeseno", "Jonathan Hans", "", "Inventec Corp"], ["Chen", "Trista Pei-Chun", "", "Inventec Corp"], ["Chen", "Wei-Chao", "", "Inventec Corp"]]}, {"id": "2005.03372", "submitter": "Peng Wang", "authors": "Peng Wang, Lingjie Liu, Nenglun Chen, Hung-Kuo Chu, Christian\n  Theobalt, Wenping Wang", "title": "Vid2Curve: Simultaneous Camera Motion Estimation and Thin Structure\n  Reconstruction from an RGB Video", "comments": "Accepted by SIGGRAPH 2020", "journal-ref": null, "doi": "10.1145/3386569.3392476", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thin structures, such as wire-frame sculptures, fences, cables, power lines,\nand tree branches, are common in the real world. It is extremely challenging to\nacquire their 3D digital models using traditional image-based or depth-based\nreconstruction methods because thin structures often lack distinct point\nfeatures and have severe self-occlusion. We propose the first approach that\nsimultaneously estimates camera motion and reconstructs the geometry of complex\n3D thin structures in high quality from a color video captured by a handheld\ncamera. Specifically, we present a new curve-based approach to estimate\naccurate camera poses by establishing correspondences between featureless thin\nobjects in the foreground in consecutive video frames, without requiring visual\ntexture in the background scene to lock on. Enabled by this effective\ncurve-based camera pose estimation strategy, we develop an iterative\noptimization method with tailored measures on geometry, topology as well as\nself-occlusion handling for reconstructing 3D thin structures. Extensive\nvalidations on a variety of thin structures show that our method achieves\naccurate camera pose estimation and faithful reconstruction of 3D thin\nstructures with complex shape and topology at a level that has not been\nattained by other existing reconstruction methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 10:39:20 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:48:19 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 04:57:24 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingjie", ""], ["Chen", "Nenglun", ""], ["Chu", "Hung-Kuo", ""], ["Theobalt", "Christian", ""], ["Wang", "Wenping", ""]]}, {"id": "2005.04065", "submitter": "David Schedl", "authors": "Indrajit Kurmi and David C. Schedl and Oliver Bimber", "title": "Fast Automatic Visibility Optimization for Thermal Synthetic Aperture\n  Visualization", "comments": "5 pages, 4 figures, 1 table, in IEEE Geoscience and Remote Sensing\n  Letters, 2020", "journal-ref": null, "doi": "10.1109/LGRS.2020.2987471", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe and validate the first fully automatic parameter\noptimization for thermal synthetic aperture visualization. It replaces previous\nmanual exploration of the parameter space, which is time consuming and error\nprone. We prove that the visibility of targets in thermal integral images is\nproportional to the variance of the targets' image. Since this is invariant to\nocclusion it represents a suitable objective function for optimization. Our\nfindings have the potential to enable fully autonomous search and recuse\noperations with camera drones.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:28:03 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "2005.04107", "submitter": "Yuki Koyama", "authors": "Yuki Koyama, Issei Sato, Masataka Goto", "title": "Sequential Gallery for Interactive Visual Design Optimization", "comments": "To be published at ACM Trans. Graph. (Proc. SIGGRAPH 2020); Project\n  page available at https://koyama.xyz/project/sequential_gallery/", "journal-ref": "ACM Trans. Graph. 39, 4 (July 2020), pp.88:1-88:12", "doi": "10.1145/3386569.3392444", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual design tasks often involve tuning many design parameters. For example,\ncolor grading of a photograph involves many parameters, some of which\nnon-expert users might be unfamiliar with. We propose a novel user-in-the-loop\noptimization method that allows users to efficiently find an appropriate\nparameter set by exploring such a high-dimensional design space through much\neasier two-dimensional search subtasks. This method, called sequential plane\nsearch, is based on Bayesian optimization to keep necessary queries to users as\nfew as possible. To help users respond to plane-search queries, we also propose\nusing a gallery-based interface that provides options in the two-dimensional\nsubspace arranged in an adaptive grid view. We call this interactive framework\nSequential Gallery since users sequentially select the best option from the\noptions provided by the interface. Our experiment with synthetic functions\nshows that our sequential plane search can find satisfactory solutions in fewer\niterations than baselines. We also conducted a preliminary user study, results\nof which suggest that novices can effectively complete search tasks with\nSequential Gallery in a photo-enhancement scenario.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:24:35 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Koyama", "Yuki", ""], ["Sato", "Issei", ""], ["Goto", "Masataka", ""]]}, {"id": "2005.04323", "submitter": "Zhaoming Xie", "authors": "Zhaoming Xie, Hung Yu Ling, Nam Hee Kim, Michiel van de Panne", "title": "ALLSTEPS: Curriculum-driven Learning of Stepping Stone Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are highly adept at walking in environments with foot placement\nconstraints, including stepping-stone scenarios where the footstep locations\nare fully constrained. Finding good solutions to stepping-stone locomotion is a\nlongstanding and fundamental challenge for animation and robotics. We present\nfully learned solutions to this difficult problem using reinforcement learning.\nWe demonstrate the importance of a curriculum for efficient learning and\nevaluate four possible curriculum choices compared to a non-curriculum\nbaseline. Results are presented for a simulated human character, a realistic\nbipedal robot simulation and a monster character, in each case producing\nrobust, plausible motions for challenging stepping stone sequences and\nterrains.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 00:16:38 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 00:45:00 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Xie", "Zhaoming", ""], ["Ling", "Hung Yu", ""], ["Kim", "Nam Hee", ""], ["van de Panne", "Michiel", ""]]}, {"id": "2005.04464", "submitter": "Yanran Guan", "authors": "Yanran Guan, Han Liu, Kun Liu, Kangxue Yin, Ruizhen Hu, Oliver van\n  Kaick, Yan Zhang, Ersin Yumer, Nathan Carr, Radomir Mech, Hao Zhang", "title": "FAME: 3D Shape Generation via Functionality-Aware Model Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a modeling tool which can evolve a set of 3D objects in a\nfunctionality-aware manner. Our goal is for the evolution to generate large and\ndiverse sets of plausible 3D objects for data augmentation, constrained\nmodeling, as well as open-ended exploration to possibly inspire new designs.\nStarting with an initial population of 3D objects belonging to one or more\nfunctional categories, we evolve the shapes through part recombination to\nproduce generations of hybrids or crossbreeds between parents from the\nheterogeneous shape collection. Evolutionary selection of offsprings is guided\nboth by a functional plausibility score derived from functionality analysis of\nshapes in the initial population and user preference, as in a design gallery.\nSince cross-category hybridization may result in offsprings not belonging to\nany of the known functional categories, we develop a means for functionality\npartial matching to evaluate functional plausibility on partial shapes. We show\na variety of plausible hybrid shapes generated by our functionality-aware model\nevolution, which can complement existing datasets as training data and boost\nthe performance of contemporary data-driven segmentation schemes, especially in\nchallenging cases. Our tool supports constrained modeling, allowing users to\nrestrict or steer the model evolution with functionality labels. At the same\ntime, unexpected yet functional object prototypes can emerge during open-ended\nexploration owing to structure breaking when evolving a heterogeneous\ncollection.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 15:30:28 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 02:19:12 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 18:24:24 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Guan", "Yanran", ""], ["Liu", "Han", ""], ["Liu", "Kun", ""], ["Yin", "Kangxue", ""], ["Hu", "Ruizhen", ""], ["van Kaick", "Oliver", ""], ["Zhang", "Yan", ""], ["Yumer", "Ersin", ""], ["Carr", "Nathan", ""], ["Mech", "Radomir", ""], ["Zhang", "Hao", ""]]}, {"id": "2005.05386", "submitter": "Tiago Novello", "authors": "Tiago Novello, Vin\\'icius da Silva, and Luiz Velho", "title": "Design and visualization of Riemannian metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local and global illumination were recently defined in Riemannian manifolds\nto visualize classical Non-Euclidean spaces. This work focuses on Riemannian\nmetric construction in $\\mathbb{R}^3$ to explore special effects like warping,\nmirages, and deformations. We investigate the possibility of using graphs of\nfunctions and diffeomorphism to produce such effects. For these, their\nRiemannian metrics and geodesics derivations are provided, and ways of\naccumulating such metrics. We visualize, in \"real-time\", the resulting\nRiemannian manifolds using a ray tracing implemented on top of Nvidia RTX GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 19:07:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Novello", "Tiago", ""], ["da Silva", "Vin\u00edcius", ""], ["Velho", "Luiz", ""]]}, {"id": "2005.05406", "submitter": "Majid Masoumi", "authors": "Majid Masoumi, Marcel Marcoux, Laurence Maignel, Candido Pomar", "title": "SpectralWeight: a spectral graph wavelet framework for weight prediction\n  of pork cuts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for the quality assessment of pork\ncarcasses using 3D shape analysis. First, we make a 3D model of a pork\nhalf-carcass using a 3D scanner and then we take advantage of spectral graph\nwavelet signature (SGWS) to build a local spectral descriptor. Next, we\naggregate the extracted features using the bag-of-geometric-words paradigm to\nglobally represent the half-carcass shape. We then employ partial least-squares\nregression to predict the weight of pork cuts for the quality assessment of\ncarcasses. Our results demonstrate that SpectralWeight can predict the weight\nof different pork cuts and tissues with high accuracy. Although in this study\nwe evaluate the performance of SGWS for the weight prediction of pork\ndissection, our framework is fairly general and enables new ways to estimate\nthe quality and economical value of carcasses of different animals.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 00:57:21 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Masoumi", "Majid", ""], ["Marcoux", "Marcel", ""], ["Maignel", "Laurence", ""], ["Pomar", "Candido", ""]]}, {"id": "2005.05732", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Peizhuo Li, Dani Lischinski, Olga Sorkine-Hornung,\n  Daniel Cohen-Or, Baoquan Chen", "title": "Skeleton-Aware Networks for Deep Motion Retargeting", "comments": "SIGGRAPH 2020. Project page:\n  https://deepmotionediting.github.io/retargeting , Video:\n  https://www.youtube.com/watch?v=ym8Tnmiz5N8", "journal-ref": null, "doi": "10.1145/3386569.3392462", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel deep learning framework for data-driven motion\nretargeting between skeletons, which may have different structure, yet\ncorresponding to homeomorphic graphs. Importantly, our approach learns how to\nretarget without requiring any explicit pairing between the motions in the\ntraining set. We leverage the fact that different homeomorphic skeletons may be\nreduced to a common primal skeleton by a sequence of edge merging operations,\nwhich we refer to as skeletal pooling. Thus, our main technical contribution is\nthe introduction of novel differentiable convolution, pooling, and unpooling\noperators. These operators are skeleton-aware, meaning that they explicitly\naccount for the skeleton's hierarchical structure and joint adjacency, and\ntogether they serve to transform the original motion into a collection of deep\ntemporal features associated with the joints of the primal skeleton. In other\nwords, our operators form the building blocks of a new deep motion processing\nframework that embeds the motion into a common latent space, shared by a\ncollection of homeomorphic skeletons. Thus, retargeting can be achieved simply\nby encoding to, and decoding from this latent space. Our experiments show the\neffectiveness of our framework for motion retargeting, as well as motion\nprocessing in general, compared to existing approaches. Our approach is also\nquantitatively evaluated on a synthetic dataset that contains pairs of motions\napplied to different skeletons. To the best of our knowledge, our method is the\nfirst to perform retargeting between skeletons with differently sampled\nkinematic chains, without any paired examples.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 12:51:40 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aberman", "Kfir", ""], ["Li", "Peizhuo", ""], ["Lischinski", "Dani", ""], ["Sorkine-Hornung", "Olga", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2005.05751", "submitter": "Kfir Aberman", "authors": "Kfir Aberman, Yijia Weng, Dani Lischinski, Daniel Cohen-Or, Baoquan\n  Chen", "title": "Unpaired Motion Style Transfer from Video to Animation", "comments": "SIGGRAPH 2020. Project page:\n  https://deepmotionediting.github.io/style_transfer , Video:\n  https://www.youtube.com/watch?v=m04zuBSdGrc , Code:\n  https://github.com/DeepMotionEditing/deep-motion-editing", "journal-ref": null, "doi": "10.1145/3386569.3392469", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring the motion style from one animation clip to another, while\npreserving the motion content of the latter, has been a long-standing problem\nin character animation. Most existing data-driven approaches are supervised and\nrely on paired data, where motions with the same content are performed in\ndifferent styles. In addition, these approaches are limited to transfer of\nstyles that were seen during training. In this paper, we present a novel\ndata-driven framework for motion style transfer, which learns from an unpaired\ncollection of motions with style labels, and enables transferring motion styles\nnot observed during training. Furthermore, our framework is able to extract\nmotion styles directly from videos, bypassing 3D reconstruction, and apply them\nto the 3D input motion. Our style transfer network encodes motions into two\nlatent codes, for content and for style, each of which plays a different role\nin the decoding (synthesis) process. While the content code is decoded into the\noutput motion by several temporal convolutional layers, the style code modifies\ndeep features via temporally invariant adaptive instance normalization (AdaIN).\nMoreover, while the content code is encoded from 3D joint rotations, we learn a\ncommon embedding for style from either 3D or 2D joint positions, enabling style\nextraction from videos. Our results are comparable to the state-of-the-art,\ndespite not requiring paired training data, and outperform other methods when\ntransferring previously unseen styles. To our knowledge, we are the first to\ndemonstrate style transfer directly from videos to 3D animations - an ability\nwhich enables one to extend the set of style examples far beyond motions\ncaptured by MoCap systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:21:27 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Aberman", "Kfir", ""], ["Weng", "Yijia", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Chen", "Baoquan", ""]]}, {"id": "2005.06278", "submitter": "Hadi Abdi Khojasteh", "authors": "Hadi Abdi Khojasteh", "title": "A Survey on Patch-based Synthesis: GPU Implementation and Optimization", "comments": "117 pages, 38 figures, in Persian", "journal-ref": null, "doi": "10.13140/RG.2.2.29490.86729/1", "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis surveys the research in patch-based synthesis and algorithms for\nfinding correspondences between small local regions of images. We additionally\nexplore a large kind of applications of this new fast randomized matching\ntechnique. One of the algorithms we have studied in particular is PatchMatch,\ncan find similar regions or \"patches\" of an image one to two orders of\nmagnitude faster than previous techniques. The algorithmic program is driven by\napplying mathematical properties of nearest neighbors in natural images. It is\nobserved that neighboring correspondences tend to be similar or \"coherent\" and\nuse this observation in algorithm in order to quickly converge to an\napproximate solution. The algorithm is the most general form can find k-nearest\nneighbor matching, using patches that translate, rotate, or scale, using\narbitrary descriptors, and between two or more images. Speed-ups are obtained\nover various techniques in an exceeding range of those areas. We have explored\nmany applications of PatchMatch matching algorithm. In computer graphics, we\nhave explored removing unwanted objects from images, seamlessly moving objects\nin images, changing image aspect ratios, and video summarization. In computer\nvision we have explored denoising images, object detection, detecting image\nforgeries, and detecting symmetries. We conclude by discussing the restrictions\nof our algorithmic program, GPU implementation and areas for future analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 19:25:28 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Khojasteh", "Hadi Abdi", ""]]}, {"id": "2005.06469", "submitter": "Erich Bremer", "authors": "Erich Bremer, Jonas Almeida, Joel Saltz", "title": "Representing Whole Slide Cancer Image Features with Hilbert Curves", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DB cs.IR q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regions of Interest (ROI) contain morphological features in pathology whole\nslide images (WSI) are delimited with polygons[1]. These polygons are often\nrepresented in either a textual notation (with the array of edges) or in a\nbinary mask form. Textual notations have an advantage of human readability and\nportability, whereas, binary mask representations are more useful as the input\nand output of feature-extraction pipelines that employ deep learning\nmethodologies. For any given whole slide image, more than a million cellular\nfeatures can be segmented generating a corresponding number of polygons. The\ncorpus of these segmentations for all processed whole slide images creates\nvarious challenges for filtering specific areas of data for use in interactive\nreal-time and multi-scale displays and analysis. Simple range queries of image\nlocations do not scale and, instead, spatial indexing schemes are required. In\nthis paper we propose using Hilbert Curves simultaneously for spatial indexing\nand as a polygonal ROI representation. This is achieved by using a series of\nHilbert Curves[2] creating an efficient and inherently spatially-indexed\nmachine-usable form. The distinctive property of Hilbert curves that enables\nboth mask and polygon delimitation of ROIs is that the elements of the vector\nextracted ro describe morphological features maintain their relative positions\nfor different scales of the same image.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:38:24 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Bremer", "Erich", ""], ["Almeida", "Jonas", ""], ["Saltz", "Joel", ""]]}, {"id": "2005.06671", "submitter": "Dawoon Jung", "authors": "Dawoon Jung (1), Fridger Schrempp (2) and Seunghee Son (1) ((1) Korea\n  Aerospace Research Institute (KARI), Daejeon, Korea, (2) Deutsches\n  Elektronen-Synchrotron (DESY), Hamburg, Germany)", "title": "Optimally Fast Soft Shadows on Curved Terrain with Dynamic Programming\n  and Maximum Mipmaps", "comments": "16 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, novel method of efficiently rendering ray cast soft\nshadows on curved terrain by using dynamic programming and maximum mipmaps to\nrapidly find a global minimum shadow cost in constant runtime complexity.\nAdditionally, we apply a new method of reducing view ray computation times that\npre-displaces the terrain mesh to bootstrap ray starting positions. Combining\nthese two methods, our ray casting engine runs in real-time with more than 200%\nspeed up over uniform ray stepping with comparable image quality and without\nhardware ray tracing acceleration. To add support for accurate planetary\nephemerides and interactive features, we integrated the engine into\ncelestia.Sci, a general space simulation software. We demonstrate the ability\nof our engine to accurately handle a large range of distance scales by using it\nto generate videos of lunar landing trajectories. The numerical error when\ncompared with real lunar mission imagery is small, demonstrating the accuracy\nand efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 00:06:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Jung", "Dawoon", ""], ["Schrempp", "Fridger", ""], ["Son", "Seunghee", ""]]}, {"id": "2005.06998", "submitter": "Jeremy Youngquist", "authors": "Jeremy Youngquist and J\\\"org Peters and Meera Sitharam", "title": "Plane-Activated Mapped Microstructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Querying and interacting with models of massive material micro-structure\nrequires localized on-demand generation of the micro-structure since the\nfull-scale storing and retrieving is cost prohibitive. When the micro-structure\nis efficiently represented as the image of a canonical structure under a\nnon-linear space deformation to allow it to conform to curved shape, the\nadditional challenge is to relate the query of the mapped micro-structure back\nto its canonical structure. This paper presents an efficient algorithm to pull\nback a mapped micro-structure to a partition of the canonical domain structure\ninto boxes and only activates boxes whose image is likely intersected by a\nplane. The active boxes are organized into a forest whose trees are traversed\ndepth first to generate mapped micro-structure only of the active boxes. The\ntraversal supports, for example, 3D print slice generation in additive\nmanufacturing.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 14:20:42 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Youngquist", "Jeremy", ""], ["Peters", "J\u00f6rg", ""], ["Sitharam", "Meera", ""]]}, {"id": "2005.07335", "submitter": "Marcel Santos", "authors": "Marcel Santana Santos, Tsang Ing Ren, Nima Khademi Kalantari", "title": "Single Image HDR Reconstruction Using a CNN with Masked Features and\n  Perceptual Loss", "comments": "10 pages, 13 figures, to be published in ACM SIGGRAPH 2020. For\n  project page see http://faculty.cs.tamu.edu/nimak/Papers/SIGGRAPH2020_HDR/", "journal-ref": null, "doi": "10.1145/3386569.3392403", "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital cameras can only capture a limited range of real-world scenes'\nluminance, producing images with saturated pixels. Existing single image high\ndynamic range (HDR) reconstruction methods attempt to expand the range of\nluminance, but are not able to hallucinate plausible textures, producing\nresults with artifacts in the saturated areas. In this paper, we present a\nnovel learning-based approach to reconstruct an HDR image by recovering the\nsaturated pixels of an input LDR image in a visually pleasing way. Previous\ndeep learning-based methods apply the same convolutional filters on\nwell-exposed and saturated pixels, creating ambiguity during training and\nleading to checkerboard and halo artifacts. To overcome this problem, we\npropose a feature masking mechanism that reduces the contribution of the\nfeatures from the saturated areas. Moreover, we adapt the VGG-based perceptual\nloss function to our application to be able to synthesize visually pleasing\ntextures. Since the number of HDR images for training is limited, we propose to\ntrain our system in two stages. Specifically, we first train our system on a\nlarge number of images for image inpainting task and then fine-tune it on HDR\nreconstruction. Since most of the HDR examples contain smooth regions that are\nsimple to reconstruct, we propose a sampling strategy to select challenging\ntraining patches during the HDR fine-tuning stage. We demonstrate through\nexperimental results that our approach can reconstruct visually pleasing HDR\nresults, better than the current state of the art on a wide range of scenes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 03:13:44 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Santos", "Marcel Santana", ""], ["Ren", "Tsang Ing", ""], ["Kalantari", "Nima Khademi", ""]]}, {"id": "2005.07547", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Online path sampling control with progressive spatio-temporal filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces progressive spatio-temporal filtering, an efficient\nmethod to build all-frequency approximations to the light transport\ndistribution into a scene by filtering individual samples produced by an\nunderlying path sampler, using online, iterative algorithms and data-structures\nthat exploit both the spatial and temporal coherence of the approximated light\nfield. Unlike previous approaches, the proposed method is both more efficient,\ndue to its use of an iterative temporal feedback loop that massively improves\nconvergence to a noise-free approximant, and more flexible, due to its\nintroduction of a spatio-directional hashing representation that allows to\nencode directional variations like those due to glossy reflections. We then\nintroduce four different methods to employ the resulting approximations to\ncontrol the underlying path sampler and/or modify its associated estimator,\ngreatly reducing its variance and enhancing its robustness to complex lighting\nscenarios. The core algorithms are highly scalable and low-overhead, requiring\nonly minor modifications to an existing path tracer.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 13:52:41 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 08:24:14 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "2005.07702", "submitter": "Filip Andersson", "authors": "Filip Andersson, Simon Arvidsson", "title": "Generative Adversarial Networks for photo to Hayao Miyazaki style\n  cartoons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper takes on the problem of transferring the style of cartoon images\nto real-life photographic images by implementing previous work done by\nCartoonGAN. We trained a Generative Adversial Network(GAN) on over 60 000\nimages from works by Hayao Miyazaki at Studio Ghibli. To evaluate our results,\nwe conducted a qualitative survey comparing our results with two\nstate-of-the-art methods. 117 survey results indicated that our model on\naverage outranked state-of-the-art methods on cartoon-likeness.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 19:26:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Andersson", "Filip", ""], ["Arvidsson", "Simon", ""]]}, {"id": "2005.07727", "submitter": "David Bau iii", "authors": "David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou,\n  Jun-Yan Zhu, Antonio Torralba", "title": "Semantic Photo Manipulation with a Generative Image Prior", "comments": "SIGGRAPH 2019", "journal-ref": "ACM Transactions on Graphics (TOG) 38.4 (2019)", "doi": "10.1145/3306346.3323023", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of GANs in synthesizing images conditioned on\ninputs such as a user sketch, text, or semantic labels, manipulating the\nhigh-level attributes of an existing natural photograph with GANs is\nchallenging for two reasons. First, it is hard for GANs to precisely reproduce\nan input image. Second, after manipulation, the newly synthesized pixels often\ndo not fit the original image. In this paper, we address these issues by\nadapting the image prior learned by GANs to image statistics of an individual\nimage. Our method can accurately reconstruct the input image and synthesize new\ncontent, consistent with the appearance of the input image. We demonstrate our\ninteractive system on several semantic image editing tasks, including\nsynthesizing new objects consistent with background, removing unwanted objects,\nand changing the appearance of an object. Quantitative and qualitative\ncomparisons against several existing methods demonstrate the effectiveness of\nour method.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:22:05 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 19:53:55 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bau", "David", ""], ["Strobelt", "Hendrik", ""], ["Peebles", "William", ""], ["Wulff", "Jonas", ""], ["Zhou", "Bolei", ""], ["Zhu", "Jun-Yan", ""], ["Torralba", "Antonio", ""]]}, {"id": "2005.07728", "submitter": "Yotam Nitzan", "authors": "Yotam Nitzan, Amit Bermano, Yangyan Li, Daniel Cohen-Or", "title": "Face Identity Disentanglement via Latent Space Mapping", "comments": "23 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning disentangled representations of data is a fundamental problem in\nartificial intelligence. Specifically, disentangled latent representations\nallow generative models to control and compose the disentangled factors in the\nsynthesis process. Current methods, however, require extensive supervision and\ntraining, or instead, noticeably compromise quality. In this paper, we present\na method that learns how to represent data in a disentangled way, with minimal\nsupervision, manifested solely using available pre-trained networks. Our key\ninsight is to decouple the processes of disentanglement and synthesis, by\nemploying a leading pre-trained unconditional image generator, such as\nStyleGAN. By learning to map into its latent space, we leverage both its\nstate-of-the-art quality, and its rich and expressive latent space, without the\nburden of training it. We demonstrate our approach on the complex and high\ndimensional domain of human heads. We evaluate our method qualitatively and\nquantitatively, and exhibit its success with de-identification operations and\nwith temporal identity coherency in image sequences. Through extensive\nexperimentation, we show that our method successfully disentangles identity\nfrom other facial attributes, surpassing existing methods, even though they\nrequire more training and supervision.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 18:24:49 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 16:24:06 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 12:24:42 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Nitzan", "Yotam", ""], ["Bermano", "Amit", ""], ["Li", "Yangyan", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2005.07865", "submitter": "Yizhi Wang", "authors": "Yizhi Wang, Yue Gao, Zhouhui Lian", "title": "Attribute2Font: Creating Fonts You Want From Attributes", "comments": "SIGGRAPH 2020 techniqual paper; Wang and Gao contribute equally;\n  Code: https://hologerry.github.io/Attr2Font/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Font design is now still considered as an exclusive privilege of professional\ndesigners, whose creativity is not possessed by existing software systems.\nNevertheless, we also notice that most commercial font products are in fact\nmanually designed by following specific requirements on some attributes of\nglyphs, such as italic, serif, cursive, width, angularity, etc. Inspired by\nthis fact, we propose a novel model, Attribute2Font, to automatically create\nfonts by synthesizing visually-pleasing glyph images according to\nuser-specified attributes and their corresponding values. To the best of our\nknowledge, our model is the first one in the literature which is capable of\ngenerating glyph images in new font styles, instead of retrieving existing\nfonts, according to given values of specified font attributes. Specifically,\nAttribute2Font is trained to perform font style transfer between any two fonts\nconditioned on their attribute values. After training, our model can generate\nglyph images in accordance with an arbitrary set of font attribute values.\nFurthermore, a novel unit named Attribute Attention Module is designed to make\nthose generated glyph images better embody the prominent font attributes.\nConsidering that the annotations of font attribute values are extremely\nexpensive to obtain, a semi-supervised learning scheme is also introduced to\nexploit a large number of unlabeled fonts. Experimental results demonstrate\nthat our model achieves impressive performance on many tasks, such as creating\nglyph images in new font styles, editing existing fonts, interpolation among\ndifferent fonts, etc.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 04:06:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Yizhi", ""], ["Gao", "Yue", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2005.08000", "submitter": "Nikolaos Zioulis Mr.", "authors": "Vasileios Gkitsas (1) and Nikolaos Zioulis (1 and 2) and Federico\n  Alvarez (2) and Dimitrios Zarpalas (1) and Petros Daras (1) ((1) Centre for\n  Research and Technology Hellas, (2) Universidad Politecnica de Madrid)", "title": "Deep Lighting Environment Map Estimation from Spherical Panoramas", "comments": "Code and models available at\n  https://vcl3d.github.io/DeepPanoramaLighting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a scene's lighting is a very important task when compositing\nsynthetic content within real environments, with applications in mixed reality\nand post-production. In this work we present a data-driven model that estimates\nan HDR lighting environment map from a single LDR monocular spherical panorama.\nIn addition to being a challenging and ill-posed problem, the lighting\nestimation task also suffers from a lack of facile illumination ground truth\ndata, a fact that hinders the applicability of data-driven methods. We approach\nthis problem differently, exploiting the availability of surface geometry to\nemploy image-based relighting as a data generator and supervision mechanism.\nThis relies on a global Lambertian assumption that helps us overcome issues\nrelated to pre-baked lighting. We relight our training data and complement the\nmodel's supervision with a photometric loss, enabled by a differentiable\nimage-based relighting technique. Finally, since we predict spherical spectral\ncoefficients, we show that by imposing a distribution prior on the predicted\ncoefficients, we can greatly boost performance. Code and models available at\nhttps://vcl3d.github.io/DeepPanoramaLighting.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 14:23:05 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Gkitsas", "Vasileios", "", "1 and 2"], ["Zioulis", "Nikolaos", "", "1 and 2"], ["Alvarez", "Federico", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2005.08468", "submitter": "Debashis Mukherjee", "authors": "Debashis Mukherjee", "title": "An error reduced and uniform parameter approximation in fitting of\n  B-spline curves to data points", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximating data points in three or higher dimension space based on cubic\nB-spline curve is presented. Representations for planar curves, are merged and\nextended to the higher dimension. The curve is fitted to the order of data\npoints, or uniform parameter values are assumed for the points. Tangents are\nassumed at the data points, corresponding to the property used in cardinal\nsplines, for shape preserving and visually pleasing fit. Control points of\npiecewise continuous cubic bezier curves, meeting the boundary conditions of\ncardinal spline segments, are used for b-spline curve in corresponding\ncoordinate planes. Approximation using error computed in the least square\nsense, based on a fraction of data points, is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 06:00:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Mukherjee", "Debashis", ""]]}, {"id": "2005.08873", "submitter": "Thomas Peters", "authors": "Kirk E. Jordan, Ji Li, Thomas J. Peters", "title": "Knot Morphing Algorithm for Quantum `Fragile Topology'", "comments": "9 pages, 2 figures, submitted to Physics Letters A", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A knot theoretic algorithm is proposed to model `fragile topology' of quantum\nphysics.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 22:01:47 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Jordan", "Kirk E.", ""], ["Li", "Ji", ""], ["Peters", "Thomas J.", ""]]}, {"id": "2005.08891", "submitter": "Yi Zhou", "authors": "Yi Zhou, Jingwan Lu, Connelly Barnes, Jimei Yang, Sitao Xiang, Hao li", "title": "Generative Tweening: Long-term Inbetweening of 3D Human Motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate complex and realistic human body animations at scale,\nwhile following specific artistic constraints, has been a fundamental goal for\nthe game and animation industry for decades. Popular techniques include\nkey-framing, physics-based simulation, and database methods via motion graphs.\nRecently, motion generators based on deep learning have been introduced.\nAlthough these learning models can automatically generate highly intricate\nstylized motions of arbitrary length, they still lack user control. To this\nend, we introduce the problem of long-term inbetweening, which involves\nautomatically synthesizing complex motions over a long time interval given very\nsparse keyframes by users. We identify a number of challenges related to this\nproblem, including maintaining biomechanical and keyframe constraints,\npreserving natural motions, and designing the entire motion sequence\nholistically while considering all constraints. We introduce a biomechanically\nconstrained generative adversarial network that performs long-term inbetweening\nof human motions, conditioned on keyframe constraints. This network uses a\nnovel two-stage approach where it first predicts local motion in the form of\njoint angles, and then predicts global motion, i.e. the global path that the\ncharacter follows. Since there are typically a number of possible motions that\ncould satisfy the given user constraints, we also enable our network to\ngenerate a variety of outputs with a scheme that we call Motion DNA. This\napproach allows the user to manipulate and influence the output content by\nfeeding seed motions (DNA) to the network. Trained with 79 classes of captured\nmotion data, our network performs robustly on a variety of highly complex\nmotion styles.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:04:34 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 05:36:46 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhou", "Yi", ""], ["Lu", "Jingwan", ""], ["Barnes", "Connelly", ""], ["Yang", "Jimei", ""], ["Xiang", "Sitao", ""], ["li", "Hao", ""]]}, {"id": "2005.08925", "submitter": "Xuaner Zhang", "authors": "Xuaner Cecilia Zhang, Jonathan T. Barron, Yun-Ta Tsai, Rohit Pandey,\n  Xiuming Zhang, Ren Ng, David E. Jacobs", "title": "Portrait Shadow Manipulation", "comments": "(updated version); SIGGRAPH 2020;Project webpage:\n  https://people.eecs.berkeley.edu/~cecilia77/project-pages/portrait Video:\n  https://youtu.be/M_qYTXhzyac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Casually-taken portrait photographs often suffer from unflattering lighting\nand shadowing because of suboptimal conditions in the environment. Aesthetic\nqualities such as the position and softness of shadows and the lighting ratio\nbetween the bright and dark parts of the face are frequently determined by the\nconstraints of the environment rather than by the photographer. Professionals\naddress this issue by adding light shaping tools such as scrims, bounce cards,\nand flashes. In this paper, we present a computational approach that gives\ncasual photographers some of this control, thereby allowing poorly-lit\nportraits to be relit post-capture in a realistic and easily-controllable way.\nOur approach relies on a pair of neural networks---one to remove foreign\nshadows cast by external objects, and another to soften facial shadows cast by\nthe features of the subject and to add a synthetic fill light to improve the\nlighting ratio. To train our first network we construct a dataset of real-world\nportraits wherein synthetic foreign shadows are rendered onto the face, and we\nshow that our network learns to remove those unwanted shadows. To train our\nsecond network we use a dataset of Light Stage scans of human subjects to\nconstruct input/output pairs of input images harshly lit by a small light\nsource, and variably softened and fill-lit output images of each face. We\npropose a way to explicitly encode facial symmetry and show that our dataset\nand training procedure enable the model to generalize to images taken in the\nwild. Together, these networks enable the realistic and aesthetically pleasing\nenhancement of shadows and lights in real-world portrait images\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:51:34 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 17:49:55 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Zhang", "Xuaner Cecilia", ""], ["Barron", "Jonathan T.", ""], ["Tsai", "Yun-Ta", ""], ["Pandey", "Rohit", ""], ["Zhang", "Xiuming", ""], ["Ng", "Ren", ""], ["Jacobs", "David E.", ""]]}, {"id": "2005.08944", "submitter": "Kevin Feng", "authors": "Kevin Feng", "title": "Saving the Sonorine: Photovisual Audio Recovery Using Image Processing\n  and Computer Vision Techniques", "comments": "This version has been removed by arXiv administrators because the\n  submitter did not have the right to agree to the license applied at the time\n  of submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.GR eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel technique to recover audio from sonorines, an\nearly 20th century form of analogue sound storage. Our method uses high\nresolution photographs of sonorines under different lighting conditions to\nobserve the change in reflection behavior of the physical surface features and\ncreate a three-dimensional height map of the surface. Sound can then be\nextracted using height information within the surface's grooves, mimicking a\nphysical stylus on a phonograph. Unlike traditional playback methods, our\nmethod has the advantage of being contactless: the medium will not incur damage\nand wear from being played repeatedly. We compare the results of our technique\nto a previously successful contactless method using flatbed scans of the\nsonorines, and conclude with future research that can be applied to this\nphotovisual approach to audio recovery.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 00:45:26 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 00:51:35 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 20:08:01 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Feng", "Kevin", ""]]}, {"id": "2005.09226", "submitter": "Zhixin Li", "authors": "Zhixin Li, Wenyuan Zhang, Jie Shan", "title": "Holistic Parameteric Reconstruction of Building Models from Point Clouds", "comments": "Remote Sens. Spatial Inf. Sci., 2020", "journal-ref": null, "doi": "10.5194/isprs-archives-XLIII-B2-2020-689-2020", "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models are conventionally reconstructed by building roof points\nplanar segmentation and then using a topology graph to group the planes\ntogether. Roof edges and vertices are then mathematically represented by\nintersecting segmented planes. Technically, such solution is based on\nsequential local fitting, i.e., the entire data of one building are not\nsimultaneously participating in determining the building model. As a\nconsequence, the solution is lack of topological integrity and geometric rigor.\nFundamentally different from this traditional approach, we propose a holistic\nparametric reconstruction method which means taking into consideration the\nentire point clouds of one building simultaneously. In our work, building\nmodels are reconstructed from predefined parametric (roof) primitives. We first\nuse a well-designed deep neural network to segment and identify primitives in\nthe given building point clouds. A holistic optimization strategy is then\nintroduced to simultaneously determine the parameters of a segmented primitive.\nIn the last step, the optimal parameters are used to generate a watertight\nbuilding model in CityGML format. The airborne LiDAR dataset RoofN3D with\npredefined roof types is used for our test. It is shown that PointNet++ applied\nto the entire dataset can achieve an accuracy of 83% for primitive\nclassification. For a subset of 910 buildings in RoofN3D, the holistic approach\nis then used to determine the parameters of primitives and reconstruct the\nbuildings. The achieved overall quality of reconstruction is 0.08 meters for\npoint-surface-distance or 0.7 times RMSE of the input LiDAR points. The study\ndemonstrates the efficiency and capability of the proposed approach and its\npotential to handle large scale urban point clouds.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 05:42:23 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Li", "Zhixin", ""], ["Zhang", "Wenyuan", ""], ["Shan", "Jie", ""]]}, {"id": "2005.09704", "submitter": "Zili Yi", "authors": "Zili Yi, Qiang Tang, Shekoofeh Azizi, Daesik Jang, Zhan Xu", "title": "Contextual Residual Aggregation for Ultra High-Resolution Image\n  Inpainting", "comments": "CVPR 2020 oral paper. 22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently data-driven image inpainting methods have made inspiring progress,\nimpacting fundamental image editing tasks such as object removal and damaged\nimage repairing. These methods are more effective than classic approaches,\nhowever, due to memory limitations they can only handle low-resolution inputs,\ntypically smaller than 1K. Meanwhile, the resolution of photos captured with\nmobile devices increases up to 8K. Naive up-sampling of the low-resolution\ninpainted result can merely yield a large yet blurry result. Whereas, adding a\nhigh-frequency residual image onto the large blurry image can generate a sharp\nresult, rich in details and textures. Motivated by this, we propose a\nContextual Residual Aggregation (CRA) mechanism that can produce high-frequency\nresiduals for missing contents by weighted aggregating residuals from\ncontextual patches, thus only requiring a low-resolution prediction from the\nnetwork. Since convolutional layers of the neural network only need to operate\non low-resolution inputs and outputs, the cost of memory and computing power is\nthus well suppressed. Moreover, the need for high-resolution training datasets\nis alleviated. In our experiments, we train the proposed model on small images\nwith resolutions 512x512 and perform inference on high-resolution images,\nachieving compelling inpainting quality. Our model can inpaint images as large\nas 8K with considerable hole sizes, which is intractable with previous\nlearning-based approaches. We further elaborate on the light-weight design of\nthe network architecture, achieving real-time performance on 2K images on a GTX\n1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:55:32 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Yi", "Zili", ""], ["Tang", "Qiang", ""], ["Azizi", "Shekoofeh", ""], ["Jang", "Daesik", ""], ["Xu", "Zhan", ""]]}, {"id": "2005.09941", "submitter": "Magnus Palmblad", "authors": "Reinier Vleugels and Magnus Palmblad", "title": "Non-Uniform Gaussian Blur of Hexagonal Bins in Cartesian Coordinates", "comments": "10 pages, 6 figures. Figures 1-5 made in LaTeX and look OK in\n  preview. Typos corrected in Version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent application of the Bokeh Python library for visualizing\nphysico-chemical properties of chemical entities text-mined from the scientific\nliterature, we found ourselves facing the task of smoothing hexagonally binned\ndata in Cartesian coordinates. To the best of our knowledge, no documentation\nfor how to do this exist in the public domain. This short paper shows how to\naccomplish this in general and for Bokeh in particular. We illustrate the\nmethod with a real-world example and discuss some potential advantages of using\nhexagonal bins in these and similar applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 09:59:57 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 14:22:59 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Vleugels", "Reinier", ""], ["Palmblad", "Magnus", ""]]}, {"id": "2005.10510", "submitter": "Junbum Cha", "authors": "Junbum Cha, Sanghyuk Chun, Gayoung Lee, Bado Lee, Seonghyeon Kim, and\n  Hwalsuk Lee", "title": "Few-shot Compositional Font Generation with Dual Memory", "comments": "ECCV 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating a new font library is a very labor-intensive and time-consuming\njob for glyph-rich scripts. Despite the remarkable success of existing font\ngeneration methods, they have significant drawbacks; they require a large\nnumber of reference images to generate a new font set, or they fail to capture\ndetailed styles with only a few samples. In this paper, we focus on\ncompositional scripts, a widely used letter system in the world, where each\nglyph can be decomposed by several components. By utilizing the\ncompositionality of compositional scripts, we propose a novel font generation\nframework, named Dual Memory-augmented Font Generation Network (DM-Font), which\nenables us to generate a high-quality font library with only a few samples. We\nemploy memory components and global-context awareness in the generator to take\nadvantage of the compositionality. In the experiments on Korean-handwriting\nfonts and Thai-printing fonts, we observe that our method generates a\nsignificantly better quality of samples with faithful stylization compared to\nthe state-of-the-art generation methods quantitatively and qualitatively.\nSource code is available at https://github.com/clovaai/dmfont.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 08:13:40 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:47:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cha", "Junbum", ""], ["Chun", "Sanghyuk", ""], ["Lee", "Gayoung", ""], ["Lee", "Bado", ""], ["Kim", "Seonghyeon", ""], ["Lee", "Hwalsuk", ""]]}, {"id": "2005.10663", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf", "title": "Wish You Were Here: Context-Aware Human Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for inserting objects, specifically humans, into\nexisting images, such that they blend in a photorealistic manner, while\nrespecting the semantic context of the scene. Our method involves three\nsubnetworks: the first generates the semantic map of the new person, given the\npose of the other persons in the scene and an optional bounding box\nspecification. The second network renders the pixels of the novel person and\nits blending mask, based on specifications in the form of multiple appearance\ncomponents. A third network refines the generated face in order to match those\nof the target person. Our experiments present convincing high-resolution\noutputs in this novel and challenging application domain. In addition, the\nthree networks are evaluated individually, demonstrating for example, state of\nthe art results in pose transfer benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 14:09:14 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""]]}, {"id": "2005.11084", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Gal Metzer, Raja Giryes, Daniel Cohen-Or", "title": "Point2Mesh: A Self-Prior for Deformable Meshes", "comments": "SIGGRAPH 2020; Project page:\n  https://ranahanocka.github.io/point2mesh/", "journal-ref": null, "doi": "10.1145/3386569.3392415", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Point2Mesh, a technique for reconstructing a\nsurface mesh from an input point cloud. Instead of explicitly specifying a\nprior that encodes the expected shape properties, the prior is defined\nautomatically using the input point cloud, which we refer to as a self-prior.\nThe self-prior encapsulates reoccurring geometric repetitions from a single\nshape within the weights of a deep neural network. We optimize the network\nweights to deform an initial mesh to shrink-wrap a single input point cloud.\nThis explicitly considers the entire reconstructed shape, since shared local\nkernels are calculated to fit the overall object. The convolutional kernels are\noptimized globally across the entire shape, which inherently encourages\nlocal-scale geometric self-similarity across the shape surface. We show that\nshrink-wrapping a point cloud with a self-prior converges to a desirable\nsolution; compared to a prescribed smoothness prior, which often becomes\ntrapped in undesirable local minima. While the performance of traditional\nreconstruction approaches degrades in non-ideal conditions that are often\npresent in real world scanning, i.e., unoriented normals, noise and missing\n(low density) parts, Point2Mesh is robust to non-ideal conditions. We\ndemonstrate the performance of Point2Mesh on a large variety of shapes with\nvarying complexity.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 10:01:04 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hanocka", "Rana", ""], ["Metzer", "Gal", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2005.11269", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo, Samuel E. Schmidt, Olaf T von Ramm", "title": "Software Implementation of Optimized Bicubic Interpolated Scan\n  Conversion in Echocardiography", "comments": "10 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the image-quality-guided strategy for optimization of\nbicubic interpolation and interpolated scan conversion algorithms. This\nstrategy uses feature selection through line chart data visualization technique\nand first index of the minimum absolute difference between computed scores and\nideal scores to determine the image quality guided coefficient k that changes\nall sixteen BIC coefficients to new coefficients on which the OBIC\ninterpolation algorithm is based. Perceptual evaluations of cropped sectored\nimages from Matlab software implementation of interpolated scan conversion\nalgorithms are presented. Also, IQA metrics-based evaluation is presented and\ndemonstrates that the overall performance of the OBIC algorithm is 92.22% when\ncompared with BIC alone, but becomes 57.22% with all other methods mentioned.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:46:27 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Rukundo", "Olivier", ""], ["Schmidt", "Samuel E.", ""], ["von Ramm", "Olaf T", ""]]}, {"id": "2005.11617", "submitter": "Jingwei Huang", "authors": "Jingwei Huang, Chiyu Max Jiang, Baiqiang Leng, Bin Wang and Leonidas\n  Guibas", "title": "MeshODE: A Robust and Scalable Framework for Mesh Deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MeshODE, a scalable and robust framework for pairwise CAD model\ndeformation without prespecified correspondences. Given a pair of shapes, our\nframework provides a novel shape feature-preserving mapping function that\ncontinuously deforms one model to the other by minimizing fitting and rigidity\nlosses based on the non-rigid iterative-closest-point (ICP) algorithm. We\naddress two challenges in this problem, namely the design of a powerful\ndeformation function and obtaining a feature-preserving CAD deformation. While\ntraditional deformation directly optimizes for the coordinates of the mesh\nvertices or the vertices of a control cage, we introduce a deep bijective\nmapping that utilizes a flow model parameterized as a neural network. Our\nfunction has the capacity to handle complex deformations, produces deformations\nthat are guaranteed free of self-intersections, and requires low rigidity\nconstraining for geometry preservation, which leads to a better fitting quality\ncompared with existing methods. It additionally enables continuous deformation\nbetween two arbitrary shapes without supervision for intermediate shapes.\nFurthermore, we propose a robust preprocessing pipeline for raw CAD meshes\nusing feature-aware subdivision and a uniform graph template representation to\naddress artifacts in raw CAD models including self-intersections, irregular\ntriangles, topologically disconnected components, non-manifold edges, and\nnonuniformly distributed vertices. This facilitates a fast deformation\noptimization process that preserves global and local details. Our code is\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 22:53:04 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Huang", "Jingwei", ""], ["Jiang", "Chiyu Max", ""], ["Leng", "Baiqiang", ""], ["Wang", "Bin", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2005.11621", "submitter": "Jingwei Huang", "authors": "Jingwei Huang, Yichao Zhou and Leonidas Guibas", "title": "ManifoldPlus: A Robust and Scalable Watertight Manifold Surface\n  Generation Method for Triangle Soups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ManifoldPlus, a method for robust and scalable conversion of\ntriangle soups to watertight manifolds. While many algorithms in computer\ngraphics require the input mesh to be a watertight manifold, in practice many\nmeshes designed by artists are often for visualization purposes, and thus have\nnon-manifold structures such as incorrect connectivity, ambiguous face\norientation, double surfaces, open boundaries, self-intersections, etc.\nExisting methods suffer from problems in the inputs with face orientation and\nzero-volume structures. Additionally most methods do not scale to meshes of\nhigh complexity. In this paper, we propose a method that extracts exterior\nfaces between occupied voxels and empty voxels, and uses a projection-based\noptimization method to accurately recover a watertight manifold that resembles\nthe reference mesh. Compared to previous methods, our methodology is simpler.\nIt does not rely on face normals of the input triangle soups and can accurately\nrecover zero-volume structures. Our algorithm is scalable, because it employs\nan adaptive Gauss-Seidel method for shape optimization, in which each step is\nan easy-to-solve convex problem. We test ManifoldPlus on ModelNet10 and\nAccuCity datasets to verify that our methods can generate watertight meshes\nranging from object-level shapes to city-level models. Furthermore, through our\nexperimental evaluations, we show that our method is more robust, efficient and\naccurate than the state-of-the-art. Our implementation is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 23:27:44 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Huang", "Jingwei", ""], ["Zhou", "Yichao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2005.11622", "submitter": "Norman Tatro", "authors": "N. Joseph Tatro, Stefan C. Schonsheck, Rongjie Lai", "title": "Unsupervised Geometric Disentanglement for Surfaces via CFAN-VAE", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric disentanglement, the separation of latent codes for intrinsic (i.e.\nidentity) and extrinsic(i.e. pose) geometry, is a prominent task for generative\nmodels of non-Euclidean data such as 3D deformable models. It provides greater\ninterpretability of the latent space, and leads to more control in generation.\nThis work introduces a mesh feature, the conformal factor and normal feature\n(CFAN),for use in mesh convolutional autoencoders. We further propose CFAN-VAE,\na novel architecture that disentangles identity and pose using the CFAN\nfeature. Requiring no label information on the identity or pose during\ntraining, CFAN-VAE achieves geometric disentanglement in an unsupervisedway.\nOur comprehensive experiments, including reconstruction, interpolation,\ngeneration, and identity/pose transfer, demonstrate CFAN-VAE achieves\nstate-of-the-art performance on unsupervised geometric disentanglement. We also\nsuccessfully detect a level of geometric disentanglement in mesh convolutional\nautoencoders that encode xyz-coordinates directly by registering its latent\nspace to that of CFAN-VAE.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 23:28:10 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 01:50:38 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Tatro", "N. Joseph", ""], ["Schonsheck", "Stefan C.", ""], ["Lai", "Rongjie", ""]]}, {"id": "2005.11799", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari and Subhasis Chaudhuri", "title": "Haptic Rendering of Thin, Deformable Objects with Spatially Varying\n  Stiffness", "comments": "Accepted in Eurohaptics 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, we often come across soft objects having spatially varying\nstiffness, such as human palm or a wart on the skin. In this paper, we propose\na novel approach to render thin, deformable objects having spatially varying\nstiffness (inhomogeneous material). We use the classical Kirchhoff thin plate\ntheory to compute the deformation. In general, the physics-based rendering of\nan arbitrary 3D surface is complex and time-consuming. Therefore, we\napproximate the 3D surface locally by a 2D plane using an area-preserving\nmapping technique - Gall-Peters mapping. Once the deformation is computed by\nsolving a fourth-order partial differential equation, we project the points\nback onto the original object for proper haptic rendering. The method was\nvalidated through user experiments and was found to be realistic.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 16:43:24 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:30:59 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2005.12489", "submitter": "Mengyu Ma", "authors": "Mengyu Ma, Ye Wu, Xue Ouyang, Luo Chen, Jun Li, Ning Jing", "title": "HiVision: Rapid Visualization of Large-Scale Spatial Vector Data", "comments": "19 pages 16 figures", "journal-ref": "Computers & Geosciences,2021,147(104665)", "doi": "10.1016/j.cageo.2020.104665", "report-no": null, "categories": "cs.GR cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid visualization of large-scale spatial vector data is a long-standing\nchallenge in Geographic Information Science. In existing methods, the\ncomputation overheads grow rapidly with data volumes, leading to the\nincapability of providing real-time visualization for large-scale spatial\nvector data, even with parallel acceleration technologies. To fill the gap, we\npresent HiVision, a display-driven visualization model for large-scale spatial\nvector data. Different from traditional data-driven methods, the computing\nunits in HiVision are pixels rather than spatial objects to achieve real-time\nperformance, and efficient spatial-index-based strategies are introduced to\nestimate the topological relationships between pixels and spatial objects.\nHiVision can maintain exceedingly good performance regardless of the data\nvolume due to the stable pixel number for display. In addition, an optimized\nparallel computing architecture is proposed in HiVision to ensure the ability\nof real-time visualization. Experiments show that our approach outperforms\ntraditional methods in rendering speed and visual effects while dealing with\nlarge-scale spatial vector data, and can provide interactive visualization of\ndatasets with billion-scale points/segments/edges in real-time with flexible\nrendering styles. The HiVision code is open-sourced at\nhttps://github.com/MemoryMmy/HiVision with an online demonstration.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 02:48:40 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 05:53:02 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Ma", "Mengyu", ""], ["Wu", "Ye", ""], ["Ouyang", "Xue", ""], ["Chen", "Luo", ""], ["Li", "Jun", ""], ["Jing", "Ning", ""]]}, {"id": "2005.12518", "submitter": "Shilin Zhu", "authors": "Shilin Zhu", "title": "Survey: Machine Learning in Production Rendering", "comments": "This was the survey I did for my PhD research exam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, machine learning-based approaches have had some great\nsuccess for rendering animated feature films. This survey summarizes several of\nthe most dramatic improvements in using deep neural networks over traditional\nrendering methods, such as better image quality and lower computational\noverhead. More specifically, this survey covers the fundamental principles of\nmachine learning and its applications, such as denoising, path guiding,\nrendering participating media, and other notoriously difficult light transport\nsituations. Some of these techniques have already been used in the latest\nreleased animations while others are still in the continuing development by\nresearchers in both academia and movie studios. Although learning-based\nrendering methods still have some open issues, they have already demonstrated\npromising performance in multiple parts of the rendering pipeline, and people\nare continuously making new attempts.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 05:23:32 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Zhu", "Shilin", ""]]}, {"id": "2005.12662", "submitter": "Zihao Wang", "authors": "Zihao Wang, Clair Vandersteen, Thomas Demarcy, Dan Gnansia, Charles\n  Raffaelli, Nicolas Guevara, Herv\\'e Delingette", "title": "A Deep Learning based Fast Signed Distance Map Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIDL/2020/ExtendedAbstract/b2N5ZuEouu", "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signed distance map (SDM) is a common representation of surfaces in medical\nimage analysis and machine learning. The computational complexity of SDM for 3D\nparametric shapes is often a bottleneck in many applications, thus limiting\ntheir interest. In this paper, we propose a learning based SDM generation\nneural network which is demonstrated on a tridimensional cochlea shape model\nparameterized by 4 shape parameters. The proposed SDM Neural Network generates\na cochlea signed distance map depending on four input parameters and we show\nthat the deep learning approach leads to a 60 fold improvement in the time of\ncomputation compared to more classical SDM generation methods. Therefore, the\nproposed approach achieves a good trade-off between accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 12:36:19 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Zihao", ""], ["Vandersteen", "Clair", ""], ["Demarcy", "Thomas", ""], ["Gnansia", "Dan", ""], ["Raffaelli", "Charles", ""], ["Guevara", "Nicolas", ""], ["Delingette", "Herv\u00e9", ""]]}, {"id": "2005.12772", "submitter": "Tiago Novello", "authors": "Tiago Novello, Vin\\'icius da Silva, Luiz Velho", "title": "How to see the eight Thurston geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.GR math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A manifold is a topological space that is locally Euclidean. Manifolds are\nimportant because they arise naturally in a variety of mathematical and\nphysical applications as global objects with simpler local structure. In this\npaper we propose a technique for immersive visualization of relevant\nthree-dimensional manifolds in the context of the Geometrization conjecture.\nThe algorithm generalizes traditional computer graphics ray tracing. To do so\nwe use several related definitions and results dating back to the works of\nPoincar\\'e, Thurston, and Perelman.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 14:50:46 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 15:47:10 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 19:42:18 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Novello", "Tiago", ""], ["da Silva", "Vin\u00edcius", ""], ["Velho", "Luiz", ""]]}, {"id": "2005.13532", "submitter": "Aayush Bansal", "authors": "Aayush Bansal, Minh Vo, Yaser Sheikh, Deva Ramanan, Srinivasa\n  Narasimhan", "title": "4D Visualization of Dynamic Events from Unconstrained Multi-View Videos", "comments": "Project Page - http://www.cs.cmu.edu/~aayushb/Open4D/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven approach for 4D space-time visualization of dynamic\nevents from videos captured by hand-held multiple cameras. Key to our approach\nis the use of self-supervised neural networks specific to the scene to compose\nstatic and dynamic aspects of an event. Though captured from discrete\nviewpoints, this model enables us to move around the space-time of the event\ncontinuously. This model allows us to create virtual cameras that facilitate:\n(1) freezing the time and exploring views; (2) freezing a view and moving\nthrough time; and (3) simultaneously changing both time and view. We can also\nedit the videos and reveal occluded objects for a given view if it is visible\nin any of the other views. We validate our approach on challenging in-the-wild\nevents captured using up to 15 mobile cameras.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 17:57:19 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Bansal", "Aayush", ""], ["Vo", "Minh", ""], ["Sheikh", "Yaser", ""], ["Ramanan", "Deva", ""], ["Narasimhan", "Srinivasa", ""]]}, {"id": "2005.14370", "submitter": "Deok-Kyeong Jang", "authors": "Deok-Kyeong Jang and Sung-Hee Lee", "title": "Constructing Human Motion Manifold with Sequential Networks", "comments": "11 pages, It will be published at Computer Graphics Forum", "journal-ref": null, "doi": "10.1111/cgf.14028", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel recurrent neural network-based method to\nconstruct a latent motion manifold that can represent a wide range of human\nmotions in a long sequence. We introduce several new components to increase the\nspatial and temporal coverage in motion space while retaining the details of\nmotion capture data. These include new regularization terms for the motion\nmanifold, combination of two complementary decoders for predicting joint\nrotations and joint velocities, and the addition of the forward kinematics\nlayer to consider both joint rotation and position errors. In addition, we\npropose a set of loss terms that improve the overall quality of the motion\nmanifold from various aspects, such as the capability of reconstructing not\nonly the motion but also the latent manifold vector, and the naturalness of the\nmotion through adversarial loss. These components contribute to creating\ncompact and versatile motion manifold that allows for creating new motions by\nperforming random sampling and algebraic operations, such as interpolation and\nanalogy, in the latent motion manifold.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 02:45:46 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Jang", "Deok-Kyeong", ""], ["Lee", "Sung-Hee", ""]]}, {"id": "2005.14695", "submitter": "Micha Pfeiffer", "authors": "Micha Pfeiffer, Carina Riediger, Stefan Leger, Jens-Peter K\\\"uhn,\n  Danilo Seppelt, Ralf-Thorsten Hoffmann, J\\\"urgen Weitz and Stefanie Speidel", "title": "Non-Rigid Volume to Surface Registration using a Data-Driven\n  Biomechanical Model", "comments": "Provisionally accepted for MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-rigid registration is a key component in soft-tissue navigation. We focus\non laparoscopic liver surgery, where we register the organ model obtained from\na preoperative CT scan to the intraoperative partial organ surface,\nreconstructed from the laparoscopic video. This is a challenging task due to\nsparse and noisy intraoperative data, real-time requirements and many unknowns\n- such as tissue properties and boundary conditions. Furthermore, establishing\ncorrespondences between pre- and intraoperative data can be extremely difficult\nsince the liver usually lacks distinct surface features and the used imaging\nmodalities suffer from very different types of noise. In this work, we train a\nconvolutional neural network to perform both the search for surface\ncorrespondences as well as the non-rigid registration in one step. The network\nis trained on physically accurate biomechanical simulations of randomly\ngenerated, deforming organ-like structures. This enables the network to\nimmediately generalize to a new patient organ without the need to re-train. We\nadd various amounts of noise to the intraoperative surfaces during training,\nmaking the network robust to noisy intraoperative data. During inference, the\nnetwork outputs the displacement field which matches the preoperative volume to\nthe partial intraoperative surface. In multiple experiments, we show that the\nnetwork translates well to real data while maintaining a high inference speed.\nOur code is made available online.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 17:35:23 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Pfeiffer", "Micha", ""], ["Riediger", "Carina", ""], ["Leger", "Stefan", ""], ["K\u00fchn", "Jens-Peter", ""], ["Seppelt", "Danilo", ""], ["Hoffmann", "Ralf-Thorsten", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}]