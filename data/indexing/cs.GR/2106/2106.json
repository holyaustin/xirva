[{"id": "2106.00220", "submitter": "Nicholas Sharp", "authors": "Mark Gillespie, Nicholas Sharp, Keenan Crane", "title": "Integer Coordinates for Intrinsic Geometry Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a general, efficient, and provably robust\nrepresentation for intrinsic triangulations. These triangulations have emerged\nas a powerful tool for robust geometry processing of surface meshes, taking a\nlow-quality mesh and retriangulating it with high-quality intrinsic triangles.\nHowever, existing representations either support only edge flips, or do not\noffer a robust procedure to recover the common subdivision, that is, how the\nintrinsic triangulation sits along the original surface. To build a\ngeneral-purpose robust structure, we extend the framework of normal\ncoordinates, which have been deeply studied in topology, as well as the more\nrecent idea of roundabouts from geometry processing, to support a variety of\nmesh processing operations like vertex insertions, edge splits, etc. The basic\nidea is to store an integer per mesh edge counting the number of times a curve\ncrosses that edge. We show that this paradigm offers a highly effective\nrepresentation for intrinsic triangulations with strong robustness guarantees.\nThe resulting data structure is general and efficient, while offering a\nguarantee of always encoding a valid subdivision. Among other things, this\nallows us to generate a high-quality intrinsic Delaunay refinement of all\nmanifold meshes in the challenging Thingi10k dataset for the first time. This\nenables a broad class of existing surface geometry algorithms to be applied\nout-of-the-box to low-quality triangulations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:23:04 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gillespie", "Mark", ""], ["Sharp", "Nicholas", ""], ["Crane", "Keenan", ""]]}, {"id": "2106.01266", "submitter": "Leonardo Fanzeres", "authors": "Leonardo A. Fanzeres and Climent Nadeu", "title": "Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep\n  Dense Network Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR cs.MM eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The motivation of our research is to develop a sound-to-image (S2I)\ntranslation system for enabling a human receiver to visually infer the\noccurrence of sound related events. We expect the computer to 'imagine' the\nscene from the captured sound, generating original images that picture the\nsound emitting source. Previous studies on similar topics opted for simplified\napproaches using data with low content diversity and/or strong supervision.\nDifferently, we propose to perform unsupervised S2I translation using thousands\nof distinct and unknown scenes, with slightly pre-cleaned data, just enough to\nguarantee aural-visual semantic coherence. To that end, we employ conditional\ngenerative adversarial networks (GANs) with a deep densely connected generator.\nBesides, we implemented a moving-average adversarial loss to address GANs\ntraining instability. Though the specified S2I translation problem is quite\nchallenging, we were able to generalize the translator model enough to obtain\nmore than 14%, in average, of interpretable and semantically coherent images\ntranslated from unknown sounds. Additionally, we present a solution using\ninformativity classifiers to perform quantitative evaluation of S2I\ntranslation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:20:43 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fanzeres", "Leonardo A.", ""], ["Nadeu", "Climent", ""]]}, {"id": "2106.01326", "submitter": "Joseph Chen", "authors": "Joseph Chen, Ko-Wei Tai, Wen-Chin Chen, and Ming Ouhyoung", "title": "Robust Voxelization and Visualization by Improved Tetrahedral Mesh\n  Generation", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When obtaining interior 3D voxel data from triangular meshes, most existing\nmethods fail to handle low quality meshes which happens to take up a big\nportion on the internet. In this work we present a robust voxelization method\nthat is based on tetrahedral mesh generation within a user defined error bound.\nComparing to other tetrahedral mesh generation methods, our method produces\nmuch higher quality tetrahedral meshes as the intermediate outcome, which\nallows us to utilize a faster voxelization algorithm that is based on a\nstronger assumption. We show the results comparing to various methods including\nthe state-of-the-art. Our contribution includes a framework which takes\ntriangular mesh as an input and produces voxelized data, a proof to an unproved\nalgorithm that performs better than the state-of-the-art, and various\nexperiments including parallelization built on the GPU and CPU. We further\ntested our method on various dataset including Princeton ModelNet and Thingi10k\nto show the robustness of the framework, where near 100% availability is\nachieved, while others can only achieve around 50%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:38:22 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 08:35:30 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chen", "Joseph", ""], ["Tai", "Ko-Wei", ""], ["Chen", "Wen-Chin", ""], ["Ouhyoung", "Ming", ""]]}, {"id": "2106.01504", "submitter": "Paul McLachlan", "authors": "Ryan Killea, Yun Li, Saeed Bastani, Paul McLachlan", "title": "DeepCompress: Efficient Point Cloud Geometry Compression", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Point clouds are a basic data type that is increasingly of interest as 3D\ncontent becomes more ubiquitous. Applications using point clouds include\nvirtual, augmented, and mixed reality and autonomous driving. We propose a more\nefficient deep learning-based encoder architecture for point clouds compression\nthat incorporates principles from established 3D object detection and image\ncompression architectures. Through an ablation study, we show that\nincorporating the learned activation function from Computational Efficient\nNeural Image Compression (CENIC) and designing more parameter-efficient\nconvolutional blocks yields dramatic gains in efficiency and performance. Our\nproposed architecture incorporates Generalized Divisive Normalization\nactivations and propose a spatially separable InceptionV4-inspired block. We\nthen evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized\nFull Bodies dataset to evaluate our model's performance. Our proposed\nmodifications outperform the baseline approaches by a small margin in terms of\nBjontegard delta rate and PSNR values, yet reduces necessary encoder\nconvolution operations by 8 percent and reduces total encoder parameters by 20\npercent. Our proposed architecture, when considered on its own, has a small\npenalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit\nrate in Point to Plane Distance for the same peak signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:18:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Killea", "Ryan", ""], ["Li", "Yun", ""], ["Bastani", "Saeed", ""], ["McLachlan", "Paul", ""]]}, {"id": "2106.01505", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, John Femiani, Peter Wonka", "title": "Barbershop: GAN-based Image Compositing using Segmentation Masks", "comments": "Project page: https://zpdesu.github.io/Barbershop/ Video:\n  https://youtu.be/ZU-yrAvoJfQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Seamlessly blending features from multiple images is extremely challenging\nbecause of complex relationships in lighting, geometry, and partial occlusion\nwhich cause coupling between different parts of the image. Even though recent\nwork on GANs enables synthesis of realistic hair or faces, it remains difficult\nto combine them into a single, coherent, and plausible image rather than a\ndisjointed set of image patches. We present a novel solution to image blending,\nparticularly for the problem of hairstyle transfer, based on GAN-inversion. We\npropose a novel latent space for image blending which is better at preserving\ndetail and encoding spatial information, and propose a new GAN-embedding\nalgorithm which is able to slightly modify images to conform to a common\nsegmentation mask. Our novel representation enables the transfer of the visual\nproperties from multiple reference images including specific details such as\nmoles and wrinkles, and because we do image blending in a latent-space we are\nable to synthesize images that are coherent. Our approach avoids blending\nartifacts present in other approaches and finds a globally consistent image.\nOur results demonstrate a significant improvement over the current state of the\nart in a user study, with users preferring our blending solution over 95\npercent of the time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:20:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Femiani", "John", ""], ["Wonka", "Peter", ""]]}, {"id": "2106.01553", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang, Yang Liu, Yu-Qi Yang, Xin Tong", "title": "Spline Positional Encoding for Learning 3D Implicit Signed Distance\n  Fields", "comments": "Accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer perceptrons (MLPs) have been successfully used to represent 3D\nshapes implicitly and compactly, by mapping 3D coordinates to the corresponding\nsigned distance values or occupancy values. In this paper, we propose a novel\npositional encoding scheme, called Spline Positional Encoding, to map the input\ncoordinates to a high dimensional space before passing them to MLPs, for\nhelping to recover 3D signed distance fields with fine-scale geometric details\nfrom unorganized 3D point clouds. We verified the superiority of our approach\nover other positional encoding schemes on tasks of 3D shape reconstruction from\ninput point clouds and shape space learning. The efficacy of our approach\nextended to image reconstruction is also demonstrated and evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:37:47 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Liu", "Yang", ""], ["Yang", "Yu-Qi", ""], ["Tong", "Xin", ""]]}, {"id": "2106.01970", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Pratul P. Srinivasan, Boyang Deng, Paul Debevec,\n  William T. Freeman, Jonathan T. Barron", "title": "NeRFactor: Neural Factorization of Shape and Reflectance Under an\n  Unknown Illumination", "comments": "Project Page:\n  https://people.csail.mit.edu/xiuming/projects/nerfactor/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recovering the shape and spatially-varying\nreflectance of an object from posed multi-view images of the object illuminated\nby one unknown lighting condition. This enables the rendering of novel views of\nthe object under arbitrary environment lighting and editing of the object's\nmaterial properties. The key to our approach, which we call Neural Radiance\nFactorization (NeRFactor), is to distill the volumetric geometry of a Neural\nRadiance Field (NeRF) [Mildenhall et al. 2020] representation of the object\ninto a surface representation and then jointly refine the geometry while\nsolving for the spatially-varying reflectance and the environment lighting.\nSpecifically, NeRFactor recovers 3D neural fields of surface normals, light\nvisibility, albedo, and Bidirectional Reflectance Distribution Functions\n(BRDFs) without any supervision, using only a re-rendering loss, simple\nsmoothness priors, and a data-driven BRDF prior learned from real-world BRDF\nmeasurements. By explicitly modeling light visibility, NeRFactor is able to\nseparate shadows from albedo and synthesize realistic soft or hard shadows\nunder arbitrary lighting conditions. NeRFactor is able to recover convincing 3D\nmodels for free-viewpoint relighting in this challenging and underconstrained\ncapture setup for both synthetic and real scenes. Qualitative and quantitative\nexperiments show that NeRFactor outperforms classic and deep learning-based\nstate of the art across various tasks. Our code and data are available at\npeople.csail.mit.edu/xiuming/projects/nerfactor/.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:18:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Xiuming", ""], ["Srinivasan", "Pratul P.", ""], ["Deng", "Boyang", ""], ["Debevec", "Paul", ""], ["Freeman", "William T.", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "2106.01981", "submitter": "Boris Oreshkin N", "authors": "Boris N. Oreshkin and Florent Bocquelet and F\\'elix G. Harvey and Bay\n  Raitt and Dominic Laflamme", "title": "ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on the development of a learnable neural representation of\nhuman pose for advanced AI assisted animation tooling. Specifically, we tackle\nthe problem of constructing a full static human pose based on sparse and\nvariable user inputs (e.g. locations and/or orientations of a subset of body\njoints). To solve this problem, we propose a novel neural architecture that\ncombines residual connections with prototype encoding of a partially specified\npose to create a new complete pose from the learned latent space. We show that\nour architecture outperforms a baseline based on Transformer, both in terms of\naccuracy and computational efficiency. Additionally, we develop a user\ninterface to integrate our neural model in Unity, a real-time 3D development\nplatform. Furthermore, we introduce two new datasets representing the static\nhuman pose modeling problem, based on high-quality human motion capture data,\nwhich will be released publicly along with model code.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 16:56:58 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 14:05:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Oreshkin", "Boris N.", ""], ["Bocquelet", "Florent", ""], ["Harvey", "F\u00e9lix G.", ""], ["Raitt", "Bay", ""], ["Laflamme", "Dominic", ""]]}, {"id": "2106.02019", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao\n  Gu, Christian Theobalt", "title": "Neural Actor: Neural Free-view Synthesis of Human Actors with Pose\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Actor (NA), a new method for high-quality synthesis of\nhumans from arbitrary viewpoints and under arbitrary controllable poses. Our\nmethod is built upon recent neural scene representation and rendering works\nwhich learn representations of geometry and appearance from only 2D images.\nWhile existing works demonstrated compelling rendering of static scenes and\nplayback of dynamic scenes, photo-realistic reconstruction and rendering of\nhumans with neural implicit methods, in particular under user-controlled novel\nposes, is still difficult. To address this problem, we utilize a coarse body\nmodel as the proxy to unwarp the surrounding 3D space into a canonical pose. A\nneural radiance field learns pose-dependent geometric deformations and pose-\nand view-dependent appearance effects in the canonical space from multi-view\nvideo input. To synthesize novel views of high fidelity dynamic geometry and\nappearance, we leverage 2D texture maps defined on the body model as latent\nvariables for predicting residual deformations and the dynamic appearance.\nExperiments demonstrate that our method achieves better quality than the\nstate-of-the-arts on playback as well as novel pose synthesis, and can even\ngeneralize well to new poses that starkly differ from the training poses.\nFurthermore, our method also supports body shape control of the synthesized\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:40:48 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Liu", "Lingjie", ""], ["Habermann", "Marc", ""], ["Rudnev", "Viktor", ""], ["Sarkar", "Kripasindhu", ""], ["Gu", "Jiatao", ""], ["Theobalt", "Christian", ""]]}, {"id": "2106.02285", "submitter": "Zheng-Ning Liu", "authors": "Shi-Min Hu, Zheng-Ning Liu, Meng-Hao Guo, Jun-Xiong Cai, Jiahui Huang,\n  Tai-Jiang Mu, Ralph R. Martin", "title": "Subdivision-Based Mesh Convolution Networks", "comments": "Codes are available in https://github.com/lzhengning/SubdivNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have made great breakthroughs in 2D\ncomputer vision. However, the irregular structure of meshes makes it hard to\nexploit the power of CNNs directly. A subdivision surface provides a\nhierarchical multi-resolution structure, and each face in a closed 2-manifold\ntriangle mesh is exactly adjacent to three faces. Motivated by these two\nproperties, this paper introduces a novel and flexible CNN framework, named\nSubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.\nMaking an analogy between mesh faces and pixels in a 2D image allows us to\npresent a mesh convolution operator to aggregate local features from adjacent\nfaces. By exploiting face neighborhoods, this convolution can support standard\n2D convolutional network concepts, e.g. variable kernel size, stride, and\ndilation. Based on the multi-resolution hierarchy, we propose a spatial uniform\npooling layer which merges four faces into one and an upsampling method which\nsplits one face into four. As a result, many popular 2D CNN architectures can\nbe readily adapted to processing 3D meshes. Meshes with arbitrary connectivity\ncan be remeshed to hold Loop subdivision sequence connectivity via\nself-parameterization, making SubdivNet a general approach. Experiments on mesh\nclassification, segmentation, correspondence, and retrieval from the real-world\ndemonstrate the effectiveness and efficiency of SubdivNet.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:50:34 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hu", "Shi-Min", ""], ["Liu", "Zheng-Ning", ""], ["Guo", "Meng-Hao", ""], ["Cai", "Jun-Xiong", ""], ["Huang", "Jiahui", ""], ["Mu", "Tai-Jiang", ""], ["Martin", "Ralph R.", ""]]}, {"id": "2106.02335", "submitter": "Mikkel Abrahamsen", "authors": "Mikkel Abrahamsen", "title": "Covering Polygons is Even Harder", "comments": "41 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon\n$\\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex\npolygons whose union is $\\mathcal P$. It is known that MCC is\n$\\mathsf{NP}$-hard [Culberson & Reckhow: Covering polygons is hard, FOCS\n1988/Journal of Algorithms 1994] and in $\\exists\\mathbb{R}$ [O'Rourke: The\ncomplexity of computing minimum convex covers for polygons, Allerton 1982]. We\nprove that MCC is $\\exists\\mathbb{R}$-hard, and the problem is thus\n$\\exists\\mathbb{R}$-complete. In other words, the problem is equivalent to\ndeciding whether a system of polynomial equations and inequalities with integer\ncoefficients has a real solution.\n  If a cover for our constructed polygon exists, then so does a cover\nconsisting entirely of triangles. As a byproduct, we therefore also establish\nthat it is $\\exists\\mathbb{R}$-complete to decide whether $k$ triangles cover a\ngiven polygon.\n  The issue that it was not known if finding a minimum cover is in\n$\\mathsf{NP}$ has repeatedly been raised in the literature, and it was\nmentioned as a \"long-standing open question\" already in 2001 [Eidenbenz &\nWidmayer: An approximation algorithm for minimum convex cover with logarithmic\nperformance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that\nassuming the widespread belief that $\\mathsf{NP}\\neq\\exists\\mathbb{R}$, the\nproblem is not in $\\mathsf{NP}$.\n  An implication of the result is that many natural approaches to finding small\ncovers are bound to give suboptimal solutions in some cases, since irrational\ncoordinates of arbitrarily high algebraic degree can be needed for the corners\nof the pieces in an optimal solution.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:29:48 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Abrahamsen", "Mikkel", ""]]}, {"id": "2106.02634", "submitter": "Vincent Sitzmann", "authors": "Vincent Sitzmann, Semon Rezchikov, William T. Freeman, Joshua B.\n  Tenenbaum, Fredo Durand", "title": "Light Field Networks: Neural Scene Representations with\n  Single-Evaluation Rendering", "comments": "First two authors contributed equally. Project website:\n  https://vsitzmann.github.io/lfns/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring representations of 3D scenes from 2D observations is a fundamental\nproblem of computer graphics, computer vision, and artificial intelligence.\nEmerging 3D-structured neural scene representations are a promising approach to\n3D scene understanding. In this work, we propose a novel neural scene\nrepresentation, Light Field Networks or LFNs, which represent both geometry and\nappearance of the underlying 3D scene in a 360-degree, four-dimensional light\nfield parameterized via a neural implicit representation. Rendering a ray from\nan LFN requires only a *single* network evaluation, as opposed to hundreds of\nevaluations per ray for ray-marching or volumetric based renderers in\n3D-structured neural scene representations. In the setting of simple scenes, we\nleverage meta-learning to learn a prior over LFNs that enables multi-view\nconsistent light field reconstruction from as little as a single image\nobservation. This results in dramatic reductions in time and memory complexity,\nand enables real-time rendering. The cost of storing a 360-degree light field\nvia an LFN is two orders of magnitude lower than conventional methods such as\nthe Lumigraph. Utilizing the analytical differentiability of neural implicit\nrepresentations and a novel parameterization of light space, we further\ndemonstrate the extraction of sparse depth maps from LFNs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:54:49 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Sitzmann", "Vincent", ""], ["Rezchikov", "Semon", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""], ["Durand", "Fredo", ""]]}, {"id": "2106.02711", "submitter": "Wamiq Reyaz Para", "authors": "Wamiq Reyaz Para, Shariq Farooq Bhat, Paul Guerrero, Tom Kelly, Niloy\n  Mitra, Leonidas Guibas, Peter Wonka", "title": "SketchGen: Generating Constrained CAD Sketches", "comments": "21 pages, 12 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-aided design (CAD) is the most widely used modeling approach for\ntechnical design. The typical starting point in these designs is 2D sketches\nwhich can later be extruded and combined to obtain complex three-dimensional\nassemblies. Such sketches are typically composed of parametric primitives, such\nas points, lines, and circular arcs, augmented with geometric constraints\nlinking the primitives, such as coincidence, parallelism, or orthogonality.\nSketches can be represented as graphs, with the primitives as nodes and the\nconstraints as edges. Training a model to automatically generate CAD sketches\ncan enable several novel workflows, but is challenging due to the complexity of\nthe graphs and the heterogeneity of the primitives and constraints. In\nparticular, each type of primitive and constraint may require a record of\ndifferent size and parameter types. We propose SketchGen as a generative model\nbased on a transformer architecture to address the heterogeneity problem by\ncarefully designing a sequential language for the primitives and constraints\nthat allows distinguishing between different primitive or constraint types and\ntheir parameters, while encouraging our model to re-use information across\nrelated parameters, encoding shared structure. A particular highlight of our\nwork is the ability to produce primitives linked via constraints that enables\nthe final output to be further regularized via a constraint solver. We evaluate\nour model by demonstrating constraint prediction for given sets of primitives\nand full sketch generation from scratch, showing that our approach\nsignificantly out performs the state-of-the-art in CAD sketch generation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:45:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Para", "Wamiq Reyaz", ""], ["Bhat", "Shariq Farooq", ""], ["Guerrero", "Paul", ""], ["Kelly", "Tom", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas", ""], ["Wonka", "Peter", ""]]}, {"id": "2106.03437", "submitter": "Kaizhi Yang", "authors": "Kaizhi Yang and Xuejin Chen", "title": "Unsupervised Learning for Cuboid Shape Abstraction via Joint\n  Segmentation from Point Clouds", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3450626.3459873", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing complex 3D objects as simple geometric primitives, known as\nshape abstraction, is important for geometric modeling, structural analysis,\nand shape synthesis. In this paper, we propose an unsupervised shape\nabstraction method to map a point cloud into a compact cuboid representation.\nWe jointly predict cuboid allocation as part segmentation and cuboid shapes and\nenforce the consistency between the segmentation and shape abstraction for\nself-learning. For the cuboid abstraction task, we transform the input point\ncloud into a set of parametric cuboids using a variational auto-encoder\nnetwork. The segmentation network allocates each point into a cuboid\nconsidering the point-cuboid affinity. Without manual annotations of parts in\npoint clouds, we design four novel losses to jointly supervise the two branches\nin terms of geometric similarity and cuboid compactness. We evaluate our method\non multiple shape collections and demonstrate its superiority over existing\nshape abstraction methods. Moreover, based on our network architecture and\nlearned representations, our approach supports various applications including\nstructured shape generation, shape interpolation, and structural shape\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:15:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yang", "Kaizhi", ""], ["Chen", "Xuejin", ""]]}, {"id": "2106.03452", "submitter": "Songyou Peng", "authors": "Songyou Peng, Chiyu \"Max\" Jiang, Yiyi Liao, Michael Niemeyer, Marc\n  Pollefeys, Andreas Geiger", "title": "Shape As Points: A Differentiable Poisson Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, neural implicit representations gained popularity in 3D\nreconstruction due to their expressiveness and flexibility. However, the\nimplicit nature of neural implicit representations results in slow inference\ntime and requires careful initialization. In this paper, we revisit the classic\nyet ubiquitous point cloud representation and introduce a differentiable\npoint-to-mesh layer using a differentiable formulation of Poisson Surface\nReconstruction (PSR) that allows for a GPU-accelerated fast solution of the\nindicator function given an oriented point cloud. The differentiable PSR layer\nallows us to efficiently and differentiably bridge the explicit 3D point\nrepresentation with the 3D mesh via the implicit indicator field, enabling\nend-to-end optimization of surface reconstruction metrics such as Chamfer\ndistance. This duality between points and meshes hence allows us to represent\nshapes as oriented point clouds, which are explicit, lightweight and\nexpressive. Compared to neural implicit representations, our Shape-As-Points\n(SAP) model is more interpretable, lightweight, and accelerates inference time\nby one order of magnitude. Compared to other explicit representations such as\npoints, patches, and meshes, SAP produces topology-agnostic, watertight\nmanifold surfaces. We demonstrate the effectiveness of SAP on the task of\nsurface reconstruction from unoriented point clouds and learning-based\nreconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 09:28:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Peng", "Songyou", ""], ["Jiang", "Chiyu \"Max\"", ""], ["Liao", "Yiyi", ""], ["Niemeyer", "Michael", ""], ["Pollefeys", "Marc", ""], ["Geiger", "Andreas", ""]]}, {"id": "2106.03804", "submitter": "Daniel Rebain", "authors": "Daniel Rebain, Ke Li, Vincent Sitzmann, Soroosh Yazdani, Kwang Moo Yi,\n  Andrea Tagliasacchi", "title": "Deep Medial Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit representations of geometry, such as occupancy fields or signed\ndistance fields (SDF), have recently re-gained popularity in encoding 3D solid\nshape in a functional form. In this work, we introduce medial fields: a field\nfunction derived from the medial axis transform (MAT) that makes available\ninformation about the underlying 3D geometry that is immediately useful for a\nnumber of downstream tasks. In particular, the medial field encodes the local\nthickness of a 3D shape, and enables O(1) projection of a query point onto the\nmedial axis. To construct the medial field we require nothing but the SDF of\nthe shape itself, thus allowing its straightforward incorporation in any\napplication that relies on signed distance fields. Working in unison with the\nO(1) surface projection supported by the SDF, the medial field opens the door\nfor an entirely new set of efficient, shape-aware operations on implicit\nrepresentations. We present three such applications, including a modification\nto sphere tracing that renders implicit representations with better convergence\nproperties, a fast construction method for memory-efficient rigid-body\ncollision proxies, and an efficient approximation of ambient occlusion that\nremains stable with respect to viewpoint variations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:15:38 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rebain", "Daniel", ""], ["Li", "Ke", ""], ["Sitzmann", "Vincent", ""], ["Yazdani", "Soroosh", ""], ["Yi", "Kwang Moo", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "2106.03847", "submitter": "Omri Avrahami", "authors": "Omri Avrahami, Dani Lischinski, Ohad Fried", "title": "GAN Cocktail: mixing GANs without dataset access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Today's generative models are capable of synthesizing high-fidelity images,\nbut each model specializes on a specific target domain. This raises the need\nfor model merging: combining two or more pretrained generative models into a\nsingle unified one. In this work we tackle the problem of model merging, given\ntwo constraints that often come up in the real world: (1) no access to the\noriginal training data, and (2) without increasing the size of the neural\nnetwork. To the best of our knowledge, model merging under these constraints\nhas not been studied thus far. We propose a novel, two-stage solution. In the\nfirst stage, we transform the weights of all the models to the same parameter\nspace by a technique we term model rooting. In the second stage, we merge the\nrooted models by averaging their weights and fine-tuning them for each specific\ndomain, using only data generated by the original trained models. We\ndemonstrate that our approach is superior to baseline methods and to existing\ntransfer learning techniques, and investigate several applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:59:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Avrahami", "Omri", ""], ["Lischinski", "Dani", ""], ["Fried", "Ohad", ""]]}, {"id": "2106.03988", "submitter": "Zohreh Shaghaghian", "authors": "Zohreh Shaghaghian, Wei Yan, Dezhen Song", "title": "Towards Learning Geometric Transformations through Play: An AR-powered\n  approach", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3463914.3463915", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the excessive developments of architectural parametric platforms,\nparametric design is often interpreted as an architectural style rather than a\ncomputational method. Also, the problem is still a lack of knowledge and skill\nabout the technical application of parametric design in architectural\nmodelling. Students often dive into utilizing complex digital modelling without\nhaving a competent pedagogical context to learn algorithmic thinking and the\ncorresponding logic behind digital and parametric modelling. The insufficient\nskills and superficial knowledge often result in utilizing the modelling\nsoftware through trial and error, not taking full advantage of what it has to\noffer. Geometric transformations as the fundamental functions of parametric\nmodelling is explored in this study to anchor learning essential components in\nparametric modelling. Students need to understand the differences between\nvariables, parameters, functions and their relations. Fologram, an Augmented\nReality tool, is utilized in this study to learn geometric transformation and\nits components in an intuitive way. A LEGO set is used as an editable physical\nmodel to improve spatial skill through hand movement beside an instant feedback\nin the physical environment.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:09:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shaghaghian", "Zohreh", ""], ["Yan", "Wei", ""], ["Song", "Dezhen", ""]]}, {"id": "2106.04004", "submitter": "Jiaman Li", "authors": "Jiaman Li, Ruben Villegas, Duygu Ceylan, Jimei Yang, Zhengfei Kuang,\n  Hao Li, Yajie Zhao", "title": "Task-Generic Hierarchical Human Motion Prior using VAEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep generative model that describes human motions can benefit a wide range\nof fundamental computer vision and graphics tasks, such as providing robustness\nto video-based human pose estimation, predicting complete body movements for\nmotion capture systems during occlusions, and assisting key frame animation\nwith plausible movements. In this paper, we present a method for learning\ncomplex human motions independent of specific tasks using a combined global and\nlocal latent space to facilitate coarse and fine-grained modeling.\nSpecifically, we propose a hierarchical motion variational autoencoder (HM-VAE)\nthat consists of a 2-level hierarchical latent space. While the global latent\nspace captures the overall global body motion, the local latent space enables\nto capture the refined poses of the different body parts. We demonstrate the\neffectiveness of our hierarchical motion variational autoencoder in a variety\nof tasks including video-based human pose estimation, motion completion from\npartial observations, and motion synthesis from sparse key-frames. Even though,\nour model has not been trained for any of these tasks specifically, it provides\nsuperior performance than task-specific alternatives. Our general-purpose human\nmotion prior model can fix corrupted human body animations and generate\ncomplete movements from incomplete observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 23:11:42 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Jiaman", ""], ["Villegas", "Ruben", ""], ["Ceylan", "Duygu", ""], ["Yang", "Jimei", ""], ["Kuang", "Zhengfei", ""], ["Li", "Hao", ""], ["Zhao", "Yajie", ""]]}, {"id": "2106.04521", "submitter": "Dan Reznik", "authors": "Iverton Darlan and Dan Reznik", "title": "An App for Visual Exploration, Discovery, and Sharing of Poncelet\n  3-Periodic Phenomena", "comments": "19 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.DS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a browser-based application built for the real-time visualization\nof the beauteous dynamic geometry of Poncelet 3-periodic families. The focus is\non highly responsive, visually smooth, \"live\" experimentation with old and new\nphenomena involving loci of triangle centers and/or metric invariants. Another\nfocus is on the production of beautiful color-filled images of loci. Once a\nlive browser-based simulation is defined, it can be easily shared with\ncolleagues and/or inserted as links in publications, eliminating the need of\ntime-consuming video production and uploads.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:59:19 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:15:25 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Darlan", "Iverton", ""], ["Reznik", "Dan", ""]]}, {"id": "2106.04912", "submitter": "I-Chao Shen", "authors": "I-Chao Shen, Bing-Yu Chen", "title": "ClipGen: A Deep Generative Model for Clipart Vectorization and Synthesis", "comments": "15 pages, TVCG2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3084944 10.1109/TVCG.2021.3084944\n  10.1109/TVCG.2021.3084944 10.1109/TVCG.2021.3084944 10.1109/TVCG.2021.3084944", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning-based approach for automatically\nvectorizing and synthesizing the clipart of man-made objects. Given a raster\nclipart image and its corresponding object category (e.g., airplanes), the\nproposed method sequentially generates new layers, each of which is composed of\na new closed path filled with a single color. The final result is obtained by\ncompositing all layers together into a vector clipart image that falls into the\ntarget category. The proposed approach is based on an iterative generative\nmodel that (i) decides whether to continue synthesizing a new layer and (ii)\ndetermines the geometry and appearance of the new layer. We formulated a joint\nloss function for training our generative model, including the shape\nsimilarity, symmetry, and local curve smoothness losses, as well as vector\ngraphics rendering accuracy loss for synthesizing clipart recognizable by\nhumans. We also introduced a collection of man-made object clipart, ClipNet,\nwhich is composed of closed-path layers, and two designed preprocessing tasks\nto clean up and enrich the original raw clipart. To validate the proposed\napproach, we conducted several experiments and demonstrated its ability to\nvectorize and synthesize various clipart categories. We envision that our\ngenerative model can facilitate efficient and intuitive clipart designs for\nnovice users and graphic designers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:48:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Shen", "I-Chao", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "2106.05143", "submitter": "Bruno Roy", "authors": "Bruno Roy, Pierre Poulin, and Eric Paquette", "title": "Neural UpFlow: A Scene Flow Learning Approach to Increase the Apparent\n  Resolution of Particle-Based Liquids", "comments": "14 pages, 18 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel up-resing technique for generating high-resolution liquids\nbased on scene flow estimation using deep neural networks. Our approach infers\nand synthesizes small- and large-scale details solely from a low-resolution\nparticle-based liquid simulation. The proposed network leverages neighborhood\ncontributions to encode inherent liquid properties throughout convolutions. We\nalso propose a particle-based approach to interpolate between liquids generated\nfrom varying simulation discretizations using a state-of-the-art bidirectional\noptical flow solver method for fluids in addition to a novel key-event\ntopological alignment constraint. In conjunction with the neighborhood\ncontributions, our loss formulation allows the inference model throughout\nepochs to reward important differences in regard to significant gaps in\nsimulation discretizations. Even when applied in an untested simulation setup,\nour approach is able to generate plausible high-resolution details. Using this\ninterpolation approach and the predicted displacements, our approach combines\nthe input liquid properties with the predicted motion to infer semi-Lagrangian\nadvection. We furthermore showcase how the proposed interpolation approach can\nfacilitate generating large simulation datasets with a subset of initial\ncondition parameters.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:36:23 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Roy", "Bruno", ""], ["Poulin", "Pierre", ""], ["Paquette", "Eric", ""]]}, {"id": "2106.05161", "submitter": "Seungbae Bang", "authors": "Rinat Abdrashitov, Seungbae Bang, David I.W. Levin, Karan Singh, Alec\n  Jacobson", "title": "Interactive Modelling of Volumetric Musculoskeletal Anatomy", "comments": "13 pages, 20 figures, SIGGRAPH 2021", "journal-ref": "ACM Trans. Graph., Vol. 40, No. 4, Article 122. Publication date:\n  August 2021", "doi": "10.1145/3450626.3459769", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new approach for modelling musculoskeletal anatomy. Unlike\nprevious methods, we do not model individual muscle shapes as geometric\nprimitives (polygonal meshes, NURBS etc.). Instead, we adopt a volumetric\nsegmentation approach where every point in our volume is assigned to a muscle,\nfat, or bone tissue. We provide an interactive modelling tool where the user\ncontrols the segmentation via muscle curves and we visualize the muscle shapes\nusing volumetric rendering. Muscle curves enable intuitive yet powerful control\nover the muscle shapes. This representation allows us to automatically handle\nintersections between different tissues (musclemuscle, muscle-bone, and\nmuscle-skin) during the modelling and automates computation of muscle fiber\nfields. We further introduce a novel algorithm for converting the volumetric\nmuscle representation into tetrahedral or surface geometry for use in\ndownstream tasks. Additionally, we introduce an interactive skeleton authoring\ntool that allows the users to create skeletal anatomy starting from only a skin\nmesh using a library of bone parts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:08:56 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Abdrashitov", "Rinat", ""], ["Bang", "Seungbae", ""], ["Levin", "David I. W.", ""], ["Singh", "Karan", ""], ["Jacobson", "Alec", ""]]}, {"id": "2106.05187", "submitter": "Wang Yifan", "authors": "Wang Yifan, Lukas Rahmann, Olga Sorkine-Hornung", "title": "Geometry-Consistent Neural Shape Representation with Implicit\n  Displacement Fields", "comments": "includes supplementary; ver2 corrected typos in eq(1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present implicit displacement fields, a novel representation for detailed\n3D geometry. Inspired by a classic surface deformation technique, displacement\nmapping, our method represents a complex surface as a smooth base surface plus\na displacement along the base's normal directions, resulting in a\nfrequency-based shape decomposition, where the high frequency signal is\nconstrained geometrically by the low frequency signal. Importantly, this\ndisentanglement is unsupervised thanks to a tailored architectural design that\nhas an innate frequency hierarchy by construction. We explore implicit\ndisplacement field surface reconstruction and detail transfer and demonstrate\nsuperior representational power, training stability and generalizability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:26:18 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 09:25:15 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yifan", "Wang", ""], ["Rahmann", "Lukas", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "2106.05264", "submitter": "Relja Arandjelovi\\'c", "authors": "Relja Arandjelovi\\'c, Andrew Zisserman", "title": "NeRF in detail: Learning to sample for view synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural radiance fields (NeRF) methods have demonstrated impressive novel view\nsynthesis performance. The core approach is to render individual rays by\nquerying a neural network at points sampled along the ray to obtain the density\nand colour of the sampled points, and integrating this information using the\nrendering equation. Since dense sampling is computationally prohibitive, a\ncommon solution is to perform coarse-to-fine sampling.\n  In this work we address a clear limitation of the vanilla coarse-to-fine\napproach -- that it is based on a heuristic and not trained end-to-end for the\ntask at hand. We introduce a differentiable module that learns to propose\nsamples and their importance for the fine network, and consider and compare\nmultiple alternatives for its neural architecture. Training the proposal module\nfrom scratch can be unstable due to lack of supervision, so an effective\npre-training strategy is also put forward. The approach, named `NeRF in detail'\n(NeRF-ID), achieves superior view synthesis quality over NeRF and the\nstate-of-the-art on the synthetic Blender benchmark and on par or better\nperformance on the real LLFF-NeRF scenes. Furthermore, by leveraging the\npredicted sample importance, a 25% saving in computation can be achieved\nwithout significantly sacrificing the rendering quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:59:10 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2106.05306", "submitter": "Yifei Li", "authors": "Yifei Li, Tao Du, Kui Wu, Jie Xu, Wojciech Matusik", "title": "DiffCloth: Differentiable Cloth Simulation with Dry Frictional Contact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloth simulation has wide applications including computer animation, garment\ndesign, and robot-assisted dressing. In this work, we present a differentiable\ncloth simulator whose additional gradient information facilitates cloth-related\napplications. Our differentiable simulator extends the state-of-the-art cloth\nsimulator based on Projective Dynamics and with dry frictional contact governed\nby the Signorini-Coulomb law. We derive gradients with contact in this forward\nsimulation framework and speed up the computation with Jacobi iteration\ninspired by previous differentiable simulation work. To our best knowledge, we\npresent the first differentiable cloth simulator with the Coulomb law of\nfriction. We demonstrate the efficacy of our simulator in various applications,\nincluding system identification, manipulation, inverse design, and a\nreal-to-sim task. Many of our applications have not been demonstrated in\nprevious differentiable cloth simulators. The gradient information from our\nsimulator enables efficient gradient-based task solvers from which we observe a\nsubstantial speedup over standard gradient-free methods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 18:02:10 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Li", "Yifei", ""], ["Du", "Tao", ""], ["Wu", "Kui", ""], ["Xu", "Jie", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2106.05375", "submitter": "Madhawa Vidanapathirana", "authors": "Madhawa Vidanapathirana, Qirui Wu, Yasutaka Furukawa, Angel X. Chang\n  and Manolis Savva", "title": "Plan2Scene: Converting Floorplans to 3D Scenes", "comments": "This paper is accepted to CVPR 2021. For code, data and pretrained\n  models, see https://3dlg-hcvc.github.io/plan2scene/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of converting a floorplan and a set of associated photos\nof a residence into a textured 3D mesh model, a task which we call Plan2Scene.\nOur system 1) lifts a floorplan image to a 3D mesh model; 2) synthesizes\nsurface textures based on the input photos; and 3) infers textures for\nunobserved surfaces using a graph neural network architecture. To train and\nevaluate our system we create indoor surface texture datasets, and augment a\ndataset of floorplans and photos from prior work with rectified surface crops\nand additional annotations. Our approach handles the challenge of producing\ntileable textures for dominant surfaces such as floors, walls, and ceilings\nfrom a sparse set of unaligned photos that only partially cover the residence.\nQualitative and quantitative evaluations show that our system produces\nrealistic 3D interior models, outperforming baseline approaches on a suite of\ntexture quality metrics and as measured by a holistic user study.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 20:32:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vidanapathirana", "Madhawa", ""], ["Wu", "Qirui", ""], ["Furukawa", "Yasutaka", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""]]}, {"id": "2106.05429", "submitter": "Jakob Weiss", "authors": "Jakob Weiss, Nassir Navab", "title": "Deep Direct Volume Rendering: Learning Visual Feature Mappings From\n  Exemplary Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Volume Rendering is an important technique for visualizing three-dimensional\nscalar data grids and is commonly employed for scientific and medical image\ndata. Direct Volume Rendering (DVR) is a well established and efficient\nrendering algorithm for volumetric data. Neural rendering uses deep neural\nnetworks to solve inverse rendering tasks and applies techniques similar to\nDVR. However, it has not been demonstrated successfully for the rendering of\nscientific volume data.\n  In this work, we introduce Deep Direct Volume Rendering (DeepDVR), a\ngeneralization of DVR that allows for the integration of deep neural networks\ninto the DVR algorithm. We conceptualize the rendering in a latent color space,\nthus enabling the use of deep architectures to learn implicit mappings for\nfeature extraction and classification, replacing explicit feature design and\nhand-crafted transfer functions. Our generalization serves to derive novel\nvolume rendering architectures that can be trained end-to-end directly from\nexamples in image space, obviating the need to manually define and fine-tune\nmultidimensional transfer functions while providing superior classification\nstrength. We further introduce a novel stepsize annealing scheme to accelerate\nthe training of DeepDVR models and validate its effectiveness in a set of\nexperiments. We validate our architectures on two example use cases: (1)\nlearning an optimized rendering from manually adjusted reference images for a\nsingle volume and (2) learning advanced visualization concepts like shading and\nsemantic colorization that generalize to unseen volume data.\n  We find that deep volume rendering architectures with explicit modeling of\nthe DVR pipeline effectively enable end-to-end learning of scientific volume\nrendering tasks from target images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 23:03:00 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Weiss", "Jakob", ""], ["Navab", "Nassir", ""]]}, {"id": "2106.05779", "submitter": "Tejan Karmali", "authors": "Rahul Venkatesh, Tejan Karmali, Sarthak Sharma, Aurobrata Ghosh, R.\n  Venkatesh Babu, L\\'aszl\\'o A. Jeni, Maneesh Singh", "title": "Deep Implicit Surface Point Prediction Networks", "comments": "22 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural representations of 3D shapes as implicit functions have been\nshown to produce high fidelity models surpassing the resolution-memory\ntrade-off faced by the explicit representations using meshes and point clouds.\nHowever, most such approaches focus on representing closed shapes. Unsigned\ndistance function (UDF) based approaches have been proposed recently as a\npromising alternative to represent both open and closed shapes. However, since\nthe gradients of UDFs vanish on the surface, it is challenging to estimate\nlocal (differential) geometric properties like the normals and tangent planes\nwhich are needed for many downstream applications in vision and graphics. There\nare additional challenges in computing these properties efficiently with a\nlow-memory footprint. This paper presents a novel approach that models such\nsurfaces using a new class of implicit representations called the closest\nsurface-point (CSP) representation. We show that CSP allows us to represent\ncomplex surfaces of any topology (open or closed) with high fidelity. It also\nallows for accurate and efficient computation of local geometric properties. We\nfurther demonstrate that it leads to efficient implementation of downstream\nalgorithms like sphere-tracing for rendering the 3D surface as well as to\ncreate explicit mesh-based representations. Extensive experimental evaluation\non the ShapeNet dataset validate the above contributions with results\nsurpassing the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:31:54 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 03:26:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Venkatesh", "Rahul", ""], ["Karmali", "Tejan", ""], ["Sharma", "Sarthak", ""], ["Ghosh", "Aurobrata", ""], ["Babu", "R. Venkatesh", ""], ["Jeni", "L\u00e1szl\u00f3 A.", ""], ["Singh", "Maneesh", ""]]}, {"id": "2106.06533", "submitter": "Anand Bhattad", "authors": "Anand Bhattad, Aysegul Dundar, Guilin Liu, Andrew Tao, Bryan Catanzaro", "title": "View Generalization for Single Image Textured 3D Models", "comments": "CVPR 2021. Project website:\n  https://nv-adlr.github.io/view-generalization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can easily infer the underlying 3D geometry and texture of an object\nonly from a single 2D image. Current computer vision methods can do this, too,\nbut suffer from view generalization problems - the models inferred tend to make\npoor predictions of appearance in novel views. As for generalization problems\nin machine learning, the difficulty is balancing single-view accuracy (cf.\ntraining error; bias) with novel view accuracy (cf. test error; variance). We\ndescribe a class of models whose geometric rigidity is easily controlled to\nmanage this tradeoff. We describe a cycle consistency loss that improves view\ngeneralization (roughly, a model from a generated view should predict the\noriginal view well). View generalization of textures requires that models share\ntexture information, so a car seen from the back still has headlights because\nother cars have headlights. We describe a cycle consistency loss that\nencourages model textures to be aligned, so as to encourage sharing. We compare\nour method against the state-of-the-art method and show both qualitative and\nquantitative improvements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:59:57 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Bhattad", "Anand", ""], ["Dundar", "Aysegul", ""], ["Liu", "Guilin", ""], ["Tao", "Andrew", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2106.06561", "submitter": "Min Jin Chong", "authors": "Min Jin Chong, David Forsyth", "title": "GANs N' Roses: Stable, Controllable, Diverse Image to Image Translation\n  (works for videos too!)", "comments": "code is here https://github.com/mchong6/GANsNRoses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to learn a map that takes a content code, derived from a face\nimage, and a randomly chosen style code to an anime image. We derive an\nadversarial loss from our simple and effective definitions of style and\ncontent. This adversarial loss guarantees the map is diverse -- a very wide\nrange of anime can be produced from a single content code. Under plausible\nassumptions, the map is not just diverse, but also correctly represents the\nprobability of an anime, conditioned on an input face. In contrast, current\nmultimodal generation procedures cannot capture the complex styles that appear\nin anime. Extensive quantitative experiments support the idea the map is\ncorrect. Extensive qualitative results show that the method can generate a much\nmore diverse range of styles than SOTA comparisons. Finally, we show that our\nformalization of content and style allows us to perform video to video\ntranslation without ever training on videos.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:23:00 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chong", "Min Jin", ""], ["Forsyth", "David", ""]]}, {"id": "2106.06588", "submitter": "Sophia Henn", "authors": "Sophia Henn, Abigail Sticha, Timothy Burley, Ernesto Verdeja, Paul\n  Brenner", "title": "Visualization Techniques to Enhance Automated Event Extraction", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust visualization of complex data is critical for the effective use of NLP\nfor event classification, as the volume of data is large and the\nhigh-dimensional structure of text makes data challenging to summarize\nsuccinctly. In event extraction tasks in particular, visualization can aid in\nunderstanding and illustrating the textual relationships from which machine\nlearning tools produce insights. Through our case study which seeks to identify\npotential triggers of state-led mass killings from news articles using NLP, we\ndemonstrate how visualizations can aid in each stage, from exploratory analysis\nof raw data, to machine learning training analysis, and finally post-inference\nvalidation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:24:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Henn", "Sophia", ""], ["Sticha", "Abigail", ""], ["Burley", "Timothy", ""], ["Verdeja", "Ernesto", ""], ["Brenner", "Paul", ""]]}, {"id": "2106.06593", "submitter": "Min Jin Chong", "authors": "Kedan Li, Min jin Chong, Jeffrey Zhang, Jingen Liu", "title": "Toward Accurate and Realistic Outfits Visualization with Attention to\n  Details", "comments": "Accepted to CVPR2021. Live demo here https://revery.ai/demo.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual try-on methods aim to generate images of fashion models wearing\narbitrary combinations of garments. This is a challenging task because the\ngenerated image must appear realistic and accurately display the interaction\nbetween garments. Prior works produce images that are filled with artifacts and\nfail to capture important visual details necessary for commercial applications.\nWe propose Outfit Visualization Net (OVNet) to capture these important details\n(e.g. buttons, shading, textures, realistic hemlines, and interactions between\ngarments) and produce high quality multiple-garment virtual try-on images.\nOVNet consists of 1) a semantic layout generator and 2) an image generation\npipeline using multiple coordinated warps. We train the warper to output\nmultiple warps using a cascade loss, which refines each successive warp to\nfocus on poorly generated regions of a previous warp and yields consistent\nimprovements in detail. In addition, we introduce a method for matching outfits\nwith the most suitable model and produce significant improvements for both our\nand other previous try-on methods. Through quantitative and qualitative\nanalysis, we demonstrate our method generates substantially higher-quality\nstudio images compared to prior works for multi-garment outfits. An interactive\ninterface powered by this method has been deployed on fashion e-commerce\nwebsites and received overwhelmingly positive feedback.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 19:53:34 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Kedan", ""], ["Chong", "Min jin", ""], ["Zhang", "Jeffrey", ""], ["Liu", "Jingen", ""]]}, {"id": "2106.06807", "submitter": "Niall Williams", "authors": "Niall L. Williams, Aniket Bera, Dinesh Manocha", "title": "Redirected Walking in Static and Dynamic Scenes Using Visibility\n  Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new approach for redirected walking in static and dynamic scenes\nthat uses techniques from robot motion planning to compute the redirection\ngains that steer the user on collision-free paths in the physical space. Our\nfirst contribution is a mathematical framework for redirected walking using\nconcepts from motion planning and configuration spaces. This framework\nhighlights various geometric and perceptual constraints that tend to make\ncollision-free redirected walking difficult. We use our framework to propose an\nefficient solution to the redirection problem that uses the notion of\nvisibility polygons to compute the free spaces in the physical environment and\nthe virtual environment. The visibility polygon provides a concise\nrepresentation of the entire space that is visible, and therefore walkable, to\nthe user from their position within an environment. Using this representation\nof walkable space, we apply redirected walking to steer the user to regions of\nthe visibility polygon in the physical environment that closely match the\nregion that the user occupies in the visibility polygon in the virtual\nenvironment. We show that our algorithm is able to steer the user along paths\nthat result in significantly fewer resets than existing state-of-the-art\nalgorithms in both static and dynamic scenes. Our project website is available\nat https://gamma.umd.edu/vis_poly/.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 16:12:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:12:57 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 20:12:49 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Williams", "Niall L.", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2106.06866", "submitter": "Pradyumna Reddy", "authors": "Pradyumna Reddy, Zhifei Zhang, Matthew Fisher, Hailin Jin, Zhaowen\n  Wang, Niloy J. Mitra", "title": "A Multi-Implicit Neural Representation for Fonts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Fonts are ubiquitous across documents and come in a variety of styles. They\nare either represented in a native vector format or rasterized to produce fixed\nresolution images. In the first case, the non-standard representation prevents\nbenefiting from latest network architectures for neural representations; while,\nin the latter case, the rasterized representation, when encoded via networks,\nresults in loss of data fidelity, as font-specific discontinuities like edges\nand corners are difficult to represent using neural networks. Based on the\nobservation that complex fonts can be represented by a superposition of a set\nof simpler occupancy functions, we introduce \\textit{multi-implicits} to\nrepresent fonts as a permutation-invariant set of learned implict functions,\nwithout losing features (e.g., edges and corners). However, while\nmulti-implicits locally preserve font features, obtaining supervision in the\nform of ground truth multi-channel signals is a problem in itself. Instead, we\npropose how to train such a representation with only local supervision, while\nthe proposed neural architecture directly finds globally consistent\nmulti-implicits for font families. We extensively evaluate the proposed\nrepresentation for various tasks including reconstruction, interpolation, and\nsynthesis to demonstrate clear advantages with existing alternatives.\nAdditionally, the representation naturally enables glyph completion, wherein a\nsingle characteristic font is used to synthesize a whole font family in the\ntarget style.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 21:40:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Reddy", "Pradyumna", ""], ["Zhang", "Zhifei", ""], ["Fisher", "Matthew", ""], ["Jin", "Hailin", ""], ["Wang", "Zhaowen", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2106.07718", "submitter": "Wilson Est\\'ecio Marc\\'ilio J\\'unior", "authors": "Wilson E. Marc\\'ilio-Jr and Danilo M. Eler and Fernando V. Paulovich\n  and Rafael M. Martins", "title": "HUMAP: Hierarchical Uniform Manifold Approximation and Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) techniques help analysts to understand patterns\nin high-dimensional spaces. These techniques, often represented by scatter\nplots, are employed in diverse science domains and facilitate similarity\nanalysis among clusters and data samples. For datasets containing many\ngranularities or when analysis follows the information visualization mantra,\nhierarchical DR techniques are the most suitable approach since they present\nmajor structures beforehand and details on demand. However, current\nhierarchical DR techniques are not fully capable of addressing literature\nproblems because they do not preserve the projection mental map across\nhierarchical levels or are not suitable for most data types. This work presents\nHUMAP, a novel hierarchical dimensionality reduction technique designed to be\nflexible on preserving local and global structures and preserve the mental map\nthroughout hierarchical exploration. We provide empirical evidence of our\ntechnique's superiority compared with current hierarchical approaches and show\ntwo case studies to demonstrate its strengths.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:27:54 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Paulovich", "Fernando V.", ""], ["Martins", "Rafael M.", ""]]}, {"id": "2106.07852", "submitter": "Zhenyu Zhang Dr.", "authors": "Zhenyu Zhang, Yanhao Ge, Renwang Chen, Ying Tai, Yan Yan, Jian Yang,\n  Chengjie Wang, Jilin Li, Feiyue Huang", "title": "Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo\n  Collection", "comments": "CVPR 2021 Oral, 11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric face modeling aims to reconstruct 3D face only from images\nwithout shape assumptions. While plausible facial details are predicted, the\nmodels tend to over-depend on local color appearance and suffer from ambiguous\nnoise. To address such problem, this paper presents a novel Learning to\nAggregate and Personalize (LAP) framework for unsupervised robust 3D face\nmodeling. Instead of using controlled environment, the proposed method\nimplicitly disentangles ID-consistent and scene-specific face from\nunconstrained photo set. Specifically, to learn ID-consistent face, LAP\nadaptively aggregates intrinsic face factors of an identity based on a novel\ncurriculum learning approach with relaxed consistency loss. To adapt the face\nfor a personalized scene, we propose a novel attribute-refining network to\nmodify ID-consistent face with target attribute and details. Based on the\nproposed method, we make unsupervised 3D face modeling benefit from meaningful\nimage facial structure and possibly higher resolutions. Extensive experiments\non benchmarks show LAP recovers superior or competitive face shape and texture,\ncompared with state-of-the-art (SOTA) methods with or without prior and\nsupervision.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 03:10:17 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Ge", "Yanhao", ""], ["Chen", "Renwang", ""], ["Tai", "Ying", ""], ["Yan", "Yan", ""], ["Yang", "Jian", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2106.08034", "submitter": "Jose A. Iglesias-Guitian", "authors": "Jose A. Iglesias-Guitian, Prajita Mane and Bochang Moon", "title": "Real-Time Denoising of Volumetric Path Tracing for Direct Volume\n  Rendering", "comments": "13 pages, 19 figures, project page available at\n  http://www.j4lley.com/content/publications/2020-ieee-tvcg-mcdvr-denoising/index.html\n  IEEE Transactions on Visualization and Computer Graphics (2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3037680", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct Volume Rendering (DVR) using Volumetric Path Tracing (VPT) is a\nscientific visualization technique that simulates light transport with objects'\nmatter using physically-based lighting models. Monte Carlo (MC) path tracing is\noften used with surface models, yet its application for volumetric models is\ndifficult due to the complexity of integrating MC light-paths in volumetric\nmedia with none or smooth material boundaries. Moreover, auxiliary\ngeometry-buffers (G-buffers) produced for volumes are typically very noisy,\nfailing to guide image denoisers relying on that information to preserve image\ndetails. This makes existing real-time denoisers, which take noise-free\nG-buffers as their input, less effective when denoising VPT images. We propose\nthe necessary modifications to an image-based denoiser previously used when\nrendering surface models, and demonstrate effective denoising of VPT images. In\nparticular, our denoising exploits temporal coherence between frames, without\nrelying on noise-free G-buffers, which has been a common assumption of existing\ndenoisers for surface-models. Our technique preserves high-frequency details\nthrough a weighted recursive least squares that handles heterogeneous noise for\nvolumetric models. We show for various real data sets that our method improves\nthe visual fidelity and temporal stability of VPT during classic DVR operations\nsuch as camera movements, modifications of the light sources, and editions to\nthe volume transfer function.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:40:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Iglesias-Guitian", "Jose A.", ""], ["Mane", "Prajita", ""], ["Moon", "Bochang", ""]]}, {"id": "2106.08562", "submitter": "Eduardo Pavez", "authors": "Eduardo Pavez, Andre L. Souto, Ricardo L. De Queiroz, Antonio Ortega", "title": "Multi-resolution intra-predictive coding of 3D point cloud attributes", "comments": "5 pages, 5 figures, Accepted at 2021 IEEE International Conference on\n  Image Processing (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an intra frame predictive strategy for compression of 3D point\ncloud attributes. Our approach is integrated with the region adaptive graph\nFourier transform (RAGFT), a multi-resolution transform formed by a composition\nof localized block transforms, which produces a set of low pass (approximation)\nand high pass (detail) coefficients at multiple resolutions. Since the\ntransform operations are spatially localized, RAGFT coefficients at a given\nresolution may still be correlated. To exploit this phenomenon, we propose an\nintra-prediction strategy, in which decoded approximation coefficients are used\nto predict uncoded detail coefficients. The prediction residuals are then\nquantized and entropy coded. For the 8i dataset, we obtain gains up to 0.5db as\ncompared to intra predicted point cloud compresion based on the region adaptive\nHaar transform (RAHT).\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 05:56:26 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pavez", "Eduardo", ""], ["Souto", "Andre L.", ""], ["De Queiroz", "Ricardo L.", ""], ["Ortega", "Antonio", ""]]}, {"id": "2106.09198", "submitter": "Haoran Xie", "authors": "Haoran Xie and Yuki Fujita and Kazunori Miyata", "title": "Learning Perceptual Manifold of Fonts", "comments": "9 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Along the rapid development of deep learning techniques in generative models,\nit is becoming an urgent issue to combine machine intelligence with human\nintelligence to solve the practical applications. Motivated by this\nmethodology, this work aims to adjust the machine generated character fonts\nwith the effort of human workers in the perception study. Although numerous\nfonts are available online for public usage, it is difficult and challenging to\ngenerate and explore a font to meet the preferences for common users. To solve\nthe specific issue, we propose the perceptual manifold of fonts to visualize\nthe perceptual adjustment in the latent space of a generative model of fonts.\nIn our framework, we adopt the variational autoencoder network for the font\ngeneration. Then, we conduct a perceptual study on the generated fonts from the\nmulti-dimensional latent space of the generative model. After we obtained the\ndistribution data of specific preferences, we utilize manifold learning\napproach to visualize the font distribution. In contrast to the conventional\nuser interface in our user study, the proposed font-exploring user interface is\nefficient and helpful in the designated user preference.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:22:52 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xie", "Haoran", ""], ["Fujita", "Yuki", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2106.09486", "submitter": "Demetris Marnerides", "authors": "Demetris Marnerides, Thomas Bashford-Rogers, Kurt Debattista", "title": "Deep HDR Hallucination for Inverse Tone Mapping", "comments": null, "journal-ref": "Sensors 2021, 21, 4032", "doi": "10.3390/s21124032", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Inverse Tone Mapping (ITM) methods attempt to reconstruct High Dynamic Range\n(HDR) information from Low Dynamic Range (LDR) image content. The dynamic range\nof well-exposed areas must be expanded and any missing information due to\nover/under-exposure must be recovered (hallucinated). The majority of methods\nfocus on the former and are relatively successful, while most attempts on the\nlatter are not of sufficient quality, even ones based on Convolutional Neural\nNetworks (CNNs). A major factor for the reduced inpainting quality in some\nworks is the choice of loss function. Work based on Generative Adversarial\nNetworks (GANs) shows promising results for image synthesis and LDR inpainting,\nsuggesting that GAN losses can improve inverse tone mapping results. This work\npresents a GAN-based method that hallucinates missing information from badly\nexposed areas in LDR images and compares its efficacy with alternative\nvariations. The proposed method is quantitatively competitive with\nstate-of-the-art inverse tone mapping methods, providing good dynamic range\nexpansion for well-exposed areas and plausible hallucinations for saturated and\nunder-exposed areas. A density-based normalisation method, targeted for HDR\ncontent, is also proposed, as well as an HDR data augmentation method targeted\nfor HDR hallucination.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:35:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Marnerides", "Demetris", ""], ["Bashford-Rogers", "Thomas", ""], ["Debattista", "Kurt", ""]]}, {"id": "2106.09509", "submitter": "Yuhao Zhu", "authors": "Joshua Romphf, Elias Neuman-Donihue, Gregory Heyworth, Yuhao Zhu", "title": "Resurrect3D: An Open and Customizable Platform for Visualizing and\n  Analyzing Cultural Heritage Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Art and culture, at their best, lie in the act of discovery and exploration.\nThis paper describes Resurrect3D, an open visualization platform for both\ncasual users and domain experts to explore cultural artifacts. To that end,\nResurrect3D takes two steps. First, it provides an interactive cultural\nheritage toolbox, providing not only commonly used tools in cultural heritage\nsuch as relighting and material editing, but also the ability for users to\ncreate an interactive \"story\": a saved session with annotations and\nvisualizations others can later replay. Second, Resurrect3D exposes a set of\nprogramming interfaces to extend the toolbox. Domain experts can develop custom\ntools that perform artifact-specific visualization and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:59:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Romphf", "Joshua", ""], ["Neuman-Donihue", "Elias", ""], ["Heyworth", "Gregory", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2106.09696", "submitter": "Abhinanda Ranjit Punnakkal", "authors": "Abhinanda R. Punnakkal (1), Arjun Chandrasekaran (1), Nikos Athanasiou\n  (1), Alejandra Quiros-Ramirez (2), Michael J. Black (1) ((1) Max Planck\n  Institute for Intelligent Systems, (2) Universitat Konstanz)", "title": "BABEL: Bodies, Action and Behavior with English Labels", "comments": "11 pages, 4 figures, Accepted in CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the semantics of human movement -- the what, how and why of the\nmovement -- is an important problem that requires datasets of human actions\nwith semantic labels. Existing datasets take one of two approaches. Large-scale\nvideo datasets contain many action labels but do not contain ground-truth 3D\nhuman motion. Alternatively, motion-capture (mocap) datasets have precise body\nmotions but are limited to a small number of actions. To address this, we\npresent BABEL, a large dataset with language labels describing the actions\nbeing performed in mocap sequences. BABEL consists of action labels for about\n43 hours of mocap sequences from AMASS. Action labels are at two levels of\nabstraction -- sequence labels describe the overall action in the sequence, and\nframe labels describe all actions in every frame of the sequence. Each frame\nlabel is precisely aligned with the duration of the corresponding action in the\nmocap sequence, and multiple actions can overlap. There are over 28k sequence\nlabels, and 63k frame labels in BABEL, which belong to over 250 unique action\ncategories. Labels from BABEL can be leveraged for tasks like action\nrecognition, temporal action localization, motion synthesis, etc. To\ndemonstrate the value of BABEL as a benchmark, we evaluate the performance of\nmodels on 3D action recognition. We demonstrate that BABEL poses interesting\nlearning challenges that are applicable to real-world scenarios, and can serve\nas a useful benchmark of progress in 3D action recognition. The dataset,\nbaseline method, and evaluation code is made available, and supported for\nacademic research purposes at https://babel.is.tue.mpg.de/.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:51:14 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 21:03:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Punnakkal", "Abhinanda R.", ""], ["Chandrasekaran", "Arjun", ""], ["Athanasiou", "Nikos", ""], ["Quiros-Ramirez", "Alejandra", ""], ["Black", "Michael J.", ""]]}, {"id": "2106.10031", "submitter": "Jiabao Lei", "authors": "Jiabao Lei, Kui Jia, Yi Ma", "title": "Learning and Meshing from Deep Implicit Surface Networks Using an\n  Efficient Implementation of Analytic Marching", "comments": "arXiv admin note: text overlap with arXiv:2002.06597", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of object or scene surfaces has tremendous applications in\ncomputer vision, computer graphics, and robotics. In this paper, we study a\nfundamental problem in this context about recovering a surface mesh from an\nimplicit field function whose zero-level set captures the underlying surface.\nTo achieve the goal, existing methods rely on traditional meshing algorithms;\nwhile promising, they suffer from loss of precision learned in the implicit\nsurface networks, due to the use of discrete space sampling in marching cubes.\nGiven that an MLP with activations of Rectified Linear Unit (ReLU) partitions\nits input space into a number of linear regions, we are motivated to connect\nthis local linearity with a same property owned by the desired result of\npolygon mesh. More specifically, we identify from the linear regions,\npartitioned by an MLP based implicit function, the analytic cells and analytic\nfaces that are associated with the function's zero-level isosurface. We prove\nthat under mild conditions, the identified analytic faces are guaranteed to\nconnect and form a closed, piecewise planar surface. Based on the theorem, we\npropose an algorithm of analytic marching, which marches among analytic cells\nto exactly recover the mesh captured by an implicit surface network. We also\nshow that our theory and algorithm are equally applicable to advanced MLPs with\nshortcut connections and max pooling. Given the parallel nature of analytic\nmarching, we contribute AnalyticMesh, a software package that supports\nefficient meshing of implicit surface networks via CUDA parallel computing, and\nmesh simplification for efficient downstream processing. We apply our method to\ndifferent settings of generative shape modeling using implicit surface\nnetworks. Extensive experiments demonstrate our advantages over existing\nmethods in terms of both meshing accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:06:28 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Lei", "Jiabao", ""], ["Jia", "Kui", ""], ["Ma", "Yi", ""]]}, {"id": "2106.10363", "submitter": "Ergun Akleman", "authors": "Anantha Natarajan, Jiaqi Cui, Ergun Akleman, Vinayak Krishnamurthy", "title": "Construction of Planar and Symmetric Truss Structures with Interlocking\n  Edge Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an algorithmic approach to design and construct\nplanar truss structures based on symmetric lattices using modular elements. The\nmethod of assembly is similar to Leonardo grids as they both rely on the\nproperty of interlocking. In theory, our modular elements can be assembled by\nthe same type of binary operations. Our modular elements embody the principle\nof geometric interlocking, a principle recently introduced in literature that\nallows for pieces of an assembly to be interlocked in a way that they can\nneither be assembled nor disassembled unless the pieces are subjected to\ndeformation or breakage. We demonstrate that breaking the pieces can indeed\nfacilitate the effective assembly of these pieces through the use of a simple\nkey-in-hole concept. As a result, these modular elements can be assembled\ntogether to form an interlocking structure, in which the locking pieces apply\nthe force necessary to hold the entire assembly together.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:35:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Natarajan", "Anantha", ""], ["Cui", "Jiaqi", ""], ["Akleman", "Ergun", ""], ["Krishnamurthy", "Vinayak", ""]]}, {"id": "2106.10592", "submitter": "Wilson Est\\'ecio Marc\\'ilio J\\'unior", "authors": "Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Fernando V. Paulovich, Jos\\'e\n  F. Rodrigues-Jr, Almir O. Artero", "title": "ExplorerTree: a focus+context exploration approach for 2D embeddings", "comments": null, "journal-ref": "Big Data Research 25 (2021)", "doi": "10.1016/j.bdr.2021.100239", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In exploratory tasks involving high-dimensional datasets, dimensionality\nreduction (DR) techniques help analysts to discover patterns and other useful\ninformation. Although scatter plot representations of DR results allow for\ncluster identification and similarity analysis, such a visual metaphor presents\nproblems when the number of instances of the dataset increases, resulting in\ncluttered visualizations. In this work, we propose a scatter plot-based\nmultilevel approach to display DR results and address clutter-related problems\nwhen visualizing large datasets, together with the definition of a methodology\nto use focus+context interaction on non-hierarchical embeddings. The proposed\ntechnique, called ExplorerTree, uses a sampling selection technique on scatter\nplots to reduce visual clutter and guide users through exploratory tasks. We\ndemonstrate ExplorerTree's effectiveness through a use case, where we visually\nexplore activation images of the convolutional layers of a neural network.\nFinally, we also conducted a user experiment to evaluate ExplorerTree's ability\nto convey embedding structures using different sampling strategies.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 01:03:59 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 16:05:07 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Paulovich", "Fernando V.", ""], ["Rodrigues-Jr", "Jos\u00e9 F.", ""], ["Artero", "Almir O.", ""]]}, {"id": "2106.10689", "submitter": "Peng Wang", "authors": "Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura,\n  Wenping Wang", "title": "NeuS: Learning Neural Implicit Surfaces by Volume Rendering for\n  Multi-view Reconstruction", "comments": "22 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a novel neural surface reconstruction method, called NeuS, for\nreconstructing objects and scenes with high fidelity from 2D image inputs.\nExisting neural surface reconstruction approaches, such as DVR and IDR, require\nforeground mask as supervision, easily get trapped in local minima, and\ntherefore struggle with the reconstruction of objects with severe\nself-occlusion or thin structures. Meanwhile, recent neural methods for novel\nview synthesis, such as NeRF and its variants, use volume rendering to produce\na neural scene representation with robustness of optimization, even for highly\ncomplex objects. However, extracting high-quality surfaces from this learned\nimplicit representation is difficult because there are not sufficient surface\nconstraints in the representation. In NeuS, we propose to represent a surface\nas the zero-level set of a signed distance function (SDF) and develop a new\nvolume rendering method to train a neural SDF representation. We observe that\nthe conventional volume rendering method causes inherent geometric errors (i.e.\nbias) for surface reconstruction, and therefore propose a new formulation that\nis free of bias in the first order of approximation, thus leading to more\naccurate surface reconstruction even without the mask supervision. Experiments\non the DTU dataset and the BlendedMVS dataset show that NeuS outperforms the\nstate-of-the-arts in high-quality surface reconstruction, especially for\nobjects and scenes with complex structures and self-occlusion.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 12:59:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingjie", ""], ["Liu", "Yuan", ""], ["Theobalt", "Christian", ""], ["Komura", "Taku", ""], ["Wang", "Wenping", ""]]}, {"id": "2106.11272", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Hao Zhang", "title": "Neural Marching Cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 17:18:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 00:13:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Zhiqin", ""], ["Zhang", "Hao", ""]]}, {"id": "2106.11423", "submitter": "Liwen Hu", "authors": "Huiwen Luo, Koki Nagano, Han-Wei Kung, Mclean Goldwhite, Qingguo Xu,\n  Zejian Wang, Lingyu Wei, Liwen Hu, Hao Li", "title": "Normalized Avatar Synthesis Using StyleGAN and Perceptual Refinement", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a highly robust GAN-based framework for digitizing a normalized\n3D avatar of a person from a single unconstrained photo. While the input image\ncan be of a smiling person or taken in extreme lighting conditions, our method\ncan reliably produce a high-quality textured model of a person's face in\nneutral expression and skin textures under diffuse lighting condition.\nCutting-edge 3D face reconstruction methods use non-linear morphable face\nmodels combined with GAN-based decoders to capture the likeness and details of\na person but fail to produce neutral head models with unshaded albedo textures\nwhich is critical for creating relightable and animation-friendly avatars for\nintegration in virtual environments. The key challenges for existing methods to\nwork is the lack of training and ground truth data containing normalized 3D\nfaces. We propose a two-stage approach to address this problem. First, we adopt\na highly robust normalized 3D face generator by embedding a non-linear\nmorphable face model into a StyleGAN2 network. This allows us to generate\ndetailed but normalized facial assets. This inference is then followed by a\nperceptual refinement step that uses the generated assets as regularization to\ncope with the limited available training samples of normalized faces. We\nfurther introduce a Normalized Face Dataset, which consists of a combination\nphotogrammetry scans, carefully selected photographs, and generated fake people\nwith neutral expressions in diffuse lighting conditions. While our prepared\ndataset contains two orders of magnitude less subjects than cutting edge\nGAN-based 3D facial reconstruction methods, we show that it is possible to\nproduce high-quality normalized face models for very challenging unconstrained\ninput images, and demonstrate superior performance to the current\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 21:57:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Luo", "Huiwen", ""], ["Nagano", "Koki", ""], ["Kung", "Han-Wei", ""], ["Goldwhite", "Mclean", ""], ["Xu", "Qingguo", ""], ["Wang", "Zejian", ""], ["Wei", "Lingyu", ""], ["Hu", "Liwen", ""], ["Li", "Hao", ""]]}, {"id": "2106.12138", "submitter": "Tushar Athawale", "authors": "Tushar M. Athawale, Alireza Entezari, Bei Wang, and Chris R. Johnson", "title": "Statistical Rendering for Visualization of Red Sea Eddy Simulation Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.ao-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the effects of ocean eddies is important in oceanology for gaining\ninsights into transport of energy and biogeochemical particles. We present an\napplication of statistical visualization algorithms for the analysis of the Red\nSea eddy simulation ensemble. Specifically, we demonstrate the applications of\nstatistical volume rendering and statistical Morse complex summary maps to a\nvelocity magnitude field for studying the eddy positions in the flow dataset.\nIn statistical volume rendering, we model per-voxel data uncertainty using\nnoise models, such as parametric and nonparametric, and study the propagation\nof uncertainty into the volume rendering pipeline. In the statistical Morse\ncomplex summary maps, we derive histograms charactering uncertainty of gradient\nflow destinations to understand Morse complex topological variations across the\nensemble. We demonstrate the utility of our statistical visualizations for an\neffective analysis of the potential eddy positions and their spatial\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 03:09:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Athawale", "Tushar M.", ""], ["Entezari", "Alireza", ""], ["Wang", "Bei", ""], ["Johnson", "Chris R.", ""]]}, {"id": "2106.12302", "submitter": "Stylianos Ploumpis", "authors": "Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou,\n  Stefanos Zafeiriou", "title": "3D human tongue reconstruction from single \"in-the-wild\" images", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D face reconstruction from a single image is a task that has garnered\nincreased interest in the Computer Vision community, especially due to its\nbroad use in a number of applications such as realistic 3D avatar creation,\npose invariant face recognition and face hallucination. Since the introduction\nof the 3D Morphable Model in the late 90's, we witnessed an explosion of\nresearch aiming at particularly tackling this task. Nevertheless, despite the\nincreasing level of detail in the 3D face reconstructions from single images\nmainly attributed to deep learning advances, finer and highly deformable\ncomponents of the face such as the tongue are still absent from all 3D face\nmodels in the literature, although being very important for the realness of the\n3D avatar representations. In this work we present the first, to the best of\nour knowledge, end-to-end trainable pipeline that accurately reconstructs the\n3D face together with the tongue. Moreover, we make this pipeline robust in\n\"in-the-wild\" images by introducing a novel GAN method tailored for 3D tongue\nsurface generation. Finally, we make publicly available to the community the\nfirst diverse tongue dataset, consisting of 1,800 raw scans of 700 individuals\nvarying in gender, age, and ethnicity backgrounds. As we demonstrate in an\nextensive series of quantitative as well as qualitative experiments, our model\nproves to be robust and realistically captures the 3D tongue structure, even in\nadverse \"in-the-wild\" conditions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:49:34 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Ploumpis", "Stylianos", ""], ["Moschoglou", "Stylianos", ""], ["Triantafyllou", "Vasileios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2106.12372", "submitter": "Thomas M\\\"uller", "authors": "Thomas M\\\"uller, Fabrice Rousselle, Jan Nov\\'ak, Alexander Keller", "title": "Real-time Neural Radiance Caching for Path Tracing", "comments": "To appear at SIGGRAPH 2021. 16 pages, 16 figures", "journal-ref": null, "doi": "10.1145/3450626.3459812", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time neural radiance caching method for path-traced global\nillumination. Our system is designed to handle fully dynamic scenes, and makes\nno assumptions about the lighting, geometry, and materials. The data-driven\nnature of our approach sidesteps many difficulties of caching algorithms, such\nas locating, interpolating, and updating cache points. Since pretraining neural\nnetworks to handle novel, dynamic scenes is a formidable generalization\nchallenge, we do away with pretraining and instead achieve generalization via\nadaptation, i.e. we opt for training the radiance cache while rendering. We\nemploy self-training to provide low-noise training targets and simulate\ninfinite-bounce transport by merely iterating few-bounce training updates. The\nupdates and cache queries incur a mild overhead -- about 2.6ms on full HD\nresolution -- thanks to a streaming implementation of the neural network that\nfully exploits modern hardware. We demonstrate significant noise reduction at\nthe cost of little induced bias, and report state-of-the-art, real-time\nperformance on a number of challenging scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:09:58 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 08:09:48 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["M\u00fcller", "Thomas", ""], ["Rousselle", "Fabrice", ""], ["Nov\u00e1k", "Jan", ""], ["Keller", "Alexander", ""]]}, {"id": "2106.12633", "submitter": "Anthony Steed", "authors": "Anthony Steed, Tuukka M. Takala, Daniel Archer, Wallace Lages, Robert\n  W. Lindeman", "title": "Directions for 3D User Interface Research from Consumer VR Games", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the continuing development of affordable immersive virtual reality (VR)\nsystems, there is now a growing market for consumer content. The current form\nof consumer systems is not dissimilar to the lab-based VR systems of the past\n30 years: the primary input mechanism is a head-tracked display and one or two\ntracked hands with buttons and joysticks on hand-held controllers. Over those\n30 years, a very diverse academic literature has emerged that covers design and\nergonomics of 3D user interfaces (3DUIs). However, the growing consumer market\nhas engaged a very broad range of creatives that have built a very diverse set\nof designs. Sometimes these designs adopt findings from the academic\nliterature, but other times they experiment with completely novel or\ncounter-intuitive mechanisms. In this paper and its online adjunct, we report\non novel 3DUI design patterns that are interesting from both design and\nresearch perspectives: they are highly novel, potentially broadly re-usable\nand/or suggest interesting avenues for evaluation. The supplemental material,\nwhich is a living document, is a crowd-sourced repository of interesting\npatterns. This paper is a curated snapshot of those patterns that were\nconsidered to be the most fruitful for further elaboration.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 19:05:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Steed", "Anthony", ""], ["Takala", "Tuukka M.", ""], ["Archer", "Daniel", ""], ["Lages", "Wallace", ""], ["Lindeman", "Robert W.", ""]]}, {"id": "2106.12643", "submitter": "Przemyslaw Musialski", "authors": "Stefan Pillwein, Johanna K\\\"ubert, Florian Rist, Przemyslaw Musialski", "title": "Design and Fabrication of Multi-Patch Elastic Geodesic Grid Structures", "comments": "15 pagers, 20 figures. arXiv admin note: substantial text overlap\n  with arXiv:2010.08062", "journal-ref": null, "doi": "10.1016/j.cag.2021.06.002", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastic geodesic grids (EGG) are lightweight structures that can be deployed\nto approximate designer-provided free-form surfaces. Initially, the grids are\nperfectly flat, during deployment, a curved shape emerges, as grid elements\nbend and twist. Their layout is based on networks of geodesic curves and is\nfound geometrically. Encoded in the planar grids is the intrinsic shape of the\ndesign surface. Such structures may serve purposes like free-form\nsub-structures, panels, sun and rain protectors, pavilions, etc. However, so\nfar the EGG have only been investigated using a generic set of design surfaces\nand small-scale desktop models. Some limitations become apparent when\nconsidering more sophisticated design surfaces, like from free-form\narchitecture. Due to characteristics like high local curvature or non-geodesic\nboundaries, they may be captured only poorly by a single EGG.\n  We show how decomposing such surfaces into smaller patches serves as an\neffective strategy to tackle these problems. We furthermore show that elastic\ngeodesic grids are in fact well suited for this approach. Finally, we present a\nshowcase model of some meters in size and discuss practical aspects concerning\nfabrication, size, and easy deployment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 19:56:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Pillwein", "Stefan", ""], ["K\u00fcbert", "Johanna", ""], ["Rist", "Florian", ""], ["Musialski", "Przemyslaw", ""]]}, {"id": "2106.12655", "submitter": "Ante Qu", "authors": "Ante Qu and Doug L. James", "title": "Fast Linking Numbers for Topology Verification of Loopy Structures", "comments": "Published at Siggraph 2021. Copyright (C) 2021 Association for\n  Computing Machinery. Paper webpage and code at\n  https://graphics.stanford.edu/papers/fastlinkingnumbers/", "journal-ref": "ACM Trans. Graph. 40, 4, Article 106 (August 2021), 19 pages", "doi": "10.1145/3450626.3459778", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It is increasingly common to model, simulate, and process complex materials\nbased on loopy structures, such as in yarn-level cloth garments, which possess\ntopological constraints between inter-looping curves. While the input model may\nsatisfy specific topological linkages between pairs of closed loops, subsequent\nprocessing may violate those topological conditions. In this paper, we explore\na family of methods for efficiently computing and verifying linking numbers\nbetween closed curves, and apply these to applications in geometry processing,\nanimation, and simulation, so as to verify that topological invariants are\npreserved during and after processing of the input models. Our method has three\nstages: (1) we identify potentially interacting loop-loop pairs, then (2)\ncarefully discretize each loop's spline curves into line segments so as to\nenable (3) efficient linking number evaluation using accelerated kernels based\non either counting projected segment-segment crossings, or by evaluating the\nGauss linking integral using direct or fast summation methods (Barnes-Hut or\nfast multipole methods). We evaluate CPU and GPU implementations of these\nmethods on a suite of test problems, including yarn-level cloth and chainmail,\nthat involve significant processing: physics-based relaxation and animation,\nuser-modeled deformations, curve compression and reparameterization. We show\nthat topology errors can be efficiently identified to enable more robust\nprocessing of loopy structures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 20:52:37 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Qu", "Ante", ""], ["James", "Doug L.", ""]]}, {"id": "2106.12802", "submitter": "Qiqi Hou", "authors": "Qiqi Hou, Zhan Li, Carl S Marshall, Selvakumar Panneer, Feng Liu", "title": "Fast Monte Carlo Rendering via Multi-Resolution Sampling", "comments": "Graphic Interface 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monte Carlo rendering algorithms are widely used to produce photorealistic\ncomputer graphics images. However, these algorithms need to sample a\nsubstantial amount of rays per pixel to enable proper global illumination and\nthus require an immense amount of computation. In this paper, we present a\nhybrid rendering method to speed up Monte Carlo rendering algorithms. Our\nmethod first generates two versions of a rendering: one at a low resolution\nwith a high sample rate (LRHS) and the other at a high resolution with a low\nsample rate (HRLS). We then develop a deep convolutional neural network to fuse\nthese two renderings into a high-quality image as if it were rendered at a high\nresolution with a high sample rate. Specifically, we formulate this fusion task\nas a super resolution problem that generates a high resolution rendering from a\nlow resolution input (LRHS), assisted with the HRLS rendering. The HRLS\nrendering provides critical high frequency details which are difficult to\nrecover from the LRHS for any super resolution methods. Our experiments show\nthat our hybrid rendering algorithm is significantly faster than the\nstate-of-the-art Monte Carlo denoising methods while rendering high-quality\nimages when tested on both our own BCR dataset and the Gharbi dataset.\n\\url{https://github.com/hqqxyy/msspl}\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:35:27 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Hou", "Qiqi", ""], ["Li", "Zhan", ""], ["Marshall", "Carl S", ""], ["Panneer", "Selvakumar", ""], ["Liu", "Feng", ""]]}, {"id": "2106.13228", "submitter": "Keunhong Park", "authors": "Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T. Barron, Sofien\n  Bouaziz, Dan B Goldman, Ricardo Martin-Brualla, Steven M. Seitz", "title": "HyperNeRF: A Higher-Dimensional Representation for Topologically Varying\n  Neural Radiance Fields", "comments": "Project page: https://hypernerf.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Radiance Fields (NeRF) are able to reconstruct scenes with\nunprecedented fidelity, and various recent works have extended NeRF to handle\ndynamic scenes. A common approach to reconstruct such non-rigid scenes is\nthrough the use of a learned deformation field mapping from coordinates in each\ninput image into a canonical template coordinate space. However, these\ndeformation-based approaches struggle to model changes in topology, as\ntopological changes require a discontinuity in the deformation field, but these\ndeformation fields are necessarily continuous. We address this limitation by\nlifting NeRFs into a higher dimensional space, and by representing the 5D\nradiance field corresponding to each individual input image as a slice through\nthis \"hyper-space\". Our method is inspired by level set methods, which model\nthe evolution of surfaces as slices through a higher dimensional surface. We\nevaluate our method on two tasks: (i) interpolating smoothly between \"moments\",\ni.e., configurations of the scene, seen in the input images while maintaining\nvisual plausibility, and (ii) novel-view synthesis at fixed moments. We show\nthat our method, which we dub HyperNeRF, outperforms existing methods on both\ntasks by significant margins. Compared to Nerfies, HyperNeRF reduces average\nerror rates by 8.6% for interpolation and 8.8% for novel-view synthesis, as\nmeasured by LPIPS.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:59:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Park", "Keunhong", ""], ["Sinha", "Utkarsh", ""], ["Hedman", "Peter", ""], ["Barron", "Jonathan T.", ""], ["Bouaziz", "Sofien", ""], ["Goldman", "Dan B", ""], ["Martin-Brualla", "Ricardo", ""], ["Seitz", "Steven M.", ""]]}, {"id": "2106.13299", "submitter": "George Drettakis", "authors": "Julien Philip and S\\'ebastien Morgenthaler and Micha\\\"el Gharbi and\n  George Drettakis", "title": "Free-viewpoint Indoor Neural Relighting from Multi-view Stereo", "comments": null, "journal-ref": "ACM Transactions on Graphics (2021)", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a neural relighting algorithm for captured indoors scenes, that\nallows interactive free-viewpoint navigation. Our method allows illumination to\nbe changed synthetically, while coherently rendering cast shadows and complex\nglossy materials. We start with multiple images of the scene and a 3D mesh\nobtained by multi-view stereo (MVS) reconstruction. We assume that lighting is\nwell-explained as the sum of a view-independent diffuse component and a\nview-dependent glossy term concentrated around the mirror reflection direction.\nWe design a convolutional network around input feature maps that facilitate\nlearning of an implicit representation of scene materials and illumination,\nenabling both relighting and free-viewpoint navigation. We generate these input\nmaps by exploiting the best elements of both image-based and physically-based\nrendering. We sample the input views to estimate diffuse scene irradiance, and\ncompute the new illumination caused by user-specified light sources using path\ntracing. To facilitate the network's understanding of materials and synthesize\nplausible glossy reflections, we reproject the views and compute mirror images.\nWe train the network on a synthetic dataset where each scene is also\nreconstructed with MVS. We show results of our algorithm relighting real indoor\nscenes and performing free-viewpoint navigation with complex and realistic\nglossy reflections, which so far remained out of reach for view-synthesis\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:09:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Philip", "Julien", ""], ["Morgenthaler", "S\u00e9bastien", ""], ["Gharbi", "Micha\u00ebl", ""], ["Drettakis", "George", ""]]}, {"id": "2106.13416", "submitter": "Yuki Endo", "authors": "Yuki Endo, Yoshihiro Kanamori", "title": "Diversifying Semantic Image Synthesis and Editing via Class- and\n  Layer-wise VAEs", "comments": "Accepted to Pacific Graphics 2020, codes available at\n  https://github.com/endo-yuki-t/DiversifyingSMIS", "journal-ref": null, "doi": "10.1111/cgf.14164", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image synthesis is a process for generating photorealistic images\nfrom a single semantic mask. To enrich the diversity of multimodal image\nsynthesis, previous methods have controlled the global appearance of an output\nimage by learning a single latent space. However, a single latent code is often\ninsufficient for capturing various object styles because object appearance\ndepends on multiple factors. To handle individual factors that determine object\nstyles, we propose a class- and layer-wise extension to the variational\nautoencoder (VAE) framework that allows flexible control over each object class\nat the local to global levels by learning multiple latent spaces. Furthermore,\nwe demonstrate that our method generates images that are both plausible and\nmore diverse compared to state-of-the-art methods via extensive experiments\nwith real and synthetic datasets inthree different domains. We also show that\nour method enables a wide range of applications in image synthesis and editing\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:12:05 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 06:56:09 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Endo", "Yuki", ""], ["Kanamori", "Yoshihiro", ""]]}, {"id": "2106.13425", "submitter": "Guoxian Song", "authors": "Guoxian Song and Tat-Jen Cham and Jianfei Cai and Jianmin Zheng", "title": "Half-body Portrait Relighting with Overcomplete Lighting Representation", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.14384", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a neural-based model for relighting a half-body portrait image by\nsimply referring to another portrait image with the desired lighting condition.\nRather than following classical inverse rendering methodology that involves\nestimating normals, albedo and environment maps, we implicitly encode the\nsubject and lighting in a latent space, and use these latent codes to generate\nrelighted images by neural rendering. A key technical innovation is the use of\na novel overcomplete lighting representation, which facilitates lighting\ninterpolation in the latent space, as well as helping regularize the\nself-organization of the lighting latent space during training. In addition, we\npropose a novel multiplicative neural render that more effectively combines the\nsubject and lighting latent codes for rendering. We also created a large-scale\nphotorealistic rendered relighting dataset for training, which allows our model\nto generalize well to real images. Extensive experiments demonstrate that our\nsystem not only outperforms existing methods for referral-based portrait\nrelighting, but also has the capability generate sequences of relighted images\nvia lighting rotations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 04:45:23 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Song", "Guoxian", ""], ["Cham", "Tat-Jen", ""], ["Cai", "Jianfei", ""], ["Zheng", "Jianmin", ""]]}, {"id": "2106.13679", "submitter": "Giovanni Trappolini", "authors": "Giovanni Trappolini, Luca Cosmo, Luca Moschella, Riccardo Marin,\n  Simone Melzi, Emanuele Rodol\\`a", "title": "Shape registration in the time of transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a transformer-based procedure for the efficient\nregistration of non-rigid 3D point clouds. The proposed approach is data-driven\nand adopts for the first time the transformer architecture in the registration\ntask. Our method is general and applies to different settings. Given a fixed\ntemplate with some desired properties (e.g. skinning weights or other animation\ncues), we can register raw acquired data to it, thereby transferring all the\ntemplate properties to the input geometry. Alternatively, given a pair of\nshapes, our method can register the first onto the second (or vice-versa),\nobtaining a high-quality dense correspondence between the two. In both\ncontexts, the quality of our results enables us to target real applications\nsuch as texture transfer and shape interpolation. Furthermore, we also show\nthat including an estimation of the underlying density of the surface eases the\nlearning process. By exploiting the potential of this architecture, we can\ntrain our model requiring only a sparse set of ground truth correspondences\n($10\\sim20\\%$ of the total points). The proposed model and the analysis that we\nperform pave the way for future exploration of transformer-based architectures\nfor registration and matching applications. Qualitative and quantitative\nevaluations demonstrate that our pipeline outperforms state-of-the-art methods\nfor deformable and unordered 3D data registration on different datasets and\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:02:30 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 07:56:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Trappolini", "Giovanni", ""], ["Cosmo", "Luca", ""], ["Moschella", "Luca", ""], ["Marin", "Riccardo", ""], ["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2106.13871", "submitter": "Guillermo Valle-P\\'erez", "authors": "Guillermo Valle-P\\'erez, Gustav Eje Henter, Jonas Beskow, Andr\\'e\n  Holzapfel, Pierre-Yves Oudeyer, Simon Alexanderson", "title": "Transflower: probabilistic autoregressive dance generation with\n  multimodal attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling a\nhigh-dimensional continuous motion signal, conditioned on an audio signal. In\nthis work we make two contributions to tackle this problem. First, we present a\nnovel probabilistic autoregressive architecture that models the distribution\nover future poses with a normalizing flow conditioned on previous poses as well\nas music context, using a multimodal transformer encoder. Second, we introduce\nthe currently largest 3D dance-motion dataset, obtained with a variety of\nmotion-capture technologies, and including both professional and casual\ndancers. Using this dataset, we compare our new model against two baselines,\nvia objective metrics and a user study, and show that both the ability to model\na probability distribution, as well as being able to attend over a large motion\nand music context are necessary to produce interesting, diverse, and realistic\ndance that matches the music.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:14:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Valle-P\u00e9rez", "Guillermo", ""], ["Henter", "Gustav Eje", ""], ["Beskow", "Jonas", ""], ["Holzapfel", "Andr\u00e9", ""], ["Oudeyer", "Pierre-Yves", ""], ["Alexanderson", "Simon", ""]]}, {"id": "2106.14132", "submitter": "Yang-tian Sun", "authors": "Yang-tian Sun, Hao-zhi Huang, Xuan Wang, Yu-kun Lai, Wei Liu, Lin Gao", "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering", "comments": "Video link: https://www.bilibili.com/video/BV1y64y1C7ge/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 03:40:22 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 05:54:05 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sun", "Yang-tian", ""], ["Huang", "Hao-zhi", ""], ["Wang", "Xuan", ""], ["Lai", "Yu-kun", ""], ["Liu", "Wei", ""], ["Gao", "Lin", ""]]}, {"id": "2106.14274", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen, Andrea Tagliasacchi, Hao Zhang", "title": "Learning Mesh Representations via Binary Space Partitioning Tree\n  Networks", "comments": "Accepted to TPAMI. This is the extended journal version of BSP-Net\n  (arXiv:1911.06971) from CVPR 2020", "journal-ref": null, "doi": "10.1109/TPAMI.2021.3093440", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 16:37:54 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 00:24:22 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chen", "Zhiqin", ""], ["Tagliasacchi", "Andrea", ""], ["Zhang", "Hao", ""]]}, {"id": "2106.14313", "submitter": "Wenchao Li", "authors": "Wenchao Li, Yun Wang, He Huang, Weiwei Cui, Haidong Zhang, Huamin Qu,\n  Dongmei Zhang", "title": "AniVis: Generating Animated Transitions Between Statistical Charts with\n  a Tree Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animated transitions help viewers understand changes between related\nvisualizations. To clearly present the underlying relations between statistical\ncharts, animation authors need to have a high level of expertise and a\nconsiderable amount of time to describe the relations with reasonable animation\nstages. We present AniVis, an automated approach for generating animated\ntransitions to demonstrate the changes between two statistical charts. AniVis\nmodels each statistical chart into a tree-based structure. Given an input chart\npair, the differences of data and visual properties of the chart pair are\nformalized as tree edit operations. The edit operations can be mapped to atomic\ntransition units. Through this approach, the animated transition between two\ncharts can be expressed as a set of transition units. Then, we conduct a\nformative study to understand people's preferences for animation sequences.\nBased on the study, we propose a set of principles and a sequence composition\nalgorithm to compose the transition units into a meaningful animation sequence.\nFinally, we synthesize these units together to deliver a smooth and intuitive\nanimated transition between charts. To test our approach, we present a\nprototype system and its generated results to illustrate the usage of our\nframework. We perform a comparative study to assess the transition sequence\nderived from the tree model. We further collect qualitative feedback to\nevaluate the effectiveness and usefulness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 19:17:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Wenchao", ""], ["Wang", "Yun", ""], ["Huang", "He", ""], ["Cui", "Weiwei", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2106.14360", "submitter": "David Palmer", "authors": "David R. Palmer (1), Oded Stein (1), and Justin Solomon (1) ((1)\n  Massachusetts Institute of Technology)", "title": "Frame Field Operators", "comments": "15 pages, 15 figures. To be published in proceedings of the 2021\n  Symposium on Geometry Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential operators are widely used in geometry processing for problem\ndomains like spectral shape analysis, data interpolation, parametrization and\nmapping, and meshing. In addition to the ubiquitous cotangent Laplacian,\nanisotropic second-order operators, as well as higher-order operators such as\nthe Bilaplacian, have been discretized for specialized applications. In this\npaper, we study a class of operators that generalizes the fourth-order\nBilaplacian to support anisotropic behavior. The anisotropy is parametrized by\na symmetric frame field, first studied in connection with quadrilateral and\nhexahedral meshing, which allows for fine-grained control of local directions\nof variation. We discretize these operators using a mixed finite element\nscheme, verify convergence of the discretization, study the behavior of the\noperator under pullback, and present potential applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:12:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Palmer", "David R.", ""], ["Stein", "Oded", ""], ["Solomon", "Justin", ""]]}, {"id": "2106.14736", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael Neff, Hedvig\n  Kjellstr\\\"om, Gustav Eje Henter", "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for\n  Generating Representational Gestures from Speech", "comments": "Accepted for publication at the ACM International Conference on\n  Intelligent Virtual Agents (IVA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:07:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kucherenko", "Taras", ""], ["Nagy", "Rajmund", ""], ["Jonell", "Patrik", ""], ["Neff", "Michael", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2106.14773", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Designing color symmetry in stigmergic art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color symmetry is an extension of symmetry imposed by isometric\ntransformations and means that the colors of geometrical objects are assigned\naccording to the symmetry properties of the objects. A color symmetry permutes\nthe coloring of the objects consistently with their symmetry group. We apply\nthis concept to bio-inspired generative art. Therefore, we interpret the\ngeometrical objects as motifs that may repeat themselves with a\nsymmetry-consistent coloring. The motifs are obtained by design principles from\nstigmergy. We discuss a design procedure and present visual results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:47:15 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "2106.14879", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Fabian Andres Prada, Timur Bagautdinov, Weipeng Xu,\n  Yuan Dong, He Wen, Jessica Hodgins, Chenglei Wu", "title": "Explicit Clothing Modeling for an Animatable Full-Body Avatar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown great progress in building photorealistic animatable\nfull-body codec avatars, but these avatars still face difficulties in\ngenerating high-fidelity animation of clothing. To address the difficulties, we\npropose a method to build an animatable clothed body avatar with an explicit\nrepresentation of the clothing on the upper body from multi-view captured\nvideos. We use a two-layer mesh representation to separately register the 3D\nscans with templates. In order to improve the photometric correspondence across\ndifferent frames, texture alignment is then performed through inverse rendering\nof the clothing geometry and texture predicted by a variational autoencoder. We\nthen train a new two-layer codec avatar with separate modeling of the upper\nclothing and the inner body layer. To learn the interaction between the body\ndynamics and clothing states, we use a temporal convolution network to predict\nthe clothing latent code based on a sequence of input skeletal poses. We show\nphotorealistic animation output for three different actors, and demonstrate the\nadvantage of our clothed-body avatars over single-layer avatars in the previous\nwork. We also show the benefit of an explicit clothing model which allows the\nclothing texture to be edited in the animation output.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:58:40 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 19:51:00 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Xiang", "Donglai", ""], ["Prada", "Fabian Andres", ""], ["Bagautdinov", "Timur", ""], ["Xu", "Weipeng", ""], ["Dong", "Yuan", ""], ["Wen", "He", ""], ["Hodgins", "Jessica", ""], ["Wu", "Chenglei", ""]]}, {"id": "2106.14942", "submitter": "Alexander Bergman", "authors": "Alexander W. Bergman and Petr Kellnhofer and Gordon Wetzstein", "title": "Fast Training of Neural Lumigraph Representations using Meta Learning", "comments": "Project website:\n  http://www.computationalimaging.org/publications/metanlr/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:55:50 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bergman", "Alexander W.", ""], ["Kellnhofer", "Petr", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2106.15179", "submitter": "Hendrik Richter", "authors": "Hendrik Richter", "title": "Wrong Colored Vermeer: Color-Symmetric Image Distortion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color symmetry implies that the colors of geometrical objects are assigned\naccording to their symmetry properties. It is defined by associating the\nelements of the symmetry group with a color permutation. I use this concept for\ngenerative art and apply symmetry-consistent color distortions to images of\npaintings by Johannes Vermeer. The color permutations are realized as mappings\nof the HSV color space onto itself.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 08:51:23 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Richter", "Hendrik", ""]]}, {"id": "2106.15280", "submitter": "Yiqin Zhao", "authors": "Yiqin Zhao and Tian Guo", "title": "Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile\n  Augmented Reality", "comments": null, "journal-ref": null, "doi": "10.1145/3458864.3467886", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional lighting provides the foundation for achieving\nspatially-variant photorealistic 3D rendering, a desirable property for mobile\naugmented reality applications. However, in practice, estimating\nomnidirectional lighting can be challenging due to limitations such as partial\npanoramas of the rendering positions, and the inherent environment lighting and\nmobile user dynamics. A new opportunity arises recently with the advancements\nin mobile 3D vision, including built-in high-accuracy depth sensors and deep\nlearning-powered algorithms, which provide the means to better sense and\nunderstand the physical surroundings. Centering the key idea of 3D vision, in\nthis work, we design an edge-assisted framework called Xihe to provide mobile\nAR applications the ability to obtain accurate omnidirectional lighting\nestimation in real time. Specifically, we develop a novel sampling technique\nthat efficiently compresses the raw point cloud input generated at the mobile\ndevice. This technique is derived based on our empirical analysis of a recent\n3D indoor dataset and plays a key role in our 3D vision-based lighting\nestimator pipeline design. To achieve the real-time goal, we develop a tailored\nGPU pipeline for on-device point cloud processing and use an encoding technique\nthat reduces network transmitted bytes. Finally, we present an adaptive\ntriggering strategy that allows Xihe to skip unnecessary lighting estimations\nand a practical way to provide temporal coherent rendering integration with the\nmobile AR ecosystem. We evaluate both the lighting estimation accuracy and time\nof Xihe using a reference mobile application developed with Xihe's APIs. Our\nresults show that Xihe takes as fast as 20.67ms per lighting estimation and\nachieves 9.4% better estimation accuracy than a state-of-the-art neural\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 13:48:29 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhao", "Yiqin", ""], ["Guo", "Tian", ""]]}, {"id": "2106.15306", "submitter": "Daniel Ruijters", "authors": "Daniel Ruijters", "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:57:25 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ruijters", "Daniel", ""]]}, {"id": "2106.15308", "submitter": "Daniel Ruijters", "authors": "Robert Homan, Ren\\'e van Rijsselt, Daniel Ruijters", "title": "Automatic 2D-3D Registration without Contrast Agent during Neurovascular\n  Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusing live fluoroscopy images with a 3D rotational reconstruction of the\nvasculature allows to navigate endovascular devices in minimally invasive\nneuro-vascular treatment, while reducing the usage of harmful iodine contrast\nmedium. The alignment of the fluoroscopy images and the 3D reconstruction is\ninitialized using the sensor information of the X-ray C-arm geometry. Patient\nmotion is then corrected by an image-based registration algorithm, based on a\ngradient difference similarity measure using digital reconstructed radiographs\nof the 3D reconstruction. This algorithm does not require the vessels in the\nfluoroscopy image to be filled with iodine contrast agent, but rather relies on\ngradients in the image (bone structures, sinuses) as landmark features. This\npaper investigates the accuracy, robustness and computation time aspects of the\nimage-based registration algorithm. Using phantom experiments 97% of the\nregistration attempts passed the success criterion of a residual registration\nerror of less than 1 mm translation and 3{\\deg} rotation. The paper establishes\na new method for validation of 2D-3D registration without requiring changes to\nthe clinical workflow, such as attaching fiducial markers. As a consequence,\nthis method can be retrospectively applied to pre-existing clinical data. For\nclinical data experiments, 87% of the registration attempts passed the\ncriterion of a residual translational error of < 1 mm, and 84% possessed a\nrotational error of < 3{\\deg}.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:16:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Homan", "Robert", ""], ["van Rijsselt", "Ren\u00e9", ""], ["Ruijters", "Daniel", ""]]}, {"id": "2106.15539", "submitter": "Ricardo L. de Queiroz", "authors": "Ricardo L. de Queiroz, Camilo Dorea, Davi R. Freitas, Maja Krivokuca,\n  Gustavo P. Sandri", "title": "Model-Centric Volumetric Point Cloud Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Point clouds have recently gained interest, especially for real-time\napplications and for 3D-scanned material, such as is used in autonomous\ndriving, architecture, and engineering, to model real estate for renovation or\ndisplay. Point clouds are associated with geometry information and attributes\nsuch as color. Be the color unique or direction-dependent (in the case of\nplenoptic point clouds), it reflects the colors observed by cameras displaced\naround the object. Hence, not only are the viewing references assumed, but the\nillumination spectrum and illumination geometry is also implicit. We propose a\nmodel-centric description of the 3D object, that is independent of the\nillumination and of the position of the cameras. We want to be able to describe\nthe objects themselves such that, at a later stage, the rendering of the model\nmay decide where to place illumination, from which it may calculate the image\nviewed by a given camera. We want to be able to describe transparent or\ntranslucid objects, mirrors, fishbowls, fog and smoke. Volumetric clouds may\nallow us to describe the air, however ``empty'', and introduce air particles,\nin a manner independent of the viewer position. For that, we rely on some\neletromagnetic properties to arrive at seven attributes per voxel that would\ndescribe the material and its color or transparency. Three attributes are for\nthe transmissivity of each color, three are for the attenuation of each color,\nand another attribute is for diffuseness. These attributes give information\nabout the object to the renderer, with whom lies the decision on how to render\nand depict each object.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:19:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["de Queiroz", "Ricardo L.", ""], ["Dorea", "Camilo", ""], ["Freitas", "Davi R.", ""], ["Krivokuca", "Maja", ""], ["Sandri", "Gustavo P.", ""]]}, {"id": "2106.15778", "submitter": "Wenming Tang", "authors": "Wenming Tang Guoping Qiu", "title": "Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object\n  Segmentation and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents new designs of graph convolutional neural networks (GCNs)\non 3D meshes for 3D object segmentation and classification. We use the faces of\nthe mesh as basic processing units and represent a 3D mesh as a graph where\neach node corresponds to a face. To enhance the descriptive power of the graph,\nwe introduce a 1-ring face neighbourhood structure to derive novel\nmulti-dimensional spatial and structure features to represent the graph nodes.\nBased on this new graph representation, we then design a densely connected\ngraph convolutional block which aggregates local and regional features as the\nkey construction component to build effective and efficient practical GCN\nmodels for 3D object classification and segmentation. We will present\nexperimental results to show that our new technique outperforms state of the\nart where our models are shown to have the smallest number of parameters and\nconsietently achieve the highest accuracies across a number of benchmark\ndatasets. We will also present ablation studies to demonstrate the soundness of\nour design principles and the effectiveness of our practical models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 02:17:16 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Qiu", "Wenming Tang Guoping", ""]]}]