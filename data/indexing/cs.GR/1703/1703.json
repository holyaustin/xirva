[{"id": "1703.00050", "submitter": "Manolis Savva", "authors": "Angel X. Chang, Mihail Eric, Manolis Savva, Christopher D. Manning", "title": "SceneSeer: 3D Scene Design with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing 3D scenes is currently a creative task that requires significant\nexpertise and effort in using complex 3D design interfaces. This effortful\ndesign process starts in stark contrast to the easiness with which people can\nuse language to describe real and imaginary environments. We present SceneSeer:\nan interactive text to 3D scene generation system that allows a user to design\n3D scenes using natural language. A user provides input text from which we\nextract explicit constraints on the objects that should appear in the scene.\nGiven these explicit constraints, the system then uses a spatial knowledge base\nlearned from an existing database of 3D scenes and 3D object models to infer an\narrangement of the objects forming a natural scene matching the input\ndescription. Using textual commands the user can then iteratively refine the\ncreated scene by adding, removing, replacing, and manipulating objects. We\nevaluate the quality of 3D scenes generated by SceneSeer in a perceptual\nevaluation experiment where we compare against manually designed scenes and\nsimpler baselines for 3D scene generation. We demonstrate how the generated\nscenes can be iteratively refined through simple natural language commands.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 20:47:47 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chang", "Angel X.", ""], ["Eric", "Mihail", ""], ["Savva", "Manolis", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1703.00061", "submitter": "Manolis Savva", "authors": "Manolis Savva, Angel X. Chang, Maneesh Agrawala", "title": "SceneSuggest: Context-driven 3D Scene Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SceneSuggest: an interactive 3D scene design system providing\ncontext-driven suggestions for 3D model retrieval and placement. Using a\npoint-and-click metaphor we specify regions in a scene in which to\nautomatically place and orient relevant 3D models. Candidate models are ranked\nusing a set of static support, position, and orientation priors learned from 3D\nscenes. We show that our suggestions enable rapid assembly of indoor scenes. We\nperform a user study comparing suggestions to manual search and selection, as\nwell as to suggestions with no automatic orientation. We find that suggestions\nreduce total modeling time by 32%, that orientation priors reduce time spent\nre-orienting objects by 27%, and that context-driven suggestions reduce the\nnumber of text queries by 50%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:21:03 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1703.00212", "submitter": "Philippe P\\'eba\\\"y", "authors": "Gu\\'enol\\'e Harel, Jacques-Bernard Lekien, Philippe P. P\\'eba\\\"y", "title": "Two New Contributions to the Visualization of AMR Grids: I. Interactive\n  Rendering of Extreme-Scale 2-Dimensional Grids II. Novel Selection Filters in\n  Arbitrary Dimension", "comments": "Keywords: scientific visualization, interactive visualization,\n  meshing, AMR, mesh refinement, tree-based, octree, VTK, parallel\n  visualization, large scale visualization, HPC, data analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here the result of continuation work, performed to further fulfill\nthe vision we outlined in [Harel,Lekien,P\\'eba\\\"y-2017] for the visualization\nand analysis of tree-based adaptive mesh refinement (AMR) simulations, using\nthe hypertree grid paradigm which we proposed.\n  The first filter presented hereafter implements an adaptive approach in order\nto accelerate the rendering of 2-dimensional AMR grids, hereby solving the\nproblem posed by the loss of interactivity that occurs when dealing with large\nand/or deeply refined meshes. Specifically, view parameters are taken into\naccount, in order to: on one hand, avoid creating surface elements that are\noutside of the view area; on the other hand, utilize level-of-detail properties\nto cull those cells that are deemed too small to be visible with respect to the\ngiven view parameters. This adaptive approach often results in a massive\nincrease in rendering performance.\n  In addition, two new selection filters provide data analysis capabilities, by\nmeans of allowing for the extraction of those cells within a hypertree grid\nthat are deemed relevant in some sense, either geometrically or topologically.\nAfter a description of these new algorithms, we illustrate their use within the\nVisualization Toolkit (VTK) in which we implemented them. This note ends with\nsome suggestions for subsequent work.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 10:21:24 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Harel", "Gu\u00e9nol\u00e9", ""], ["Lekien", "Jacques-Bernard", ""], ["P\u00e9ba\u00ff", "Philippe P.", ""]]}, {"id": "1703.00521", "submitter": "Andrew Reach", "authors": "Andrew McCaleb Reach and Chris North", "title": "The Signals and Systems Approach to Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animation is ubiquitous in visualization systems, and a common technique for\ncreating these animations is the transition. In the transition approach,\nanimations are created by smoothly interpolating a visual attribute between a\nstart and end value, reaching the end value after a specified duration. This\napproach works well when each transition for an attribute is allowed to finish\nbefore the next is triggered, but performs poorly when a new transition is\ntriggered before the current transition has finished. In particular,\ninterruptions introduce velocity discontinuities, and frequent interruptions\ncan slow down the resulting animation. To solve these problems, we model the\nproblem of animation as a signal processing problem. In our technique,\nanimations are produced by transformations of signals, or functions over time.\nIn particular, an animation is produced by transforming an input signal, a\nfunction from time to target attribute value, into an output signal, a function\nfrom time to displayed attribute value. We show that well-known\nsignal-processing techniques can be applied to produce animations that are free\nfrom velocity discontinuities even when interrupted.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 21:40:16 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Reach", "Andrew McCaleb", ""], ["North", "Chris", ""]]}, {"id": "1703.01499", "submitter": "Aditya Balu", "authors": "Aditya Balu, Sambit Ghadai, Gavin Young, Soumik Sarkar, Adarsh\n  Krishnamurthy", "title": "A Machine-Learning Framework for Design for Manufacturability", "comments": "this is a duplicate submission. Hence want to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  this is a duplicate submission(original is arXiv:1612.02141). Hence want to\nwithdraw it\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 17:37:32 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 14:55:52 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Balu", "Aditya", ""], ["Ghadai", "Sambit", ""], ["Young", "Gavin", ""], ["Sarkar", "Soumik", ""], ["Krishnamurthy", "Adarsh", ""]]}, {"id": "1703.02016", "submitter": "V\\'ictor Arellano Vicente", "authors": "Victor Arellano, Diego Gutierrez and Adrian Jarabo", "title": "Fast Back-Projection for Non-Line of Sight Reconstruction", "comments": null, "journal-ref": "Opt. Express 25, 11574-11583 (2017)", "doi": "10.1364/OE.25.011574", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated non-line of sight (NLOS) reconstruction by\nusing the time-resolved signal frommultiply scattered light. These works\ncombine ultrafast imaging systems with computation, which back-projects the\nrecorded space-time signal to build a probabilistic map of the hidden geometry.\nUnfortunately, this computation is slow, becoming a bottleneck as the imaging\ntechnology improves. In this work, we propose a new back-projection technique\nfor NLOS reconstruction, which is up to a thousand times faster than previous\nwork, with almost no quality loss. We base on the observation that the hidden\ngeometry probability map can be built as the intersection of the three-bounce\nspace-time manifolds defined by the light illuminating the hidden geometry and\nthe visible point receiving the scattered light from such hidden geometry. This\nallows us to pose the reconstruction of the hidden geometry as the voxelization\nof these space-time manifolds, which has lower theoretic complexity and is\neasily implementable in the GPU. We demonstrate the efficiency and quality of\nour technique compared against previous methods in both captured and synthetic\ndata\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 18:33:01 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 11:02:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Arellano", "Victor", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1703.02635", "submitter": "Quercus Hern\\'andez La\\'in", "authors": "Quercus Hernandez, Diego Gutierrez, Adrian Jarabo", "title": "A Computational Model of a Single-Photon Avalanche Diode Sensor for\n  Transient Imaging", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ins-det cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-Photon Avalanche Diodes (SPAD) are affordable photodetectors, capable\nto collect extremely fast low-energy events, due to their single-photon\nsensibility. This makes them very suitable for time-of-flight-based range\nimaging systems, allowing to reduce costs and power requirements, without\nsacrifizing much temporal resolution. In this work we describe a computational\nmodel to simulate the behaviour of SPAD sensors, aiming to provide a realistic\ncamera model for time-resolved light transport simulation, with applications on\nprototyping new reconstructions techniques based on SPAD time-of-flight data.\nOur model accounts for the major effects of the sensor on the incoming signal.\nWe compare our model against real-world measurements, and apply it to a variety\nof scenarios, including complex multiply-scattered light transport.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 10:17:34 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Hernandez", "Quercus", ""], ["Gutierrez", "Diego", ""], ["Jarabo", "Adrian", ""]]}, {"id": "1703.03199", "submitter": "Jan Egger", "authors": "Xiaojun Chen, Xing Li, Lu Xu, Yi Sun, Constantinus Politis, Jan Egger", "title": "Development of a computer-aided design software for dental splint in\n  orthognathic surgery", "comments": "10 pages, 10 figures, 1 table, 18 references", "journal-ref": "Sci. Rep. 6, 38867 (2016)", "doi": "10.1038/srep38867", "report-no": null, "categories": "physics.med-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the orthognathic surgery, dental splints are important and necessary to\nhelp the surgeon reposition the maxilla or mandible. However, the traditional\nmethods of manual design of dental splints are difficult and time-consuming.\nThe research on computer-aided design software for dental splints is rarely\nreported. Our purpose is to develop a novel special software named EasySplint\nto design the dental splints conveniently and efficiently. The design can be\ndivided into two steps, which are the generation of initial splint base and the\nBoolean operation between it and the maxilla-mandibular model. The initial\nsplint base is formed by ruled surfaces reconstructed using the manually picked\npoints. Then, a method to accomplish Boolean operation based on the distance\nfiled of two meshes is proposed. The interference elimination can be conducted\non the basis of marching cubes algorithm and Boolean operation. The accuracy of\nthe dental splint can be guaranteed since the original mesh is utilized to form\nthe result surface. Using EasySplint, the dental splints can be designed in\nabout 10 minutes and saved as a stereo lithography (STL) file for 3D printing\nin clinical applications. Three phantom experiments were conducted and the\nefficiency of our method was demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 09:45:30 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chen", "Xiaojun", ""], ["Li", "Xing", ""], ["Xu", "Lu", ""], ["Sun", "Yi", ""], ["Politis", "Constantinus", ""], ["Egger", "Jan", ""]]}, {"id": "1703.03202", "submitter": "Jan Egger", "authors": "Jan Egger, Markus Gall, Alois Tax, Muammer \\\"Ucal, Ulrike Zefferer,\n  Xing Li, Gord von Campe, Ute Sch\\\"afer, Dieter Schmalstieg, Xiaojun Chen", "title": "Interactive reconstructions of cranial 3D implants under MeVisLab as an\n  alternative to commercial planning software", "comments": "20 pages, 14 figures, 31 references", "journal-ref": "PLoS ONE 12(3): e0172694 (2017)", "doi": "10.1371/journal.pone.0172694", "report-no": null, "categories": "physics.med-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this publication, the interactive planning and reconstruction of cranial\n3D Implants under the medical prototyping platform MeVisLab as alternative to\ncommercial planning software is introduced. In doing so, a MeVisLab prototype\nconsisting of a customized data-flow network and an own C++ module was set up.\nAs a result, the Computer-Aided Design (CAD) software prototype guides a user\nthrough the whole workflow to generate an implant. Therefore, the workflow\nbegins with loading and mirroring the patients head for an initial curvature of\nthe implant. Then, the user can perform an additional Laplacian smoothing,\nfollowed by a Delaunay triangulation. The result is an aesthetic looking and\nwell-fitting 3D implant, which can be stored in a CAD file format, e.g.\nSTereoLithography (STL), for 3D printing. The 3D printed implant can finally be\nused for an in-depth pre-surgical evaluation or even as a real implant for the\npatient. In a nutshell, our research and development shows that a customized\nMeVisLab software prototype can be used as an alternative to complex commercial\nplanning software, which may also not be available in every clinic. Finally,\nnot to conform ourselves directly to available commercial software and look for\nother options that might improve the workflow.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 09:52:20 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Egger", "Jan", ""], ["Gall", "Markus", ""], ["Tax", "Alois", ""], ["\u00dccal", "Muammer", ""], ["Zefferer", "Ulrike", ""], ["Li", "Xing", ""], ["von Campe", "Gord", ""], ["Sch\u00e4fer", "Ute", ""], ["Schmalstieg", "Dieter", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1703.04861", "submitter": "Kun Li", "authors": "Kun Li, Jingyu Yang, Yu-Kun Lai, Daoliang Guo", "title": "Robust Non-Rigid Registration with Reweighted Position and\n  Transformation Sparsity", "comments": "IEEE Transactions on Visualization and Computer Graphics", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics ( Volume:\n  25 , Issue: 6 , June 1 2019 )", "doi": "10.1109/TVCG.2018.2832136", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-rigid registration is challenging because it is ill-posed with high\ndegrees of freedom and is thus sensitive to noise and outliers. We propose a\nrobust non-rigid registration method using reweighted sparsities on position\nand transformation to estimate the deformations between 3-D shapes. We\nformulate the energy function with position and transformation sparsity on both\nthe data term and the smoothness term, and define the smoothness constraint\nusing local rigidity. The double sparsity based non-rigid registration model is\nenhanced with a reweighting scheme, and solved by transferring the model into\nfour alternately-optimized subproblems which have exact solutions and\nguaranteed convergence. Experimental results on both public datasets and real\nscanned datasets show that our method outperforms the state-of-the-art methods\nand is more robust to noise and outliers than conventional non-rigid\nregistration methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:00:44 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 03:21:05 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Li", "Kun", ""], ["Yang", "Jingyu", ""], ["Lai", "Yu-Kun", ""], ["Guo", "Daoliang", ""]]}, {"id": "1703.05700", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Tom Yeh, Koji Yatani, Mark D. Gross", "title": "Autocomplete Textures for 3D Printing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is an essential property of physical objects that affects aesthetics,\nusability, and functionality. However, designing and applying textures to 3D\nobjects with existing tools remains difficult and time-consuming; it requires\nproficient 3D modeling skills. To address this, we investigated an\nauto-completion approach for efficient texture creation that automates the\ntedious, repetitive process of applying texture while allowing flexible\ncustomization. We developed techniques for users to select a target surface,\nsketch and manipulate a texture with 2D drawings, and then generate 3D\nprintable textures onto an arbitrary curved surface. In a controlled experiment\nour tool sped texture creation by 80% over conventional tools, a performance\ngain that is higher with more complex target surfaces. This result confirms\nthat auto-completion is powerful for creating 3D textures.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:24:01 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Suzuki", "Ryo", ""], ["Yeh", "Tom", ""], ["Yatani", "Koji", ""], ["Gross", "Mark D.", ""]]}, {"id": "1703.06003", "submitter": "Huy Phan", "authors": "Huy Q. Phan, Hongbo Fu, and Antoni B. Chan", "title": "Color Orchestra: Ordering Color Palettes for Interpolation and\n  Prediction", "comments": "IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color theme or color palette can deeply influence the quality and the feeling\nof a photograph or a graphical design. Although color palettes may come from\ndifferent sources such as online crowd-sourcing, photographs and graphical\ndesigns, in this paper, we consider color palettes extracted from fine art\ncollections, which we believe to be an abundant source of stylistic and unique\ncolor themes. We aim to capture color styles embedded in these collections by\nmeans of statistical models and to build practical applications upon these\nmodels. As artists often use their personal color themes in their paintings,\nmaking these palettes appear frequently in the dataset, we employed density\nestimation to capture the characteristics of palette data. Via density\nestimation, we carried out various predictions and interpolations on palettes,\nwhich led to promising applications such as photo-style exploration, real-time\ncolor suggestion, and enriched photo recolorization. It was, however,\nchallenging to apply density estimation to palette data as palettes often come\nas unordered sets of colors, which make it difficult to use conventional\nmetrics on them. To this end, we developed a divide-and-conquer sorting\nalgorithm to rearrange the colors in the palettes in a coherent order, which\nallows meaningful interpolation between color palettes. To confirm the\nperformance of our model, we also conducted quantitative experiments on\ndatasets of digitized paintings collected from the Internet and received\nfavorable results.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 13:25:49 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Phan", "Huy Q.", ""], ["Fu", "Hongbo", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1703.07167", "submitter": "Cosmin Anitescu", "authors": "Chiu Ling Chan, Cosmin Anitescu, Timon Rabczuk", "title": "Volumetric parametrization from a level set boundary representation with\n  PHT Splines", "comments": null, "journal-ref": "Computer-Aided Design, Volume 82, January 2017, Pages 29-41", "doi": "10.1016/j.cad.2016.08.008", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in isogeometric analysis is constructing analysis-suitable\nvolumetric meshes which can accurately represent the geometry of a given\nphysical domain. In this paper, we propose a method to derive a spline-based\nrepresentation of a domain of interest from voxel-based data. We show an\nefficient way to obtain a boundary representation of the domain by a level-set\nfunction. Then, we use the geometric information from the boundary (the normal\nvectors and curvature) to construct a matching C1 representation with\nhierarchical cubic splines. The approximation is done by a single template and\nlinear transformations (scaling, translations and rotations) without the need\nfor solving an optimization problem. We illustrate our method with several\nexamples in two and three dimensions, and show good performance on some\nstandard benchmark test problems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 10:28:00 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Chan", "Chiu Ling", ""], ["Anitescu", "Cosmin", ""], ["Rabczuk", "Timon", ""]]}, {"id": "1703.07255", "submitter": "Hao Wang", "authors": "Hao Wang, Xiaodan Liang, Hao Zhang, Dit-Yan Yeung, Eric P. Xing", "title": "ZM-Net: Real-time Zero-shot Image Manipulation Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in image processing and computer vision (e.g. colorization,\nstyle transfer) can be posed as 'manipulating' an input image into a\ncorresponding output image given a user-specified guiding signal. A holy-grail\nsolution towards generic image manipulation should be able to efficiently alter\nan input image with any personalized signals (even signals unseen during\ntraining), such as diverse paintings and arbitrary descriptive attributes.\nHowever, existing methods are either inefficient to simultaneously process\nmultiple signals (let alone generalize to unseen signals), or unable to handle\nsignals from other modalities. In this paper, we make the first attempt to\naddress the zero-shot image manipulation task. We cast this problem as\nmanipulating an input image according to a parametric model whose key\nparameters can be conditionally generated from any guiding signal (even unseen\nones). To this end, we propose the Zero-shot Manipulation Net (ZM-Net), a\nfully-differentiable architecture that jointly optimizes an\nimage-transformation network (TNet) and a parameter network (PNet). The PNet\nlearns to generate key transformation parameters for the TNet given any guiding\nsignal while the TNet performs fast zero-shot image manipulation according to\nboth signal-dependent parameters from the PNet and signal-invariant parameters\nfrom the TNet itself. Extensive experiments show that our ZM-Net can perform\nhigh-quality image manipulation conditioned on different forms of guiding\nsignals (e.g. style images and attributes) in real-time (tens of milliseconds\nper image) even for unseen signals. Moreover, a large-scale style dataset with\nover 20,000 style images is also constructed to promote further research.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 15:01:59 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:08:40 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Wang", "Hao", ""], ["Liang", "Xiaodan", ""], ["Zhang", "Hao", ""], ["Yeung", "Dit-Yan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1703.07575", "submitter": "Jan Egger", "authors": "Jan Egger, Markus Gall, J\\\"urgen Wallner, Pedro Boechat, Alexander\n  Hann, Xing Li, Xiaojun Chen, Dieter Schmalstieg", "title": "HTC Vive MeVisLab integration via OpenVR for medical applications", "comments": "14 pages, 11 figures, 40 references", "journal-ref": "PLoS ONE 12(3): e0173972 (2017)", "doi": "10.1371/journal.pone.0173972", "report-no": null, "categories": "cs.GR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality, an immersive technology that replicates an environment via\ncomputer-simulated reality, gets a lot of attention in the entertainment\nindustry. However, VR has also great potential in other areas, like the medical\ndomain, Examples are intervention planning, training and simulation. This is\nespecially of use in medical operations, where an aesthetic outcome is\nimportant, like for facial surgeries. Alas, importing medical data into Virtual\nReality devices is not necessarily trivial, in particular, when a direct\nconnection to a proprietary application is desired. Moreover, most researcher\ndo not build their medical applications from scratch, but rather leverage\nplatforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in\ncommon that they use libraries like ITK and VTK, and provide a convenient\ngraphical interface. However, ITK and VTK do not support Virtual Reality\ndirectly. In this study, the usage of a Virtual Reality device for medical data\nunder the MeVisLab platform is presented. The OpenVR library is integrated into\nthe MeVisLab platform, allowing a direct and uncomplicated usage of the head\nmounted display HTC Vive inside the MeVisLab platform. Medical data coming from\nother MeVisLab modules can directly be connected per drag-and-drop to the\nVirtual Reality module, rendering the data inside the HTC Vive for immersive\nvirtual reality inspection.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 09:21:00 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Egger", "Jan", ""], ["Gall", "Markus", ""], ["Wallner", "J\u00fcrgen", ""], ["Boechat", "Pedro", ""], ["Hann", "Alexander", ""], ["Li", "Xing", ""], ["Chen", "Xiaojun", ""], ["Schmalstieg", "Dieter", ""]]}, {"id": "1703.07729", "submitter": "Ethan Kerzner", "authors": "Ethan Kerzner, Alexander Lex, Crystal Lynn Sigulinsky, Timothy Urness,\n  Bryan William Jones, Robert E. Marc, Miriah Meyer", "title": "Graffinity: Visualizing Connectivity In Large Graphs", "comments": "The definitive version is available at http://diglib.eg.org/ and\n  http://onlinelibrary.wiley.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate graphs are prolific across many fields, including transportation\nand neuroscience. A key task in graph analysis is the exploration of\nconnectivity, to, for example, analyze how signals flow through neurons, or to\nexplore how well different cities are connected by flights. While standard\nnode-link diagrams are helpful in judging connectivity, they do not scale to\nlarge networks. Adjacency matrices also do not scale to large networks and are\nonly suitable to judge connectivity of adjacent nodes. A key approach to\nrealize scalable graph visualization are queries: instead of displaying the\nwhole network, only a relevant subset is shown. Query-based techniques for\nanalyzing connectivity in graphs, however, can also easily suffer from\ncluttering if the query result is big enough. To remedy this, we introduce\ntechniques that provide an overview of the connectivity and reveal details on\ndemand. We have two main contributions: (1) two novel visualization techniques\nthat work in concert for summarizing graph connectivity; and (2) Graffinity, an\nopen-source implementation of these visualizations supplemented by detail views\nto enable a complete analysis workflow. Graffinity was designed in a close\ncollaboration with neuroscientists and is optimized for connectomics data\nanalysis, yet the technique is applicable across domains. We validate the\nconnectivity overview and our open-source tool with illustrative examples using\nflight and connectomics data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:23:32 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Kerzner", "Ethan", ""], ["Lex", "Alexander", ""], ["Sigulinsky", "Crystal Lynn", ""], ["Urness", "Timothy", ""], ["Jones", "Bryan William", ""], ["Marc", "Robert E.", ""], ["Meyer", "Miriah", ""]]}, {"id": "1703.07869", "submitter": "Jens Grubert", "authors": "Peter Mohr, Markus Tatzgern, Jens Grubert, Dieter Schmalstieg, Denis\n  Kalkofen", "title": "Adaptive User Perspective Rendering for Handheld Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handheld Augmented Reality commonly implements some variant of magic lens\nrendering, which turns only a fraction of the user's real environment into AR\nwhile the rest of the environment remains unaffected. Since handheld AR devices\nare commonly equipped with video see-through capabilities, AR magic lens\napplications often suffer from spatial distortions, because the AR environment\nis presented from the perspective of the camera of the mobile device. Recent\napproaches counteract this distortion based on estimations of the user's head\nposition, rendering the scene from the user's perspective. To this end,\napproaches usually apply face-tracking algorithms on the front camera of the\nmobile device. However, this demands high computational resources and therefore\ncommonly affects the performance of the application beyond the already high\ncomputational load of AR applications. In this paper, we present a method to\nreduce the computational demands for user perspective rendering by applying\nlightweight optical flow tracking and an estimation of the user's motion before\nhead tracking is started. We demonstrate the suitability of our approach for\ncomputationally limited mobile devices and we compare it to device perspective\nrendering, to head tracked user perspective rendering, as well as to fixed\npoint of view user perspective rendering.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 21:56:58 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Mohr", "Peter", ""], ["Tatzgern", "Markus", ""], ["Grubert", "Jens", ""], ["Schmalstieg", "Dieter", ""], ["Kalkofen", "Denis", ""]]}, {"id": "1703.08014", "submitter": "Timo von Marcard", "authors": "Timo von Marcard, Bodo Rosenhahn, Michael J. Black, Gerard Pons-Moll", "title": "Sparse Inertial Poser: Automatic 3D Human Pose Estimation from Sparse\n  IMUs", "comments": "12 pages, Accepted at Eurographics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of making human motion capture in the wild more\npractical by using a small set of inertial sensors attached to the body. Since\nthe problem is heavily under-constrained, previous methods either use a large\nnumber of sensors, which is intrusive, or they require additional video input.\nWe take a different approach and constrain the problem by: (i) making use of a\nrealistic statistical body model that includes anthropometric constraints and\n(ii) using a joint optimization framework to fit the model to orientation and\nacceleration measurements over multiple frames. The resulting tracker Sparse\nInertial Poser (SIP) enables 3D human pose estimation using only 6 sensors\n(attached to the wrists, lower legs, back and head) and works for arbitrary\nhuman motions. Experiments on the recently released TNT15 dataset show that,\nusing the same number of sensors, SIP achieves higher accuracy than the dataset\nbaseline without using any video data. We further demonstrate the effectiveness\nof SIP on newly recorded challenging motions in outdoor scenarios such as\nclimbing or jumping over a wall.\n", "versions": [{"version": "v1", "created": "Thu, 23 Mar 2017 11:35:41 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 08:24:07 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["von Marcard", "Timo", ""], ["Rosenhahn", "Bodo", ""], ["Black", "Michael J.", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1703.09964", "submitter": "Siavash Arjomand Bigdeli", "authors": "Siavash Arjomand Bigdeli and Matthias Zwicker", "title": "Image Restoration using Autoencoding Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to leverage denoising autoencoder networks as priors to address\nimage restoration problems. We build on the key observation that the output of\nan optimal denoising autoencoder is a local mean of the true data density, and\nthe autoencoder error (the difference between the output and input of the\ntrained autoencoder) is a mean shift vector. We use the magnitude of this mean\nshift vector, that is, the distance to the local mean, as the negative log\nlikelihood of our natural image prior. For image restoration, we maximize the\nlikelihood using gradient descent by backpropagating the autoencoder error. A\nkey advantage of our approach is that we do not need to train separate networks\nfor different image restoration tasks, such as non-blind deconvolution with\ndifferent kernels, or super-resolution at different magnification factors. We\ndemonstrate state of the art results for non-blind deconvolution and\nsuper-resolution using the same autoencoding prior.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 10:51:49 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Bigdeli", "Siavash Arjomand", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1703.10405", "submitter": "Mengqi Peng", "authors": "Mengqi Peng, Jun Xing, Li-Yi Wei", "title": "Autocomplete 3D Sculpting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital sculpting is a popular means to create 3D models but remains a\nchallenging task for many users. This can be alleviated by recent advances in\ndata-driven and procedural modeling, albeit bounded by the underlying data and\nprocedures. We propose a 3D sculpting system that assists users in freely\ncreating models without predefined scope. With a brushing interface similar to\ncommon sculpting tools, our system silently records and analyzes users'\nworkflows, and predicts what they might or should do in the future to reduce\ninput labor or enhance output quality. Users can accept, ignore, or modify the\nsuggestions and thus maintain full control and individual style. They can also\nexplicitly select and clone past workflows over output model regions. Our key\nidea is to consider how a model is authored via dynamic workflows in addition\nto what it is shaped in static geometry, for more accurate analysis of user\nintentions and more general synthesis of shape structures. The workflows\ncontain potential repetitions for analysis and synthesis, including user inputs\n(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions\non an object surface), and camera viewpoints. We evaluate our method via user\nfeedbacks and authored models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 11:06:02 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Peng", "Mengqi", ""], ["Xing", "Jun", ""], ["Wei", "Li-Yi", ""]]}]