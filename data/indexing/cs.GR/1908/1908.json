[{"id": "1908.00056", "submitter": "Marcelo Ponce", "authors": "Ramses van Zon, Marcelo Ponce", "title": "Software-Enhanced Teaching and Visualization Capabilities of an\n  Ultra-High-Resolution Video Wall", "comments": "PEARC'19: \"Practice and Experience in Advanced Research Computing\",\n  July 28-August 1, 2019 - Chicago, IL, USA", "journal-ref": null, "doi": "10.1145/3332186.3333149", "report-no": null, "categories": "cs.GR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a modular approach to enhance the capabilities and\nfeatures of a visualization and teaching room using software. This approach was\napplied to a room with a large, high resolution (7680$\\times$4320 pixels),\ntiled screen of 13 $\\times$ 7.5 feet as its main display, and with a variety of\naudio and video inputs, connected over a network. Many of the techniques\ndescribed are possible because of a software-enhanced setup, utilizing existing\nhardware and a collection of mostly open-source tools, allowing to perform\ncollaborative, high-resolution visualizations as well as broadcasting and\nrecording workshops and lectures. The software approach is flexible and allows\none to add functionality without changing the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 19:05:39 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["van Zon", "Ramses", ""], ["Ponce", "Marcelo", ""]]}, {"id": "1908.00073", "submitter": "Cindy Xiong", "authors": "Cindy Xiong, Cristina R. Ceja, Casimir J.H. Ludwig, and Steven\n  Franconeri", "title": "Biased Average Position Estimates in Line and Bar Graphs:\n  Underestimation, Overestimation, and Perceptual Pull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual depictions of data, position (i.e., the vertical height of a line\nor a bar) is believed to be the most precise way to encode information compared\nto other encodings (e.g., hue). Not only are other encodings less precise than\nposition, but they can also be prone to systematic biases (e.g., color category\nboundaries can distort perceived differences between hues). By comparison,\nposition's high level of precision may seem to protect it from such biases. In\ncontrast, across three empirical studies, we show that while position may be a\nprecise form of data encoding, it can also produce systematic biases in how\nvalues are visually encoded, at least for reports of average position across a\nshort delay. In displays with a single line or a single set of bars, reports of\naverage positions were significantly biased, such that line positions were\nunderestimated and bar positions were overestimated. In displays with multiple\ndata series (i.e., multiple lines and/or sets of bars), this systematic bias\nstill persisted. We also observed an effect of \"perceptual pull\", where the\naverage position estimate for each series was 'pulled' toward the other. These\nfindings suggest that, although position may still be the most precise form of\nvisual data encoding, it can also be systematically biased.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:14:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Xiong", "Cindy", ""], ["Ceja", "Cristina R.", ""], ["Ludwig", "Casimir J. H.", ""], ["Franconeri", "Steven", ""]]}, {"id": "1908.00398", "submitter": "Minkesh Asati", "authors": "Asati Minkesh, Kraisittipong Worranitta, Miyachi Taizo", "title": "Extract and Merge: Merging extracted humans from different images\n  utilizing Mask R-CNN", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting human objects out of the various type of objects in images and\nmerging them with other scenes is manual and day-to-day work for photo editors.\nAlthough recently Adobe photoshop released \"select subject\" tool which\nautomatically selects the foreground object in an image, but still requires\nfine manual tweaking separately. In this work, we proposed an application\nutilizing Mask R-CNN (for object detection and mask segmentation) that can\nextract human instances from multiple images and merge them with a new\nbackground. This application does not add any overhead to Mask R-CNN, running\nat 5 frames per second. It can extract human instances from any number of\nimages or videos from merging them together. We also structured the code to\naccept videos of different lengths as input and length of the output-video will\nbe equal to the longest input-video. We wanted to create a simple yet effective\napplication that can serve as a base for photo editing and do most\ntime-consuming work automatically, so, editors can focus more on the design\npart. Other application could be to group people together in a single picture\nwith a new background from different images which could not be physically\ntogether. We are showing single-person and multi-person extraction and\nplacement in two different backgrounds. Also, we are showing a video example\nwith single-person extraction.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:50:00 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Minkesh", "Asati", ""], ["Worranitta", "Kraisittipong", ""], ["Taizo", "Miyachi", ""]]}, {"id": "1908.00407", "submitter": "Wenbin He", "authors": "Wenbin He, Junpeng Wang, Hanqi Guo, Ko-Chih Wang, Han-Wei Shen, Mukund\n  Raj, Youssef S. G. Nashed, Tom Peterka", "title": "InSituNet: Deep Image Synthesis for Parameter Space Exploration of\n  Ensemble Simulations", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934312", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose InSituNet, a deep learning based surrogate model to support\nparameter space exploration for ensemble simulations that are visualized in\nsitu. In situ visualization, generating visualizations at simulation time, is\nbecoming prevalent in handling large-scale simulations because of the I/O and\nstorage constraints. However, in situ visualization approaches limit the\nflexibility of post-hoc exploration because the raw simulation data are no\nlonger available. Although multiple image-based approaches have been proposed\nto mitigate this limitation, those approaches lack the ability to explore the\nsimulation parameters. Our approach allows flexible exploration of parameter\nspace for large-scale ensemble simulations by taking advantage of the recent\nadvances in deep learning. Specifically, we design InSituNet as a convolutional\nregression model to learn the mapping from the simulation and visualization\nparameters to the visualization results. With the trained model, users can\ngenerate new images for different simulation parameters under various\nvisualization settings, which enables in-depth analysis of the underlying\nensemble simulations. We demonstrate the effectiveness of InSituNet in\ncombustion, cosmology, and ocean simulations through quantitative and\nqualitative evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:07:12 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 20:38:34 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:04:57 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["He", "Wenbin", ""], ["Wang", "Junpeng", ""], ["Guo", "Hanqi", ""], ["Wang", "Ko-Chih", ""], ["Shen", "Han-Wei", ""], ["Raj", "Mukund", ""], ["Nashed", "Youssef S. G.", ""], ["Peterka", "Tom", ""]]}, {"id": "1908.00500", "submitter": "Frederik Dennig", "authors": "David Pomerenke, Frederik L. Dennig, Daniel A. Keim, Johannes Fuchs,\n  Michael Blumenschein", "title": "Slope-Dependent Rendering of Parallel Coordinates to Reduce Density\n  Distortion and Ghost Clusters", "comments": "5 pages, 5 figures, LaTeX; added DOI", "journal-ref": "2019 IEEE Visualization Conference (VIS)", "doi": "10.1109/VISUAL.2019.8933706", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinates are a popular technique to visualize multi-dimensional\ndata. However, they face a significant problem influencing the perception and\ninterpretation of patterns. The distance between two parallel lines differs\nbased on their slope. Vertical lines are rendered longer and closer to each\nother than horizontal lines. This problem is inherent in the technique and has\ntwo main consequences: (1) clusters which have a steep slope between two axes\nare visually more prominent than horizontal clusters. (2) Noise and clutter can\nbe perceived as clusters, as a few parallel vertical lines visually emerge as a\nghost cluster. Our paper makes two contributions: First, we formalize the\nproblem and show its impact. Second, we present a novel technique to reduce the\neffects by rendering the polylines of the parallel coordinates based on their\nslope: horizontal lines are rendered with the default width, lines with a steep\nslope with a thinner line. Our technique avoids density distortions of\nclusters, can be computed in linear time, and can be added on top of most\nparallel coordinate variations. To demonstrate the usefulness, we show examples\nand compare them to the classical rendering.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:51:00 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 15:21:07 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Pomerenke", "David", ""], ["Dennig", "Frederik L.", ""], ["Keim", "Daniel A.", ""], ["Fuchs", "Johannes", ""], ["Blumenschein", "Michael", ""]]}, {"id": "1908.00559", "submitter": "Kyle Hall", "authors": "Kyle Wm. Hall, Adam J. Bradley, Uta Hinrichs, Samuel Huron, Jo Wood,\n  Christopher Collins, Sheelagh Carpendale", "title": "Design by Immersion: A Transdisciplinary Approach to Problem-Driven\n  Visualizations", "comments": "The paper has been accepted to IEEE VIS (InfoVis) 2019, and will\n  appear IEEE TVCG. ACM 2012 CCS - Human-centered computing, Visualization,\n  Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934790", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While previous work exists on how to conduct and disseminate insights from\nproblem-driven visualization projects and design studies, the literature does\nnot address how to accomplish these goals in transdisciplinary teams in ways\nthat advance all disciplines involved. In this paper we introduce and define a\nnew methodological paradigm we call design by immersion, which provides an\nalternative perspective on problem-driven visualization work. Design by\nimmersion embeds transdisciplinary experiences at the center of the\nvisualization process by having visualization researchers participate in the\nwork of the target domain (or domain experts participate in visualization\nresearch). Based on our own combined experiences of working on\ncross-disciplinary, problem-driven visualization projects, we present six case\nstudies that expose the opportunities that design by immersion enables,\nincluding (1) exploring new domain-inspired visualization design spaces, (2)\nenriching domain understanding through personal experiences, and (3) building\nstrong transdisciplinary relationships. Furthermore, we illustrate how the\nprocess of design by immersion opens up a diverse set of design activities that\ncan be combined in different ways depending on the type of collaboration,\nproject, and goals. Finally, we discuss the challenges and potential pitfalls\nof design by immersion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:03:22 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 14:56:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Hall", "Kyle Wm.", ""], ["Bradley", "Adam J.", ""], ["Hinrichs", "Uta", ""], ["Huron", "Samuel", ""], ["Wood", "Jo", ""], ["Collins", "Christopher", ""], ["Carpendale", "Sheelagh", ""]]}, {"id": "1908.00575", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra,\n  Leonidas J. Guibas", "title": "StructureNet: Hierarchical Graph Networks for 3D Shape Generation", "comments": "Conditionally Accepted to Siggraph Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate novel, diverse, and realistic 3D shapes along with\nassociated part semantics and structure is central to many applications\nrequiring high-quality 3D assets or large volumes of realistic training data. A\nkey challenge towards this goal is how to accommodate diverse shape variations,\nincluding both continuous deformations of parts as well as structural or\ndiscrete alterations which add to, remove from, or modify the shape\nconstituents and compositional structure. Such object structure can typically\nbe organized into a hierarchy of constituent object parts and relationships,\nrepresented as a hierarchy of n-ary graphs. We introduce StructureNet, a\nhierarchical graph network which (i) can directly encode shapes represented as\nsuch n-ary graphs; (ii) can be robustly trained on large and complex shape\nfamilies; and (iii) can be used to generate a great diversity of realistic\nstructured shape geometries. Technically, we accomplish this by drawing\ninspiration from recent advances in graph neural networks to propose an\norder-invariant encoding of n-ary graphs, considering jointly both part\ngeometry and inter-part relations during network training. We extensively\nevaluate the quality of the learned latent spaces for various shape families\nand show significant advantages over baseline and competing methods. The\nlearned latent spaces enable several structure-aware geometry processing\napplications, including shape generation and interpolation, shape editing, or\nshape structure discovery directly from un-annotated images, point clouds, or\npartial scans.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:43:24 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Mo", "Kaichun", ""], ["Guerrero", "Paul", ""], ["Yi", "Li", ""], ["Su", "Hao", ""], ["Wonka", "Peter", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1908.00576", "submitter": "Michael Blumenschein", "authors": "Matthias Miller, Xuan Zhang, Johannes Fuchs, Michael Blumenschein", "title": "Evaluating Ordering Strategies of Star Glyph Axes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star glyphs are a well-researched visualization technique to represent\nmulti-dimensional data. They are often used in small multiple settings for a\nvisual comparison of many data points. However, their overall visual appearance\nis strongly influenced by the ordering of dimensions. To this end, two\northogonal categories of layout strategies are proposed in the literature:\norder dimensions by similarity to get homogeneously shaped glyphs vs. order by\ndissimilarity to emphasize spikes and salient shapes. While there is evidence\nthat salient shapes support clustering tasks, evaluation, and direct comparison\nof data-driven ordering strategies has not received much research attention. We\ncontribute an empirical user study to evaluate the efficiency, effectiveness,\nand user confidence in visual clustering tasks using star glyphs. In comparison\nto similarity-based ordering, our results indicate that dissimilarity-based\nstar glyph layouts support users better in clustering tasks, especially when\nclutter is present.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:45:17 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Miller", "Matthias", ""], ["Zhang", "Xuan", ""], ["Fuchs", "Johannes", ""], ["Blumenschein", "Michael", ""]]}, {"id": "1908.00580", "submitter": "Jorge Alberto Wagner Filho", "authors": "Jorge A. Wagner Filho, Wolfgang Stuerzlinger and Luciana Nedel", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive\n  Trajectory Data Exploration", "comments": "IEEE VIS InfoVis 2019 (Transactions on Visualization and Computer\n  Graphics)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934415", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Space-Time Cube enables analysts to clearly observe spatio-temporal\nfeatures in movement trajectory datasets in geovisualization. However, its\ngeneral usability is impacted by a lack of depth cues, a reported steep\nlearning curve, and the requirement for efficient 3D navigation. In this work,\nwe investigate a Space-Time Cube in the Immersive Analytics domain. Based on a\nreview of previous work and selecting an appropriate exploration metaphor, we\nbuilt a prototype environment where the cube is coupled to a virtual\nrepresentation of the analyst's real desk, and zooming and panning in space and\ntime are intuitively controlled using mid-air gestures. We compared our\nimmersive environment to a desktop-based implementation in a user study with 20\nparticipants across 7 tasks of varying difficulty, which targeted different\nuser interface features. To investigate how performance is affected in the\npresence of clutter, we explored two scenarios with different numbers of\ntrajectories. While the quantitative performance was similar for the majority\nof tasks, large differences appear when we analyze the patterns of interaction\nand consider subjective metrics. The immersive version of the Space-Time Cube\nreceived higher usability scores, much higher user preference, and was rated to\nhave a lower mental workload, without causing participants discomfort in\n25-minute-long VR sessions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:59:21 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 18:42:54 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Filho", "Jorge A. Wagner", ""], ["Stuerzlinger", "Wolfgang", ""], ["Nedel", "Luciana", ""]]}, {"id": "1908.00611", "submitter": "Daniel Weiskopf", "authors": "Daniel Weiskopf", "title": "Vis4Vis: Visualization for (Empirical) Visualization Research", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by participants of the Dagstuhl Seminar 18041 (\"Foundations of Data\n  Visualization\") and that is expected to be published by Springer. The final\n  book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate evaluation is a key component in visualization research. It is\ntypically based on empirical studies that assess visualization components or\ncomplete systems. While such studies often include the user of the\nvisualization, empirical research is not necessarily restricted to user studies\nbut may also address the technical performance of a visualization system such\nas its computational speed or memory consumption. Any such empirical experiment\nfaces the issue that the underlying visualization is becoming increasingly\nsophisticated, leading to an increasingly difficult evaluation in complex\nenvironments. Therefore, many of the established methods of empirical studies\ncan no longer capture the full complexity of the evaluation. One promising\nsolution is the use of data-rich observations that we can acquire during\nstudies to obtain more reliable interpretations of empirical research. For\nexample, we have been witnessing an increasing availability and use of\nphysiological sensor information from eye tracking, electrodermal activity\nsensors, electroencephalography, etc. Other examples are various kinds of logs\nof user activities such as mouse, keyboard, or touch interaction. Such\ndata-rich empirical studies promise to be especially useful for studies in the\nwild and similar scenarios outside of the controlled laboratory environment.\nHowever, with the growing availability of large, complex, time-dependent,\nheterogeneous, and unstructured observational data, we are facing the new\nchallenge of how we can analyze such data. This challenge can be addressed by\nestablishing the subfield of visualization for visualization (Vis4Vis):\nvisualization as a means of analyzing and communicating data from empirical\nstudies to advance visualization research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:23:53 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Weiskopf", "Daniel", ""]]}, {"id": "1908.00629", "submitter": "Danielle Szafir", "authors": "Stephen Smart, Keke Wu, Danielle Albers Szafir", "title": "Color Crafting: Automating the Construction of Designer Quality Color\n  Ramps", "comments": "IEEE VIS, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizations often encode numeric data using sequential and diverging color\nramps. Effective ramps use colors that are sufficiently discriminable, align\nwell with the data, and are aesthetically pleasing. Designers rely on years of\nexperience to create high-quality color ramps. However, it is challenging for\nnovice visualization developers that lack this experience to craft effective\nramps as most guidelines for constructing ramps are loosely defined qualitative\nheuristics that are often difficult to apply. Our goal is to enable\nvisualization developers to readily create effective color encodings using a\nsingle seed color. We do this using an algorithmic approach that models\ndesigner practices by analyzing patterns in the structure of designer-crafted\ncolor ramps. We construct these models from a corpus of 222 expert-designed\ncolor ramps, and use the results to automatically generate ramps that mimic\ndesigner practices. We evaluate our approach through an empirical study\ncomparing the outputs of our approach with designer-crafted color ramps. Our\nmodels produce ramps that support accurate and aesthetically pleasing\nvisualizations at least as well as designer ramps and that outperform\nconventional mathematical approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 21:06:23 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Smart", "Stephen", ""], ["Wu", "Keke", ""], ["Szafir", "Danielle Albers", ""]]}, {"id": "1908.00662", "submitter": "Yalong Yang", "authors": "Yalong Yang", "title": "Visualising Geographically-Embedded Origin-Destination Flows: in 2D and\n  immersive environments", "comments": "PhD Thesis, Monash University, Australia, December 2018. Update:\n  corrected typos in arXiv comments", "journal-ref": null, "doi": "10.26180/5c087e9980d8a", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis develops and evaluates effective techniques for visualisation of\nflows (e.g. of people, trade, knowledge) between places on geographic maps.\nThis geographically-embedded flow data contains information about geographic\nlocations, and flows from origin locations to destination locations. We\nexplored the design space of OD flow visualisation in both 2D and immersive\nenvironments. We do so by creating novel OD flow visualisations in both\nenvironments, and then conducting controlled user studies to evaluate different\ndesigns.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:57:09 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 23:51:49 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Yang", "Yalong", ""]]}, {"id": "1908.00902", "submitter": "Flip Phillips", "authors": "J. Farley Norman, James T. Todd, Flip Phillips", "title": "Effects of Illumination on the Categorization of Shiny Materials", "comments": "v2, 20 pages, 15 figures, 26 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present research was designed to examine how patterns of illumination\ninfluence the perceptual categorization of metal, shiny black, and shiny white\nmaterials. The stimuli depicted three possible objects that were illuminated by\nfive possible HDRI light maps, which varied in their overall distributions of\nilluminant directions and intensities. The surfaces included a low roughness\nchrome material, a shiny black material, and a shiny white material with both\ndiffuse and specular components. Observers rated each stimulus by adjusting\nfour sliders to indicate their confidence that the depicted material was metal,\nshiny black, shiny white or something else, and these adjustments were\nconstrained so that the sum of all four settings was always 100%. The results\nrevealed that the metal and shiny black categories are easily confused. For\nexample, metal materials with low intensity light maps or a narrow range of\nilluminant directions are often judged as shiny black, whereas shiny black\nmaterials with high intensity light maps or a wide range of illuminant\ndirections are often judged as metal. A spherical harmonic analysis was\nperformed on the different light maps in an effort to quantitatively predict\nhow they would bias observers' judgments of metal and shiny black surfaces.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 15:19:54 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 16:22:27 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Norman", "J. Farley", ""], ["Todd", "James T.", ""], ["Phillips", "Flip", ""]]}, {"id": "1908.01277", "submitter": "Yalong Yang", "authors": "Linda Woodburn, Yalong Yang and Kim Marriott", "title": "Interactive Visualisation of Hierarchical Quantitative Data: An\n  Evaluation", "comments": "Presented at IEEE VIS 2019 in Vancouver, Canada and included in the\n  VIS 2019 conference proceedings. Improved the image quality in the paper", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933545", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have compared three common visualisations for hierarchical quantitative\ndata, treemaps, icicle plots and sunburst charts as well as a semicircular\nvariant of sunburst charts we call the sundown chart. In a pilot study, we\nfound that the sunburst chart was least preferred. In a controlled study with\n12 participants, we compared treemaps, icicle plots and sundown charts. Treemap\nwas the least preferred and had a slower performance on a basic navigation task\nand slower performance and accuracy in hierarchy understanding tasks. The\nicicle plot and sundown chart had similar performance with slight user\npreference for the icicle plot.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 06:16:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 19:33:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 22:10:55 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Woodburn", "Linda", ""], ["Yang", "Yalong", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.01350", "submitter": "Vasileios Drakopoulos", "authors": "Dimitrios Matthes and Vasileios Drakopoulos", "title": "Another Simple but Faster Method for 2D Line Clipping", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": "10.5121/ijcga.2019.9301", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of methods for line clipping make a rather large number of\ncomparisons and involve a lot of calculations compared to modern ones. Most of\nthe times, they are not so efficient as well as not so simple and applicable to\nthe majority of cases. Besides the most popular ones, namely, Cohen-Sutherland,\nLiang-Barsky, Cyrus-Beck and Nicholl-Lee-Nicholl, other line-clipping methods\nhave been presented over the years, each one having its own advantages and\ndisadvantages. In this paper a new computation method for 2D line clipping\nagainst a rectangular window is introduced. The proposed method has been\ncompared with the afore-mentioned ones as well as with two others; namely,\nSkala and Kodituwakku-Wijeweera-Chamikara, with respect to the number of\noperations performed and the computation time. The performance of the proposed\nmethod has been found to be better than all of the above-mentioned methods and\nit is found to be very fast, simple and can be implemented easily in any\nprogramming language or integrated development environment.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 14:20:13 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Matthes", "Dimitrios", ""], ["Drakopoulos", "Vasileios", ""]]}, {"id": "1908.01608", "submitter": "Ye Yuan", "authors": "Ye Yuan, Jian Guan, Jianguo Sun", "title": "Blind SAR Image Despeckling Using Self-Supervised Dense Dilated\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despeckling is a key and indispensable step in SAR image preprocessing,\nexisting deep learning-based methods achieve SAR despeckling by learning some\nmappings between speckled (different looks) and clean images. However, there\nexist no clean SAR image in the real world. To this end, in this paper, we\npropose a self-supervised dense dilated convolutional neural network (BDSS) for\nblind SAR image despeckling. Proposed BDSS can still learn to suppress speckle\nnoise without clean ground truth by optimized for L2 loss. Besides, three\nenhanced dense blocks with dilated convolution are employed to improve network\nperformance. The synthetic and real-data experiments demonstrate that proposed\nBDSS can achieve despeckling effectively while maintaining well features such\nas edges, point targets, and radiometric. At last, we demonstrate that our\nproposed BDSS can achieve blind despeckling excellently, i.e., do not need to\ncare about the number of looks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 13:26:19 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 01:56:32 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Yuan", "Ye", ""], ["Guan", "Jian", ""], ["Sun", "Jianguo", ""]]}, {"id": "1908.01742", "submitter": "Daniil Osudin", "authors": "Daniil Osudin, Christopher Child and Yang-Hui He", "title": "Rendering Non-Euclidean Geometry in Real-Time Using Spherical and\n  Hyperbolic Trigonometry", "comments": "This is an author's version of the paper, which has been presented at\n  ICCS 2019 conference and published in conference proceedings. The final\n  authenticated publication is available online at\n  https://doi.org/10.1007/978-3-030-22750-0_49", "journal-ref": "Rodrigues J. et al. (eds) Computational Science - ICCS 2019. ICCS\n  2019. Lecture Notes in Computer Science, vol 11540. Springer, Cham", "doi": "10.1007/978-3-030-22750-0_49", "report-no": null, "categories": "cs.GR hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method of calculating and rendering shapes in a\nnon-Euclidean 2D space. In order to achieve this, we developed a physics and\ngraphics engine that uses hyperbolic trigonometry to calculate and subsequently\nrender the shapes in a 2D space of constant negative or positive curvature in\nreal-time. We have chosen to use polar coordinates to record the parameters of\nthe objects as well as an azimuthal equidistant projection to render the space\nonto the screen because of the multiple useful properties they have. For\nexample, polar coordinate system works well with trigonometric calculations,\ndue to the distance from the reference point (analogous to origin in Cartesian\ncoordinates) being one of the coordinates by definition. Azimuthal equidistant\nprojection is not a typical projection, used for neither spherical nor\nhyperbolic space, however one of the main features of our engine relies on it:\nchanging the curvature of the world in real-time without stopping the execution\nof the application in order to re-calculate the world. This is due to the\nprojection properties that work identically for both spherical and hyperbolic\nspace, as can be seen in the Figure 1 above. We will also be looking at the\ncomplexity analysis of this method as well as renderings that the engine\nproduces. Finally we will be discussing the limitations and possible\napplications of the created engine as well as potential improvements of the\ndescribed method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 17:33:45 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Osudin", "Daniil", ""], ["Child", "Christopher", ""], ["He", "Yang-Hui", ""]]}, {"id": "1908.01809", "submitter": "Jerry Jinfeng Guo", "authors": "Jerry Jinfeng Guo and Elmar Eisemann", "title": "Geometric Sample Reweighting for Monte Carlo Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general sample reweighting scheme and its underlying theory for\nthe integration of an unknown function with low dimensionality. Our method\nproduces better results than standard weighting schemes for common sampling\nstrategies, while avoiding bias. Our main insight is to link the weight\nderivation to the function reconstruction process during integration. The\nimplementation of our solution is simple and results in an improved convergence\nbehavior. We illustrate its benefit by applying our method to multiple Monte\nCarlo rendering problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 19:20:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Guo", "Jerry Jinfeng", ""], ["Eisemann", "Elmar", ""]]}, {"id": "1908.01906", "submitter": "Nathan Morrical", "authors": "Nathan Morrical, Will Usher, Ingo Wald, Valerio Pascucci", "title": "Efficient Space Skipping and Adaptive Sampling of Unstructured Volumes\n  Using Hardware Accelerated Ray Tracing", "comments": "4 pages, 6 figures, 1 supplemental page, IEEE VIS 2019 Conference\n  Short Paper, Author Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample based ray marching is an effective method for direct volume rendering\nof unstructured meshes. However, sampling such meshes remains expensive, and\nstrategies to reduce the number of samples taken have received relatively\nlittle attention. In this paper, we introduce a method for rendering\nunstructured meshes using a combination of a coarse spatial acceleration\nstructure and hardware-accelerated ray tracing. Our approach enables efficient\nempty space skipping and adaptive sampling of unstructured meshes, and\noutperforms a reference ray marcher by up to 7x.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 23:56:59 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Morrical", "Nathan", ""], ["Usher", "Will", ""], ["Wald", "Ingo", ""], ["Pascucci", "Valerio", ""]]}, {"id": "1908.01938", "submitter": "Hongwei Lin", "authors": "Chuanfeng Hu, Hongwei Lin", "title": "Heterogeneous porous scaffold generation in trivariate B-spline solid\n  with triply periodic minimal surface in the parametric domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A porous scaffold is a three-dimensional network structure composed of a\nlarge number of pores, and triply periodic minimal surfaces (TPMSs) are one of\nconventional tools for designing porous scaffolds. However, discontinuity,\nincompleteness, and high storage space requirements are the three main\nshortcomings of TPMSs for porous scaffold design. In this study, we developed\nan effective method for heterogeneous porous scaffold generation to overcome\nthe abovementioned shortcomings of TPMSs. The input of the proposed method is a\ntrivariate B-spline solid (TBSS) with a cubic parameter domain. The proposed\nmethod first constructs a threshold distribution field (TDF) in the cubic\nparameter domain, and then produces a continuous and complete TPMS within it.\nMoreover, by mapping the TPMS in the parametric domain to the TBSS, a\ncontinuous and complete porous scaffold is generated in the TBSS. In addition,\nif the TBSS does not satisfy engineering requirements, the TDF can be locally\nmodified in the parameter domain, and the porous scaffold in the TBSS can be\nrebuilt. We also defined a new storage space-saving file format based on the\nTDF to store porous scaffolds. The experimental results presented in this paper\ndemonstrate the effectiveness and efficiency of the method using a TBSS as well\nas the superior space-saving of the proposed storage format.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 03:02:50 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Hu", "Chuanfeng", ""], ["Lin", "Hongwei", ""]]}, {"id": "1908.01970", "submitter": "Yiqun Xu", "authors": "Yiqun Xu, Wei Hu, Shanshe Wang, Xinfeng Zhang, Shiqi Wang, Siwei Ma,\n  Zongming Guo, Wen Gao", "title": "Predictive Generalized Graph Fourier Transform for Attribute Compression\n  of Dynamic Point Clouds", "comments": "14 pages, 12 figures, accepted to IEEE Transactions on Circuits and\n  Systems for Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As 3D scanning devices and depth sensors advance, dynamic point clouds have\nattracted increasing attention as a format for 3D objects in motion, with\napplications in various fields such as immersive telepresence, navigation for\nautonomous driving and gaming. Nevertheless, the tremendous amount of data in\ndynamic point clouds significantly burden transmission and storage. To this\nend, we propose a complete compression framework for attributes of 3D dynamic\npoint clouds, focusing on optimal inter-coding. Firstly, we derive the optimal\ninter-prediction and predictive transform coding assuming the Gaussian Markov\nRandom Field model with respect to a spatio-temporal graph underlying the\nattributes of dynamic point clouds. The optimal predictive transform proves to\nbe the Generalized Graph Fourier Transform in terms of spatio-temporal\ndecorrelation. Secondly, we propose refined motion estimation via efficient\nregistration prior to inter-prediction, which searches the temporal\ncorrespondence between adjacent frames of irregular point clouds. Finally, we\npresent a complete framework based on the optimal inter-coding and our\npreviously proposed intra-coding, where we determine the optimal coding mode\nfrom rate-distortion optimization with the proposed offline-trained $\\lambda$-Q\nmodel. Experimental results show that we achieve around 17% bit rate reduction\non average over competitive dynamic point cloud compression methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:59:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 13:31:36 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 03:41:09 GMT"}, {"version": "v4", "created": "Mon, 10 Aug 2020 09:04:50 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xu", "Yiqun", ""], ["Hu", "Wei", ""], ["Wang", "Shanshe", ""], ["Zhang", "Xinfeng", ""], ["Wang", "Shiqi", ""], ["Ma", "Siwei", ""], ["Guo", "Zongming", ""], ["Gao", "Wen", ""]]}, {"id": "1908.02052", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Sarah Goodwin and Kim Marriott", "title": "Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2016). Awarded Best Paper Honorable Mention. Part of PhD thesis\n  arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 23:1\n  (2017) 411-420", "doi": "10.1109/TVCG.2016.2598885", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Showing flows of people and resources between multiple geographic locations\nis a challenging visualisation problem. We conducted two quantitative user\nstudies to evaluate different visual representations for such dense\nmany-to-many flows. In our first study we compared a bundled node-link flow map\nrepresentation and OD Maps [37] with a new visualisation we call MapTrix. Like\nOD Maps, MapTrix overcomes the clutter associated with a traditional flow map\nwhile providing geographic embedding that is missing in standard OD matrix\nrepresentations. We found that OD Maps and MapTrix had similar performance\nwhile bundled node-link flow map representations did not scale at all well. Our\nsecond study compared participant performance with OD Maps and MapTrix on\nlarger data sets. Again performance was remarkably similar.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:18:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Goodwin", "Sarah", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.02088", "submitter": "Yalong Yang", "authors": "Yalong Yang, Bernhard Jenny, Tim Dwyer, Kim Marriott, Haohui Chen and\n  Maxime Cordeil", "title": "Maps and Globes in Virtual Reality", "comments": "Presented at 20th EG/VGTC Conference on Visualization (EuroVis 2018).\n  Part of PhD thesis arXiv:1908.00662", "journal-ref": "Computer Graphics Forum 37:3 (2018) 427-438", "doi": "10.1111/cgf.13431", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores different ways to render world-wide geographic maps in\nvirtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's\nviewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)\nan egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved\nmap, created by projecting the map onto a section of a sphere which curves\naround the user. In all four visualisations the geographic centre can be\nsmoothly adjusted with a standard handheld VR controller and the user, through\na head-tracked headset, can physically move around the visualisation. For\ndistance comparison, exocentric globe is more accurate than egocentric globe\nand flat map. For area comparison, more time is required with exocentric and\negocentric globes than with flat and curved maps. For direction estimation, the\nexocentric globe is more accurate and faster than the other visual\npresentations. Our study participants had a weak preference for the exocentric\nglobe. Generally, the curved map had benefits over the flat map. In almost all\ncases the egocentric globe was found to be the least effective visualisation.\nOverall, our results provide support for the use of exocentric globes for\ngeographic visualisation in mixed-reality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:45:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Jenny", "Bernhard", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Chen", "Haohui", ""], ["Cordeil", "Maxime", ""]]}, {"id": "1908.02089", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Bernhard Jenny, Kim Marriott, Maxime Cordeil\n  and Haohui Chen", "title": "Origin-Destination Flow Maps in Immersive Environments", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2018). Part of PhD thesis arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 25:1\n  (2019) 693-703", "doi": "10.1109/TVCG.2018.2865192", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive virtual- and augmented-reality headsets can overlay a flat image\nagainst any surface or hang virtual objects in the space around the user. The\ntechnology is rapidly improving and may, in the long term, replace traditional\nflat panel displays in many situations. When displays are no longer\nintrinsically flat, how should we use the space around the user for abstract\ndata visualisation? In this paper, we ask this question with respect to\norigin-destination flow data in a global geographic context. We report on the\nfindings of three studies exploring different spatial encodings for flow maps.\nThe first experiment focuses on different 2D and 3D encodings for flows on flat\nmaps. We find that participants are significantly more accurate with raised\nflow paths whose height is proportional to flow distance but fastest with\ntraditional straight line 2D flows. In our second and third experiment, we\ncompared flat maps, 3D globes and a novel interactive design we call MapsLink,\ninvolving a pair of linked flat maps. We find that participants took\nsignificantly more time with MapsLink than other flow maps while the 3D globe\nwith raised flows was the fastest, most accurate, and most preferred method.\nOur work suggests that careful use of the third spatial dimension can resolve\nvisual clutter in complex flow maps.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:57:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Jenny", "Bernhard", ""], ["Marriott", "Kim", ""], ["Cordeil", "Maxime", ""], ["Chen", "Haohui", ""]]}, {"id": "1908.02111", "submitter": "Huikai Wu", "authors": "Huikai Wu, Junge Zhang, Kaiqi Huang", "title": "Point Cloud Super Resolution with Adversarial Residual Graph Networks", "comments": "Code is available at\n  https://github.com/wuhuikai/PointCloudSuperResolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Point cloud super-resolution is a fundamental problem for 3D reconstruction\nand 3D data understanding. It takes a low-resolution (LR) point cloud as input\nand generates a high-resolution (HR) point cloud with rich details. In this\npaper, we present a data-driven method for point cloud super-resolution based\non graph networks and adversarial losses. The key idea of the proposed network\nis to exploit the local similarity of point cloud and the analogy between LR\ninput and HR output. For the former, we design a deep network with graph\nconvolution. For the latter, we propose to add residual connections into graph\nconvolution and introduce a skip connection between input and output. The\nproposed network is trained with a novel loss function, which combines Chamfer\nDistance (CD) and graph adversarial loss. Such a loss function captures the\ncharacteristics of HR point cloud automatically without manual design. We\nconduct a series of experiments to evaluate our method and validate the\nsuperiority over other methods. Results show that the proposed method achieves\nthe state-of-the-art performance and have a good generalization ability to\nunseen data.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 12:44:42 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Wu", "Huikai", ""], ["Zhang", "Junge", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1908.02446", "submitter": "Jun Chen", "authors": "Jun Chen, Ryosuke Watanabe, Keisuke Nonaka, Tomoaki Konno, Hiroshi\n  Sankoh, Sei Naito", "title": "A Robust Billboard-based Free-viewpoint Video Synthesizing Algorithm for\n  Sports Scenes", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a billboard-based free-viewpoint video synthesizing algorithm for\nsports scenes that can robustly reconstruct and render a high-fidelity\nbillboard model for each object, including an occluded one, in each camera. Its\ncontributions are (1) applicable to a challenging shooting condition where a\nhigh precision 3D model cannot be built because a small number of cameras\nfeaturing wide-baseline are equipped; (2) capable of reproducing appearances of\nocclusions, that is one of the most significant issues for billboard-based\napproaches due to the ineffective detection of overlaps. To achieve\ncontributions above, the proposed method does not attempt to find a\nhigh-quality 3D model but utilizes a raw 3D model that is obtained directly\nfrom space carving. Although the model is insufficiently accurate for producing\nan impressive visual effect, precise objects segmentation and occlusions\ndetection can be performed by back-projecting it onto each camera plane. The\nbillboard model of each object in each camera is rendered according to whether\nit is occluded or not, and its location in the virtual stadium is determined\nconsidering the location of its 3D model. We synthesized free-viewpoint videos\nof two soccer sequences recorded by five cameras with the proposed and\nstate-of-art methods to demonstrate its performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 05:24:27 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 07:39:16 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Chen", "Jun", ""], ["Watanabe", "Ryosuke", ""], ["Nonaka", "Keisuke", ""], ["Konno", "Tomoaki", ""], ["Sankoh", "Hiroshi", ""], ["Naito", "Sei", ""]]}, {"id": "1908.02507", "submitter": "Yujie Yuan", "authors": "Yu-Jie Yuan, Yu-Kun Lai, Jie Yang, Hongbo Fu, Lin Gao", "title": "Mesh Variational Autoencoders with Edge Contraction Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D shape analysis is an important research topic in computer vision and\ngraphics. While existing methods have generalized image-based deep learning to\nmeshes using graph-based convolutions, the lack of an effective pooling\noperation restricts the learning capability of their networks. In this paper,\nwe propose a novel pooling operation for mesh datasets with the same\nconnectivity but different geometry, by building a mesh hierarchy using mesh\nsimplification. For this purpose, we develop a modified mesh simplification\nmethod to avoid generating highly irregularly sized triangles. Our pooling\noperation effectively encodes the correspondence between coarser and finer\nmeshes in the hierarchy. We then present a variational auto-encoder structure\nwith the edge contraction pooling and graph-based convolutions, to explore\nprobability latent spaces of 3D surfaces. Our network requires far fewer\nparameters than the original mesh VAE and thus can handle denser models thanks\nto our new pooling operation and convolutional kernels. Our evaluation also\nshows that our method has better generalization ability and is more reliable in\nvarious applications, including shape generation, shape interpolation and shape\nembedding.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 09:59:08 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Yuan", "Yu-Jie", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jie", ""], ["Fu", "Hongbo", ""], ["Gao", "Lin", ""]]}, {"id": "1908.02681", "submitter": "Markus Sch\\\"utz", "authors": "Markus Sch\\\"utz and Michael Wimmer", "title": "Rendering Point Clouds with Compute Shaders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a compute shader based point cloud rasterizer with up to 10 times\nhigher performance than classic point-based rendering with the GL_POINT\nprimitive. In addition to that, our rasterizer offers 5 byte depth-buffer\nprecision with uniform or customizable distribution, and we show that it is\npossible to implement a high-quality splatting method that blends together\noverlapping fragments while still maintaining higher frame-rates than the\ntraditional approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 15:30:20 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Sch\u00fctz", "Markus", ""], ["Wimmer", "Michael", ""]]}, {"id": "1908.02714", "submitter": "Yuki Endo", "authors": "Yoshihiro Kanamori, Yuki Endo", "title": "Relighting Humans: Occlusion-Aware Inverse Rendering for Full-Body Human\n  Images", "comments": "Published at SIGGRAPH Asia 2018 (ACM Transactions on Graphics).\n  Project page with codes, pretrained models, and human model lists is at\n  http://kanamori.cs.tsukuba.ac.jp/projects/relighting_human/", "journal-ref": null, "doi": "10.1145/3272127.3275104", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relighting of human images has various applications in image synthesis. For\nrelighting, we must infer albedo, shape, and illumination from a human\nportrait. Previous techniques rely on human faces for this inference, based on\nspherical harmonics (SH) lighting. However, because they often ignore light\nocclusion, inferred shapes are biased and relit images are unnaturally bright\nparticularly at hollowed regions such as armpits, crotches, or garment\nwrinkles. This paper introduces the first attempt to infer light occlusion in\nthe SH formulation directly. Based on supervised learning using convolutional\nneural networks (CNNs), we infer not only an albedo map, illumination but also\na light transport map that encodes occlusion as nine SH coefficients per pixel.\nThe main difficulty in this inference is the lack of training datasets compared\nto unlimited variations of human portraits. Surprisingly, geometric information\nincluding occlusion can be inferred plausibly even with a small dataset of\nsynthesized human figures, by carefully preparing the dataset so that the CNNs\ncan exploit the data coherency. Our method accomplishes more realistic\nrelighting than the occlusion-ignored formulation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 16:35:27 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kanamori", "Yoshihiro", ""], ["Endo", "Yuki", ""]]}, {"id": "1908.03118", "submitter": "Patrick Stotko", "authors": "Patrick Stotko, Stefan Krumpen, Michael Weinmann, Reinhard Klein", "title": "Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client\n  Live Telepresence", "comments": "The final version of record is available at\n  http://dx.doi.org/10.1109/ISMAR.2019.00018", "journal-ref": "In proceedings of IEEE International Symposium on Mixed and\n  Augmented Reality (ISMAR), pages 19-25, 2019", "doi": "10.1109/ISMAR.2019.00018", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing live telepresence experiences for teleconferencing or remote\ncollaboration receives increasing interest with the recent progress in\ncapturing and AR/VR technology. Whereas impressive telepresence systems have\nbeen proposed on top of on-the-fly scene capture, data transmission and\nvisualization, these systems are restricted to the immersion of single or up to\na low number of users into the respective scenarios. In this paper, we direct\nour attention on immersing significantly larger groups of people into\nlive-captured scenes as required in education, entertainment or collaboration\nscenarios. For this purpose, rather than abandoning previous approaches, we\npresent a range of optimizations of the involved reconstruction and streaming\ncomponents that allow the immersion of a group of more than 24 users within the\nsame scene - which is about a factor of 6 higher than in previous work -\nwithout introducing further latency or changing the involved consumer hardware\nsetup. We demonstrate that our optimized system is capable of generating\nhigh-quality scene reconstructions as well as providing an immersive viewing\nexperience to a large group of people within these live-captured scenes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:27:10 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 10:39:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Stotko", "Patrick", ""], ["Krumpen", "Stefan", ""], ["Weinmann", "Michael", ""], ["Klein", "Reinhard", ""]]}, {"id": "1908.03565", "submitter": "Jan Wassenberg", "authors": "Alexander Rhatushnyak, Jan Wassenberg, Jon Sneyers, Jyrki Alakuijala,\n  Lode Vandevenne, Luca Versari, Robert Obryk, Zoltan Szabadka, Evgenii\n  Kliuchnikov, Iulia-Maria Comsa, Krzysztof Potempa, Martin Bruse, Moritz\n  Firsching, Renata Khasanova, Ruud van Asseldonk, Sami Boukortt, Sebastian\n  Gomez, Thomas Fischbacher", "title": "Committee Draft of JPEG XL Image Coding System", "comments": "Royalty-free, open-source reference implementation in Q4 2019. v3\n  fixes PDF links and paper size", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  JPEG XL is a practical approach focused on scalable web distribution and\nefficient compression of high-quality images. It provides various benefits\ncompared to existing image formats: 60% size reduction at equivalent subjective\nquality; fast, parallelizable decoding and encoding configurations; features\nsuch as progressive, lossless, animation, and reversible transcoding of\nexisting JPEG with 22% size reduction; support for high-quality applications\nincluding wide gamut, higher resolution/bit depth/dynamic range, and visually\nlossless coding. The JPEG XL architecture is traditional block-transform coding\nwith upgrades to each component.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 10:22:43 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 15:46:44 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rhatushnyak", "Alexander", ""], ["Wassenberg", "Jan", ""], ["Sneyers", "Jon", ""], ["Alakuijala", "Jyrki", ""], ["Vandevenne", "Lode", ""], ["Versari", "Luca", ""], ["Obryk", "Robert", ""], ["Szabadka", "Zoltan", ""], ["Kliuchnikov", "Evgenii", ""], ["Comsa", "Iulia-Maria", ""], ["Potempa", "Krzysztof", ""], ["Bruse", "Martin", ""], ["Firsching", "Moritz", ""], ["Khasanova", "Renata", ""], ["van Asseldonk", "Ruud", ""], ["Boukortt", "Sami", ""], ["Gomez", "Sebastian", ""], ["Fischbacher", "Thomas", ""]]}, {"id": "1908.03581", "submitter": "Yixin Hu", "authors": "Yixin Hu, Teseo Schneider, Bolun Wang, Denis Zorin, Daniele Panozzo", "title": "Fast Tetrahedral Meshing in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new tetrahedral meshing method, fTetWild, to convert triangle\nsoups into high-quality tetrahedral meshes. Our method builds on the TetWild\nalgorithm, replacing the rational triangle insertion with a new incremental\napproach to construct and optimize the output mesh, interleaving triangle\ninsertion and mesh optimization. Our approach makes it possible to maintain a\nvalid floating-point tetrahedral mesh at all algorithmic stages, eliminating\nthe need for costly constructions with rational numbers used by TetWild, while\nmaintaining full robustness and similar output quality. This allows us to\nimprove on TetWild in two ways. First, our algorithm is significantly faster,\nwith running time comparable to less robust Delaunay-based tetrahedralization\nalgorithms. Second, our algorithm is guaranteed to produce a valid tetrahedral\nmesh with floating-point vertex coordinates, while TetWild produces a valid\nmesh with rational coordinates which is not guaranteed to be valid after\nfloating-point conversion. As a trade-off, our algorithm no longer guarantees\nthat all input triangles are present in the output mesh, but in practice, as\nconfirmed by our tests on the Thingi10k dataset, the algorithm always succeeds\nin inserting all input triangles.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:00:15 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 22:09:21 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Hu", "Yixin", ""], ["Schneider", "Teseo", ""], ["Wang", "Bolun", ""], ["Zorin", "Denis", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1908.04197", "submitter": "Aakanksha Rana", "authors": "Aakanksha Rana, Praveer Singh, Giuseppe Valenzise, Frederic Dufaux,\n  Nikos Komodakis, Aljosa Smolic", "title": "Deep Tone Mapping Operator for High Dynamic Range Images", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2936649", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computationally fast tone mapping operator (TMO) that can quickly adapt to\na wide spectrum of high dynamic range (HDR) content is quintessential for\nvisualization on varied low dynamic range (LDR) output devices such as movie\nscreens or standard displays. Existing TMOs can successfully tone-map only a\nlimited number of HDR content and require an extensive parameter tuning to\nyield the best subjective-quality tone-mapped output. In this paper, we address\nthis problem by proposing a fast, parameter-free and scene-adaptable deep tone\nmapping operator (DeepTMO) that yields a high-resolution and high-subjective\nquality tone mapped output. Based on conditional generative adversarial network\n(cGAN), DeepTMO not only learns to adapt to vast scenic-content (e.g., outdoor,\nindoor, human, structures, etc.) but also tackles the HDR related\nscene-specific challenges such as contrast and brightness, while preserving the\nfine-grained details. We explore 4 possible combinations of\nGenerator-Discriminator architectural designs to specifically address some\nprominent issues in HDR related deep-learning frameworks like blurring, tiling\npatterns and saturation artifacts. By exploring different influences of scales,\nloss-functions and normalization layers under a cGAN setting, we conclude with\nadopting a multi-scale model for our task. To further leverage on the\nlarge-scale availability of unlabeled HDR data, we train our network by\ngenerating targets using an objective HDR quality metric, namely Tone Mapping\nImage Quality Index (TMQI). We demonstrate results both quantitatively and\nqualitatively, and showcase that our DeepTMO generates high-resolution,\nhigh-quality output images over a large spectrum of real-world scenes. Finally,\nwe evaluate the perceived quality of our results by conducting a pair-wise\nsubjective study which confirms the versatility of our method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 15:30:55 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Rana", "Aakanksha", ""], ["Singh", "Praveer", ""], ["Valenzise", "Giuseppe", ""], ["Dufaux", "Frederic", ""], ["Komodakis", "Nikos", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.04338", "submitter": "John Kanji", "authors": "John Kanji and David I. W. Levin", "title": "Convolutional Humanoid Animation via Deformation", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new deep learning-driven approach to image-based\nsynthesis of animations involving humanoid characters. Unlike previous deep\napproaches to image-based animation our method makes no assumptions on the type\nof motion to be animated nor does it require dense temporal input to produce\nmotion. Instead we generate new animations by interpolating between user chosen\nkeyframes, arranged sparsely in time. Utilizing a novel configuration manifold\nlearning approach we interpolate suitable motions between these keyframes. In\ncontrast to previous methods, ours requires less data (animations can be\ngenerated from a single youtube video) and is broadly applicable to a wide\nrange of motions including facial motion, whole body motion and even scenes\nwith multiple characters. These improvements serve to significantly reduce the\ndifficulty in producing image-based animations of humanoid characters, allowing\neven broader audiences to express their creativity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 19:01:47 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Kanji", "John", ""], ["Levin", "David I. W.", ""]]}, {"id": "1908.04520", "submitter": "Lin Gao", "authors": "Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-Kun Lai, Hao\n  Zhang", "title": "SDM-NET: Deep Generative Network for Structured Deformable Mesh", "comments": "Conditionally Accepted to Siggraph Asia 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SDM-NET, a deep generative neural network which produces\nstructured deformable meshes. Specifically, the network is trained to generate\na spatial arrangement of closed, deformable mesh parts, which respect the\nglobal part structure of a shape collection, e.g., chairs, airplanes, etc. Our\nkey observation is that while the overall structure of a 3D shape can be\ncomplex, the shape can usually be decomposed into a set of parts, each\nhomeomorphic to a box, and the finer-scale geometry of the part can be\nrecovered by deforming the box. The architecture of SDM-NET is that of a\ntwo-level variational autoencoder (VAE). At the part level, a PartVAE learns a\ndeformable model of part geometries. At the structural level, we train a\nStructured Parts VAE (SP-VAE), which jointly learns the part structure of a\nshape collection and the part geometries, ensuring a coherence between global\nshape structure and surface details. Through extensive experiments and\ncomparisons with the state-of-the-art deep generative models of shapes, we\ndemonstrate the superiority of SDM-NET in generating meshes with visual\nquality, flexible topology, and meaningful structures, which benefit shape\ninterpolation and other subsequently modeling tasks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 07:26:15 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 09:08:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gao", "Lin", ""], ["Yang", "Jie", ""], ["Wu", "Tong", ""], ["Yuan", "Yu-Jie", ""], ["Fu", "Hongbo", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Hao", ""]]}, {"id": "1908.04694", "submitter": "Shih-Chieh Su", "authors": "Shih-Chieh Su", "title": "Channel Decomposition into Painting Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a method to decompose a convolutional layer of the deep\nneural network into painting actions. To behave like the human painter, these\nactions are driven by the cost simulating the hand movement, the paint color\nchange, the stroke shape and the stroking style. To help planning, the Mask\nR-CNN is applied to detect the object areas and decide the painting order. The\nproposed painting system introduces a variety of extensions in artistic styles,\nbased on the chosen parameters. Further experiments are performed to evaluate\nthe channel penetration and the channel sensitivity on the strokes.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 06:58:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 18:38:21 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 07:59:10 GMT"}, {"version": "v4", "created": "Tue, 12 Nov 2019 06:24:11 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Su", "Shih-Chieh", ""]]}, {"id": "1908.05411", "submitter": "David Palmer", "authors": "David Palmer (1), David Bommes (2), Justin Solomon (1) ((1)\n  Massachusetts Institute of Technology, (2) University of Bern)", "title": "Algebraic Representations for Volumetric Frame Fields", "comments": "17 pages, 20 figures", "journal-ref": "ACM Transactions on Graphics (TOG), 39(2), 1-17 (2020)", "doi": "10.1145/3366786", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field-guided parametrization methods have proven effective for quad meshing\nof surfaces; these methods compute smooth cross fields to guide the meshing\nprocess and then integrate the fields to construct a discrete mesh. A key\nchallenge in extending these methods to three dimensions, however, is\nrepresentation of field values. Whereas cross fields can be represented by\ntangent vector fields that form a linear space, the 3D analog---an octahedral\nframe field---takes values in a nonlinear manifold. In this work, we describe\nthe space of octahedral frames in the language of differential and algebraic\ngeometry. With this understanding, we develop geometry-aware tools for\noptimization of octahedral fields, namely geodesic stepping and exact\nprojection via semidefinite relaxation. Our algebraic approach not only\nprovides an elegant and mathematically-sound description of the space of\noctahedral frames but also suggests a generalization to frames whose three axes\nscale independently, better capturing the singular behavior we expect to see in\nvolumetric frame fields. These new odeco frames, so-called as they are\nrepresented by orthogonally decomposable tensors, also admit a semidefinite\nprogram--based projection operator. Our description of the spaces of octahedral\nand odeco frames suggests computing frame fields via manifold-based\noptimization algorithms; we show that these algorithms efficiently produce\nhigh-quality fields while maintaining stability and smoothness.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 04:16:26 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 23:30:36 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Palmer", "David", ""], ["Bommes", "David", ""], ["Solomon", "Justin", ""]]}, {"id": "1908.05932", "submitter": "Yuval Nirkin", "authors": "Yuval Nirkin, Yosi Keller and Tal Hassner", "title": "FSGAN: Subject Agnostic Face Swapping and Reenactment", "comments": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Face Swapping GAN (FSGAN) for face swapping and reenactment.\nUnlike previous work, FSGAN is subject agnostic and can be applied to pairs of\nfaces without requiring training on those faces. To this end, we describe a\nnumber of technical contributions. We derive a novel recurrent neural network\n(RNN)-based approach for face reenactment which adjusts for both pose and\nexpression variations and can be applied to a single image or a video sequence.\nFor video sequences, we introduce continuous interpolation of the face views\nbased on reenactment, Delaunay Triangulation, and barycentric coordinates.\nOccluded face regions are handled by a face completion network. Finally, we use\na face blending network for seamless blending of the two faces while preserving\ntarget skin color and lighting conditions. This network uses a novel Poisson\nblending loss which combines Poisson optimization with perceptual loss. We\ncompare our approach to existing state-of-the-art systems and show our results\nto be both qualitatively and quantitatively superior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:16:22 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Nirkin", "Yuval", ""], ["Keller", "Yosi", ""], ["Hassner", "Tal", ""]]}, {"id": "1908.05936", "submitter": "Patrick Stotko", "authors": "Patrick Stotko", "title": "stdgpu: Efficient STL-like Data Structures on the GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous advances in parallel computing and graphics hardware opened up\nseveral novel real-time GPU applications in the fields of computer vision,\ncomputer graphics as well as augmented reality (AR) and virtual reality (VR).\nAlthough these applications built upon established open-source frameworks that\nprovide highly optimized algorithms, they often come with custom self-written\ndata structures to manage the underlying data. In this work, we present stdgpu,\nan open-source library which defines several generic GPU data structures for\nfast and reliable data management. Rather than abandoning previous established\nframeworks, our library aims to extend them, therefore bridging the gap between\nCPU and GPU computing. This way, it provides clean and familiar interfaces and\nintegrates seamlessly into new as well as existing projects. We hope to foster\nfurther developments towards unified CPU and GPU computing and welcome\ncontributions from the community.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:37:42 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Stotko", "Patrick", ""]]}, {"id": "1908.05976", "submitter": "Vincenzo Perri", "authors": "Vincenzo Perri, Ingo Scholtes", "title": "HOTVis: Higher-Order Time-Aware Visualisation of Dynamic Graphs", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR cs.LG physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network visualisation techniques are important tools for the exploratory\nanalysis of complex systems. While these methods are regularly applied to\nvisualise data on complex networks, we increasingly have access to time series\ndata that can be modelled as temporal networks or dynamic graphs. In dynamic\ngraphs, the temporal ordering of time-stamped edges determines the causal\ntopology of a system, i.e., which nodes can, directly and indirectly, influence\neach other via a so-called causal path. This causal topology is crucial to\nunderstand dynamical processes, assess the role of nodes, or detect clusters.\nHowever, we lack graph drawing techniques that incorporate this information\ninto static visualisations. Addressing this gap, we present a novel dynamic\ngraph visualisation algorithm that utilises higher-order graphical models of\ncausal paths in time series data to compute time-aware static graph\nvisualisations. These visualisations combine the simplicity and\ninterpretability of static graphs with a time-aware layout algorithm that\nhighlights patterns in the causal topology that result from the temporal\ndynamics of edges.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 13:57:52 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 07:01:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Perri", "Vincenzo", ""], ["Scholtes", "Ingo", ""]]}, {"id": "1908.06154", "submitter": "Evgeny Lipovetsky", "authors": "Evgeny Lipovetsky, Nira Dyn", "title": "Extending editing capabilities of subdivision schemes by refinement of\n  point-normal pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the 2D circle average of [11] to a 3D binary average\nof point-normal pairs, and study its properties. We modify classical\nsurface-generating linear subdivision schemes with this average obtaining\nsurface-generating schemes refining point-normal pairs. The modified schemes\ngive the possibility to generate more geometries by editing the initial\nnormals. For the case of input data consisting of a mesh only, we present a\nmethod for computing \"naive\" initial normals from the initial mesh. The\nperformance of several modified schemes is compared to their linear variants,\nwhen operating on the same initial mesh, and examples of the editing\ncapabilities of the modified schemes are given. In addition we provide a link\nto our repository, where we store the initial and refined mesh files, and the\nimplementation code. Several videos, demonstrating the editing capabilities of\nthe initial normals are provided in our Youtube channel.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 20:23:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lipovetsky", "Evgeny", ""], ["Dyn", "Nira", ""]]}, {"id": "1908.06246", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "CompenNet++: End-to-end Full Projector Compensation", "comments": "To appear in ICCV 2019. High-res supplementary material:\n  https://www3.cs.stonybrook.edu/~hling/publication/CompenNet++_sup-high-res.pdf.\n  Code: https://github.com/BingyaoHuang/CompenNet-plusplus", "journal-ref": null, "doi": "10.1109/ICCV.2019.00726", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full projector compensation aims to modify a projector input image such that\nit can compensate for both geometric and photometric disturbance of the\nprojection surface. Traditional methods usually solve the two parts separately,\nalthough they are known to correlate with each other. In this paper, we propose\nthe first end-to-end solution, named CompenNet++, to solve the two problems\njointly. Our work non-trivially extends CompenNet, which was recently proposed\nfor photometric compensation with promising performance. First, we propose a\nnovel geometric correction subnet, which is designed with a cascaded\ncoarse-to-fine structure to learn the sampling grid directly from photometric\nsampling images. Second, by concatenating the geometric correction subset with\nCompenNet, CompenNet++ accomplishes full projector compensation and is\nend-to-end trainable. Third, after training, we significantly simplify both\ngeometric and photometric compensation parts, and hence largely improves the\nrunning time efficiency. Moreover, we construct the first setup-independent\nfull compensation benchmark to facilitate the study on this topic. In our\nthorough experiments, our method shows clear advantages over previous arts with\npromising compensation quality and meanwhile being practically convenient.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 05:28:21 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "1908.06922", "submitter": "Bum Chul Kwon", "authors": "Hwiyeon Kim, Juyoung Oh, Yunha Han, Sungahn Ko, Matthew Brehmer, Bum\n  Chul Kwon", "title": "Thumbnails for Data Stories: A Survey of Current Practices", "comments": "To appear at IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people browse online news, small thumbnail images accompanying links to\narticles attract their attention and help them to decide which articles to\nread. As an increasing proportion of online news can be construed as data\njournalism, we have witnessed a corresponding increase in the incorporation of\nvisualization in article thumbnails. However, there is little research to\nsupport alternative design choices for visualization thumbnails, which include\nresizing, cropping, simplifying, and embellishing charts appearing within the\nbody of the associated article. We therefore sought to better understand these\ndesign choices and determine what makes a visualization thumbnail inviting and\ninterpretable. This paper presents our findings from a survey of visualization\nthumbnails collected online and from conversations with data journalists and\nnews graphics designers. Our study reveals that there exists an uncharted\ndesign space, one that is in need of further empirical study. Our work can thus\nbe seen as a first step toward providing structured guidance on how to design\nthumbnails for data stories.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:48:48 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kim", "Hwiyeon", ""], ["Oh", "Juyoung", ""], ["Han", "Yunha", ""], ["Ko", "Sungahn", ""], ["Brehmer", "Matthew", ""], ["Kwon", "Bum Chul", ""]]}, {"id": "1908.06974", "submitter": "Fehmi Cirak", "authors": "Fehmi Cirak and Malcolm Sabin", "title": "Adding quadric fillets to quador lattice structures", "comments": null, "journal-ref": null, "doi": "10.1016/j.cad.2019.102754", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gupta et al. [1, 2] describe a very beautiful application of algebraic\ngeometry to lattice structures composed of quadric of revolution (quador)\nimplicit surfaces. However, the shapes created have concave edges where the\nstubs meet, and such edges can be stress-raisers which can cause significant\nproblems with, for instance, fatigue under cyclic loading. This note describes\na way in which quadric fillets can be added to these models, thus relieving\nthis problem while retaining their computational simplicity and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 11:26:38 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 12:58:29 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Cirak", "Fehmi", ""], ["Sabin", "Malcolm", ""]]}, {"id": "1908.07015", "submitter": "Shelley Kandola", "authors": "Shelley Kandola", "title": "The Topological Complexity of Spaces of Digital Jordan Curves", "comments": "53 pages, 41 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research is motivated by studying image processing algorithms through a\ntopological lens. The images we focus on here are those that have been\nsegmented by digital Jordan curves as a means of image compression. The\nalgorithms of interest are those that continuously morph one digital image into\nanother digital image. Digital Jordan curves have been studied in a variety of\nforms for decades now. Our contribution to this field is interpreting the set\nof digital Jordan curves that can exist within a given digital plane as a\nfinite topological space. Computing the topological complexity of this space\ndetermines the minimal number of continuous motion planning rules required to\ntransform one image into another, and determining the motion planners\nassociated to topological complexity provides the specific algorithms for doing\nso. The main result of Section 3 is that our space of digital Jordan curves is\nconnected, hence, its topological complexity is finite. To build up to that, we\nuse Section 2 to prove some results about paths and distance functions that are\nobvious in Hausdorff spaces, yet surprisingly elusive in $T_0$ spaces. We end\nwith Section 4, in which we study applications of these results. In particular,\nwe prove that our interpretation of the space of digital Jordan curves is the\nonly topologically correct interpretation. This article is an adaptation of the\nauthor's Ph.D. dissertation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 18:33:33 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Kandola", "Shelley", ""]]}, {"id": "1908.07117", "submitter": "Verica Lazova", "authors": "Verica Lazova, Eldar Insafutdinov, Gerard Pons-Moll", "title": "360-Degree Textures of People in Clothing from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we predict a full 3D avatar of a person from a single image. We\ninfer texture and geometry in the UV-space of the SMPL model using an\nimage-to-image translation method. Given partial texture and segmentation\nlayout maps derived from the input view, our model predicts the complete\nsegmentation map, the complete texture map, and a displacement map. The\npredicted maps can be applied to the SMPL model in order to naturally\ngeneralize to novel poses, shapes, and even new clothing. In order to learn our\nmodel in a common UV-space, we non-rigidly register the SMPL model to thousands\nof 3D scans, effectively encoding textures and geometries as images in\ncorrespondence. This turns a difficult 3D inference task into a simpler\nimage-to-image translation one. Results on rendered scans of people and images\nfrom the DeepFashion dataset demonstrate that our method can reconstruct\nplausible 3D avatars from a single image. We further use our model to digitally\nchange pose, shape, swap garments between people and edit clothing. To\nencourage research in this direction we will make the source code available for\nresearch purpose.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 00:42:40 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Lazova", "Verica", ""], ["Insafutdinov", "Eldar", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "1908.07198", "submitter": "Changgeng Zhang", "authors": "Yuefan Shen, Changgeng Zhang, Hongbo Fu, Kun Zhou, Youyi Zheng", "title": "DeepSketchHair: Deep Sketch-based 3D Hair Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sketchhair, a deep learning based tool for interactive modeling of\n3D hair from 2D sketches. Given a 3D bust model as reference, our sketching\nsystem takes as input a user-drawn sketch (consisting of hair contour and a few\nstrokes indicating the hair growing direction within a hair region), and\nautomatically generates a 3D hair model, which matches the input sketch both\nglobally and locally. The key enablers of our system are two carefully designed\nneural networks, namely, S2ONet, which converts an input sketch to a dense 2D\nhair orientation field; and O2VNet, which maps the 2D orientation field to a 3D\nvector field. Our system also supports hair editing with additional sketches in\nnew views. This is enabled by another deep neural network, V2VNet, which\nupdates the 3D vector field with respect to the new sketches. All the three\nnetworks are trained with synthetic data generated from a 3D hairstyle\ndatabase. We demonstrate the effectiveness and expressiveness of our tool using\na variety of hairstyles and also compare our method with prior art.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 07:39:21 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Shen", "Yuefan", ""], ["Zhang", "Changgeng", ""], ["Fu", "Hongbo", ""], ["Zhou", "Kun", ""], ["Zheng", "Youyi", ""]]}, {"id": "1908.07214", "submitter": "He Wang", "authors": "He Wang, Edmond S. L. Ho, Hubert P. H. Shum and Zhanxing Zhu", "title": "Spatio-temporal Manifold Learning for Human Motions via Long-horizon\n  Modeling", "comments": "12 pages, Accepted in IEEE Transaction on Visualization and Computer\n  Graphics", "journal-ref": "IEEE Transaction on Visualization and Computer Graphics, 2019", "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven modeling of human motions is ubiquitous in computer graphics and\ncomputer vision applications, such as synthesizing realistic motions or\nrecognizing actions. Recent research has shown that such problems can be\napproached by learning a natural motion manifold using deep learning to address\nthe shortcomings of traditional data-driven approaches. However, previous\nmethods can be sub-optimal for two reasons. First, the skeletal information has\nnot been fully utilized for feature extraction. Unlike images, it is difficult\nto define spatial proximity in skeletal motions in the way that deep networks\ncan be applied. Second, motion is time-series data with strong multi-modal\ntemporal correlations. A frame could be followed by several candidate frames\nleading to different motions; long-range dependencies exist where a number of\nframes in the beginning correlate to a number of frames later. Ineffective\nmodeling would either under-estimate the multi-modality and variance, resulting\nin featureless mean motion or over-estimate them resulting in jittery motions.\nIn this paper, we propose a new deep network to tackle these challenges by\ncreating a natural motion manifold that is versatile for many applications. The\nnetwork has a new spatial component for feature extraction. It is also equipped\nwith a new batch prediction model that predicts a large number of frames at\nonce, such that long-term temporally-based objective functions can be employed\nto correctly learn the motion multi-modality and variances. With our system,\nlong-duration motions can be predicted/synthesized using an open-loop setup\nwhere the motion retains the dynamics accurately. It can also be used for\ndenoising corrupted motions and synthesizing new motions with given control\nsignals. We demonstrate that our system can create superior results comparing\nto existing work in multiple applications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 08:18:58 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wang", "He", ""], ["Ho", "Edmond S. L.", ""], ["Shum", "Hubert P. H.", ""], ["Zhu", "Zhanxing", ""]]}, {"id": "1908.07452", "submitter": "Bala Krishnamoorthy", "authors": "Prashant Gupta and Bala Krishnamoorthy and Gregory Dreifus", "title": "Continuous Toolpath Planning in Additive Manufacturing", "comments": "Accepted in SPM2020; implementation details expanded, manuscript\n  revised after review. arXiv admin note: text overlap with arXiv:1812.02412", "journal-ref": "Computer-Aided Design (SPM2020 issue), Volume 127, October 2020,\n  pg: 102880", "doi": "10.1016/j.cad.2020.102880", "report-no": null, "categories": "cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework that creates a new polygonal mesh representation of\nthe sparse infill domain of a layer-by-layer 3D printing job. We guarantee the\nexistence of a single, continuous tool path covering each connected piece of\nthe domain in every layer. We present a tool path algorithm that traverses each\nsuch continuous tool path with no crossovers. The key construction at the heart\nof our framework is an Euler transformation which converts a 2-dimensional cell\ncomplex K into a new 2-complex K^ such that every vertex in the 1-skeleton G^\nof K^ has even degree. Hence G^ is Eulerian, and a Eulerian tour can be\nfollowed to print all edges in a continuous fashion. We start with a mesh K of\nthe union of polygons obtained by projecting all layers to the plane. We\ncompute its Euler transformation K^. In the slicing step, we clip K^ at each\nlayer using its polygon to obtain a complex that may not necessarily be Euler.\nWe then patch this complex by adding edges such that any odd-degree nodes\ncreated by slicing are transformed to have even degrees again. We print extra\nsupport edges in place of any segments left out to ensure there are no edges\nwithout support in the next layer. These support edges maintain the Euler\nnature of the complex. Finally we describe a tree-based search algorithm that\nbuilds the continuous tool path by traversing \"concentric\" cycles in the Euler\ncomplex. Our algorithm produces a tool path that avoids material collisions and\ncrossovers, and can be printed in a continuous fashion irrespective of complex\ngeometry or topology of the domain (e.g., holes). We implement our test our\nframework on several 3D objects. Apart from standard geometric shapes, we\ndemonstrate the framework on the Stanford bunny.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 01:40:35 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 00:25:41 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Gupta", "Prashant", ""], ["Krishnamoorthy", "Bala", ""], ["Dreifus", "Gregory", ""]]}, {"id": "1908.07587", "submitter": "Songwei Ge", "authors": "Songwei Ge, Austin Dill, Eunsu Kang, Chun-Liang Li, Lingyao Zhang,\n  Manzil Zaheer, Barnabas Poczos", "title": "Developing Creative AI to Generate Sculptural Objects", "comments": "In the Proceedings of International Symposium on Electronic Art (ISEA\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the intersection of human and machine creativity by generating\nsculptural objects through machine learning. This research raises questions\nabout both the technical details of automatic art generation and the\ninteraction between AI and people, as both artists and the audience of art. We\nintroduce two algorithms for generating 3D point clouds and then discuss their\nactualization as sculpture and incorporation into a holistic art installation.\nSpecifically, the Amalgamated DeepDream (ADD) algorithm solves the sparsity\nproblem caused by the naive DeepDream-inspired approach and generates creative\nand printable point clouds. The Partitioned DeepDream (PDD) algorithm further\nallows us to explore more diverse 3D object creation by combining point cloud\nclustering algorithms and ADD.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 20:00:25 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ge", "Songwei", ""], ["Dill", "Austin", ""], ["Kang", "Eunsu", ""], ["Li", "Chun-Liang", ""], ["Zhang", "Lingyao", ""], ["Zaheer", "Manzil", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1908.07732", "submitter": "Xuan Luo", "authors": "Xuan Luo, Yanmeng Kong, Jason Lawrence, Ricardo Martin-Brualla, and\n  Steve Seitz", "title": "KeystoneDepth: Visualizing History in 3D", "comments": "Project website: http://roxanneluo.github.io/KeystoneDepth.html ,\n  Video: https://youtu.be/5JrX-KKisC8 , More results:\n  http://roxanneluo.github.io/keystonedepth_supplementary/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the largest and most diverse collection of rectified\nstereo image pairs to the research community, KeystoneDepth, consisting of tens\nof thousands of stereographs of historical people, events, objects, and scenes\nbetween 1860 and 1963. Leveraging the Keystone-Mast raw scans from the\nCalifornia Museum of Photography, we apply multiple processing steps to produce\nclean stereo image pairs, complete with calibration data, rectification\ntransforms, and depthmaps. A second contribution is a novel approach for view\nsynthesis that runs at real-time rates on a mobile device, simulating the\nexperience of looking through an open window into these historical scenes. We\nproduce results for thousands of antique stereographs, capturing many important\nhistorical moments.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 07:35:26 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 17:56:45 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Luo", "Xuan", ""], ["Kong", "Yanmeng", ""], ["Lawrence", "Jason", ""], ["Martin-Brualla", "Ricardo", ""], ["Seitz", "Steve", ""]]}, {"id": "1908.08151", "submitter": "Seok-Hee Hong", "authors": "Seok-Hee Hong, Peter Eades, Marnijati Torkel, Ziyang Wang, David Chae,\n  Sungpack Hong, Daniel Langerenken, Hassan Chafi", "title": "Multi-level Graph Drawing using Infomap Clustering", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infomap clustering finds the community structures that minimize the expected\ndescription length of a random walk trajectory; algorithms for infomap\nclustering run fast in practice for large graphs. In this paper we leverage the\neffectiveness of Infomap clustering combined with the multi-level graph drawing\nparadigm. Experiments show that our new Infomap based multi-level algorithm\nproduces good visualization of large and complex networks, with significant\nimprovement in quality metrics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 00:24:06 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""], ["Torkel", "Marnijati", ""], ["Wang", "Ziyang", ""], ["Chae", "David", ""], ["Hong", "Sungpack", ""], ["Langerenken", "Daniel", ""], ["Chafi", "Hassan", ""]]}, {"id": "1908.08185", "submitter": "Chunyu Li", "authors": "Chunyu Li, Yusuke Monno, Hironori Hidaka and Masatoshi Okutomi", "title": "Pro-Cam SSfM: Projector-Camera System for Structure and Spectral\n  Reflectance from Motion", "comments": "Accepted by ICCV 2019. Project homepage:\n  http://www.ok.sc.e.titech.ac.jp/res/PCSSfM/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel projector-camera system for practical and\nlow-cost acquisition of a dense object 3D model with the spectral reflectance\nproperty. In our system, we use a standard RGB camera and leverage an\noff-the-shelf projector as active illumination for both the 3D reconstruction\nand the spectral reflectance estimation. We first reconstruct the 3D points\nwhile estimating the poses of the camera and the projector, which are\nalternately moved around the object, by combining multi-view structured light\nand structure-from-motion (SfM) techniques. We then exploit the projector for\nmultispectral imaging and estimate the spectral reflectance of each 3D point\nbased on a novel spectral reflectance estimation model considering the\ngeometric relationship between the reconstructed 3D points and the estimated\nprojector positions. Experimental results on several real objects demonstrate\nthat our system can precisely acquire a dense 3D model with the full spectral\nreflectance property using off-the-shelf devices.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 03:35:48 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Li", "Chunyu", ""], ["Monno", "Yusuke", ""], ["Hidaka", "Hironori", ""], ["Okutomi", "Masatoshi", ""]]}, {"id": "1908.08505", "submitter": "Aakanksha Rana", "authors": "Emin Zerman, Aakanksha Rana, Aljosa Smolic", "title": "ColorNet -- Estimating Colorfulness in Natural Images", "comments": "Accepted to IEEE International Conference on Image Processing (ICIP)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the colorfulness of a natural or virtual scene is critical for many\napplications in image processing field ranging from capturing to display. In\nthis paper, we propose the first deep learning-based colorfulness estimation\nmetric. For this purpose, we develop a color rating model which simultaneously\nlearns to extracts the pertinent characteristic color features and the mapping\nfrom feature space to the ideal colorfulness scores for a variety of natural\ncolored images. Additionally, we propose to overcome the lack of adequate\nannotated dataset problem by combining/aligning two publicly available\ncolorfulness databases using the results of a new subjective test which employs\na common subset of both databases. Using the obtained subjectively annotated\ndataset with 180 colored images, we finally demonstrate the efficacy of our\nproposed model over the traditional methods, both quantitatively and\nqualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:24:37 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Zerman", "Emin", ""], ["Rana", "Aakanksha", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1908.08506", "submitter": "Zhan Xu", "authors": "Zhan Xu, Yang Zhou, Evangelos Kalogerakis, Karan Singh", "title": "Predicting Animation Skeletons for 3D Articulated Models via Volumetric\n  Nets", "comments": "3DV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning method for predicting animation skeletons for input 3D\nmodels of articulated characters. In contrast to previous approaches that fit\npre-defined skeleton templates or predict fixed sets of joints, our method\nproduces an animation skeleton tailored for the structure and geometry of the\ninput 3D model. Our architecture is based on a stack of hourglass modules\ntrained on a large dataset of 3D rigged characters mined from the web. It\noperates on the volumetric representation of the input 3D shapes augmented with\ngeometric shape features that provide additional cues for joint and bone\nlocations. Our method also enables intuitive user control of the\nlevel-of-detail for the output skeleton. Our evaluation demonstrates that our\napproach predicts animation skeletons that are much more similar to the ones\ncreated by humans compared to several alternatives and baselines.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 17:26:46 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Xu", "Zhan", ""], ["Zhou", "Yang", ""], ["Kalogerakis", "Evangelos", ""], ["Singh", "Karan", ""]]}, {"id": "1908.08532", "submitter": "Nate Phillips Mr.", "authors": "Nate Phillips, Kristen Massey, Mohammed Safayet Arefin, and J. Edward\n  Swan II", "title": "Design, Assembly, Calibration, and Measurement of an Augmented Reality\n  Haploscope", "comments": "Accepted and presented at the IEEE VR 2018 Workshop on Perceptual and\n  Cognitive Issues in AR (PERCAR); pre-print version", "journal-ref": "Proceedings of PERCAR: The Fifth IEEE Virtual Reality Workshop on\n  Perceptual and Cognitive Issues in AR, 2019 IEEE Conference on Virtual\n  Reality and 3D User Interfaces, Osaka, Japan, March 23-27, pages 1770-1774,\n  2019", "doi": "10.1109/VR.2019.8798335", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A haploscope is an optical system which produces a carefully controlled\nvirtual image. Since the development of Wheatstone's original stereoscope in\n1838, haploscopes have been used to measure perceptual properties of human\nstereoscopic vision. This paper presents an augmented reality (AR) haploscope,\nwhich allows the viewing of virtual objects superimposed against the real\nworld. Our lab has used generations of this device to make a careful series of\nperceptual measurements of AR phenomena, which have been described in\npublications over the previous 8 years. This paper systematically describes the\ndesign, assembly, calibration, and measurement of our AR haploscope. These\nmethods have been developed and improved in our lab over the past 10 years.\nDespite the fact that 180 years have elapsed since the original report of\nWheatstone's stereoscope, we have not previously found a paper that describes\nthese kinds of details.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 23:32:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Phillips", "Nate", ""], ["Massey", "Kristen", ""], ["Arefin", "Mohammed Safayet", ""], ["Swan", "J. Edward", "II"]]}, {"id": "1908.08597", "submitter": "Danielle Bragg", "authors": "Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick\n  Boudrealt, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa\n  Kacorri, Tessa Verhoef, Christian Vogler, Meredith Ringel Morris", "title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.CY cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing successful sign language recognition, generation, and translation\nsystems requires expertise in a wide range of fields, including computer\nvision, computer graphics, natural language processing, human-computer\ninteraction, linguistics, and Deaf culture. Despite the need for deep\ninterdisciplinary knowledge, existing research occurs in separate disciplinary\nsilos, and tackles separate portions of the sign language processing pipeline.\nThis leads to three key questions: 1) What does an interdisciplinary view of\nthe current landscape reveal? 2) What are the biggest challenges facing the\nfield? and 3) What are the calls to action for people working in the field? To\nhelp answer these questions, we brought together a diverse group of experts for\na two-day workshop. This paper presents the results of that interdisciplinary\nworkshop, providing key background that is often overlooked by computer\nscientists, a review of the state-of-the-art, a set of pressing challenges, and\na call to action for the research community.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 21:05:17 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bragg", "Danielle", ""], ["Koller", "Oscar", ""], ["Bellard", "Mary", ""], ["Berke", "Larwan", ""], ["Boudrealt", "Patrick", ""], ["Braffort", "Annelies", ""], ["Caselli", "Naomi", ""], ["Huenerfauth", "Matt", ""], ["Kacorri", "Hernisa", ""], ["Verhoef", "Tessa", ""], ["Vogler", "Christian", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "1908.08893", "submitter": "Paul Rosen", "authors": "Ghulam Jilani Quadri and Paul Rosen", "title": "You Can't Publish Replication Studies (and How to Anyways)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility has been increasingly encouraged by communities of science in\norder to validate experimental conclusions, and replication studies represent a\nsignificant opportunity to vision scientists wishing contribute new perceptual\nmodels, methods, or insights to the visualization community. Unfortunately, the\nnotion of replication of previous studies does not lend itself to how we\ncommunicate research findings. Simple put, studies that re-conduct and confirm\nearlier results do not hold any novelty, a key element to the modern research\npublication system. Nevertheless, savvy researchers have discovered ways to\nproduce replication studies by embedding them into other sufficiently novel\nstudies. In this position paper, we define three methods -- re-evaluation,\nexpansion, and specialization -- for embedding a replication study into a novel\npublished work. Within this context, we provide a non-exhaustive case study on\nreplications of Cleveland and McGill's seminal work on graphical perception. As\nit turns out, numerous replication studies have been carried out based on that\nwork, which have both confirmed prior findings and shined new light on our\nunderstanding of human perception. Finally, we discuss how publishing a true\nreplication study should be avoided, while providing suggestions for how vision\nscientists and others can still use replication studies as a vehicle to\nproducing visualization research publications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 16:21:28 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Quadri", "Ghulam Jilani", ""], ["Rosen", "Paul", ""]]}, {"id": "1908.09098", "submitter": "Di Qiu", "authors": "Di Qiu, Lok-Ming Lui", "title": "Inconsistent Surface Registration via Optimization of Mapping\n  Distortions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of registering two surfaces, of which a natural\nbijection between them does not exist. More precisely, only a partial subset of\nthe source surface is assumed to be in correspondence with a subset of the\ntarget surface. We call such a problem an {\\it inconsistent surface\nregistration (ISR)} problem. This problem is challenging as the corresponding\nregions on each surface and a meaningful bijection between them have to be\nsimultaneously determined. In this paper, we propose a variational model to\nsolve the ISR problem by minimizing mapping distortions. Mapping distortions\nare described by the Beltrami coefficient as well as the differential of the\nmapping. Registration is then guided by feature landmarks and/or intensities,\nsuch as curvatures, defined on each surface. The key idea of the approach is to\ncontrol angle and scale distortions via quasiconformal theory as well as\nminimizing landmark and/or intensity mismatch. A splitting method is proposed\nto iteratively search for the optimal corresponding regions as well as the\noptimal bijection between them. Bijectivity of the mapping is easily enforced\nby a thresholding of the Beltrami coefficient. We test the proposed method on\nboth synthetic and real examples. Experimental results demonstrate the efficacy\nof our proposed model.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 06:34:07 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 11:58:22 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Qiu", "Di", ""], ["Lui", "Lok-Ming", ""]]}, {"id": "1908.09530", "submitter": "Aakash K.T.", "authors": "Aakash KT, Parikshit Sakurikar, Saurabh Saini, P. J. Narayanan", "title": "A Flexible Neural Renderer for Material Visualization", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo realism in computer generated imagery is crucially dependent on how\nwell an artist is able to recreate real-world materials in the scene. The\nworkflow for material modeling and editing typically involves manual tweaking\nof material parameters and uses a standard path tracing engine for visual\nfeedback. A lot of time may be spent in iterative selection and rendering of\nmaterials at an appropriate quality. In this work, we propose a convolutional\nneural network based workflow which quickly generates high-quality ray traced\nmaterial visualizations on a shaderball. Our novel architecture allows for\ncontrol over environment lighting and assists material selection along with the\nability to render spatially-varying materials. Additionally, our network\nenables control over environment lighting which gives an artist more freedom\nand provides better visualization of the rendered material. Comparison with\nstate-of-the-art denoising and neural rendering techniques suggests that our\nneural renderer performs faster and better. We provide a interactive\nvisualization tool and release our training dataset to foster further research\nin this area.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 08:52:53 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["KT", "Aakash", ""], ["Sakurikar", "Parikshit", ""], ["Saini", "Saurabh", ""], ["Narayanan", "P. J.", ""]]}, {"id": "1908.09707", "submitter": "Thayne Walker", "authors": "Thayne T. Walker and Nathan R. Sturtevant", "title": "Collision Detection for Agents in Multi-Agent Pathfinding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CG cs.GR cs.MA cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on the multi-agent pathfinding problem (MAPF) has begun to study\nagents with motion that is more complex, for example, with non-unit action\ndurations and kinematic constraints. An important aspect of MAPF is collision\ndetection. Many collision detection approaches exist, but often suffer from\nissues such as high computational cost or causing false negative or false\npositive detections. In practice, these issues can result in problems that\nrange from inefficiency and annoyance to catastrophic. The main contribution of\nthis technical report is to provide a high-level overview of major categories\nof collision detection, along with methods of collision detection and\nanticipatory collision avoidance for agents that are both computationally\nefficient and highly accurate.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 14:35:11 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 14:41:46 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 18:27:40 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Walker", "Thayne T.", ""], ["Sturtevant", "Nathan R.", ""]]}, {"id": "1908.09913", "submitter": "Evgeny Lipovetsky", "authors": "Evgeny Lipovetsky", "title": "Subdivision of point-normal pairs with application to smoothing feasible\n  robot path", "comments": "The text shall be reworked and extended significantly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous paper [11] we introduced a weighted binary average of two 2D\npoint-normal pairs, termed circle average, and investigated subdivision schemes\nbased on it. These schemes refine point-normal pairs in 2D, and converge to\nlimit curves and limit normals. Such a scheme has the disadvantage that the\nlimit normals are not the normals of the limit curve. In this paper we solve\nthis problem by proposing a new averaging method, and obtaining a new family of\nalgorithms based on it. We demonstrate their new editing capabilities and apply\nthis subdivision technique to smooth a precomputed feasible polygonal point\nrobot path.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 20:40:28 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 06:42:04 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 19:35:26 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lipovetsky", "Evgeny", ""]]}, {"id": "1908.10335", "submitter": "Raoul de Charette", "authors": "Shirsendu Sukanta Halder, Jean-Fran\\c{c}ois Lalonde, Raoul de Charette", "title": "Physics-Based Rendering for Improving Robustness to Rain", "comments": "ICCV 2019. Supplementary pdf / videos available on project page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the robustness to rain, we present a physically-based rain\nrendering pipeline for realistically inserting rain into clear weather images.\nOur rendering relies on a physical particle simulator, an estimation of the\nscene lighting and an accurate rain photometric modeling to augment images with\narbitrary amount of realistic rain or fog. We validate our rendering with a\nuser study, proving our rain is judged 40% more realistic that\nstate-of-the-art. Using our generated weather augmented Kitti and Cityscapes\ndataset, we conduct a thorough evaluation of deep object detection and semantic\nsegmentation algorithms and show that their performance decreases in degraded\nweather, on the order of 15% for object detection and 60% for semantic\nsegmentation. Furthermore, we show refining existing networks with our\naugmented images improves the robustness of both object detection and semantic\nsegmentation algorithms. We experiment on nuScenes and measure an improvement\nof 15% for object detection and 35% for semantic segmentation compared to\noriginal rainy performance. Augmented databases and code are available on the\nproject page.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 17:13:46 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Halder", "Shirsendu Sukanta", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["de Charette", "Raoul", ""]]}, {"id": "1908.10468", "submitter": "Ricardo Bigolin Lanfredi", "authors": "Ricardo Bigolin Lanfredi, Joyce D. Schroeder, Clement Vachet, Tolga\n  Tasdizen", "title": "Adversarial regression training for visualizing the progression of\n  chronic obstructive pulmonary disease with chest x-rays", "comments": "Accepted for MICCAI 2019", "journal-ref": "International Conference on Medical Image Computing and\n  Computer-Assisted Intervention. Springer, Cham, 2019. p. 685-693", "doi": "10.1007/978-3-030-32226-7_76", "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of what spatial elements of medical images deep learning methods\nuse as evidence is important for model interpretability, trustiness, and\nvalidation. There is a lack of such techniques for models in regression tasks.\nWe propose a method, called visualization for regression with a generative\nadversarial network (VR-GAN), for formulating adversarial training specifically\nfor datasets containing regression target values characterizing disease\nseverity. We use a conditional generative adversarial network where the\ngenerator attempts to learn to shift the output of a regressor through creating\ndisease effect maps that are added to the original images. Meanwhile, the\nregressor is trained to predict the original regression value for the modified\nimages. A model trained with this technique learns to provide visualization for\nhow the image would appear at different stages of the disease. We analyze our\nmethod in a dataset of chest x-rays associated with pulmonary function tests,\nused for diagnosing chronic obstructive pulmonary disease (COPD). For\nvalidation, we compute the difference of two registered x-rays of the same\npatient at different time points and correlate it to the generated disease\neffect map. The proposed method outperforms a technique based on classification\nand provides realistic-looking images, making modifications to images following\nwhat radiologists usually observe for this disease. Implementation code is\navailable at https://github.com/ricbl/vrgan.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 21:14:12 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Lanfredi", "Ricardo Bigolin", ""], ["Schroeder", "Joyce D.", ""], ["Vachet", "Clement", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1908.11505", "submitter": "Lan Xu", "authors": "Lan Xu, Weipeng Xu, Vladislav Golyanik, Marc Habermann, Lu Fang and\n  Christian Theobalt", "title": "EventCap: Monocular 3D Capture of High-Speed Human Motions using an\n  Event Camera", "comments": "10 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high frame rate is a critical requirement for capturing fast human\nmotions. In this setting, existing markerless image-based methods are\nconstrained by the lighting requirement, the high data bandwidth and the\nconsequent high computation overhead. In this paper, we propose EventCap ---\nthe first approach for 3D capturing of high-speed human motions using a single\nevent camera. Our method combines model-based optimization and CNN-based human\npose detection to capture high-frequency motion details and to reduce the\ndrifting in the tracking. As a result, we can capture fast motions at\nmillisecond resolution with significantly higher data efficiency than using\nhigh frame rate videos. Experiments on our new event-based fast human motion\ndataset demonstrate the effectiveness and accuracy of our method, as well as\nits robustness to challenging lighting conditions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 01:59:41 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Xu", "Lan", ""], ["Xu", "Weipeng", ""], ["Golyanik", "Vladislav", ""], ["Habermann", "Marc", ""], ["Fang", "Lu", ""], ["Theobalt", "Christian", ""]]}, {"id": "1908.11728", "submitter": "Josua Sassen", "authors": "Josua Sassen, Behrend Heeren, Klaus Hildebrandt, Martin Rumpf", "title": "Geometric optimization using nonlinear rotation-invariant coordinates", "comments": null, "journal-ref": "Computer Aided Geometric Design, 77 (2020), 101829", "doi": "10.1016/j.cagd.2020.101829", "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric optimization problems are at the core of many applications in\ngeometry processing. The choice of a representation fitting an optimization\nproblem can considerably simplify solving the problem. We consider the\nNonlinear Rotation-Invariant Coordinates (NRIC) that represent the nodal\npositions of a discrete triangular surface with fixed combinatorics as a vector\nthat stacks all edge lengths and dihedral angles of the mesh. It is known that\nthis representation associates a unique vector to an equivalence class of nodal\npositions that differ by a rigid body motion. Moreover, integrability\nconditions that ensure the existence of nodal positions that match a given\nvector of edge lengths and dihedral angles have been established. The goal of\nthis paper is to develop the machinery needed to use the NRIC for solving\ngeometric optimization problems. First, we use the integrability conditions to\nderive an implicit description of the space of discrete surfaces as a\nsubmanifold of an Euclidean space and a corresponding description of its\ntangent spaces. Secondly, we reformulate the integrability conditions using\nquaternions and provide explicit formulas for their first and second\nderivatives facilitating the use of Hessians in NRIC-based optimization\nproblems. Lastly, we introduce a fast and robust algorithm that reconstructs\nnodal positions from almost integrable NRIC. We demonstrate the benefits of\nthis approach on a collection of geometric optimization problems. Comparisons\nto alternative approaches indicate that NRIC-based optimization is particularly\neffective for problems involving near-isometric deformations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 13:27:47 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 10:25:10 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Sassen", "Josua", ""], ["Heeren", "Behrend", ""], ["Hildebrandt", "Klaus", ""], ["Rumpf", "Martin", ""]]}]