[{"id": "2009.00083", "submitter": "Jonas Lukasczyk", "authors": "Jonas Lukasczyk, Christoph Garth, Ross Maciejewski, and Julien Tierny", "title": "Localized Topological Simplification of Scalar Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a localized algorithm for the topological simplification\nof scalar data, an essential pre-processing step of topological data analysis\n(TDA). Given a scalar field f and a selection of extrema to preserve, the\nproposed localized topological simplification (LTS) derives a function g that\nis close to f and only exhibits the selected set of extrema. Specifically, sub-\nand superlevel set components associated with undesired extrema are first\nlocally flattened and then correctly embedded into the global scalar field,\nsuch that these regions are guaranteed -- from a combinatorial perspective --\nto no longer contain any undesired extrema. In contrast to previous global\napproaches, LTS only and independently processes regions of the domain that\nactually need to be simplified, which already results in a noticeable speedup.\nMoreover, due to the localized nature of the algorithm, LTS can utilize\nshared-memory parallelism to simplify regions simultaneously with a high\nparallel efficiency (70%). Hence, LTS significantly improves interactivity for\nthe exploration of simplification parameters and their effect on subsequent\ntopological analysis. For such exploration tasks, LTS brings the overall\nexecution time of a plethora of TDA pipelines from minutes down to seconds,\nwith an average observed speedup over state-of-the-art techniques of up to x36.\nFurthermore, in the special case where preserved extrema are selected based on\ntopological persistence, an adapted version of LTS partially computes the\npersistence diagram and simultaneously simplifies features below a predefined\npersistence threshold. The effectiveness of LTS, its parallel efficiency, and\nits resulting benefits for TDA are demonstrated on several simulated and\nacquired datasets from different application domains, including physics,\nchemistry, and biomedical imaging.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 19:57:40 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Lukasczyk", "Jonas", ""], ["Garth", "Christoph", ""], ["Maciejewski", "Ross", ""], ["Tierny", "Julien", ""]]}, {"id": "2009.00149", "submitter": "Timo Bolkart", "authors": "Partha Ghosh, Pravir Singh Gupta, Roy Uziel, Anurag Ranjan, Michael\n  Black, Timo Bolkart", "title": "GIF: Generative Interpretable Faces", "comments": "International Conference on 3D Vision (3DV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo-realistic visualization and animation of expressive human faces have\nbeen a long standing challenge. 3D face modeling methods provide parametric\ncontrol but generates unrealistic images, on the other hand, generative 2D\nmodels like GANs (Generative Adversarial Networks) output photo-realistic face\nimages, but lack explicit control. Recent methods gain partial control, either\nby attempting to disentangle different factors in an unsupervised manner, or by\nadding control post hoc to a pre-trained model. Unconditional GANs, however,\nmay entangle factors that are hard to undo later. We condition our generative\nmodel on pre-defined control parameters to encourage disentanglement in the\ngeneration process. Specifically, we condition StyleGAN2 on FLAME, a generative\n3D face model. While conditioning on FLAME parameters yields unsatisfactory\nresults, we find that conditioning on rendered FLAME geometry and photometric\ndetails works well. This gives us a generative 2D face model named GIF\n(Generative Interpretable Faces) that offers FLAME's parametric control. Here,\ninterpretable refers to the semantic meaning of different parameters. Given\nFLAME parameters for shape, pose, expressions, parameters for appearance,\nlighting, and an additional style vector, GIF outputs photo-realistic face\nimages. We perform an AMT based perceptual study to quantitatively and\nqualitatively evaluate how well GIF follows its conditioning. The code, data,\nand trained model are publicly available for research purposes at\nhttp://gif.is.tue.mpg.de.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 23:40:26 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 13:37:01 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ghosh", "Partha", ""], ["Gupta", "Pravir Singh", ""], ["Uziel", "Roy", ""], ["Ranjan", "Anurag", ""], ["Black", "Michael", ""], ["Bolkart", "Timo", ""]]}, {"id": "2009.00408", "submitter": "Dieter Morgenroth", "authors": "Dieter Morgenroth, Stefan Reinhardt, Daniel Weiskopf, Bernhard\n  Eberhardt", "title": "Efficient 2D Simulation on Moving 3D Surfaces", "comments": "Symposium on Computer Animation (SCA) 2020", "journal-ref": "Computer Graphics Forum, Volume 39, Number 8, 2020", "doi": "10.1111/cgf.14098", "report-no": "39-issue 8", "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to simulate fluid flow on evolving surfaces, e.g., an oil\nfilm on a water surface. Given an animated surface (e.g., extracted from a\nparticle-based fluid simulation) in three-dimensional space, we add a second\nsimulation on this base animation. In general, we solve a partial differential\nequation (PDE) on a level set surface obtained from the animated input surface.\nThe properties of the input surface are transferred to a sparse volume data\nstructure that is then used for the simulation. We introduce one-way coupling\nstrategies from input properties to our simulation and we add conservation of\nmass and momentum to existing methods that solve a PDE in a narrow-band using\nthe Closest Point Method. In this way, we efficiently compute high-resolution\n2D simulations on coarse input surfaces. Our approach helps visual effects\ncreators easily integrate a workflow to simulate material flow on evolving\nsurfaces into their existing production pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 13:21:01 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Morgenroth", "Dieter", ""], ["Reinhardt", "Stefan", ""], ["Weiskopf", "Daniel", ""], ["Eberhardt", "Bernhard", ""]]}, {"id": "2009.00887", "submitter": "Oleg Lobachev", "authors": "Oleg Lobachev, Moritz Berthold, Henriette Pfeffer, Michael Guthe,\n  Birte S. Steiniger", "title": "Inspection of histological 3D reconstructions in virtual reality", "comments": "Supplemental data under https://zenodo.org/record/4268535, video also\n  available under https://youtu.be/FQSLiW1wQOc", "journal-ref": "https://www.frontiersin.org/articles/10.3389/frvir.2021.628449/abstract", "doi": "10.3389/frvir.2021.628449", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D reconstruction is a challenging current topic in medical research. We\nperform 3D reconstructions from serial sections stained by immunohistological\nmethods. This paper presents an immersive visualisation solution to quality\ncontrol (QC), inspect, and analyse such reconstructions. QC is essential to\nestablish correct digital processing methodologies. Visual analytics, such as\nannotation placement, mesh painting, and classification utility, facilitates\nmedical research insights. We propose a visualisation in virtual reality (VR)\nfor these purposes. In this manner, we advance the microanatomical research of\nhuman bone marrow and spleen. Both 3D reconstructions and original data are\navailable in VR. Data inspection is streamlined by subtle implementation\ndetails and general immersion in VR.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:21:59 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 17:47:36 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lobachev", "Oleg", ""], ["Berthold", "Moritz", ""], ["Pfeffer", "Henriette", ""], ["Guthe", "Michael", ""], ["Steiniger", "Birte S.", ""]]}, {"id": "2009.00905", "submitter": "Sanghun Park", "authors": "Sanghun Park, Kwanggyoon Seo, Junyong Noh", "title": "Neural Crossbreed: Neural Based Image Metamorphosis", "comments": "16 pages", "journal-ref": "ACM Transactions on Graphics (Proceeding of SIGGRAPH Asia), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose Neural Crossbreed, a feed-forward neural network that can learn a\nsemantic change of input images in a latent space to create the morphing\neffect. Because the network learns a semantic change, a sequence of meaningful\nintermediate images can be generated without requiring the user to specify\nexplicit correspondences. In addition, the semantic change learning makes it\npossible to perform the morphing between the images that contain objects with\nsignificantly different poses or camera views. Furthermore, just as in\nconventional morphing techniques, our morphing network can handle shape and\nappearance transitions separately by disentangling the content and the style\ntransfer for rich usability. We prepare a training dataset for morphing using a\npre-trained BigGAN, which generates an intermediate image by interpolating two\nlatent vectors at an intended morphing value. This is the first attempt to\naddress image morphing using a pre-trained generative model in order to learn\nsemantic transformation. The experiments show that Neural Crossbreed produces\nhigh quality morphed images, overcoming various limitations associated with\nconventional approaches. In addition, Neural Crossbreed can be further extended\nfor diverse applications such as multi-image morphing, appearance transfer, and\nvideo frame interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 08:56:47 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Park", "Sanghun", ""], ["Seo", "Kwanggyoon", ""], ["Noh", "Junyong", ""]]}, {"id": "2009.00931", "submitter": "Jan Hombeck", "authors": "Jan Hombeck, Li Ji, Kai Lawonn, Charles Perin", "title": "A Study of Opacity Ranges for Transparent Overlays in 3D Landscapes", "comments": "IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {When visualizing data in a realistically rendered 3D virtual environment, it\nis often important to represent not only the 3D scene but also overlaid\ninformation about additional, abstract data. These overlays must be usefully\nvisible, i.e. be readable enough to convey the information they represent, but\nremain unobtrusive to avoid cluttering the view. We take a step toward\nestablishing guidelines for designing such overlays by studying the\nrelationship between three different patterns (filled, striped and dotted\npatterns), two pattern densities, the presence or not of a solid outline, two\ntypes of background (blank and with trees), and the opacity of the overlay. For\neach combination of factors, participants set the faintest and the strongest\nacceptable opacity values. Results from this first study suggest that i) ranges\nof acceptable opacities are around 20-70%, that ii) ranges can be extended by\n5% by using an outline, and that iii) ranges shift based on features like\npattern and density.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 10:21:18 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Hombeck", "Jan", ""], ["Ji", "Li", ""], ["Lawonn", "Kai", ""], ["Perin", "Charles", ""]]}, {"id": "2009.01424", "submitter": "Wenbo Hu", "authors": "Wenbo Hu, Menghan Xia, Chi-Wing Fu, and Tien-Tsin Wong", "title": "Mononizing Binocular Videos", "comments": "16 pages, 17 figures. Accepted in Siggraph Asia 2020", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH Asia 2020 issue)", "doi": "10.1145/3414685.3417764", "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the idea ofmono-nizingbinocular videos and a frame-work\nto effectively realize it. Mono-nize means we purposely convert abinocular\nvideo into a regular monocular video with the stereo informationimplicitly\nencoded in a visual but nearly-imperceptible form. Hence, wecan impartially\ndistribute and show the mononized video as an ordinarymonocular video. Unlike\nordinary monocular videos, we can restore from itthe original binocular video\nand show it on a stereoscopic display. To start,we formulate an\nencoding-and-decoding framework with the pyramidal de-formable fusion module to\nexploit long-range correspondences between theleft and right views, a\nquantization layer to suppress the restoring artifacts,and the compression\nnoise simulation module to resist the compressionnoise introduced by modern\nvideo codecs. Our framework is self-supervised,as we articulate our objective\nfunction with loss terms defined on the input:a monocular term for creating the\nmononized video, an invertibility termfor restoring the original video, and a\ntemporal term for frame-to-framecoherence. Further, we conducted extensive\nexperiments to evaluate ourgenerated mononized videos and restored binocular\nvideos for diverse typesof images and 3D movies. Quantitative results on both\nstandard metrics anduser perception studies show the effectiveness of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 03:04:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hu", "Wenbo", ""], ["Xia", "Menghan", ""], ["Fu", "Chi-Wing", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "2009.01456", "submitter": "Minhyuk Sung", "authors": "Minhyuk Sung, Zhenyu Jiang, Panos Achlioptas, Niloy J. Mitra, Leonidas\n  J. Guibas", "title": "DeformSyncNet: Deformation Transfer via Synchronized Shape Deformation\n  Spaces", "comments": "SIGGRAPH Asia 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape deformation is an important component in any geometry processing\ntoolbox. The goal is to enable intuitive deformations of single or multiple\nshapes or to transfer example deformations to new shapes while preserving the\nplausibility of the deformed shape(s). Existing approaches assume access to\npoint-level or part-level correspondence or establish them in a preprocessing\nphase, thus limiting the scope and generality of such approaches. We propose\nDeformSyncNet, a new approach that allows consistent and synchronized shape\ndeformations without requiring explicit correspondence information.\nTechnically, we achieve this by encoding deformations into a class-specific\nidealized latent space while decoding them into an individual, model-specific\nlinear deformation action space, operating directly in 3D. The underlying\nencoding and decoding are performed by specialized (jointly trained) neural\nnetworks. By design, the inductive bias of our networks results in a\ndeformation space with several desirable properties, such as path invariance\nacross different deformation pathways, which are then also approximately\npreserved in real space. We qualitatively and quantitatively evaluate our\nframework against multiple alternative approaches and demonstrate improved\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 05:26:32 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Sung", "Minhyuk", ""], ["Jiang", "Zhenyu", ""], ["Achlioptas", "Panos", ""], ["Mitra", "Niloy J.", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2009.01469", "submitter": "Bin Chen", "authors": "Ruizhen Hu, Juzhan Xu, Bin Chen, Minglun Gong, Hao Zhang, Hui Huang", "title": "TAP-Net: Transport-and-Pack using Reinforcement Learning", "comments": null, "journal-ref": "ACM Transactions on Graphics 2020", "doi": "10.1145/3414685.3417796", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the transport-and-pack(TAP) problem, a frequently encountered\ninstance of real-world packing, and develop a neural optimization solution\nbased on reinforcement learning. Given an initial spatial configuration of\nboxes, we seek an efficient method to iteratively transport and pack the boxes\ncompactly into a target container. Due to obstruction and accessibility\nconstraints, our problem has to add a new search dimension, i.e., finding an\noptimal transport sequence, to the already immense search space for packing\nalone. Using a learning-based approach, a trained network can learn and encode\nsolution patterns to guide the solution of new problem instances instead of\nexecuting an expensive online search. In our work, we represent the transport\nconstraints using a precedence graph and train a neural network, coined\nTAP-Net, using reinforcement learning to reward efficient and stable packing.\nThe network is built on an encoder-decoder architecture, where the encoder\nemploys convolution layers to encode the box geometry and precedence graph and\nthe decoder is a recurrent neural network (RNN) which inputs the current\nencoder output, as well as the current box packing state of the target\ncontainer, and outputs the next box to pack, as well as its orientation. We\ntrain our network on randomly generated initial box configurations, without\nsupervision, via policy gradients to learn optimal TAP policies to maximize\npacking efficiency and stability. We demonstrate the performance of TAP-Net on\na variety of examples, evaluating the network through ablation studies and\ncomparisons to baselines and alternative network designs. We also show that our\nnetwork generalizes well to larger problem instances, when trained on\nsmall-sized inputs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 06:20:17 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hu", "Ruizhen", ""], ["Xu", "Juzhan", ""], ["Chen", "Bin", ""], ["Gong", "Minglun", ""], ["Zhang", "Hao", ""], ["Huang", "Hui", ""]]}, {"id": "2009.01512", "submitter": "Julien Tierny", "authors": "Harish Doraiswamy and Julien Tierny and Paulo J. S. Silva and Luis\n  Gustavo Nonato and Claudio Silva", "title": "TopoMap: A 0-dimensional Homology Preserving Projection of\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional Projection is a fundamental tool for high-dimensional data\nanalytics and visualization. With very few exceptions, projection techniques\nare designed to map data from a high-dimensional space to a visual space so as\nto preserve some dissimilarity (similarity) measure, such as the Euclidean\ndistance for example. In fact, although adopting distinct mathematical\nformulations designed to favor different aspects of the data, most\nmultidimensional projection methods strive to preserve dissimilarity measures\nthat encapsulate geometric properties such as distances or the proximity\nrelation between data objects. However, geometric relations are not the only\ninteresting property to be preserved in a projection. For instance, the\nanalysis of particular structures such as clusters and outliers could be more\nreliably performed if the mapping process gives some guarantee as to\ntopological invariants such as connected components and loops. This paper\nintroduces TopoMap, a novel projection technique which provides topological\nguarantees during the mapping process. In particular, the proposed method\nperforms the mapping from a high-dimensional space to a visual space, while\npreserving the 0-dimensional persistence diagram of the Rips filtration of the\nhigh-dimensional data, ensuring that the filtrations generate the same\nconnected components when applied to the original as well as projected data.\nThe presented case studies show that the topological guarantee provided by\nTopoMap not only brings confidence to the visual analytic process but also can\nbe used to assist in the assessment of other projection methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 08:30:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Doraiswamy", "Harish", ""], ["Tierny", "Julien", ""], ["Silva", "Paulo J. S.", ""], ["Nonato", "Luis Gustavo", ""], ["Silva", "Claudio", ""]]}, {"id": "2009.01724", "submitter": "Jos\\'e A. Iglesias", "authors": "Jos\\'e A. Iglesias", "title": "Symmetry and scaling limits for matching of implicit surfaces based on\n  thin shell energies", "comments": "28 pages, 4 figures, 2 tables. V3: Final accepted version", "journal-ref": "ESAIM: Mathematical Modelling and Numerical Analysis,\n  55(3):1133-1161, 2021", "doi": "10.1051/m2an/2021018", "report-no": null, "categories": "math.OC cs.GR math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper by Iglesias, Rumpf and Scherzer (Found. Comput. Math.\n18(4), 2018) a variational model for deformations matching a pair of shapes\ngiven as level set functions was proposed. Its main feature is the presence of\nanisotropic energies active only in a narrow band around the hypersurfaces that\nresemble the behavior of elastic shells. In this work we consider some\nextensions and further analysis of that model. First, we present a symmetric\nenergy functional such that given two particular shapes, it assigns the same\nenergy to any given deformation as to its inverse when the roles of the shapes\nare interchanged, and introduce the adequate parameter scaling to recover a\nsurface problem when the width of the narrow band vanishes. Then, we obtain\nexistence of minimizing deformations for the symmetric energy in classes of\nbi-Sobolev homeomorphisms for small enough widths, and prove a\n$\\Gamma$-convergence result for the corresponding non-symmetric energies as the\nwidth tends to zero. Finally, numerical results on realistic shape matching\napplications demonstrating the effect of the symmetric energy are presented.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 15:03:14 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 16:06:04 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 17:32:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Iglesias", "Jos\u00e9 A.", ""]]}, {"id": "2009.01891", "submitter": "Torin McDonald", "authors": "Torin McDonald, Will Usher, Nate Morrical, Attila Gyulassy, Steve\n  Petruzza, Frederick Federer, Alessandra Angelucci, and Valerio Pascucci", "title": "Improving the Usability of Virtual Reality Neuron Tracing with\n  Topological Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the field of connectomics are working to reconstruct a map of\nneural connections in the brain in order to understand at a fundamental level\nhow the brain processes information. Constructing this wiring diagram is done\nby tracing neurons through high-resolution image stacks acquired with\nfluorescence microscopy imaging techniques. While a large number of automatic\ntracing algorithms have been proposed, these frequently rely on local features\nin the data and fail on noisy data or ambiguous cases, requiring time-consuming\nmanual correction. As a result, manual and semi-automatic tracing methods\nremain the state-of-the-art for creating accurate neuron reconstructions. We\npropose a new semi-automatic method that uses topological features to guide\nusers in tracing neurons and integrate this method within a virtual reality\n(VR) framework previously used for manual tracing. Our approach augments both\nvisualization and interaction with topological elements, allowing rapid\nunderstanding and tracing of complex morphologies. In our pilot study,\nneuroscientists demonstrated a strong preference for using our tool over prior\napproaches, reported less fatigue during tracing, and commended the ability to\nbetter understand possible paths and alternatives. Quantitative evaluation of\nthe traces reveals that users' tracing speed increased, while retaining similar\naccuracy compared to a fully manual approach.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 19:20:50 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["McDonald", "Torin", ""], ["Usher", "Will", ""], ["Morrical", "Nate", ""], ["Gyulassy", "Attila", ""], ["Petruzza", "Steve", ""], ["Federer", "Frederick", ""], ["Angelucci", "Alessandra", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2009.02005", "submitter": "Tarik Crnovrsanin", "authors": "Tarik Crnovrsanin, Shilpika, Senthil Chandrasegaran, and Kwan-Liu Ma", "title": "Staged Animation Strategies for Online Dynamic Networks", "comments": "IEEE VIS InfoVis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks -- networks that change over time -- can be categorized into\ntwo types: offline dynamic networks, where all states of the network are known,\nand online dynamic networks, where only the past states of the network are\nknown. Research on staging animated transitions in dynamic networks has focused\nmore on offline data, where rendering strategies can take into account past and\nfuture states of the network. Rendering online dynamic networks is a more\nchallenging problem since it requires a balance between timeliness for\nmonitoring tasks -- so that the animations do not lag too far behind the events\n-- and clarity for comprehension tasks -- to minimize simultaneous changes that\nmay be difficult to follow. To illustrate the challenges placed by these\nrequirements, we explore three strategies to stage animations for online\ndynamic networks: time-based, event-based, and a new hybrid approach that we\nintroduce by combining the advantages of the first two. We illustrate the\nadvantages and disadvantages of each strategy in representing low- and\nhigh-throughput data and conduct a user study involving monitoring and\ncomprehension of dynamic networks. We also conduct a follow-up, a think-aloud\nstudy combining monitoring and comprehension with experts in dynamic network\nvisualization. Our findings show that animation staging strategies that\nemphasize comprehension do better for participant response times and accuracy.\nHowever, the notion of ``comprehension'' is not always clear when it comes to\ncomplex changes in highly dynamic networks, requiring some iteration in staging\nthat the hybrid approach affords. Based on our results, we make recommendations\nfor balancing event-based and time-based parameters for our hybrid approach.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 04:26:05 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Crnovrsanin", "Tarik", ""], ["Shilpika", "", ""], ["Chandrasegaran", "Senthil", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2009.02119", "submitter": "Youngwoo Yoon", "authors": "Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee,\n  Jaehong Kim, Geehyuk Lee", "title": "Speech Gesture Generation from the Trimodal Context of Text, Audio, and\n  Speaker Identity", "comments": "16 pages; ACM Transactions on Graphics (SIGGRAPH Asia 2020)", "journal-ref": null, "doi": "10.1145/3414685.3417838", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For human-like agents, including virtual avatars and social robots, making\nproper gestures while speaking is crucial in human--agent interaction.\nCo-speech gestures enhance interaction experiences and make the agents look\nalive. However, it is difficult to generate human-like gestures due to the lack\nof understanding of how people gesture. Data-driven approaches attempt to learn\ngesticulation skills from human demonstrations, but the ambiguous and\nindividual nature of gestures hinders learning. In this paper, we present an\nautomatic gesture generation model that uses the multimodal context of speech\ntext, audio, and speaker identity to reliably generate gestures. By\nincorporating a multimodal context and an adversarial training scheme, the\nproposed model outputs gestures that are human-like and that match with speech\ncontent and rhythm. We also introduce a new quantitative evaluation metric for\ngesture generation models. Experiments with the introduced metric and\nsubjective human evaluation showed that the proposed gesture generation model\nis better than existing end-to-end generation models. We further confirm that\nour model is able to work with synthesized audio in a scenario where contexts\nare constrained, and show that different gesture styles can be generated for\nthe same speech by specifying different speaker identities in the style\nembedding space that is learned from videos of various speakers. All the code\nand data is available at\nhttps://github.com/ai4r/Gesture-Generation-from-Trimodal-Context.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:42:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Yoon", "Youngwoo", ""], ["Cha", "Bok", ""], ["Lee", "Joo-Haeng", ""], ["Jang", "Minsu", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""], ["Lee", "Geehyuk", ""]]}, {"id": "2009.02122", "submitter": "Sebastian Mazza", "authors": "Sebastian Mazza, Daniel Patel, Ivan Viola", "title": "Homomorphic-Encrypted Volume Rendering", "comments": "Paper accepted for presentation at IEEE VIS (SciVis) 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030436", "report-no": null, "categories": "cs.CR cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computationally demanding tasks are typically calculated in dedicated data\ncenters, and real-time visualizations also follow this trend. Some rendering\ntasks, however, require the highest level of confidentiality so that no other\nparty, besides the owner, can read or see the sensitive data. Here we present a\ndirect volume rendering approach that performs volume rendering directly on\nencrypted volume data by using the homomorphic Paillier encryption algorithm.\nThis approach ensures that the volume data and rendered image are\nuninterpretable to the rendering server. Our volume rendering pipeline\nintroduces novel approaches for encrypted-data compositing, interpolation, and\nopacity modulation, as well as simple transfer function design, where each of\nthese routines maintains the highest level of privacy. We present performance\nand memory overhead analysis that is associated with our privacy-preserving\nscheme. Our approach is open and secure by design, as opposed to secure through\nobscurity. Owners of the data only have to keep their secure key confidential\nto guarantee the privacy of their volume data and the rendered images. Our work\nis, to our knowledge, the first privacy-preserving remote volume-rendering\napproach that does not require that any server involved be trustworthy; even in\ncases when the server is compromised, no sensitive data will be leaked to a\nforeign party.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 11:54:48 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:09:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Mazza", "Sebastian", ""], ["Patel", "Daniel", ""], ["Viola", "Ivan", ""]]}, {"id": "2009.02216", "submitter": "Noa Fish", "authors": "Noa Fish, Lilach Perry, Amit Bermano, Daniel Cohen-Or", "title": "SketchPatch: Sketch Stylization via Seamless Patch-level Synthesis", "comments": "SIGGRAPH Asia 2020", "journal-ref": null, "doi": "10.1145/3414685.3417816", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paradigm of image-to-image translation is leveraged for the benefit of\nsketch stylization via transfer of geometric textural details. Lacking the\nnecessary volumes of data for standard training of translation systems, we\nadvocate for operation at the patch level, where a handful of stylized sketches\nprovide ample mining potential for patches featuring basic geometric\nprimitives. Operating at the patch level necessitates special consideration of\nfull sketch translation, as individual translation of patches with no regard to\nneighbors is likely to produce visible seams and artifacts at patch borders.\nAligned pairs of styled and plain primitives are combined to form input hybrids\ncontaining styled elements around the border and plain elements within, and\ngiven as input to a seamless translation (ST) generator, whose output patches\nare expected to reconstruct the fully styled patch. An adversarial addition\npromotes generalization and robustness to diverse geometries at inference time,\nforming a simple and effective system for arbitrary sketch stylization, as\ndemonstrated upon a variety of styles and sketches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 14:20:46 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Fish", "Noa", ""], ["Perry", "Lilach", ""], ["Bermano", "Amit", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2009.02256", "submitter": "Wei Xu", "authors": "Xinyi Huang, Suphanut Jamonnak, Ye Zhao, Boyu Wang, Minh Hoai, Kevin\n  Yager, Wei Xu", "title": "Interactive Visual Study of Multiple Attributes Learning Model of X-Ray\n  Scattering Images", "comments": "IEEE SciVis Conference 2020", "journal-ref": "IEEE Transactions on Visualization & Computer Graphics 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing interactive visualization tools for deep learning are mostly applied\nto the training, debugging, and refinement of neural network models working on\nnatural images. However, visual analytics tools are lacking for the specific\napplication of x-ray image classification with multiple structural attributes.\nIn this paper, we present an interactive system for domain scientists to\nvisually study the multiple attributes learning models applied to x-ray\nscattering images. It allows domain scientists to interactively explore this\nimportant type of scientific images in embedded spaces that are defined on the\nmodel prediction output, the actual labels, and the discovered feature space of\nneural networks. Users are allowed to flexibly select instance images, their\nclusters, and compare them regarding the specified visual representation of\nattributes. The exploration is guided by the manifestation of model performance\nrelated to mutual relationships among attributes, which often affect the\nlearning accuracy and effectiveness. The system thus supports domain scientists\nto improve the training dataset and model, find questionable attributes labels,\nand identify outlier images or spurious data clusters. Case studies and\nscientists feedback demonstrate its functionalities and usefulness.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 00:38:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Huang", "Xinyi", ""], ["Jamonnak", "Suphanut", ""], ["Zhao", "Ye", ""], ["Wang", "Boyu", ""], ["Hoai", "Minh", ""], ["Yager", "Kevin", ""], ["Xu", "Wei", ""]]}, {"id": "2009.02294", "submitter": "Honglin Chen", "authors": "Honglin Chen and Hsueh-Ti Derek Liu and Alec Jacobson and David I.W.\n  Levin", "title": "Chordal Decomposition for Spectral Coarsening", "comments": "16 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel solver to significantly reduce the size of a geometric\noperator while preserving its spectral properties at the lowest frequencies. We\nuse chordal decomposition to formulate a convex optimization problem which\nallows the user to control the operator sparsity pattern. This allows for a\ntrade-off between the spectral accuracy of the operator and the cost of its\napplication. We efficiently minimize the energy with a change of variables and\nachieve state-of-the-art results on spectral coarsening. Our solver further\nenables novel applications including volume-to-surface approximation and\ndetaching the operator from the mesh, i.e., one can produce a mesh tailormade\nfor visualization and optimize an operator separately for computation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 16:43:30 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 20:24:13 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chen", "Honglin", ""], ["Liu", "Hsueh-Ti Derek", ""], ["Jacobson", "Alec", ""], ["Levin", "David I. W.", ""]]}, {"id": "2009.02462", "submitter": "Jiayi Eris Zhang", "authors": "Jiayi Eris Zhang and Seungbae Bang and David I.W. Levin and Alec\n  Jacobson", "title": "Complementary Dynamics", "comments": "11 pages, 16 figures, ACM SIGGRAPH ASIA 2020", "journal-ref": null, "doi": "10.1145/3414685.3417819", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to enrich arbitrary rig animations with\nelastodynamic secondary effects. Unlike previous methods which pit rig\ndisplacements and physical forces as adversaries against each other, we\nadvocate that physics should complement artists intentions. We propose\noptimizing for elastodynamic displacements in the subspace orthogonal to\ndisplacements that can be created by the rig. This ensures that the additional\ndynamic motions do not undo the rig animation. The complementary space is high\ndimensional, algebraically constructed without manual oversight, and capable of\nrich high-frequency dynamics. Unlike prior tracking methods, we do not require\nextra painted weights, segmentation into fixed and free regions or tracking\nclusters. Our method is agnostic to the physical model and plugs into\nnon-linear FEM simulations, geometric as-rigid-as-possible energies, or\nmass-spring models. Our method does not require a particular type of rig and\nadds secondary effects to skeletal animations, cage-based deformations, wire\ndeformers, motion capture data, and rigid-body simulations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 04:54:10 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhang", "Jiayi Eris", ""], ["Bang", "Seungbae", ""], ["Levin", "David I. W.", ""], ["Jacobson", "Alec", ""]]}, {"id": "2009.02480", "submitter": "Florian Martin", "authors": "Florian Martin and Ulrich Reif", "title": "Trimmed Spline Surfaces with Accurate Boundary Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce trimmed NURBS surfaces with accurate boundary control, briefly\ncalled ABC-surfaces, as a solution to the notorious problem of constructing\nwatertight or smooth ($G^1$ and $G^2)$ multi-patch surfaces within the function\nrange of standard CAD/CAM systems and the associated file exchange formats. Our\nconstruction is based on the appropriate blend of a base surface, which traces\nout the intended global shape, and a series of reparametrized ribbons, which\ndominate the shape near the boundary.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 07:05:11 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 09:22:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Martin", "Florian", ""], ["Reif", "Ulrich", ""]]}, {"id": "2009.02494", "submitter": "Zi Ye", "authors": "Zi Ye, Nobuyuki Umetani, Takeo Igarashi, Tim Hoffmann", "title": "A curvature and density-based generative representation of shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generative model for 3D surfaces based on a\nrepresentation of shapes with mean curvature and metric, which are invariant\nunder rigid transformation. Hence, compared with existing 3D machine learning\nframeworks, our model substantially reduces the influence of translation and\nrotation. In addition, the local structure of shapes will be more precisely\ncaptured, since the curvature is explicitly encoded in our model. Specifically,\nevery surface is first conformally mapped to a canonical domain, such as a unit\ndisk or a unit sphere. Then, it is represented by two functions: the mean\ncurvature half-density and the vertex density, over this canonical domain.\nAssuming that input shapes follow a certain distribution in a latent space, we\nuse the variational autoencoder to learn the latent space representation. After\nthe learning, we can generate variations of shapes by randomly sampling the\ndistribution in the latent space. Surfaces with triangular meshes can be\nreconstructed from the generated data by applying isotropic remeshing and spin\ntransformation, which is given by Dirac equation. We demonstrate the\neffectiveness of our model on datasets of man-made and biological shapes and\ncompare the results with other methods.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 08:30:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Ye", "Zi", ""], ["Umetani", "Nobuyuki", ""], ["Igarashi", "Takeo", ""], ["Hoffmann", "Tim", ""]]}, {"id": "2009.02581", "submitter": "Dan Reznik", "authors": "Dan Reznik and Ronaldo Garcia and Hellmuth Stachel", "title": "Area-Invariant Pedal-Like Curves Derived from the Ellipse", "comments": "15 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.GR cs.RO math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study six pedal-like curves associated with the ellipse which are\narea-invariant for pedal points lying on one of two shapes: (i) a circle\nconcentric with the ellipse, or (ii) the ellipse boundary itself. Case (i) is a\ncorollary to properties of the Curvature Centroid (Kr\\\"ummungs-Schwerpunkt) of\na curve, proved by Steiner in 1825. For case (ii) we prove area invariance\nalgebraically. Explicit expressions for all invariant areas are also provided.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 18:33:59 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 15:55:14 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 21:13:47 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Reznik", "Dan", ""], ["Garcia", "Ronaldo", ""], ["Stachel", "Hellmuth", ""]]}, {"id": "2009.02660", "submitter": "Qiang Zou", "authors": "Qiang Zou, Charlie C. L. Wang, Hsi-Yung Feng", "title": "Length-optimal tool path planning for freeform surfaces with preferred\n  feed directions", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method to generate tool paths for machining\nfreeform surfaces represented either as parametric surfaces or as triangular\nmeshes. This method allows for the optimal tradeoff between the preferred feed\ndirection field and the constant scallop height, and yields a minimized overall\npath length. The optimality is achieved by formulating tool path planning as a\nPoisson problem that minimizes a simple, quadratic energy. This Poisson\nformulation considers all tool paths at once, without resorting to any\nheuristic sampling or initial tool path choosing as in existing methods, and is\nthus a globally optimal solution. Finding the optimal tool paths amounts to\nsolving a well-conditioned sparse linear system, which is computationally\nconvenient and efficient. Tool paths are represented with an implicit scheme\nthat can completely avoid the challenging topological issues of path\nsingularities and self-intersections seen in previous methods. The presented\nmethod has been validated with a series of examples and comparisons.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 07:31:29 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zou", "Qiang", ""], ["Wang", "Charlie C. L.", ""], ["Feng", "Hsi-Yung", ""]]}, {"id": "2009.02702", "submitter": "Jens Schneider", "authors": "Khaled Al-Thelaya and Marco Agus and Jens Schneider", "title": "The Mixture Graph-A Data Structure for Compressing, Rendering, and\n  Querying Segmentation Histograms", "comments": "To appear in IEEE Transacations on Visualization and Computer\n  Graphics (IEEE Vis 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel data structure, called the Mixture Graph.\nThis data structure allows us to compress, render, and query segmentation\nhistograms. Such histograms arise when building a mipmap of a volume containing\nsegmentation IDs. Each voxel in the histogram mipmap contains a convex\ncombination (mixture) of segmentation IDs. Each mixture represents the\ndistribution of IDs in the respective voxel's children. Our method factorizes\nthese mixtures into a series of linear interpolations between exactly two\nsegmentation IDs. The result is represented as a directed acyclic graph (DAG)\nwhose nodes are topologically ordered. Pruning replicate nodes in the tree\nfollowed by compression allows us to store the resulting data structure\nefficiently. During rendering, transfer functions are propagated from sources\n(leafs) through the DAG to allow for efficient, pre-filtered rendering at\ninteractive frame rates. Assembly of histogram contributions across the\nfootprint of a given volume allows us to efficiently query partial histograms,\nachieving up to 178$\\times$ speed-up over na$\\mathrm{\\\"{i}}$ve parallelized\nrange queries. Additionally, we apply the Mixture Graph to compute correctly\npre-filtered volume lighting and to interactively explore segments based on\nshape, geometry, and orientation using multi-dimensional transfer functions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 10:40:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Al-Thelaya", "Khaled", ""], ["Agus", "Marco", ""], ["Schneider", "Jens", ""]]}, {"id": "2009.02969", "submitter": "Kecheng Lu", "authors": "Kecheng Lu, Mi Feng, Xin Chen, Michael Sedlmair, Oliver Deussen, Dani\n  Lischinski, Zhanglin Cheng, Yunhai Wang", "title": "Palettailor: Discriminable Colorization for Categorical Data", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an integrated approach for creating and assigning color palettes\nto different visualizations such as multi-class scatterplots, line, and bar\ncharts. While other methods separate the creation of colors from their\nassignment, our approach takes data characteristics into account to produce\ncolor palettes, which are then assigned in a way that fosters better visual\ndiscrimination of classes. To do so, we use a customized optimization based on\nsimulated annealing to maximize the combination of three carefully designed\ncolor scoring functions: point distinctness, name difference, and color\ndiscrimination. We compare our approach to state-ofthe-art palettes with a\ncontrolled user study for scatterplots and line charts, furthermore we\nperformed a case study. Our results show that Palettailor, as a fully-automated\napproach, generates color palettes with a higher discrimination quality than\nexisting approaches. The efficiency of our optimization allows us also to\nincorporate user modifications into the color selection process.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 09:30:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lu", "Kecheng", ""], ["Feng", "Mi", ""], ["Chen", "Xin", ""], ["Sedlmair", "Michael", ""], ["Deussen", "Oliver", ""], ["Lischinski", "Dani", ""], ["Cheng", "Zhanglin", ""], ["Wang", "Yunhai", ""]]}, {"id": "2009.03044", "submitter": "Marco Fumero", "authors": "Marco Fumero, Michael Moeller, Emanuele Rodol\\`a", "title": "Nonlinear Spectral Geometry Processing via the TV Transform", "comments": "16 pages, 20 figures", "journal-ref": "ACM Trans. Graph.39, 6, Article 199 (December 2020)", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel computational framework for digital geometry processing,\nbased upon the derivation of a nonlinear operator associated to the total\nvariation functional. Such operator admits a generalized notion of spectral\ndecomposition, yielding a sparse multiscale representation akin to\nLaplacian-based methods, while at the same time avoiding undesirable\nover-smoothing effects typical of such techniques. Our approach entails\naccurate, detail-preserving decomposition and manipulation of 3D shape geometry\nwhile taking an especially intuitive form: non-local semantic details are well\nseparated into different bands, which can then be filtered and re-synthesized\nwith a straightforward linear step. Our computational framework is flexible,\ncan be applied to a variety of signals, and is easily adapted to different\ngeometry representations, including triangle meshes and point clouds. We\nshowcase our method throughout multiple applications in graphics, ranging from\nsurface and signal denoising to detail transfer and cubic stylization.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 12:05:19 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Fumero", "Marco", ""], ["Moeller", "Michael", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2009.03076", "submitter": "Stefan Zellmann", "authors": "Ingo Wald and Stefan Zellmann and Will Usher and Nate Morrical and\n  Ulrich Lang and Valerio Pascucci", "title": "Ray Tracing Structured AMR Data Using ExaBricks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to\nadapt the domain resolution to save computation and storage, and has become one\nof the dominant data representations used by scientific simulations; however,\nefficiently rendering such data remains a challenge. We present an efficient\napproach for volume- and iso-surface ray tracing of Structured AMR data on\nGPU-equipped workstations, using a combination of two different data\nstructures. Together, these data structures allow a ray tracing based renderer\nto quickly determine which segments along the ray need to be integrated and at\nwhat frequency, while also providing quick access to all data values required\nfor a smooth sample reconstruction kernel. Our method makes use of the RTX ray\ntracing hardware for surface rendering, ray marching, space skipping, and\nadaptive sampling; and allows for interactive changes to the transfer function\nand implicit iso-surfacing thresholds. We demonstrate that our method achieves\nhigh performance with little memory overhead, enabling interactive high quality\nrendering of complex AMR data sets on individual GPU workstations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 13:03:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wald", "Ingo", ""], ["Zellmann", "Stefan", ""], ["Usher", "Will", ""], ["Morrical", "Nate", ""], ["Lang", "Ulrich", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2009.03254", "submitter": "Will Usher", "authors": "Will Usher and Valerio Pascucci", "title": "Interactive Visualization of Terascale Data in the Browser: Fact or\n  Fiction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualization applications have become ubiquitous, in no small\npart thanks to the ease of wide distribution and deployment to users enabled by\nthe web browser. Scientific visualization applications, relying on native code\nlibraries and parallel processing, have been less suited to such widespread\ndistribution, as browsers do not provide the required libraries or compute\ncapabilities. In this paper, we revisit this gap in visualization technologies\nand explore how new web technologies, WebAssembly and WebGPU, can be used to\ndeploy powerful visualization solutions for large-scale scientific data in the\nbrowser. In particular, we evaluate the programming effort required to bring\nscientific visualization applications to the browser through these technologies\nand assess their competitiveness against classic native solutions. As a main\nexample, we present a new GPU-driven isosurface extraction method for\nblock-compressed data sets, that is suitable for interactive isosurface\ncomputation on large volumes in resource-constrained environments, such as the\nbrowser. We conclude that web browsers are on the verge of becoming a\ncompetitive platform for even the most demanding scientific visualization\ntasks, such as interactive visualization of isosurfaces from a 1TB DNS\nsimulation. We call on researchers and developers to consider investing in a\ncommunity software stack to ease use of these upcoming browser features to\nbring accessible scientific visualization to the browser.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:24:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Usher", "Will", ""], ["Pascucci", "Valerio", ""]]}, {"id": "2009.03259", "submitter": "Yumeng Xue", "authors": "Rongzheng Bian, Yumeng Xue, Liang Zhou, Jian Zhang, Baoquan Chen,\n  Daniel Weiskopf, Yunhai Wang", "title": "Implicit Multidimensional Projection of Local Subspaces", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3030368", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visualization method to understand the effect of\nmultidimensional projection on local subspaces, using implicit function\ndifferentiation. Here, we understand the local subspace as the multidimensional\nlocal neighborhood of data points. Existing methods focus on the projection of\nmultidimensional data points, and the neighborhood information is ignored. Our\nmethod is able to analyze the shape and directional information of the local\nsubspace to gain more insights into the global structure of the data through\nthe perception of local structures. Local subspaces are fitted by\nmultidimensional ellipses that are spanned by basis vectors. An accurate and\nefficient vector transformation method is proposed based on analytical\ndifferentiation of multidimensional projections formulated as implicit\nfunctions. The results are visualized as glyphs and analyzed using a full set\nof specifically-designed interactions supported in our efficient web-based\nvisualization tool. The usefulness of our method is demonstrated using various\nmulti- and high-dimensional benchmark datasets. Our implicit differentiation\nvector transformation is evaluated through numerical comparisons; the overall\nmethod is evaluated through exploration examples and use cases.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:27:27 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bian", "Rongzheng", ""], ["Xue", "Yumeng", ""], ["Zhou", "Liang", ""], ["Zhang", "Jian", ""], ["Chen", "Baoquan", ""], ["Weiskopf", "Daniel", ""], ["Wang", "Yunhai", ""]]}, {"id": "2009.03298", "submitter": "Kamal Gupta", "authors": "Kamal Gupta and Susmija Jabbireddy and Ketul Shah and Abhinav\n  Shrivastava and Matthias Zwicker", "title": "Improved Modeling of 3D Shapes with Multi-view Depth Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple yet effective general-purpose framework for modeling 3D\nshapes by leveraging recent advances in 2D image generation using CNNs. Using\njust a single depth image of the object, we can output a dense multi-view depth\nmap representation of 3D objects. Our simple encoder-decoder framework,\ncomprised of a novel identity encoder and class-conditional viewpoint\ngenerator, generates 3D consistent depth maps. Our experimental results\ndemonstrate the two-fold advantage of our approach. First, we can directly\nborrow architectures that work well in the 2D image domain to 3D. Second, we\ncan effectively generate high-resolution 3D shapes with low computational\nmemory. Our quantitative evaluations show that our method is superior to\nexisting depth map methods for reconstructing and synthesizing 3D objects and\nis competitive with other representations, such as point clouds, voxel grids,\nand implicit functions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 17:58:27 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Gupta", "Kamal", ""], ["Jabbireddy", "Susmija", ""], ["Shah", "Ketul", ""], ["Shrivastava", "Abhinav", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2009.03385", "submitter": "Christian Tominski", "authors": "Tom Horak, Philip Berger, Heidrun Schumann, Raimund Dachselt,\n  Christian Tominski", "title": "Responsive Matrix Cells: A Focus+Context Approach for Exploring and\n  Editing Multivariate Graphs", "comments": "Tom Horak and Philip Berger are joint first authors", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030371", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix visualizations are a useful tool to provide a general overview of a\ngraph's structure. For multivariate graphs, a remaining challenge is to cope\nwith the attributes that are associated with nodes and edges. Addressing this\nchallenge, we propose responsive matrix cells as a focus+context approach for\nembedding additional interactive views into a matrix. Responsive matrix cells\nare local zoomable regions of interest that provide auxiliary data exploration\nand editing facilities for multivariate graphs. They behave responsively by\nadapting their visual contents to the cell location, the available display\nspace, and the user task. Responsive matrix cells enable users to reveal\ndetails about the graph, compare node and edge attributes, and edit data values\ndirectly in a matrix without resorting to external views or tools. We report\nthe general design considerations for responsive matrix cells covering the\nvisual and interactive means necessary to support a seamless data exploration\nand editing. Responsive matrix cells have been implemented in a web-based\nprototype based on which we demonstrate the utility of our approach. We\ndescribe a walk-through for the use case of analyzing a graph of soccer players\nand report on insights from a preliminary user feedback session.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 19:30:01 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Horak", "Tom", ""], ["Berger", "Philip", ""], ["Schumann", "Heidrun", ""], ["Dachselt", "Raimund", ""], ["Tominski", "Christian", ""]]}, {"id": "2009.03434", "submitter": "Jerry  Van Aken", "authors": "Jerry R. Van Aken", "title": "A Fast Parametric Ellipse Algorithm", "comments": "32 pages, 7 figures, C++ source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a 2-D graphics algorithm that uses shifts and adds to\nprecisely plot a series of points on an ellipse of any shape and orientation.\nThe algorithm can also plot an elliptic arc that starts and ends at arbitrary\nangles. The ellipse algorithm described here is largely based on earlier papers\nby Van Aken and Simar [1,2], which extend Marvin Minsky's well-known circle\nalgorithm [3,4,5] to ellipses, and show how to cancel out the sources of error\nin Minsky's original algorithm. A new flatness test is presented for\nautomatically controlling the spacing between points plotted on an ellipse or\nelliptic arc. Most of the calculations performed by the ellipse algorithm and\nflatness test use fixed-point addition and shift operations, and thus are\nwell-suited to run on less-powerful processors. C++ source code listings are\nincluded.\n  Keywords: parametric ellipse algorithm, rotated ellipse, Minsky circle\nalgorithm, flatness, elliptic arc, conjugate diameters, affine invariance\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 21:41:44 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 06:40:00 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 05:07:17 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 21:25:45 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Van Aken", "Jerry R.", ""]]}, {"id": "2009.03667", "submitter": "Konstantinos Gavriil", "authors": "Konstantinos Gavriil, Ruslan Guseinov, Jes\\'us P\\'erez, Davide Pellis,\n  Paul Henderson, Florian Rist, Helmut Pottmann, Bernd Bickel", "title": "Computational Design of Cold Bent Glass Fa\\c{c}ades", "comments": null, "journal-ref": null, "doi": "10.1145/3414685.3417843", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cold bent glass is a promising and cost-efficient method for realizing doubly\ncurved glass fa\\c{c}ades. They are produced by attaching planar glass sheets to\ncurved frames and require keeping the occurring stress within safe limits.\nHowever, it is very challenging to navigate the design space of cold bent glass\npanels due to the fragility of the material, which impedes the form-finding for\npractically feasible and aesthetically pleasing cold bent glass fa\\c{c}ades. We\npropose an interactive, data-driven approach for designing cold bent glass\nfa\\c{c}ades that can be seamlessly integrated into a typical architectural\ndesign pipeline. Our method allows non-expert users to interactively edit a\nparametric surface while providing real-time feedback on the deformed shape and\nmaximum stress of cold bent glass panels. Designs are automatically refined to\nminimize several fairness criteria while maximal stresses are kept within glass\nlimits. We achieve interactive frame rates by using a differentiable Mixture\nDensity Network trained from more than a million simulations. Given a curved\nboundary, our regression model is capable of handling multistable\nconfigurations and accurately predicting the equilibrium shape of the panel and\nits corresponding maximal stress. We show predictions are highly accurate and\nvalidate our results with a physical realization of a cold bent glass surface.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:14:37 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gavriil", "Konstantinos", ""], ["Guseinov", "Ruslan", ""], ["P\u00e9rez", "Jes\u00fas", ""], ["Pellis", "Davide", ""], ["Henderson", "Paul", ""], ["Rist", "Florian", ""], ["Pottmann", "Helmut", ""], ["Bickel", "Bernd", ""]]}, {"id": "2009.03707", "submitter": "Varshini Subhash", "authors": "Varshini Subhash, Karran Pandey, Vijay Natarajan", "title": "GPU Parallel Computation of Morse-Smale Complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morse-Smale complex is a well studied topological structure that\nrepresents the gradient flow behavior of a scalar function. It supports\nmulti-scale topological analysis and visualization of large scientific data.\nIts computation poses significant algorithmic challenges when considering large\nscale data and increased feature complexity. Several parallel algorithms have\nbeen proposed towards the fast computation of the 3D Morse-Smale complex. The\nnon-trivial structure of the saddle-saddle connections are not amenable to\nparallel computation. This paper describes a fine grained parallel method for\ncomputing the Morse-Smale complex that is implemented on a GPU. The\nsaddle-saddle reachability is first determined via a transformation into a\nsequence of vector operations followed by the path traversal, which is achieved\nvia a sequence of matrix operations. Computational experiments show that the\nmethod achieves up to 7x speedup over current shared memory implementations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 12:52:43 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 06:17:35 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Subhash", "Varshini", ""], ["Pandey", "Karran", ""], ["Natarajan", "Vijay", ""]]}, {"id": "2009.03784", "submitter": "Wenchao Li", "authors": "Wenchao Li, Yun Wang, Haidong Zhang, Huamin Qu", "title": "Improving Engagement of Animated Visualization with Visual Foreshadowing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animated visualization is becoming increasingly popular as a compelling way\nto illustrate changes in time series data. However, maintaining the viewer's\nfocus throughout the entire animation is difficult because of its\ntime-consuming nature. Viewers are likely to become bored and distracted during\nthe ever-changing animated visualization. Informed by the role of foreshadowing\nthat builds the expectation in film and literature, we introduce visual\nforeshadowing to improve the engagement of animated visualizations. In\nspecific, we propose designs of visual foreshadowing that engage the audience\nwhile watching the animation. To demonstrate our approach, we built a\nproof-of-concept animated visualization authoring tool that incorporates visual\nforeshadowing techniques with various styles. Our user study indicates the\neffectiveness of our foreshadowing techniques on improving engagement for\nanimated visualization.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 14:25:16 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Li", "Wenchao", ""], ["Wang", "Yun", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""]]}, {"id": "2009.04177", "submitter": "Ke Zhang", "authors": "Ke Zhang, Yukun Su, Xiwang Guo, Liang Qi, and Zhenbing Zhao", "title": "MU-GAN: Facial Attribute Editing based on Multi-attention Mechanism", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attribute editing has mainly two objectives: 1) translating image from\na source domain to a target one, and 2) only changing the facial regions\nrelated to a target attribute and preserving the attribute-excluding details.\nIn this work, we propose a Multi-attention U-Net-based Generative Adversarial\nNetwork (MU-GAN). First, we replace a classic convolutional encoder-decoder\nwith a symmetric U-Net-like structure in a generator, and then apply an\nadditive attention mechanism to build attention-based U-Net connections for\nadaptively transferring encoder representations to complement a decoder with\nattribute-excluding detail and enhance attribute editing ability. Second, a\nself-attention mechanism is incorporated into convolutional layers for modeling\nlong-range and multi-level dependencies across image regions. experimental\nresults indicate that our method is capable of balancing attribute editing\nability and details preservation ability, and can decouple the correlation\namong attributes. It outperforms the state-of-the-art methods in terms of\nattribute manipulation accuracy and image quality.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 09:25:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Zhang", "Ke", ""], ["Su", "Yukun", ""], ["Guo", "Xiwang", ""], ["Qi", "Liang", ""], ["Zhao", "Zhenbing", ""]]}, {"id": "2009.04294", "submitter": "Marco Livesu", "authors": "Marco Livesu, Gianmarco Cherchi, Riccardo Scateni, Marco Attene", "title": "Deterministic Linear Time Constrained Triangulation using Simplified\n  Earcut", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangulation algorithms that conform to a set of non-intersecting input\nsegments typically proceed in an incremental fashion, by inserting points\nfirst, and then segments. Inserting a segment amounts to: (1) deleting all the\ntriangles it intersects; (2) filling the so generated hole with two polygons\nthat have the wanted segment as shared edge; (3) triangulate each polygon\nseparately. In this paper we prove that these polygons are such that all their\nconvex vertices but two can be used to form triangles in an earcut fashion,\nwithout the need to check whether other polygon points are located within each\near. The fact that any simple polygon contains at least three convex vertices\nguarantees the existence of a valid ear to cut, ensuring convergence. Not only\nthis translates to an optimal deterministic linear time triangulation\nalgorithm, but such algorithm is also trivial to implement. We formally prove\nthe correctness of our approach, also validating it in practical applications\nand comparing it with prior art.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 13:37:00 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 14:03:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Livesu", "Marco", ""], ["Cherchi", "Gianmarco", ""], ["Scateni", "Riccardo", ""], ["Attene", "Marco", ""]]}, {"id": "2009.04531", "submitter": "Tobias Rapp", "authors": "Tobias Rapp, Carsten Dachsbacher", "title": "Uncertain Transport in Unsteady Flows", "comments": "Accepted to IEEE VIS 2020 short papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study uncertainty in the dynamics of time-dependent flows by identifying\nbarriers and enhancers to stochastic transport. This topological segmentation\nis closely related to the theory of Lagrangian coherent structures and is based\non a recently introduced quantity, the diffusion barrier strength (DBS). The\nDBS is defined similar to the finite-time Lyapunov exponent (FTLE), but\nincorporates diffusion during flow integration. Height ridges of the DBS\nindicate stochastic transport barriers and enhancers, i.e. material surfaces\nthat are minimally or maximally diffusive. To apply these concepts to\nreal-world data, we represent uncertainty in a flow by a stochastic\ndifferential equation that consists of a deterministic and a stochastic\ncomponent modeled by a Gaussian. With this formulation we identify barriers and\nenhancers to stochastic transport, without performing expensive Monte Carlo\nsimulation and with a computational complexity comparable to FTLE. In addition,\nwe propose a complementary visualization to convey the absolute scale of\nuncertainties in the Lagrangian frame of reference. This enables us to study\nuncertainty in real-world datasets, for example due to small deviations, data\nreduction, or estimated from multiple ensemble runs.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 07:08:26 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Rapp", "Tobias", ""], ["Dachsbacher", "Carsten", ""]]}, {"id": "2009.04592", "submitter": "Dan Casas", "authors": "Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas", "title": "Fully Convolutional Graph Neural Networks for Parametric Virtual Try-On", "comments": "Project website\n  http://mslab.es/projects/FullyConvolutionalGraphVirtualTryOn . Accepted to\n  ACM SIGGRAPH / Eurographics Symposium on Computer Animation, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based approach for virtual try-on applications based on\na fully convolutional graph neural network. In contrast to existing data-driven\nmodels, which are trained for a specific garment or mesh topology, our fully\nconvolutional model can cope with a large family of garments, represented as\nparametric predefined 2D panels with arbitrary mesh topology, including long\ndresses, shirts, and tight tops. Under the hood, our novel geometric deep\nlearning approach learns to drape 3D garments by decoupling the three different\nsources of deformations that condition the fit of clothing: garment type,\ntarget body shape, and material. Specifically, we first learn a regressor that\npredicts the 3D drape of the input parametric garment when worn by a mean body\nshape. Then, after a mesh topology optimization step where we generate a\nsufficient level of detail for the input garment type, we further deform the\nmesh to reproduce deformations caused by the target body shape. Finally, we\npredict fine-scale details such as wrinkles that depend mostly on the garment\nmaterial. We qualitatively and quantitatively demonstrate that our fully\nconvolutional approach outperforms existing methods in terms of generalization\ncapabilities and memory requirements, and therefore it opens the door to more\ngeneral learning-based models for virtual try-on applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 22:38:03 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Vidaurre", "Raquel", ""], ["Santesteban", "Igor", ""], ["Garces", "Elena", ""], ["Casas", "Dan", ""]]}, {"id": "2009.04601", "submitter": "Botong Qu", "authors": "Botong Qu, Lawrence Roy, Yue Zhang, and Eugene Zhang", "title": "Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and\n  Seamless Extraction", "comments": "15 pages, 13 figures, to be published in VIS2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode surfaces are the generalization of degenerate curves and neutral\nsurfaces, which constitute 3D symmetric tensor field topology. Efficient\nanalysis and visualization of mode surfaces can provide additional insight into\nnot only degenerate curves and neutral surfaces, but also how these features\ntransition into each other. Moreover, the geometry and topology of mode\nsurfaces can help domain scientists better understand the tensor fields in\ntheir applications. Existing mode surface extraction methods can miss features\nin the surfaces. Moreover, the mode surfaces extracted from neighboring cells\nhave gaps, which make their subsequent analysis difficult. In this paper, we\nprovide novel analysis on the topological structures of mode surfaces,\nincluding a common parameterization of all mode surfaces of a tensor field\nusing 2D asymmetric tensors. This allows us to not only better understand the\nstructures in mode surfaces and their interactions with degenerate curves and\nneutral surfaces, but also develop an efficient algorithm to seamlessly extract\nmode surfaces, including neutral surfaces. The seamless mode surfaces enable\nefficient analysis of their geometric structures, such as the principal\ncurvature directions. We apply our analysis and visualization to a number of\nsolid mechanics data sets.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 23:40:31 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Qu", "Botong", ""], ["Roy", "Lawrence", ""], ["Zhang", "Yue", ""], ["Zhang", "Eugene", ""]]}, {"id": "2009.04792", "submitter": "Mengdie Zhuang", "authors": "Mengdie Zhuang, Dave Concannon, and Ed Manley", "title": "A Framework for Evaluating Dashboards in Healthcare", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of \"information overload\", effective information provision is\nessential for enabling rapid response and critical decision making. In making\nsense of diverse information sources, data dashboards have become an\nindispensable tool, providing fast, effective, adaptable, and personalized\naccess to information for professionals and the general public alike. However,\nthese objectives place a heavy requirement on dashboards as information\nsystems, resulting in poor usability and ineffective design. Understanding\nthese shortfalls is a challenge given the absence of a consistent and\ncomprehensive approach to dashboard evaluation. In this paper we systematically\nreview literature on dashboard implementation in the healthcare domain, a field\nwhere dashboards have been employed widely, and in which there is widespread\ninterest for improving the current state of the art, and subsequently analyse\napproaches taken towards evaluation. We draw upon consolidated dashboard\nliterature and our own observations to introduce a general definition of\ndashboards which is more relevant to current trends, together with a dashboard\ntask-based classification, which underpin our subsequent analysis. From a total\nof 81 papers we derive seven evaluation scenarios - task performance, behaviour\nchange, interaction workflow, perceived engagement, potential utility,\nalgorithm performance and system implementation. These scenarios distinguish\ndifferent evaluation purposes which we illustrate through measurements, example\nstudies, and common challenges in evaluation study design. We provide a\nbreakdown of each evaluation scenario, and highlight some of the subtle and\nless well posed questions. We conclude by outlining a number of active\ndiscussion points and a set of dashboard evaluation best practices for the\nacademic, clinical and software development communities alike.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 11:50:29 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Zhuang", "Mengdie", ""], ["Concannon", "Dave", ""], ["Manley", "Ed", ""]]}, {"id": "2009.04927", "submitter": "Changjian Li", "authors": "Changjian Li, Hao Pan, Adrien Bousseau and Niloy J. Mitra", "title": "Sketch2CAD: Sequential CAD Modeling by Sketching in Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sketch-based CAD modeling system, where users create objects\nincrementally by sketching the desired shape edits, which our system\nautomatically translates to CAD operations. Our approach is motivated by the\nclose similarities between the steps industrial designers follow to draw 3D\nshapes, and the operations CAD modeling systems offer to create similar shapes.\nTo overcome the strong ambiguity with parsing 2D sketches, we observe that in a\nsketching sequence, each step makes sense and can be interpreted in the\n\\emph{context} of what has been drawn before. In our system, this context\ncorresponds to a partial CAD model, inferred in the previous steps, which we\nfeed along with the input sketch to a deep neural network in charge of\ninterpreting how the model should be modified by that sketch. Our deep network\narchitecture then recognizes the intended CAD operation and segments the sketch\naccordingly, such that a subsequent optimization estimates the parameters of\nthe operation that best fit the segmented sketch strokes. Since there exists no\ndatasets of paired sketching and CAD modeling sequences, we train our system by\ngenerating synthetic sequences of CAD operations that we render as line\ndrawings. We present a proof of concept realization of our algorithm supporting\nfour frequently used CAD operations. Using our system, participants are able to\nquickly model a large and diverse set of objects, demonstrating Sketch2CAD to\nbe an alternate way of interacting with current CAD modeling systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:11:56 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Li", "Changjian", ""], ["Pan", "Hao", ""], ["Bousseau", "Adrien", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "2009.05284", "submitter": "Jianan Li", "authors": "Jianan Li, Jimei Yang, Jianming Zhang, Chang Liu, Christina Wang,\n  Tingfa Xu", "title": "Attribute-conditioned Layout GAN for Automatic Graphic Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling layout is an important first step for graphic design. Recently,\nmethods for generating graphic layouts have progressed, particularly with\nGenerative Adversarial Networks (GANs). However, the problem of specifying the\nlocations and sizes of design elements usually involves constraints with\nrespect to element attributes, such as area, aspect ratio and reading-order.\nAutomating attribute conditional graphic layouts remains a complex and unsolved\nproblem. In this paper, we introduce Attribute-conditioned Layout GAN to\nincorporate the attributes of design elements for graphic layout generation by\nforcing both the generator and the discriminator to meet attribute conditions.\nDue to the complexity of graphic designs, we further propose an element dropout\nmethod to make the discriminator look at partial lists of elements and learn\ntheir local patterns. In addition, we introduce various loss designs following\ndifferent design principles for layout optimization. We demonstrate that the\nproposed method can synthesize graphic layouts conditioned on different element\nattributes. It can also adjust well-designed layouts to new sizes while\nretaining elements' original reading-orders. The effectiveness of our method is\nvalidated through a user study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 08:34:17 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Li", "Jianan", ""], ["Yang", "Jimei", ""], ["Zhang", "Jianming", ""], ["Liu", "Chang", ""], ["Wang", "Christina", ""], ["Xu", "Tingfa", ""]]}, {"id": "2009.06184", "submitter": "Yifan Wang", "authors": "Yifan Wang, Guoli Yan, Haikuan Zhu, Sagar Buch, Ying Wang, Ewart Mark\n  Haacke, Jing Hua, and Zichun Zhong", "title": "VC-Net: Deep Volume-Composition Networks for Segmentation and\n  Visualization of Highly Sparse and Noisy Image Data", "comments": "15 pages, 10 figures, proceeding to IEEE Transactions on\n  Visualization and Computer Graphics (TVCG) (IEEE SciVis 2020), October, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of our work is to present a new visualization-guided computing\nparadigm to combine direct 3D volume processing and volume rendered clues for\neffective 3D exploration such as extracting and visualizing microstructures\nin-vivo. However, it is still challenging to extract and visualize high\nfidelity 3D vessel structure due to its high sparseness, noisiness, and complex\ntopology variations. In this paper, we present an end-to-end deep learning\nmethod, VC-Net, for robust extraction of 3D microvasculature through embedding\nthe image composition, generated by maximum intensity projection (MIP), into 3D\nvolume image learning to enhance the performance. The core novelty is to\nautomatically leverage the volume visualization technique (MIP) to enhance the\n3D data exploration at deep learning level. The MIP embedding features can\nenhance the local vessel signal and are adaptive to the geometric variability\nand scalability of vessels, which is crucial in microvascular tracking. A\nmulti-stream convolutional neural network is proposed to learn the 3D volume\nand 2D MIP features respectively and then explore their inter-dependencies in a\njoint volume-composition embedding space by unprojecting the MIP features into\n3D volume embedding space. The proposed framework can better capture small /\nmicro vessels and improve vessel connectivity. To our knowledge, this is the\nfirst deep learning framework to construct a joint convolutional embedding\nspace, where the computed vessel probabilities from volume rendering based 2D\nprojection and 3D volume can be explored and integrated synergistically.\nExperimental results are compared with the traditional 3D vessel segmentation\nmethods and the deep learning state-of-the-art on public and real patient\n(micro-)cerebrovascular image datasets. Our method demonstrates the potential\nin a powerful MR arteriogram and venogram diagnosis of vascular diseases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 04:15:02 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wang", "Yifan", ""], ["Yan", "Guoli", ""], ["Zhu", "Haikuan", ""], ["Buch", "Sagar", ""], ["Wang", "Ying", ""], ["Haacke", "Ewart Mark", ""], ["Hua", "Jing", ""], ["Zhong", "Zichun", ""]]}, {"id": "2009.06295", "submitter": "Hassan Ahmed Sial", "authors": "Hassan Sial, Ramon Baldrich, Maria Vanrell", "title": "Deep intrinsic decomposition trained on surreal scenes yet with\n  realistic light effects", "comments": null, "journal-ref": "JOSA A 2020", "doi": "10.1364/JOSAA.37.000001", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of intrinsic images still remains a challenging task due to\nweaknesses of ground-truth datasets, which either are too small or present\nnon-realistic issues. On the other hand, end-to-end deep learning architectures\nstart to achieve interesting results that we believe could be improved if\nimportant physical hints were not ignored. In this work, we present a twofold\nframework: (a) a flexible generation of images overcoming some classical\ndataset problems such as larger size jointly with coherent lighting appearance;\nand (b) a flexible architecture tying physical properties through intrinsic\nlosses. Our proposal is versatile, presents low computation time, and achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 09:45:49 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sial", "Hassan", ""], ["Baldrich", "Ramon", ""], ["Vanrell", "Maria", ""]]}, {"id": "2009.06309", "submitter": "Liang Zhou", "authors": "Liang Zhou, Chris R. Johnson, and Daniel Weiskopf", "title": "Data-Driven Space-Filling Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven space-filling curve method for 2D and 3D\nvisualization. Our flexible curve traverses the data elements in the spatial\ndomain in a way that the resulting linearization better preserves features in\nspace compared to existing methods. We achieve such data coherency by\ncalculating a Hamiltonian path that approximately minimizes an objective\nfunction that describes the similarity of data values and location coherency in\na neighborhood. Our extended variant even supports multiscale data via\nquadtrees and octrees. Our method is useful in many areas of visualization,\nincluding multivariate or comparative visualization, ensemble visualization of\n2D and 3D data on regular grids, or multiscale visual analysis of particle\nsimulations. The effectiveness of our method is evaluated with numerical\ncomparisons to existing techniques and through examples of ensemble and\nmultivariate datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 10:21:33 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Liang", ""], ["Johnson", "Chris R.", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2009.06574", "submitter": "Junpeng Wang", "authors": "Christoph Neuhauser, Junpeng Wang, and R\\\"udiger Westermann", "title": "Interactive Focus+Context Rendering for Hexahedral Mesh Inspection", "comments": "under review at IEEE Transactions on Visualization and Computer\n  Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2021.3074607", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual inspection of a hexahedral mesh with respect to element quality is\ndifficult due to clutter and occlusions that are produced when rendering all\nelement faces or their edges simultaneously. Current approaches overcome this\nproblem by using focus on specific elements that are then rendered opaque, and\ncarving away all elements occluding their view. In this work, we make use of\nadvanced GPU shader functionality to generate a focus+context rendering that\nhighlights the elements in a selected region and simultaneously conveys the\nglobal mesh structure in the surrounding. To achieve this, we propose a gradual\ntransition from edge-based focus rendering to volumetric context rendering, by\ncombining fragment shader-based edge and face rendering with per-pixel fragment\nlists. A fragment shader smoothly transitions between wireframe and face-based\nrendering, including focus-dependent rendering style and depth-dependent edge\nthickness and halos, and per-pixel fragment lists are used to blend fragments\nin correct visibility order. To maintain the global mesh structure in the\ncontext regions, we propose a new method to construct a sheet-based\nlevel-of-detail hierarchy and smoothly blend it with volumetric information.\nThe user guides the exploration process by moving a lens-like hotspot. Since\nall operations are performed on the GPU, interactive frame rates are achieved\neven for large meshes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:05:33 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Neuhauser", "Christoph", ""], ["Wang", "Junpeng", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2009.06944", "submitter": "Jan Bender", "authors": "Dan Koschier, Jan Bender, Barbara Solenthaler, Matthias Teschner", "title": "Smoothed Particle Hydrodynamics Techniques for the Physics Based\n  Simulation of Fluids and Solids", "comments": "Eurographics Tutorial, 2019", "journal-ref": null, "doi": "10.2312/egt.20191035", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphics research on Smoothed Particle Hydrodynamics (SPH) has produced\nfantastic visual results that are unique across the board of research\ncommunities concerned with SPH simulations. Generally, the SPH formalism serves\nas a spatial discretization technique, commonly used for the numerical\nsimulation of continuum mechanical problems such as the simulation of fluids,\nhighly viscous materials, and deformable solids. Recent advances in the field\nhave made it possible to efficiently simulate massive scenes with highly\ncomplex boundary geometries on a single PC [Com16b, Com16a]. Moreover, novel\ntechniques allow to robustly handle interactions among various materials\n[Com18,Com17]. As of today, graphics-inspired pressure solvers, neighborhood\nsearch algorithms, boundary formulations, and other contributions often serve\nas core components in commercial software for animation purposes [Nex17] as\nwell as in computer-aided engineering software [FIF16].\n  This tutorial covers various aspects of SPH simulations. Governing equations\nfor mechanical phenomena and their SPH discretizations are discussed. Concepts\nand implementations of core components such as neighborhood search algorithms,\npressure solvers, and boundary handling techniques are presented.\nImplementation hints for the realization of SPH solvers for fluids, elastic\nsolids, and rigid bodies are given. The tutorial combines the introduction of\ntheoretical concepts with the presentation of actual implementations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 09:29:48 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Koschier", "Dan", ""], ["Bender", "Jan", ""], ["Solenthaler", "Barbara", ""], ["Teschner", "Matthias", ""]]}, {"id": "2009.07047", "submitter": "Ziyu Wan", "authors": "Ziyu Wan, Bo Zhang, Dongdong Chen, Pan Zhang, Dong Chen, Jing Liao,\n  Fang Wen", "title": "Old Photo Restoration via Deep Latent Space Translation", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:2004.09484", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to restore old photos that suffer from severe degradation through\na deep learning approach. Unlike conventional restoration tasks that can be\nsolved through supervised learning, the degradation in real photos is complex\nand the domain gap between synthetic images and real old photos makes the\nnetwork fail to generalize. Therefore, we propose a novel triplet domain\ntranslation network by leveraging real photos along with massive synthetic\nimage pairs. Specifically, we train two variational autoencoders (VAEs) to\nrespectively transform old photos and clean photos into two latent spaces. And\nthe translation between these two latent spaces is learned with synthetic\npaired data. This translation generalizes well to real photos because the\ndomain gap is closed in the compact latent space. Besides, to address multiple\ndegradations mixed in one old photo, we design a global branch with apartial\nnonlocal block targeting to the structured defects, such as scratches and dust\nspots, and a local branch targeting to the unstructured defects, such as noises\nand blurriness. Two branches are fused in the latent space, leading to improved\ncapability to restore old photos from multiple defects. Furthermore, we apply\nanother face refinement network to recover fine details of faces in the old\nphotos, thus ultimately generating photos with enhanced perceptual quality.\nWith comprehensive experiments, the proposed pipeline demonstrates superior\nperformance over state-of-the-art methods as well as existing commercial tools\nin terms of visual quality for old photos restoration.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 08:51:53 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Wan", "Ziyu", ""], ["Zhang", "Bo", ""], ["Chen", "Dongdong", ""], ["Zhang", "Pan", ""], ["Chen", "Dong", ""], ["Liao", "Jing", ""], ["Wen", "Fang", ""]]}, {"id": "2009.07378", "submitter": "Tomas Hodan", "authors": "Tomas Hodan, Martin Sundermeyer, Bertram Drost, Yann Labbe, Eric\n  Brachmann, Frank Michel, Carsten Rother, Jiri Matas", "title": "BOP Challenge 2020 on 6D Object Localization", "comments": "In ECCV 2020 Workshops Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the evaluation methodology, datasets, and results of the\nBOP Challenge 2020, the third in a series of public competitions organized with\nthe goal to capture the status quo in the field of 6D object pose estimation\nfrom an RGB-D image. In 2020, to reduce the domain gap between synthetic\ntraining and real test RGB images, the participants were provided 350K\nphotorealistic training images generated by BlenderProc4BOP, a new open-source\nand light-weight physically-based renderer (PBR) and procedural data generator.\nMethods based on deep neural networks have finally caught up with methods based\non point pair features, which were dominating previous editions of the\nchallenge. Although the top-performing methods rely on RGB-D image channels,\nstrong results were achieved when only RGB channels were used at both training\nand test time - out of the 26 evaluated methods, the third method was trained\non RGB channels of PBR and real images, while the fifth on RGB channels of PBR\nimages only. Strong data augmentation was identified as a key component of the\ntop-performing CosyPose method, and the photorealism of PBR images was\ndemonstrated effective despite the augmentation. The online evaluation system\nstays open and is available on the project website: bop.felk.cvut.cz.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 22:35:14 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 12:09:44 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hodan", "Tomas", ""], ["Sundermeyer", "Martin", ""], ["Drost", "Bertram", ""], ["Labbe", "Yann", ""], ["Brachmann", "Eric", ""], ["Michel", "Frank", ""], ["Rother", "Carsten", ""], ["Matas", "Jiri", ""]]}, {"id": "2009.07647", "submitter": "Dan Reznik", "authors": "Dan Reznik and Ronaldo Garcia", "title": "Related by Similarity II: Poncelet 3-Periodics in the Homothetic Pair\n  and the Brocard Porism", "comments": "13 pages, 5 figures, 3 tables, 5 videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previously we showed the family of 3-periodics in the elliptic billiard\n(confocal pair) is the image under a variable similarity transform of poristic\ntriangles (those with non-concentric, fixed incircle and circumcircle). Both\nfamilies conserve the ratio of inradius to circumradius and therefore also the\nsum of cosines. This is consisten with the fact that a similarity preserves\nangles. Here we study two new Poncelet 3-periodic families also tied to each\nother via a variable similarity: (i) a first one interscribed in a pair of\nconcentric, homothetic ellipses, and (ii) a second non-concentric one known as\nthe Brocard porism: fixed circumcircle and Brocard inellipse. The Brocard\npoints of this family are stationary at the foci of the inellipse. A key common\ninvariant is the Brocard angle, and therefore the sum of cotangents. This\nraises an interesting question: given a non-concentric Poncelet family (limited\nor not to the outer conic being a circle), can a similar doppelg\\\"anger always\nbe found interscribed in a concentric, axis-aligned ellipse and/or conic pair?\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:48:20 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Reznik", "Dan", ""], ["Garcia", "Ronaldo", ""]]}, {"id": "2009.07833", "submitter": "Erika Lu", "authors": "Erika Lu, Forrester Cole, Tali Dekel, Weidi Xie, Andrew Zisserman,\n  David Salesin, William T. Freeman, Michael Rubinstein", "title": "Layered Neural Rendering for Retiming People in Video", "comments": "To appear in SIGGRAPH Asia 2020. Project webpage:\n  https://retiming.github.io/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for retiming people in an ordinary, natural\nvideo---manipulating and editing the time in which different motions of\nindividuals in the video occur. We can temporally align different motions,\nchange the speed of certain actions (speeding up/slowing down, or entirely\n\"freezing\" people), or \"erase\" selected people from the video altogether. We\nachieve these effects computationally via a dedicated learning-based layered\nvideo representation, where each frame in the video is decomposed into separate\nRGBA layers, representing the appearance of different people in the video. A\nkey property of our model is that it not only disentangles the direct motions\nof each person in the input video, but also correlates each person\nautomatically with the scene changes they generate---e.g., shadows,\nreflections, and motion of loose clothing. The layers can be individually\nretimed and recombined into a new video, allowing us to achieve realistic,\nhigh-quality renderings of retiming effects for real-world videos depicting\ncomplex actions and involving multiple individuals, including dancing,\ntrampoline jumping, or group running.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 17:48:26 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lu", "Erika", ""], ["Cole", "Forrester", ""], ["Dekel", "Tali", ""], ["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""], ["Salesin", "David", ""], ["Freeman", "William T.", ""], ["Rubinstein", "Michael", ""]]}, {"id": "2009.08026", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones, Theresa Barton, Xianghao Xu, Kai Wang, Ellen Jiang,\n  Paul Guerrero, Niloy J. Mitra, and Daniel Ritchie", "title": "ShapeAssembly: Learning to Generate Programs for 3D Shape Structure\n  Synthesis", "comments": "Accepted to Siggraph Asia 2020; project page:\n  https://rkjones4.github.io/shapeAssembly.html", "journal-ref": null, "doi": "10.1145/3414685.3417812", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually authoring 3D shapes is difficult and time consuming; generative\nmodels of 3D shapes offer compelling alternatives. Procedural representations\nare one such possibility: they offer high-quality and editable results but are\ndifficult to author and often produce outputs with limited diversity. On the\nother extreme are deep generative models: given enough data, they can learn to\ngenerate any class of shape but their outputs have artifacts and the\nrepresentation is not editable. In this paper, we take a step towards achieving\nthe best of both worlds for novel 3D shape synthesis. We propose ShapeAssembly,\na domain-specific \"assembly-language\" for 3D shape structures. ShapeAssembly\nprograms construct shapes by declaring cuboid part proxies and attaching them\nto one another, in a hierarchical and symmetrical fashion. Its functions are\nparameterized with free variables, so that one program structure is able to\ncapture a family of related shapes. We show how to extract ShapeAssembly\nprograms from existing shape structures in the PartNet dataset. Then we train a\ndeep generative model, a hierarchical sequence VAE, that learns to write novel\nShapeAssembly programs. The program captures the subset of variability that is\ninterpretable and editable. The deep model captures correlations across shape\ncollections that are hard to express procedurally. We evaluate our approach by\ncomparing shapes output by our generated programs to those from other recent\nshape structure synthesis models. We find that our generated shapes are more\nplausible and physically-valid than those of other methods. Additionally, we\nassess the latent spaces of these models, and find that ours is better\nstructured and produces smoother interpolations. As an application, we use our\ngenerative model and differentiable program interpreter to infer and fit shape\nprograms to unstructured geometry, such as point clouds.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 02:26:45 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Jones", "R. Kenny", ""], ["Barton", "Theresa", ""], ["Xu", "Xianghao", ""], ["Wang", "Kai", ""], ["Jiang", "Ellen", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2009.08279", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi", "title": "Efficient conformal parameterization of multiply-connected surfaces\n  using quasi-conformal theory", "comments": null, "journal-ref": "Journal of Scientific Computing, 87(3), 70 (2021)", "doi": "10.1007/s10915-021-01479-y", "report-no": null, "categories": "cs.GR cs.CG math.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal mapping, a classical topic in complex analysis and differential\ngeometry, has become a subject of great interest in the area of surface\nparameterization in recent decades with various applications in science and\nengineering. However, most of the existing conformal parameterization\nalgorithms only focus on simply-connected surfaces and cannot be directly\napplied to surfaces with holes. In this work, we propose two novel algorithms\nfor computing the conformal parameterization of multiply-connected surfaces. We\nfirst develop an efficient method for conformally parameterizing an open\nsurface with one hole to an annulus on the plane. Based on this method, we then\ndevelop an efficient method for conformally parameterizing an open surface with\n$k$ holes onto a unit disk with $k$ circular holes. The conformality and\nbijectivity of the mappings are ensured by quasi-conformal theory. Numerical\nexperiments and applications are presented to demonstrate the effectiveness of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 03:38:23 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 13:43:24 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Choi", "Gary P. T.", ""]]}, {"id": "2009.08692", "submitter": "Satoshi Iizuka", "authors": "Satoshi Iizuka and Edgar Simo-Serra", "title": "DeepRemaster: Temporal Source-Reference Attention Networks for\n  Comprehensive Video Enhancement", "comments": "Accepted to SIGGRAPH Asia 2019. Project page:\n  http://iizuka.cs.tsukuba.ac.jp/projects/remastering/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remastering of vintage film comprises of a diversity of sub-tasks\nincluding super-resolution, noise removal, and contrast enhancement which aim\nto restore the deteriorated film medium to its original state. Additionally,\ndue to the technical limitations of the time, most vintage film is either\nrecorded in black and white, or has low quality colors, for which colorization\nbecomes necessary. In this work, we propose a single framework to tackle the\nentire remastering task semi-interactively. Our work is based on temporal\nconvolutional neural networks with attention mechanisms trained on videos with\ndata-driven deterioration simulation. Our proposed source-reference attention\nallows the model to handle an arbitrary number of reference color images to\ncolorize long videos without the need for segmentation while maintaining\ntemporal consistency. Quantitative analysis shows that our framework\noutperforms existing approaches, and that, in contrast to existing approaches,\nthe performance of our framework increases with longer videos and more\nreference color images.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 08:55:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Iizuka", "Satoshi", ""], ["Simo-Serra", "Edgar", ""]]}, {"id": "2009.08941", "submitter": "Hassan Ahmed Sial", "authors": "Hassan A. Sial, Ramon Baldrich, Maria Vanrell, Dimitris Samaras", "title": "Light Direction and Color Estimation from Single Image with Deep\n  Regression", "comments": "Conference: London Imaging Meeting 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to estimate the direction and color of the scene light\nsource from a single image. Our method is based on two main ideas: (a) we use a\nnew synthetic dataset with strong shadow effects with similar constraints to\nthe SID dataset; (b) we define a deep architecture trained on the mentioned\ndataset to estimate the direction and color of the scene light source. Apart\nfrom showing good performance on synthetic images, we additionally propose a\npreliminary procedure to obtain light positions of the Multi-Illumination\ndataset, and, in this way, we also prove that our trained model achieves good\nperformance when it is applied to real scenes.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 17:33:49 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Sial", "Hassan A.", ""], ["Baldrich", "Ramon", ""], ["Vanrell", "Maria", ""], ["Samaras", "Dimitris", ""]]}, {"id": "2009.09029", "submitter": "Rahul Arora", "authors": "Rahul Arora and Karan Singh", "title": "Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality", "comments": "Accepted to ACM Transactions on Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex 3D curves can be created by directly drawing mid-air in immersive\nenvironments (Augmented and Virtual Realities). Drawing mid-air strokes\nprecisely on the surface of a 3D virtual object, however, is difficult;\nnecessitating a projection of the mid-air stroke onto the user \"intended\"\nsurface curve. We present the first detailed investigation of the fundamental\nproblem of 3D stroke projection in VR. An assessment of the design requirements\nof real-time drawing of curves on 3D objects in VR is followed by the\ndefinition and classification of multiple techniques for 3D stroke projection.\nWe analyze the advantages and shortcomings of these approaches both\ntheoretically and via practical pilot testing. We then formally evaluate the\ntwo most promising techniques spraycan and mimicry with 20 users in VR. The\nstudy shows a strong qualitative and quantitative user preference for our novel\nstroke mimicry projection algorithm. We further illustrate the effectiveness\nand utility of stroke mimicry, to draw complex 3D curves on surfaces for\nvarious artistic and functional design applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 19:01:08 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:42:07 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Arora", "Rahul", ""], ["Singh", "Karan", ""]]}, {"id": "2009.09144", "submitter": "Bojian Wu", "authors": "Jiahui Lyu, Bojian Wu, Dani Lischinski, Daniel Cohen-Or, Hui Huang", "title": "Differentiable Refraction-Tracing for Mesh Reconstruction of Transparent\n  Objects", "comments": "13 pages, 21 figures", "journal-ref": "ACM Trans. on Graphics (Proc. of SIGGRAPH Asia 2020)", "doi": "10.1145/3414685.3417815", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the 3D geometry of transparent objects is a challenging task,\nill-suited for general-purpose scanning and reconstruction techniques, since\nthese cannot handle specular light transport phenomena. Existing\nstate-of-the-art methods, designed specifically for this task, either involve a\ncomplex setup to reconstruct complete refractive ray paths, or leverage a\ndata-driven approach based on synthetic training data. In either case, the\nreconstructed 3D models suffer from over-smoothing and loss of fine detail.\nThis paper introduces a novel, high precision, 3D acquisition and\nreconstruction method for solid transparent objects. Using a static background\nwith a coded pattern, we establish a mapping between the camera view rays and\nlocations on the background. Differentiable tracing of refractive ray paths is\nthen used to directly optimize a 3D mesh approximation of the object, while\nsimultaneously ensuring silhouette consistency and smoothness. Extensive\nexperiments and comparisons demonstrate the superior accuracy of our method.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 02:29:00 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Lyu", "Jiahui", ""], ["Wu", "Bojian", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""], ["Huang", "Hui", ""]]}, {"id": "2009.09302", "submitter": "Jonghyun Kim", "authors": "Suyeon Choi, Jonghyun Kim, Yifan Peng, Gordon Wetzstein", "title": "Michelson Holography: Dual-SLM Holography with Camera-in-the-loop\n  Optimization", "comments": null, "journal-ref": null, "doi": "10.1364/OPTICA.410622", "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Michelson Holography (MH), a holographic display technology that\noptimizes image quality for emerging holographic near-eye displays. Using two\nspatial light modulators, MH is capable of leveraging destructive interference\nto optically cancel out undiffracted light corrupting the observed image. We\ncalibrate this system using emerging camera-in-the-loop holography techniques\nand demonstrate state-of-the-art holographic 2D image quality.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:32:58 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 03:26:42 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Choi", "Suyeon", ""], ["Kim", "Jonghyun", ""], ["Peng", "Yifan", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2009.09357", "submitter": "Won Joon Yun", "authors": "Won Joon Yun, Joongheon Kim", "title": "3D Modeling and WebVR Implementation using Azure Kinect, Open3D, and\n  Three.js", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method of extracting an RGB-D image usingAzure Kinect,\na depth camera, creating afragment,i.e., 6D images (RGBXYZ), usingOpen3D,\ncreatingit as a point cloud object, and implementing webVR usingthree.js.\nFurthermore, it presents limitations and potentials for development.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 05:45:36 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yun", "Won Joon", ""], ["Kim", "Joongheon", ""]]}, {"id": "2009.09485", "submitter": "Ayush Tewari", "authors": "Ayush Tewari, Mohamed Elgharib, Mallikarjun B R., Florian Bernard,\n  Hans-Peter Seidel, Patrick P\\'erez, Michael Zollh\\\"ofer, Christian Theobalt", "title": "PIE: Portrait Image Embedding for Semantic Control", "comments": "To appear in SIGGRAPH Asia 2020. Project webpage:\n  https://gvv.mpi-inf.mpg.de/projects/PIE/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing of portrait images is a very popular and important research topic\nwith a large variety of applications. For ease of use, control should be\nprovided via a semantically meaningful parameterization that is akin to\ncomputer animation controls. The vast majority of existing techniques do not\nprovide such intuitive and fine-grained control, or only enable coarse editing\nof a single isolated control parameter. Very recently, high-quality\nsemantically controlled editing has been demonstrated, however only on\nsynthetically created StyleGAN images. We present the first approach for\nembedding real portrait images in the latent space of StyleGAN, which allows\nfor intuitive editing of the head pose, facial expression, and scene\nillumination in the image. Semantic editing in parameter space is achieved\nbased on StyleRig, a pretrained neural network that maps the control space of a\n3D morphable face model to the latent space of the GAN. We design a novel\nhierarchical non-linear optimization problem to obtain the embedding. An\nidentity preservation energy term allows spatially coherent edits while\nmaintaining facial integrity. Our approach runs at interactive frame rates and\nthus allows the user to explore the space of possible edits. We evaluate our\napproach on a wide set of portrait photos, compare it to the current state of\nthe art, and validate the effectiveness of its components in an ablation study.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 17:53:51 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tewari", "Ayush", ""], ["Elgharib", "Mohamed", ""], ["R.", "Mallikarjun B", ""], ["Bernard", "Florian", ""], ["Seidel", "Hans-Peter", ""], ["P\u00e9rez", "Patrick", ""], ["Zollh\u00f6fer", "Michael", ""], ["Theobalt", "Christian", ""]]}, {"id": "2009.09500", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat and Sergii A. Zori", "title": "3D Primitives Gpgpu Generation for Volume Visualization in 3D Graphics\n  Systems", "comments": "12 pages, 9 figures", "journal-ref": "journal of king abdulaziz university computing and information\n  technology sciences, Vol. 9, issue 2, 2020", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses the study of 3D graphic volume primitive computer\nsystem generation (3D segments) based on General Purpose Graphics Processing\nUnit (GPGPU) technology for 3D volume visualization systems. It is based on the\ngeneral method of Volume 3D primitive generation and an algorithm for the\nvoxelization of 3D lines, previously proposed and studied by the authors. We\nconsidered the Compute Unified Device Architect (CUDA) implementation of a\nparametric method for generating 3D line segments and characteristics of\ngeneration on modern Graphics Processing Units. Experiments on the test bench\nshowed the relative inefficiency of generating a single 3D line segment and the\nefficiency of generating both fixed and arbitrary length of 3D segments on a\nGraphics Processing Unit (GPU). Experimental studies have proven the\neffectiveness and the quality of produced solutions by our method, when\ncompared to existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 19:02:32 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Zori", "Sergii A.", ""]]}, {"id": "2009.09501", "submitter": "Anas Al-Oraiqat Dr.", "authors": "Anas M. Al-Oraiqat and Sergii A. Zori", "title": "3D Pseudo Stereo Visualization with Gpgpu Support", "comments": "14 pages, 8 figures", "journal-ref": "journal of king abdulaziz university computing and information\n  technology sciences, Vol. 9, Issue 2, 2020", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses the study of a computer system for creating 3D\npseudo-stereo images and videos using hardware and software support for\naccelerating a synthesis process based on General Purpose Graphics Processing\nUnit (GPGPU) technology. Based on the general strategy of 3D pseudo-stereo\nsynthesis previously proposed by the authors, Compute Unified Device Architect\n(CUDA) method considers the main implementation stages of 3D pseudo-stereo\nsynthesis: (i) the practical implementation study; (ii) the synthesis\ncharacteristics for obtaining images; (iii) the video in Ultra-High Definition\n(UHD) 4K resolution using the Graphics Processing Unit (GPU). Respectively with\nthese results of 4K content test on evaluation systems with a GPU the\nacceleration average of 60.6 and 6.9 times is obtained for images and videos.\nThe research results show consistency with previously identified forecasts for\nprocessing 4K image frames. They are confirming the possibility of synthesizing\n3D pseudo-stereo algorithms in real time using powerful support for modern\nGraphics Processing Unit/Graphics Processing Clusters (GPU/GPC).\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2020 19:06:03 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Al-Oraiqat", "Anas M.", ""], ["Zori", "Sergii A.", ""]]}, {"id": "2009.09808", "submitter": "Thomas Davies", "authors": "Thomas Davies and Derek Nowrouzezahrai and Alec Jacobson", "title": "On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neural implicit outputs a number indicating whether the given query point\nin space is inside, outside, or on a surface. Many prior works have focused on\n_latent-encoded_ neural implicits, where a latent vector encoding of a specific\nshape is also fed as input. While affording latent-space interpolation, this\ncomes at the cost of reconstruction accuracy for any _single_ shape. Training a\nspecific network for each 3D shape, a _weight-encoded_ neural implicit may\nforgo the latent vector and focus reconstruction accuracy on the details of a\nsingle shape. While previously considered as an intermediary representation for\n3D scanning tasks or as a toy-problem leading up to latent-encoding tasks,\nweight-encoded neural implicits have not yet been taken seriously as a 3D shape\nrepresentation. In this paper, we establish that weight-encoded neural\nimplicits meet the criteria of a first-class 3D shape representation. We\nintroduce a suite of technical contributions to improve reconstruction\naccuracy, convergence, and robustness when learning the signed distance field\ninduced by a polygonal mesh -- the _de facto_ standard representation. Viewed\nas a lossy compression, our conversion outperforms standard techniques from\ngeometry processing. Compared to previous latent- and weight-encoded neural\nimplicits we demonstrate superior robustness, scalability, and performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 23:10:19 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 15:17:02 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2021 21:27:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Davies", "Thomas", ""], ["Nowrouzezahrai", "Derek", ""], ["Jacobson", "Alec", ""]]}, {"id": "2009.09928", "submitter": "Yue Liu", "authors": "Yue Liu, Alex Colburn, Mehlika Inanici", "title": "Deep Neural Network Approach for Annual Luminance Simulations", "comments": null, "journal-ref": null, "doi": "10.1080/19401493.2020.1803404", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Annual luminance maps provide meaningful evaluations for occupants' visual\ncomfort, preferences, and perception. However, acquiring long-term luminance\nmaps require labor-intensive and time-consuming simulations or impracticable\nlong-term field measurements. This paper presents a novel data-driven machine\nlearning approach that makes annual luminance-based evaluations more efficient\nand accessible. The methodology is based on predicting the annual luminance\nmaps from a limited number of point-in-time high dynamic range imagery by\nutilizing a deep neural network (DNN). Panoramic views are utilized, as they\ncan be post-processed to study multiple view directions. The proposed DNN model\ncan faithfully predict high-quality annual panoramic luminance maps from one of\nthe three options within 30 minutes training time: a) point-in-time luminance\nimagery spanning 5% of the year, when evenly distributed during daylight hours,\nb) one-month hourly imagery generated or collected continuously during daylight\nhours around the equinoxes (8% of the year); or c) 9 days of hourly data\ncollected around the spring equinox, summer and winter solstices (2.5% of the\nyear) all suffice to predict the luminance maps for the rest of the year. The\nDNN predicted high-quality panoramas are validated against Radiance (RPICT)\nrenderings using a series of quantitative and qualitative metrics. The most\nefficient predictions are achieved with 9 days of hourly data collected around\nthe spring equinox, summer and winter solstices. The results clearly show that\npractitioners and researchers can efficiently incorporate long-term\nluminance-based metrics over multiple view directions into the design and\nresearch processes using the proposed DNN workflow.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 20:19:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Liu", "Yue", ""], ["Colburn", "Alex", ""], ["Inanici", "Mehlika", ""]]}, {"id": "2009.09940", "submitter": "Guan Li", "authors": "Guan Li, Junpeng Wang, Han-Wei Shen, Kaixin Chen, Guihua Shan, and\n  Zhonghua Lu", "title": "CNNPruner: Pruning Convolutional Neural Networks with Visual Analytics", "comments": "10 pages,15 figures, Accepted for presentation at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have demonstrated extraordinarily good\nperformance in many computer vision tasks. The increasing size of CNN models,\nhowever, prevents them from being widely deployed to devices with limited\ncomputational resources, e.g., mobile/embedded devices. The emerging topic of\nmodel pruning strives to address this problem by removing less important\nneurons and fine-tuning the pruned networks to minimize the accuracy loss.\nNevertheless, existing automated pruning solutions often rely on a numerical\nthreshold of the pruning criteria, lacking the flexibility to optimally balance\nthe trade-off between model size and accuracy. Moreover, the complicated\ninterplay between the stages of neuron pruning and model fine-tuning makes this\nprocess opaque, and therefore becomes difficult to optimize. In this paper, we\naddress these challenges through a visual analytics approach, named CNNPruner.\nIt considers the importance of convolutional filters through both instability\nand sensitivity, and allows users to interactively create pruning plans\naccording to a desired goal on model size or accuracy. Also, CNNPruner\nintegrates state-of-the-art filter visualization techniques to help users\nunderstand the roles that different filters played and refine their pruning\nplans. Through comprehensive case studies on CNNs with real-world sizes, we\nvalidate the effectiveness of CNNPruner.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 02:08:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Li", "Guan", ""], ["Wang", "Junpeng", ""], ["Shen", "Han-Wei", ""], ["Chen", "Kaixin", ""], ["Shan", "Guihua", ""], ["Lu", "Zhonghua", ""]]}, {"id": "2009.10711", "submitter": "Donglai Xiang", "authors": "Donglai Xiang, Fabian Prada, Chenglei Wu, Jessica Hodgins", "title": "MonoClothCap: Towards Temporally Coherent Clothing Capture from\n  Monocular RGB Video", "comments": "3DV 2020 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to capture temporally coherent dynamic clothing\ndeformation from a monocular RGB video input. In contrast to the existing\nliterature, our method does not require a pre-scanned personalized mesh\ntemplate, and thus can be applied to in-the-wild videos. To constrain the\noutput to a valid deformation space, we build statistical deformation models\nfor three types of clothing: T-shirt, short pants and long pants. A\ndifferentiable renderer is utilized to align our captured shapes to the input\nframes by minimizing the difference in both silhouette, segmentation, and\ntexture. We develop a UV texture growing method which expands the visible\ntexture region of the clothing sequentially in order to minimize drift in\ndeformation tracking. We also extract fine-grained wrinkle detail from the\ninput videos by fitting the clothed surface to the normal maps estimated by a\nconvolutional neural network. Our method produces temporally coherent\nreconstruction of body and clothing from monocular video. We demonstrate\nsuccessful clothing capture results from a variety of challenging videos.\nExtensive quantitative experiments demonstrate the effectiveness of our method\non metrics including body pose error and surface reconstruction error of the\nclothing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 17:54:38 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:23:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Xiang", "Donglai", ""], ["Prada", "Fabian", ""], ["Wu", "Chenglei", ""], ["Hodgins", "Jessica", ""]]}, {"id": "2009.10796", "submitter": "Zander Majercik", "authors": "Zander Majercik, Adam Marrs, Josef Spjut, Morgan McGuire", "title": "Scaling Probe-Based Real-Time Dynamic Global Illumination for Production", "comments": "Supplemental video: https://youtu.be/vbJ2aNI94Ho Journal of Computer\n  Graphics Techniques (published version):\n  http://www.jcgt.org/published/0010/02/01/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute several practical extensions to the probe based\nirradiance-field-with-visibility representation to improve image quality,\nconstant and asymptotic performance, memory efficiency, and artist control. We\ndeveloped these extensions in the process of incorporating the previous work\ninto the global illumination solutions of the NVIDIA RTXGI SDK, the Unity and\nUnreal Engine 4 game engines, and proprietary engines for several commercial\ngames. These extensions include: a single, intuitive tuning parameter (the\n\"self-shadow\" bias); heuristics to speed transitions in the global\nillumination; reuse of irradiance data as prefiltered radiance for recursive\nglossy reflection; a probe state machine to prune work that will not affect the\nfinal image; and multiresolution cascaded volumes for large worlds.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 20:23:32 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 23:07:47 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 01:03:29 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Majercik", "Zander", ""], ["Marrs", "Adam", ""], ["Spjut", "Josef", ""], ["McGuire", "Morgan", ""]]}, {"id": "2009.11148", "submitter": "Pepe Eulzer", "authors": "Pepe Eulzer, Sabine Bauer, Francis Kilian, Kai Lawonn", "title": "Visualization of Human Spine Biomechanics for Spinal Surgery", "comments": "9+2 pages, 11 figures, to be published in IEEE Transactions on\n  Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030388", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a visualization application, designed for the exploration of human\nspine simulation data. Our goal is to support research in biomechanical spine\nsimulation and advance efforts to implement simulation-backed analysis in\nsurgical applications. Biomechanical simulation is a state-of-the-art technique\nfor analyzing load distributions of spinal structures. Through the inclusion of\npatient-specific data, such simulations may facilitate personalized treatment\nand customized surgical interventions. Difficulties in spine modelling and\nsimulation can be partly attributed to poor result representation, which may\nalso be a hindrance when introducing such techniques into a clinical\nenvironment. Comparisons of measurements across multiple similar anatomical\nstructures and the integration of temporal data make commonly available\ndiagrams and charts insufficient for an intuitive and systematic display of\nresults. Therefore, we facilitate methods such as multiple coordinated views,\nabstraction and focus and context to display simulation outcomes in a dedicated\ntool. By linking the result data with patient-specific anatomy, we make\nrelevant parameters tangible for clinicians. Furthermore, we introduce new\nconcepts to show the directions of impact force vectors, which were not\naccessible before. We integrated our toolset into a spine segmentation and\nsimulation pipeline and evaluated our methods with both surgeons and\nbiomechanical researchers. When comparing our methods against standard\nrepresentations that are currently in use, we found increases in accuracy and\nspeed in data exploration tasks. In a qualitative review, domain experts deemed\nthe tool highly useful when dealing with simulation result data, which\ntypically combines time-dependent patient movement and the resulting force\ndistributions on spinal structures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 13:50:11 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Eulzer", "Pepe", ""], ["Bauer", "Sabine", ""], ["Kilian", "Francis", ""], ["Lawonn", "Kai", ""]]}, {"id": "2009.11422", "submitter": "Bruno Augusto Nassif Travencolo", "authors": "Jean R. Ponciano, Claudio D. G. Linhares, Elaine R. Faria, and Bruno\n  A. N. Travencolo", "title": "An Online and Nonuniform Timeslicing Method for Network Visualisation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2021.04.006", "report-no": null, "categories": "cs.SI cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analysis of temporal networks comprises an effective way to understand\nthe network dynamics, facilitating the identification of patterns, anomalies,\nand other network properties, thus resulting in fast decision making. The\namount of data in real-world networks, however, may result in a layout with\nhigh visual clutter due to edge overlapping. This is particularly relevant in\nthe so-called streaming networks, in which edges are continuously arriving\n(online) and in non-stationary distribution. All three network dimensions,\nnamely node, edge, and time, can be manipulated to reduce such clutter and\nimprove readability. This paper presents an online and nonuniform timeslicing\nmethod, thus considering the underlying network structure and addressing\nstreaming network analyses. We conducted experiments using two real-world\nnetworks to compare our method against uniform and nonuniform timeslicing\nstrategies. The results show that our method automatically selects timeslices\nthat effectively reduce visual clutter in periods with bursts of events. As a\nconsequence, decision making based on the identification of global temporal\npatterns becomes faster and more reliable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 00:21:56 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ponciano", "Jean R.", ""], ["Linhares", "Claudio D. G.", ""], ["Faria", "Elaine R.", ""], ["Travencolo", "Bruno A. N.", ""]]}, {"id": "2009.12216", "submitter": "Jon McCormack", "authors": "Jon McCormack and Andy Lomas", "title": "Deep Learning of Individual Aesthetics", "comments": "Author preprint of article for Neural Computing and Applications.\n  arXiv admin note: substantial text overlap with arXiv:2004.06874", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate evaluation of human aesthetic preferences represents a major\nchallenge for creative evolutionary and generative systems research. Prior work\nhas tended to focus on feature measures of the artefact, such as symmetry,\ncomplexity and coherence. However, research models from Psychology suggest that\nhuman aesthetic experiences encapsulate factors beyond the artefact, making\naccurate computational models very difficult to design. The interactive genetic\nalgorithm (IGA) circumvents the problem through human-in-the-loop, subjective\nevaluation of aesthetics, but is limited due to user fatigue and small\npopulation sizes. In this paper we look at how recent advances in deep learning\ncan assist in automating personal aesthetic judgement. Using a leading artist's\ncomputer art dataset, we investigate the relationship between image measures,\nsuch as complexity, and human aesthetic evaluation. We use dimension reduction\nmethods to visualise both genotype and phenotype space in order to support the\nexploration of new territory in a generative system. Convolutional Neural\nNetworks trained on the artist's prior aesthetic evaluations are used to\nsuggest new possibilities similar or between known high quality\ngenotype-phenotype mappings. We integrate this classification and discovery\nsystem into a software tool for evolving complex generative art and design.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 03:04:28 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["McCormack", "Jon", ""], ["Lomas", "Andy", ""]]}, {"id": "2009.12395", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Aakash Parikh, Xiyu Zhai, Melody Mao, Luisa\n  Caldas, Allen Y. Yang", "title": "SceneGen: Generative Contextual Scene Augmentation using Scene Graph\n  Priors", "comments": "19 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing experiences are constrained by the real-world surroundings\nof the user. In such experiences, augmenting virtual objects to existing scenes\nrequire a contextual approach, where geometrical conflicts are avoided, and\nfunctional and plausible relationships to other objects are maintained in the\ntarget environment. Yet, due to the complexity and diversity of user\nenvironments, automatically calculating ideal positions of virtual content that\nis adaptive to the context of the scene is considered a challenging task.\nMotivated by this problem, in this paper we introduce SceneGen, a generative\ncontextual augmentation framework that predicts virtual object positions and\norientations within existing scenes. SceneGen takes a semantically segmented\nscene as input, and outputs positional and orientational probability maps for\nplacing virtual content. We formulate a novel spatial Scene Graph\nrepresentation, which encapsulates explicit topological properties between\nobjects, object groups, and the room. We believe providing explicit and\nintuitive features plays an important role in informative content creation and\nuser interaction of spatial computing settings, a quality that is not captured\nin implicit models. We use kernel density estimation (KDE) to build a\nmultivariate conditional knowledge model trained using prior spatial Scene\nGraphs extracted from real-world 3D scanned data. To further capture\norientational properties, we develop a fast pose annotation tool to extend\ncurrent real-world datasets with orientational labels. Finally, to demonstrate\nour system in action, we develop an Augmented Reality application, in which\nobjects can be contextually augmented in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:36:27 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:06:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Parikh", "Aakash", ""], ["Zhai", "Xiyu", ""], ["Mao", "Melody", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""]]}, {"id": "2009.12933", "submitter": "Yawei Ge", "authors": "Yawei Ge, Heike Hofmann", "title": "A grammar of graphics framework for generalized parallel coordinate\n  plots", "comments": "26 pages, 14 figures. For the implementation in R, see:\n  https://github.com/yaweige/ggpcp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinate plots (PCP) are a useful tool in exploratory data\nanalysis of high-dimensional numerical data. The use of PCPs is limited when\nworking with categorical variables or a mix of categorical and continuous\nvariables. In this paper, we propose generalized parallel coordinate plots\n(GPCP) to extend the ability of PCPs from just numeric variables to dealing\nseamlessly with a mix of categorical and numeric variables in a single plot. In\nthis process we find that existing solutions for categorical values only, such\nas hammock plots or parsets become edge cases in the new framework. By focusing\non individual observation rather a marginal frequency we gain additional\nflexibility. The resulting approach is implemented in the R package ggpcp.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 19:55:17 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Ge", "Yawei", ""], ["Hofmann", "Heike", ""]]}, {"id": "2009.12967", "submitter": "Connor Daly", "authors": "Connor Daly", "title": "Recognition and Synthesis of Object Transport Motion", "comments": "46 pages, MEng thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning typically requires vast numbers of training examples in order\nto be used successfully. Conversely, motion capture data is often expensive to\ngenerate, requiring specialist equipment, along with actors to generate the\nprescribed motions, meaning that motion capture datasets tend to be relatively\nsmall. Motion capture data does however provide a rich source of information\nthat is becoming increasingly useful in a wide variety of applications, from\ngesture recognition in human-robot interaction, to data driven animation.\n  This project illustrates how deep convolutional networks can be used,\nalongside specialized data augmentation techniques, on a small motion capture\ndataset to learn detailed information from sequences of a specific type of\nmotion (object transport). The project shows how these same augmentation\ntechniques can be scaled up for use in the more complex task of motion\nsynthesis.\n  By exploring recent developments in the concept of Generative Adversarial\nModels (GANs), specifically the Wasserstein GAN, this project outlines a model\nthat is able to successfully generate lifelike object transportation motions,\nwith the generated samples displaying varying styles and transport strategies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 22:13:26 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Daly", "Connor", ""]]}, {"id": "2009.13133", "submitter": "Pascal Nardini", "authors": "Pascal Nardini, Min Chen, Roxana Bujack, Michael B\\\"ottinger, and\n  Gerik Scheuermann", "title": "A Testing Environment for Continuous Colormaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer science disciplines (e.g., combinatorial optimization, natural\nlanguage processing, and information retrieval) use standard or established\ntest suites for evaluating algorithms. In visualization, similar approaches\nhave been adopted in some areas (e.g., volume visualization), while user\ntestimonies and empirical studies have been the dominant means of evaluation in\nmost other areas, such as designing colormaps. In this paper, we propose to\nestablish a test suite for evaluating the design of colormaps. With such a\nsuite, the users can observe the effects when different continuous colormaps\nare applied to planar scalar fields that may exhibit various characteristic\nfeatures, such as jumps, local extrema, ridge or valley lines, different\ndistributions of scalar values, different gradients, different signal\nfrequencies, different levels of noise, and so on. The suite also includes an\nexpansible collection of real-world data sets including the most popular data\nfor colormap testing in the visualization literature. The test suite has been\nintegrated into a web-based application for creating continuous colormaps\n(https://ccctool.com/), facilitating close inter-operation between design and\nevaluation processes. This new facility complements traditional evaluation\nmethods such as user testimonies and empirical studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 08:28:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Nardini", "Pascal", ""], ["Chen", "Min", ""], ["Bujack", "Roxana", ""], ["B\u00f6ttinger", "Michael", ""], ["Scheuermann", "Gerik", ""]]}, {"id": "2009.13339", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma and Maks Ovsjanikov", "title": "Weakly Supervised Deep Functional Map for Shape Matching", "comments": "Accepted to appear in proceedings of Neurips 2020. Code available at:\n  \\url{https://github.com/Not-IITian/Weakly-supervised-Functional-map}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of deep functional maps have been proposed recently, from fully\nsupervised to totally unsupervised, with a range of loss functions as well as\ndifferent regularization terms. However, it is still not clear what are minimum\ningredients of a deep functional map pipeline and whether such ingredients\nunify or generalize all recent work on deep functional maps. We show\nempirically minimum components for obtaining state of the art results with\ndifferent loss functions, supervised as well as unsupervised. Furthermore, we\npropose a novel framework designed for both full-to-full as well as partial to\nfull shape matching that achieves state of the art results on several benchmark\ndatasets outperforming even the fully supervised methods by a significant\nmargin. Our code is publicly available at\nhttps://github.com/Not-IITian/Weakly-supervised-Functional-map\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:06:46 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Sharma", "Abhishek", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "2009.13349", "submitter": "Teseo Schneider", "authors": "Bolun Wang, Zachary Ferguson, Teseo Schneider, Xin Jiang, Marco\n  Attene, Daniele Panozzo", "title": "A Large Scale Benchmark and an Inclusion-Based Algorithm for Continuous\n  Collision Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a large scale benchmark for continuous collision detection (CCD)\nalgorithms, composed of queries manually constructed to highlight challenging\ndegenerate cases and automatically generated using existing simulators to cover\ncommon cases. We use the benchmark to evaluate the accuracy, correctness, and\nefficiency of state-of-the-art continuous collision detection algorithms, both\nwith and without minimal separation. We discover that, despite the widespread\nuse of CCD algorithms, existing algorithms are either: (1) correct but\nimpractically slow, (2) efficient but incorrect, introducing false negatives\nwhich will lead to interpenetration, or (3) correct but over conservative,\nreporting a large number of false positives which might lead to inaccuracies\nwhen integrated in a simulator. By combining the seminal interval root finding\nalgorithm introduced by Snyder in 1992 with modern predicate design techniques,\nwe propose a simple and efficient CCD algorithm. This algorithm is competitive\nwith state of the art methods in terms of runtime while conservatively\nreporting the time of impact and allowing explicit trade off between runtime\nefficiency and number of false positives reported.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:16:03 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Wang", "Bolun", ""], ["Ferguson", "Zachary", ""], ["Schneider", "Teseo", ""], ["Jiang", "Xin", ""], ["Attene", "Marco", ""], ["Panozzo", "Daniele", ""]]}, {"id": "2009.13856", "submitter": "Maayan Shuvi", "authors": "Maayan Shuvi, Noa Fish, Kfir Aberman, Ariel Shamir, Daniel Cohen-Or", "title": "Neural Alignment for Face De-pixelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple method to reconstruct a high-resolution video from a\nface-video, where the identity of a person is obscured by pixelization. This\nconcealment method is popular because the viewer can still perceive a human\nface figure and the overall head motion. However, we show in our experiments\nthat a fairly good approximation of the original video can be reconstructed in\na way that compromises anonymity. Our system exploits the simultaneous\nsimilarity and small disparity between close-by video frames depicting a human\nface, and employs a spatial transformation component that learns the alignment\nbetween the pixelated frames. Each frame, supported by its aligned surrounding\nframes, is first encoded, then decoded to a higher resolution. Reconstruction\nand perceptual losses promote adherence to the ground-truth, and an adversarial\nloss assists in maintaining domain faithfulness. There is no need for explicit\ntemporal coherency loss as it is maintained implicitly by the alignment of\nneighboring frames and reconstruction. Although simple, our framework\nsynthesizes high-quality face reconstructions, demonstrating that given the\nstatistical prior of a human face, multiple aligned pixelated frames contain\nsufficient information to reconstruct a high-quality approximation of the\noriginal signal.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 08:29:15 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Shuvi", "Maayan", ""], ["Fish", "Noa", ""], ["Aberman", "Kfir", ""], ["Shamir", "Ariel", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2009.14514", "submitter": "Prashant Goswami", "authors": "Prashant Goswami and Christopher Batty", "title": "Asynchronous Liquids: Regional Time Stepping for Faster SPH and PCISPH", "comments": "Please see the ReadMe.txt file", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents novel and efficient strategies to spatially adapt the\namount of computational effort applied based on the local dynamics of a free\nsurface flow, for both classic weakly compressible SPH (WCSPH) and\npredictive-corrective incompressible SPH (PCISPH). Using a convenient and\nreadily parallelizable block-based approach, different regions of the fluid are\nassigned differing time steps and solved at different rates to minimize\ncomputational cost. Our approach for WCSPH scheme extends an asynchronous SPH\ntechnique from compressible flow of astrophysical phenomena to the\nincompressible free surface setting, and further accelerates it by entirely\ndecoupling the time steps of widely spaced particles. Similarly, our approach\nto PCISPH adjusts the the number of iterations of density correction applied to\ndifferent regions, and asynchronously updates the neighborhood regions used to\nperform these corrections; this sharply reduces the computational cost of\nslowly deforming regions while preserving the standard density invariant. We\ndemonstrate our approaches on a number of highly dynamic scenarios,\ndemonstrating that they can typically double the speed of a simulation compared\nto standard methods while achieving visually consistent results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:05:12 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Goswami", "Prashant", ""], ["Batty", "Christopher", ""]]}, {"id": "2009.14535", "submitter": "Yanrui Xu", "authors": "Sinuo Liu, Xiaokun Wang, Xiaojuan Ban, Yanrui Xu, Jing Zhou,\n  Ji\\v{r}\\'i Kosinka, Alexandru C.Telea", "title": "Turbulent Details Simulation for SPH Fluids via Vorticity Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major issue in Smoothed Particle Hydrodynamics (SPH) approaches is the\nnumerical dissipation during the projection process, especially under coarse\ndiscretizations. High-frequency details, such as turbulence and vortices, are\nsmoothed out, leading to unrealistic results. To address this issue, we\nintroduce a Vorticity Refinement (VR) solver for SPH fluids with negligible\ncomputational overhead. In this method, the numerical dissipation of the\nvorticity field is recovered by the difference between the theoretical and the\nactual vorticity, so as to enhance turbulence details. Instead of solving the\nBiot-Savart integrals, a stream function, which is easier and more efficient to\nsolve, is used to relate the vorticity field to the velocity field. We obtain\nturbulence effects of different intensity levels by changing an adjustable\nparameter. Since the vorticity field is enhanced according to the curl field,\nour method can not only amplify existing vortices, but also capture additional\nturbulence. Our VR solver is straightforward to implement and can be easily\nintegrated into existing SPH methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 09:34:11 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Liu", "Sinuo", ""], ["Wang", "Xiaokun", ""], ["Ban", "Xiaojuan", ""], ["Xu", "Yanrui", ""], ["Zhou", "Jing", ""], ["Kosinka", "Ji\u0159\u00ed", ""], ["Telea", "Alexandru C.", ""]]}, {"id": "2009.14624", "submitter": "Jing Ren", "authors": "Jing Ren, Mikhail Panine, Peter Wonka, and Maks Ovsjanikov", "title": "Structured Regularization of Functional Map Computations", "comments": "Eurographics Symposium on Geometry Processing 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of non-rigid shape matching using the functional map\nframework. Specifically, we analyze a commonly used approach for regularizing\nfunctional maps, which consists in penalizing the failure of the unknown map to\ncommute with the Laplace-Beltrami operators on the source and target shapes. We\nshow that this approach has certain undesirable fundamental theoretical\nlimitations, and can be undefined even for trivial maps in the smooth setting.\nInstead we propose a novel, theoretically well-justified approach for\nregularizing functional maps, by using the notion of the resolvent of the\nLaplacian operator. In addition, we provide a natural one-parameter family of\nregularizers, that can be easily tuned depending on the expected approximate\nisometry of the input shape pair. We show on a wide range of shape\ncorrespondence scenarios that our novel regularization leads to an improvement\nin the quality of the estimated functional, and ultimately pointwise\ncorrespondences before and after commonly-used refinement techniques.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 12:43:49 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ren", "Jing", ""], ["Panine", "Mikhail", ""], ["Wonka", "Peter", ""], ["Ovsjanikov", "Maks", ""]]}]