[{"id": "1202.1444", "submitter": "Augusto Salazar Mr.", "authors": "Augusto Salazar, Stefanie Wuhrer, Chang Shu and Flavio Prieto", "title": "Fully Automatic Expression-Invariant Face Correspondence", "comments": null, "journal-ref": "Machine Vision and Applications, 25(4):859-879, 2014", "doi": "10.1007/s00138-013-0579-9", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing accurate point-to-point correspondences\namong a set of human face scans with varying expressions. Our fully automatic\napproach does not require any manually placed markers on the scan. Instead, the\napproach learns the locations of a set of landmarks present in a database and\nuses this knowledge to automatically predict the locations of these landmarks\non a newly available scan. The predicted landmarks are then used to compute\npoint-to-point correspondences between a template model and the newly available\nscan. To accurately fit the expression of the template to the expression of the\nscan, we use as template a blendshape model. Our algorithm was tested on a\ndatabase of human faces of different ethnic groups with strongly varying\nexpressions. Experimental results show that the obtained point-to-point\ncorrespondence is both highly accurate and consistent for most of the tested 3D\nface models.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 15:05:05 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 22:52:30 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Salazar", "Augusto", ""], ["Wuhrer", "Stefanie", ""], ["Shu", "Chang", ""], ["Prieto", "Flavio", ""]]}, {"id": "1202.1808", "submitter": "Surekha Mariam Varghese Dr.", "authors": "Kurien Zacharia, Eldo P. Elias, Surekha Mariam Varghese", "title": "Personalised product design using virtual interactive techniques", "comments": "10 pages; International Journal of Computer Graphics & Animation\n  (IJCGA) Vol.2, No.1, January 2012", "journal-ref": null, "doi": "10.5121/ijcga.2012.2101", "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of Virtual Interactive Techniques for personalized product design is\ndescribed in this paper. Usually products are designed and built by considering\ngeneral usage patterns and Prototyping is used to mimic the static or working\nbehaviour of an actual product before manufacturing the product. The user does\nnot have any control on the design of the product. Personalized design\npostpones design to a later stage. It allows for personalized selection of\nindividual components by the user. This is implemented by displaying the\nindividual components over a physical model constructed using Cardboard or\nThermocol in the actual size and shape of the original product. The components\nof the equipment or product such as screen, buttons etc. are then projected\nusing a projector connected to the computer into the physical model. Users can\ninteract with the prototype like the original working equipment and they can\nselect, shape, position the individual components displayed on the interaction\npanel using simple hand gestures. Computer Vision techniques as well as sound\nprocessing techniques are used to detect and recognize the user gestures\ncaptured using a web camera and microphone.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 20:26:26 GMT"}], "update_date": "2012-02-09", "authors_parsed": [["Zacharia", "Kurien", ""], ["Elias", "Eldo P.", ""], ["Varghese", "Surekha Mariam", ""]]}, {"id": "1202.1841", "submitter": "Ferihane Kboubi", "authors": "F\\'erihane Kboubi, Anja Habacha Chaibi and Mohamed BenAhmed", "title": "Semantic Visualization and Navigation in Textual Corpus", "comments": "11 pages, 6 figures", "journal-ref": "International Journal of Information Sciences and Techniques\n  (IJIST) Vol.2, No.1, January 2012", "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.GR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a survey of related work on the information visualization\ndomain and study the real integration of the cartography paradigms in actual\ninformation search systems. Based on this study, we propose a semantic\nvisualization and navigation approach which offer to users three search modes:\nprecise search, connotative search and thematic search. The objective is to\npropose to the users of an information search system, new interaction paradigms\nwhich support the semantic aspect of the considered information space and guide\nusers in their searches by assisting them to locate their interest center and\nto improve serendipity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 21:38:11 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Kboubi", "F\u00e9rihane", ""], ["Chaibi", "Anja Habacha", ""], ["BenAhmed", "Mohamed", ""]]}, {"id": "1202.2868", "submitter": "Drazen Lucanin MSc", "authors": "Drazen Lucanin", "title": "Visual definition of procedures for automatic virtual scene generation", "comments": "Master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  With more and more digital media, especially in the field of virtual reality\nwhere detailed and convincing scenes are much required, procedural scene\ngeneration is a big helping tool for artists. A problem is that defining scene\ndescriptions through these procedures usually requires a knowledge in formal\nlanguage grammars, programming theory and manually editing textual files using\na strict syntax, making it less intuitive to use. Luckily, graphical user\ninterfaces has made a lot of tasks on computers easier to perform and out of\nthe belief that creating computer programs can also be one of them, visual\nprogramming languages (VPLs) have emerged. The goal in VPLs is to shift more\nwork from the programmer to the integrated development environment (IDE),\nmaking programming an user-friendlier task.\n  In this thesis, an approach of using a VPL for defining procedures that\nautomatically generate virtual scenes is presented. The methods required to\nbuild a VPL are presented, including a novel method of generating readable code\nin a structured programming language. Also, the methods for achieving basic\nprinciples of VPLs will be shown -- suitable visual presentation of information\nand guiding the programmer in the right direction using constraints. On the\nother hand, procedural generation methods are presented in the context of\nvisual programming -- adapting the application programming interface (API) of\nthese methods to better serve the user. The main focus will be on the methods\nfor urban modeling, such as building, city layout and details generation with\nrandom number generation used to create non-deterministic scenes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 16:58:00 GMT"}], "update_date": "2012-02-15", "authors_parsed": [["Lucanin", "Drazen", ""]]}, {"id": "1202.5360", "submitter": "Fei Yang", "authors": "Fei Yang, Yong Cao, and Jie Tian", "title": "Efficient and Effective Volume Visualization with Enhanced Isosurface\n  Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with full volume rendering, isosurface rendering has several well\nrecognized advantages in efficiency and accuracy. However, standard isosurface\nrendering has some limitations in effectiveness. First, it uses a monotone\ncolored approach and can only visualize the geometry features of an isosurface.\nThe lack of the capability to illustrate the material property and the internal\nstructures behind an isosurface has been a big limitation of this method in\napplications. Another limitation of isosurface rendering is the difficulty to\nreveal physically meaningful structures, which are hidden in one or multiple\nisosurfaces. As such, the application requirements of extract and recombine\nstructures of interest can not be implemented effectively with isosurface\nrendering. In this work, we develop an enhanced isosurface rendering technique\nto improve the effectiveness while maintaining the performance efficiency of\nthe standard isosurface rendering. First, an isosurface color enhancement\nmethod is proposed to illustrate the neighborhood density and to reveal some of\nthe internal structures. Second, we extend the structure extraction capability\nof isosurface rendering by enabling explicit scene exploration within a\n3D-view, using surface peeling, voxel-selecting, isosurface segmentation, and\nmulti-surface-structure visualization. Our experiments show that the color\nenhancement not only improves the visual fidelity of the rendering, but also\nreveals the internal structures without significant increase of the\ncomputational cost. Explicit scene exploration is also demonstrated as a\npowerful tool in some application scenarios, such as displaying multiple\nabdominal organs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2012 01:39:12 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Yang", "Fei", ""], ["Cao", "Yong", ""], ["Tian", "Jie", ""]]}, {"id": "1202.6609", "submitter": "Gilles Falquet", "authors": "Claudine M\\'etral, Nizar Ghoula, Gilles Falquet", "title": "Towards an Integrated Visualization Of Semantically Enriched 3D City\n  Models: An Ontology of 3D Visualization Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D city models - which represent in 3 dimensions the geometric elements of a\ncity - are increasingly used for an intended wide range of applications. Such\nuses are made possible by using semantically enriched 3D city models and by\npresenting such enriched 3D city models in a way that allows decision-making\nprocesses to be carried out from the best choices among sets of objectives, and\nacross issues and scales. In order to help in such a decision-making process we\nhave defined a framework to find the best visualization technique(s) for a set\nof potentially heterogeneous data that have to be visualized within the same 3D\ncity model, in order to perform a given task in a specific context. We have\nchosen an ontology-based approach. This approach and the specification and use\nof the resulting ontology of 3D visualization techniques are described in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 17:15:53 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 13:33:10 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["M\u00e9tral", "Claudine", ""], ["Ghoula", "Nizar", ""], ["Falquet", "Gilles", ""]]}]