[{"id": "1701.01595", "submitter": "Yu Guang Wang", "authors": "Yu Guang Wang, Houying Zhu", "title": "Analysis of Framelet Transforms on a Simplex", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct framelets associated with a sequence of\nquadrature rules on the simplex $T^{2}$ in $\\mathbb{R}^{2}$. We give the\nframelet transforms -- decomposition and reconstruction of the coefficients for\nframelets of a function on $T^{2}$. We prove that the reconstruction is exact\nwhen the framelets are tight. We give an example of construction of framelets\nand show that the framelet transforms can be computed as fast as FFT.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 11:14:19 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 16:21:12 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 12:40:17 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Wang", "Yu Guang", ""], ["Zhu", "Houying", ""]]}, {"id": "1701.02123", "submitter": "Changsoo Je", "authors": "Changsoo Je, Kyuhyoung Choi, Sang Wook Lee", "title": "Green-Blue Stripe Pattern for Range Sensing from a Single Image", "comments": "7 pages, 5 figures. Updated version of a conference paper", "journal-ref": "Proc. 30th Fall Semiannual Conference of Korea Information Science\n  Society, vol. 2, pp. 661-663, Seoul, Korea, October, 2003", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel method for rapid high-resolution range\nsensing using green-blue stripe pattern. We use green and blue for designing\nhigh-frequency stripe projection pattern. For accurate and reliable range\nrecovery, we identify the stripe patterns by our color-stripe segmentation and\nunwrapping algorithms. The experimental result for a naked human face shows the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 10:16:11 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 02:31:41 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Je", "Changsoo", ""], ["Choi", "Kyuhyoung", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1701.02357", "submitter": "Bharat Singh", "authors": "Carlos Castillo, Soham De, Xintong Han, Bharat Singh, Abhay Kumar\n  Yadav, and Tom Goldstein", "title": "Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware\n  Semantic Segmentation", "comments": null, "journal-ref": "ICASSP 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer is an important task in which the style of a source image is\nmapped onto that of a target image. The method is useful for synthesizing\nderivative works of a particular artist or specific painting. This work\nconsiders targeted style transfer, in which the style of a template image is\nused to alter only part of a target image. For example, an artist may wish to\nalter the style of only one particular object in a target image without\naltering the object's general morphology or surroundings. This is useful, for\nexample, in augmented reality applications (such as the recently released\nPokemon GO), where one wants to alter the appearance of a single real-world\nobject in an image frame to make it appear as a cartoon. Most notably, the\nrendering of real-world objects into cartoon characters has been used in a\nnumber of films and television show, such as the upcoming series Son of Zorn.\nWe present a method for targeted style transfer that simultaneously segments\nand stylizes single objects selected by the user. The method uses a Markov\nrandom field model to smooth and anti-alias outlier pixels near object\nboundaries, so that stylized objects naturally blend into their surroundings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:30:03 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Castillo", "Carlos", ""], ["De", "Soham", ""], ["Han", "Xintong", ""], ["Singh", "Bharat", ""], ["Yadav", "Abhay Kumar", ""], ["Goldstein", "Tom", ""]]}, {"id": "1701.03230", "submitter": "Oussama Remil", "authors": "Oussama Remil, Qian Xie, Xingyu Xie, Kai Xu, Jun Wang", "title": "Surface Reconstruction with Data-driven Exemplar Priors", "comments": "13 pages, 15 figures, 1 table, Preprint submitted to CAD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework to reconstruct 3D models from raw\nscanned points by learning the prior knowledge of a specific class of objects.\nUnlike previous work that heuristically specifies particular regularities and\ndefines parametric models, our shape priors are learned directly from existing\n3D models under a framework based on affinity propagation. Given a database of\n3D models within the same class of objects, we build a comprehensive library of\n3D local shape priors. We then formulate the problem to select\nas-few-as-possible priors from the library, referred to as exemplar priors.\nThese priors are sufficient to represent the 3D shapes of the whole class of\nobjects from where they are generated. By manipulating these priors, we are\nable to reconstruct geometrically faithful models with the same class of\nobjects from raw point clouds. Our framework can be easily generalized to\nreconstruct various categories of 3D objects that have more geometrically or\ntopologically complex structures. Comprehensive experiments exhibit the power\nof our exemplar priors for gracefully solving several problems in 3D shape\nreconstruction such as preserving sharp features, recovering fine details and\nso on.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 04:51:15 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Remil", "Oussama", ""], ["Xie", "Qian", ""], ["Xie", "Xingyu", ""], ["Xu", "Kai", ""], ["Wang", "Jun", ""]]}, {"id": "1701.03754", "submitter": "Sharon Lin", "authors": "Sharon Lin, Matthew Fisher, Angela Dai, Pat Hanrahan", "title": "LayerBuilder: Layer Decomposition for Interactive Image and Video Color\n  Editing", "comments": "12 pages; added reference to Tan et al. 2016 and more image\n  attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring and editing colors in images is a common task in graphic design and\nphotography. However, allowing for interactive recoloring while preserving\nsmooth color blends in the image remains a challenging problem. We present\nLayerBuilder, an algorithm that decomposes an image or video into a linear\ncombination of colored layers to facilitate color-editing applications. These\nlayers provide an interactive and intuitive means for manipulating individual\ncolors. Our approach reduces color layer extraction to a fast iterative linear\nsystem. Layer Builder uses locally linear embedding, which represents pixels as\nlinear combinations of their neighbors, to reduce the number of variables in\nthe linear solve and extract layers that can better preserve color blending\neffects. We demonstrate our algorithm on recoloring a variety of images and\nvideos, and show its overall effectiveness in recoloring quality and time\ncomplexity compared to previous approaches. We also show how this\nrepresentation can benefit other applications, such as automatic recoloring\nsuggestion, texture synthesis, and color-based filtering.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:48:44 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 23:13:01 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Lin", "Sharon", ""], ["Fisher", "Matthew", ""], ["Dai", "Angela", ""], ["Hanrahan", "Pat", ""]]}, {"id": "1701.03981", "submitter": "Josef Faller", "authors": "Josef Faller, Brendan Z. Allison, Clemens Brunner, Reinhold Scherer,\n  Dieter Schmalstieg, Gert Pfurtscheller and Christa Neuper", "title": "A feasibility study on SSVEP-based interaction with motivating and\n  immersive virtual and augmented reality", "comments": "6 Pages, correspondence: josef.faller@gmail.com, Technical Report\n  (2010) Graz University of Technology, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive steady-state visual evoked potential (SSVEP) based\nbrain-computer interface (BCI) systems offer high bandwidth compared to other\nBCI types and require only minimal calibration and training. Virtual reality\n(VR) has been already validated as effective, safe, affordable and motivating\nfeedback modality for BCI experiments. Augmented reality (AR) enhances the\nphysical world by superimposing informative, context sensitive, computer\ngenerated content. In the context of BCI, AR can be used as a friendlier and\nmore intuitive real-world user interface, thereby facilitating a more seamless\nand goal directed interaction. This can improve practicality and usability of\nBCI systems and may help to compensate for their low bandwidth. In this\nfeasibility study, three healthy participants had to finish a complex\nnavigation task in immersive VR and AR conditions using an online SSVEP BCI.\nTwo out of three subjects were successful in all conditions. To our knowledge,\nthis is the first work to present an SSVEP BCI that operates using target\nstimuli integrated in immersive VR and AR (head-mounted display and camera).\nThis research direction can benefit patients by introducing more intuitive and\neffective real-world interaction (e.g. smart home control). It may also be\nrelevant for user groups that require or benefit from hands free operation\n(e.g. due to temporary situational disability).\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 01:58:47 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Faller", "Josef", ""], ["Allison", "Brendan Z.", ""], ["Brunner", "Clemens", ""], ["Scherer", "Reinhold", ""], ["Schmalstieg", "Dieter", ""], ["Pfurtscheller", "Gert", ""], ["Neuper", "Christa", ""]]}, {"id": "1701.04303", "submitter": "Fei Hou", "authors": "Fei Hou, Qian Sun, Zheng Fang, Yong-Jin Liu, Shi-Min Hu, Hong Qin,\n  Aimin Hao, Ying He", "title": "Poisson Vector Graphics (PVG) and Its Closed-Form Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Poisson vector graphics, an extension of the popular\nfirst-order diffusion curves, for generating smooth-shaded images. Armed with\ntwo new types of primitives, namely Poisson curves and Poisson regions, PVG can\neasily produce photorealistic effects such as specular highlights, core\nshadows, translucency and halos. Within the PVG framework, users specify color\nas the Dirichlet boundary condition of diffusion curves and control tone by\noffsetting the Laplacian, where both controls are simply done by mouse click\nand slider dragging. The separation of color and tone not only follows the\nbasic drawing principle that is widely adopted by professional artists, but\nalso brings three unique features to PVG, i.e., local hue change, ease of\nextrema control, and permit of intersection among geometric primitives, making\nPVG an ideal authoring tool.\n  To render PVG, we develop an efficient method to solve 2D Poisson's equations\nwith piecewise constant Laplacians. In contrast to the conventional finite\nelement method that computes numerical solutions only, our method expresses the\nsolution using harmonic B-spline, whose basis functions can be constructed\nlocally and the control coefficients are obtained by solving a small sparse\nlinear system. Our closed-form solver is numerically stable and it supports\nrandom access evaluation, zooming-in of arbitrary resolution and anti-aliasing.\nAlthough the harmonic B-spline based solutions are approximate, computational\nresults show that the relative mean error is less than 0.3%, which cannot be\ndistinguished by naked eyes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 14:33:15 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Hou", "Fei", ""], ["Sun", "Qian", ""], ["Fang", "Zheng", ""], ["Liu", "Yong-Jin", ""], ["Hu", "Shi-Min", ""], ["Qin", "Hong", ""], ["Hao", "Aimin", ""], ["He", "Ying", ""]]}, {"id": "1701.04383", "submitter": "Hasan Ali Aky\\\"urek", "authors": "Hasan Ali Aky\\\"urek, Erkan \\\"Ulker, Bar{\\i}\\c{s} Ko\\c{c}er", "title": "Automatic Knot Adjustment Using Dolphin Echolocation Algorithm for\n  B-Spline Curve Approximation", "comments": "The Journal of MacroTrends in Technology and Innovation, Vol 4. Issue\n  1. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, a new approach to solve the cubic B-spline curve fitting\nproblem is presented based on a meta-heuristic algorithm called \" dolphin\necholocation \". The method minimizes the proximity error value of the selected\nnodes that measured using the least squares method and the Euclidean distance\nmethod of the new curve generated by the reverse engineering. The results of\nthe proposed method are compared with the genetic algorithm. As a result, this\nnew method seems to be successful.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:20:32 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Aky\u00fcrek", "Hasan Ali", ""], ["\u00dclker", "Erkan", ""], ["Ko\u00e7er", "Bar\u0131\u015f", ""]]}, {"id": "1701.05754", "submitter": "Daniel Beale Dr", "authors": "Daniel Beale", "title": "User-guided free-form asset modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper a new system for piecewise primitive surface recovery on point\nclouds is presented, which allows a novice user to sketch areas of interest in\norder to guide the fitting process. The algorithm is demonstrated against a\nbenchmark technique for autonomous surface fitting, and, contrasted against\nexisting literature in user guided surface recovery, with empirical evidence.\nIt is concluded that the system is an improvement to the current documented\nliterature for its visual quality when modelling objects which are composed of\npiecewise primitive shapes, and, in its ability to fill large holes on occluded\nsurfaces using free-form input.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 10:58:20 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Beale", "Daniel", ""]]}, {"id": "1701.06507", "submitter": "Carlo Innamorati", "authors": "Carlo Innamorati, Tobias Ritschel, Tim Weyrich, Niloy J. Mitra", "title": "Plausible Shading Decomposition For Layered Photo Retouching", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photographers routinely compose multiple manipulated photos of the same scene\n(layers) into a single image, which is better than any individual photo could\nbe alone. Similarly, 3D artists set up rendering systems to produce layered\nimages to contain only individual aspects of the light transport, which are\ncomposed into the final result in post-production. Regrettably, both approaches\neither take considerable time to capture, or remain limited to synthetic\nscenes. In this paper, we suggest a system to allow decomposing a single image\ninto a plausible shading decomposition (PSD) that approximates effects such as\nshadow, diffuse illumination, albedo, and specular shading. This decomposition\ncan then be manipulated in any off-the-shelf image manipulation software and\nrecomposited back. We perform such a decomposition by learning a convolutional\nneural network trained using synthetic data. We demonstrate the effectiveness\nof our decomposition on synthetic (i.e., rendered) and real data (i.e.,\nphotographs), and use them for common photo manipulation, which are nearly\nimpossible to perform otherwise from single images.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 17:05:22 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 17:59:10 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Innamorati", "Carlo", ""], ["Ritschel", "Tobias", ""], ["Weyrich", "Tim", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1701.06641", "submitter": "Valero Laparra", "authors": "Valero Laparra, Alex Berardino, Johannes Ball\\'e, and Eero P.\n  Simoncelli", "title": "Perceptually Optimized Image Rendering", "comments": null, "journal-ref": "J. Optical Society of America, A. 34(9):1511-1525. Sep 2017", "doi": "10.1364/JOSAA.34.001511", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for rendering photographic images, taking into account\ndisplay limitations, so as to optimize perceptual similarity between the\nrendered image and the original scene. We formulate this as a constrained\noptimization problem, in which we minimize a measure of perceptual\ndissimilarity, the Normalized Laplacian Pyramid Distance (NLPD), which mimics\nthe early stage transformations of the human visual system. When rendering\nimages acquired with higher dynamic range than that of the display, we find\nthat the optimized solution boosts the contrast of low-contrast features\nwithout introducing significant artifacts, yielding results of comparable\nvisual quality to current state-of-the art methods with no manual intervention\nor parameter settings. We also examine a variety of other display constraints,\nincluding limitations on minimum luminance (black point), mean luminance (as a\nproxy for energy consumption), and quantized luminance levels (halftoning).\nFinally, we show that the method may be used to enhance details and contrast of\nimages degraded by optical scattering (e.g. fog).\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 21:38:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Laparra", "Valero", ""], ["Berardino", "Alex", ""], ["Ball\u00e9", "Johannes", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1701.07110", "submitter": "Giuseppe Santucci", "authors": "Enrico Bertini and Giuseppe Santucci", "title": "By chance is not enough: Preserving relative density through non uniform\n  sampling", "comments": "Keywords: visual clutter, visual quality metrics, non-uniform\n  sampling", "journal-ref": "Proceedings - Eighth International Conference on Information\n  Visualisation, IV 2004; London; United Kingdom; 14 July 2004 through 16 July\n  2004; Category numberPR02177; Code 63599", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with visualizations containing large data set is a challenging issue\nand, in the field of Information Visualization, almost every visual technique\nreveals its drawback when visualizing large number of items. To deal with this\nproblem we introduce a formal environment, modeling in a virtual space the\nimage features we are interested in (e.g, absolute and relative density,\nclusters, etc.) and we define some metrics able to characterize the image\ndecay. Such metrics drive our automatic techniques (i.e., not uniform sampling)\nrescuing the image features and making them visible to the user. In this paper\nwe focus on 2D scatter-plots, devising a novel non uniform data sampling\nstrategy able to preserve in an effective way relative densities.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 23:35:09 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Bertini", "Enrico", ""], ["Santucci", "Giuseppe", ""]]}, {"id": "1701.07403", "submitter": "Alexander Keller", "authors": "Ken Dahm and Alexander Keller", "title": "Learning Light Transport the Reinforced Way", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the equations of reinforcement learning and light transport\nsimulation are related integral equations. Based on this correspondence, a\nscheme to learn importance while sampling path space is derived. The new\napproach is demonstrated in a consistent light transport simulation algorithm\nthat uses reinforcement learning to progressively learn where light comes from.\nAs using this information for importance sampling includes information about\nvisibility, too, the number of light transport paths with zero contribution is\ndramatically reduced, resulting in much less noisy images within a fixed time\nbudget.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:50:19 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 12:57:10 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Dahm", "Ken", ""], ["Keller", "Alexander", ""]]}, {"id": "1701.08893", "submitter": "Connelly Barnes", "authors": "Eric Risser, Pierre Wilmot, Connelly Barnes", "title": "Stable and Controllable Neural Texture Synthesis and Style Transfer\n  Using Histogram Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, methods have been proposed that perform texture synthesis and style\ntransfer by using convolutional neural networks (e.g. Gatys et al.\n[2015,2016]). These methods are exciting because they can in some cases create\nresults with state-of-the-art quality. However, in this paper, we show these\nmethods also have limitations in texture quality, stability, requisite\nparameter tuning, and lack of user controls. This paper presents a multiscale\nsynthesis pipeline based on convolutional neural networks that ameliorates\nthese issues. We first give a mathematical explanation of the source of\ninstabilities in many previous approaches. We then improve these instabilities\nby using histogram losses to synthesize textures that better statistically\nmatch the exemplar. We also show how to integrate localized style losses in our\nmultiscale framework. These losses can improve the quality of large features,\nimprove the separation of content and style, and offer artistic controls such\nas paint by numbers. We demonstrate that our approach offers improved quality,\nconvergence in fewer iterations, and more stability over the optimization.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 02:37:19 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 23:30:20 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Risser", "Eric", ""], ["Wilmot", "Pierre", ""], ["Barnes", "Connelly", ""]]}]