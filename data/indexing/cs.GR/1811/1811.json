[{"id": "1811.00222", "submitter": "Kaidi Cao", "authors": "Kaidi Cao, Jing Liao, Lu Yuan", "title": "CariGANs: Unpaired Photo-to-Caricature Translation", "comments": "To appear at SIGGRAPH Asia 2018", "journal-ref": "ACM Transactions on Graphics, Vol. 37, No. 6, Article 244.\n  Publication date: November 2018", "doi": "10.1145/3272127.3275046", "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial caricature is an art form of drawing faces in an exaggerated way to\nconvey humor or sarcasm. In this paper, we propose the first Generative\nAdversarial Network (GAN) for unpaired photo-to-caricature translation, which\nwe call \"CariGANs\". It explicitly models geometric exaggeration and appearance\nstylization using two components: CariGeoGAN, which only models the\ngeometry-to-geometry transformation from face photos to caricatures, and\nCariStyGAN, which transfers the style appearance from caricatures to face\nphotos without any geometry deformation. In this way, a difficult cross-domain\ntranslation problem is decoupled into two easier tasks. The perceptual study\nshows that caricatures generated by our CariGANs are closer to the hand-drawn\nones, and at the same time better persevere the identity, compared to\nstate-of-the-art methods. Moreover, our CariGANs allow users to control the\nshape exaggeration degree and change the color/texture style by tuning the\nparameters or giving an example caricature.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 04:39:20 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 03:47:13 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Cao", "Kaidi", ""], ["Liao", "Jing", ""], ["Yuan", "Lu", ""]]}, {"id": "1811.00328", "submitter": "Yu-Hong Yeung", "authors": "Yu-Hong Yeung, Alex Pothen, Jessica Crouch", "title": "AMPS: A Real-time Mesh Cutting Algorithm for Surgical Simulations", "comments": "20 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the AMPS algorithm, a finite element solution method that combines\nprincipal submatrix updates and Schur complement techniques, well-suited for\ninteractive simulations of deformation and cutting of finite element meshes.\nOur approach features real-time solutions to the updated stiffness matrix\nsystems to account for interactive changes in mesh connectivity and boundary\nconditions. Updates are accomplished by an augmented matrix formulation of the\nstiffness equations to maintain its consistency with changes to the underlying\nmodel without refactorization at each timestep. As changes accumulate over\nmultiple simulation timesteps, the augmented solution algorithm enables tens or\nhundreds of updates per second. Acceleration schemes that exploit sparsity,\nmemoization and parallelization lead to the updates being computed in\nreal-time. The complexity analysis and experimental results for this method\ndemonstrate that it scales linearly with the problem size. Results for cutting\nand deformation of 3D elastic models are reported for meshes with node counts\nup to 50,000, and involve models of astigmatism surgery and the brain.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:53:20 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Yeung", "Yu-Hong", ""], ["Pothen", "Alex", ""], ["Crouch", "Jessica", ""]]}, {"id": "1811.00548", "submitter": "Erva Ulu", "authors": "Erva Ulu", "title": "Enhancing the Structural Performance of Additively Manufactured Objects", "comments": "PhD Thesis 2018, Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately quantify the performance an additively manufactured\n(AM) product is important for a widespread industry adoption of AM as the\ndesign is required to: (1) satisfy geometrical constraints, (2) satisfy\nstructural constraints dictated by its intended function, and (3) be cost\neffective compared to traditional manufacturing methods. Optimization\ntechniques offer design aids in creating cost-effective structures that meet\nthe prescribed structural objectives. The fundamental problem in existing\napproaches lies in the difficulty to quantify the structural performance as\neach unique design leads to a new set of analyses to determine the structural\nrobustness and such analyses can be very costly due to the complexity of in-use\nforces experienced by the structure. This work develops computationally\ntractable methods tailored to maximize the structural performance of AM\nproducts. A geometry preserving build orientation optimization method as well\nas data-driven shape optimization approaches to structural design are\npresented. Proposed methods greatly enhance the value of AM technology by\ntaking advantage of the design space enabled by it for a broad class of\nproblems involving complex in-use loads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 17:09:29 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Ulu", "Erva", ""]]}, {"id": "1811.02517", "submitter": "Rajaditya Mukherjee", "authors": "Rajaditya Mukherjee, Qingyang Li, Zhili Chen, Shicheng Chu and Huamin\n  Wang", "title": "NeuralDrop: DNN-based Simulation of Small-Scale Liquid Flows on Solids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small-scale liquid flows on solid surfaces provide convincing details in\nliquid animation, but they are difficult to be simulated with efficiency and\nfidelity, mostly due to the complex nature of the surface tension at the\ncontact front where liquid, air, and solid meet. In this paper, we propose to\nsimulate the dynamics of new liquid drops from captured real-world liquid flow\ndata, using deep neural networks. To achieve this goal, we develop a data\ncapture system that acquires liquid flow patterns from hundreds of real-world\nwater drops. We then convert raw data into compact data for training neural\nnetworks, in which liquid drops are represented by their contact fronts in a\nLagrangian form. Using the LSTM units based on recurrent neural networks, our\nneural networks serve three purposes in our simulator: predicting the contour\nof a contact front, predicting the color field gradient of a contact front, and\nfinally predicting whether a contact front is going to break or not. Using\nthese predictions, our simulator recovers the overall shape of a liquid drop at\nevery time step, and handles merging and splitting events by simple operations.\nThe experiment shows that our trained neural networks are able to perform\npredictions well. The whole simulator is robust, convenient to use, and capable\nof generating realistic small-scale liquid effects in animation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 17:37:25 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Mukherjee", "Rajaditya", ""], ["Li", "Qingyang", ""], ["Chen", "Zhili", ""], ["Chu", "Shicheng", ""], ["Wang", "Huamin", ""]]}, {"id": "1811.02626", "submitter": "J\\'er\\'emie Dumas", "authors": "J\\'er\\'emie Dumas, Jon\\`as Mart\\'inez, Sylvain Lefebvre, Li-Yi Wei", "title": "Printable Aggregate Elements", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Aggregating base elements into rigid objects such as furniture or sculptures\nis a great way for designers to convey a specific look and feel. Unfortunately,\nthere is no existing solution to help model structurally sound aggregates. The\nchallenges stem from the fact that the final shape and its structural\nproperties emerge from the arrangements of the elements, whose sizes are large\nso that they remain easily identifiable. Therefore there is a very tight\ncoupling between the object shape, structural properties, and the precise\nlayout of the elements.\n  We present the first method to create aggregates of elements that are\nstructurally sound and can be manufactured on 3D printers. Rather than having\nto assemble an aggregate shape by painstakingly positioning elements one by\none, users of our method only have to describe the structural purpose of the\ndesired object. This is done by specifying a set of external forces and\nattachment points. The algorithm then automatically optimizes a layout of\nuser-provided elements that answers the specified scenario. The elements can\nhave arbitrary shapes: convex, concave, elongated, and can be allowed to\ndeform.\n  Our approach creates connections between elements through small overlaps\npreserving their appearance, while optimizing for the global rigidity of the\nresulting aggregate. We formulate a topology optimization problem whose design\nvariables are the positions and orientations of individual elements. Global\nrigidity is maximized through a dedicated gradient descent scheme. Due to the\nchallenging setting -- number of elements, arbitrary shapes, orientation, and\nconstraints in 3D -- we propose several novel steps to achieve convergence.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:47:37 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Dumas", "J\u00e9r\u00e9mie", ""], ["Mart\u00ednez", "Jon\u00e0s", ""], ["Lefebvre", "Sylvain", ""], ["Wei", "Li-Yi", ""]]}, {"id": "1811.03151", "submitter": "Kathleen Greene", "authors": "K. Gretchen Greene", "title": "DragonPaint: Rule based bootstrapping for small data with an application\n  to cartoon coloring", "comments": null, "journal-ref": "In Proceedings of the Fourth International Conference on\n  Predictive Applications and APIs, 82, 1-9, Boston, MA, USA, 2018. PMLR", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we confront the problem of deep learning's big labeled data\nrequirements, offer a rule based strategy for extreme augmentation of small\ndata sets and apply that strategy with the image to image translation model by\nIsola et al. (2016) to automate cel style cartoon coloring with very limited\ntraining data. While our experimental results using geometric rules and\ntransformations demonstrate the performance of our methods on an image\ntranslation task with industry applications in art, design and animation, we\nalso propose the use of rules on partial data sets as a generalizable small\ndata strategy, potentially applicable across data types and domains.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 21:23:31 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Greene", "K. Gretchen", ""]]}, {"id": "1811.03374", "submitter": "Alexander Keller", "authors": "Nikolaus Binder and Alexander Keller", "title": "Fast, High Precision Ray/Fiber Intersection using Tight, Disjoint\n  Bounding Volumes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and identifying the shortcomings of current subdivision methods for\nfinding intersections of rays with fibers defined by the surface of a circular\ncontour swept along a B\\'ezier curve, we present a new algorithm that improves\nprecision and performance. Instead of the inefficient pruning using overlapping\naxis aligned bounding boxes and determining the closest point of approach of\nthe ray and the curve, we prune using disjoint bounding volumes defined by\ncylinders and calculate the intersections on the limit surface. This in turn\nallows for computing accurate parametric position and normal in the point of\nintersection. The iteration requires only one bit per subdivision to avoid\ncostly stack memory operations. At a low number of subdivisions, the\nperformance of the high precision algorithm is competitive, while for a high\nnumber of subdivisions it dramatically outperforms the state-of-the-art.\nBesides an extensive mathematical analysis, source code is provided.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 12:21:43 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Binder", "Nikolaus", ""], ["Keller", "Alexander", ""]]}, {"id": "1811.03510", "submitter": "Alexander Keller", "authors": "Nikolaus Binder and Alexander Keller", "title": "Massively Parallel Stackless Ray Tracing of Catmull-Clark Subdivision\n  Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and efficient method for intersecting rays with\nCatmull-Clark subdivision surfaces. It takes advantage of the approximation\ndemocratized by OpenSubdiv, in which regular patches are represented by tensor\nproduct B\\'ezier surfaces and irregular ones are approximated using Gregory\npatches. Our algorithm operates solely on the original patch data and can\nprocess both patch types simultaneously with only a small amount of control\nflow divergence. Besides introducing an optimized method to determine axis\naligned bounding boxes of Gregory patches restricted in the parametric domain,\nseveral techniques are introduced that accelerate the recursive subdivision\nprocess including stackless operation, efficient work distribution, and control\nflow optimizations. The algorithm is especially useful for quick turnarounds\nduring patch editing and animation playback.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:58:40 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Binder", "Nikolaus", ""], ["Keller", "Alexander", ""]]}, {"id": "1811.04337", "submitter": "Hsien-Yu Meng", "authors": "Hsien-Yu Meng, Lin Gao, YuKun Lai, Dinesh Manocha", "title": "VV-Net: Voxel VAE Net with Group Convolutions for Point Cloud\n  Segmentation", "comments": "Accepted by International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for point cloud segmentation. Our approach\ntransforms unstructured point clouds into regular voxel grids, and further uses\na kernel-based interpolated variational autoencoder (VAE) architecture to\nencode the local geometry within each voxel. Traditionally, the voxel\nrepresentation only comprises Boolean occupancy information which fails to\ncapture the sparsely distributed points within voxels in a compact manner. In\norder to handle sparse distributions of points, we further employ radial basis\nfunctions (RBF) to compute a local, continuous representation within each\nvoxel. Our approach results in a good volumetric representation that\neffectively tackles noisy point cloud datasets and is more robust for learning.\nMoreover, we further introduce group equivariant CNN to 3D, by defining the\nconvolution operator on a symmetry group acting on $\\mathbb{Z}^3$ and its\nisomorphic sets. This improves the expressive capacity without increasing\nparameters, leading to more robust segmentation results. We highlight the\nperformance on standard benchmarks and show that our approach outperforms\nstate-of-the-art segmentation algorithms on the ShapeNet and S3DIS datasets.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 03:21:36 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 19:46:28 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Meng", "Hsien-Yu", ""], ["Gao", "Lin", ""], ["Lai", "YuKun", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1811.04756", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla and Sebastian Maisch and Tobias Ritschel and Timo\n  Ropinski", "title": "Deep-learning the Latent Space of Light Transport", "comments": "Eurographics Symposium on Rendering 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a method to directly deep-learn light transport, i. e., the\nmapping from a 3D geometry-illumination-material configuration to a shaded 2D\nimage. While many previous learning methods have employed 2D convolutional\nneural networks applied to images, we show for the first time that light\ntransport can be learned directly in 3D. The benefit of 3D over 2D is, that the\nformer can also correctly capture illumination effects related to occluded\nand/or semi-transparent geometry. To learn 3D light transport, we represent the\n3D scene as an unstructured 3D point cloud, which is later, during rendering,\nprojected to the 2D output image. Thus, we suggest a two-stage operator\ncomprising of a 3D network that first transforms the point cloud into a latent\nrepresentation, which is later on projected to the 2D output image using a\ndedicated 3D-2D network in a second step. We will show that our approach\nresults in improved quality in terms of temporal coherence while retaining most\nof the computational efficiency of common 2D methods. As a consequence, the\nproposed two stage-operator serves as a valuable extension to modern deferred\nshading approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 14:55:58 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 10:22:08 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Maisch", "Sebastian", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "1811.05046", "submitter": "Felix Hamza-Lup", "authors": "Felix Hamza-Lup, Marcel Maghiar", "title": "Web3D Graphics enabled through Sensor Networks for Cost-Effective\n  Assessment and Management of Energy Efficiency in Buildings", "comments": null, "journal-ref": "Graphical Models Journal, Volume 88, 2016, Pages 66-74, ISSN\n  1524-0703", "doi": "10.1016/j.gmod.2016.03.005", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen the advent of numerous building energy efficiency\nvisualization and simulation systems; however, most of them rely on theoretical\nthermal models to suggest building structural design for new constructions and\nmodifications for existing ones. Sustainable methods of construction have made\ntremendous progress. The example of the German Energy-Plus- House technology\nuses a combination of (almost) zero-carbon passive heating technologies. A\nweb-enabled X3D visualization and simulation system coupled with a\ncost-effective set of temperature/humidity sensors can provide valuable\ninsights into building design, materials and construction that can lead to\nsignificant energy savings and an improved thermal comfort for residents,\nresulting in superior building energy efficiency. A cost-effective\nhardware-software prototype system is proposed in this paper that can provide\nreal-time data driven visualization or offline simulation of 3D thermal maps\nfor residential and/or commercial buildings on the Web.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 23:37:46 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Hamza-Lup", "Felix", ""], ["Maghiar", "Marcel", ""]]}, {"id": "1811.05674", "submitter": "Chungang Zhu", "authors": "Ying-Ying Yu, Hui Ma, Chun-Gang Zhu", "title": "Total Positivity of A Kind of Generalized Toric-Bernstein Basis", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized totally positive bases are widely used in many fields.Based on\nthe generalized Vandermonde determinant, the normalized total positivity of a\nkind of generalized toric-Bernstein basis is proved, which is defined on a set\nof real points. By this result, the progressive iterative approximation\nproperty of the generalized toric-B\\'{e}zier curve is obtained.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:02:10 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 01:31:39 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Yu", "Ying-Ying", ""], ["Ma", "Hui", ""], ["Zhu", "Chun-Gang", ""]]}, {"id": "1811.06229", "submitter": "Meng Zhang", "authors": "Meng Zhang and Youyi Zheng", "title": "Hair-GANs: Recovering 3D Hair Structure from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Hair-GANs, an architecture of generative adversarial networks,\nto recover the 3D hair structure from a single image. The goal of our networks\nis to build a parametric transformation from 2D hair maps to 3D hair structure.\nThe 3D hair structure is represented as a 3D volumetric field which encodes\nboth the occupancy and the orientation information of the hair strands. Given a\nsingle hair image, we first align it with a bust model and extract a set of 2D\nmaps encoding the hair orientation information in 2D, along with the bust depth\nmap to feed into our Hair-GANs. With our generator network, we compute the 3D\nvolumetric field as the structure guidance for the final hair synthesis. The\nmodeling results not only resemble the hair in the input image but also\npossesses many vivid details in other views. The efficacy of our method is\ndemonstrated by using a variety of hairstyles and comparing with the prior art.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 08:28:00 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Zhang", "Meng", ""], ["Zheng", "Youyi", ""]]}, {"id": "1811.06600", "submitter": "Qiang Zou", "authors": "Qiang Zou, Jibin Zhao", "title": "Iso-parametric tool path planning for point clouds", "comments": "16 pages, 12 figures", "journal-ref": "Computer-Aided Design 45(11) 2013 1459-1468", "doi": "10.1016/j.cad.2013.07.001", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational consuming and non-robust reconstruction from point clouds\nto either meshes or spline surfaces motivates the direct tool path planning for\npoint clouds. In this paper, a novel approach for planning iso-parametric tool\npath from a point cloud is presented. The planning depends on the\nparameterization of point clouds. Accordingly, a conformal map is employed to\nbuild the parameterization which leads to a significant simplification of\ncomputing tool path parameters and boundary conformed paths. Then, Tool path is\ngenerated through linear interpolation with the forward and side step computed\nagainst specified chord deviation and scallop height, respectively.\nExperimental results are given to illustrate effectiveness of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:48:46 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zou", "Qiang", ""], ["Zhao", "Jibin", ""]]}, {"id": "1811.06896", "submitter": "Marta Nu\\~nez-Garcia", "authors": "Marta Nu\\~nez-Garcia, Gabriel Bernardino, Francisco Alarc\\'on, Gala\n  Caixal, Llu\\'is Mont, Oscar Camara, Constantine Butakoff", "title": "Fast quasi-conformal regional flattening of the left atrium", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-dimensional representation of 3D anatomical structures is a simple and\nintuitive way for analysing patient information across populations and image\nmodalities. It also allows convenient visualizations that can be included in\nclinical reports for a fast overview of the whole structure. While cardiac\nventricles, especially the left ventricle, have an established standard\nrepresentation (e.g. bull's eye plot), the 2D depiction of the left atrium (LA)\nis challenging due to its sub-structural complexity including the pulmonary\nveins (PV) and the left atrial appendage (LAA). Quasi-conformal flattening\ntechniques, successfully applied to cardiac ventricles, require additional\nconstraints in the case of the LA to place the PV and LAA in the same\ngeometrical 2D location for different cases. Some registration-based methods\nhave been proposed but 3D (or 2D) surface registration is time-consuming and\nprone to errors if the geometries are very different. We propose a novel atrial\nflattening methodology where a quasi-conformal 2D map of the LA is obtained\nquickly and without errors related to registration. In our approach, the LA is\ndivided into 5 regions which are then mapped to their analogue two-dimensional\nregions. A dataset of 67 human left atria from magnetic resonance images (MRI)\nwas studied to derive a population-based 2D LA template representing the\naveraged relative locations of the PVs and LAA. The clinical application of the\nproposed methodology is illustrated on different use cases including the\nintegration of MRI and electroanatomical data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 16:36:48 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 13:01:15 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Nu\u00f1ez-Garcia", "Marta", ""], ["Bernardino", "Gabriel", ""], ["Alarc\u00f3n", "Francisco", ""], ["Caixal", "Gala", ""], ["Mont", "Llu\u00eds", ""], ["Camara", "Oscar", ""], ["Butakoff", "Constantine", ""]]}, {"id": "1811.07014", "submitter": "Konstantinos Zampogiannis", "authors": "Konstantinos Zampogiannis, Cornelia Fermuller, Yiannis Aloimonos", "title": "Topology-Aware Non-Rigid Point Cloud Registration", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2019.2940655", "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a non-rigid registration pipeline for pairs of\nunorganized point clouds that may be topologically different. Standard warp\nfield estimation algorithms, even under robust, discontinuity-preserving\nregularization, tend to produce erratic motion estimates on boundaries\nassociated with `close-to-open' topology changes. We overcome this limitation\nby exploiting backward motion: in the opposite motion direction, a\n`close-to-open' event becomes `open-to-close', which is by default handled\ncorrectly. At the core of our approach lies a general, topology-agnostic warp\nfield estimation algorithm, similar to those employed in recently introduced\ndynamic reconstruction systems from RGB-D input. We improve motion estimation\non boundaries associated with topology changes in an efficient post-processing\nphase. Based on both forward and (inverted) backward warp hypotheses, we\nexplicitly detect regions of the deformed geometry that undergo topological\nchanges by means of local deformation criteria and broadly classify them as\n`contacts' or `separations'. Subsequently, the two motion hypotheses are\nseamlessly blended on a local basis, according to the type and proximity of\ndetected events. Our method achieves state-of-the-art motion estimation\naccuracy on the MPI Sintel dataset. Experiments on a custom dataset with\ntopological event annotations demonstrate the effectiveness of our pipeline in\nestimating motion on event boundaries, as well as promising performance in\nexplicit topological event detection.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 20:08:47 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 22:54:07 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 21:19:00 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zampogiannis", "Konstantinos", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1811.07023", "submitter": "Kathleen Greene", "authors": "K. G. Greene", "title": "An Infinite Parade of Giraffes: Expressive Augmentation and Complexity\n  Layers for Cartoon Drawing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore creative image generation constrained by small\ndata. To partially automate the creation of cartoon sketches consistent with a\nspecific designer's style, where acquiring a very large original image set is\nimpossible or cost prohibitive, we exploit domain specific knowledge for a huge\nreduction in original image requirements, creating an effectively infinite\nnumber of cartoon giraffes from just nine original drawings. We introduce\n\"expressive augmentations\" for cartoon sketches, mathematical transformations\nthat create broad domain appropriate variation, far beyond the usual affine\ntransformations, and we show that chained GANs models trained on the temporal\nstages of drawing or \"complexity layers\" can effectively add character\nappropriate details and finish new drawings in the designer's style.\n  We discuss the application of these tools in design processes for textiles,\ngraphics, architectural elements and interior design.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:28:12 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Greene", "K. G.", ""]]}, {"id": "1811.07385", "submitter": "Long Nguyen", "authors": "Tommy Dang, Long Nguyen, Abdullah Karim, Venkatesh Uddameri", "title": "STOAViz: Visualizing Saturated Thickness of Ogallala Aquifer", "comments": null, "journal-ref": null, "doi": "10.2312/envirvis.20171102", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce STOAViz, a visual analytics tool for analyzing\nthe saturated thickness of the Ogallala aquifer. The saturated thicknesses are\nmonitored by sensors integrated on wells distributed on a vast geographic area.\nOur analytics application also captures the trends and patterns (such as\naverage/standard deviation over time, sudden increase/decrease of saturated\nthicknesses) of water on an individual well and a group of wells based on their\ngeographic locations. To highlight the usefulness and effectiveness of STOAViz,\nwe demonstrate it on the Southern High Plains Aquifer of Texas. The work was\ndeveloped using feedback from experts at the water resource center at a\nuniversity. Moreover, our technique can be applied on any geographic areas\nwhere wells and their measurements are available.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 19:27:56 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Dang", "Tommy", ""], ["Nguyen", "Long", ""], ["Karim", "Abdullah", ""], ["Uddameri", "Venkatesh", ""]]}, {"id": "1811.07390", "submitter": "Long Nguyen", "authors": "Long Nguyen, Abdullah Karim", "title": "A Study on 3D Surface Graph Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface graphs have been used in many application domains to represent\nthree-dimensional (3D) data. Another approach to representing 3D data is making\nprojections onto two-dimensional (2D) graphs. This approach will result in\nmultiple displays, which is time-consuming in switching between different\nscreens for a different perspective. In this work, we study the performance of\n3D version of popular 2D visualization techniques for time series: horizon\ngraph, small multiple, and simple line graph. We explore discrimination tasks\nwith respect to each visualization technique that requires simultaneous\nrepresentations. We demonstrate our study by visualizing saturated thickness of\nthe Ogallala aquifer - the Southern High Plains Aquifer of Texas in multiple\nyears. For the evaluation, we design comparison and discrimination tasks and\nautomatically record result performed by a group of students at a university.\nOur results show that 3D small multiples perform well with stable accuracy over\nnumbers of occurrences. On the other hand, shared-space visualization within a\nsingle 3D coordinate system is more efficient with small number of simultaneous\ngraphs. 3D horizon graph loses its competence in the 3D coordinate system with\nthe lowest accuracy comparing to other techniques. Our demonstration of 3D\nspatial-temporal is also presented on the Southern High Plains Aquifer of Texas\nfrom 2010 to 2016.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 20:01:04 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nguyen", "Long", ""], ["Karim", "Abdullah", ""]]}, {"id": "1811.07441", "submitter": "Nadav Schor", "authors": "Nadav Schor, Oren Katzir, Hao Zhang and Daniel Cohen-Or", "title": "CompoNet: Learning to Generate the Unseen by Part Synthesis and\n  Composition", "comments": "Accepted to ICCV 2019. Code: https://github.com/nschor/CompoNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven generative modeling has made remarkable progress by leveraging\nthe power of deep neural networks. A reoccurring challenge is how to enable a\nmodel to generate a rich variety of samples from the entire target\ndistribution, rather than only from a distribution confined to the training\ndata. In other words, we would like the generative model to go beyond the\nobserved samples and learn to generate ``unseen'', yet still plausible, data.\nIn our work, we present CompoNet, a generative neural network for 2D or 3D\nshapes that is based on a part-based prior, where the key idea is for the\nnetwork to synthesize shapes by varying both the shape parts and their\ncompositions. Treating a shape not as an unstructured whole, but as a\n(re-)composable set of deformable parts, adds a combinatorial dimension to the\ngenerative process to enrich the diversity of the output, encouraging the\ngenerator to venture more into the ``unseen''. We show that our part-based\nmodel generates richer variety of plausible shapes compared with baseline\ngenerative models. To this end, we introduce two quantitative metrics to\nevaluate the diversity of a generative model and assess how well the generated\ndata covers both the training data and unseen data from the same target\ndistribution. Code is available at https://github.com/nschor/CompoNet.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:45:17 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 23:26:45 GMT"}, {"version": "v3", "created": "Sun, 4 Aug 2019 08:25:20 GMT"}, {"version": "v4", "created": "Sun, 1 Sep 2019 19:30:51 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Schor", "Nadav", ""], ["Katzir", "Oren", ""], ["Zhang", "Hao", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1811.07580", "submitter": "Qiang Zou", "authors": "Qiang Zou, Juyong Zhang, Bailin Deng, Jibin Zhao", "title": "Iso-level tool path planning for free-form surfaces", "comments": "Journal paper, 11 figures", "journal-ref": "Computer-Aided Design 53 (2014): 117-125", "doi": "10.1016/j.cad.2014.04.006", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of tool path planning is to maximize the efficiency against some\ngiven precision criteria. In practice, scallop height should be kept constant\nto avoid unnecessary cutting, while the tool path should be smooth enough to\nmaintain a high feed rate. However, iso-scallop and smoothness often conflict\nwith each other. Existing methods smooth iso-scallop paths one-by-one, which\nmake the final tool path far from being globally optimal. This paper proposes a\nnew framework for tool path optimization. It views a family of iso-level curves\nof a scalar function defined over the surface as tool path so that desired tool\npath can be generated by finding the function that minimizes certain energy\nfunctional and different objectives can be considered simultaneously. We use\nthe framework to plan globally optimal tool path with respect to iso-scallop\nand smoothness. The energy functionals for planning iso-scallop, smoothness,\nand optimal tool path are respectively derived, and the path topology is\nstudied too. Experimental results are given to show the effectiveness of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 09:49:39 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zou", "Qiang", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""], ["Zhao", "Jibin", ""]]}, {"id": "1811.07791", "submitter": "David Fuentes-Jimenez", "authors": "David Fuentes-Jimenez, David Casillas-Perez, Daniel Pizarro, Toby\n  Collins and Adrien Bartoli", "title": "Deep Shape-from-Template: Wide-Baseline, Dense and Fast Registration and\n  Deformable Reconstruction from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Deep Shape-from-Template (DeepSfT), a novel Deep Neural Network\n(DNN) method for solving real-time automatic registration and 3D reconstruction\nof a deformable object viewed in a single monocular image.DeepSfT advances the\nstate-of-the-art in various aspects. Compared to existing DNN SfT methods, it\nis the first fully convolutional real-time approach that handles an arbitrary\nobject geometry, topology and surface representation. It also does not require\nground truth registration with real data and scales well to very complex object\nmodels with large numbers of elements. Compared to previous non-DNN SfT\nmethods, it does not involve numerical optimization at run-time, and is a\ndense, wide-baseline solution that does not demand, and does not suffer from,\nfeature-based matching. It is able to process a single image with significant\ndeformation and viewpoint changes, and handles well the core challenges of\nocclusions, weak texture and blur. DeepSfT is based on residual encoder-decoder\nstructures and refining blocks. It is trained end-to-end with a novel\ncombination of supervised learning from simulated renderings of the object\nmodel and semi-supervised automatic fine-tuning using real data captured with a\nstandard RGB-D camera. The cameras used for fine-tuning and run-time can be\ndifferent, making DeepSfT practical for real-world use. We show that DeepSfT\nsignificantly outperforms state-of-the-art wide-baseline approaches for\nnon-trivial templates, with quantitative and qualitative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 16:39:27 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 15:13:33 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 03:12:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fuentes-Jimenez", "David", ""], ["Casillas-Perez", "David", ""], ["Pizarro", "Daniel", ""], ["Collins", "Toby", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1811.08004", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Shiyang Cheng and Maja Pantic and Stefanos\n  Zafeiriou", "title": "Photorealistic Facial Synthesis in the Dimensional Affect Space", "comments": "arXiv admin note: substantial text overlap with arXiv:1811.05027", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for synthesizing facial affect, which is\nbased on our annotating 600,000 frames of the 4DFAB database in terms of\nvalence and arousal. The input of this approach is a pair of these emotional\nstate descriptors and a neutral 2D image of a person to whom the corresponding\naffect will be synthesized. Given this target pair, a set of 3D facial meshes\nis selected, which is used to build a blendshape model and generate the new\nfacial affect. To synthesize the affect on the 2D neutral image, 3DMM fitting\nis performed and the reconstructed face is deformed to generate the target\nfacial expressions. Last, the new face is rendered into the original image.\nBoth qualitative and quantitative experimental studies illustrate the\ngeneration of realistic images, when the neutral image is sampled from a\nvariety of well known databases, such as the Aff-Wild, AFEW, Multi-PIE,\nAFEW-VA, BU-3DFE, Bosphorus.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 01:30:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Cheng", "Shiyang", ""], ["Pantic", "Maja", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1811.08053", "submitter": "Felix Hamza-Lup", "authors": "Neha R. Hippalgaonkar, Alexa D. Sider, Felix G. Hamza-Lup, Anand P.\n  Santhanam, Bala Jaganathan, Celina Imielinska, Jannick P. Rolland", "title": "Generating Classes of 3D Virtual Mandibles for AR-Based Medical\n  Simulation", "comments": null, "journal-ref": "Journal of the Society for Simulation in Healthcare (2008), vol.\n  3(2), pp. 103-110", "doi": "10.1097/SIH.0b013e31816b5d54", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation and modeling represent promising tools for several application\ndomains from engineering to forensic science and medicine. Advances in 3D\nimaging technology convey paradigms such as augmented reality (AR) and mixed\nreality inside promising simulation tools for the training industry. Motivated\nby the requirement for superimposing anatomically correct 3D models on a Human\nPatient Simulator (HPS) and visualizing them in an AR environment, the purpose\nof this research effort is to derive method for scaling a source human mandible\nto a target human mandible. Results show that, given a distance between two\nsame landmarks on two different mandibles, a relative scaling factor may be\ncomputed. Using this scaling factor, results show that a 3D virtual mandible\nmodel can be made morphometrically equivalent to a real target-specific\nmandible within a 1.30 millimeter average error bound. The virtual mandible may\nbe further used as a reference target for registering other anatomical models,\nsuch as the lungs, on the HPS. Such registration will be made possible by\nphysical constraints among the mandible and the spinal column in the horizontal\nnormal rest position.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 03:29:56 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Hippalgaonkar", "Neha R.", ""], ["Sider", "Alexa D.", ""], ["Hamza-Lup", "Felix G.", ""], ["Santhanam", "Anand P.", ""], ["Jaganathan", "Bala", ""], ["Imielinska", "Celina", ""], ["Rolland", "Jannick P.", ""]]}, {"id": "1811.08170", "submitter": "Lei Li", "authors": "Lei Li, Changqing Zou, Youyi Zheng, Qingkun Su, Hongbo Fu, Chiew-Lan\n  Tai", "title": "Sketch-R2CNN: An Attentive Network for Vector Sketch Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Freehand sketching is a dynamic process where points are sequentially sampled\nand grouped as strokes for sketch acquisition on electronic devices. To\nrecognize a sketched object, most existing methods discard such important\ntemporal ordering and grouping information from human and simply rasterize\nsketches into binary images for classification. In this paper, we propose a\nnovel single-branch attentive network architecture RNN-Rasterization-CNN\n(Sketch-R2CNN for short) to fully leverage the dynamics in sketches for\nrecognition. Sketch-R2CNN takes as input only a vector sketch with grouped\nsequences of points, and uses an RNN for stroke attention estimation in the\nvector space and a CNN for 2D feature extraction in the pixel space\nrespectively. To bridge the gap between these two spaces in neural networks, we\npropose a neural line rasterization module to convert the vector sketch along\nwith the attention estimated by RNN into a bitmap image, which is subsequently\nconsumed by CNN. The neural line rasterization module is designed in a\ndifferentiable way to yield a unified pipeline for end-to-end learning. We\nperform experiments on existing large-scale sketch recognition benchmarks and\nshow that by exploiting the sketch dynamics with the attention mechanism, our\nmethod is more robust and achieves better performance than the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 10:44:34 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Li", "Lei", ""], ["Zou", "Changqing", ""], ["Zheng", "Youyi", ""], ["Su", "Qingkun", ""], ["Fu", "Hongbo", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "1811.08180", "submitter": "Ning Yu", "authors": "Ning Yu, Larry Davis, Mario Fritz", "title": "Attributing Fake Images to GANs: Learning and Analyzing GAN Fingerprints", "comments": "Accepted to ICCV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nincreasing success in generating photorealistic images. But they also raise\nchallenges to visual forensics and model attribution. We present the first\nstudy of learning GAN fingerprints towards image attribution and using them to\nclassify an image as real or GAN-generated. For GAN-generated images, we\nfurther identify their sources. Our experiments show that (1) GANs carry\ndistinct model fingerprints and leave stable fingerprints in their generated\nimages, which support image attribution; (2) even minor differences in GAN\ntraining can result in different fingerprints, which enables fine-grained model\nauthentication; (3) fingerprints persist across different image frequencies and\npatches and are not biased by GAN artifacts; (4) fingerprint finetuning is\neffective in immunizing against five types of adversarial image perturbations;\nand (5) comparisons also show our learned fingerprints consistently outperform\nseveral baselines in a variety of setups.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:11:21 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 13:19:40 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 17:11:32 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Yu", "Ning", ""], ["Davis", "Larry", ""], ["Fritz", "Mario", ""]]}, {"id": "1811.10036", "submitter": "Gustavo Patow", "authors": "O. Rogla, N. Pelechano, G. Patow", "title": "Procedural Crowd Generation for Semantically Augmented Virtual Cities", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authoring realistic behaviors to populate a large virtual city can be a\ncumbersome, time-consuming and error-prone task. Believable crowds require the\neffort of storytellers and programming experts working together for long\nperiods of time. In this work, we present a new framework to allow users to\ngenerate populated environments in an easier and faster way, by relying on the\nuse of procedural techniques. Our framework consists of the procedural\ngeneration of semantically-augmented virtual cities to drive the procedural\ngeneration and simulation of crowds. The main novelty lies in the generation of\nagendas for each individual inhabitant (alone or as part of a family) by using\na rule-based grammar that combines city semantics with the autonomous persons'\ncharacteristics. Real-world data can be used to accommodate the generation of a\nvirtual population, thus enabling the recreation of more realistic scenarios.\nUsers can author a new population or city by editing rule files with the\nflexibility of re-using, combining or extending the rules of previous\npopulations. The results show how logical and consistent behaviors can be\neasily generated for a large crowd providing a good starting point to bring\nvirtual cities to life.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 15:33:34 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Rogla", "O.", ""], ["Pelechano", "N.", ""], ["Patow", "G.", ""]]}, {"id": "1811.10121", "submitter": "Abhishek Sharma", "authors": "Abhishek Sharma", "title": "Foreground Clustering for Joint Segmentation and Localization in Videos\n  and Images", "comments": "In Proceedings of NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework in which video/image segmentation and\nlocalization are cast into a single optimization problem that integrates\ninformation from low level appearance cues with that of high level localization\ncues in a very weakly supervised manner. The proposed framework leverages two\nrepresentations at different levels, exploits the spatial relationship between\nbounding boxes and superpixels as linear constraints and simultaneously\ndiscriminates between foreground and background at bounding box and superpixel\nlevel. Different from previous approaches that mainly rely on discriminative\nclustering, we incorporate a foreground model that minimizes the histogram\ndifference of an object across all image frames. Exploiting the geometric\nrelation between the superpixels and bounding boxes enables the transfer of\nsegmentation cues to improve localization output and vice-versa. Inclusion of\nthe foreground model generalizes our discriminative framework to video data\nwhere the background tends to be similar and thus, not discriminative. We\ndemonstrate the effectiveness of our unified framework on the YouTube Object\nvideo dataset, Internet Object Discovery dataset and Pascal VOC 2007.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:00:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Sharma", "Abhishek", ""]]}, {"id": "1811.10175", "submitter": "Zongyi Xu", "authors": "Zongyi Xu, Qianni Zhang, Shiyang Cheng", "title": "Multilevel active registration for kinect human body scans: from low\n  quality to high quality", "comments": "14 pages, the Journal of Multimedia Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration of 3D human body has been a challenging research topic for over\ndecades. Most of the traditional human body registration methods require manual\nassistance, or other auxiliary information such as texture and markers. The\nmajority of these methods are tailored for high-quality scans from expensive\nscanners. Following the introduction of the low-quality scans from\ncost-effective devices such as Kinect, the 3D data capturing of human body\nbecomes more convenient and easier. However, due to the inevitable holes,\nnoises and outliers in the low-quality scan, the registration of human body\nbecomes even more challenging. To address this problem, we propose a fully\nautomatic active registration method which deforms a high-resolution template\nmesh to match the low-quality human body scans. Our registration method\noperates on two levels of statistical shape models: (1) the first level is a\nholistic body shape model that defines the basic figure of human; (2) the\nsecond level includes a set of shape models for every body part, aiming at\ncapturing more body details. Our fitting procedure follows a coarse-to-fine\napproach that is robust and efficient. Experiments show that our method is\ncomparable with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 04:22:54 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Xu", "Zongyi", ""], ["Zhang", "Qianni", ""], ["Cheng", "Shiyang", ""]]}, {"id": "1811.10352", "submitter": "Zhijie Wu", "authors": "Zhijie Wu and Chunjin Song and Yang Zhou and Minglun Gong and Hui\n  Huang", "title": "EFANet: Exchangeable Feature Alignment Network for Arbitrary Style\n  Transfer", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer has been an important topic both in computer vision and\ngraphics. Since the seminal work of Gatys et al. first demonstrates the power\nof stylization through optimization in the deep feature space, quite a few\napproaches have achieved real-time arbitrary style transfer with\nstraightforward statistic matching techniques. In this work, our key\nobservation is that only considering features in the input style image for the\nglobal deep feature statistic matching or local patch swap may not always\nensure a satisfactory style transfer; see e.g., Figure 1. Instead, we propose a\nnovel transfer framework, EFANet, that aims to jointly analyze and better align\nexchangeable features extracted from content and style image pair. In this way,\nthe style features from the style image seek for the best compatibility with\nthe content information in the content image, leading to more structured\nstylization results. In addition, a new whitening loss is developed for\npurifying the computed content features and better fusion with styles in\nfeature space. Qualitative and quantitative experiments demonstrate the\nadvantages of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 13:15:23 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 12:16:25 GMT"}, {"version": "v3", "created": "Sat, 21 Dec 2019 18:36:11 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Wu", "Zhijie", ""], ["Song", "Chunjin", ""], ["Zhou", "Yang", ""], ["Gong", "Minglun", ""], ["Huang", "Hui", ""]]}, {"id": "1811.10597", "submitter": "David Bau iii", "authors": "David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B.\n  Tenenbaum, William T. Freeman, Antonio Torralba", "title": "GAN Dissection: Visualizing and Understanding Generative Adversarial\n  Networks", "comments": "18 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved impressive\nresults for many real-world applications, and many GAN variants have emerged\nwith improvements in sample quality and training stability. However, they have\nnot been well visualized or understood. How does a GAN represent our visual\nworld internally? What causes the artifacts in GAN results? How do\narchitectural choices affect GAN learning? Answering such questions could\nenable us to develop new insights and better models.\n  In this work, we present an analytic framework to visualize and understand\nGANs at the unit-, object-, and scene-level. We first identify a group of\ninterpretable units that are closely related to object concepts using a\nsegmentation-based network dissection method. Then, we quantify the causal\neffect of interpretable units by measuring the ability of interventions to\ncontrol objects in the output. We examine the contextual relationship between\nthese units and their surroundings by inserting the discovered object concepts\ninto new images. We show several practical applications enabled by our\nframework, from comparing internal representations across different layers,\nmodels, and datasets, to improving GANs by locating and removing\nartifact-causing units, to interactively manipulating objects in a scene. We\nprovide open source interpretation tools to help researchers and practitioners\nbetter understand their GAN models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 18:59:07 GMT"}, {"version": "v2", "created": "Sat, 8 Dec 2018 22:56:10 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bau", "David", ""], ["Zhu", "Jun-Yan", ""], ["Strobelt", "Hendrik", ""], ["Zhou", "Bolei", ""], ["Tenenbaum", "Joshua B.", ""], ["Freeman", "William T.", ""], ["Torralba", "Antonio", ""]]}, {"id": "1811.10943", "submitter": "Francis Williams", "authors": "Francis Williams, Teseo Schneider, Claudio Silva, Denis Zorin, Joan\n  Bruna, Daniele Panozzo", "title": "Deep Geometric Prior for Surface Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a discrete surface from a point cloud is a fundamental\ngeometry processing problem that has been studied for decades, with many\nmethods developed. We propose the use of a deep neural network as a geometric\nprior for surface reconstruction. Specifically, we overfit a neural network\nrepresenting a local chart parameterization to part of an input point cloud\nusing the Wasserstein distance as a measure of approximation. By jointly\nfitting many such networks to overlapping parts of the point cloud, while\nenforcing a consistency condition, we compute a manifold atlas. By sampling\nthis atlas, we can produce a dense reconstruction of the surface approximating\nthe input cloud. The entire procedure does not require any training data or\nexplicit regularization, yet, we show that it is able to perform remarkably\nwell: not introducing typical overfitting artifacts, and approximating sharp\nfeatures closely at the same time. We experimentally show that this geometric\nprior produces good results for both man-made objects containing sharp features\nand smoother organic objects, as well as noisy inputs. We compare our method\nwith a number of well-known reconstruction methods on a standard surface\nreconstruction benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 12:50:46 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 00:31:43 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Williams", "Francis", ""], ["Schneider", "Teseo", ""], ["Silva", "Claudio", ""], ["Zorin", "Denis", ""], ["Bruna", "Joan", ""], ["Panozzo", "Daniele", ""]]}, {"id": "1811.11155", "submitter": "Krishna Kumar Singh", "authors": "Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "title": "FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained\n  Object Generation and Discovery", "comments": null, "journal-ref": "CVPR 2019 (Oral Presentation)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FineGAN, a novel unsupervised GAN framework, which disentangles\nthe background, object shape, and object appearance to hierarchically generate\nimages of fine-grained object categories. To disentangle the factors without\nsupervision, our key idea is to use information theory to associate each factor\nto a latent code, and to condition the relationships between the codes in a\nspecific way to induce the desired hierarchy. Through extensive experiments, we\nshow that FineGAN achieves the desired disentanglement to generate realistic\nand diverse images belonging to fine-grained classes of birds, dogs, and cars.\nUsing FineGAN's automatically learned features, we also cluster real images as\na first attempt at solving the novel problem of unsupervised fine-grained\nobject category discovery. Our code/models/demo can be found at\nhttps://github.com/kkanshul/finegan\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 18:44:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:44:24 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Singh", "Krishna Kumar", ""], ["Ojha", "Utkarsh", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1811.11286", "submitter": "Wang Yifan", "authors": "Wang Yifan, Shihao Wu, Hui Huang, Daniel Cohen-Or, Olga\n  Sorkine-Hornung", "title": "Patch-based Progressive 3D Point Set Upsampling", "comments": "accepted to cvpr2019, code available at https://github.com/yifita/P3U", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a detail-driven deep neural network for point set upsampling. A\nhigh-resolution point set is essential for point-based rendering and surface\nreconstruction. Inspired by the recent success of neural image super-resolution\ntechniques, we progressively train a cascade of patch-based upsampling networks\non different levels of detail end-to-end. We propose a series of architectural\ndesign contributions that lead to a substantial performance boost. The effect\nof each technical contribution is demonstrated in an ablation study.\nQualitative and quantitative experiments show that our method significantly\noutperforms the state-of-the-art learning-based and optimazation-based\napproaches, both in terms of handling low-resolution inputs and revealing\nhigh-fidelity details.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 22:01:55 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 12:42:31 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 17:08:50 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Yifan", "Wang", ""], ["Wu", "Shihao", ""], ["Huang", "Hui", ""], ["Cohen-Or", "Daniel", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1811.11465", "submitter": "Arianna Rampini", "authors": "Luca Cosmo, Mikhail Panine, Arianna Rampini, Maks Ovsjanikov, Michael\n  M. Bronstein, Emanuele Rodol\\`a", "title": "Isospectralization, or how to hear shape, style, and correspondence", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": "10.1109/CVPR.2019.00771", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question whether one can recover the shape of a geometric object from its\nLaplacian spectrum ('hear the shape of the drum') is a classical problem in\nspectral geometry with a broad range of implications and applications. While\ntheoretically the answer to this question is negative (there exist examples of\niso-spectral but non-isometric manifolds), little is known about the practical\npossibility of using the spectrum for shape reconstruction and optimization. In\nthis paper, we introduce a numerical procedure called isospectralization,\nconsisting of deforming one shape to make its Laplacian spectrum match that of\nanother. We implement the isospectralization procedure using modern\ndifferentiable programming techniques and exemplify its applications in some of\nthe classical and notoriously hard problems in geometry processing, computer\nvision, and graphics such as shape reconstruction, pose and style transfer, and\ndense deformable correspondence.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 09:51:28 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 11:17:46 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Cosmo", "Luca", ""], ["Panine", "Mikhail", ""], ["Rampini", "Arianna", ""], ["Ovsjanikov", "Maks", ""], ["Bronstein", "Michael M.", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "1811.11482", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Image Reconstruction with Predictive Filter Flow", "comments": "https://www.ics.uci.edu/~skong2/pff.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, interpretable framework for solving a wide range of\nimage reconstruction problems such as denoising and deconvolution. Given a\ncorrupted input image, the model synthesizes a spatially varying linear filter\nwhich, when applied to the input image, reconstructs the desired output. The\nmodel parameters are learned using supervised or self-supervised training. We\ntest this model on three tasks: non-uniform motion blur removal,\nlossy-compression artifact reduction and single image super resolution. We\ndemonstrate that our model substantially outperforms state-of-the-art methods\non all these tasks and is significantly faster than optimization-based\napproaches to deconvolution. Unlike models that directly predict output pixel\nvalues, the predicted filter flow is controllable and interpretable, which we\ndemonstrate by visualizing the space of predicted filters for different tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 10:17:14 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1811.11606", "submitter": "Philipp Henzler", "authors": "Philipp Henzler, Niloy Mitra, Tobias Ritschel", "title": "Escaping Plato's Cave: 3D Shape From Adversarial Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PlatonicGAN to discover the 3D structure of an object class from\nan unstructured collection of 2D images, i.e., where no relation between photos\nis known, except that they are showing instances of the same category. The key\nidea is to train a deep neural network to generate 3D shapes which, when\nrendered to images, are indistinguishable from ground truth images (for a\ndiscriminator) under various camera poses. Discriminating 2D images instead of\n3D shapes allows tapping into unstructured 2D photo collections instead of\nrelying on curated (e.g., aligned, annotated, etc.) 3D data sets. To establish\nconstraints between 2D image observation and their 3D interpretation, we\nsuggest a family of rendering layers that are effectively differentiable. This\nfamily includes visual hull, absorption-only (akin to x-ray), and\nemission-absorption. We can successfully reconstruct 3D shapes from\nunstructured 2D images and extensively evaluate PlatonicGAN on a range of\nsynthetic and real data sets achieving consistent improvements over baseline\nmethods. We further show that PlatonicGAN can be combined with 3D supervision\nto improve on and in some cases even surpass the quality of 3D-supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 14:58:22 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 09:37:44 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2019 15:04:04 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 09:17:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Henzler", "Philipp", ""], ["Mitra", "Niloy", ""], ["Ritschel", "Tobias", ""]]}, {"id": "1811.11745", "submitter": "Tim Brooks", "authors": "Tim Brooks, Jonathan T. Barron", "title": "Learning to Synthesize Motion Blur", "comments": "http://timothybrooks.com/tech/motion-blur/ . IEEE Conference on\n  Computer Vision and Pattern Recognition (CVPR), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a technique for synthesizing a motion blurred image from a pair of\nunblurred images captured in succession. To build this system we motivate and\ndesign a differentiable \"line prediction\" layer to be used as part of a neural\nnetwork architecture, with which we can learn a system to regress from image\npairs to motion blurred images that span the capture time of the input image\npair. Training this model requires an abundance of data, and so we design and\nexecute a strategy for using frame interpolation techniques to generate a\nlarge-scale synthetic dataset of motion blurred images and their respective\ninputs. We additionally capture a high quality test set of real motion blurred\nimages, synthesized from slow motion videos, with which we evaluate our model\nagainst several baseline techniques that can be used to synthesize motion blur.\nOur model produces higher accuracy output than our baselines, and is\nsignificantly faster than baselines with competitive accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 21:55:55 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 07:28:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Brooks", "Tim", ""], ["Barron", "Jonathan T.", ""]]}, {"id": "1811.11941", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Shane Farrar, Erik Leon", "title": "Interactive X-ray and proton therapy training and simulation", "comments": null, "journal-ref": "International Journal on Computer Assisted Radiology and Surgery,\n  Vol. 10(10) (2015), pp. 1675-1683", "doi": "10.1007/s11548-015-1229-7", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External beam X-ray therapy (XRT) and proton therapy (PT) are effective and\nwidely accepted forms of treatment for many types of cancer. However, the\nprocedures require extensive computerized planning. Current planning systems\nfor both XRT and PT have insufficient visual aid to combine real patient data\nwith the treatment device geometry to account for unforeseen collisions among\nsystem components and the patient. We are proposing a cost-effective method to\nextract patient specific S-reps in real time, and combine them with the\ntreatment system geometry to provide a comprehensive simulation of the XRT/PT\ntreatment room. The X3D standard is used to implement and deploy the simulator\non the web, enabling its use not only for remote specialists' collaboration,\nsimulation, and training, but also for patient education.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:24:03 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Farrar", "Shane", ""], ["Leon", "Erik", ""]]}, {"id": "1811.11947", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Ivan Sopin, Omar Zeidan", "title": "Online External Beam Radiation Treatment Simulator", "comments": null, "journal-ref": "International Journal of Computer Assisted Radiology and Surgery\n  (2008), Vol. 3(4), pp. 275-281", "doi": "10.1007/s11548-008-0232-7", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiation therapy is an effective and widely accepted form of treatment for\nmany types of cancer that requires extensive computerized planning.\nUnfortunately, current treatment planning systems have limited or no visual aid\nthat combines patient volumetric models extracted from patient-specific CT data\nwith the treatment device geometry in a 3D interactive simulation. We\nillustrate the potential of 3D simulation in radiation therapy with a web-based\ninteractive system that combines novel standards and technologies. We discuss\nrelated research efforts in this area and present in detail several components\nof the simulator. An objective assessment of the accuracy of the simulator and\na usability study prove the potential of such a system for simulation and\ntraining.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:54:30 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Sopin", "Ivan", ""], ["Zeidan", "Omar", ""]]}, {"id": "1811.12373", "submitter": "Ke Li", "authors": "Ke Li, Tianhao Zhang, Jitendra Malik", "title": "Diverse Image Synthesis from Semantic Layouts via Conditional IMLE", "comments": "18 pages, 16 figures; IEEE International Conference on Computer\n  Vision (ICCV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing methods for conditional image synthesis are only able to\ngenerate a single plausible image for any given input, or at best a fixed\nnumber of plausible images. In this paper, we focus on the problem of\ngenerating images from semantic segmentation maps and present a simple new\nmethod that can generate an arbitrary number of images with diverse appearance\nfor the same semantic layout. Unlike most existing approaches which adopt the\nGAN framework, our method is based on the recently introduced Implicit Maximum\nLikelihood Estimation (IMLE) framework. Compared to the leading approach, our\nmethod is able to generate more diverse images while producing fewer artifacts\ndespite using the same architecture. The learned latent space also has sensible\nstructure despite the lack of supervision that encourages such behaviour.\nVideos and code are available at\nhttps://people.eecs.berkeley.edu/~ke.li/projects/imle/scene_layouts/.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 18:36:00 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 17:54:53 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Ke", ""], ["Zhang", "Tianhao", ""], ["Malik", "Jitendra", ""]]}, {"id": "1811.12463", "submitter": "Kai Wang", "authors": "Daniel Ritchie, Kai Wang, Yu-an Lin", "title": "Fast and Flexible Indoor Scene Synthesis via Deep Convolutional\n  Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, fast and flexible pipeline for indoor scene synthesis that\nis based on deep convolutional generative models. Our method operates on a\ntop-down image-based representation, and inserts objects iteratively into the\nscene by predicting their category, location, orientation and size with\nseparate neural network modules. Our pipeline naturally supports automatic\ncompletion of partial scenes, as well as synthesis of complete scenes. Our\nmethod is significantly faster than the previous image-based method and\ngenerates result that outperforms it and other state-of-the-art deep generative\nscene models in terms of faithfulness to training data and perceived visual\nquality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:03:28 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ritchie", "Daniel", ""], ["Wang", "Kai", ""], ["Lin", "Yu-an", ""]]}, {"id": "1811.12464", "submitter": "Adam White", "authors": "Adam R White, Li Bai", "title": "Increasing the Capability of Neural Networks for Surface Reconstruction\n  from Noisy Point Clouds", "comments": "8 pages, quantitative and qualitative results of comparisons between\n  ANN regression methods incorporating Isomap vs LLE, bSpline, multi-depth\n  point cloud sampling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds upon the current methods to increase their capability and\nautomation for 3D surface construction from noisy and potentially sparse point\nclouds. It presents an analysis of an artificial neural network surface\nregression and mapping method, describing caveats, improvements and\njustification for the different approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:12:26 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:08:15 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["White", "Adam R", ""], ["Bai", "Li", ""]]}, {"id": "1811.12517", "submitter": "Daniel J\\\"onsson", "authors": "Daniel J\\\"onsson, Peter Steneteg, Erik Sund\\'en, Rickard Englund,\n  Sathish Kottravel, Martin Falk, Anders Ynnerman, Ingrid Hotz, Timo Ropinski", "title": "Inviwo -- A Visualization System with Usage Abstraction Levels", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2920639", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of today's visualization applications demands specific\nvisualization systems tailored for the development of these applications.\nFrequently, such systems utilize levels of abstraction to improve the\napplication development process, for instance by providing a data flow network\neditor. Unfortunately, these abstractions result in several issues, which need\nto be circumvented through an abstraction-centered system design. Often, a high\nlevel of abstraction hides low level details, which makes it difficult to\ndirectly access the underlying computing platform, which would be important to\nachieve an optimal performance. Therefore, we propose a layer structure\ndeveloped for modern and sustainable visualization systems allowing developers\nto interact with all contained abstraction levels. We refer to this interaction\ncapabilities as usage abstraction levels, since we target application\ndevelopers with various levels of experience. We formulate the requirements for\nsuch a system, derive the desired architecture, and present how the concepts\nhave been exemplary realized within the Inviwo visualization system.\nFurthermore, we address several specific challenges that arise during the\nrealization of such a layered architecture, such as communication between\ndifferent computing platforms, performance centered encapsulation, as well as\nlayer-independent development by supporting cross layer documentation and\ndebugging capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:27:20 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 18:40:31 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["J\u00f6nsson", "Daniel", ""], ["Steneteg", "Peter", ""], ["Sund\u00e9n", "Erik", ""], ["Englund", "Rickard", ""], ["Kottravel", "Sathish", ""], ["Falk", "Martin", ""], ["Ynnerman", "Anders", ""], ["Hotz", "Ingrid", ""], ["Ropinski", "Timo", ""]]}, {"id": "1811.12543", "submitter": "Rickard Br\\\"uel Gabrielsson", "authors": "Rickard Br\\\"uel-Gabrielsson, Vignesh Ganapathi-Subramanian, Primoz\n  Skraba, Leonidas J. Guibas", "title": "Topology-Aware Surface Reconstruction for Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to inform the reconstruction of a surface from a point\nscan through topological priors. The reconstruction is based on basis functions\nwhich are optimized to provide a good fit to the point scan while satisfying\npredefined topological constraints. We optimize the parameters of a model to\nobtain likelihood function over the reconstruction domain. The topological\nconstraints are captured by persistence diagrams which are incorporated in the\noptimization algorithm promote the correct topology. The result is a novel\ntopology-aware technique which can: 1.) weed out topological noise from point\nscans, and 2.) capture certain nuanced properties of the underlying shape which\ncould otherwise be lost while performing surface reconstruction. We showcase\nresults reconstructing shapes with multiple potential topologies, compare to\nother classical surface construction techniques, and show the completion of\nreal scan data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 23:55:13 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:49:55 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Br\u00fcel-Gabrielsson", "Rickard", ""], ["Ganapathi-Subramanian", "Vignesh", ""], ["Skraba", "Primoz", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1811.12597", "submitter": "Hongwei Lin", "authors": "Hongwei Lin, Hao Huang, Chuanfeng Hu", "title": "Constructing Trivariate B-splines with Positive Jacobian by Pillow\n  Operation and Geometric Iterative Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of isogeometric analysis has prompted a need for methods to\ngenerate Trivariate B-spline Solids (TBS) with positive Jacobian. However, it\nis difficult to guarantee a positive Jacobian of a TBS since the geometric\npre-condition for ensuring the positive Jacobian is very complicated. In this\npaper, we propose a method for generating TBSs with guaranteed positive\nJacobian. For the study, we used a tetrahedral (tet) mesh model and segmented\nit into sub-volumes using the pillow operation. Then, to reduce the difficulty\nin ensuring a positive Jacobian, we separately fitted the boundary curves and\nsurfaces and the sub-volumes using a geometric iterative fitting algorithm.\nFinally, the smoothness between adjacent TBSs is improved. The experimental\nexamples presented in this paper demonstrate the effectiveness and efficiency\nof the developed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:21:19 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Lin", "Hongwei", ""], ["Huang", "Hao", ""], ["Hu", "Chuanfeng", ""]]}, {"id": "1811.12599", "submitter": "Hongwei Lin", "authors": "Chuanfeng Hu, Hongwei Lin", "title": "Gregory Solid Construction for Polyhedral Volume Parameterization by\n  Sparse Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In isogeometric analysis, it is frequently required to handle the geometric\nmodels enclosed by four-sided or non-four-sided boundary patches, such as\ntrimmed surfaces. In this paper, we develop a Gregory solid based method to\nparameterize those models. First, we extend the Gregory patch representation to\nthe trivariate Gregory solid representation. Second, the trivariate Gregory\nsolid representation is employed to interpolate the boundary patches of a\ngeometric model, thus generating the polyhedral volume parametrization. To\nimprove the regularity of the polyhedral volume parametrization, we formulate\nthe construction of the trivariate Gregory solid as a sparse optimization\nproblem, where the optimization objective function is a linear combination of\nsome terms, including a sparse term aiming to reduce the negative Jacobian area\nof the Gregory solid. Then, the alternating direction method of multipliers\n(ADMM) is used to solve the sparse optimization problem. Lots of experimental\nexamples illustrated in this paper demonstrate the effectiveness and efficiency\nof the developed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:27:11 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hu", "Chuanfeng", ""], ["Lin", "Hongwei", ""]]}, {"id": "1811.12670", "submitter": "Ziwei Liu", "authors": "Weidong Yin, Ziwei Liu, Chen Change Loy", "title": "Instance-level Facial Attributes Transfer with Geometry-Aware Flow", "comments": "To appear in AAAI 2019. Code and models are available at:\n  https://github.com/wdyin/GeoGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of instance-level facial attribute transfer without\npaired training data, e.g. faithfully transferring the exact mustache from a\nsource face to a target face. This is a more challenging task than the\nconventional semantic-level attribute transfer, which only preserves the\ngeneric attribute style instead of instance-level traits. We propose the use of\ngeometry-aware flow, which serves as a well-suited representation for modeling\nthe transformation between instance-level facial attributes. Specifically, we\nleverage the facial landmarks as the geometric guidance to learn the\ndifferentiable flows automatically, despite of the large pose gap existed.\nGeometry-aware flow is able to warp the source face attribute into the target\nface context and generate a warp-and-blend result. To compensate for the\npotential appearance gap between source and target faces, we propose a\nhallucination sub-network that produces an appearance residual to further\nrefine the warp-and-blend result. Finally, a cycle-consistency framework\nconsisting of both attribute transfer module and attribute removal module is\ndesigned, so that abundant unpaired face images can be used as training data.\nExtensive evaluations validate the capability of our approach in transferring\ninstance-level facial attributes faithfully across large pose and appearance\ngaps. Thanks to the flow representation, our approach can readily be applied to\ngenerate realistic details on high-resolution images.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 08:43:00 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Yin", "Weidong", ""], ["Liu", "Ziwei", ""], ["Loy", "Chen Change", ""]]}]