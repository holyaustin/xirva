[{"id": "1906.00124", "submitter": "Oskar Elek", "authors": "Oskar Elek and Manu M. Thomas and Angus Forbes", "title": "Learning Patterns in Sample Distributions for Monte Carlo Variance\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a novel a-posteriori variance reduction approach in\nMonte Carlo image synthesis. Unlike most established methods based on lateral\nfiltering in the image space, our proposition is to produce the best possible\nestimate for each pixel separately, from all the samples drawn for it. To\nenable this, we systematically study the per-pixel sample distributions for\ndiverse scene configurations. Noting that these are too complex to be\ncharacterized by standard statistical distributions (e.g. Gaussians), we\nidentify patterns recurring in them and exploit those for training a\nvariance-reduction model based on neural nets. In result, we obtain numerically\nbetter estimates compared to simple averaging of samples. This method is\ncompatible with existing image-space denoising methods, as the improved\nestimates of our model can be used for further processing. We conclude by\ndiscussing how the proposed model could in future be extended for fully\nprogressive rendering with constant memory footprint and scene-sensitive\noutput.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 00:20:51 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Elek", "Oskar", ""], ["Thomas", "Manu M.", ""], ["Forbes", "Angus", ""]]}, {"id": "1906.00161", "submitter": "Naveed Akhtar Dr.", "authors": "Jian Liu, Naveed Akhtar, Ajmal Mian", "title": "Temporally Coherent Full 3D Mesh Human Pose Recovery from Monocular\n  Video", "comments": "Updated bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Deep Learning have recently made it possible to recover full 3D\nmeshes of human poses from individual images. However, extension of this notion\nto videos for recovering temporally coherent poses still remains unexplored. A\nmajor challenge in this regard is the lack of appropriately annotated video\ndata for learning the desired deep models. Existing human pose datasets only\nprovide 2D or 3D skeleton joint annotations, whereas the datasets are also\nrecorded in constrained environments. We first contribute a technique to\nsynthesize monocular action videos with rich 3D annotations that are suitable\nfor learning computational models for full mesh 3D human pose recovery.\nCompared to the existing methods which simply \"texture-map\" clothes onto the 3D\nhuman pose models, our approach incorporates Physics based realistic cloth\ndeformations with the human body movements. The generated videos cover a large\nvariety of human actions, poses, and visual appearances, whereas the\nannotations record accurate human pose dynamics and human body surface\ninformation. Our second major contribution is an end-to-end trainable Recurrent\nNeural Network for full pose mesh recovery from monocular video. Using the\nproposed video data and LSTM based recurrent structure, our network explicitly\nlearns to model the temporal coherence in videos and imposes geometric\nconsistency over the recovered meshes. We establish the effectiveness of the\nproposed model with quantitative and qualitative analysis using the proposed\nand benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 06:39:35 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 03:53:35 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Liu", "Jian", ""], ["Akhtar", "Naveed", ""], ["Mian", "Ajmal", ""]]}, {"id": "1906.00544", "submitter": "Yudong Guo", "authors": "Yudong Guo, Luo Jiang, Lin Cai, Juyong Zhang", "title": "3D Magic Mirror: Automatic Video to 3D Caricature Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is an abstraction of a real person which distorts or exaggerates\ncertain features, but still retains a likeness. While most existing works focus\non 3D caricature reconstruction from 2D caricatures or translating 2D photos to\n2D caricatures, this paper presents a real-time and automatic algorithm for\ncreating expressive 3D caricatures with caricature style texture map from 2D\nphotos or videos. To solve this challenging ill-posed reconstruction problem\nand cross-domain translation problem, we first reconstruct the 3D face shape\nfor each frame, and then translate 3D face shape from normal style to\ncaricature style by a novel identity and expression preserving VAE-CycleGAN.\nBased on a labeling formulation, the caricature texture map is constructed from\na set of multi-view caricature images generated by CariGANs. The effectiveness\nand efficiency of our method are demonstrated by comparison with baseline\nimplementations. The perceptual study shows that the 3D caricatures generated\nby our method meet people's expectations of 3D caricature style.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 03:12:29 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Guo", "Yudong", ""], ["Jiang", "Luo", ""], ["Cai", "Lin", ""], ["Zhang", "Juyong", ""]]}, {"id": "1906.01071", "submitter": "Keshav Dasu", "authors": "Keshav Dasu, Kwan-Liu Ma, Joyce Ma, Jennifer Frazier", "title": "Sea of Genes: Combining Animation and Narrative Strategies to Visualize\n  Metagenomic Data for Museums", "comments": "This manuscript has been accepted to VIS 2020 and TVCG 9 pages 2\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the application of narrative strategies to present a complex and\nunfamiliar metagenomics dataset to the public in a science museum. Our dataset\ncontains information about microbial gene expressions that scientists use to\ninfer the behavior of microbes. This exhibit had three goals: to inform (the)\npublic about microbes' behavior, cycles, and patterns; to link their behavior\nto the concept of gene expression; and to highlight scientists' use of gene\nexpression data to understand the role of microbes. To address these three\ngoals, we created a visualization with three narrative layers, each layer\ncorresponding to a goal. This study presented us with an opportunity to assess\nexisting frameworks for narrative visualization in a naturalistic setting. We\npresent three successive rounds of design and evaluation of our attempts to\nengage visitors with complex data through narrative visualization. We highlight\nour design choices and their underlying rationale based on extant theories. We\nconclude that a central animation based on a curated dataset could successfully\nachieve our first goal, i.e., to communicate the aggregate behavior and\ninteractions of microbes. We failed to achieve our second goal and had limited\nsuccess with the third goal. Overall, this study highlights the challenges of\ntelling multi-layered stories and the need for new frameworks for communicating\nlayered stories in public settings.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:41:16 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 07:08:23 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dasu", "Keshav", ""], ["Ma", "Kwan-Liu", ""], ["Ma", "Joyce", ""], ["Frazier", "Jennifer", ""]]}, {"id": "1906.01524", "submitter": "Ohad Fried", "authors": "Ohad Fried, Ayush Tewari, Michael Zollh\\\"ofer, Adam Finkelstein, Eli\n  Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, Maneesh\n  Agrawala", "title": "Text-based Editing of Talking-head Video", "comments": "A version with higher resolution images can be downloaded from the\n  authors' website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing talking-head video to change the speech content or to remove filler\nwords is challenging. We propose a novel method to edit talking-head video\nbased on its transcript to produce a realistic output video in which the\ndialogue of the speaker has been modified, while maintaining a seamless\naudio-visual flow (i.e. no jump cuts). Our method automatically annotates an\ninput talking-head video with phonemes, visemes, 3D face pose and geometry,\nreflectance, expression and scene illumination per frame. To edit a video, the\nuser has to only edit the transcript, and an optimization strategy then chooses\nsegments of the input corpus as base material. The annotated parameters\ncorresponding to the selected segments are seamlessly stitched together and\nused to produce an intermediate video representation in which the lower half of\nthe face is rendered with a parametric face model. Finally, a recurrent video\ngeneration network transforms this representation to a photorealistic video\nthat matches the edited transcript. We demonstrate a large variety of edits,\nsuch as the addition, removal, and alteration of words, as well as convincing\nlanguage translation and full sentence synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 15:35:16 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Fried", "Ohad", ""], ["Tewari", "Ayush", ""], ["Zollh\u00f6fer", "Michael", ""], ["Finkelstein", "Adam", ""], ["Shechtman", "Eli", ""], ["Goldman", "Dan B", ""], ["Genova", "Kyle", ""], ["Jin", "Zeyu", ""], ["Theobalt", "Christian", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1906.01627", "submitter": "Giuseppe Patane'", "authors": "M. Attene, S. Biasotti, S. Bertoluzza, D. Cabiddu, M. Livesu, G.\n  Patan\\`e, M. Pennacchio, D. Prada, M. Spagnuolo", "title": "Benchmark of Polygon Quality Metrics for Polytopal Element Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polytopal Element Methods (PEM) allow to solve differential equations on\ngeneral polygonal and polyhedral grids, potentially offering great flexibility\nto mesh generation algorithms. Differently from classical finite element\nmethods, where the relation between the geometric properties of the mesh and\nthe performances of the solver are well known, the characterization of a good\npolytopal element is still subject to ongoing research. Current shape\nregularity criteria are quite restrictive, and greatly limit the set of valid\nmeshes. Nevertheless, numerical experiments revealed that PEM solvers can\nperform well on meshes that are far outside the strict boundaries imposed by\nthe current theory, suggesting that the real capabilities of these methods are\nmuch higher. In this work, we propose a benchmark to study the correlation\nbetween general 2D polygonal meshes and PEM solvers. The benchmark aims to\nexplore the space of 2D polygonal meshes and polygonal quality metrics, in\norder to identify weaker shape-regularity criteria under which the considered\nmethods can reliably work. The proposed tool is quite general, and can be\npotentially used to study any PEM solver. Besides discussing the basics of the\nbenchmark, in the second part of the paper we demonstrate its application on a\nrepresentative member of the PEM family, namely the Virtual Element Method,\nalso discussing our findings.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 09:11:45 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Attene", "M.", ""], ["Biasotti", "S.", ""], ["Bertoluzza", "S.", ""], ["Cabiddu", "D.", ""], ["Livesu", "M.", ""], ["Patan\u00e8", "G.", ""], ["Pennacchio", "M.", ""], ["Prada", "D.", ""], ["Spagnuolo", "M.", ""]]}, {"id": "1906.01689", "submitter": "You Xie", "authors": "Maximilian Werhahn, You Xie, Mengyu Chu, Nils Thuerey", "title": "A Multi-Pass GAN for Fluid Flow Super-Resolution", "comments": "SCA 2019, further details at:\n  https://ge.in.tum.de/publications/2019-multi-pass-gan/", "journal-ref": "Proc. ACM Comput. Graph. Interact. Tech. 2, 2, Article 10 (July\n  2019)", "doi": "10.1145/3340251", "report-no": null, "categories": "cs.GR cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method to up-sample volumetric functions with generative\nneural networks using several orthogonal passes. Our method decomposes\ngenerative problems on Cartesian field functions into multiple smaller\nsub-problems that can be learned more efficiently. Specifically, we utilize two\nseparate generative adversarial networks: the first one up-scales slices which\nare parallel to the XY-plane, whereas the second one refines the whole volume\nalong the Z-axis working on slices in the YZ-plane. In this way, we obtain full\ncoverage for the 3D target function and can leverage spatio-temporal\nsupervision with a set of discriminators. Additionally, we demonstrate that our\nmethod can be combined with curriculum learning and progressive growing\napproaches. We arrive at a first method that can up-sample volumes by a factor\nof eight along each dimension, i.e., increasing the number of degrees of\nfreedom by 512. Large volumetric up-scaling factors such as this one have\npreviously not been attainable as the required number of weights in the neural\nnetworks renders adversarial training runs prohibitively difficult. We\ndemonstrate the generality of our trained networks with a series of comparisons\nto previous work, a variety of complex 3D results, and an analysis of the\nresulting performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 19:22:36 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Werhahn", "Maximilian", ""], ["Xie", "You", ""], ["Chu", "Mengyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1906.02426", "submitter": "Cho Ying Wu", "authors": "Cho-Ying Wu, Ulrich Neumann", "title": "Salient Building Outline Enhancement and Extraction Using Iterative L0\n  Smoothing and Line Enhancing", "comments": "Accepted to ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, our goal is salient building outline enhancement and\nextraction from images taken from consumer cameras using L0 smoothing. We\naddress weak outlines and over-smoothing problem. Weak outlines are often\nundetected by edge extractors or easily smoothed out. We propose an iterative\nmethod, including the smoothing cell and sharpening cell. In the smoothing\ncell, we iteratively enlarge the smoothing level of the L0 smoothing. In the\nsharpening cell, we use Hough Transform to extract lines, based on the\nassumption that salient outlines for buildings are usually straight, and\nenhance those extracted lines. Our goal is to enhance line structures and do\nthe L0 smoothing simultaneously. Also, we propose to create building masks from\nsemantic segmentation using an encoder-decoder network. The masks filter out\nirrelevant edges. We also provide an evaluation dataset on this task.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 05:44:12 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1906.02470", "submitter": "Jie An", "authors": "Jie An, Haoyi Xiong, Jinwen Ma, Jiebo Luo and Jun Huan", "title": "StyleNAS: An Empirical Study of Neural Architecture Search to Uncover\n  Surprisingly Fast End-to-End Universal Style Transfer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has been widely studied for designing\ndiscriminative deep learning models such as image classification, object\ndetection, and semantic segmentation. As a large number of priors have been\nobtained through the manual design of architectures in the fields, NAS is\nusually considered as a supplement approach. In this paper, we have\nsignificantly expanded the application areas of NAS by performing an empirical\nstudy of NAS to search generative models, or specifically, auto-encoder based\nuniversal style transfer, which lacks systematic exploration, if any, from the\narchitecture search aspect. In our work, we first designed a search space where\ncommon operators for image style transfer such as VGG-based encoders, whitening\nand coloring transforms (WCT), convolution kernels, instance normalization\noperators, and skip connections were searched in a combinatorial approach. With\na simple yet effective parallel evolutionary NAS algorithm with multiple\nobjectives, we derived the first group of end-to-end deep networks for\nuniversal photorealistic style transfer. Comparing to random search, a NAS\nmethod that is gaining popularity recently, we demonstrated that carefully\ndesigned search strategy leads to much better architecture design. Finally\ncompared to existing universal style transfer networks for photorealistic\nrendering such as PhotoWCT that stacks multiple well-trained auto-encoders and\nWCT transforms in a non-end-to-end manner, the architectures designed by\nStyleNAS produce better style-transferred images with details preserving, using\na tiny number of operators/parameters, and enjoying around 500x inference time\nspeed-up.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 08:21:04 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["An", "Jie", ""], ["Xiong", "Haoyi", ""], ["Ma", "Jinwen", ""], ["Luo", "Jiebo", ""], ["Huan", "Jun", ""]]}, {"id": "1906.03027", "submitter": "Tim Kuipers", "authors": "Tim Kuipers, Jun Wu and Charlie C. L. Wang", "title": "CrossFill: Foam Structures with Graded Density for Continuous Material\n  Extrusion", "comments": "Submission to Symposium on Solid and Physical Modeling 2019", "journal-ref": "Computer-Aided Design 114C (2019) pp. 37-50", "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fabrication flexibility of 3D printing has sparked a lot of interest in\ndesigning structures with spatially graded material properties. In this paper,\nwe propose a new type of density graded structure that is particularly designed\nfor 3D printing systems based on filament extrusion. In order to ensure\nhigh-quality fabrication results, extrusion-based 3D printing requires not only\nthat the structures are self-supporting, but also that extrusion toolpaths are\ncontinuous and free of self-overlap. The structure proposed in this paper,\ncalled CrossFill, complies with these requirements. In particular, CrossFill is\na self-supporting foam structure, for which each layer is fabricated by a\nsingle, continuous and overlap-free path of material extrusion. Our method for\ngenerating CrossFill is based on a space-filling surface that employs spatially\nvarying subdivision levels. Dithering of the subdivision levels is performed to\naccurately reproduce a prescribed density distribution. We demonstrate the\neffectiveness of CrossFill on a number of experimental tests and applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 11:52:52 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Kuipers", "Tim", ""], ["Wu", "Jun", ""], ["Wang", "Charlie C. L.", ""]]}, {"id": "1906.03039", "submitter": "Lingjing Wang", "authors": "Lingjing Wang, Xiang Li, Jianchun Chen and Yi Fang", "title": "Coherent Point Drift Networks: Unsupervised Learning of Non-Rigid Point\n  Set Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given new pairs of source and target point sets, standard point set\nregistration methods often repeatedly conduct the independent iterative search\nof desired geometric transformation to align the source point set with the\ntarget one. This limits their use in applications to handle the real-time point\nset registration with large volume dataset. This paper presents a novel method,\nnamed coherent point drift networks (CPD-Net), for the unsupervised learning of\ngeometric transformation towards real-time non-rigid point set registration. In\ncontrast to previous efforts (e.g. coherent point drift), CPD-Net can learn\ndisplacement field function to estimate geometric transformation from a\ntraining dataset, consequently, to predict the desired geometric transformation\nfor the alignment of previously unseen pairs without any additional iterative\noptimization process. Furthermore, CPD-Net leverages the power of deep neural\nnetworks to fit an arbitrary function, that adaptively accommodates different\nlevels of complexity of the desired geometric transformation. Particularly,\nCPD-Net is proved with a theoretical guarantee to learn a continuous\ndisplacement vector function that could further avoid imposing additional\nparametric smoothness constraint as in previous works. Our experiments verify\nthe impressive performance of CPD-Net for non-rigid point set registration on\nvarious 2D/3D datasets, even in the presence of significant displacement noise,\noutliers, and missing points. Our code will be available at\nhttps://github.com/nyummvc/CPD-Net.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:12:22 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 11:06:57 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 12:29:46 GMT"}, {"version": "v4", "created": "Sun, 14 Jul 2019 04:33:23 GMT"}, {"version": "v5", "created": "Sun, 28 Jul 2019 17:43:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wang", "Lingjing", ""], ["Li", "Xiang", ""], ["Chen", "Jianchun", ""], ["Fang", "Yi", ""]]}, {"id": "1906.03299", "submitter": "Zhiheng Kang", "authors": "Kang Zhiheng and Li Ning", "title": "PyramNet: Point Cloud Pyramid Attention Network and Graph Embedding\n  Module for Classification and Segmentation", "comments": "Accepted for presentation at ICONIP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tide of artificial intelligence, we try to apply deep learning to\nunderstand 3D data. Point cloud is an important 3D data structure, which can\naccurately and directly reflect the real world. In this paper, we propose a\nsimple and effective network, which is named PyramNet, suites for point cloud\nobject classification and semantic segmentation in 3D scene. We design two new\noperators: Graph Embedding Module(GEM) and Pyramid Attention Network(PAN).\nSpecifically, GEM projects point cloud onto the graph and practices the\ncovariance matrix to explore the relationship between points, so as to improve\nthe local feature expression ability of the model. PAN assigns some strong\nsemantic features to each point to retain fine geometric features as much as\npossible. Furthermore, we provide extensive evaluation and analysis for the\neffectiveness of PyramNet. Empirically, we evaluate our model on ModelNet40,\nShapeNet and S3DIS.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 19:06:24 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 04:04:47 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhiheng", "Kang", ""], ["Ning", "Li", ""]]}, {"id": "1906.03355", "submitter": "Thomas Nestmeyer", "authors": "Thomas Nestmeyer, Jean-Fran\\c{c}ois Lalonde, Iain Matthews, Andreas M.\n  Lehrmann", "title": "Learning Physics-guided Face Relighting under Directional Light", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relighting is an essential step in realistically transferring objects from a\ncaptured image into another environment. For example, authentic telepresence in\nAugmented Reality requires faces to be displayed and relit consistent with the\nobserver's scene lighting. We investigate end-to-end deep learning\narchitectures that both de-light and relight an image of a human face. Our\nmodel decomposes the input image into intrinsic components according to a\ndiffuse physics-based image formation model. We enable non-diffuse effects\nincluding cast shadows and specular highlights by predicting a residual\ncorrection to the diffuse render. To train and evaluate our model, we collected\na portrait database of 21 subjects with various expressions and poses. Each\nsample is captured in a controlled light stage setup with 32 individual light\nsources. Our method creates precise and believable relighting results and\ngeneralizes to complex illumination conditions and challenging poses, including\nwhen the subject is not looking straight at the camera.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 23:16:34 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 12:33:08 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nestmeyer", "Thomas", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""], ["Matthews", "Iain", ""], ["Lehrmann", "Andreas M.", ""]]}, {"id": "1906.03366", "submitter": "Yixuan He", "authors": "Yixuan He, Tianyi Hu, Delu Zeng", "title": "Scan-flood Fill(SCAFF): an Efficient Automatic Precise Region Filling\n  Algorithm for Complicated Regions", "comments": "9 pages, 8 figures, CEFRL 2019: 3rd International Workshop on Compact\n  and Efficient Feature Representation and Learning in Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, instant level labeling for supervised machine learning requires a\nconsiderable number of filled masks. In this paper, we propose an efficient\nautomatic region filling algorithm for complicated regions. Distinguishing\nbetween adjacent connected regions, the Main Filling Process scans through all\npixels and fills all the pixels except boundary ones with either exterior or\ninterior label color. In this way, we succeed in classifying all the pixels\ninside the region except boundary ones in the given image to form two groups: a\nbackground group and a mask group. We then set all exterior label pixels to\nbackground color, and interior label pixels to mask color. With this algorithm,\nwe are able to generate output masks precisely and efficiently even for\ncomplicated regions as long as boundary pixels are given. Experimental results\nshow that the proposed algorithm can generate precise masks that allow for\nvarious machine learning tasks such as supervised training. This algorithm can\neffectively handle multiple regions, complicated `holes' and regions whose\nboundaries touch the image border. By testing the algorithm on both toy and\npractical images, we show that the performance of Scan-flood Fill(SCAFF) has\nachieved favorable results.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 01:05:02 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 01:47:55 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["He", "Yixuan", ""], ["Hu", "Tianyi", ""], ["Zeng", "Delu", ""]]}, {"id": "1906.03841", "submitter": "Xiao Li", "authors": "Xiao Li, Yue Dong, Pieter Peers, Xin Tong", "title": "Synthesizing 3D Shapes from Silhouette Image Collections using\n  Multi-projection Generative Adversarial Networks", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new weakly supervised learning-based method for generating novel\ncategory-specific 3D shapes from unoccluded image collections. Our method is\nweakly supervised and only requires silhouette annotations from unoccluded,\ncategory-specific objects. Our method does not require access to the object's\n3D shape, multiple observations per object from different views, intra-image\npixel-correspondences, or any view annotations. Key to our method is a novel\nmulti-projection generative adversarial network (MP-GAN) that trains a 3D shape\ngenerator to be consistent with multiple 2D projections of the 3D shapes, and\nwithout direct access to these 3D shapes. This is achieved through multiple\ndiscriminators that encode the distribution of 2D projections of the 3D shapes\nseen from a different views. Additionally, to determine the view information\nfor each silhouette image, we also train a view prediction network on\nvisualizations of 3D shapes synthesized by the generator. We iteratively\nalternate between training the generator and training the view prediction\nnetwork. We validate our multi-projection GAN on both synthetic and real image\ndatasets. Furthermore, we also show that multi-projection GANs can aid in\nlearning other high-dimensional distributions from lower dimensional training\ndatasets, such as material-class specific spatially varying reflectance\nproperties from images.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:45:35 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Xiao", ""], ["Dong", "Yue", ""], ["Peers", "Pieter", ""], ["Tong", "Xin", ""]]}, {"id": "1906.03856", "submitter": "Giuseppe Patan\\`e", "authors": "G. Patan\\`e", "title": "Laplacian Spectral Basis Functions", "comments": null, "journal-ref": "Computer Aided Geometric Design Computer-Aided Geometric Design,\n  Volume 65, October 2018, Pages 31-47", "doi": "10.1016/j.cagd.2018.07.002", "report-no": null, "categories": "cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing a signal as a linear combination of a set of basis functions is\ncentral in a wide range of applications, such as approximation, de-noising,\ncompression, shape correspondence and comparison. In this context, our paper\naddresses the main aspects of signal approximation, such as the definition,\ncomputation, and comparison of basis functions on arbitrary 3D shapes. Focusing\non the class of basis functions induced by the Laplace-Beltrami operator and\nits spectrum, we introduce the diffusion and Laplacian spectral basis\nfunctions, which are then compared with the harmonic and Laplacian\neigenfunctions. As main properties of these basis functions, which are commonly\nused for numerical geometry processing and shape analysis, we discuss the\npartition of the unity and non-negativity; the intrinsic definition and\ninvariance with respect to shape transformations (e.g., translation, rotation,\nuniform scaling); the locality, smoothness, and orthogonality; the numerical\nstability with respect to the domain discretisation; the computational cost and\nstorage overhead. Finally, we consider geometric metrics, such as the area,\nconformal, and kernel-based norms, for the comparison and characterisation of\nthe main properties of the Laplacian basis functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 09:15:17 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Patan\u00e8", "G.", ""]]}, {"id": "1906.04003", "submitter": "Andrea Raffo", "authors": "Andrea Raffo and Silvia Biasotti", "title": "Data-driven quasi-interpolant spline surfaces for point cloud\n  approximation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cag.2020.05.004", "report-no": null, "categories": "cs.NA cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate a local surface approximation, the Weighted\nQuasi Interpolant Spline Approximation (wQISA), specifically designed for large\nand noisy point clouds. We briefly describe the properties of the wQISA\nrepresentation and introduce a novel data-driven implementation, which combines\nprediction capability and complexity efficiency. We provide an extended\ncomparative analysis with other continuous approximations on real data,\nincluding different types of surfaces and levels of noise, such as 3D models,\nterrain data and digital environmental data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:23:14 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 19:09:48 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 13:49:42 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Raffo", "Andrea", ""], ["Biasotti", "Silvia", ""]]}, {"id": "1906.04173", "submitter": "Wang Yifan", "authors": "Wang Yifan, Felice Serena, Shihao Wu, Cengiz \\\"Oztireli, Olga\n  Sorkine-Hornung", "title": "Differentiable Surface Splatting for Point-based Geometry Processing", "comments": "This version is contains camera-ready manuscript for SIGGRAPH Asia\n  2019", "journal-ref": null, "doi": "10.1145/3355089.3356513", "report-no": null, "categories": "cs.GR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Differentiable Surface Splatting (DSS), a high-fidelity\ndifferentiable renderer for point clouds. Gradients for point locations and\nnormals are carefully designed to handle discontinuities of the rendering\nfunction. Regularization terms are introduced to ensure uniform distribution of\nthe points on the underlying surface. We demonstrate applications of DSS to\ninverse rendering for geometry synthesis and denoising, where large scale\ntopological changes, as well as small scale detail modifications, are\naccurately and robustly handled without requiring explicit connectivity,\noutperforming state-of-the-art techniques. The data and code are at\nhttps://github.com/yifita/DSS.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 12:30:27 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 16:10:54 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 06:34:40 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Yifan", "Wang", ""], ["Serena", "Felice", ""], ["Wu", "Shihao", ""], ["\u00d6ztireli", "Cengiz", ""], ["Sorkine-Hornung", "Olga", ""]]}, {"id": "1906.04576", "submitter": "Sebastian Maisch", "authors": "Simon Besenthal, Sebastian Maisch, Timo Ropinski", "title": "Multi-Resolution Rendering for Computationally Expensive Lighting\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many lighting methods used in computer graphics such as indirect illumination\ncan have very high computational costs and need to be approximated for\nreal-time applications. These costs can be reduced by means of upsampling\ntechniques which tend to introduce artifacts and affect the visual quality of\nthe rendered image. This paper suggests a versatile approach for accelerating\nthe rendering of screen space methods while maintaining the visual quality.\nThis is achieved by exploiting the low frequency nature of many of these\nillumination methods and the geometrical continuity of the scene. First the\nscreen space is dynamically divided into separate sub-images, then the\nillumination is rendered for each sub-image in an adequate resolution and\nfinally the sub-images are put together in order to compose the final image.\nTherefore we identify edges in the scene and generate masks precisely\nspecifying which part of the image is included in which sub-image. The masks\ntherefore determine which part of the image is rendered in which resolution. A\nstep wise upsampling and merging process then allows optically soft transitions\nbetween the different resolution levels. For this paper, the introduced\nmulti-resolution rendering method was implemented and tested on three commonly\nused lighting methods. These are screen space ambient occlusion, soft shadow\nmapping and screen space global illumination.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:23:24 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Besenthal", "Simon", ""], ["Maisch", "Sebastian", ""], ["Ropinski", "Timo", ""]]}, {"id": "1906.04777", "submitter": "Pieter Peers", "authors": "Victoria L. Cooper, James C. Bieron, Pieter Peers", "title": "Estimating Homogeneous Data-driven BRDF Parameters from a Reflectance\n  Map under Known Natural Lighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate robust estimation of the model parameters of a\nfully-linear data-driven BRDF model from a reflectance map under known natural\nlighting. To regularize the estimation of the model parameters, we leverage the\nreflectance similarities within a material class. We approximate the space of\nhomogeneous BRDFs using a Gaussian mixture model, and assign a material class\nto each Gaussian in the mixture model. We formulate the estimation of the model\nparameters as a non-linear maximum a-posteriori optimization, and introduce a\nlinear approximation that estimates a solution per material class from which\nthe best solution is selected. We demonstrate the efficacy and robustness of\nour method using the MERL BRDF database under a variety of natural lighting\nconditions, and we provide a proof-of-concept real-world experiment.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 19:28:09 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Cooper", "Victoria L.", ""], ["Bieron", "James C.", ""], ["Peers", "Pieter", ""]]}, {"id": "1906.04910", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Aartika Rai, Subhransu Maji, Rui Wang", "title": "Inferring 3D Shapes from Image Collections using Adversarial Networks", "comments": "Source code: https://github.com/matheusgadelha/PrGAN . arXiv admin\n  note: substantial text overlap with arXiv:1612.05872", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of learning a probabilistic distribution over\nthree-dimensional shapes given two-dimensional views of multiple objects taken\nfrom unknown viewpoints. Our approach called projective generative adversarial\nnetwork (PrGAN) trains a deep generative model of 3D shapes whose projections\n(or renderings) match the distributions of the provided 2D distribution. The\naddition of a differentiable projection module allows us to infer the\nunderlying 3D shape distribution without access to any explicit 3D or viewpoint\nannotation during the learning phase. We show that our approach produces 3D\nshapes of comparable quality to GANs trained directly on 3D data. %for a number\nof shape categoriesincluding chairs, airplanes, and cars. Experiments also show\nthat the disentangled representation of 2D shapes into geometry and viewpoint\nleads to a good generative model of 2D shapes. The key advantage of our model\nis that it estimates 3D shape, viewpoint, and generates novel views from an\ninput image in a completely unsupervised manner. We further investigate how the\ngenerative models can be improved if additional information such as depth,\nviewpoint or part segmentations is available at training time. To this end, we\npresent new differentiable projection operators that can be used by PrGAN to\nlearn better 3D generative models. Our experiments show that our method can\nsuccessfully leverage extra visual cues to create more diverse and accurate\nshapes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 00:28:19 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Gadelha", "Matheus", ""], ["Rai", "Aartika", ""], ["Maji", "Subhransu", ""], ["Wang", "Rui", ""]]}, {"id": "1906.05075", "submitter": "Matteo Paolo Lanaro", "authors": "Matteo P. Lanaro, H\\'el\\`ene Perrier, David Coeurjolly, Victor\n  Ostromoukhov, Alessandro Rizzi", "title": "Blue-noise sampling for human retinal cone spatial distribution modeling", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel method for modeling human retinal cone\ndistribution. It is based on Blue-noise sampling algorithms that share\ninteresting properties with the sampling performed by the mosaic formed by cone\nphotoreceptors in the retina. Here we present the method together with a series\nof examples of various real retinal patches. The same samples have also been\ncreated with alternative algorithms and compared with plots of the center of\nthe inner segments of cone photoreceptors from imaged retinas. Results are\nevaluated with different distance measure used in the field, like\nnearest-neighbor analysis and pair correlation function. The proposed method\ncan describe features of a human retinal cone distribution with a certain\ndegree of similarity to the available data and can be efficiently used for\nmodeling local patches of retina.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 12:49:15 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Lanaro", "Matteo P.", ""], ["Perrier", "H\u00e9l\u00e8ne", ""], ["Coeurjolly", "David", ""], ["Ostromoukhov", "Victor", ""], ["Rizzi", "Alessandro", ""]]}, {"id": "1906.05260", "submitter": "Baptiste Angles", "authors": "Baptiste Angles, Daniel Rebain, Miles Macklin, Brian Wyvill, Loic\n  Barthe, JP Lewis, Javier von der Pahlen, Shahram Izadi, Julien Valentin,\n  Sofien Bouaziz, Andrea Tagliasacchi", "title": "VIPER: Volume Invariant Position-based Elastic Rods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the formulation of position-based rods to include elastic\nvolumetric deformations. We achieve this by introducing an additional degree of\nfreedom per vertex -- isotropic scale (and its velocity). Including scale\nenriches the space of possible deformations, allowing the simulation of\nvolumetric effects, such as a reduction in cross-sectional area when a rod is\nstretched. We rigorously derive the continuous formulation of its elastic\nenergy potentials, and hence its associated position-based dynamics (PBD)\nupdates to realize this model, enabling the simulation of up to 26000 DOFs at\n140 Hz in our GPU implementation. We further show how rods can provide a\ncompact alternative to tetrahedral meshes for the representation of complex\nmuscle deformations, as well as providing a convenient representation for\ncollision detection. This is achieved by modeling a muscle as a bundle of rods,\nfor which we also introduce a technique to automatically convert a muscle\nsurface mesh into a rods-bundle. Finally, we show how rods and/or bundles can\nbe skinned to a surface mesh to drive its deformation, resulting in an\nalternative to cages for real-time volumetric deformation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 17:37:21 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Angles", "Baptiste", ""], ["Rebain", "Daniel", ""], ["Macklin", "Miles", ""], ["Wyvill", "Brian", ""], ["Barthe", "Loic", ""], ["Lewis", "JP", ""], ["von der Pahlen", "Javier", ""], ["Izadi", "Shahram", ""], ["Valentin", "Julien", ""], ["Bouaziz", "Sofien", ""], ["Tagliasacchi", "Andrea", ""]]}, {"id": "1906.05710", "submitter": "Alec Jacobson", "authors": "Alec Jacobson", "title": "RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed\n  Joints and Precision-Cut Rods", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RodSteward, a design-to-assembly system for creating\nfurniture-scale structures composed of 3D printed joints and precision-cut\nrods. The RodSteward systems consists of: RSDesigner, a fabrication-aware\ndesign interface that visualizes accurate geometries during edits and\nidentifies infeasible designs; physical fabrication of parts via novel fully\nautomatic construction of solid 3D-printable joint geometries and automatically\ngenerated cutting plans for rods; and RSAssembler, a guided-assembly interface\nthat prompts the user to place parts in order while showing a focus+context\nvisualization of the assembly in progress. We demonstrate the effectiveness of\nour tools with a number of example constructions of varying complexity, style\nand parameter choices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:14:32 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Jacobson", "Alec", ""]]}, {"id": "1906.05797", "submitter": "Julian Straub", "authors": "Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans,\n  Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton\n  Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang\n  Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias\n  Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat,\n  Renzo De Nardi, Michael Goesele, Steven Lovegrove, Richard Newcombe", "title": "The Replica Dataset: A Digital Replica of Indoor Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Replica, a dataset of 18 highly photo-realistic 3D indoor scene\nreconstructions at room and building scale. Each scene consists of a dense\nmesh, high-resolution high-dynamic-range (HDR) textures, per-primitive semantic\nclass and instance information, and planar mirror and glass reflectors. The\ngoal of Replica is to enable machine learning (ML) research that relies on\nvisually, geometrically, and semantically realistic generative models of the\nworld - for instance, egocentric computer vision, semantic segmentation in 2D\nand 3D, geometric inference, and the development of embodied agents (virtual\nrobots) performing navigation, instruction following, and question answering.\nDue to the high level of realism of the renderings from Replica, there is hope\nthat ML systems trained on Replica may transfer directly to real world image\nand video data. Together with the data, we are releasing a minimal C++ SDK as a\nstarting point for working with the Replica dataset. In addition, Replica is\n`Habitat-compatible', i.e. can be natively used with AI Habitat for training\nand testing embodied agents.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:29:58 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Straub", "Julian", ""], ["Whelan", "Thomas", ""], ["Ma", "Lingni", ""], ["Chen", "Yufan", ""], ["Wijmans", "Erik", ""], ["Green", "Simon", ""], ["Engel", "Jakob J.", ""], ["Mur-Artal", "Raul", ""], ["Ren", "Carl", ""], ["Verma", "Shobhit", ""], ["Clarkson", "Anton", ""], ["Yan", "Mingfei", ""], ["Budge", "Brian", ""], ["Yan", "Yajie", ""], ["Pan", "Xiaqing", ""], ["Yon", "June", ""], ["Zou", "Yuyang", ""], ["Leon", "Kimberly", ""], ["Carter", "Nigel", ""], ["Briales", "Jesus", ""], ["Gillingham", "Tyler", ""], ["Mueggler", "Elias", ""], ["Pesqueira", "Luis", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""], ["Strasdat", "Hauke M.", ""], ["De Nardi", "Renzo", ""], ["Goesele", "Michael", ""], ["Lovegrove", "Steven", ""], ["Newcombe", "Richard", ""]]}, {"id": "1906.05921", "submitter": "Nicolas Guigui", "authors": "N. Guigui (EPIONE, UCA), Shuman Jia (EPIONE, UCA), Maxime Sermesant\n  (EPIONE, UCA), Xavier Pennec (EPIONE, UCA)", "title": "Symmetric Algorithmic Components for Shape Analysis with Diffeomorphisms", "comments": null, "journal-ref": "Geometric Science of Information 2019, Aug 2019, Toulouse, France", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational anatomy, the statistical analysis of temporal deformations\nand inter-subject variability relies on shape registration. However, the\nnumerical integration and optimization required in diffeomorphic registration\noften lead to important numerical errors. In many cases, it is well known that\nthe error can be drastically reduced in the presence of a symmetry. In this\nwork, the leading idea is to approximate the space of deformations and images\nwith a possibly non-metric symmetric space structure using an involution, with\nthe aim to perform parallel transport. Through basic properties of symmetries,\nwe investigate how the implementations of a midpoint and the involution compare\nwith the ones of the Riemannian exponential and logarithm on diffeomorphisms\nand propose a modification of these maps using registration errors. This leads\nus to identify transvections, the composition of two symmetries, as a mean to\nmeasure how far from symmetric the underlying structure is. We test our method\non a set of 138 cardiac shapes and demonstrate improved numerical consistency\nin the Pole Ladder scheme.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 09:29:46 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Guigui", "N.", "", "EPIONE, UCA"], ["Jia", "Shuman", "", "EPIONE, UCA"], ["Sermesant", "Maxime", "", "EPIONE, UCA"], ["Pennec", "Xavier", "", "EPIONE, UCA"]]}, {"id": "1906.06113", "submitter": "Hamid Laga", "authors": "Hamid Laga", "title": "A Survey on Deep Learning Architectures for Image-based Depth\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating depth from RGB images is a long-standing ill-posed problem, which\nhas been explored for decades by the computer vision, graphics, and machine\nlearning communities. In this article, we provide a comprehensive survey of the\nrecent developments in this field. We will focus on the works which use deep\nlearning techniques to estimate depth from one or multiple images. Deep\nlearning, coupled with the availability of large training datasets, have\nrevolutionized the way the depth reconstruction problem is being approached by\nthe research community. In this article, we survey more than 100 key\ncontributions that appeared in the past five years, summarize the most commonly\nused pipelines, and discuss their benefits and limitations. In retrospect of\nwhat has been achieved so far, we also conjecture what the future may hold for\nlearning-based depth reconstruction research.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 10:22:14 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Laga", "Hamid", ""]]}, {"id": "1906.06520", "submitter": "Sebastian Weiss", "authors": "Sebastian Weiss, Mengyu Chu, Nils Thuerey, R\\\"udiger Westermann", "title": "Volumetric Isosurface Rendering with Deep Learning-Based\n  Super-Resolution", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2956697", "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rendering an accurate image of an isosurface in a volumetric field typically\nrequires large numbers of data samples. Reducing the number of required samples\nlies at the core of research in volume rendering. With the advent of deep\nlearning networks, a number of architectures have been proposed recently to\ninfer missing samples in multi-dimensional fields, for applications such as\nimage super-resolution and scan completion. In this paper, we investigate the\nuse of such architectures for learning the upscaling of a low-resolution\nsampling of an isosurface to a higher resolution, with high fidelity\nreconstruction of spatial detail and shading. We introduce a fully\nconvolutional neural network, to learn a latent representation generating a\nsmooth, edge-aware normal field and ambient occlusions from a low-resolution\nnormal and depth field. By adding a frame-to-frame motion loss into the\nlearning stage, the upscaling can consider temporal variations and achieves\nimproved frame-to-frame coherence. We demonstrate the quality of the network\nfor isosurfaces which were never seen during training, and discuss remote and\nin-situ visualization as well as focus+context visualization as potential\napplications\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 10:24:39 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Weiss", "Sebastian", ""], ["Chu", "Mengyu", ""], ["Thuerey", "Nils", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "1906.06543", "submitter": "Hamid Laga", "authors": "Xian-Feng Han, Hamid Laga, Mohammed Bennamoun", "title": "Image-based 3D Object Reconstruction: State-of-the-Art and Trends in the\n  Deep Learning Era", "comments": "arXiv admin note: text overlap with arXiv:1806.06098,\n  arXiv:1712.06584, arXiv:1804.10975 by other authors", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  Nov. 2019", "doi": "10.1109/TPAMI.2019.2954885", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction is a longstanding ill-posed problem, which has been\nexplored for decades by the computer vision, computer graphics, and machine\nlearning communities. Since 2015, image-based 3D reconstruction using\nconvolutional neural networks (CNN) has attracted increasing interest and\ndemonstrated an impressive performance. Given this new era of rapid evolution,\nthis article provides a comprehensive survey of the recent developments in this\nfield. We focus on the works which use deep learning techniques to estimate the\n3D shape of generic objects either from a single or multiple RGB images. We\norganize the literature based on the shape representations, the network\narchitectures, and the training mechanisms they use. While this survey is\nintended for methods which reconstruct generic objects, we also review some of\nthe recent works which focus on specific object classes such as human body\nshapes and faces. We provide an analysis and comparison of the performance of\nsome key papers, summarize some of the open problems in this field, and discuss\npromising directions for future research.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:35:05 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 15:51:43 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 14:01:31 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Han", "Xian-Feng", ""], ["Laga", "Hamid", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1906.06600", "submitter": "Oliver Bimber", "authors": "Indrajit Kurmi and David C. Schedl and Oliver Bimber", "title": "A Statistical View on Synthetic Aperture Imaging for Occlusion Removal", "comments": "10 pages, 11 figures, IEEE Sensors Jounral (accepted)", "journal-ref": null, "doi": "10.1109/JSEN.2019.2922731", "report-no": "upload03", "categories": "cs.GR cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic apertures find applications in many fields, such as radar, radio\ntelescopes, microscopy, sonar, ultrasound, LiDAR, and optical imaging. They\napproximate the signal of a single hypothetical wide aperture sensor with\neither an array of static small aperture sensors or a single moving small\naperture sensor. Common sense in synthetic aperture sampling is that a dense\nsampling pattern within a wide aperture is required to reconstruct a clear\nsignal. In this article we show that there exists practical limits to both,\nsynthetic aperture size and number of samples for the application of occlusion\nremoval. This leads to an understanding on how to design synthetic aperture\nsampling patterns and sensors in a most optimal and practically efficient way.\nWe apply our findings to airborne optical sectioning which uses camera drones\nand synthetic aperture imaging to computationally remove occluding vegetation\nor trees for inspecting ground surfaces.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 18:28:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Kurmi", "Indrajit", ""], ["Schedl", "David C.", ""], ["Bimber", "Oliver", ""]]}, {"id": "1906.06642", "submitter": "Fei Wen", "authors": "Fei Wen, Rendong Ying, Yipeng Liu, Peilin Liu, Trieu-Kien Truong", "title": "A Simple Local Minimal Intensity Prior and An Improved Algorithm for\n  Blind Image Deblurring", "comments": "14 pages, 16 figures", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  2020, https://ieeexplore.ieee.org/document/9241002", "doi": "10.1109/TCSVT.2020.3034137", "report-no": null, "categories": "eess.IV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring is a long standing challenging problem in image\nprocessing and low-level vision. Recently, sophisticated priors such as dark\nchannel prior, extreme channel prior, and local maximum gradient prior, have\nshown promising effectiveness. However, these methods are computationally\nexpensive. Meanwhile, since these priors involved subproblems cannot be solved\nexplicitly, approximate solution is commonly used, which limits the best\nexploitation of their capability. To address these problems, this work firstly\nproposes a simplified sparsity prior of local minimal pixels, namely patch-wise\nminimal pixels (PMP). The PMP of clear images is much more sparse than that of\nblurred ones, and hence is very effective in discriminating between clear and\nblurred images. Then, a novel algorithm is designed to efficiently exploit the\nsparsity of PMP in deblurring. The new algorithm flexibly imposes sparsity\ninducing on the PMP under the MAP framework rather than directly uses the half\nquadratic splitting algorithm. By this, it avoids non-rigorous approximation\nsolution in existing algorithms, while being much more computationally\nefficient. Extensive experiments demonstrate that the proposed algorithm can\nachieve better practical stability compared with state-of-the-arts. In terms of\ndeblurring quality, robustness and computational efficiency, the new algorithm\nis superior to state-of-the-arts. Code for reproducing the results of the new\nmethod is available at https://github.com/FWen/deblur-pmp.git.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 03:55:19 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 15:58:16 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 03:45:50 GMT"}, {"version": "v4", "created": "Fri, 31 Jan 2020 08:06:57 GMT"}, {"version": "v5", "created": "Thu, 29 Oct 2020 09:32:38 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wen", "Fei", ""], ["Ying", "Rendong", ""], ["Liu", "Yipeng", ""], ["Liu", "Peilin", ""], ["Truong", "Trieu-Kien", ""]]}, {"id": "1906.06726", "submitter": "Ulrik G\\\"unther", "authors": "Ulrik G\\\"unther, Tobias Pietzsch, Aryaman Gupta, Kyle I.S. Harrington,\n  Pavel Tomancak, Stefan Gumhold, Ivo F. Sbalzarini", "title": "scenery: Flexible Virtual Reality Visualization on the Java VM", "comments": "Added IEEE DOI, version published at VIS 2019", "journal-ref": "2019 IEEE Visualization Conference (VIS), Vancouver, BC, Canada,\n  2019, pp. 1-5", "doi": "10.1109/VISUAL.2019.8933605", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Life science today involves computational analysis of a large amount and\nvariety of data, such as volumetric data acquired by state-of-the-art\nmicroscopes, or mesh data from analysis of such data or simulations.\nVisualization is often the first step in making sense of data, and a crucial\npart of building and debugging analysis pipelines. It is therefore important\nthat visualizations can be quickly prototyped, as well as developed or embedded\ninto full applications. In order to better judge spatiotemporal relationships,\nimmersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and\nassociated controllers are becoming invaluable tools. In this work we introduce\nscenery, a flexible VR/AR visualization framework for the Java VM that can\nhandle mesh and large volumetric data, containing multiple views, timepoints,\nand color channels. scenery is free and open-source software, works on all\nmajor platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce\nscenery's main features and example applications, such as its use in VR for\nmicroscopy, in the biomedical image analysis software Fiji, or for visualizing\nagent-based simulations.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 17:01:20 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 13:55:29 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 18:17:35 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["G\u00fcnther", "Ulrik", ""], ["Pietzsch", "Tobias", ""], ["Gupta", "Aryaman", ""], ["Harrington", "Kyle I. S.", ""], ["Tomancak", "Pavel", ""], ["Gumhold", "Stefan", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1906.06751", "submitter": "Adriano Raposo", "authors": "Adriano N. Raposo and Abel J.P. Gomes", "title": "Pi-surfaces: products of implicit surfaces towards constructive\n  composition of 3D objects", "comments": null, "journal-ref": "WSCG 2019 27. International Conference in Central Europe on\n  Computer Graphics, Visualization and Computer Vision", "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit functions provide a fundamental basis to model 3D objects, no matter\nthey are rigid or deformable, in computer graphics and geometric modeling. This\npaper introduces a new constructive scheme of implicitly-defined 3D objects\nbased on products of implicit functions. This scheme is in contrast with\npopular approaches like blobbies, meta balls and soft objects, which rely on\nthe sum of specific implicit functions to fit a 3D object to a set of spheres.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:52:31 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Raposo", "Adriano N.", ""], ["Gomes", "Abel J. P.", ""]]}, {"id": "1906.07316", "submitter": "John Flynn", "authors": "John Flynn, Michael Broxton, Paul Debevec, Matthew DuVall, Graham\n  Fyffe, Ryan Overbeck, Noah Snavely, Richard Tucker", "title": "DeepView: View Synthesis with Learned Gradient Descent", "comments": "See https://augmentedperception.github.io/deepview/ for more results,\n  video and an interactive viewer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to view synthesis using multiplane images (MPIs).\nBuilding on recent advances in learned gradient descent, our algorithm\ngenerates an MPI from a set of sparse camera viewpoints. The resulting method\nincorporates occlusion reasoning, improving performance on challenging scene\nfeatures such as object boundaries, lighting reflections, thin structures, and\nscenes with high depth complexity. We show that our method achieves\nhigh-quality, state-of-the-art results on two datasets: the Kalantari light\nfield dataset, and a new camera array dataset, Spaces, which we make publicly\navailable.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:29:27 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Flynn", "John", ""], ["Broxton", "Michael", ""], ["Debevec", "Paul", ""], ["DuVall", "Matthew", ""], ["Fyffe", "Graham", ""], ["Overbeck", "Ryan", ""], ["Snavely", "Noah", ""], ["Tucker", "Richard", ""]]}, {"id": "1906.07409", "submitter": "Chenyang Zhu", "authors": "Lintao Zheng, Chenyang Zhu, Jiazhao Zhang, Hang Zhao, Hui Huang,\n  Matthias Niessner and Kai Xu", "title": "Active Scene Understanding via Online Semantic Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to robot-operated active understanding of unknown\nindoor scenes, based on online RGBD reconstruction with semantic segmentation.\nIn our method, the exploratory robot scanning is both driven by and targeting\nat the recognition and segmentation of semantic objects from the scene. Our\nalgorithm is built on top of the volumetric depth fusion framework (e.g.,\nKinectFusion) and performs real-time voxel-based semantic labeling over the\nonline reconstructed volume. The robot is guided by an online estimated\ndiscrete viewing score field (VSF) parameterized over the 3D space of 2D\nlocation and azimuth rotation. VSF stores for each grid the score of the\ncorresponding view, which measures how much it reduces the uncertainty\n(entropy) of both geometric reconstruction and semantic labeling. Based on VSF,\nwe select the next best views (NBV) as the target for each time step. We then\njointly optimize the traverse path and camera trajectory between two adjacent\nNBVs, through maximizing the integral viewing score (information gain) along\npath and trajectory. Through extensive evaluation, we show that our method\nachieves efficient and accurate online scene parsing during exploratory\nscanning.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 07:15:27 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zheng", "Lintao", ""], ["Zhu", "Chenyang", ""], ["Zhang", "Jiazhao", ""], ["Zhao", "Hang", ""], ["Huang", "Hui", ""], ["Niessner", "Matthias", ""], ["Xu", "Kai", ""]]}, {"id": "1906.07631", "submitter": "Fehmi Cirak", "authors": "Ge Yin, Xiao Xiao, Fehmi Cirak", "title": "Topologically robust CAD model generation for structural optimisation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113102", "report-no": null, "categories": "cs.GR cs.CE cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided design (CAD) models play a crucial role in the design,\nmanufacturing and maintenance of products. Therefore, the mesh-based finite\nelement descriptions common in structural optimisation must be first translated\ninto CAD models. Currently, this can at best be performed semi-manually. We\npropose a fully automated and topologically accurate approach to synthesise a\nstructurally-sound parametric CAD model from topology optimised finite element\nmodels. Our solution is to first convert the topology optimised structure into\na spatial frame structure and then to regenerate it in a CAD system using\nstandard constructive solid geometry (CSG) operations. The obtained parametric\nCAD models are compact, that is, have as few as possible geometric parameters,\nwhich makes them ideal for editing and further processing within a CAD system.\nThe critical task of converting the topology optimised structure into an\noptimal spatial frame structure is accomplished in several steps. We first\ngenerate from the topology optimised voxel model a one-voxel-wide voxel chain\nmodel using a topology-preserving skeletonisation algorithm from digital\ntopology. The weighted undirected graph defined by the voxel chain model yields\na spatial frame structure after processing it with standard graph algorithms.\nSubsequently, we optimise the cross-sections and layout of the frame members to\nrecover its optimality, which may have been compromised during the conversion\nprocess. At last, we generate the obtained frame structure in a CAD system by\nrepeatedly combining primitive solids, like cylinders and spheres, using\nboolean operations. The resulting solid model is a boundary representation\n(B-Rep) consisting of trimmed non-uniform rational B-spline (NURBS) curves and\nsurfaces.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 15:09:02 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 22:20:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Yin", "Ge", ""], ["Xiao", "Xiao", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1906.07645", "submitter": "Alice Othmani", "authors": "Alice Othmani, Fakhri Torkhani, Jean-Marie Favreau", "title": "3D Geometric salient patterns analysis on 3D meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Pattern analysis is a wide domain that has wide applicability in many fields.\nIn fact, texture analysis is one of those fields, since the texture is defined\nas a set of repetitive or quasi-repetitive patterns. Despite its importance in\nanalyzing 3D meshes, geometric texture analysis is less studied by geometry\nprocessing community. This paper presents a new efficient approach for\ngeometric texture analysis on 3D triangular meshes. The proposed method is a\nscale-aware approach that takes as input a 3D mesh and a user-scale. It\nprovides, as a result, a similarity-based clustering of texels in meaningful\nclasses. Experimental results of the proposed algorithm are presented for both\nreal-world and synthetic meshes within various textures. Furthermore, the\nefficiency of the proposed approach was experimentally demonstrated under mesh\nsimplification and noise addition on the mesh surface. In this paper, we\npresent a practical application for semantic annotation of 3D geometric salient\ntexels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:43:14 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Othmani", "Alice", ""], ["Torkhani", "Fakhri", ""], ["Favreau", "Jean-Marie", ""]]}, {"id": "1906.07716", "submitter": "Daniel Karl I. Weidele", "authors": "Daniel Karl I. Weidele", "title": "Conditional Parallel Coordinates", "comments": "5 pages, 8 figures, VIS 2019 Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel Coordinates are a popular data visualization technique for\nmultivariate data. Dating back to as early as 1880 PC are nearly as old as John\nSnow's famous cholera outbreak map of 1855, which is frequently regarded as a\nhistoric landmark for modern data visualization. Numerous extensions have been\nproposed to address integrity, scalability and readability. We make a new case\nto employ PC on conditional data, where additional dimensions are only unfolded\nif certain criteria are met in an observation. Compared to standard PC which\noperate on a flat set of dimensions the ontology of our input to Conditional\nParallel Coordinates is of hierarchical nature. We therefore briefly review\nrelated work around hierarchical PC using aggregation or nesting techniques.\nOur contribution is a visualization to seamlessly adapt PC for conditional data\nunder preservation of intuitive interaction patterns to select or highlight\npolylines. We conclude with intuitions on how to operate CPC on two data sets:\nan AutoML hyperparameter search log, and session results from a conversational\nagent.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 17:58:49 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 13:43:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Weidele", "Daniel Karl I.", ""]]}, {"id": "1906.07751", "submitter": "Stephen Lombardi", "authors": "Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz,\n  Andreas Lehrmann, Yaser Sheikh", "title": "Neural Volumes: Learning Dynamic Renderable Volumes from Images", "comments": "Accepted to SIGGRAPH 2019", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38, 4, Article 65", "doi": "10.1145/3306346.3323020", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and rendering of dynamic scenes is challenging, as natural scenes\noften contain complex phenomena such as thin structures, evolving topology,\ntranslucency, scattering, occlusion, and biological motion. Mesh-based\nreconstruction and tracking often fail in these cases, and other approaches\n(e.g., light field video) typically rely on constrained viewing conditions,\nwhich limit interactivity. We circumvent these difficulties by presenting a\nlearning-based approach to representing dynamic objects inspired by the\nintegral projection model used in tomographic imaging. The approach is\nsupervised directly from 2D images in a multi-view capture setting and does not\nrequire explicit reconstruction or tracking of the object. Our method has two\nprimary components: an encoder-decoder network that transforms input images\ninto a 3D volume representation, and a differentiable ray-marching operation\nthat enables end-to-end training. By virtue of its 3D representation, our\nconstruction extrapolates better to novel viewpoints compared to screen-space\nrendering techniques. The encoder-decoder architecture learns a latent\nrepresentation of a dynamic scene that enables us to produce novel content\nsequences not seen during training. To overcome memory limitations of\nvoxel-based representations, we learn a dynamic irregular grid structure\nimplemented with a warp field during ray-marching. This structure greatly\nimproves the apparent resolution and reduces grid-like artifacts and jagged\nmotion. Finally, we demonstrate how to incorporate surface-based\nrepresentations into our volumetric-learning framework for applications where\nthe highest resolution is required, using facial performance capture as a case\nin point.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:21:46 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Lombardi", "Stephen", ""], ["Simon", "Tomas", ""], ["Saragih", "Jason", ""], ["Schwartz", "Gabriel", ""], ["Lehrmann", "Andreas", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1906.07870", "submitter": "Zaiqiang Wu", "authors": "Zaiqiang Wu, Wei Jiang", "title": "Analytical Derivatives for Differentiable Renderer: 3D Pose Estimation\n  by Silhouette Consistency", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable render is widely used in optimization-based 3D reconstruction\nwhich requires gradients from differentiable operations for gradient-based\noptimization. The existing differentiable renderers obtain the gradients of\nrendering via numerical technique which is of low accuracy and efficiency.\nMotivated by this fact, a differentiable mesh renderer with analytical\ngradients is proposed. The main obstacle of rasterization based rendering being\ndifferentiable is the discrete sampling operation. To make the rasterization\ndifferentiable, the pixel intensity is defined as a double integral over the\npixel area and the integral is approximated by anti-aliasing with an average\nfilter. Then the analytical gradients with respect to the vertices coordinates\ncan be derived from the continuous definition of pixel intensity. To\ndemonstrate the effectiveness and efficiency of the proposed differentiable\nrenderer, experiments of 3D pose estimation by only multi-viewpoint silhouettes\nwere conducted. The experimental results show that 3D pose estimation without\n3D and 2D joints supervision is capable of producing competitive results both\nqualitatively and quantitatively. The experimental results also show that the\nproposed differentiable renderer is of higher accuracy and efficiency compared\nwith previous method of differentiable renderer.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:34:12 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wu", "Zaiqiang", ""], ["Jiang", "Wei", ""]]}, {"id": "1906.07968", "submitter": "Bernie Liu", "authors": "Xinyu Wei, Mengjia Zhou, Bernie Liu", "title": "Camouflage Design of Analysis Based on HSV Color Statistics and K-means\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since ancient times, it has been essential to adopting camouflage on the\nbattlefield, whether it is in the forefront, in-depth or the rear. The\ntraditional evaluation method is made up of people opinion. By watching target\nor looking at the pictures, and determine the effect of camouflage, so it can\nbe more influenced by man's subjective factors. And now, in order to\nobjectively reflect the camouflage effect, we set up a model through using\nimages similarity to evaluate camouflage effect. Image similarity comparison is\ndivided into two main image feature comparison: image color features and\ntexture features of images. We now using computer design camouflage, camouflage\npattern design is divided into two aspects of design color and design plaques.\nFor the design of the color, we based on HSV color model, and as for the design\nof plague, the key steps are the background color edge extraction, we adopt\nalgorithm based on k-means clustering analysis of the method of background\ncolor edge extraction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 08:30:53 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wei", "Xinyu", ""], ["Zhou", "Mengjia", ""], ["Liu", "Bernie", ""]]}, {"id": "1906.08433", "submitter": "Qiang Zou", "authors": "Qiang Zou, Hsi-Yung Feng", "title": "A decision-support method for information inconsistency resolution in\n  direct modeling of CAD models", "comments": "20 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.aei.2020.101087", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct modeling is a very recent CAD paradigm that can provide unprecedented\nmodeling flexibility. It, however, lacks the parametric capability, which is\nindispensable to modern CAD systems. For direct modeling to have this\ncapability, an additional associativity information layer in the form of\ngeometric constraint systems needs to be incorporated into direct modeling.\nThis is no trivial matter due to the possible inconsistencies between the\nassociativity information and geometry information in a model after direct\nedits. The major issue of resolving such inconsistencies is that there often\nexist many resolution options. The challenge lies in avoiding invalid\nresolution options and prioritizing valid ones. This paper presents an\neffective method to support the user in making decisions among the resolution\noptions. In particular, the method can provide automatic information\ninconsistency reasoning, avoid invalid resolution options completely, and guide\nthe choice among valid resolution options. Case studies and comparisons have\nbeen conducted to demonstrate the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 03:58:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zou", "Qiang", ""], ["Feng", "Hsi-Yung", ""]]}, {"id": "1906.09457", "submitter": "Paul Rosen", "authors": "Paul Rosen, Ashley Suh, Christopher Salgado, Mustafa Hajij", "title": "TopoLines: Topological Smoothing for Line Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line charts are commonly used to visualize a series of data values. When the\ndata are noisy, smoothing is applied to make the signal more apparent.\nConventional methods used to smooth line charts, e.g., using subsampling or\nfilters, such as median, Gaussian, or low-pass, each optimize for different\nproperties of the data. The properties generally do not include retaining peaks\n(i.e., local minima and maxima) in the data, which is an important feature for\ncertain visual analytics tasks. We present TopoLines, a method for smoothing\nline charts using techniques from Topological Data Analysis. The design goal of\nTopoLines is to maintain prominent peaks in the data while minimizing any\nresidual error. We evaluate TopoLines for 2 visual analytics tasks by comparing\nto 5 popular line smoothing methods with data from 4 application domains.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 14:59:54 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 20:08:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rosen", "Paul", ""], ["Suh", "Ashley", ""], ["Salgado", "Christopher", ""], ["Hajij", "Mustafa", ""]]}, {"id": "1906.09740", "submitter": "Robert Konrad", "authors": "Robert Konrad, Anastasios Angelopoulos, and Gordon Wetzstein", "title": "Gaze-Contingent Ocular Parallax Rendering for Virtual Reality", "comments": "Video: https://www.youtube.com/watch?v=FvBYYAObJNM&feature=youtu.be\n  Project Page:\n  http://www.computationalimaging.org/publications/gaze-contingent-ocular-parallax-rendering-for-virtual-reality/", "journal-ref": "ACM Trans. Graph. 39, 2, Article 10 (April 2020), 12 pages", "doi": "10.1145/3361330", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive computer graphics systems strive to generate perceptually realistic\nuser experiences. Current-generation virtual reality (VR) displays are\nsuccessful in accurately rendering many perceptually important effects,\nincluding perspective, disparity, motion parallax, and other depth cues. In\nthis article, we introduce ocular parallax rendering, a technology that\naccurately renders small amounts of gaze-contingent parallax capable of\nimproving depth perception and realism in VR. Ocular parallax describes the\nsmall amounts of depth-dependent image shifts on the retina that are created as\nthe eye rotates. The effect occurs because the centers of rotation and\nprojection of the eye are not the same. We study the perceptual implications of\nocular parallax rendering by designing and conducting a series of user\nexperiments. Specifically, we estimate perceptual detection and discrimination\nthresholds for this effect and demonstrate that it is clearly visible in most\nVR applications. Additionally, we show that ocular parallax rendering provides\nan effective ordinal depth cue and it improves the impression of realistic\ndepth in VR.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:11:30 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 00:03:48 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Konrad", "Robert", ""], ["Angelopoulos", "Anastasios", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1906.09787", "submitter": "I-Chao Shen", "authors": "I-Chao Shen, Ming-Shiuan Chen, Chun-Kai Huang, Bing-Yu Chen", "title": "ZomeFab: Cost-effective Hybrid Fabrication with Zometools", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, personalized fabrication has received considerable attention\nbecause of the widespread use of consumer-level three-dimensional (3D)\nprinters. However, such 3D printers have drawbacks, such as long production\ntime and limited output size, which hinder large-scale rapid-prototyping. In\nthis paper, for the time- and cost-effective fabrication of large-scale\nobjects, we propose a hybrid 3D fabrication method that combines 3D printing\nand the Zometool construction set, which is a compact, sturdy, and reusable\nstructure for infill fabrication. The proposed method significantly reduces\nfabrication cost and time by printing only thin 3D outer shells. In addition,\nwe design an optimization framework to generate both a Zometool structure and\nprinted surface partitions by optimizing several criteria, including\nprintability, material cost, and Zometool structure complexity. Moreover, we\ndemonstrate the effectiveness of the proposed method by fabricating various\nlarge-scale 3D models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:45:50 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 02:17:48 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Shen", "I-Chao", ""], ["Chen", "Ming-Shiuan", ""], ["Huang", "Chun-Kai", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1906.09840", "submitter": "I-Chao Shen", "authors": "Toby Chong Long Hin, I-Chao Shen, Issei Sato, Takeo Igarashi", "title": "Interactive Optimization of Generative Image Modeling using Sequential\n  Subspace Search and Content-based Guidance", "comments": "13 pages, Toby Chong Long Hin and I-Chao Shen contributed equally to\n  the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative image modeling techniques such as GAN demonstrate highly\nconvincing image generation result. However, user interaction is often\nnecessary to obtain the desired results. Existing attempts add interactivity\nbut require either tailored architectures or extra data. We present a\nhuman-in-the-optimization method that allows users to directly explore and\nsearch the latent vector space of generative image modeling. Our system\nprovides multiple candidates by sampling the latent vector space, and the user\nselects the best blending weights within the subspace using multiple sliders.\nIn addition, the user can express their intention through image editing tools.\nThe system samples latent vectors based on inputs and presents new candidates\nto the user iteratively. An advantage of our formulation is that one can apply\nour method to arbitrary pre-trained model without developing specialized\narchitecture or data. We demonstrate our method with various generative image\nmodeling applications, and show superior performance in a comparative user\nstudy with prior art iGAN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:34:37 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 01:53:04 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 09:05:11 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hin", "Toby Chong Long", ""], ["Shen", "I-Chao", ""], ["Sato", "Issei", ""], ["Igarashi", "Takeo", ""]]}, {"id": "1906.10669", "submitter": "Erva Ulu", "authors": "Erva Ulu, James McCann, Levent Burak Kara", "title": "Structural Design Using Laplacian Shells", "comments": "Eurographics Symposium on Geometry Processing (SGP) 2019 / Computer\n  Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to design lightweight shell objects that are\nstructurally robust under the external forces they may experience during use.\nGiven an input 3D model and a general description of the external forces, our\nalgorithm generates a structurally-sound minimum weight shell object. Our\napproach works by altering the local shell thickness repeatedly based on the\nstresses that develop inside the object. A key issue in shell design is that\nlarge thickness values might result in self-intersections on the inner boundary\ncreating a significant computational challenge during optimization. To address\nthis, we propose a shape parametrization based on the solution to the Laplace's\nequation that guarantees smooth and intersection-free shell boundaries.\nCombined with our gradient-free optimization algorithm, our method provides a\npractical solution to the structural design of hollow objects with a single\ninner cavity. We demonstrate our method on a variety of problems with arbitrary\n3D models under complex force configurations and validate its performance with\nphysical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 17:06:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ulu", "Erva", ""], ["McCann", "James", ""], ["Kara", "Levent Burak", ""]]}, {"id": "1906.10886", "submitter": "Peng Gao", "authors": "Zibin Zhou, Fei Wang, Wenjuan Xi, Huaying Chen, Peng Gao, Chengkang He", "title": "Joint Multi-frame Detection and Segmentation for Multi-cell Tracking", "comments": "Accepted by International Conference on Image and Graphics (ICIG\n  2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking living cells in video sequence is difficult, because of cell\nmorphology and high similarities between cells. Tracking-by-detection methods\nare widely used in multi-cell tracking. We perform multi-cell tracking based on\nthe cell centroid detection, and the performance of the detector has high\nimpact on tracking performance. In this paper, UNet is utilized to extract\ninter-frame and intra-frame spatio-temporal information of cells. Detection\nperformance of cells in mitotic phase is improved by multi-frame input. Good\ndetection results facilitate multi-cell tracking. A mitosis detection algorithm\nis proposed to detect cell mitosis and the cell lineage is built up. Another\nUNet is utilized to acquire primary segmentation. Jointly using detection and\nprimary segmentation, cells can be fine segmented in highly dense cell\npopulation. Experiments are conducted to evaluate the effectiveness of our\nmethod, and results show its state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:41:11 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Zhou", "Zibin", ""], ["Wang", "Fei", ""], ["Xi", "Wenjuan", ""], ["Chen", "Huaying", ""], ["Gao", "Peng", ""], ["He", "Chengkang", ""]]}, {"id": "1906.11478", "submitter": "Isaak Lim", "authors": "Isaak Lim, Moritz Ibing, Leif Kobbelt", "title": "A Convolutional Decoder for Point Clouds using Adaptive Instance\n  Normalization", "comments": "Symposium on Geometry Processing 2019", "journal-ref": "Computer Graphics Forum 38 (5), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic synthesis of high quality 3D shapes is an ongoing and challenging\narea of research. While several data-driven methods have been proposed that\nmake use of neural networks to generate 3D shapes, none of them reach the level\nof quality that deep learning synthesis approaches for images provide. In this\nwork we present a method for a convolutional point cloud decoder/generator that\nmakes use of recent advances in the domain of image synthesis. Namely, we use\nAdaptive Instance Normalization and offer an intuition on why it can improve\ntraining. Furthermore, we propose extensions to the minimization of the\ncommonly used Chamfer distance for auto-encoding point clouds. In addition, we\nshow that careful sampling is important both for the input geometry and in our\npoint cloud generation process to improve results. The results are evaluated in\nan auto-encoding setup to offer both qualitative and quantitative analysis. The\nproposed decoder is validated by an extensive ablation study and is able to\noutperform current state of the art results in a number of experiments. We show\nthe applicability of our method in the fields of point cloud upsampling, single\nview reconstruction, and shape synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:53:02 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Lim", "Isaak", ""], ["Ibing", "Moritz", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1906.11557", "submitter": "Valentin Deschaintre", "authors": "Valentin Deschaintre, Miika Aittala, Fredo Durand, George Drettakis\n  and Adrien Bousseau", "title": "Flexible SVBRDF Capture with a Multi-Image Deep Network", "comments": "Accepted to EGSR 2019 in the CGF track", "journal-ref": "Computer Graphics Forum (EGSR Conference Proceedings), 38, 4(July\n  2019), 13 pages", "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empowered by deep learning, recent methods for material capture can estimate\na spatially-varying reflectance from a single photograph. Such lightweight\ncapture is in stark contrast with the tens or hundreds of pictures required by\ntraditional optimization-based approaches. However, a single image is often\nsimply not enough to observe the rich appearance of real-world materials. We\npresent a deep-learning method capable of estimating material appearance from a\nvariable number of uncalibrated and unordered pictures captured with a handheld\ncamera and flash. Thanks to an order-independent fusing layer, this\narchitecture extracts the most useful information from each picture, while\nbenefiting from strong priors learned from data. The method can handle both\nview and light direction variation without calibration. We show how our method\nimproves its prediction with the number of input pictures, and reaches high\nquality reconstructions with as little as 1 to 10 images -- a sweet spot\nbetween existing single-image and complex multi-image approaches.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 11:32:12 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Deschaintre", "Valentin", ""], ["Aittala", "Miika", ""], ["Durand", "Fredo", ""], ["Drettakis", "George", ""], ["Bousseau", "Adrien", ""]]}, {"id": "1906.11633", "submitter": "Lilian Weng", "authors": "Maciek Chociej, Peter Welinder, Lilian Weng", "title": "ORRB -- OpenAI Remote Rendering Backend", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the OpenAI Remote Rendering Backend (ORRB), a system that allows\nfast and customizable rendering of robotics environments. It is based on the\nUnity3d game engine and interfaces with the MuJoCo physics simulation library.\nORRB was designed with visual domain randomization in mind. It is optimized for\ncloud deployment and high throughput operation. We are releasing it to the\npublic under a liberal MIT license: https://github.com/openai/orrb .\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 16:58:46 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chociej", "Maciek", ""], ["Welinder", "Peter", ""], ["Weng", "Lilian", ""]]}, {"id": "1906.11686", "submitter": "Christoph Gebhardt", "authors": "Christoph Gebhardt, Stefan Stevsic, Otmar Hilliges", "title": "Optimizing for Aesthetically Pleasing Quadrotor Camera Motion", "comments": null, "journal-ref": "ACM Transactions on Graphics (TOG), Volume 37 Issue 4, August\n  2018, Article No. 90", "doi": "10.1145/3197517.3201390", "report-no": null, "categories": "cs.GR cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first contribute a large scale online study (N=400) to\nbetter understand aesthetic perception of aerial video. The results indicate\nthat it is paramount to optimize smoothness of trajectories across all\nkeyframes. However, for experts timing control remains an essential tool.\nSatisfying this dual goal is technically challenging because it requires giving\nup desirable properties in the optimization formulation. Second, informed by\nthis study we propose a method that optimizes positional and temporal reference\nfit jointly. This allows to generate globally smooth trajectories, while\nretaining user control over reference timings. The formulation is posed as a\nvariable, infinite horizon, contour-following algorithm. Finally, a comparative\nlab study indicates that our optimization scheme outperforms the\nstate-of-the-art in terms of perceived usability and preference of resulting\nvideos. For novices our method produces smoother and better looking results and\nalso experts benefit from generated timings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:28:28 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Gebhardt", "Christoph", ""], ["Stevsic", "Stefan", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1906.11738", "submitter": "Waleed Yousef", "authors": "Waleed A. Yousef, Ahmed A. Abouelkahire, Omar S. Marzouk, Sameh K.\n  Mohamed, Mohamed N. Alaggan", "title": "DVP: Data Visualization Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify two major steps in data analysis, data exploration for\nunderstanding and observing patterns/relationships in data; and construction,\ndesign and assessment of various models to formalize these relationships. For\neach step, there exists a large set of tools and software. For the first step,\nmany visualization tools exist, such as, GGobi, Parallax, and Crystal Vision,\nand most recently tableau and plottly. For the second step, many Scientific\nComputing Environments (SCEs) exist, such as, Matlab, Mathematica, R and\nPython. However, there does not exist a tool which allows for seamless two-way\ninteraction between visualization tools and SCEs. We have designed and\nimplemented a data visualization platform (DVP) with an architecture and design\nthat attempts to bridge this gap. DVP connects seamlessly to SCEs to bring the\ncomputational capabilities to the visualization methods in a single coherent\nplatform. DVP is designed with two interfaces, the desktop stand alone version\nand the online interface. To illustrate the power of DVP design, a free demo\nfor the online interface of DVP is available \\citep{DVP} and very low-level\ndesign details are explained in this article. Since DVP was launched, circa\n2012, the present manuscript was not published since today for\ncommercialization and patent considerations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:35:31 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 20:06:53 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Yousef", "Waleed A.", ""], ["Abouelkahire", "Ahmed A.", ""], ["Marzouk", "Omar S.", ""], ["Mohamed", "Sameh K.", ""], ["Alaggan", "Mohamed N.", ""]]}, {"id": "1906.11925", "submitter": "Xiaojun Zhai", "authors": "Sahar Soheilian Esfahani, Xiaojun Zhai, Minsi Chen, Abbes Amira,\n  Faycal Bensaali, Julien AbiNahed, Sarada Dakua, Georges Younes, Robin A.\n  Richardson, Peter V. Coveney", "title": "HEMELB Acceleration and Visualization for Cerebral Aneurysms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakness in the wall of a cerebral artery causing a dilation or ballooning\nof the blood vessel is known as a cerebral aneurysm. Optimal treatment requires\nfast and accurate diagnosis of the aneurysm. HemeLB is a fluid dynamics solver\nfor complex geometries developed to provide neurosurgeons with information\nrelated to the flow of blood in and around aneurysms. On a cost efficient\nplatform, HemeLB could be employed in hospitals to provide surgeons with the\nsimulation results in real-time. In this work, we developed an improved version\nof HemeLB for GPU implementation and result visualization. A visualization\nplatform for smooth interaction with end users is also presented. Finally, a\ncomprehensive evaluation of this implementation is reported. The results\ndemonstrate that the proposed implementation achieves a maximum performance of\n15,168,964 site updates per second, and is capable of speeding up HemeLB for\ndeployment in hospitals and clinical investigations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:23:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Esfahani", "Sahar Soheilian", ""], ["Zhai", "Xiaojun", ""], ["Chen", "Minsi", ""], ["Amira", "Abbes", ""], ["Bensaali", "Faycal", ""], ["AbiNahed", "Julien", ""], ["Dakua", "Sarada", ""], ["Younes", "Georges", ""], ["Richardson", "Robin A.", ""], ["Coveney", "Peter V.", ""]]}, {"id": "1906.11957", "submitter": "Amir Abdi", "authors": "Amir H. Abdi, Mehran Pesteie, Eitan Prisman, Purang Abolmaesumi,\n  Sidney Fels", "title": "Variational Shape Completion for Virtual Planning of Jaw Reconstructive\n  Surgery", "comments": "Proceedings of Medical Image Computing and Computer Assisted\n  Intervention - {MICCAI} 2019", "journal-ref": null, "doi": "10.1007/978-3-030-32254-0\\_26", "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The premorbid geometry of the mandible is of significant relevance in jaw\nreconstructive surgeries and occasionally unknown to the surgical team. In this\npaper, an optimization framework is introduced to train deep models for\ncompletion (reconstruction) of the missing segments of the bone based on the\nremaining healthy structure. To leverage the contextual information of the\nsurroundings of the dissected region, the voxel-weighted Dice loss is\nintroduced. To address the non-deterministic nature of the shape completion\nproblem, we leverage a weighted multi-target probabilistic solution which is an\nextension to the conditional variational autoencoder (CVAE). This approach\nconsiders multiple targets as acceptable reconstructions, each weighted\naccording to their conformity with the original shape. We quantify the\nperformance gain of the proposed method against similar algorithms, including\nCVAE, where we report statistically significant improvements in both\ndeterministic and probabilistic paradigms. The probabilistic model is also\nevaluated on its ability to generate anatomically relevant variations for the\nmissing bone. As a unique aspect of this work, the model is tested on real\nsurgical cases where the clinical relevancy of its reconstructions and their\ncompliance with surgeon's virtual plan are demonstrated as necessary steps\ntowards clinical adoption.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:45:46 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 23:42:47 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 18:44:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Abdi", "Amir H.", ""], ["Pesteie", "Mehran", ""], ["Prisman", "Eitan", ""], ["Abolmaesumi", "Purang", ""], ["Fels", "Sidney", ""]]}, {"id": "1906.11999", "submitter": "Chaoyang He", "authors": "Chaoyang He, Ming Li", "title": "Efficient Spatial Anti-Aliasing Rendering for Line Joins on Vector Maps", "comments": "4 pages. Submitted to 1st ACM SIGSPATIAL International Workshop on\n  Spatial Gems (SpatialGems 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The spatial anti-aliasing technique for line joins (intersections of the road\nsegments) on vector maps is exclusively crucial to visual experience and system\nperformance. Due to limitations of OpenGL API, one common practice to achieve\nthe anti-aliased effect is splicing multiple triangles at varying scale levels\nto approximate the fan-shaped line joins. However, this approximation\ninevitably produces some unreality, and the system rendering performance is not\noptimal. To circumvent these drawbacks, in this paper, we propose a simple but\nefficient algorithm which uses only two triangles to substitute the multiple\ntriangles approximation and then renders a realistic fan-shaped curve with\nalpha operation based on geometrical relation computing. Our experiment shows\nit has advantages of a realistic anti-aliasing effect, less memory cost, higher\nframe rate, and drawing line joins without overlapping rendering. Our proposed\nspatial anti-aliasing technique has been widely used in Internet Maps such as\nTencent Mobile Maps and Tencent Automotive Maps.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 23:58:37 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["He", "Chaoyang", ""], ["Li", "Ming", ""]]}, {"id": "1906.12337", "submitter": "Dmitriy Smirnov", "authors": "Dmitriy Smirnov, Mikhail Bessmeltsev and Justin Solomon", "title": "Learning Manifold Patch-Based Representations of Man-Made Shapes", "comments": "Accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing the right representation for geometry is crucial for making 3D\nmodels compatible with existing applications. Focusing on piecewise-smooth\nman-made shapes, we propose a new representation that is usable in conventional\nCAD modeling pipelines and can also be learned by deep neural networks. We\ndemonstrate its benefits by applying it to the task of sketch-based modeling.\nGiven a raster image, our system infers a set of parametric surfaces that\nrealize the input in 3D. To capture piecewise smooth geometry, we learn a\nspecial shape representation: a deformable parametric template composed of\nCoons patches. Naively training such a system, however, is hampered by\nnon-manifold artifacts in the parametric shapes and by a lack of data. To\naddress this, we introduce loss functions that bias the network to output\nnon-self-intersecting shapes and implement them as part of a fully\nself-supervised system, automatically generating both shape templates and\nsynthetic training data. We develop a testbed for sketch-based modeling,\ndemonstrate shape interpolation, and provide comparison to related work.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:36:48 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 21:57:42 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 23:31:07 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Smirnov", "Dmitriy", ""], ["Bessmeltsev", "Mikhail", ""], ["Solomon", "Justin", ""]]}]