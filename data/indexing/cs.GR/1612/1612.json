[{"id": "1612.00132", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Aditya Deshpande, David Forsyth", "title": "CDVAE: Co-embedding Deep Variational Auto Encoder for Conditional\n  Variational Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems such as predicting a new shading field (Y) for an image (X) are\nambiguous: many very distinct solutions are good. Representing this ambiguity\nrequires building a conditional model P(Y|X) of the prediction, conditioned on\nthe image. Such a model is difficult to train, because we do not usually have\ntraining data containing many different shadings for the same image. As a\nresult, we need different training examples to share data to produce good\nmodels. This presents a danger we call \"code space collapse\" - the training\nprocedure produces a model that has a very good loss score, but which\nrepresents the conditional distribution poorly. We demonstrate an improved\nmethod for building conditional models by exploiting a metric constraint on\ntraining data that prevents code space collapse. We demonstrate our model on\ntwo example tasks using real data: image saturation adjustment, image\nrelighting. We describe quantitative metrics to evaluate ambiguous generation\nresults. Our results quantitatively and qualitatively outperform different\nstrong baselines.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 03:40:42 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 03:21:34 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Lu", "Jiajun", ""], ["Deshpande", "Aditya", ""], ["Forsyth", "David", ""]]}, {"id": "1612.00522", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Kalyan Sunkavalli, Nathan Carr, Sunil Hadap, David Forsyth", "title": "A Visual Representation for Editing Face Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for editing face images, which enables numerous\nexciting applications including face relighting, makeup transfer and face\ndetail editing. Our face edits are based on a visual representation, which\nincludes geometry, face segmentation, albedo, illumination and detail map. To\nrecover our visual representation, we start by estimating geometry using a\nmorphable face model, then decompose the face image to recover the albedo, and\nthen shade the geometry with the albedo and illumination. The residual between\nour shaded geometry and the input image produces our detail map, which carries\nhigh frequency information that is either insufficiently or incorrectly\ncaptured by our shading process. By manipulating the detail map, we can edit\nface images with reality and identity preserved. Our representation allows\nvarious applications. First, it allows a user to directly manipulate various\nillumination. Second, it allows non-parametric makeup transfer with input\nface's distinctive identity features preserved. Third, it allows non-parametric\nmodifications to the face appearance by transferring details. For face\nrelighting and detail editing, we evaluate via a user study and our method\noutperforms other methods. For makeup transfer, we evaluate via an online\nattractiveness evaluation system, and can reliably make people look younger and\nmore attractive. We also show extensive qualitative comparisons to existing\nmethods, and have significant improvements over previous techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:07:38 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Lu", "Jiajun", ""], ["Sunkavalli", "Kalyan", ""], ["Carr", "Nathan", ""], ["Hadap", "Sunil", ""], ["Forsyth", "David", ""]]}, {"id": "1612.00523", "submitter": "Shunsuke Saito", "authors": "Shunsuke Saito, Lingyu Wei, Liwen Hu, Koki Nagano, Hao Li", "title": "Photorealistic Facial Texture Inference Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven inference method that can synthesize a\nphotorealistic texture map of a complete 3D face model given a partial 2D view\nof a person in the wild. After an initial estimation of shape and low-frequency\nalbedo, we compute a high-frequency partial texture map, without the shading\ncomponent, of the visible face area. To extract the fine appearance details\nfrom this incomplete input, we introduce a multi-scale detail analysis\ntechnique based on mid-layer feature correlations extracted from a deep\nconvolutional neural network. We demonstrate that fitting a convex combination\nof feature correlations from a high-resolution face database can yield a\nsemantically plausible facial detail description of the entire face. A complete\nand photorealistic texture map can then be synthesized by iteratively\noptimizing for the reconstructed feature correlations. Using these\nhigh-resolution textures and a commercial rendering framework, we can produce\nhigh-fidelity 3D renderings that are visually comparable to those obtained with\nstate-of-the-art multi-view face capture systems. We demonstrate successful\nface reconstructions from a wide range of low resolution input images,\nincluding those of historical figures. In addition to extensive evaluations, we\nvalidate the realism of our results using a crowdsourced user study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 00:14:12 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Saito", "Shunsuke", ""], ["Wei", "Lingyu", ""], ["Hu", "Liwen", ""], ["Nagano", "Koki", ""], ["Li", "Hao", ""]]}, {"id": "1612.00814", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, Honglak Lee", "title": "Perspective Transformer Nets: Learning Single-View 3D Object\n  Reconstruction without 3D Supervision", "comments": "published at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding the 3D world is a fundamental problem in computer vision.\nHowever, learning a good representation of 3D objects is still an open problem\ndue to the high dimensionality of the data and many factors of variation\ninvolved. In this work, we investigate the task of single-view 3D object\nreconstruction from a learning agent's perspective. We formulate the learning\nprocess as an interaction between 3D and 2D representations and propose an\nencoder-decoder network with a novel projection loss defined by the perspective\ntransformation. More importantly, the projection loss enables the unsupervised\nlearning using 2D observation without explicit 3D supervision. We demonstrate\nthe ability of the model in generating 3D volume from a single 2D image with\nthree sets of experiments: (1) learning from single-class objects; (2) learning\nfrom multi-class objects and (3) testing on novel object classes. Results show\nsuperior performance and better generalization ability for 3D object\nreconstruction when the projection loss is involved.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 05:51:37 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 07:08:48 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 02:40:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yan", "Xinchen", ""], ["Yang", "Jimei", ""], ["Yumer", "Ersin", ""], ["Guo", "Yijie", ""], ["Lee", "Honglak", ""]]}, {"id": "1612.01944", "submitter": "Ke Liu", "authors": "Ke Liu, Ye Guo, Zeyun Yu", "title": "Porous Structure Design in Tissue Engineering Using Anisotropic Radial\n  Basis Function", "comments": "Department of Computer Science, University of Wisconsin Milwaukee", "journal-ref": "International Symposium on Visual Computing (ISVC) 2018, Advances\n  in Visual Computing, Lecture Notes in Computer Science, vol. 11241, pp. 79-90", "doi": "10.1007/978-3-030-03801-4_8", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of additive manufacturing in last decade greatly improves tissue\nengineering. During the manufacturing of porous scaffold, simplified but\nfunctionally equivalent models are getting focused for practically reasons.\nScaffolds can be classified into regular porous scaffolds and irregular porous\nscaffolds. Several methodologies are developed to design these scaffolds. A\nnovel method is proposed in this paper using anisotropic radial basis function\n(ARBF) interpolation. This is method uses geometric models such as volumetric\nmeshes as input and proves to be flexible because geometric models are able to\ncapture the characteristics of complex tissues easily. Moreover, this method is\nstraightforward and easy to implement.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:42:44 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 22:00:34 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Ke", ""], ["Guo", "Ye", ""], ["Yu", "Zeyun", ""]]}, {"id": "1612.02261", "submitter": "Julie Digne", "authors": "Julie Digne and S\\'ebastien Valette and Rapha\\\"elle Chaine", "title": "Sparse Geometric Representation Through Local Shape Probing", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2017", "doi": "10.1109/TVCG.2017.2719024", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new shape analysis approach based on the non-local analysis of\nlocal shape variations. Our method relies on a novel description of shape\nvariations, called Local Probing Field (LPF), which describes how a local\nprobing operator transforms a pattern onto the shape. By carefully optimizing\nthe position and orientation of each descriptor, we are able to capture shape\nsimilarities and gather them into a geometrically relevant dictionary over\nwhich the shape decomposes sparsely. This new representation permits to handle\nshapes with mixed intrinsic dimensionality (e.g. shapes containing both\nsurfaces and curves) and to encode various shape features such as boundaries.\nOur shape representation has several potential applications; here we\ndemonstrate its efficiency for shape resampling and point set denoising for\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 14:26:36 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 15:37:46 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Digne", "Julie", ""], ["Valette", "S\u00e9bastien", ""], ["Chaine", "Rapha\u00eblle", ""]]}, {"id": "1612.02509", "submitter": "Ayushi Sinha", "authors": "Ayushi Sinha, Michael Kazhdan", "title": "Geodesics using Waves: Computing Distances using Wave Propagation", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new method for computing approximate geodesic\ndistances. We introduce the wave method for approximating geodesic distances\nfrom a point on a manifold mesh. Our method involves the solution of two linear\nsystems of equations. One system of equations is solved repeatedly to propagate\nthe wave on the entire mesh, and one system is solved once after wave\npropagation is complete in order to compute the approximate geodesic distances\nup to an additive constant. However, these systems need to be pre-factored only\nonce, and can be solved efficiently at each iteration. All of our tests\nrequired approximately between 300 and 400 iterations, which were completed in\na few seconds. Therefore, this method can approximate geodesic distances\nquickly, and the approximation is highly accurate.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 01:57:47 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Sinha", "Ayushi", ""], ["Kazhdan", "Michael", ""]]}, {"id": "1612.02808", "submitter": "Evangelos Kalogerakis", "authors": "Evangelos Kalogerakis, Melinos Averkiou, Subhransu Maji, Siddhartha\n  Chaudhuri", "title": "3D Shape Segmentation with Projective Convolutional Networks", "comments": "This is an updated version of our CVPR 2017 paper. We incorporated\n  new experiments that demonstrate ShapePFCN performance under the case of\n  consistent *upright* orientation and an additional input channel in our\n  rendered images for encoding height from the ground plane (upright axis\n  coordinate values). Performance is improved in this setting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a deep architecture for segmenting 3D objects into\ntheir labeled semantic parts. Our architecture combines image-based Fully\nConvolutional Networks (FCNs) and surface-based Conditional Random Fields\n(CRFs) to yield coherent segmentations of 3D shapes. The image-based FCNs are\nused for efficient view-based reasoning about 3D object parts. Through a\nspecial projection layer, FCN outputs are effectively aggregated across\nmultiple views and scales, then are projected onto the 3D object surfaces.\nFinally, a surface-based CRF combines the projected outputs with geometric\nconsistency cues to yield coherent segmentations. The whole architecture\n(multi-view FCNs and CRF) is trained end-to-end. Our approach significantly\noutperforms the existing state-of-the-art methods in the currently largest\nsegmentation benchmark (ShapeNet). Finally, we demonstrate promising\nsegmentation results on noisy 3D shapes acquired from consumer-grade depth\ncameras.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 20:46:32 GMT"}, {"version": "v2", "created": "Sun, 16 Apr 2017 01:48:57 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 05:46:17 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kalogerakis", "Evangelos", ""], ["Averkiou", "Melinos", ""], ["Maji", "Subhransu", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "1612.04227", "submitter": "Chaoyang Jiang", "authors": "Chaoyang Jiang, Yeng Chai Soh, Hua Li, Mustafa K. Masood, Zhe Wei,\n  Xiaoli Zhou, and Deqing Zhai", "title": "CFD results calibration from sparse sensor observations with a case\n  study for indoor thermal map", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.GR math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current CFD calibration work has mainly focused on the CFD model calibration.\nHowever no known work has considered the calibration of the CFD results. In\nthis paper, we take inspiration from the image editing problem to develop a\nmethodology to calibrate CFD simulation results based on sparse sensor\nobservations. We formulate the calibration of CFD results as an optimization\nproblem. The cost function consists of two terms. One term guarantees a good\nlocal adjustment of the simulation results based on the sparse sensor\nobservations. The other term transmits the adjustment from local regions around\nsensing locations to the global domain. The proposed method can enhance the CFD\nsimulation results while preserving the overall original profile. An experiment\nin an air-conditioned room was implemented to verify the effectiveness of the\nproposed method. In the experiment, four sensor observations were used to\ncalibrate a simulated thermal map with 167x365 data points. The experimental\nresults show that the proposed method is effective and practical.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 15:19:37 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Jiang", "Chaoyang", ""], ["Soh", "Yeng Chai", ""], ["Li", "Hua", ""], ["Masood", "Mustafa K.", ""], ["Wei", "Zhe", ""], ["Zhou", "Xiaoli", ""], ["Zhai", "Deqing", ""]]}, {"id": "1612.04336", "submitter": "Eric Bruneton", "authors": "Eric Bruneton", "title": "A Qualitative and Quantitative Evaluation of 8 Clear Sky Models", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2016.2622272", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a qualitative and quantitative evaluation of 8 clear sky models\nused in Computer Graphics. We compare the models with each other as well as\nwith measurements and with a reference model from the physics community. After\na short summary of the physics of the problem, we present the measurements and\nthe reference model, and how we \"invert\" it to get the model parameters. We\nthen give an overview of each CG model, and detail its scope, its algorithmic\ncomplexity, and its results using the same parameters as in the reference\nmodel. We also compare the models with a perceptual study. Our quantitative\nresults confirm that the less simplifications and approximations are used to\nsolve the physical equations, the more accurate are the results. We conclude\nwith a discussion of the advantages and drawbacks of each model, and how to\nfurther improve their accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:03:36 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Bruneton", "Eric", ""]]}, {"id": "1612.04337", "submitter": "Tian Qi Chen", "authors": "Tian Qi Chen and Mark Schmidt", "title": "Fast Patch-based Style Transfer of Arbitrary Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artistic style transfer is an image synthesis problem where the content of an\nimage is reproduced with the style of another. Recent works show that a\nvisually appealing style transfer can be achieved by using the hidden\nactivations of a pretrained convolutional neural network. However, existing\nmethods either apply (i) an optimization procedure that works for any style\nimage but is very expensive, or (ii) an efficient feedforward network that only\nallows a limited number of trained styles. In this work we propose a simpler\noptimization objective based on local matching that combines the content\nstructure and style textures in a single layer of the pretrained network. We\nshow that our objective has desirable properties such as a simpler optimization\nlandscape, intuitive parameter tuning, and consistent frame-by-frame\nperformance on video. Furthermore, we use 80,000 natural images and 80,000\npaintings to train an inverse network that approximates the result of the\noptimization. This results in a procedure for artistic style transfer that is\nefficient but also allows arbitrary content and style images.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 20:05:37 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Chen", "Tian Qi", ""], ["Schmidt", "Mark", ""]]}, {"id": "1612.04956", "submitter": "Or Litany", "authors": "Or Litany, Tal Remez, Alex Bronstein", "title": "Cloud Dictionary: Sparse Coding and Modeling for Point Clouds", "comments": "Signal Processing with Adaptive Sparse Structured Representations\n  (SPARS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of range sensors such as LIDAR and time-of-flight\ncameras, 3D point cloud scans have become ubiquitous in computer vision\napplications, the most prominent ones being gesture recognition and autonomous\ndriving. Parsimony-based algorithms have shown great success on images and\nvideos where data points are sampled on a regular Cartesian grid. We propose an\nadaptation of these techniques to irregularly sampled signals by using\ncontinuous dictionaries. We present an example application in the form of point\ncloud denoising.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 07:53:27 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:45:44 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Litany", "Or", ""], ["Remez", "Tal", ""], ["Bronstein", "Alex", ""]]}, {"id": "1612.05064", "submitter": "Christian Tominski", "authors": "Stefan Gladisch, Valerius Weigandt, Heidrun Schumann, Christian\n  Tominski", "title": "Orthogonal Edge Routing for the EditLens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EditLens is an interactive lens technique that supports the editing of\ngraphs. The user can insert, update, or delete nodes and edges while\nmaintaining an already existing layout of the graph. For the nodes and edges\nthat are affected by an edit operation, the EditLens suggests suitable\nlocations and routes, which the user can accept or adjust. For this purpose,\nthe EditLens requires an efficient routing algorithm that can compute results\nat interactive framerates. Existing algorithms cannot fully satisfy the needs\nof the EditLens. This paper describes a novel algorithm that can compute\northogonal edge routes for incremental edit operations of graphs. Tests\nindicate that, in general, the algorithm is better than alternative solutions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:52:30 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Gladisch", "Stefan", ""], ["Weigandt", "Valerius", ""], ["Schumann", "Heidrun", ""], ["Tominski", "Christian", ""]]}, {"id": "1612.05395", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Charted Metropolis Light Transport", "comments": "15 pages, 6 figures - to be published in the SIGGRAPH 2017\n  proceedings", "journal-ref": null, "doi": "10.1145/3072959.3073677", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, inspired by a simpler reformulation of primary sample\nspace Metropolis light transport, we derive a novel family of general Markov\nchain Monte Carlo algorithms called charted Metropolis-Hastings, that\nintroduces the notion of sampling charts to extend a given sampling domain and\nmaking it easier to sample the desired target distribution and escape from\nlocal maxima through coordinate changes. We further apply the novel algorithms\nto light transport simulation, obtaining a new type of algorithm called charted\nMetropolis light transport, that can be seen as a bridge between primary sample\nspace and path space Metropolis light transport. The new algorithms require to\nprovide only right inverses of the sampling functions, a property that we\nbelieve crucial to make them practical in the context of light transport\nsimulation. We further propose a method to integrate density estimation into\nthis framework through a novel scheme that uses it as an independence sampler.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 08:41:06 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 09:51:25 GMT"}, {"version": "v3", "created": "Wed, 21 Dec 2016 16:58:16 GMT"}, {"version": "v4", "created": "Mon, 30 Jan 2017 20:02:11 GMT"}, {"version": "v5", "created": "Sun, 5 Feb 2017 20:39:49 GMT"}, {"version": "v6", "created": "Wed, 26 Apr 2017 17:09:08 GMT"}, {"version": "v7", "created": "Fri, 28 Apr 2017 06:43:30 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1612.07353", "submitter": "YoungBeom Kim", "authors": "YoungBeom Kim, Byung-Ha Park, Kwang-Mo Jung, JungHyun Han", "title": "Data-driven Shoulder Inverse Kinematics", "comments": "13 pages, 14 figures, IJCGA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a shoulder inverse kinematics (IK) technique. Shoulder\ncomplex is comprised of the sternum, clavicle, ribs, scapula, humerus, and four\njoints. The shoulder complex shows specific motion pattern, such as Scapulo\nhumeral rhythm. As a result, if a motion of the shoulder isgenerated without\nthe knowledge of kinesiology, it will be seen as un-natural. The proposed\ntechnique generates motion of the shoulder complex about the orientation of the\nupper arm by interpolating the measurement data. The shoulder IK method allows\nnovice animators to generate natural shoulder motions easily. As a result, this\ntechnique improves the quality of character animation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 02:11:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Kim", "YoungBeom", ""], ["Park", "Byung-Ha", ""], ["Jung", "Kwang-Mo", ""], ["Han", "JungHyun", ""]]}, {"id": "1612.08731", "submitter": "Gabriel Peyr\\'e", "authors": "Gabriel Peyr\\'e, Lena\\\"ic Chizat, Fran\\c{c}ois-Xavier Vialard, Justin\n  Solomon", "title": "Quantum Optimal Transport for Tensor Field Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a new notion of optimal transport (OT) between tensor\nfields, which are measures whose values are positive semidefinite (PSD)\nmatrices. This \"quantum\" formulation of OT (Q-OT) corresponds to a relaxed\nversion of the classical Kantorovich transport problem, where the fidelity\nbetween the input PSD-valued measures is captured using the geometry of the\nVon-Neumann quantum entropy. We propose a quantum-entropic regularization of\nthe resulting convex optimization problem, which can be solved efficiently\nusing an iterative scaling algorithm. This method is a generalization of the\ncelebrated Sinkhorn algorithm to the quantum setting of PSD matrices. We extend\nthis formulation and the quantum Sinkhorn algorithm to compute barycenters\nwithin a collection of input tensor fields. We illustrate the usefulness of the\nproposed approach on applications to procedural noise generation, anisotropic\nmeshing, diffusion tensor imaging and spectral texture synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 14:06:20 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 09:23:06 GMT"}, {"version": "v3", "created": "Wed, 19 Apr 2017 06:48:56 GMT"}, {"version": "v4", "created": "Sun, 23 Jul 2017 08:17:44 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Peyr\u00e9", "Gabriel", ""], ["Chizat", "Lena\u00efc", ""], ["Vialard", "Fran\u00e7ois-Xavier", ""], ["Solomon", "Justin", ""]]}, {"id": "1612.08927", "submitter": "Asad Khan Mr.", "authors": "Asad Khan, Luo Jiang, Wei Li, Ligang Liu", "title": "Fast color transfer from multiple images", "comments": "arXiv admin note: text overlap with arXiv:1610.04861", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color transfer between images uses the statistics information of image\neffectively. We present a novel approach of local color transfer between images\nbased on the simple statistics and locally linear embedding. A sketching\ninterface is proposed for quickly and easily specifying the color\ncorrespondences between target and source image. The user can specify the\ncorrespondences of local region using scribes, which more accurately transfers\nthe target color to the source image while smoothly preserving the boundaries,\nand exhibits more natural output results. Our algorithm is not restricted to\none-to-one image color transfer and can make use of more than one target images\nto transfer the color in different regions in the source image. Moreover, our\nalgorithm does not require to choose the same color style and image size\nbetween source and target images. We propose the sub-sampling to reduce the\ncomputational load. Comparing with other approaches, our algorithm is much\nbetter in color blending in the input data. Our approach preserves the other\ncolor details in the source image. Various experimental results show that our\napproach specifies the correspondences of local color region in source and\ntarget images. And it expresses the intention of users and generates more\nactual and natural results of visual effect.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 16:50:55 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Khan", "Asad", ""], ["Jiang", "Luo", ""], ["Li", "Wei", ""], ["Liu", "Ligang", ""]]}]