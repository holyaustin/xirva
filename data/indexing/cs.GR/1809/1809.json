[{"id": "1809.00886", "submitter": "Xiao Li", "authors": "Xiao Li, Yue Dong, Pieter Peers, Xin Tong", "title": "Modeling Surface Appearance from a Single Photograph using\n  Self-augmented Convolutional Neural Networks", "comments": "Accepted to SIGGRAPH 2017", "journal-ref": "ACM Transactions on Graphics (TOG), 36(4), 45, 2017", "doi": "10.1145/3072959.3073641", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional neural network (CNN) based solution for modeling\nphysically plausible spatially varying surface reflectance functions (SVBRDF)\nfrom a single photograph of a planar material sample under unknown natural\nillumination. Gathering a sufficiently large set of labeled training pairs\nconsisting of photographs of SVBRDF samples and corresponding reflectance\nparameters, is a difficult and arduous process. To reduce the amount of\nrequired labeled training data, we propose to leverage the appearance\ninformation embedded in unlabeled images of spatially varying materials to\nself-augment the training process. Starting from an initial approximative\nnetwork obtained from a small set of labeled training pairs, we estimate\nprovisional model parameters for each unlabeled training exemplar. Given this\nprovisional reflectance estimate, we then synthesize a novel temporary labeled\ntraining pair by rendering the exact corresponding image under a new lighting\ncondition. After refining the network using these additional training samples,\nwe re-estimate the provisional model parameters for the unlabeled data and\nrepeat the self-augmentation process until convergence. We demonstrate the\nefficacy of the proposed network structure on spatially varying wood, metals,\nand plastics, as well as thoroughly validate the effectiveness of the\nself-augmentation training process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 10:53:59 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Xiao", ""], ["Dong", "Yue", ""], ["Peers", "Pieter", ""], ["Tong", "Xin", ""]]}, {"id": "1809.01334", "submitter": "Carsten Burstedde", "authors": "Carsten Burstedde", "title": "Distributed-Memory Forest-of-Octrees Raycasting", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an MPI-parallel algorithm for the in-situ visualization of\ncomputational data that is built around a distributed linear forest-of-octrees\ndata structure. Such octrees are frequently used in element-based numerical\nsimulations; they store the leaves of the tree that are local in the curent\nparallel partition.\n  We proceed in three stages. First, we prune all elements whose bounding box\nis not visible by a parallel top-down traversal, and repartition the remaining\nones for load-balancing. Second, we intersect each element with every ray\npassing its box to derive color and opacity values for the ray segment. To\nreduce data, we aggregate the segments up the octree in a strictly distributed\nfashion in cycles of coarsening and repartition. Third, we composite all\nremaining ray segments to a tiled partition of the image and write it to disk\nusing parallel I/O.\n  The scalability of the method derives from three concepts: By exploiting the\nspace filling curve encoding of the octrees and by relying on recently\ndeveloped tree algorithms for top-down partition traversal, we are able to\ndetermine sender/receiver pairs without handshaking and/or collective\ncommunication. Furthermore, by partnering the linear traversal of tree leaves\nwith the group action of the attenuation/emission ODE along each segment, we\navoid back-to-front sorting of elements throughout. Lastly, the method is\nproblem adaptive with respect to the refinement and partition of the elements\nand to the accuracy of ODE integration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 05:50:01 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Burstedde", "Carsten", ""]]}, {"id": "1809.01354", "submitter": "Quan Chen", "authors": "Quan Chen, Tiezheng Ge, Yanyu Xu, Zhiqiang Zhang, Xinxin Yang, Kun Gai", "title": "Semantic Human Matting", "comments": "ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human matting, high quality extraction of humans from natural images, is\ncrucial for a wide variety of applications. Since the matting problem is\nseverely under-constrained, most previous methods require user interactions to\ntake user designated trimaps or scribbles as constraints. This user-in-the-loop\nnature makes them difficult to be applied to large scale data or time-sensitive\nscenarios. In this paper, instead of using explicit user input constraints, we\nemploy implicit semantic constraints learned from data and propose an automatic\nhuman matting algorithm (SHM). SHM is the first algorithm that learns to\njointly fit both semantic information and high quality details with deep\nnetworks. In practice, simultaneously learning both coarse semantics and fine\ndetails is challenging. We propose a novel fusion strategy which naturally\ngives a probabilistic estimation of the alpha matte. We also construct a very\nlarge dataset with high quality annotations consisting of 35,513 unique\nforegrounds to facilitate the learning and evaluation of human matting.\nExtensive experiments on this dataset and plenty of real images show that SHM\nachieves comparable results with state-of-the-art interactive matting methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 06:50:24 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 12:36:31 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Quan", ""], ["Ge", "Tiezheng", ""], ["Xu", "Yanyu", ""], ["Zhang", "Zhiqiang", ""], ["Yang", "Xinxin", ""], ["Gai", "Kun", ""]]}, {"id": "1809.01471", "submitter": "Ecem Sogancioglu", "authors": "Ecem Sogancioglu, Shi Hu, Davide Belli, Bram van Ginneken", "title": "Chest X-ray Inpainting with Deep Generative Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative adversarial networks have been successfully applied to inpainting\nin natural images. However, the current state-of-the-art models have not yet\nbeen widely adopted in the medical imaging domain. In this paper, we\ninvestigate the performance of three recently published deep learning based\ninpainting models: context encoders, semantic image inpainting, and the\ncontextual attention model, applied to chest x-rays, as the chest exam is the\nmost commonly performed radiological procedure. We train these generative\nmodels on 1.2M 128 $\\times$ 128 patches from 60K healthy x-rays, and learn to\npredict the center 64 $\\times$ 64 region in each patch. We test the models on\nboth the healthy and abnormal radiographs. We evaluate the results by visual\ninspection and comparing the PSNR scores. The outputs of the models are in most\ncases highly realistic. We show that the methods have potential to enhance and\ndetect abnormalities. In addition, we perform a 2AFC observer study and show\nthat an experienced human observer performs poorly in detecting inpainted\nregions, particularly those generated by the contextual attention model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 09:21:22 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Sogancioglu", "Ecem", ""], ["Hu", "Shi", ""], ["Belli", "Davide", ""], ["van Ginneken", "Bram", ""]]}, {"id": "1809.01720", "submitter": "Gregg Helt", "authors": "Gregg Helt", "title": "Extending Mandelbox Fractals with Shape Inversions", "comments": "slightly modified version of paper published in conferences\n  proceedings of Bridges 2018: Mathematics, Art, Music, Architecture,\n  Education, Culture, \"Extending Mandelbox Fractals with Shape Inversions\",\n  Gregg Helt, July 2018, pp. 547-550", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mandelbox is a recently discovered class of escape-time fractals which\nuse a conditional combination of reflection, spherical inversion, scaling, and\ntranslation to transform points under iteration. In this paper we introduce a\nnew extension to Mandelbox fractals which replaces spherical inversion with a\nmore generalized shape inversion. We then explore how this technique can be\nused to generate new fractals in 2D, 3D, and 4D.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 20:25:28 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Helt", "Gregg", ""]]}, {"id": "1809.01890", "submitter": "Koichi Hamada", "authors": "Koichi Hamada, Kentaro Tachibana, Tianqi Li, Hiroto Honda, Yusuke\n  Uchida", "title": "Full-body High-resolution Anime Generation with Progressive\n  Structure-conditional Generative Adversarial Networks", "comments": "Accepted to ECCV 2018 Workshop: Computer Vision for Fashion, Art and\n  Design. Project page is at https://dena.com/intl/anime-generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Progressive Structure-conditional Generative Adversarial Networks\n(PSGAN), a new framework that can generate full-body and high-resolution\ncharacter images based on structural information. Recent progress in generative\nadversarial networks with progressive training has made it possible to generate\nhigh-resolution images. However, existing approaches have limitations in\nachieving both high image quality and structural consistency at the same time.\nOur method tackles the limitations by progressively increasing the resolution\nof both generated images and structural conditions during training. In this\npaper, we empirically demonstrate the effectiveness of this method by showing\nthe comparison with existing approaches and video generation results of diverse\nanime characters at 1024x1024 based on target pose sequences. We also create a\nnovel dataset containing full-body 1024x1024 high-resolution images and exact\n2D pose keypoints using Unity 3D Avatar models.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 09:09:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Hamada", "Koichi", ""], ["Tachibana", "Kentaro", ""], ["Li", "Tianqi", ""], ["Honda", "Hiroto", ""], ["Uchida", "Yusuke", ""]]}, {"id": "1809.02057", "submitter": "Jeong JOon Park", "authors": "Jeong Joon Park, Richard Newcombe, Steve Seitz", "title": "Surface Light Field Fusion", "comments": "Project Website: http://grail.cs.washington.edu/projects/slfusion/", "journal-ref": "3DV 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for interactively scanning highly reflective objects\nwith a commodity RGBD sensor. In addition to shape, our approach models the\nsurface light field, encoding scene appearance from all directions. By\nfactoring the surface light field into view-independent and\nwavelength-independent components, we arrive at a representation that can be\nrobustly estimated with IR-equipped commodity depth sensors, and achieves high\nquality results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 15:45:05 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Park", "Jeong Joon", ""], ["Newcombe", "Richard", ""], ["Seitz", "Steve", ""]]}, {"id": "1809.02560", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su and Matheus Gadelha and Rui Wang and Subhransu Maji", "title": "A Deeper Look at 3D Shape Classifiers", "comments": "Accepted to Second Workshop on 3D Reconstruction Meets Semantics,\n  ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the role of representations and architectures for classifying\n3D shapes in terms of their computational efficiency, generalization, and\nrobustness to adversarial transformations. By varying the number of training\nexamples and employing cross-modal transfer learning we study the role of\ninitialization of existing deep architectures for 3D shape classification. Our\nanalysis shows that multiview methods continue to offer the best generalization\neven without pretraining on large labeled image datasets, and even when trained\non simplified inputs such as binary silhouettes. Furthermore, the performance\nof voxel-based 3D convolutional networks and point-based architectures can be\nimproved via cross-modal transfer from image representations. Finally, we\nanalyze the robustness of 3D shape classifiers to adversarial transformations\nand present a novel approach for generating adversarial perturbations of a 3D\nshape for multiview classifiers using a differentiable renderer. We find that\npoint-based networks are more robust to point position perturbations while\nvoxel-based and multiview networks are easily fooled with the addition of\nimperceptible noise to the input.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 16:10:23 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 18:14:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "1809.03144", "submitter": "I-Chao Shen", "authors": "I-Chao Shen, Yi-Hau Wang, Yu-Mei Chen, Bing-Yu Chen", "title": "Texturing and Deforming Meshes with Casual Images", "comments": "6 pages. Longer version of SIGGRAPH ASIA 2012 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using (casual) images to texture 3D models is a common way to create\nrealistic 3D models, which is a very important task in computer graphics.\nHowever, if the shape of the casual image does not look like the target model\nor the target mapping area, the textured model will become strange since the\nimage will be distorted very much. In this paper, we present a novel texturing\nand deforming approach for mapping the pattern and shape of a casual image to a\n3D model at the same time based on an alternating least-square approach.\nThrough a photogrammetric method, we project the target model onto the source\nimage according to the estimated camera model. Then, the target model is\ndeformed according to the shape of the source image using a surface-based\ndeformation method while minimizing the image distortion simultaneously. The\nprocesses are performed iteratively until convergence. Hence, our method can\nachieve texture mapping, shape deformation, and detail-preserving at once, and\ncan obtain more reasonable texture mapped results than traditional methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 05:43:55 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Shen", "I-Chao", ""], ["Wang", "Yi-Hau", ""], ["Chen", "Yu-Mei", ""], ["Chen", "Bing-Yu", ""]]}, {"id": "1809.03618", "submitter": "Rafael Ballester-Ripoll", "authors": "Rafael Ballester-Ripoll, Renato Pajarola", "title": "Visualization of High-dimensional Scalar Functions Using Principal\n  Parameterizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG cs.MM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insightful visualization of multidimensional scalar fields, in particular\nparameter spaces, is key to many fields in computational science and\nengineering. We propose a principal component-based approach to visualize such\nfields that accurately reflects their sensitivity to input parameters. The\nmethod performs dimensionality reduction on the vast $L^2$ Hilbert space formed\nby all possible partial functions (i.e., those defined by fixing one or more\ninput parameters to specific values), which are projected to low-dimensional\nparameterized manifolds such as 3D curves, surfaces, and ensembles thereof. Our\nmapping provides a direct geometrical and visual interpretation in terms of\nSobol's celebrated method for variance-based sensitivity analysis. We\nfurthermore contribute a practical realization of the proposed method by means\nof tensor decomposition, which enables accurate yet interactive integration and\nmultilinear principal component analysis of high-dimensional models.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:35:12 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Ballester-Ripoll", "Rafael", ""], ["Pajarola", "Renato", ""]]}, {"id": "1809.04765", "submitter": "Shu Liang", "authors": "Shu Liang, Xiufeng Huang, Xianyu Meng, Kunyao Chen, Linda G. Shapiro,\n  Ira Kemelmacher-Shlizerman", "title": "Video to Fully Automatic 3D Hair Model", "comments": "supplementary video: https://www.youtube.com/watch?v=so_CMv7Xd40", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine taking a selfie video with your mobile phone and getting as output a\n3D model of your head (face and 3D hair strands) that can be later used in VR,\nAR, and any other domain. State of the art hair reconstruction methods allow\neither a single photo (thus compromising 3D quality) or multiple views, but\nthey require manual user interaction (manual hair segmentation and capture of\nfixed camera views that span full 360 degree). In this paper, we describe a\nsystem that can completely automatically create a reconstruction from any video\n(even a selfie video), and we don't require specific views, since taking your\n-90 degree, 90 degree, and full back views is not feasible in a selfie capture.\n  In the core of our system, in addition to the automatization components, hair\nstrands are estimated and deformed in 3D (rather than 2D as in state of the\nart) thus enabling superior results. We provide qualitative, quantitative, and\nMechanical Turk human studies that support the proposed system, and show\nresults on a diverse variety of videos (8 different celebrity videos, 9 selfie\nmobile videos, spanning age, gender, hair length, type, and styling).\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:14:53 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Liang", "Shu", ""], ["Huang", "Xiufeng", ""], ["Meng", "Xianyu", ""], ["Chen", "Kunyao", ""], ["Shapiro", "Linda G.", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "1809.05050", "submitter": "Kai Xu", "authors": "Xiaogang Wang, Bin Zhou, Haiyue Fang, Xiaowu Chen, Qinping Zhao, Kai\n  Xu", "title": "Learning to Group and Label Fine-Grained Shape Components", "comments": "Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu\n  (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics, 2018", "doi": "10.1145/3272127.3275009", "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A majority of stock 3D models in modern shape repositories are assembled with\nmany fine-grained components. The main cause of such data form is the\ncomponent-wise modeling process widely practiced by human modelers. These\nmodeling components thus inherently reflect some function-based shape\ndecomposition the artist had in mind during modeling. On the other hand,\nmodeling components represent an over-segmentation since a functional part is\nusually modeled as a multi-component assembly. Based on these observations, we\nadvocate that labeled segmentation of stock 3D models should not overlook the\nmodeling components and propose a learning solution to grouping and labeling of\nthe fine-grained components. However, directly characterizing the shape of\nindividual components for the purpose of labeling is unreliable, since they can\nbe arbitrarily tiny and semantically meaningless. We propose to generate part\nhypotheses from the components based on a hierarchical grouping strategy, and\nperform labeling on those part groups instead of directly on the components.\nPart hypotheses are mid-level elements which are more probable to carry\nsemantic information. A multiscale 3D convolutional neural network is trained\nto extract context-aware features for the hypotheses. To accomplish a labeled\nsegmentation of the whole shape, we formulate higher-order conditional random\nfields (CRFs) to infer an optimal label assignment for all components.\nExtensive experiments demonstrate that our method achieves significantly robust\nlabeling results on raw 3D models from public shape repositories. Our work also\ncontributes the first benchmark for component-wise labeling.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:31:43 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Xiaogang", ""], ["Zhou", "Bin", ""], ["Fang", "Haiyue", ""], ["Chen", "Xiaowu", ""], ["Zhao", "Qinping", ""], ["Xu", "Kai", ""]]}, {"id": "1809.05398", "submitter": "Kai Xu", "authors": "Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, Renjiao Yi, Hao Zhang", "title": "SCORES: Shape Composition with Recursive Substructure Priors", "comments": "Accepted to SIGGRAPH Asia 2018. Corresponding Author: Kai Xu\n  (kevin.kai.xu@gmail.com)", "journal-ref": "ACM Transactions on Graphics, 2018", "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SCORES, a recursive neural network for shape composition. Our\nnetwork takes as input sets of parts from two or more source 3D shapes and a\nrough initial placement of the parts. It outputs an optimized part structure\nfor the composed shape, leading to high-quality geometry construction. A unique\nfeature of our composition network is that it is not merely learning how to\nconnect parts. Our goal is to produce a coherent and plausible 3D shape,\ndespite large incompatibilities among the input parts. The network may\nsignificantly alter the geometry and structure of the input parts and\nsynthesize a novel shape structure based on the inputs, while adding or\nremoving parts to minimize a structure plausibility loss. We design SCORES as a\nrecursive autoencoder network. During encoding, the input parts are recursively\ngrouped to generate a root code. During synthesis, the root code is decoded,\nrecursively, to produce a new, coherent part assembly. Assembled shape\nstructures may be novel, with little global resemblance to training exemplars,\nyet have plausible substructures. SCORES therefore learns a hierarchical\nsubstructure shape prior based on per-node losses. It is trained on structured\nshapes from ShapeNet, and is applied iteratively to reduce the plausibility\nloss.We showresults of shape composition from multiple sources over different\ncategories of man-made shapes and compare with state-of-the-art alternatives,\ndemonstrating that our network can significantly expand the range of composable\nshapes for assembly-based modeling.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:20:06 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Zhu", "Chenyang", ""], ["Xu", "Kai", ""], ["Chaudhuri", "Siddhartha", ""], ["Yi", "Renjiao", ""], ["Zhang", "Hao", ""]]}, {"id": "1809.05491", "submitter": "Xiuming Zhang", "authors": "Xiuming Zhang, Tali Dekel, Tianfan Xue, Andrew Owens, Qiurui He,\n  Jiajun Wu, Stefanie Mueller, William T. Freeman", "title": "MoSculp: Interactive Visualization of Shape and Time", "comments": "UIST 2018. Project page: http://mosculp.csail.mit.edu/", "journal-ref": null, "doi": "10.1145/3242587.3242592", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that allows users to visualize complex human motion via\n3D motion sculptures---a representation that conveys the 3D structure swept by\na human body as it moves through space. Given an input video, our system\ncomputes the motion sculptures and provides a user interface for rendering it\nin different styles, including the options to insert the sculpture back into\nthe original video, render it in a synthetic scene or physically print it.\n  To provide this end-to-end workflow, we introduce an algorithm that estimates\nthat human's 3D geometry over time from a set of 2D images and develop a\n3D-aware image-based rendering approach that embeds the sculpture back into the\nscene. By automating the process, our system takes motion sculpture creation\nout of the realm of professional artists, and makes it applicable to a wide\nrange of existing video material.\n  By providing viewers with 3D information, motion sculptures reveal space-time\nmotion information that is difficult to perceive with the naked eye, and allow\nviewers to interpret how different parts of the object interact over time. We\nvalidate the effectiveness of this approach with user studies, finding that our\nmotion sculpture visualizations are significantly more informative about motion\nthan existing stroboscopic and space-time visualization methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:27:08 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 17:56:47 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhang", "Xiuming", ""], ["Dekel", "Tali", ""], ["Xue", "Tianfan", ""], ["Owens", "Andrew", ""], ["He", "Qiurui", ""], ["Wu", "Jiajun", ""], ["Mueller", "Stefanie", ""], ["Freeman", "William T.", ""]]}, {"id": "1809.05881", "submitter": "Eugene d'Eon", "authors": "Eugene d'Eon", "title": "A Reciprocal Formulation of Nonexponential Radiative Transfer. 2: Monte\n  Carlo Estimation and Diffusion Approximation", "comments": "3rd revision based on reviewer feedback", "journal-ref": null, "doi": "10.1080/23324309.2019.1677717", "report-no": null, "categories": "cond-mat.stat-mech cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When lifting the assumption of spatially-independent scattering centers in\nclassical linear transport theory, collision rate is no longer proportional to\nangular flux / radiance because the macroscopic cross-section $\\Sigma_t(s)$\ndepends on the distance $s$ to the previous collision or boundary. We\ngeneralize collision and track-length estimators to support unbiased estimation\nof either flux integrals or collision rates in generalized radiative transfer\n(GRT). To provide benchmark solutions for the Monte Carlo estimators, we derive\nthe four Green's functions for the isotropic point source in infinite media\nwith isotropic scattering. Additionally, new moment-preserving diffusion\napproximations for these Green's functions are derived, which reduce to\nalgebraic expressions involving the first four moments of the free-path lengths\nbetween collisions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 14:26:59 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 01:23:35 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 08:02:43 GMT"}, {"version": "v4", "created": "Sun, 10 Mar 2019 07:38:53 GMT"}, {"version": "v5", "created": "Fri, 9 Aug 2019 07:25:43 GMT"}, {"version": "v6", "created": "Sat, 9 Nov 2019 19:21:31 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["d'Eon", "Eugene", ""]]}, {"id": "1809.05910", "submitter": "Rana Hanocka", "authors": "Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar Fleishman and\n  Daniel Cohen-Or", "title": "MeshCNN: A Network with an Edge", "comments": "For a two-minute explanation video see https://bit.ly/meshcnnvideo", "journal-ref": null, "doi": "10.1145/3306346.3322959", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes provide an efficient representation for 3D shapes. They\nexplicitly capture both shape surface and topology, and leverage non-uniformity\nto represent large flat regions as well as sharp, intricate features. This\nnon-uniformity and irregularity, however, inhibits mesh analysis efforts using\nneural networks that combine convolution and pooling operations. In this paper,\nwe utilize the unique properties of the mesh for a direct analysis of 3D shapes\nusing MeshCNN, a convolutional neural network designed specifically for\ntriangular meshes. Analogous to classic CNNs, MeshCNN combines specialized\nconvolution and pooling layers that operate on the mesh edges, by leveraging\ntheir intrinsic geodesic connections. Convolutions are applied on edges and the\nfour edges of their incident triangles, and pooling is applied via an edge\ncollapse operation that retains surface topology, thereby, generating new mesh\nconnectivity for the subsequent convolutions. MeshCNN learns which edges to\ncollapse, thus forming a task-driven process where the network exposes and\nexpands the important features while discarding the redundant ones. We\ndemonstrate the effectiveness of our task-driven pooling on various learning\ntasks applied to 3D meshes.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 16:32:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 11:30:57 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Hanocka", "Rana", ""], ["Hertz", "Amir", ""], ["Fish", "Noa", ""], ["Giryes", "Raja", ""], ["Fleishman", "Shachar", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1809.06047", "submitter": "Daniel Mlakar", "authors": "Daniel Mlakar, Martin Winter, Hans-Peter Seidel, Markus Steinberger,\n  Rhaleb Zayer", "title": "AlSub: Fully Parallel and Modular Subdivision", "comments": "Changed structure Added content Improved descriptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, mesh subdivision---the process of forging smooth free-form\nsurfaces from coarse polygonal meshes---has become an indispensable production\ninstrument. Although subdivision performance is crucial during simulation,\nanimation and rendering, state-of-the-art approaches still rely on serial\nimplementations for complex parts of the subdivision process. Therefore, they\noften fail to harness the power of modern parallel devices, like the graphics\nprocessing unit (GPU), for large parts of the algorithm and must resort to\ntime-consuming serial preprocessing. In this paper, we show that a complete\nparallelization of the subdivision process for modern architectures is\npossible. Building on sparse matrix linear algebra, we show how to structure\nthe complete subdivision process into a sequence of algebra operations. By\nrestructuring and grouping these operations, we adapt the process for different\nuse cases, such as regular subdivision of dynamic meshes, uniform subdivision\nfor immutable topology, and feature-adaptive subdivision for efficient\nrendering of animated models. As the same machinery is used for all use cases,\nidentical subdivision results are achieved in all parts of the production\npipeline. As a second contribution, we show how these linear algebra\nformulations can effectively be translated into efficient GPU kernels. Applying\nour strategies to $\\sqrt{3}$, Loop and Catmull-Clark subdivision shows\nsignificant speedups of our approach compared to state-of-the-art solutions,\nwhile we completely avoid serial preprocessing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:17:43 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 06:47:48 GMT"}, {"version": "v3", "created": "Wed, 16 Jan 2019 10:14:51 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Mlakar", "Daniel", ""], ["Winter", "Martin", ""], ["Seidel", "Hans-Peter", ""], ["Steinberger", "Markus", ""], ["Zayer", "Rhaleb", ""]]}, {"id": "1809.06320", "submitter": "Armin Becher", "authors": "Armin Becher, Jens Angerer, Thomas Grauschopf", "title": "Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in\n  Distributed Virtual Reality Systems", "comments": "GI VR/AR Workshop 2018, Virtuelle und Erweiterte Realit\\\"at - 15.\n  Workshop der GI-Fachgruppe VR/AR, D\\\"usseldorf, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Virtual Reality systems enable globally dispersed users to\ninteract with each other in a shared virtual environment. In such systems,\ndifferent types of latencies occur. For a good VR experience, they need to be\ncontrolled. The time delay between the user's head motion and the corresponding\ndisplay output of the VR system might lead to adverse effects such as a reduced\nsense of presence or motion sickness. Additionally, high network latency among\nworldwide locations makes collaboration between users more difficult and leads\nto misunderstandings. To evaluate the performance and optimize dispersed VR\nsolutions it is therefore important to measure those delays. In this work, a\nnovel, easy to set up, and inexpensive method to measure local and remote\nsystem latency will be described. The measuring setup consists of a\nmicrocontroller, a microphone, a piezo buzzer, a photosensor, and a\npotentiometer. With these components, it is possible to measure\nmotion-to-photon and mouth-to-ear latency of various VR systems. By using\nGPS-receivers for timecode-synchronization it is also possible to obtain the\nend-to-end delays between different worldwide locations. The described system\nwas used to measure local and remote latencies of two HMD based distributed VR\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 16:48:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Becher", "Armin", ""], ["Angerer", "Jens", ""], ["Grauschopf", "Thomas", ""]]}, {"id": "1809.06664", "submitter": "Isaak Lim", "authors": "Isaak Lim, Alexander Dielen, Marcel Campen, Leif Kobbelt", "title": "A Simple Approach to Intrinsic Correspondence Learning on Unstructured\n  3D Meshes", "comments": "Presented at the ECCV workshop on Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of representation of 3D geometry is of vital importance when it\ncomes to leveraging the recent advances in the field of machine learning for\ngeometry processing tasks. For common unstructured surface meshes\nstate-of-the-art methods rely on patch-based or mapping-based techniques that\nintroduce resampling operations in order to encode neighborhood information in\na structured and regular manner. We investigate whether such resampling can be\navoided, and propose a simple and direct encoding approach. It does not only\nincrease processing efficiency due to its simplicity - its direct nature also\navoids any loss in data fidelity. To evaluate the proposed method, we perform a\nnumber of experiments in the challenging domain of intrinsic, non-rigid shape\ncorrespondence estimation. In comparisons to current methods we observe that\nour approach is able to achieve highly competitive results.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 12:19:20 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 19:16:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lim", "Isaak", ""], ["Dielen", "Alexander", ""], ["Campen", "Marcel", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1809.06911", "submitter": "David Orden", "authors": "David Orden and Encarnaci\\'on Fern\\'andez-Fern\\'andez and Jos\\'e M.\n  Rodr\\'iguez-Nogales and Josefina Vila-Crespo", "title": "Testing SensoGraph, a geometric approach for fast sensory evaluation", "comments": "21 pages, 7 figures, 3 tables. Accepted at Food Quality and\n  Preference", "journal-ref": "Food Quality and Preference 72 (2019), 1-9", "doi": "10.1016/j.foodqual.2018.09.005", "report-no": null, "categories": "cs.CG cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SensoGraph, a novel approach for fast sensory\nevaluation using two-dimensional geometric techniques. In the tasting sessions,\nthe assessors follow their own criteria to place samples on a tablecloth,\naccording to the similarity between samples. In order to analyse the data\ncollected, first a geometric clustering is performed to each tablecloth,\nextracting connections between the samples. Then, these connections are used to\nconstruct a global similarity matrix. Finally, a graph drawing algorithm is\nused to obtain a 2D consensus graphic, which reflects the global opinion of the\npanel by (1) positioning closer those samples that have been globally perceived\nas similar and (2) showing the strength of the connections between samples. The\nproposal is validated by performing four tasting sessions, with three types of\npanels tasting different wines, and by developing a new software to implement\nthe proposed techniques. The results obtained show that the graphics provide\nsimilar positionings of the samples as the consensus maps obtained by multiple\nfactor analysis (MFA), further providing extra information about connections\nbetween samples, not present in any previous method. The main conclusion is\nthat the use of geometric techniques provides information complementary to MFA,\nand of a different type. Finally, the method proposed is computationally able\nto manage a significantly larger number of assessors than MFA, which can be\nuseful for the comparison of pictures by a huge number of consumers, via the\nInternet.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 07:38:31 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Orden", "David", ""], ["Fern\u00e1ndez-Fern\u00e1ndez", "Encarnaci\u00f3n", ""], ["Rodr\u00edguez-Nogales", "Jos\u00e9 M.", ""], ["Vila-Crespo", "Josefina", ""]]}, {"id": "1809.07009", "submitter": "Yuchi Huo", "authors": "Yuchi Huo and Sung-Eui Yoon", "title": "Light Field Neural Network", "comments": "Need some time to produce and test the prototype", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an optical neural network system made by off-the-shelf\ncomponents. In order to test the evaluate the physical property of the proposed\nsystem, we are making a prototype. After further discussions with our\ncooperators, we are agreed that the prototype implementation may take longer\ntime than we expected earlier. Therefore we reach a consensus on withdrawing\nthe paper until the physical data is available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 04:19:28 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 04:09:35 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Huo", "Yuchi", ""], ["Yoon", "Sung-Eui", ""]]}, {"id": "1809.07417", "submitter": "Li Yi", "authors": "Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su,\n  Leonidas Guibas", "title": "Deep Part Induction from Articulated Object Pairs", "comments": null, "journal-ref": null, "doi": "10.1145/3272127.3275027", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object functionality is often expressed through part articulation -- as when\nthe two rigid parts of a scissor pivot against each other to perform the\ncutting function. Such articulations are often similar across objects within\nthe same functional category. In this paper, we explore how the observation of\ndifferent articulation states provides evidence for part structure and motion\nof 3D objects. Our method takes as input a pair of unsegmented shapes\nrepresenting two different articulation states of two functionally related\nobjects, and induces their common parts along with their underlying rigid\nmotion. This is a challenging setting, as we assume no prior shape structure,\nno prior shape category information, no consistent shape orientation, the\narticulation states may belong to objects of different geometry, plus we allow\ninputs to be noisy and partial scans, or point clouds lifted from RGB images.\nOur method learns a neural network architecture with three modules that\nrespectively propose correspondences, estimate 3D deformation flows, and\nperform segmentation. To achieve optimal performance, our architecture\nalternates between correspondence, deformation flow, and segmentation\nprediction iteratively in an ICP-like fashion. Our results demonstrate that our\nmethod significantly outperforms state-of-the-art techniques in the task of\ndiscovering articulated parts of objects. In addition, our part induction is\nobject-class agnostic and successfully generalizes to new and unseen objects.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:07:18 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Yi", "Li", ""], ["Huang", "Haibin", ""], ["Liu", "Difan", ""], ["Kalogerakis", "Evangelos", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1809.07917", "submitter": "Peng-Shuai Wang", "authors": "Peng-Shuai Wang and Chun-Yu Sun and Yang Liu and Xin Tong", "title": "Adaptive O-CNN: A Patch-based Deep Representation of 3D Shapes", "comments": null, "journal-ref": "ACM Transactions on Graphics, 2018", "doi": "10.1145/3272127.3275050", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an Adaptive Octree-based Convolutional Neural Network (Adaptive\nO-CNN) for efficient 3D shape encoding and decoding. Different from\nvolumetric-based or octree-based CNN methods that represent a 3D shape with\nvoxels in the same resolution, our method represents a 3D shape adaptively with\noctants at different levels and models the 3D shape within each octant with a\nplanar patch. Based on this adaptive patch-based representation, we propose an\nAdaptive O-CNN encoder and decoder for encoding and decoding 3D shapes. The\nAdaptive O-CNN encoder takes the planar patch normal and displacement as input\nand performs 3D convolutions only at the octants at each level, while the\nAdaptive O-CNN decoder infers the shape occupancy and subdivision status of\noctants at each level and estimates the best plane normal and displacement for\neach leaf octant. As a general framework for 3D shape analysis and generation,\nthe Adaptive O-CNN not only reduces the memory and computational cost, but also\noffers better shape generation capability than the existing 3D-CNN approaches.\nWe validate Adaptive O-CNN in terms of efficiency and effectiveness on\ndifferent shape analysis and generation tasks, including shape classification,\n3D autoencoding, shape prediction from a single image, and shape completion for\nnoisy and incomplete point clouds.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 02:24:48 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Wang", "Peng-Shuai", ""], ["Sun", "Chun-Yu", ""], ["Liu", "Yang", ""], ["Tong", "Xin", ""]]}, {"id": "1809.08044", "submitter": "Julian Iseringhausen", "authors": "Julian Iseringhausen and Matthias B. Hullin", "title": "Non-Line-of-Sight Reconstruction using Efficient Transient Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to see beyond the direct line of sight is an intriguing\nprospective and could benefit a wide variety of important applications. Recent\nwork has demonstrated that time-resolved measurements of indirect diffuse light\ncontain valuable information for reconstructing shape and reflectance\nproperties of objects located around a corner. In this paper, we introduce a\nnovel reconstruction scheme that, by design, produces solutions that are\nconsistent with state-of-the-art physically-based rendering. Our method\ncombines an efficient forward model (a custom renderer for time-resolved\nthree-bounce indirect light transport) with an optimization framework to\nreconstruct object geometry in an analysis-by-synthesis sense. We evaluate our\nalgorithm on a variety of synthetic and experimental input data, and show that\nit gracefully handles uncooperative scenes with high levels of noise or\nnon-diffuse material reflectance.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 11:34:44 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 11:41:21 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Iseringhausen", "Julian", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1809.08893", "submitter": "Faruk Diblen", "authors": "Faruk Diblen, Jisk Attema, Rena Bakhshi, Sascha Caron, Luc Hendriks,\n  Bob Stienen", "title": "SPOT: Open Source framework for scientific data repository and\n  interactive visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPOT is an open source and free visual data analytics tool for\nmulti-dimensional data-sets. Its web-based interface allows a quick analysis of\ncomplex data interactively. The operations on data such as aggregation and\nfiltering are implemented. The generated charts are responsive and OpenGL\nsupported. It follows FAIR principles to allow reuse and comparison of the\npublished data-sets. The software also support PostgreSQL database for\nscalability.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 18:04:32 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Diblen", "Faruk", ""], ["Attema", "Jisk", ""], ["Bakhshi", "Rena", ""], ["Caron", "Sascha", ""], ["Hendriks", "Luc", ""], ["Stienen", "Bob", ""]]}, {"id": "1809.09270", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Weidong Shi", "title": "Next Generation of Star Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present two new ideas for generating star patterns and\nfilling the gaps during the tile operation. Firstly, we introduce a novel\nparametric method based on concentric circles for generating stars and\nrosettes. Using proposed method, completely different stars and rosettes and a\nset of new and complex star patterns convert to each other only by changing\nnine parameters. Secondly, we demonstrate how three equal tangent circles can\nbe used as a base for generating tile elements. For this reason a surrounded\ncircle is created among tangent circles, which represents the gaps in hexagonal\npacking. Afterwards, we use our first idea for filling the tangent circles and\nsurrounded circle. This parametric approach can be used for generating infinite\nnew star patterns, which some of them will be presented in result section.Two\nAndroid apps of proposed method called Starking and Tilerking are available in\nGoogle app store.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 00:42:57 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Shi", "Weidong", ""]]}, {"id": "1809.09761", "submitter": "Keunhong Park", "authors": "Keunhong Park, Konstantinos Rematas, Ali Farhadi, Steven M. Seitz", "title": "PhotoShape: Photorealistic Materials for Large-Scale Shape Collections", "comments": "To be presented at SIGGRAPH Asia 2018. Project page:\n  https://keunhong.com/publications/photoshape/", "journal-ref": null, "doi": "10.1145/3272127.3275066", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing online 3D shape repositories contain thousands of 3D models but lack\nphotorealistic appearance. We present an approach to automatically assign\nhigh-quality, realistic appearance models to large scale 3D shape collections.\nThe key idea is to jointly leverage three types of online data -- shape\ncollections, material collections, and photo collections, using the photos as\nreference to guide assignment of materials to shapes. By generating a large\nnumber of synthetic renderings, we train a convolutional neural network to\nclassify materials in real photos, and employ 3D-2D alignment techniques to\ntransfer materials to different parts of each shape model. Our system produces\nphotorealistic, relightable, 3D shapes (PhotoShapes).\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 00:01:03 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Park", "Keunhong", ""], ["Rematas", "Konstantinos", ""], ["Farhadi", "Ali", ""], ["Seitz", "Steven M.", ""]]}, {"id": "1809.10402", "submitter": "Yining Lang", "authors": "Yining Lang, Wei Liang, Yujia Wang, Lap-Fai Yu", "title": "3D Face Synthesis Driven by Personality Impression", "comments": "8pages;6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing 3D faces that give certain personality impressions is commonly\nneeded in computer games, animations, and virtual world applications for\nproducing realistic virtual characters. In this paper, we propose a novel\napproach to synthesize 3D faces based on personality impression for creating\nvirtual characters. Our approach consists of two major steps. In the first\nstep, we train classifiers using deep convolutional neural networks on a\ndataset of images with personality impression annotations, which are capable of\npredicting the personality impression of a face. In the second step, given a 3D\nface and a desired personality impression type as user inputs, our approach\noptimizes the facial details against the trained classifiers, so as to\nsynthesize a face which gives the desired personality impression. We\ndemonstrate our approach for synthesizing 3D faces giving desired personality\nimpressions on a variety of 3D face models. Perceptual studies show that the\nperceived personality impressions of the synthesized faces agree with the\ntarget personality impressions specified for synthesizing the faces. Please\nrefer to the supplementary materials for all results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 08:34:43 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Lang", "Yining", ""], ["Liang", "Wei", ""], ["Wang", "Yujia", ""], ["Yu", "Lap-Fai", ""]]}, {"id": "1809.10526", "submitter": "Tomer Weiss", "authors": "Tomer Weiss, Alan Litteneker, Noah Duncan, Masaki Nakada, Chenfanfu\n  Jiang, Lap-Fai Yu, Demetri Terzopoulos", "title": "Fast and Scalable Position-Based Layout Synthesis", "comments": "13 pages", "journal-ref": "Transactions on Visualization and Computer Graphics, 21 August\n  2018", "doi": "10.1109/TVCG.2018.2866436", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The arrangement of objects into a layout can be challenging for non-experts,\nas is affirmed by the existence of interior design professionals. Recent\nresearch into the automation of this task has yielded methods that can\nsynthesize layouts of objects respecting aesthetic and functional constraints\nthat are non-linear and competing. These methods usually adopt a stochastic\noptimization scheme, which samples from different layout configurations, a\nprocess that is slow and inefficient. We introduce an physics-motivated,\ncontinuous layout synthesis technique, which results in a significant gain in\nspeed and is readily scalable. We demonstrate our method on a variety of\nexamples and show that it achieves results similar to conventional layout\nsynthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster\nby at least an order of magnitude and can handle layouts of unprecedented size\nas well as tightly-packed layouts that can overwhelm McMC.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 14:07:30 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Weiss", "Tomer", ""], ["Litteneker", "Alan", ""], ["Duncan", "Noah", ""], ["Nakada", "Masaki", ""], ["Jiang", "Chenfanfu", ""], ["Yu", "Lap-Fai", ""], ["Terzopoulos", "Demetri", ""]]}, {"id": "1809.10940", "submitter": "Emanuele Rodol\\`a", "authors": "Emanuele Rodol\\`a, Zorah L\\\"ahner, Alex M. Bronstein, Michael M.\n  Bronstein, Justin Solomon", "title": "Functional Maps Representation on Product Manifolds", "comments": "Accepted to Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the tasks of representing, analyzing and manipulating maps\nbetween shapes. We model maps as densities over the product manifold of the\ninput shapes; these densities can be treated as scalar functions and therefore\nare manipulable using the language of signal processing on manifolds. Being a\nmanifold itself, the product space endows the set of maps with a geometry of\nits own, which we exploit to define map operations in the spectral domain; we\nalso derive relationships with other existing representations (soft maps and\nfunctional maps). To apply these ideas in practice, we discretize product\nmanifolds and their Laplace--Beltrami operators, and we introduce localized\nspectral analysis of the product manifold as a novel tool for map processing.\nOur framework applies to maps defined between and across 2D and 3D shapes\nwithout requiring special adjustment, and it can be implemented efficiently\nwith simple operations on sparse matrices.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 10:05:49 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 13:32:47 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Rodol\u00e0", "Emanuele", ""], ["L\u00e4hner", "Zorah", ""], ["Bronstein", "Alex M.", ""], ["Bronstein", "Michael M.", ""], ["Solomon", "Justin", ""]]}, {"id": "1809.11003", "submitter": "Hongyu Liu", "authors": "Jinhong Li, Hongyu Liu, Wing-Yan Tsui and Xianchao Wang", "title": "An inverse scattering approach for geometric body generation: a machine\n  learning perspective", "comments": "22pages, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the 2D and 3D geometric shape generation\nby prescribing a set of characteristic values of a specific geometric body. One\nof the major motivations of our study is the 3D human body generation in\nvarious applications. We develop a novel method that can generate the desired\nbody with customized characteristic values. The proposed method follows a\nmachine-learning flavour that generates the inferred geometric body with the\ninput characteristic parameters from a training dataset. One of the critical\ningredients and novelties of our method is the borrowing of inverse scattering\ntechniques in the theory of wave propagation to the body generation. This is\ndone by establishing a delicate one-to-one correspondence between a geometric\nbody and the far-field pattern of a source scattering problem governed by the\nHelmholtz system. It in turn enables us to establish a one-to-one\ncorrespondence between the geometric body space and the function space defined\nby the far-field patterns. Hence, the far-field patterns can act as the shape\ngenerators. The shape generation with prescribed characteristic parameters is\nachieved by first manipulating the shape generators and then reconstructing the\ncorresponding geometric body from the obtained shape generator by a stable\nmultiple-frequency Fourier method. Our method is easy to implement and produces\nmore efficient and stable body generations. We provide both theoretical\nanalysis and extensive numerical experiments for the proposed method. The study\nis the first attempt to introduce inverse scattering approaches in combination\nwith machine learning to the geometric body generation and it opens up many\nopportunities for further developments.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 12:48:57 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Li", "Jinhong", ""], ["Liu", "Hongyu", ""], ["Tsui", "Wing-Yan", ""], ["Wang", "Xianchao", ""]]}]