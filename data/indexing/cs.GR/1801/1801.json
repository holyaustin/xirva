[{"id": "1801.00432", "submitter": "Vaclav Skala", "authors": "Michal Smolik, Vaclav Skala and Ondrej Nedved", "title": "A Comparative Study of LOWESS and RBF Approximations for Visualization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-42108-7_31", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation methods are widely used in many fields and many techniques have\nbeen published already. This comparative study presents a comparison of LOWESS\n(Locally weighted scatterplot smoothing) and RBF (Radial Basis Functions)\napproximation methods on noisy data as they use different approaches. The RBF\napproach is generally convenient for high dimensional scattered data sets. The\nLOWESS method needs finding a subset of nearest points if data are scattered.\nThe experiments proved that LOWESS approximation gives slightly better results\nthan RBF in the case of lower dimension, while in the higher dimensional case\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 11:51:37 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Smolik", "Michal", ""], ["Skala", "Vaclav", ""], ["Nedved", "Ondrej", ""]]}, {"id": "1801.00441", "submitter": "Vaclav Skala", "authors": "Vaclav Skala", "title": "A Fast Algorithm for Line Clipping by Convex Polyhedron in E3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new algorithm for line clipping against convex polyhedron is given. The\nsuggested algorithm is faster for higher number of facets of the given\npolyhedron than the traditional Cyrus-Beck's and others algorithms with\ncomplexity O(N) . The suggested algorithm has O(N) complexity in the worst N\ncase and expected O(sqrt(N))) complexity. The speed up is achieved because of\n'known order' of triangles. Some principal results of comparisons of selected\nalgorithms are presented and give some imagination how the proposed algorithm\ncould be used effectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 13:13:31 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Skala", "Vaclav", ""]]}, {"id": "1801.00442", "submitter": "Vaclav Skala", "authors": "Vaclav Skala", "title": "O(lgN) Line Clipping Algorithm in E2", "comments": null, "journal-ref": null, "doi": "10.1016/0097-8493(94)90064-7", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new O(lg N) line clipping algorithm in E2 against a convex window is\npresented. The main advantage of the presented algorithm is the principal\nacceleration of the line clipping problem solution. A comparison of the\nproposed algorithm with others shows a significant improvement in run-time.\nExperimental results for selected known algorithms are also shown.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 13:25:16 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Skala", "Vaclav", ""]]}, {"id": "1801.00968", "submitter": "Yi Xiao", "authors": "Yi Xiao, Xiang Cao, Xianyi Zhu, Renzhi Yang, Yan Zheng", "title": "Joint convolutional neural pyramid for depth map super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution depth map can be inferred from a low-resolution one with the\nguidance of an additional high-resolution texture map of the same scene.\nRecently, deep neural networks with large receptive fields are shown to benefit\napplications such as image completion. Our insight is that super resolution is\nsimilar to image completion, where only parts of the depth values are precisely\nknown. In this paper, we present a joint convolutional neural pyramid model\nwith large receptive fields for joint depth map super-resolution. Our model\nconsists of three sub-networks, two convolutional neural pyramids concatenated\nby a normal convolutional neural network. The convolutional neural pyramids\nextract information from large receptive fields of the depth map and guidance\nmap, while the convolutional neural network effectively transfers useful\nstructures of the guidance image to the depth image. Experimental results show\nthat our model outperforms existing state-of-the-art algorithms not only on\ndata pairs of RGB/depth images, but also on other data pairs like\ncolor/saliency and color-scribbles/colorized images.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 11:53:34 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Xiao", "Yi", ""], ["Cao", "Xiang", ""], ["Zhu", "Xianyi", ""], ["Yang", "Renzhi", ""], ["Zheng", "Yan", ""]]}, {"id": "1801.01155", "submitter": "Mathias Kanzler", "authors": "Mathias Kanzler, Marc Rautenhaus, R\\\"udiger Westermann", "title": "A Voxel-based Rendering Pipeline for Large 3D Line Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a voxel-based rendering pipeline for large 3D line sets that\nemploys GPU ray-casting to achieve scalable rendering including transparency\nand global illumination effects that cannot be achieved with GPU rasterization.\nEven for opaque lines we demonstrate superior rendering performance compared to\nGPU rasterization of lines, and when transparency is used we can interactively\nrender large amounts of lines that are infeasible to be rendered via\nrasterization. To achieve this, we propose a direction-preserving encoding of\nlines into a regular voxel grid, along with the quantization of directions\nusing face-to-face connectivity in this grid. On the regular grid structure,\nparallel GPU ray-casting is used to determine visible fragments in correct\nvisibility order. To enable interactive rendering of global illumination\neffects like low-frequency shadows and ambient occlusions, illumination\nsimulation is performed during ray-casting on a level-of-detail (LoD) line\nrepresentation that considers the number of lines and their lengths per voxel.\nIn this way we can render effects which are very difficult to render via GPU\nrasterization. A detailed performance and quality evaluation compares our\napproach to rasterization-based rendering of lines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 20:28:47 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Kanzler", "Mathias", ""], ["Rautenhaus", "Marc", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "1801.01922", "submitter": "Mikhail Bessmeltsev", "authors": "Mikhail Bessmeltsev, Justin Solomon", "title": "Vectorization of Line Drawings via PolyVector Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image tracing is a foundational component of the workflow in graphic design,\nengineering, and computer animation, linking hand-drawn concept images to\ncollections of smooth curves needed for geometry processing and editing. Even\nfor clean line drawings, modern algorithms often fail to faithfully vectorize\njunctions, or points at which curves meet; this produces vector drawings with\nincorrect connectivity. This subtle issue undermines the practical application\nof vectorization tools and accounts for hesitance among artists and engineers\nto use automatic vectorization software. To address this issue, we propose a\nnovel image vectorization method based on state-of-the-art mathematical\nalgorithms for frame field processing. Our algorithm is tailored specifically\nto disambiguate junctions without sacrificing quality.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 21:26:48 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 21:46:14 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bessmeltsev", "Mikhail", ""], ["Solomon", "Justin", ""]]}, {"id": "1801.02453", "submitter": "Danielle Ezuz", "authors": "Danielle Ezuz, Justin Solomon and Mirela Ben-Chen", "title": "Reversible Harmonic Maps between Discrete Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information transfer between triangle meshes is of great importance in\ncomputer graphics and geometry processing. To facilitate this process, a smooth\nand accurate map is typically required between the two meshes. While such maps\ncan sometimes be computed between nearly-isometric meshes, the more general\ncase of meshes with diverse geometries remains challenging. We propose a novel\napproach for direct map computation between triangle meshes without mapping to\nan intermediate domain, which optimizes for the harmonicity and reversibility\nof the forward and backward maps. Our method is general both in the information\nit can receive as input, e.g. point landmarks, a dense map or a functional map,\nand in the diversity of the geometries to which it can be applied. We\ndemonstrate that our maps exhibit lower conformal distortion than the\nstate-of-the-art, while succeeding in correctly mapping key features of the\ninput shapes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 14:51:57 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Ezuz", "Danielle", ""], ["Solomon", "Justin", ""], ["Ben-Chen", "Mirela", ""]]}, {"id": "1801.03924", "submitter": "Richard Zhang", "authors": "Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver\n  Wang", "title": "The Unreasonable Effectiveness of Deep Features as a Perceptual Metric", "comments": "Accepted to CVPR 2018; Code and data available at\n  https://www.github.com/richzhang/PerceptualSimilarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is nearly effortless for humans to quickly assess the perceptual\nsimilarity between two images, the underlying processes are thought to be quite\ncomplex. Despite this, the most widely used perceptual metrics today, such as\nPSNR and SSIM, are simple, shallow functions, and fail to account for many\nnuances of human perception. Recently, the deep learning community has found\nthat features of the VGG network trained on ImageNet classification has been\nremarkably useful as a training loss for image synthesis. But how perceptual\nare these so-called \"perceptual losses\"? What elements are critical for their\nsuccess? To answer these questions, we introduce a new dataset of human\nperceptual similarity judgments. We systematically evaluate deep features\nacross different architectures and tasks and compare them with classic metrics.\nWe find that deep features outperform all previous metrics by large margins on\nour dataset. More surprisingly, this result is not restricted to\nImageNet-trained VGG features, but holds across different deep architectures\nand levels of supervision (supervised, self-supervised, or even unsupervised).\nOur results suggest that perceptual similarity is an emergent property shared\nacross deep visual representations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 18:54:17 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 19:25:07 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zhang", "Richard", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""], ["Shechtman", "Eli", ""], ["Wang", "Oliver", ""]]}, {"id": "1801.04486", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Can Computers Create Art?", "comments": "to appear in Arts, special issue on Machine as Artist (21st Century)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This essay discusses whether computers, using Artificial Intelligence (AI),\ncould create art. First, the history of technologies that automated aspects of\nart is surveyed, including photography and animation. In each case, there were\ninitial fears and denial of the technology, followed by a blossoming of new\ncreative and professional opportunities for artists. The current hype and\nreality of Artificial Intelligence (AI) tools for art making is then discussed,\ntogether with predictions about how AI tools will be used. It is then\nspeculated about whether it could ever happen that AI systems could be credited\nwith authorship of artwork. It is theorized that art is something created by\nsocial agents, and so computers cannot be credited with authorship of art in\nour current understanding. A few ways that this could change are also\nhypothesized.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 21:04:13 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 03:37:22 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 06:25:17 GMT"}, {"version": "v4", "created": "Thu, 8 Feb 2018 19:23:07 GMT"}, {"version": "v5", "created": "Mon, 19 Mar 2018 06:32:02 GMT"}, {"version": "v6", "created": "Tue, 8 May 2018 03:45:56 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1801.04619", "submitter": "Ryan Webster", "authors": "Ryan Webster", "title": "Innovative Non-parametric Texture Synthesis via Patch Permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we present a non-parametric texture synthesis algorithm capable\nof producing plausible images without copying large tiles of the exemplar. We\nfocus on a simple synthesis algorithm, where we explore two patch match\nheuristics; the well known Bidirectional Similarity (BS) measure and a\nheuristic that finds near permutations using the solution of an entropy\nregularized optimal transport (OT) problem. Innovative synthesis is achieved\nwith a small patch size, where global plausibility relies on the qualities of\nthe match. For OT, less entropic regularization also meant near permutations\nand more plausible images. We examine the tile maps of the synthesized images,\nshowing that they are indeed novel superpositions of the input and contain few\nor no verbatim copies. Synthesis results are compared to a statistical method,\nnamely a random convolutional network. We conclude by remarking simple\nalgorithms using only the input image can synthesize textures decently well and\ncall for more modest approaches in future algorithm design.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 22:33:50 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Webster", "Ryan", ""]]}, {"id": "1801.05264", "submitter": "Shadrokh Samavi", "authors": "Hamidreza Zarrabi, Ali Emami, Nader Karimi, Shadrokh Samavi", "title": "Adaptive Reversible Watermarking Based on Linear Prediction for Medical\n  Videos", "comments": "Algorithms are now presented in a standard format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible video watermarking can guarantee that the watermark logo and the\noriginal frame can be recovered from the watermarked frame without any\ndistortion. Although reversible video watermarking has successfully been\napplied in multimedia, its application has not been extensively explored in\nmedical videos. Reversible watermarking in medical videos is still a\nchallenging problem. The existing reversible video watermarking algorithms,\nwhich are based on error prediction expansion, use motion vectors for\nprediction. In this study, we propose an adaptive reversible watermarking\nmethod for medical videos. We suggest using temporal correlations for improving\nthe prediction accuracy. Hence, two temporal neighbor pixels in upcoming frames\nare used alongside the four spatial rhombus neighboring pixels to minimize the\nprediction error. To the best of our knowledge, this is the first time this\nmethod is applied to medical videos. The method helps to protect patients'\npersonal and medical information by watermarking, i.e., increase the security\nof Health Information Systems (HIS). Experimental results demonstrate the high\nquality of the proposed watermarking method based on PSNR metric and a large\ncapacity for data hiding in medical videos.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 05:17:11 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 17:37:10 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zarrabi", "Hamidreza", ""], ["Emami", "Ali", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""]]}, {"id": "1801.06761", "submitter": "Lequan Yu", "authors": "Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng", "title": "PU-Net: Point Cloud Upsampling Network", "comments": "accepted by CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning and analyzing 3D point clouds with deep networks is challenging due\nto the sparseness and irregularity of the data. In this paper, we present a\ndata-driven point cloud upsampling technique. The key idea is to learn\nmulti-level features per point and expand the point set via a multi-branch\nconvolution unit implicitly in feature space. The expanded feature is then\nsplit to a multitude of features, which are then reconstructed to an upsampled\npoint set. Our network is applied at a patch-level, with a joint loss function\nthat encourages the upsampled points to remain on the underlying surface with a\nuniform distribution. We conduct various experiments using synthesis and scan\ndata to evaluate our method and demonstrate its superiority over some baseline\nmethods and an optimization-based method. Results show that our upsampled\npoints have better uniformity and are located closer to the underlying\nsurfaces.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:10:52 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 06:20:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yu", "Lequan", ""], ["Li", "Xianzhi", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1801.06928", "submitter": "Wei Liu", "authors": "Wei Liu, Wei Xu, Xiaogang Chen, Xiaolin Huang, Chunhua Shen, Jie Yang", "title": "Edge-Preserving Piecewise Linear Image Smoothing Using Piecewise\n  Constant Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image smoothing filters in the literature assume a piecewise constant\nmodel of smoothed output images. However, the piecewise constant model\nassumption can cause artifacts such as gradient reversals in applications such\nas image detail enhancement, HDR tone mapping, etc. In these applications, a\npiecewise linear model assumption is more preferred. In this paper, we propose\na simple yet very effective framework to smooth images of piecewise linear\nmodel assumption using classical filters with the piecewise constant model\nassumption. Our method is capable of handling with gradient reversal artifacts\ncaused by the piecewise constant model assumption. In addition, our method can\nfurther help accelerated methods, which need to quantize image intensity values\ninto different bins, to achieve similar results that need a large number of\nbins using a much smaller number of bins. This can greatly reduce the\ncomputational cost. We apply our method to various classical filters with the\npiecewise constant model assumption. Experimental results of several\napplications show the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 01:18:06 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Liu", "Wei", ""], ["Xu", "Wei", ""], ["Chen", "Xiaogang", ""], ["Huang", "Xiaolin", ""], ["Shen", "Chunhua", ""], ["Yang", "Jie", ""]]}, {"id": "1801.07632", "submitter": "Zeyuan Chen", "authors": "Zeyuan Chen, Shaoliang Nie, Tianfu Wu, and Christopher G. Healey", "title": "High Resolution Face Completion with Multiple Controllable Attributes\n  via Fully End-to-End Progressive Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning approach for high resolution face completion with\nmultiple controllable attributes (e.g., male and smiling) under arbitrary\nmasks. Face completion entails understanding both structural meaningfulness and\nappearance consistency locally and globally to fill in \"holes\" whose content do\nnot appear elsewhere in an input image. It is a challenging task with the\ndifficulty level increasing significantly with respect to high resolution, the\ncomplexity of \"holes\" and the controllable attributes of filled-in fragments.\nOur system addresses the challenges by learning a fully end-to-end framework\nthat trains generative adversarial networks (GANs) progressively from low\nresolution to high resolution with conditional vectors encoding controllable\nattributes.\n  We design novel network architectures to exploit information across multiple\nscales effectively and efficiently. We introduce new loss functions encouraging\nsharp completion. We show that our system can complete faces with large\nstructural and appearance variations using a single feed-forward pass of\ncomputation with mean inference time of 0.007 seconds for images at 1024 x 1024\nresolution. We also perform a pilot human study that shows our approach\noutperforms state-of-the-art face completion methods in terms of rank analysis.\nThe code will be released upon publication.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:12:26 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Chen", "Zeyuan", ""], ["Nie", "Shaoliang", ""], ["Wu", "Tianfu", ""], ["Healey", "Christopher G.", ""]]}, {"id": "1801.07791", "submitter": "Yangyan Li", "authors": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen", "title": "PointCNN: Convolution On $\\mathcal{X}$-Transformed Points", "comments": "To be published in NIPS 2018, code available at\n  https://github.com/yangyanli/PointCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and general framework for feature learning from point\nclouds. The key to the success of CNNs is the convolution operator that is\ncapable of leveraging spatially-local correlation in data represented densely\nin grids (e.g. images). However, point clouds are irregular and unordered, thus\ndirectly convolving kernels against features associated with the points, will\nresult in desertion of shape information and variance to point ordering. To\naddress these problems, we propose to learn an $\\mathcal{X}$-transformation\nfrom the input points, to simultaneously promote two causes. The first is the\nweighting of the input features associated with the points, and the second is\nthe permutation of the points into a latent and potentially canonical order.\nElement-wise product and sum operations of the typical convolution operator are\nsubsequently applied on the $\\mathcal{X}$-transformed features. The proposed\nmethod is a generalization of typical CNNs to feature learning from point\nclouds, thus we call it PointCNN. Experiments show that PointCNN achieves on\npar or better performance than state-of-the-art methods on multiple challenging\nbenchmark datasets and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 22:07:21 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 11:45:08 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 02:11:12 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 01:33:31 GMT"}, {"version": "v5", "created": "Mon, 5 Nov 2018 09:31:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Yangyan", ""], ["Bu", "Rui", ""], ["Sun", "Mingchao", ""], ["Wu", "Wei", ""], ["Di", "Xinhan", ""], ["Chen", "Baoquan", ""]]}, {"id": "1801.07892", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, Thomas S. Huang", "title": "Generative Image Inpainting with Contextual Attention", "comments": "Accepted in CVPR 2018; add CelebA-HQ results; open sourced;\n  interactive demo available: http://jhyu.me/demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning based approaches have shown promising results for the\nchallenging task of inpainting large missing regions in an image. These methods\ncan generate visually plausible image structures and textures, but often create\ndistorted structures or blurry textures inconsistent with surrounding areas.\nThis is mainly due to ineffectiveness of convolutional neural networks in\nexplicitly borrowing or copying information from distant spatial locations. On\nthe other hand, traditional texture and patch synthesis approaches are\nparticularly suitable when it needs to borrow textures from the surrounding\nregions. Motivated by these observations, we propose a new deep generative\nmodel-based approach which can not only synthesize novel image structures but\nalso explicitly utilize surrounding image features as references during network\ntraining to make better predictions. The model is a feed-forward, fully\nconvolutional neural network which can process images with multiple holes at\narbitrary locations and with variable sizes during the test time. Experiments\non multiple datasets including faces (CelebA, CelebA-HQ), textures (DTD) and\nnatural images (ImageNet, Places2) demonstrate that our proposed approach\ngenerates higher-quality inpainting results than existing ones. Code, demo and\nmodels are available at: https://github.com/JiahuiYu/generative_inpainting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 08:04:55 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 21:46:22 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Yu", "Jiahui", ""], ["Lin", "Zhe", ""], ["Yang", "Jimei", ""], ["Shen", "Xiaohui", ""], ["Lu", "Xin", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1801.08093", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Greg Turk and C.Karen Liu", "title": "Learning Symmetric and Low-energy Locomotion", "comments": "Accepted to SIGGRAPH 2018. Supplementary video:\n  https://www.youtube.com/watch?v=zkH90rU-uew&feature=youtu.be", "journal-ref": "ACM Transactions on Graphics 37(4), August 2018", "doi": "10.1145/3197517.3201397", "report-no": null, "categories": "cs.LG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning locomotion skills is a challenging problem. To generate realistic\nand smooth locomotion, existing methods use motion capture, finite state\nmachines or morphology-specific knowledge to guide the motion generation\nalgorithms. Deep reinforcement learning (DRL) is a promising approach for the\nautomatic creation of locomotion control. Indeed, a standard benchmark for DRL\nis to automatically create a running controller for a biped character from a\nsimple reward function. Although several different DRL algorithms can\nsuccessfully create a running controller, the resulting motions usually look\nnothing like a real runner. This paper takes a minimalist learning approach to\nthe locomotion problem, without the use of motion examples, finite state\nmachines, or morphology-specific knowledge. We introduce two modifications to\nthe DRL approach that, when used together, produce locomotion behaviors that\nare symmetric, low-energy, and much closer to that of a real person. First, we\nintroduce a new term to the loss function (not the reward function) that\nencourages symmetric actions. Second, we introduce a new curriculum learning\nmethod that provides modulated physical assistance to help the character with\nleft/right balance and forward movement. The algorithm automatically computes\nappropriate assistance to the character and gradually relaxes this assistance,\nso that eventually the character learns to move entirely without help. Because\nour method does not make use of motion capture data, it can be applied to a\nvariety of character morphologies. We demonstrate locomotion controllers for\nthe lower half of a biped, a full humanoid, a quadruped, and a hexapod. Our\nresults show that learned policies are able to produce symmetric, low-energy\ngaits. In addition, speed-appropriate gait patterns emerge without any guidance\nfrom motion examples or contact planning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:37:35 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 20:10:36 GMT"}, {"version": "v3", "created": "Sat, 12 May 2018 14:30:23 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Yu", "Wenhao", ""], ["Turk", "Greg", ""], ["Liu", "C. Karen", ""]]}, {"id": "1801.08163", "submitter": "Kushal Kafle", "authors": "Kushal Kafle, Brian Price, Scott Cohen, Christopher Kanan", "title": "DVQA: Understanding Data Visualizations via Question Answering", "comments": "CVPR 2018 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bar charts are an effective way to convey numeric information, but today's\nalgorithms cannot parse them. Existing methods fail when faced with even minor\nvariations in appearance. Here, we present DVQA, a dataset that tests many\naspects of bar chart understanding in a question answering framework. Unlike\nvisual question answering (VQA), DVQA requires processing words and answers\nthat are unique to a particular bar chart. State-of-the-art VQA algorithms\nperform poorly on DVQA, and we propose two strong baselines that perform\nconsiderably better. Our work will enable algorithms to automatically extract\nnumeric and semantic information from vast quantities of bar charts found in\nscientific publications, Internet articles, business reports, and many other\nareas.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 19:47:04 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 17:42:15 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Kafle", "Kushal", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Kanan", "Christopher", ""]]}, {"id": "1801.08336", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis", "title": "Big Data Visualization Tools", "comments": "This article appears in Encyclopedia of Big Data Technologies,\n  Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is the presentation of data in a pictorial or graphical\nformat, and a data visualization tool is the software that generates this\npresentation. Data visualization provides users with intuitive means to\ninteractively explore and analyze data, enabling them to effectively identify\ninteresting patterns, infer correlations and causalities, and supports\nsense-making activities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 10:16:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:03:28 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Bikakis", "Nikos", ""]]}, {"id": "1801.08863", "submitter": "Gholamreza Anbarjafari", "authors": "Morteza Daneshmand, Ahmed Helmi, Egils Avots, Fatemeh Noroozi, Fatih\n  Alisinanoglu, Hasan Sait Arslan, Jelena Gorbova, Rain Eric Haamer, Cagri\n  Ozcinar, Gholamreza Anbarjafari", "title": "3D Scanning: A Comprehensive Survey", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an overview of 3D scanning methodologies and technologies\nproposed in the existing scientific and industrial literature. Throughout the\npaper, various types of the related techniques are reviewed, which consist,\nmainly, of close-range, aerial, structure-from-motion and terrestrial\nphotogrammetry, and mobile, terrestrial and airborne laser scanning, as well as\ntime-of-flight, structured-light and phase-comparison methods, along with\ncomparative and combinational studies, the latter being intended to help make a\nclearer distinction on the relevance and reliability of the possible choices.\nMoreover, outlier detection and surface fitting procedures are discussed\nconcisely, which are necessary post-processing stages.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 00:13:15 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Daneshmand", "Morteza", ""], ["Helmi", "Ahmed", ""], ["Avots", "Egils", ""], ["Noroozi", "Fatemeh", ""], ["Alisinanoglu", "Fatih", ""], ["Arslan", "Hasan Sait", ""], ["Gorbova", "Jelena", ""], ["Haamer", "Rain Eric", ""], ["Ozcinar", "Cagri", ""], ["Anbarjafari", "Gholamreza", ""]]}, {"id": "1801.09358", "submitter": "Andrew Reach", "authors": "Andrew Reach and Chris North", "title": "Smooth, Efficient, and Interruptible Zooming and Panning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a novel technique for smooth and efficient zooming and\npanning based on dynamical systems in hyperbolic space. Unlike the technique of\nvan Wijk and Nuij, the animations produced by our technique are smooth at the\nendpoints and when interrupted by a change of target. To analyze the results of\nour technique, we introduce world/screen diagrams, a novel technique for\nvisualizing zooming and panning animations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 04:39:56 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Reach", "Andrew", ""], ["North", "Chris", ""]]}, {"id": "1801.09710", "submitter": "Nils Thuerey", "authors": "You Xie, Erik Franz, Mengyu Chu, Nils Thuerey", "title": "tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution\n  Fluid Flow", "comments": "ACM Transaction on Graphics (SIGGRAPH 2018), further info:\n  https://ge.in.tum.de/publications/tempogan/", "journal-ref": "ACM Trans. Graph. 37, 4, Article 95 (July 2018)", "doi": "10.1145/3072959.3073643", "report-no": null, "categories": "cs.LG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a temporally coherent generative model addressing the\nsuper-resolution problem for fluid flows. Our work represents a first approach\nto synthesize four-dimensional physics fields with neural networks. Based on a\nconditional generative adversarial network that is designed for the inference\nof three-dimensional volumetric data, our model generates consistent and\ndetailed results by using a novel temporal discriminator, in addition to the\ncommonly used spatial one. Our experiments show that the generator is able to\ninfer more realistic high-resolution details by using additional physical\nquantities, such as low-resolution velocities or vorticities. Besides\nimprovements in the training process and in the generated outputs, these inputs\noffer means for artistic control as well. We additionally employ a\nphysics-aware data augmentation step, which is crucial to avoid overfitting and\nto reduce memory requirements. In this way, our network learns to generate\nadvected quantities with highly detailed, realistic, and temporally coherent\nfeatures. Our method works instantaneously, using only a single time-step of\nlow-resolution fluid data. We demonstrate the abilities of our method using a\nvariety of complex inputs and applications in two and three dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 19:11:13 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 08:49:58 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Xie", "You", ""], ["Franz", "Erik", ""], ["Chu", "Mengyu", ""], ["Thuerey", "Nils", ""]]}, {"id": "1801.09847", "submitter": "Qian-Yi Zhou", "authors": "Qian-Yi Zhou and Jaesik Park and Vladlen Koltun", "title": "Open3D: A Modern Library for 3D Data Processing", "comments": "http://www.open3d.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open3D is an open-source library that supports rapid development of software\nthat deals with 3D data. The Open3D frontend exposes a set of carefully\nselected data structures and algorithms in both C++ and Python. The backend is\nhighly optimized and is set up for parallelization. Open3D was developed from a\nclean slate with a small and carefully considered set of dependencies. It can\nbe set up on different platforms and compiled from source with minimal effort.\nThe code is clean, consistently styled, and maintained via a clear code review\nmechanism. Open3D has been used in a number of published research projects and\nis actively deployed in the cloud. We welcome contributions from the\nopen-source community.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:33:20 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Zhou", "Qian-Yi", ""], ["Park", "Jaesik", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1801.10040", "submitter": "Christos Mousas", "authors": "Yaoyuan Cui, Christos Mousas", "title": "Animation-by-Demonstration Computer Puppetry Authoring Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents Master of Puppets (MOP), an animation-by-demonstration\nframework that allows users to control the motion of virtual characters\n(puppets) in real time. In the first step, the user is asked to perform the\nnecessary actions that correspond to the character's motions. The user's\nactions are recorded, and a hidden Markov model (HMM) is used to learn the\ntemporal profile of the actions. During the runtime of the framework, the user\ncontrols the motions of the virtual character based on the specified\nactivities. The advantage of the MOP framework is that it recognizes and\nfollows the progress of the user's actions in real time. Based on the forward\nalgorithm, the method predicts the evolution of the user's actions, which\ncorresponds to the evolution of the character's motion. This method treats\ncharacters as puppets that can perform only one motion at a time. This means\nthat combinations of motion segments (motion synthesis), as well as the\ninterpolation of individual motion sequences, are not provided as\nfunctionalities. By implementing the framework and presenting several computer\npuppetry scenarios, its efficiency and flexibility in animating virtual\ncharacters is demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 15:05:15 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Cui", "Yaoyuan", ""], ["Mousas", "Christos", ""]]}, {"id": "1801.10434", "submitter": "Zhong Li", "authors": "Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, Jingyi Yu", "title": "Robust 3D Human Motion Reconstruction Via Dynamic Template Construction", "comments": "3DV 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-view human body capture systems, the recovered 3D geometry or even\nthe acquired imagery data can be heavily corrupted due to occlusions, noise,\nlimited field of- view, etc. Direct estimation of 3D pose, body shape or motion\non these low-quality data has been traditionally challenging.In this paper, we\npresent a graph-based non-rigid shape registration framework that can\nsimultaneously recover 3D human body geometry and estimate pose/motion at high\nfidelity.Our approach first generates a global full-body template by\nregistering all poses in the acquired motion sequence.We then construct a\ndeformable graph by utilizing the rigid components in the global template. We\ndirectly warp the global template graph back to each motion frame in order to\nfill in missing geometry. Specifically, we combine local rigidity and temporal\ncoherence constraints to maintain geometry and motion consistencies.\nComprehensive experiments on various scenes show that our method is accurate\nand robust even in the presence of drastic motions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:03:29 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Li", "Zhong", ""], ["Ji", "Yu", ""], ["Yang", "Wei", ""], ["Ye", "Jinwei", ""], ["Yu", "Jingyi", ""]]}]