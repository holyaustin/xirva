[{"id": "1609.00754", "submitter": "Antonio Cesarano", "authors": "Antonio Cesarano, FIlomena Ferrucci, Mario Torre", "title": "A heuristic extending the Squarified treemapping algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A heuristic extending the Squarified Treemap technique for the representation\nof hierarchical information as treemaps is presented. The original technique\ngives high quality treemap views, since items are laid out with rectangles that\napproximate squares, allowing easy comparison and selection operations. New key\nsteps, with a low computational impact, have been introduced to yield treemaps\nwith even better aspect ratios and higher homogeneity among items.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2016 21:47:47 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Cesarano", "Antonio", ""], ["Ferrucci", "FIlomena", ""], ["Torre", "Mario", ""]]}, {"id": "1609.00958", "submitter": "Jonas Graetz (Dittmann)", "authors": "Jonas Graetz", "title": "High performance volume ray casting: A branchless generalized Joseph\n  projector", "comments": "Major revision of previous version. The presentation is focused by\n  reducing technical verbosity, and the performed benchmarks are explained more\n  detailed. Figures have been updated accordingly. The literature review has\n  been reworked, and benchmarks on newer hardware have been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A concise and highly performant branchless formulation of a Joseph-type\ninterpolating ray-casting algorithm for the computation of X-ray projections is\npresented. It efficiently utilizes the hardware resources of modern graphics\nprocessing units at the scale of their theoretic maximum performance reaching\naccess rates of 600 GB/s within read-and-write memory, and is further shown to\ndo so without compromising on image quality. The computation of X-ray\nprojections from discrete voxel grids is an ubiquitous task in many problems\nrelated to volume image processing, including tomographic reconstruction and\nvisualization. Although its central role has given rise to numerous\npublications discussing the optimal modeling of ray-volume intersections, a\nunique benchmark in this respect does not exist. Here, a 3D Shepp-Logan phantom\nis used, which allows the computation of analytic reference projections that\ncan further serve as input to iterative reconstructions without committing the\ninverse crime. The proposed algorithm (GJP) is compared to the competing and\nwidely adopted digital differential analyzer (DDA), which computes exact\nline-box intersections. It is thereby found to outperform the DDA on recent\ngraphics processors in all respects: Despite accessing twice as much memory,\nthe GJP is still able to calculate projections twice as fast. It further\nexhibits considerably less discretization artifacts, and neither oversampling\nof the DDA nor a smooth interpolation kernel within the GJP are able to improve\non these results in any respect.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 16:46:30 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 16:56:31 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Graetz", "Jonas", ""]]}, {"id": "1609.01317", "submitter": "Nils Kopal <", "authors": "Nils Kopal", "title": "Volume Raycasting mit OpenCL", "comments": "11 pages, 15 figures, a German paper written for a 3D modeling\n  masters course in applied computer science at the University of\n  Duisburg-Essen in 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This German paper was written entirely at the University of Duisburg-Essen in\n2011 for a 3D modeling masters course in applied computer science. We publish\nthis paper, thus, interested people can acquire a first impression of the topic\n\"volume raycasting\". In addition to writing this paper, we developed a\nfunctioning open-source OpenCL raycaster. A video of this raycaster is\navailable: http://www.youtube.com/watch?v=VMMsQnf4zEY. Additionally, we\narchived and published the complete source code of the raycaster in the Google\nCode Archive: http://code.google.com/p/gputracer/. If this is no longer the\ncase, those who are interested can also write an email to the author, hence, we\ncan provide the source code.\n  This paper provides an introduction and overview of the topic \"volume ray\ncasting with OpenCL\". We show how volume data can be loaded, manipulated, and\nvisualized by modern GPUs in real time. In addition, we present basic\nalgorithms and data structures that are necessary for building such a\nraycaster. Then, we describe how we built a rudimentary raycaster using OpenCL\nand .NET C#. Furthermore, we analyze different gradient operators\n(CentralDifference, Sobel3D and Zucker-Hummel) for surface detection and show\nan evaluation of these with respect to their performance. Finally, we present\noptimization techniques (hitpoint refinement, adaptive sampling, octrees, and\nempty-space-skipping) for improving a raycaster.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 20:26:24 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Kopal", "Nils", ""]]}, {"id": "1609.01499", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Rolf K\\\"ohler and Bernhard Sch\\\"olkopf and\n  Michael Hirsch", "title": "Depth Estimation Through a Generative Model of Light Field Synthesis", "comments": "German Conference on Pattern Recognition (GCPR) 2016", "journal-ref": null, "doi": "10.1007/978-3-319-45886-1_35", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field photography captures rich structural information that may\nfacilitate a number of traditional image processing and computer vision tasks.\nA crucial ingredient in such endeavors is accurate depth recovery. We present a\nnovel framework that allows the recovery of a high quality continuous depth map\nfrom light field data. To this end we propose a generative model of a light\nfield that is fully parametrized by its corresponding depth map. The model\nallows for the integration of powerful regularization techniques such as a\nnon-local means prior, facilitating accurate depth map estimation.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 11:43:08 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["K\u00f6hler", "Rolf", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Hirsch", "Michael", ""]]}, {"id": "1609.02072", "submitter": "Etienne Ferrier", "authors": "Etienne Ferrier", "title": "Sampling BSSRDFs with non-perpendicular incidence", "comments": "Supervised by Prof. Wenzel Jakob (EPFL). Contains 9 pages, 13\n  figures, 4 tables, 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sub-surface scattering is key to our perception of translucent materials.\nModels based on diffusion theory are used to render such materials in a\nrealistic manner by evaluating an approximation of the material BSSRDF at any\ntwo points of the surface. Under the assumption of perpendicular incidence,\nthis BSSRDF approximation can be tabulated over 2 dimensions to provide fast\nevaluation and importance sampling. However, accounting for non-perpendicular\nincidence with the same approach would require to tabulate over 4 dimensions,\nmaking the model too large for practical applications. In this report, we\npresent a method to efficiently evaluate and importance sample the\nmulti-scattering component of diffusion based BSSRDFs for non-perpendicular\nincidence. Our approach is based on tabulating a compressed angular model of\nPhoton Beam Diffusion. We explain how to generate, evaluate and sample our\nmodel. We show that 1 MiB is enough to store a model of the multi-scattering\nBSSRDF that is within $0.5\\%$ relative error of Photon Beam Diffusion. Finally,\nwe present a method to use our model in a Monte Carlo particle tracer and show\nresults of our implementation in PBRT.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 17:03:19 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Ferrier", "Etienne", ""]]}, {"id": "1609.02612", "submitter": "Carl Vondrick", "authors": "Carl Vondrick and Hamed Pirsiavash and Antonio Torralba", "title": "Generating Videos with Scene Dynamics", "comments": "NIPS 2016. See more at http://web.mit.edu/vondrick/tinyvideo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We capitalize on large amounts of unlabeled video in order to learn a model\nof scene dynamics for both video recognition tasks (e.g. action classification)\nand video generation tasks (e.g. future prediction). We propose a generative\nadversarial network for video with a spatio-temporal convolutional architecture\nthat untangles the scene's foreground from the background. Experiments suggest\nthis model can generate tiny videos up to a second at full frame rate better\nthan simple baselines, and we show its utility at predicting plausible futures\nof static images. Moreover, experiments and visualizations show the model\ninternally learns useful features for recognizing actions with minimal\nsupervision, suggesting scene dynamics are a promising signal for\nrepresentation learning. We believe generative video models can impact many\napplications in video understanding and simulation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 22:29:52 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 03:13:10 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 13:58:10 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vondrick", "Carl", ""], ["Pirsiavash", "Hamed", ""], ["Torralba", "Antonio", ""]]}, {"id": "1609.02974", "submitter": "Nima Khademi Kalantari", "authors": "Nima Khademi Kalantari, Ting-Chun Wang, Ravi Ramamoorthi", "title": "Learning-Based View Synthesis for Light Field Cameras", "comments": "in ACM Transactions on Graphics 2016", "journal-ref": null, "doi": "10.1145/2980179.2980251", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of consumer light field cameras, light field imaging\nhas recently become widespread. However, there is an inherent trade-off between\nthe angular and spatial resolution, and thus, these cameras often sparsely\nsample in either spatial or angular domain. In this paper, we use machine\nlearning to mitigate this trade-off. Specifically, we propose a novel\nlearning-based approach to synthesize new views from a sparse set of input\nviews. We build upon existing view synthesis techniques and break down the\nprocess into disparity and color estimation components. We use two sequential\nconvolutional neural networks to model these two components and train both\nnetworks simultaneously by minimizing the error between the synthesized and\nground truth images. We show the performance of our approach using only four\ncorner sub-aperture views from the light fields captured by the Lytro Illum\ncamera. Experimental results show that our approach synthesizes high-quality\nimages that are superior to the state-of-the-art techniques on a variety of\nchallenging real-world scenes. We believe our method could potentially decrease\nthe required angular resolution of consumer light field cameras, which allows\ntheir spatial resolution to increase.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 23:33:38 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kalantari", "Nima Khademi", ""], ["Wang", "Ting-Chun", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1609.02994", "submitter": "Marco Visentini-Scarzanella", "authors": "Takuto Hirukawa, Marco Visentini-Scarzanella, Hiroshi Kawasaki, Ryo\n  Furukawa, Shinsaku Hiura", "title": "Simultaneous independent image display technique on multiple 3D objects", "comments": "Accepted to ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new system to visualize depth-dependent patterns and images on\nsolid objects with complex geometry using multiple projectors. The system,\ndespite consisting of conventional passive LCD projectors, is able to project\ndifferent images and patterns depending on the spatial location of the object.\nThe technique is based on the simple principle that multiple patterns projected\nfrom multiple projectors interfere constructively with each other when their\npatterns are projected on the same object. Previous techniques based on the\nsame principle can only achieve 1) low resolution volume colorization or 2)\nhigh resolution images but only on a limited number of flat planes. In this\npaper, we discretize a 3D object into a number of 3D points so that high\nresolution images can be projected onto the complex shapes. We also propose a\ndynamic ranges expansion technique as well as an efficient optimization\nprocedure based on epipolar constraints.\n  Such technique can be used to the extend projection mapping to have spatial\ndependency, which is desirable for practical applications. We also demonstrate\nthe system potential as a visual instructor for object placement and\nassembling. Experiments prove the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 02:16:51 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Hirukawa", "Takuto", ""], ["Visentini-Scarzanella", "Marco", ""], ["Kawasaki", "Hiroshi", ""], ["Furukawa", "Ryo", ""], ["Hiura", "Shinsaku", ""]]}, {"id": "1609.03032", "submitter": "Haichuan Song", "authors": "Hai-Chuan Song, Nicolas Ray, Dmitry Sokolov, Sylvain Lefebvre", "title": "Anti-aliasing for fused filament deposition", "comments": "14 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Layered manufacturing inherently suffers from staircase defects along\nsurfaces that are gently slopped with respect to the build direction. Reducing\nthe slice thickness improves the situation but never resolves it completely as\nflat layers remain a poor approximation of the true surface in these regions.\nIn addition, reducing the slice thickness largely increases the print time. In\nthis work we focus on a simple yet effective technique to improve the print\naccuracy for layered manufacturing by filament deposition. Our method works\nwith standard three-axis 3D filament printers (e.g. the typical, widely\navailable 3D printers), using standard extrusion nozzles. It better reproduces\nthe geometry of sloped surfaces without increasing the print time. Our key idea\nis to perform a local anti-aliasing, working at a sub-layer accuracy to produce\nslightly curved deposition paths and reduce approximation errors. This is\ninspired by Computer Graphics anti-aliasing techniques which consider sub-pixel\nprecision to treat aliasing effects. We show that the necessary deviation in\nheight compared to standard slicing is bounded by half the layer thickness.\nTherefore, the height changes remain small and plastic deposition remains\nreliable. We further split and order paths to minimize defects due to the\nextruder nozzle shape, avoiding any change to the existing hardware. We apply\nand analyze our approach on 3D printed examples, showing that our technique\ngreatly improves surface accuracy and silhouette quality while keeping the\nprint time nearly identical.\n", "versions": [{"version": "v1", "created": "Sat, 10 Sep 2016 11:46:08 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 15:58:58 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Song", "Hai-Chuan", ""], ["Ray", "Nicolas", ""], ["Sokolov", "Dmitry", ""], ["Lefebvre", "Sylvain", ""]]}, {"id": "1609.05268", "submitter": "Takayuki Itoh", "authors": "Takayuki Itoh, Ashnil Kumar, Karsten Klein, Jinman Kim", "title": "High-Dimensional Data Visualization by Interactive Construction of\n  Low-Dimensional Parallel Coordinate Plots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinate plots (PCPs) are among the most useful techniques for the\nvisualization and exploration of high-dimensional data spaces. They are\nespecially useful for the representation of correlations among the dimensions,\nwhich identify relationships and interdependencies between variables. However,\nwithin these high-dimensional spaces, PCPs face difficulties in displaying the\ncorrelation between combinations of dimensions and generally require additional\ndisplay space as the number of dimensions increases. In this paper, we present\na new technique for high-dimensional data visualization in which a set of\nlow-dimensional PCPs are interactively constructed by sampling user-selected\nsubsets of the high-dimensional data space. In our technique, we first\nconstruct a graph visualization of sets of well-correlated dimensions. Users\nobserve this graph and are able to interactively select the dimensions by\nsampling from its cliques, thereby dynamically specifying the most relevant\nlower dimensional data to be used for the construction of focused PCPs. Our\ninteractive sampling overcomes the shortcomings of the PCPs by enabling the\nvisualization of the most meaningful dimensions (i.e., the most relevant\ninformation) from high-dimensional spaces. We demonstrate the effectiveness of\nour technique through two case studies, where we show that the proposed\ninteractive low-dimensional space constructions were pivotal for visualizing\nthe high-dimensional data and discovering new patterns.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 01:45:11 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Itoh", "Takayuki", ""], ["Kumar", "Ashnil", ""], ["Klein", "Karsten", ""], ["Kim", "Jinman", ""]]}, {"id": "1609.05283", "submitter": "Maneesh Agrawala", "authors": "Jonathan Harper and Maneesh Agrawala", "title": "Converting Basic D3 Charts into Reusable Style Templates", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for converting a basic D3 chart into a reusable style\ntemplate. Then, given a new data source we can apply the style template to\ngenerate a chart that depicts the new data, but in the style of the template.\nTo construct the style template we first deconstruct the input D3 chart to\nrecover its underlying structure: the data, the marks and the mappings that\ndescribe how the marks encode the data. We then rank the perceptual\neffectiveness of the deconstructed mappings. To apply the resulting style\ntemplate to a new data source we first obtain importance ranks for each new\ndata field. We then adjust the template mappings to depict the source data by\nmatching the most important data fields to the most perceptually effective\nmappings. We show how the style templates can be applied to source data in the\nform of either a data table or another D3 chart. While our implementation\nfocuses on generating templates for basic chart types (e.g. variants of bar\ncharts, line charts, dot plots, scatterplots, etc.), these are the most\ncommonly used chart types today. Users can easily find such basic D3 charts on\nthe Web, turn them into templates, and immediately see how their own data would\nlook in the visual style (e.g. colors, shapes, fonts, etc.) of the templates.\nWe demonstrate the effectiveness of our approach by applying a diverse set of\nstyle templates to a variety of source datasets.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 05:08:24 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 04:04:10 GMT"}, {"version": "v3", "created": "Sat, 1 Apr 2017 16:42:22 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Harper", "Jonathan", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1609.05328", "submitter": "Jan Vr\\v{s}ek", "authors": "Michal Bizzarri and Miroslav L\\'avi\\v{c}ka and Zby\\v{n}ek \\v{S}\\'ir\n  and Jan Vr\\v{s}ek", "title": "Hermite interpolation by piecewise polynomial surfaces with polynomial\n  area element", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the construction of polynomial 2-surfaces which\npossess a polynomial area element. In particular we study these surfaces in the\nEuclidean space $\\mathbb R^3$ (where they are equivalent to the PN surfaces)\nand in the Minkowski space $\\mathbb R^{3,1}$ (where they provide the MOS\nsurfaces). We show generally in real vector spaces of any dimension and any\nmetric that the Gram determinant of a parametric set of subspaces is a perfect\nsquare if and only if the Gram determinant of its orthogonal complement is a\nperfect square. Consequently the polynomial surfaces of a given degree with\npolynomial area element can be constructed from the prescribed normal fields\nsolving a system of linear equations. The degree of the constructed surface\ndepending on the degree and the quality of the prescribed normal field is\ninvestigated and discussed. We use the presented approach to interpolate a\nnetwork of points and associated normals with piecewise polynomial surfaces\nwith polynomial area element and demonstrate our method on a number of examples\n(constructions of quadrilateral as well as triangular patches\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 12:54:41 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Bizzarri", "Michal", ""], ["L\u00e1vi\u010dka", "Miroslav", ""], ["\u0160\u00edr", "Zby\u0148ek", ""], ["Vr\u0161ek", "Jan", ""]]}, {"id": "1609.05344", "submitter": "Alastair Toft", "authors": "Alastair Toft, Huw Bowles and Daniel Zimmermann", "title": "Optimisations for Real-Time Volumetric Cloudscapes", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric cloudscapes are prohibitively expensive to render in real time\nwithout extensive optimisations. Previous approaches render the clouds to an\noffscreen buffer at one quarter resolution and update a fraction of the pixels\nper frame, drawing the remaining pixels by temporal reprojection. We present an\nalternative approach, reducing the number of raymarching steps and adding a\nrandomly jittered offset to the raymarch. We use an analytical integration\ntechnique to make results consistent with a lower number of raymarching steps.\nTo remove noise from the resulting image we apply a temporal anti-aliasing\nimplementation. The result is a technique producing visually similar results\nwith 1/16 the number of steps.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 14:43:17 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Toft", "Alastair", ""], ["Bowles", "Huw", ""], ["Zimmermann", "Daniel", ""]]}, {"id": "1609.05561", "submitter": "Ricardo Fabbri", "authors": "Anil Usumezbas and Ricardo Fabbri and Benjamin B. Kimia", "title": "From Multiview Image Curves to 3D Drawings", "comments": "Expanded ECCV 2016 version with tweaked figures and including an\n  overview of the supplementary material available at\n  multiview-3d-drawing.sourceforge.net", "journal-ref": "Lecture Notes in Computer Science, 9908, pp 70-87, september 2016", "doi": "10.1007/978-3-319-46493-0_5", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing 3D scenes from multiple views has made impressive strides in\nrecent years, chiefly by correlating isolated feature points, intensity\npatterns, or curvilinear structures. In the general setting - without\ncontrolled acquisition, abundant texture, curves and surfaces following\nspecific models or limiting scene complexity - most methods produce unorganized\npoint clouds, meshes, or voxel representations, with some exceptions producing\nunorganized clouds of 3D curve fragments. Ideally, many applications require\nstructured representations of curves, surfaces and their spatial relationships.\nThis paper presents a step in this direction by formulating an approach that\ncombines 2D image curves into a collection of 3D curves, with topological\nconnectivity between them represented as a 3D graph. This results in a 3D\ndrawing, which is complementary to surface representations in the same sense as\na 3D scaffold complements a tent taut over it. We evaluate our results against\ntruth on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2016 22:20:35 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Usumezbas", "Anil", ""], ["Fabbri", "Ricardo", ""], ["Kimia", "Benjamin B.", ""]]}, {"id": "1609.06536", "submitter": "Samuli Laine", "authors": "Samuli Laine, Tero Karras, Timo Aila, Antti Herva, Shunsuke Saito,\n  Ronald Yu, Hao Li, Jaakko Lehtinen", "title": "Production-Level Facial Performance Capture Using Deep Convolutional\n  Neural Networks", "comments": "Final SCA 2017 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a real-time deep learning framework for video-based facial\nperformance capture -- the dense 3D tracking of an actor's face given a\nmonocular video. Our pipeline begins with accurately capturing a subject using\na high-end production facial capture pipeline based on multi-view stereo\ntracking and artist-enhanced animations. With 5-10 minutes of captured footage,\nwe train a convolutional neural network to produce high-quality output,\nincluding self-occluded regions, from a monocular video sequence of that\nsubject. Since this 3D facial performance capture is fully automated, our\nsystem can drastically reduce the amount of labor involved in the development\nof modern narrative-driven video games or films involving realistic digital\ndoubles of actors and potentially hours of animated dialogue per character. We\ncompare our results with several state-of-the-art monocular real-time facial\ncapture techniques and demonstrate compelling animation inference in\nchallenging areas such as eyes and lips.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 12:55:59 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 13:54:51 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Laine", "Samuli", ""], ["Karras", "Tero", ""], ["Aila", "Timo", ""], ["Herva", "Antti", ""], ["Saito", "Shunsuke", ""], ["Yu", "Ronald", ""], ["Li", "Hao", ""], ["Lehtinen", "Jaakko", ""]]}, {"id": "1609.07049", "submitter": "Matan Sela", "authors": "Matan Sela, Nadav Toledo, Yaron Honen, Ron Kimmel", "title": "Customized Facial Constant Positive Air Pressure (CPAP) Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep apnea is a syndrome that is characterized by sudden breathing halts\nwhile sleeping. One of the common treatments involves wearing a mask that\ndelivers continuous air flow into the nostrils so as to maintain a steady air\npressure. These masks are designed for an average facial model and are often\ndifficult to adjust due to poor fit to the actual patient. The incompatibility\nis characterized by gaps between the mask and the face, which deteriorates the\nimpermeability of the mask and leads to air leakage. We suggest a fully\nautomatic approach for designing a personalized nasal mask interface using a\nfacial depth scan. The interfaces generated by the proposed method accurately\nfit the geometry of the scanned face, and are easy to manufacture. The proposed\nmethod utilizes cheap commodity depth sensors and 3D printing technologies to\nefficiently design and manufacture customized masks for patients suffering from\nsleep apnea.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 16:11:57 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Sela", "Matan", ""], ["Toledo", "Nadav", ""], ["Honen", "Yaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.07738", "submitter": "Matan Sela", "authors": "Alon Shtern, Matan Sela and Ron Kimmel", "title": "Fast Blended Transformations for Partial Shape Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic estimation of skinning transformations is a popular way to deform a\nsingle reference shape into a new pose by providing a small number of control\nparameters. We generalize this approach by efficiently enabling the use of\nmultiple exemplar shapes. Using a small set of representative natural poses, we\npropose to express an unseen appearance by a low-dimensional linear subspace,\nspecified by a redundant dictionary of weighted vertex positions. Minimizing a\nnonlinear functional that regulates the example manifold, the suggested\napproach supports local-rigid deformations of articulated objects, as well as\nnearly isometric embeddings of smooth shapes. A real-time non-rigid deformation\nsystem is demonstrated, and a shape completion and partial registration\nframework is introduced. These applications can recover a target pose and\nimplicit inverse kinematics from a small number of examples and just a few\nvertex positions. The result reconstruction is more accurate compared to\nstate-of-the-art reduced deformable models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 13:20:58 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Shtern", "Alon", ""], ["Sela", "Matan", ""], ["Kimmel", "Ron", ""]]}, {"id": "1609.08313", "submitter": "Jun Yang", "authors": "Jun Yang, Zhenhua Tian", "title": "Unsupervised Co-segmentation of 3D Shapes via Functional Maps", "comments": "14 pages, 8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised method for co-segmentation of a set of 3D shapes\nfrom the same class with the aim of segmenting the input shapes into consistent\nsemantic parts and establishing their correspondence across the set. Starting\nfrom meaningful pre-segmentation of all given shapes individually, we construct\nthe correspondence between same candidate parts and obtain the labels via\nfunctional maps. And then, we use these labels to mark the input shapes and\nobtain results of co-segmentation. The core of our algorithm is to seek for an\noptimal correspondence between semantically similar parts through functional\nmaps and mark such shape parts. Experimental results on the benchmark datasets\nshow the efficiency of this method and comparable accuracy to the\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 08:35:14 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Yang", "Jun", ""], ["Tian", "Zhenhua", ""]]}, {"id": "1609.08685", "submitter": "Soren Pirk", "authors": "S\\\"oren Pirk, Vojtech Krs, Kaimo Hu, Suren Deepak Rajasekaran, Hao\n  Kang, Bedrich Benes, Yusuke Yoshiyasu, Leonidas J. Guibas", "title": "Understanding and Exploiting Object Interaction Landscapes", "comments": "14 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions play a key role in understanding objects and scenes, for both\nvirtual and real world agents. We introduce a new general representation for\nproximal interactions among physical objects that is agnostic to the type of\nobjects or interaction involved. The representation is based on tracking\nparticles on one of the participating objects and then observing them with\nsensors appropriately placed in the interaction volume or on the interaction\nsurfaces. We show how to factorize these interaction descriptors and project\nthem into a particular participating object so as to obtain a new functional\ndescriptor for that object, its interaction landscape, capturing its observed\nuse in a spatio-temporal framework. Interaction landscapes are independent of\nthe particular interaction and capture subtle dynamic effects in how objects\nmove and behave when in functional use. Our method relates objects based on\ntheir function, establishes correspondences between shapes based on functional\nkey points and regions, and retrieves peer and partner objects with respect to\nan interaction.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 22:00:56 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 20:13:56 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Pirk", "S\u00f6ren", ""], ["Krs", "Vojtech", ""], ["Hu", "Kaimo", ""], ["Rajasekaran", "Suren Deepak", ""], ["Kang", "Hao", ""], ["Benes", "Bedrich", ""], ["Yoshiyasu", "Yusuke", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1609.08729", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini and Viswanathan Swaminathan", "title": "Adaptive 360 VR Video Streaming: Divide and Conquer!", "comments": "IEEE International Symposium on Multimedia 2016 (ISM '16), December\n  4-7, San Jose, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.GR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional multimedia applications such as games and videos are still\npopular, there has been a significant interest in the recent years towards new\n3D media such as 3D immersion and Virtual Reality (VR) applications, especially\n360 VR videos. 360 VR video is an immersive spherical video where the user can\nlook around during playback. Unfortunately, 360 VR videos are extremely\nbandwidth intensive, and therefore are difficult to stream at acceptable\nquality levels. In this paper, we propose an adaptive bandwidth-efficient 360\nVR video streaming system using a divide and conquer approach. In our approach,\nwe propose a dynamic view-aware adaptation technique to tackle the huge\nstreaming bandwidth demands of 360 VR videos. We spatially divide the videos\ninto multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe\nthe spatial relationship of tiles in the 360-degree space, and prioritize the\ntiles in the Field of View (FoV). In order to describe such tiled\nrepresentations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We\nspatially partition the underlying 3D mesh, and construct an efficient 3D\ngeometry mesh called hexaface sphere to optimally represent a tiled 360 VR\nvideo in the 3D space. Our initial evaluation results report up to 72%\nbandwidth savings on 360 VR video streaming with minor negative quality impacts\ncompared to the baseline scenario when no adaptations is applied.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 02:07:12 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 17:05:33 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2016 18:45:41 GMT"}, {"version": "v4", "created": "Mon, 23 Jan 2017 17:18:33 GMT"}, {"version": "v5", "created": "Fri, 17 Nov 2017 17:03:33 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Swaminathan", "Viswanathan", ""]]}]