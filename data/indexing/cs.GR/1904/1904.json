[{"id": "1904.00275", "submitter": "Mei-Yun Chen", "authors": "Mei-Yun Chen, Ya-Bo Huang, Sheng-Ping Chang and Ming Ouhyoung", "title": "Prediction Model for Semitransparent Watercolor Pigment Mixtures Using\n  Deep Learning with a Dataset of Transmittance and Reflectance", "comments": "26 pages and 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning color mixing is difficult for novice painters. In order to support\nnovice painters in learning color mixing, we propose a prediction model for\nsemitransparent pigment mixtures and use its prediction results to create a\nSmart Palette system. Such a system is constructed by first building a\nwatercolor dataset with two types of color mixing data, indicated by\ntransmittance and reflectance: incrementation of the same primary pigment and a\nmixture of two different pigments. Next, we apply the collected data to a deep\nneural network to train a model for predicting the results of semitransparent\npigment mixtures. Finally, we constructed a Smart Palette that provides\neasily-followable instructions on mixing a target color with two primary\npigments in real life: when users pick a pixel, an RGB color, from an image,\nthe system returns its mixing recipe which indicates the two primary pigments\nbeing used and their quantities.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 19:27:33 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chen", "Mei-Yun", ""], ["Huang", "Ya-Bo", ""], ["Chang", "Sheng-Ping", ""], ["Ouhyoung", "Ming", ""]]}, {"id": "1904.00306", "submitter": "Morando Franco", "authors": "Franco Morando", "title": "Decomposition and Modeling in the Non-Manifold domain", "comments": "PhD. Thesis", "journal-ref": null, "doi": "10.13140/RG.2.2.15402.88008/1", "report-no": "DISI-TH-2003-02", "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The problem of decomposing non-manifold object has already been studied in\nsolid modeling. However, the few proposed solutions are limited to the problem\nof decomposing solids described through their boundaries. In this thesis we\nstudy the problem of decomposing an arbitrary non-manifold simplicial complex\ninto more regular components. A formal notion of decomposition is developed\nusing combinatorial topology. The proposed decomposition is unique, for a given\ncomplex, and is computable for complexes of any dimension. A decomposition\nalgorithm is proposed that is linear w.r.t. the size of the input. In three or\nhigher dimensions a decomposition into manifold parts is not always possible.\nThus, in higher dimensions, we decompose a non-manifold into a decidable super\nclass of manifolds, that we call, Initial-Quasi-Manifolds. We also defined a\ntwo-layered data structure, the Extended Winged data structure. This data\nstructure is a dimension independent data structure conceived to model\nnon-manifolds through their decomposition into initial-quasi-manifold parts.\nOur two layered data structure describes the structure of the decomposition and\neach component separately. In the second layer we encode the connectivity\nstructure of the decomposition. We analyze the space requirements of the\nExtended Winged data structure and give algorithms to build and navigate it.\nFinally, we discuss time requirements for the computation of topological\nrelations and show that, for surfaces and tetrahedralizations, embedded in real\n3D space, all topological relations can be extracted in optimal time. This\napproach offers a compact, dimension independent, representation for\nnon-manifolds that can be useful whenever the modeled object has few\nnon-manifold singularities.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2019 23:23:44 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Morando", "Franco", ""]]}, {"id": "1904.00722", "submitter": "Micha Pfeiffer", "authors": "Micha Pfeiffer, Carina Riediger, J\\\"urgen Weitz, Stefanie Speidel", "title": "Learning Soft Tissue Behavior of Organs for Surgical Navigation with\n  Convolutional Neural Networks", "comments": "Accepted at IPCAI 2019; submitted to IJCARS (under revision). Source\n  code will be released upon publication in IJCARS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: In surgical navigation, pre-operative organ models are presented to\nsurgeons during the intervention to help them in efficiently finding their\ntarget. In the case of soft tissue, these models need to be deformed and\nadapted to the current situation by using intra-operative sensor data. A\npromising method to realize this are real-time capable biomechanical models.\n  Methods: We train a fully convolutional neural network to estimate a\ndisplacement field of all points inside an organ when given only the\ndisplacement of a part of the organ's surface. The network trains on entirely\nsynthetic data of random organ-like meshes, which allows us to generate much\nmore data than is otherwise available. The input and output data is discretized\ninto a regular grid, allowing us to fully utilize the capabilities of\nconvolutional operators and to train and infer in a highly parallelized manner.\n  Results: The system is evaluated on in-silico liver models, phantom liver\ndata and human in-vivo breathing data. We test the performance with varying\nmaterial parameters, organ shapes and amount of visible surface. Even though\nthe network is only trained on synthetic data, it adapts well to the various\ncases and gives a good estimation of the internal organ displacement. The\ninference runs at over 50 frames per second.\n  Conclusions: We present a novel method for training a data-driven, real-time\ncapable deformation model. The accuracy is comparable to other registration\nmethods, it adapts very well to previously unseen organs and does not need to\nbe re-trained for every patient. The high inferring speed makes this method\nuseful for many applications such as surgical navigation and real-time\nsimulation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 07:42:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Pfeiffer", "Micha", ""], ["Riediger", "Carina", ""], ["Weitz", "J\u00fcrgen", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1904.00824", "submitter": "Sebastian Hartwig", "authors": "Sebastian Hartwig, Timo Ropinski", "title": "Training Object Detectors on Synthetic Images Containing Reflecting\n  Materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the grand challenges of deep learning is the requirement to obtain\nlarge labeled training data sets. While synthesized data sets can be used to\novercome this challenge, it is important that these data sets close the reality\ngap, i.e., a model trained on synthetic image data is able to generalize to\nreal images. Whereas, the reality gap can be considered bridged in several\napplication scenarios, training on synthesized images containing reflecting\nmaterials requires further research. Since the appearance of objects with\nreflecting materials is dominated by the surrounding environment, this\ninteraction needs to be considered during training data generation. Therefore,\nwithin this paper we examine the effect of reflecting materials in the context\nof synthetic image generation for training object detectors. We investigate the\ninfluence of rendering approach used for image synthesis, the effect of domain\nrandomization, as well as the amount of used training data. To be able to\ncompare our results to the state-of-the-art, we focus on indoor scenes as they\nhave been investigated extensively. Within this scenario, bathroom furniture is\na natural choice for objects with reflecting materials, for which we report our\nfindings on real and synthetic testing data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:27:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Hartwig", "Sebastian", ""], ["Ropinski", "Timo", ""]]}, {"id": "1904.01152", "submitter": "Elias Salom\\~ao Helou Neto", "authors": "Elias S. Helou, Marcelo V. W. Zibetti, Leon Axel, Kai Tobias Block,\n  Ravinder R. Regatte, Gabor T. Herman", "title": "The Discrete Fourier Transform for Golden Angle Linogram Sampling", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab44ee", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the Discrete-Time Fourier Transform (DTFT) at points of a\nfinite domain arises in many imaging applications. A new approach to this task,\nthe Golden Angle Linogram Fourier Domain (GALFD), is presented, together with a\ncomputationally fast and accurate tool, named Golden Angle Linogram Evaluation\n(GALE), for approximating the DTFT at points of a GALFD. A GALFD resembles a\nLinogram Fourier Domain (LFD), which is efficient and accurate. A limitation of\nlinograms is that embedding an LFD into a larger one requires many extra\npoints, at least doubling the domain's cardinality. The GALFD, on the other\nhand, allows for incremental inclusion of relatively few data points.\nApproximation error bounds and floating point operations counts are presented\nto show that GALE computes accurately and efficiently the DTFT at the points of\na GALFD. The ability to extend the data collection in small increments is\nbeneficial in applications such as Magnetic Resonance Imaging. Experiments for\nsimulated and for real-world data are presented to substantiate the theoretical\nclaims. The mathematical analysis, algorithms, and software developed in the\npaper are equally suitable to other angular distributions of rays and therefore\nwe bring the benefits of linograms to arbitrary radial patterns.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 00:23:13 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:50:24 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Helou", "Elias S.", ""], ["Zibetti", "Marcelo V. W.", ""], ["Axel", "Leon", ""], ["Block", "Kai Tobias", ""], ["Regatte", "Ravinder R.", ""], ["Herman", "Gabor T.", ""]]}, {"id": "1904.01175", "submitter": "Chloe LeGendre", "authors": "Chloe LeGendre, Wan-Chun Ma, Graham Fyffe, John Flynn, Laurent\n  Charbonnel, Jay Busch, Paul Debevec", "title": "DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method to infer plausible high dynamic range\n(HDR), omnidirectional illumination given an unconstrained, low dynamic range\n(LDR) image from a mobile phone camera with a limited field of view (FOV). For\ntraining data, we collect videos of various reflective spheres placed within\nthe camera's FOV, leaving most of the background unoccluded, leveraging that\nmaterials with diverse reflectance functions reveal different lighting cues in\na single exposure. We train a deep neural network to regress from the LDR\nbackground image to HDR lighting by matching the LDR ground truth sphere images\nto those rendered with the predicted illumination using image-based relighting,\nwhich is differentiable. Our inference runs at interactive frame rates on a\nmobile device, enabling realistic rendering of virtual objects into real scenes\nfor mobile mixed reality. Training on automatically exposed and white-balanced\nvideos, we improve the realism of rendered objects compared to the state-of-the\nart methods for both indoor and outdoor scenes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:15:09 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["LeGendre", "Chloe", ""], ["Ma", "Wan-Chun", ""], ["Fyffe", "Graham", ""], ["Flynn", "John", ""], ["Charbonnel", "Laurent", ""], ["Busch", "Jay", ""], ["Debevec", "Paul", ""]]}, {"id": "1904.01428", "submitter": "Lingjing Wang", "authors": "Lingjing Wang, Jianchun Chen, Xiang Li, Yi Fang", "title": "Non-Rigid Point Set Registration Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Point set registration is defined as a process to determine the spatial\ntransformation from the source point set to the target one. Existing methods\noften iteratively search for the optimal geometric transformation to register a\ngiven pair of point sets, driven by minimizing a predefined alignment loss\nfunction. In contrast, the proposed point registration neural network (PR-Net)\nactively learns the registration pattern as a parametric function from a\ntraining dataset, consequently predict the desired geometric transformation to\nalign a pair of point sets. PR-Net can transfer the learned knowledge (i.e.\nregistration pattern) from registering training pairs to testing ones without\nadditional iterative optimization. Specifically, in this paper, we develop\nnovel techniques to learn shape descriptors from point sets that help formulate\na clear correlation between source and target point sets. With the defined\ncorrelation, PR-Net tends to predict the transformation so that the source and\ntarget point sets can be statistically aligned, which in turn leads to an\noptimal spatial geometric registration. PR-Net achieves robust and superior\nperformance for non-rigid registration of point sets, even in presence of\nGaussian noise, outliers, and missing points, but requires much less time for\nregistering large number of pairs. More importantly, for a new pair of point\nsets, PR-Net is able to directly predict the desired transformation using the\nlearned model without repetitive iterative optimization routine. Our code is\navailable at https://github.com/Lingjing324/PR-Net.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 14:01:59 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Wang", "Lingjing", ""], ["Chen", "Jianchun", ""], ["Li", "Xiang", ""], ["Fang", "Yi", ""]]}, {"id": "1904.01509", "submitter": "Jian Xue", "authors": "Yanfu Yan, Ke Lu, Jian Xue, Pengcheng Gao, Jiayi Lyu", "title": "FEAFA: A Well-Annotated Dataset for Facial Expression Analysis and 3D\n  Facial Animation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression analysis based on machine learning requires large number of\nwell-annotated data to reflect different changes in facial motion. Publicly\navailable datasets truly help to accelerate research in this area by providing\na benchmark resource, but all of these datasets, to the best of our knowledge,\nare limited to rough annotations for action units, including only their\nabsence, presence, or a five-level intensity according to the Facial Action\nCoding System. To meet the need for videos labeled in great detail, we present\na well-annotated dataset named FEAFA for Facial Expression Analysis and 3D\nFacial Animation. One hundred and twenty-two participants, including children,\nyoung adults and elderly people, were recorded in real-world conditions. In\naddition, 99,356 frames were manually labeled using Expression Quantitative\nTool developed by us to quantify 9 symmetrical FACS action units, 10\nasymmetrical (unilateral) FACS action units, 2 symmetrical FACS action\ndescriptors and 2 asymmetrical FACS action descriptors, and each action unit or\naction descriptor is well-annotated with a floating point number between 0 and\n1. To provide a baseline for use in future research, a benchmark for the\nregression of action unit values based on Convolutional Neural Networks are\npresented. We also demonstrate the potential of our FEAFA dataset for 3D facial\nanimation. Almost all state-of-the-art algorithms for facial animation are\nachieved based on 3D face reconstruction. We hence propose a novel method that\ndrives virtual characters only based on action unit value regression of the 2D\nvideo frames of source actors.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 15:50:11 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Yan", "Yanfu", ""], ["Lu", "Ke", ""], ["Xue", "Jian", ""], ["Gao", "Pengcheng", ""], ["Lyu", "Jiayi", ""]]}, {"id": "1904.02348", "submitter": "Yanchao Wang", "authors": "Yan-Chao Wang and Feng Lin and Hock-Soon Seah", "title": "Orthogonal Voronoi Diagram and Treemap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel space partitioning strategy for implicit\nhierarchy visualization such that the new plot not only has a tidy layout\nsimilar to the treemap, but also is flexible to data changes similar to the\nVoronoi treemap. To achieve this, we define a new distance function and\nneighborhood relationship between sites so that space will be divided by\naxis-aligned segments. Then a sweepline+skyline based heuristic algorithm is\nproposed to allocate the partitioned spaces to form an orthogonal Voronoi\ndiagram with orthogonal rectangles. To the best of our knowledge, it is the\nfirst time to use a sweepline-based strategy for the Voronoi treemap. Moreover,\nwe design a novel strategy to initialize the diagram status and modify the\nstatus update procedure so that the generation of our plot is more effective\nand efficient. We show that the proposed algorithm has an O(nlog(n)) complexity\nwhich is the same as the state-of-the-art Voronoi treemap. To this end, we show\nvia experiments on the artificial dataset and real-world dataset the\nperformance of our algorithm in terms of computation time, converge rate, and\naspect ratio. Finally, we discuss the pros and cons of our method and make a\nconclusion.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:05:49 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Yan-Chao", ""], ["Lin", "Feng", ""], ["Seah", "Hock-Soon", ""]]}, {"id": "1904.02524", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg, Benjamin Audenrith", "title": "x3ogre: Connecting X3D to a state of the art rendering engine", "comments": null, "journal-ref": null, "doi": "10.1145/3055624.3075949", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We connect X3D to the state of the art OGRE renderer using our prototypical\nx3ogre implementation. At this we perform a comparison of both on a conceptual\nlevel, highlighting similarities and differences. Our implementation allows\nswapping X3D concepts for OGRE concepts and vice versa. We take advantage of\nthis to analyse current shortcomings in X3D and propose X3D extensions to\novercome those.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 12:48:40 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Rojtberg", "Pavel", ""], ["Audenrith", "Benjamin", ""]]}, {"id": "1904.02526", "submitter": "Eric Heim", "authors": "Eric Heim", "title": "Constrained Generative Adversarial Networks for Interactive Image\n  Generation", "comments": "To Appear in the Proceedings of the 2019 Conference on Computer\n  Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have received a great deal of\nattention due in part to recent success in generating original, high-quality\nsamples from visual domains. However, most current methods only allow for users\nto guide this image generation process through limited interactions. In this\nwork we develop a novel GAN framework that allows humans to be \"in-the-loop\" of\nthe image generation process. Our technique iteratively accepts relative\nconstraints of the form \"Generate an image more like image A than image B\".\nAfter each constraint is given, the user is presented with new outputs from the\nGAN, informing the next round of feedback. This feedback is used to constrain\nthe output of the GAN with respect to an underlying semantic space that can be\ndesigned to model a variety of different notions of similarity (e.g. classes,\nattributes, object relationships, color, etc.). In our experiments, we show\nthat our GAN framework is able to generate images that are of comparable\nquality to equivalent unsupervised GANs while satisfying a large number of the\nconstraints provided by users, effectively changing a GAN into one that allows\nusers interactive control over image generation without sacrificing image\nquality.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 17:59:41 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Heim", "Eric", ""]]}, {"id": "1904.02756", "submitter": "Amir Hertz", "authors": "Amir Hertz, Sharon Fogel, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "Blind Visual Motif Removal from a Single Image", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many images shared over the web include overlaid objects, or visual motifs,\nsuch as text, symbols or drawings, which add a description or decoration to the\nimage. For example, decorative text that specifies where the image was taken,\nrepeatedly appears across a variety of different images. Often, the reoccurring\nvisual motif, is semantically similar, yet, differs in location, style and\ncontent (e.g. text placement, font and letters). This work proposes a deep\nlearning based technique for blind removal of such objects. In the blind\nsetting, the location and exact geometry of the motif are unknown. Our approach\nsimultaneously estimates which pixels contain the visual motif, and synthesizes\nthe underlying latent image. It is applied to a single input image, without any\nuser assistance in specifying the location of the motif, achieving\nstate-of-the-art results for blind removal of both opaque and semi-transparent\nvisual motifs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:17:05 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Hertz", "Amir", ""], ["Fogel", "Sharon", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1904.02898", "submitter": "Tiago Ribeiro", "authors": "Tiago Ribeiro and Ana Paiva", "title": "Nutty-based Robot Animation -- Principles and Practices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot animation is a new form of character animation that extends the\ntraditional process by allowing the animated motion to become more interactive\nand adaptable during interaction with users in real-world settings. This paper\nreviews how this new type of character animation has evolved and been shaped\nfrom character animation principles and practices. We outline some new\nparadigms that aim at allowing character animators to become robot animators,\nand to properly take part in the development of social robots. One such\nparadigm consists of the 12 principles of robot animation, which describes\ngeneral concepts that both animators and robot developers should consider in\norder to properly understand each other. We also introduce the concept of\nKinematronics, for specifying the controllable and programmable expressive\nabilities of robots, and the Nutty Workflow and Pipeline. The Nutty Pipeline\nintroduces the concept of the Programmable Robot Animation Engine, which allows\nto generate, compose and blend various types of animation sources into a final,\ninteraction-enabled motion that can be rendered on robots in real-time during\nreal-world interactions. The Nutty Motion Filter is described and exemplified\nas a technique that allows an open-loop motion controller to apply physical\nlimits to the motion while still allowing to tweak the shape and expressivity\nof the resulting motion. Additionally, we describe some types of tools that can\nbe developed and integrated into Nutty-based workflows and pipelines, which\nallow animation artists to perform an integral part of the expressive behaviour\ndevelopment within social robots, and thus to evolve from standard (3D)\ncharacter animators, towards a full-stack type of robot animators.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:13:01 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 15:27:52 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 17:34:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ribeiro", "Tiago", ""], ["Paiva", "Ana", ""]]}, {"id": "1904.03258", "submitter": "Fehmi Cirak", "authors": "Qiaoling Zhang, Fehmi Cirak", "title": "Manifold-based isogeometric analysis basis functions with prescribed\n  sharp features", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2019.112659", "report-no": null, "categories": "math.NA cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce manifold-based basis functions for isogeometric analysis of\nsurfaces with arbitrary smoothness, prescribed $C^0$ continuous creases and\nboundaries. The utility of the manifold-based surface construction techniques\nin isogeometric analysis was demonstrated in Majeed and Cirak (CMAME, 2017).\nThe respective basis functions are derived by combining differential-geometric\nmanifold techniques with conformal parametrisations and the partition of unity\nmethod. The connectivity of a given unstructured quadrilateral control mesh in\n$\\mathbb R^3$ is used to define a set of overlapping charts. Each vertex with\nits attached elements is assigned a corresponding conformally parametrised\nplanar chart domain in $\\mathbb R^2$, so that a quadrilateral element is\npresent on four different charts. On the collection of unconnected chart\ndomains, the partition of unity method is used for approximation. The\ntransition functions required for navigating between the chart domains are\ncomposed out of conformal maps. The necessary smooth partition of unity, or\nblending, functions for the charts are assembled from tensor-product B-spline\npieces and require in contrast to earlier constructions no normalisation.\nCreases are introduced across user tagged edges of the control mesh. Planar\nchart domains that include creased edges or are adjacent to the domain boundary\nrequire special local polynomial approximants. Three different types of chart\ndomain geometries are necessary to consider boundaries and arbitrary number and\narrangement of creases. The new chart domain geometries are chosen so that it\nbecomes trivial to establish local polynomial approximants that are always\n$C^0$ continuous across tagged edges. The derived non-rational manifold-based\nbasis functions are particularly well suited for isogeometric analysis of\nKirchhoff-Love thin shells with kinks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 20:04:43 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 16:58:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Zhang", "Qiaoling", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1904.03278", "submitter": "Naureen Mahmood", "authors": "Naureen Mahmood (Meshcapade GmbH), Nima Ghorbani (MPI for Intelligent\n  Systems), Nikolaus F. Troje (York University), Gerard Pons-Moll (MPI for\n  Informatics) and Michael J. Black (MPI for Intelligent Systems)", "title": "AMASS: Archive of Motion Capture as Surface Shapes", "comments": "12 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large datasets are the cornerstone of recent advances in computer vision\nusing deep learning. In contrast, existing human motion capture (mocap)\ndatasets are small and the motions limited, hampering progress on learning\nmodels of human motion. While there are many different datasets available, they\neach use a different parameterization of the body, making it difficult to\nintegrate them into a single meta dataset. To address this, we introduce AMASS,\na large and varied database of human motion that unifies 15 different optical\nmarker-based mocap datasets by representing them within a common framework and\nparameterization. We achieve this using a new method, MoSh++, that converts\nmocap data into realistic 3D human meshes represented by a rigged body model;\nhere we use SMPL [doi:10.1145/2816795.2818013], which is widely used and\nprovides a standard skeletal representation as well as a fully rigged surface\nmesh. The method works for arbitrary marker sets, while recovering soft-tissue\ndynamics and realistic hand motion. We evaluate MoSh++ and tune its\nhyperparameters using a new dataset of 4D body scans that are jointly recorded\nwith marker-based mocap. The consistent representation of AMASS makes it\nreadily useful for animation, visualization, and generating training data for\ndeep learning. Our dataset is significantly richer than previous human motion\ncollections, having more than 40 hours of motion data, spanning over 300\nsubjects, more than 11,000 motions, and will be publicly available to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:00:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Mahmood", "Naureen", "", "Meshcapade GmbH"], ["Ghorbani", "Nima", "", "MPI for Intelligent\n  Systems"], ["Troje", "Nikolaus F.", "", "York University"], ["Pons-Moll", "Gerard", "", "MPI for\n  Informatics"], ["Black", "Michael J.", "", "MPI for Intelligent Systems"]]}, {"id": "1904.03620", "submitter": "Varshaneya V", "authors": "Varshaneya V, S Balasubramanian and Vineeth N Balasubramanian", "title": "Teaching GANs to Sketch in Vector Format", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sketching is more fundamental to human cognition than speech. Deep Neural\nNetworks (DNNs) have achieved the state-of-the-art in speech-related tasks but\nhave not made significant development in generating stroke-based sketches a.k.a\nsketches in vector format. Though there are Variational Auto Encoders (VAEs)\nfor generating sketches in vector format, there is no Generative Adversarial\nNetwork (GAN) architecture for the same. In this paper, we propose a standalone\nGAN architecture SkeGAN and a VAE-GAN architecture VASkeGAN, for sketch\ngeneration in vector format. SkeGAN is a stochastic policy in Reinforcement\nLearning (RL), capable of generating both multidimensional continuous and\ndiscrete outputs. VASkeGAN hybridizes a VAE and a GAN, in order to couple the\nefficient representation of data by VAE with the powerful generating\ncapabilities of a GAN, to produce visually appealing sketches. We also propose\na new metric called the Ske-score which quantifies the quality of vector\nsketches. We have validated that SkeGAN and VASkeGAN generate visually\nappealing sketches by using Human Turing Test and Ske-score.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 10:23:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["V", "Varshaneya", ""], ["Balasubramanian", "S", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1904.04290", "submitter": "Ricardo Martin Brualla", "authors": "Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit\n  Pandey, Noah Snavely, Ricardo Martin-Brualla", "title": "Neural Rerendering in the Wild", "comments": "To be presented at CVPR 2019 (oral). Supplementary video available at\n  http://youtu.be/E1crWQn_kmY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore total scene capture -- recording, modeling, and rerendering a\nscene under varying appearance such as season and time of day. Starting from\ninternet photos of a tourist landmark, we apply traditional 3D reconstruction\nto register the photos and approximate the scene as a point cloud. For each\nphoto, we render the scene points into a deep framebuffer, and train a neural\nnetwork to learn the mapping of these initial renderings to the actual photos.\nThis rerendering network also takes as input a latent appearance vector and a\nsemantic mask indicating the location of transient objects like pedestrians.\nThe model is evaluated on several datasets of publicly available images\nspanning a broad range of illumination conditions. We create short videos\ndemonstrating realistic manipulation of the image viewpoint, appearance, and\nsemantic labeling. We also compare results with prior work on scene\nreconstruction from internet photos.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 18:30:14 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Meshry", "Moustafa", ""], ["Goldman", "Dan B", ""], ["Khamis", "Sameh", ""], ["Hoppe", "Hugues", ""], ["Pandey", "Rohit", ""], ["Snavely", "Noah", ""], ["Martin-Brualla", "Ricardo", ""]]}, {"id": "1904.04335", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "End-to-end Projector Photometric Compensation", "comments": "To appear in the 2019 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR). Source code and dataset are available at\n  https://github.com/BingyaoHuang/compennet", "journal-ref": null, "doi": "10.1109/CVPR.2019.00697", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projector photometric compensation aims to modify a projector input image\nsuch that it can compensate for disturbance from the appearance of projection\nsurface. In this paper, for the first time, we formulate the compensation\nproblem as an end-to-end learning problem and propose a convolutional neural\nnetwork, named CompenNet, to implicitly learn the complex compensation\nfunction. CompenNet consists of a UNet-like backbone network and an autoencoder\nsubnet. Such architecture encourages rich multi-level interactions between the\ncamera-captured projection surface image and the input image, and thus captures\nboth photometric and environment information of the projection surface. In\naddition, the visual details and interaction information are carried to deeper\nlayers along the multi-level skip convolution layers. The architecture is of\nparticular importance for the projector compensation task, for which only a\nsmall training dataset is allowed in practice. Another contribution we make is\na novel evaluation benchmark, which is independent of system setup and thus\nquantitatively verifiable. Such benchmark is not previously available, to our\nbest knowledge, due to the fact that conventional evaluation requests the\nhardware system to actually project the final results. Our key idea, motivated\nfrom our end-to-end problem formulation, is to use a reasonable surrogate to\navoid such projection process so as to be setup-independent. Our method is\nevaluated carefully on the benchmark, and the results show that our end-to-end\nlearning solution outperforms state-of-the-arts both qualitatively and\nquantitatively by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 20:07:27 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "1904.04603", "submitter": "Leonardo Fernandez-Jambrina", "authors": "Leonardo Fernandez-Jambrina and Francisco Perez-Arribas", "title": "Developable surface patches bounded by NURBS curves", "comments": "17 pages, 17 figures. Accepted for publication in Journal of\n  Computational Mathematics", "journal-ref": "Journal of Computational Mathematics 38, 693-709 (2020)", "doi": "10.4208/jcm.1904-m2018-0209", "report-no": null, "categories": "cs.GR math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct developable surface patches which are bounded by\ntwo rational or NURBS curves, though the resulting patch is not a rational or\nNURBS surface in general. This is accomplished by reparameterizing one of the\nboundary curves. The reparameterization function is the solution of an\nalgebraic equation. For the relevant case of cubic or cubic spline curves, this\nequation is quartic at most, quadratic if the curves are Bezier or splines and\nlie on parallel planes, and hence it may be solved either by standard\nanalytical or numerical methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 11:34:57 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Fernandez-Jambrina", "Leonardo", ""], ["Perez-Arribas", "Francisco", ""]]}, {"id": "1904.04769", "submitter": "Julian Iseringhausen", "authors": "Julian Iseringhausen and Michael Weinmann and Weizhen Huang and\n  Matthias B. Hullin", "title": "Computational Parquetry: Fabricated Style Transfer with Wood Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parquetry is the art and craft of decorating a surface with a pattern of\ndifferently colored veneers of wood, stone or other materials. Traditionally,\nthe process of designing and making parquetry has been driven by color, using\nthe texture found in real wood only for stylization or as a decorative effect.\nHere, we introduce a computational pipeline that draws from the rich natural\nstructure of strongly textured real-world veneers as a source of detail in\norder to approximate a target image as faithfully as possible using a\nmanageable number of parts. This challenge is closely related to the\nestablished problems of patch-based image synthesis and stylization in some\nways, but fundamentally different in others. Most importantly, the limited\navailability of resources (any piece of wood can only be used once) turns the\nrelatively simple problem of finding the right piece for the target location\ninto the combinatorial problem of finding optimal parts while avoiding resource\ncollisions. We introduce an algorithm that allows to efficiently solve an\napproximation to the problem. It further addresses challenges like gamut\nmapping, feature characterization and the search for fabricable cuts. We\ndemonstrate the effectiveness of the system by fabricating a selection of\n\"photo-realistic\" pieces of parquetry from different kinds of unstained wood\nveneer.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 16:26:19 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 13:01:46 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Iseringhausen", "Julian", ""], ["Weinmann", "Michael", ""], ["Huang", "Weizhen", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1904.04890", "submitter": "Francis Williams", "authors": "Francis Williams, Alexander Bock, Harish Doraiswamy, Cassandra\n  Donatelli, Kayla Hall, Adam Summers, Daniele Panozzo, Cl\\'audio T. Silva", "title": "Unwind: Interactive Fish Straightening", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376846", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ScanAllFish project is a large-scale effort to scan all the world's\n33,100 known species of fishes. It has already generated thousands of\nvolumetric CT scans of fish species which are available on open access\nplatforms such as the Open Science Framework. To achieve a scanning rate\nrequired for a project of this magnitude, many specimens are grouped together\ninto a single tube and scanned all at once. The resulting data contain many\nfish which are often bent and twisted to fit into the scanner. Our system,\nUnwind, is a novel interactive visualization and processing tool which\nextracts, unbends, and untwists volumetric images of fish with minimal user\ninteraction. Our approach enables scientists to interactively unwarp these\nvolumes to remove the undesired torque and bending using a piecewise-linear\nskeleton extracted by averaging isosurfaces of a harmonic function connecting\nthe head and tail of each fish. The result is a volumetric dataset of a\nindividual, straight fish in a canonical pose defined by the marine biologist\nexpert user. We have developed Unwind in collaboration with a team of marine\nbiologists: Our system has been deployed in their labs, and is presently being\nused for dataset construction, biomechanical analysis, and the generation of\nfigures for scientific publication.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:07:24 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:55:13 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Williams", "Francis", ""], ["Bock", "Alexander", ""], ["Doraiswamy", "Harish", ""], ["Donatelli", "Cassandra", ""], ["Hall", "Kayla", ""], ["Summers", "Adam", ""], ["Panozzo", "Daniele", ""], ["Silva", "Cl\u00e1udio T.", ""]]}, {"id": "1904.04953", "submitter": "Issam Damaj", "authors": "Issam Damaj (Dhofar University)", "title": "High Performance Reconfigurable Computing Systems", "comments": "53 pages, 14 tables, 15 figures", "journal-ref": "Supercomp. Res. Adv. (2008) 116-143", "doi": null, "report-no": null, "categories": "cs.AR cs.GR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid progress and advancement in electronic chips technology provide a\nvariety of new implementation options for system engineers. The choice varies\nbetween the flexible programs running on a general-purpose processor (GPP) and\nthe fixed hardware implementation using an application specific integrated\ncircuit (ASIC). Many other implementation options present, for instance, a\nsystem with a RISC processor and a DSP core. Other options include graphics\nprocessors and microcontrollers. Specialist processors certainly improve\nperformance over general-purpose ones, but this comes as a quid pro quo for\nflexibility. Combining the flexibility of GPPs and the high performance of\nASICs leads to the introduction of reconfigurable computing (RC) as a new\nimplementation option with a balance between versatility and speed. The focus\nof this chapter is on introducing reconfigurable computers as modern super\ncomputing architectures. The chapter also investigates the main reasons behind\nthe current advancement in the development of RC-systems. Furthermore, a\ntechnical survey of various RC-systems is included laying common grounds for\ncomparisons. In addition, this chapter mainly presents case studies implemented\nunder the MorphoSys RC-system. The selected case studies belong to different\nareas of application, such as, computer graphics and information coding.\nParallel versions of the studied algorithms are developed to match the\ntopologies supported by the MorphoSys. Performance evaluation and results\nanalyses are included for implementations with different characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:24:46 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Damaj", "Issam", "", "Dhofar University"]]}, {"id": "1904.04954", "submitter": "Chungang Zhu", "authors": "Jing-Gai Li, Chun-Gang Zhu", "title": "Curve and surface construction based on the generalized toric-Bernstein\n  basis functions", "comments": "28 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of parametric curve and surface plays important role in\ncomputer aided geometric design (CAGD), computer aided design (CAD), and\ngeometric modeling. In this paper, we define a new kind of blending functions\nassociated with a real points set, called generalized toric-Bernstein\n(GT-Bernstein) basis functions. Then the generalized toric-Bezier (GT-B\\'ezier)\ncurves and surfaces are constructed based on the GT-Bernstein basis functions,\nwhich are the projections of the (irrational) toric varieties in fact and the\ngeneralizations of the classical rational B\\'ezier curves and surfaces and\ntoric surface patches. Furthermore, we also study the properties of the\npresented curves and surfaces, including the limiting properties of weights and\nknots. Some representative examples verify the properties and results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:28:41 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Li", "Jing-Gai", ""], ["Zhu", "Chun-Gang", ""]]}, {"id": "1904.04998", "submitter": "Ariel Gordon", "authors": "Ariel Gordon, Hanhan Li, Rico Jonschkowski, Anelia Angelova", "title": "Depth from Videos in the Wild: Unsupervised Monocular Depth Learning\n  from Unknown Cameras", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV), 2019,\n  pp. 8977-8986", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for simultaneous learning of depth, egomotion,\nobject motion, and camera intrinsics from monocular videos, using only\nconsistency across neighboring video frames as supervision signal. Similarly to\nprior work, our method learns by applying differentiable warping to frames and\ncomparing the result to adjacent ones, but it provides several improvements: We\naddress occlusions geometrically and differentiably, directly using the depth\nmaps as predicted during training. We introduce randomized layer normalization,\na novel powerful regularizer, and we account for object motion relative to the\nscene. To the best of our knowledge, our work is the first to learn the camera\nintrinsic parameters, including lens distortion, from video in an unsupervised\nmanner, thereby allowing us to extract accurate depth and motion from arbitrary\nvideos of unknown origin at scale. We evaluate our results on the Cityscapes,\nKITTI and EuRoC datasets, establishing new state of the art on depth prediction\nand odometry, and demonstrate qualitatively that depth prediction can be\nlearned from a collection of YouTube videos.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 04:16:30 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Gordon", "Ariel", ""], ["Li", "Hanhan", ""], ["Jonschkowski", "Rico", ""], ["Angelova", "Anelia", ""]]}, {"id": "1904.05124", "submitter": "Phong Nguyen-Ha", "authors": "Phong Nguyen-Ha, Lam Huynh, Esa Rahtu, Janne Heikkila", "title": "Predicting Novel Views Using Generative Adversarial Query Network", "comments": "12 pages, 4 figures, accepted for presentation at the Scandinavian\n  Conference on Image Analysis 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of predicting a novel view of the scene using an arbitrary number\nof observations is a challenging problem for computers as well as for humans.\nThis paper introduces the Generative Adversarial Query Network (GAQN), a\ngeneral learning framework for novel view synthesis that combines Generative\nQuery Network (GQN) and Generative Adversarial Networks (GANs). The\nconventional GQN encodes input views into a latent representation that is used\nto generate a new view through a recurrent variational decoder. The proposed\nGAQN builds on this work by adding two novel aspects: First, we extend the\ncurrent GQN architecture with an adversarial loss function for improving the\nvisual quality and convergence speed. Second, we introduce a feature-matching\nloss function for stabilizing the training procedure. The experiments\ndemonstrate that GAQN is able to produce high-quality results and faster\nconvergence compared to the conventional approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 11:49:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nguyen-Ha", "Phong", ""], ["Huynh", "Lam", ""], ["Rahtu", "Esa", ""], ["Heikkila", "Janne", ""]]}, {"id": "1904.05373", "submitter": "Hang Su", "authors": "Hang Su, Varun Jampani, Deqing Sun, Orazio Gallo, Erik Learned-Miller,\n  and Jan Kautz", "title": "Pixel-Adaptive Convolutional Neural Networks", "comments": "CVPR 2019. Video introduction: https://youtu.be/gsQZbHuR64o", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutions are the fundamental building block of CNNs. The fact that their\nweights are spatially shared is one of the main reasons for their widespread\nuse, but it also is a major limitation, as it makes convolutions content\nagnostic. We propose a pixel-adaptive convolution (PAC) operation, a simple yet\neffective modification of standard convolutions, in which the filter weights\nare multiplied with a spatially-varying kernel that depends on learnable, local\npixel features. PAC is a generalization of several popular filtering techniques\nand thus can be used for a wide range of use cases. Specifically, we\ndemonstrate state-of-the-art performance when PAC is used for deep joint image\nupsampling. PAC also offers an effective alternative to fully-connected CRF\n(Full-CRF), called PAC-CRF, which performs competitively, while being\nconsiderably faster. In addition, we also demonstrate that PAC can be used as a\ndrop-in replacement for convolution layers in pre-trained networks, resulting\nin consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:02:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Su", "Hang", ""], ["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Gallo", "Orazio", ""], ["Learned-Miller", "Erik", ""], ["Kautz", "Jan", ""]]}, {"id": "1904.05440", "submitter": "Ashutosh Modi", "authors": "Yeyao Zhang, Eleftheria Tsipidi, Sasha Schriber, Mubbasir Kapadia,\n  Markus Gross, Ashutosh Modi", "title": "Generating Animations from Screenplays", "comments": "9+1+6 Pages, Accepted at StarSEM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.GR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating animation from natural language text finds\napplication in a number of areas e.g. movie script writing, instructional\nvideos, and public safety. However, translating natural language text into\nanimation is a challenging task. Existing text-to-animation systems can handle\nonly very simple sentences, which limits their applications. In this paper, we\ndevelop a text-to-animation system which is capable of handling complex\nsentences. We achieve this by introducing a text simplification step into the\nprocess. Building on an existing animation generation system for screenwriting,\nwe create a robust NLP pipeline to extract information from screenplays and map\nthem to the system's knowledge base. We develop a set of linguistic\ntransformation rules that simplify complex sentences. Information extracted\nfrom the simplified sentences is used to generate a rough storyboard and video\ndepicting the text. Our sentence simplification module outperforms existing\nsystems in terms of BLEU and SARI metrics.We further evaluated our system via a\nuser study: 68 % participants believe that our system generates reasonable\nanimation from input screenplays.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:04:54 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Zhang", "Yeyao", ""], ["Tsipidi", "Eleftheria", ""], ["Schriber", "Sasha", ""], ["Kapadia", "Mubbasir", ""], ["Gross", "Markus", ""], ["Modi", "Ashutosh", ""]]}, {"id": "1904.05448", "submitter": "Soraia Musse", "authors": "Cliceres dal Bianco, Soraia Raupp Musse", "title": "Predicting Future Pedestrian Motion in Video Sequences using Crowd\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While human and group analysis have become an important area in last decades,\nsome current and relevant applications involve to estimate future motion of\npedestrians in real video sequences. This paper presents a method to provide\nmotion estimation of real pedestrians in next seconds, using crowd simulation.\nOur method is based on Physics and heuristics and use BioCrowds as crowd\nsimulation methodology to estimate future positions of people in video\nsequences. Results show that our method for estimation works well even for\ncomplex videos where events can happen. The maximum achieved average error is\n$2.72$cm when estimating the future motion of 32 pedestrians with more than 2\nseconds in advance. This paper discusses this and other results.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 21:25:24 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Bianco", "Cliceres dal", ""], ["Musse", "Soraia Raupp", ""]]}, {"id": "1904.05537", "submitter": "Leonid Keselman", "authors": "Leonid Keselman, Martial Hebert", "title": "Direct Fitting of Gaussian Mixture Models", "comments": "Accepted to the Conference on Computer and Robot Vision 2019. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When fitting Gaussian Mixture Models to 3D geometry, the model is typically\nfit to point clouds, even when the shapes were obtained as 3D meshes. Here we\npresent a formulation for fitting Gaussian Mixture Models (GMMs) directly to a\ntriangular mesh instead of using points sampled from its surface. Part of this\nwork analyzes a general formulation for evaluating likelihood of geometric\nobjects. This modification enables fitting higher-quality GMMs under a wider\nrange of initialization conditions. Additionally, models obtained from this\nfitting method are shown to produce an improvement in 3D registration for both\nmeshes and RGB-D frames. This result is general and applicable to arbitrary\ngeometric objects, including representing uncertainty from sensor measurements.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 05:19:52 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 02:34:47 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Keselman", "Leonid", ""], ["Hebert", "Martial", ""]]}, {"id": "1904.05814", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Umut \\c{S}im\\c{s}ekli", "title": "Probabilistic Permutation Synchronization using the Riemannian Structure\n  of the Birkhoff Polytope", "comments": "To appear as oral presentation at CVPR 2019. 20 pages including the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.NA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an entirely new geometric and probabilistic approach to\nsynchronization of correspondences across multiple sets of objects or images.\nIn particular, we present two algorithms: (1) Birkhoff-Riemannian L-BFGS for\noptimizing the relaxed version of the combinatorially intractable cycle\nconsistency loss in a principled manner, (2) Birkhoff-Riemannian Langevin Monte\nCarlo for generating samples on the Birkhoff Polytope and estimating the\nconfidence of the found solutions. To this end, we first introduce the very\nrecently developed Riemannian geometry of the Birkhoff Polytope. Next, we\nintroduce a new probabilistic synchronization model in the form of a Markov\nRandom Field (MRF). Finally, based on the first order retraction operators, we\nformulate our problem as simulating a stochastic differential equation and\ndevise new integrators. We show on both synthetic and real datasets that we\nachieve high quality multi-graph matching results with faster convergence and\nreliable confidence/uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:12:50 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Birdal", "Tolga", ""], ["\u015eim\u015fekli", "Umut", ""]]}, {"id": "1904.06447", "submitter": "Kyle Genova", "authors": "Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T.\n  Freeman, Thomas Funkhouser", "title": "Learning Shape Templates with Structured Implicit Functions", "comments": "12 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template 3D shapes are useful for many tasks in graphics and vision,\nincluding fitting observation data, analyzing shape collections, and\ntransferring shape attributes. Because of the variety of geometry and topology\nof real-world shapes, previous methods generally use a library of hand-made\ntemplates. In this paper, we investigate learning a general shape template from\ndata. To allow for widely varying geometry and topology, we choose an implicit\nsurface representation based on composition of local shape elements. While long\nknown to computer graphics, this representation has not yet been explored in\nthe context of machine learning for vision. We show that structured implicit\nfunctions are suitable for learning and allow a network to smoothly and\nsimultaneously fit multiple classes of shapes. The learned shape template\nsupports applications such as shape exploration, correspondence, abstraction,\ninterpolation, and semantic segmentation from an RGB image.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 23:15:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Genova", "Kyle", ""], ["Cole", "Forrester", ""], ["Vlasic", "Daniel", ""], ["Sarna", "Aaron", ""], ["Freeman", "William T.", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1904.06903", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Muchen Li, Wenxiu Sun", "title": "Learning Deformable Kernels for Image and Video Denoising", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the classical denoising methods restore clear results by selecting\nand averaging pixels in the noisy input. Instead of relying on hand-crafted\nselecting and averaging strategies, we propose to explicitly learn this process\nwith deep neural networks. Specifically, we propose deformable 2D kernels for\nimage denoising where the sampling locations and kernel weights are both\nlearned. The proposed kernel naturally adapts to image structures and could\neffectively reduce the oversmoothing artifacts. Furthermore, we develop 3D\ndeformable kernels for video denoising to more efficiently sample pixels across\nthe spatial-temporal space. Our method is able to solve the misalignment issues\nof large motion from dynamic scenes. For better training our video denoising\nmodel, we introduce the trilinear sampler and a new regularization term. We\ndemonstrate that the proposed method performs favorably against the\nstate-of-the-art image and video denoising approaches on both synthetic and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 08:15:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Xu", "Xiangyu", ""], ["Li", "Muchen", ""], ["Sun", "Wenxiu", ""]]}, {"id": "1904.07601", "submitter": "Yongcheng Liu", "authors": "Yongcheng Liu, Bin Fan, Shiming Xiang and Chunhong Pan", "title": "Relation-Shape Convolutional Neural Network for Point Cloud Analysis", "comments": "Accepted to CVPR 2019 as an oral presentation. Project page at\n  https://yochengliu.github.io/Relation-Shape-CNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is very challenging, as the shape implied in irregular\npoints is difficult to capture. In this paper, we propose RS-CNN, namely,\nRelation-Shape Convolutional Neural Network, which extends regular grid CNN to\nirregular configuration for point cloud analysis. The key to RS-CNN is learning\nfrom relation, i.e., the geometric topology constraint among points.\nSpecifically, the convolutional weight for local point set is forced to learn a\nhigh-level relation expression from predefined geometric priors, between a\nsampled point from this point set and the others. In this way, an inductive\nlocal representation with explicit reasoning about the spatial layout of points\ncan be obtained, which leads to much shape awareness and robustness. With this\nconvolution as a basic operator, RS-CNN, a hierarchical architecture can be\ndeveloped to achieve contextual shape-aware learning for point cloud analysis.\nExtensive experiments on challenging benchmarks across three tasks verify\nRS-CNN achieves the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 11:28:51 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 06:56:46 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 03:55:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1904.07615", "submitter": "Pedro Hermosilla Casajus", "authors": "Pedro Hermosilla, Tobias Ritschel, Timo Ropinski", "title": "Total Denoising: Unsupervised Learning of 3D Point Cloud Cleaning", "comments": "Proceedings of ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that denoising of 3D point clouds can be learned unsupervised,\ndirectly from noisy 3D point cloud data only. This is achieved by extending\nrecent ideas from learning of unsupervised image denoisers to unstructured 3D\npoint clouds. Unsupervised image denoisers operate under the assumption that a\nnoisy pixel observation is a random realization of a distribution around a\nclean pixel value, which allows appropriate learning on this distribution to\neventually converge to the correct value. Regrettably, this assumption is not\nvalid for unstructured points: 3D point clouds are subject to total noise, i.\ne., deviations in all coordinates, with no reliable pixel grid. Thus, an\nobservation can be the realization of an entire manifold of clean 3D points,\nwhich makes a na\\\"ive extension of unsupervised image denoisers to 3D point\nclouds impractical. Overcoming this, we introduce a spatial prior term, that\nsteers converges to the unique closest out of the many possible modes on a\nmanifold. Our results demonstrate unsupervised denoising performance similar to\nthat of supervised learning with clean data when given enough training examples\n- whereby we do not need any pairs of noisy and clean training data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 12:11:26 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 22:53:52 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Hermosilla", "Pedro", ""], ["Ritschel", "Tobias", ""], ["Ropinski", "Timo", ""]]}, {"id": "1904.07865", "submitter": "Jing Ren", "authors": "Simone Melzi, Jing Ren, Emanuele Rodol\\`a, Abhishek Sharma, Peter\n  Wonka, and Maks Ovsjanikov", "title": "ZoomOut: Spectral Upsampling for Efficient Shape Correspondence", "comments": "14 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and efficient method for refining maps or correspondences\nby iterative upsampling in the spectral domain that can be implemented in a few\nlines of code. Our main observation is that high quality maps can be obtained\neven if the input correspondences are noisy or are encoded by a small number of\ncoefficients in a spectral basis. We show how this approach can be used in\nconjunction with existing initialization techniques across a range of\napplication scenarios, including symmetry detection, map refinement across\ncomplete shapes, non-rigid partial shape matching and function transfer. In\neach application we demonstrate an improvement with respect to both the quality\nof the results and the computational speed compared to the best competing\nmethods, with up to two orders of magnitude speed-up in some applications. We\nalso demonstrate that our method is both robust to noisy input and is scalable\nwith respect to shape complexity. Finally, we present a theoretical\njustification for our approach, shedding light on structural properties of\nfunctional maps.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 17:04:13 GMT"}, {"version": "v2", "created": "Sun, 21 Apr 2019 07:47:12 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 13:12:15 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 13:15:05 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Melzi", "Simone", ""], ["Ren", "Jing", ""], ["Rodol\u00e0", "Emanuele", ""], ["Sharma", "Abhishek", ""], ["Wonka", "Peter", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1904.08225", "submitter": "Sascha Brandt", "authors": "Sascha Brandt, Claudius J\\\"ahn, Matthias Fischer, Friedhelm Meyer auf\n  der Heide", "title": "Rendering of Complex Heterogenous Scenes using Progressive Blue Surfels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for rendering highly complex 3D scenes in real-time by\ngenerating uniformly distributed points on the scene's visible surfaces. The\ntechnique is applicable to a wide range of scene types, like scenes directly\nbased on complex and detailed CAD data consisting of billions of polygons (in\ncontrast to scenes handcrafted solely for visualization). This allows to\nvisualize such scenes smoothly even in VR on a HMD with good image quality,\nwhile maintaining the necessary frame-rates. In contrast to other point based\nrendering methods, we place points in an approximated blue noise distribution\nonly on visible surfaces and store them in a highly GPU efficient data\nstructure, allowing to progressively refine the number of rendered points to\nmaximize the image quality for a given target frame rate. Our evaluation shows\nthat scenes consisting of a high amount of polygons can be rendered with\ninteractive frame rates with good visual quality on standard hardware.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 12:31:58 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Brandt", "Sascha", ""], ["J\u00e4hn", "Claudius", ""], ["Fischer", "Matthias", ""], ["der Heide", "Friedhelm Meyer auf", ""]]}, {"id": "1904.08379", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf, Yaniv Taigman", "title": "Vid2Game: Controllable Characters Extracted from Real-World Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are given a video of a person performing a certain activity, from which we\nextract a controllable model. The model generates novel image sequences of that\nperson, according to arbitrary user-defined control signals, typically marking\nthe displacement of the moving body. The generated video can have an arbitrary\nbackground, and effectively capture both the dynamics and appearance of the\nperson.\n  The method is based on two networks. The first network maps a current pose,\nand a single-instance control signal to the next pose. The second network maps\nthe current pose, the new pose, and a given background, to an output frame.\nBoth networks include multiple novelties that enable high-quality performance.\nThis is demonstrated on multiple characters extracted from various videos of\ndancers and athletes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:26:14 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1904.08921", "submitter": "Dmitriy Smirnov", "authors": "Dmitriy Smirnov, Matthew Fisher, Vladimir G. Kim, Richard Zhang,\n  Justin Solomon", "title": "Deep Parametric Shape Predictions using Distance Fields", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tasks in graphics and vision demand machinery for converting shapes into\nconsistent representations with sparse sets of parameters; these\nrepresentations facilitate rendering, editing, and storage. When the source\ndata is noisy or ambiguous, however, artists and engineers often manually\nconstruct such representations, a tedious and potentially time-consuming\nprocess. While advances in deep learning have been successfully applied to\nnoisy geometric data, the task of generating parametric shapes has so far been\ndifficult for these methods. Hence, we propose a new framework for predicting\nparametric shape primitives using deep learning. We use distance fields to\ntransition between shape parameters like control points and input data on a\npixel grid. We demonstrate efficacy on 2D and 3D tasks, including font\nvectorization and surface abstraction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 17:55:57 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 15:07:10 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Smirnov", "Dmitriy", ""], ["Fisher", "Matthew", ""], ["Kim", "Vladimir G.", ""], ["Zhang", "Richard", ""], ["Solomon", "Justin", ""]]}, {"id": "1904.09530", "submitter": "Kevin Karsch", "authors": "Kevin Karsch, John C. Hart", "title": "Snaxels on a Plane", "comments": null, "journal-ref": null, "doi": "10.1145/2024676.2024683", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many algorithms exist for tracing various contours for illustrating a\nmeshed object, few algorithms organize these contours into region-bounding\nclosed loops. Tracing closed-loop boundaries on a mesh can be problematic due\nto switchbacks caused by subtle surface variation, and the organization of\nthese regions into a planar map can lead to many small region components due to\nimprecision and noise. This paper adapts \"snaxels,\" an energy minimizing active\ncontour method designed for robust mesh processing, and repurposes it to\ngenerate visual, shadow and shading contours, and a simplified visual-surface\nplanar map, useful for stylized vector art illustration of the mesh. The snaxel\nactive contours can also track contours as the mesh animates, and\nframe-to-frame correspondences between snaxels lead to a new method to convert\nthe moving contours on a 3-D animated mesh into 2-D SVG curve animations for\nefficient embedding in Flash, PowerPoint and other dynamic vector art\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:36:36 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Karsch", "Kevin", ""], ["Hart", "John C.", ""]]}, {"id": "1904.10213", "submitter": "Marco Livesu", "authors": "Chrystiano Ara\\'ujo, Daniela Cabiddu, Marco Attene, Marco Livesu,\n  Nicholas Vining, Alla Sheffer", "title": "Surface2Volume: Surface Segmentation Conforming Assemblable Volumetric\n  Partition", "comments": "Accepted at SIGGRAPH 2019. Chrystiano Ara\\'ujo and Daniela Cabiddu\n  are joint first authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users frequently seek to fabricate objects whose outer surfaces consist of\nregions with different surface attributes, such as color or material.\nManufacturing such objects in a single piece is often challenging or even\nimpossible. The alternative is to partition them into single-attribute\nvolumetric parts that can be fabricated separately and then assembled to form\nthe target object. Facilitating this approach requires partitioning the input\nmodel into parts that conform to the surface segmentation and that can be moved\napart with no collisions. We propose Surface2Volume, a partition algorithm\ncapable of producing such assemblable parts, each of which is affiliated with a\nsingle attribute, the outer surface of whose assembly conforms to the input\nsurface geometry and segmentation. In computing the partition we strictly\nenforce conformity with surface segmentation and assemblability, and optimize\nfor ease of fabrication by minimizing part count, promoting part simplicity,\nand simplifying assembly sequencing. We note that computing the desired\npartition requires solving for three types of variables: per-part assembly\ntrajectories, partition topology, i.e. the connectivity of the interface\nsurfaces separating the different parts, and the geometry, or location, of\nthese interfaces. We efficiently produce the desired partitions by addressing\none type of variables at a time: first computing the assembly trajectories,\nthen determining interface topology, and finally computing interface locations\nthat allow parts assemblability. We algorithmically identify inputs that\nnecessitate sequential assembly, and partition these inputs gradually by\ncomputing and disassembling a subset of assemblable parts at a time. We\ndemonstrate our method....\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 09:08:28 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Ara\u00fajo", "Chrystiano", ""], ["Cabiddu", "Daniela", ""], ["Attene", "Marco", ""], ["Livesu", "Marco", ""], ["Vining", "Nicholas", ""], ["Sheffer", "Alla", ""]]}, {"id": "1904.10379", "submitter": "Eran Treister", "authors": "Moshe Eliasof, Andrei Sharf, Eran Treister", "title": "Multi-modal 3D Shape Reconstruction Under Calibration Uncertainty using\n  Parametric Level Set Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of 3D shape reconstruction from multi-modal data,\ngiven uncertain calibration parameters. Typically, 3D data modalities can be in\ndiverse forms such as sparse point sets, volumetric slices, 2D photos and so\non. To jointly process these data modalities, we exploit a parametric level set\nmethod that utilizes ellipsoidal radial basis functions. This method not only\nallows us to analytically and compactly represent the object, it also confers\non us the ability to overcome calibration related noise that originates from\ninaccurate acquisition parameters. This essentially implicit regularization\nleads to a highly robust and scalable reconstruction, surpassing other\ntraditional methods. In our results we first demonstrate the ability of the\nmethod to compactly represent complex objects. We then show that our\nreconstruction method is robust both to a small number of measurements and to\nnoise in the acquisition parameters. Finally, we demonstrate our reconstruction\nabilities from diverse modalities such as volume slices obtained from liquid\ndisplacement (similar to CTscans and XRays), and visual measurements obtained\nfrom shape silhouettes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 15:19:39 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 12:52:17 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Eliasof", "Moshe", ""], ["Sharf", "Andrei", ""], ["Treister", "Eran", ""]]}, {"id": "1904.10754", "submitter": "Ruqi Huang", "authors": "Ruqi Huang and Marie-Julie Rakotosaona and Panos Achlioptas and\n  Leonidas Guibas and Maks Ovsjanikov", "title": "OperatorNet: Recovering 3D Shapes From Difference Operators", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning-based framework for reconstructing 3D shapes\nfrom functional operators, compactly encoded as small-sized matrices. To this\nend we introduce a novel neural architecture, called OperatorNet, which takes\nas input a set of linear operators representing a shape and produces its 3D\nembedding. We demonstrate that this approach significantly outperforms previous\npurely geometric methods for the same problem. Furthermore, we introduce a\nnovel functional operator, which encodes the extrinsic or pose-dependent shape\ninformation, and thus complements purely intrinsic pose-oblivious operators,\nsuch as the classical Laplacian. Coupled with this novel operator, our\nreconstruction network achieves very high reconstruction accuracy, even in the\npresence of incomplete information about a shape, given a soft or functional\nmap expressed in a reduced basis. Finally, we demonstrate that the\nmultiplicative functional algebra enjoyed by these operators can be used to\nsynthesize entirely new unseen shapes, in the context of shape interpolation\nand shape analogy applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 11:47:08 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 16:27:34 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Huang", "Ruqi", ""], ["Rakotosaona", "Marie-Julie", ""], ["Achlioptas", "Panos", ""], ["Guibas", "Leonidas", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1904.10795", "submitter": "Zeqing Fu", "authors": "Zeqing Fu, Wei Hu, Zongming Guo", "title": "3D Dynamic Point Cloud Inpainting via Temporal Consistency on Graphs", "comments": "7 pages, 5 figures, accepted by IEEE ICME 2020 at 2020.04.03. arXiv\n  admin note: text overlap with arXiv:1810.03973", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of 3D laser scanning techniques and depth sensors, 3D\ndynamic point clouds have attracted increasing attention as a representation of\n3D objects in motion, enabling various applications such as 3D immersive\ntele-presence, gaming and navigation. However, dynamic point clouds usually\nexhibit holes of missing data, mainly due to the fast motion, the limitation of\nacquisition and complicated structure. Leveraging on graph signal processing\ntools, we represent irregular point clouds on graphs and propose a novel\ninpainting method exploiting both intra-frame self-similarity and inter-frame\nconsistency in 3D dynamic point clouds. Specifically, for each missing region\nin every frame of the point cloud sequence, we search for its self-similar\nregions in the current frame and corresponding ones in adjacent frames as\nreferences. Then we formulate dynamic point cloud inpainting as an optimization\nproblem based on the two types of references, which is regularized by a\ngraph-signal smoothness prior. Experimental results show the proposed approach\noutperforms three competing methods significantly, both in objective and\nsubjective quality.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 08:18:56 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 15:01:51 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Fu", "Zeqing", ""], ["Hu", "Wei", ""], ["Guo", "Zongming", ""]]}, {"id": "1904.11084", "submitter": "Rodolfo Migon Favaretto", "authors": "Victor Araujo and Rodolfo Migon Favaretto and Paulo Knob and Soraia\n  Raupp Musse and Felipe Vilanova and Angelo Brandelli Costa", "title": "How much do you perceive this? An analysis on perceptions of geometric\n  features, personalities and emotions in virtual humans (Extended Version)", "comments": "Extended Version of a paper published at IVA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to evaluate people's perception regarding geometric features,\npersonalities and emotions characteristics in virtual humans. For this, we use\nas a basis, a dataset containing the tracking files of pedestrians captured\nfrom spontaneous videos and visualized them as identical virtual humans. The\ngoal is to focus on their behavior and not being distracted by other features.\nIn addition to tracking files containing their positions, the dataset also\ncontains pedestrian emotions and personalities detected using Computer Vision\nand Pattern Recognition techniques. We proceed with our analysis in order to\nanswer the question if subjects can perceive geometric features as\ndistances/speeds as well as emotions and personalities in video sequences when\npedestrians are represented by virtual humans. Regarding the participants, an\namount of 73 people volunteered for the experiment. The analysis was divided in\ntwo parts: i) evaluation on perception of geometric characteristics, such as\ndensity, angular variation, distances and speeds, and ii) evaluation on\npersonality and emotion perceptions. Results indicate that, even without\nexplaining to the participants the concepts of each personality or emotion and\nhow they were calculated (considering geometric characteristics), in most of\nthe cases, participants perceived the personality and emotion expressed by the\nvirtual agents, in accordance with the available ground truth.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2019 21:50:34 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Araujo", "Victor", ""], ["Favaretto", "Rodolfo Migon", ""], ["Knob", "Paulo", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo Brandelli", ""]]}, {"id": "1904.11116", "submitter": "Zahra Montazeri", "authors": "Zahra Montazeri, Chang Xiao, Yun (Raymond) Fei, Changxi Zheng, and\n  Shuang Zhao", "title": "Mechanics-Aware Modeling of Cloth Appearance", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2019", "doi": "10.1109/TVCG.2019.2937301", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-appearance models have brought unprecedented fidelity and details to\ncloth rendering.\n  Yet, these models neglect fabric mechanics: when a piece of cloth interacts\nwith the environment, its yarn and fiber arrangement usually changes in\nresponse to external contact and tension forces.\n  Since subtle changes of a fabric's microstructures can greatly affect its\nmacroscopic appearance, mechanics-driven appearance variation of fabrics has\nbeen a phenomenon that remains to be captured.\n  We introduce a mechanics-aware model that adapts the microstructures of cloth\nyarns in a physics-based manner.\n  Our technique works on two distinct physical scales: using physics-based\nsimulations of individual yarns, we capture the rearrangement of yarn-level\nstructures in response to external forces.\n  These yarn structures are further enriched to obtain appearance-driving\nfiber-level details.\n  The cross-scale enrichment is made practical through a new parameter fitting\nalgorithm for simulation, an augmented procedural yarn model coupled with a\ncustom-design regression neural network.\n  We train the network using a dataset generated by joint simulations at both\nthe yarn and the fiber levels.\n  Through several examples, we demonstrate that our model is capable of\nsynthesizing photorealistic cloth appearance in a %dynamic and mechanically\nplausible way.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 01:29:50 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 01:35:47 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 14:33:51 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Montazeri", "Zahra", "", "Raymond"], ["Xiao", "Chang", "", "Raymond"], ["Yun", "", "", "Raymond"], ["Fei", "", ""], ["Zheng", "Changxi", ""], ["Zhao", "Shuang", ""]]}, {"id": "1904.11433", "submitter": "Evan Drumwright", "authors": "Ryan Elandt, Evan Drumwright, Michael Sherman, and Andy Ruina", "title": "A pressure field model for fast, robust approximation of net contact\n  force and moment between nominally rigid objects", "comments": "(revised in accordance with the IROS camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approximate model for predicting the net contact wrench\nbetween nominally rigid objects for use in simulation, control, and state\nestimation. The model combines and generalizes two ideas: a bed of springs (an\n\"elastic foundation\") and hydrostatic pressure. In this model, continuous\npressure fields are computed offline for the interior of each nominally rigid\nobject. Unlike hydrostatics or elastic foundations, the pressure fields need\nnot satisfy mechanical equilibrium conditions. When two objects nominally\noverlap, a contact surface is defined where the two pressure fields are equal.\nThis static pressure is supplemented with a dissipative rate-dependent pressure\nand friction to determine tractions on the contact surface. The contact wrench\nbetween pairs of objects is an integral of traction contributions over this\nsurface. The model evaluates much faster than elasticity-theory models, while\nshowing the essential trends of force, moment, and stiffness increase with\ncontact load. It yields continuous wrenches even for non-convex objects and\ncoarse meshes. The method shows promise as sufficiently fast, accurate, and\nrobust for design-in-simulation of robot controllers.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 16:14:52 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 20:48:39 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Elandt", "Ryan", ""], ["Drumwright", "Evan", ""], ["Sherman", "Michael", ""], ["Ruina", "Andy", ""]]}, {"id": "1904.11621", "submitter": "Amlan Kar", "authors": "Amlan Kar, Aayush Prakash, Ming-Yu Liu, Eric Cameracci, Justin Yuan,\n  Matt Rusiniak, David Acuna, Antonio Torralba, Sanja Fidler", "title": "Meta-Sim: Learning to Generate Synthetic Datasets", "comments": "Webpage: https://nv-tlabs.github.io/meta-sim/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training models to high-end performance requires availability of large\nlabeled datasets, which are expensive to get. The goal of our work is to\nautomatically synthesize labeled datasets that are relevant for a downstream\ntask. We propose Meta-Sim, which learns a generative model of synthetic scenes,\nand obtain images as well as its corresponding ground-truth via a graphics\nengine. We parametrize our dataset generator with a neural network, which\nlearns to modify attributes of scene graphs obtained from probabilistic scene\ngrammars, so as to minimize the distribution gap between its rendered outputs\nand target data. If the real dataset comes with a small labeled validation set,\nwe additionally aim to optimize a meta-objective, i.e. downstream task\nperformance. Experiments show that the proposed method can greatly improve\ncontent generation quality over a human-engineered probabilistic scene grammar,\nboth qualitatively and quantitatively as measured by performance on a\ndownstream task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 23:18:36 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kar", "Amlan", ""], ["Prakash", "Aayush", ""], ["Liu", "Ming-Yu", ""], ["Cameracci", "Eric", ""], ["Yuan", "Justin", ""], ["Rusiniak", "Matt", ""], ["Acuna", "David", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "1904.11704", "submitter": "Philippe Lepinard", "authors": "Philippe L\\'epinard (IRG), S\\'ebastien Loz\\'e", "title": "XR: Enabling training mode in the human brain XR: Enabling training mode\n  in the human brain", "comments": null, "journal-ref": "MODSIM World 2019, Apr 2019, Norfolk, United States", "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The face of simulation-based training has greatly evolved, with the most\nrecent tools giving the ability to create virtual environments that rival\nrealism. At first glance, it might appear that what the training sector needs\nis the most realistic simulators possible, but traditional simulators are not\nnecessarily the most efficient or practical training tools. With all that these\nnew technologies have to offer; the challenge is to go back to the core of\ntraining needs and identify the right vector of sensory cues that will most\neffectively enable training mode in the human brain. Bigger and Pricier doesn't\nnecessarily mean better. Simulation with cross-reality content (XR), which by\ndefinition encompasses virtual reality (VR), mixed reality (MR), and augmented\nreality (AR), is the most practical solution for deploying any kind of\nsimulation-based training. The authors of this paper (a teacher and a\ntechnology expert) share their experiences and expose XR-specific best\npractices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :\nStarting his career in the modeling and simulation community more than 15 years\nago, S{\\'e}bastien has focused on learning about the latest simulation\ninnovations and sharing information on how experts have solved their\nchallenges. He worked on the COTS integration at CAE and the Presagis focusing\non Simulation and Visualization products. More recently, Sebastien put together\nsimulation and training teams and strategies for emerging companies like CM\nLabs and D-BOX. He is now the Simulations Industry Manager at Epic Games,\nfocusing on helping companies develop real-time solutions for simulation-based\ntraining. Philippe Lepinard: Former military helicopter pilot and simulation\nofficer, Philippe L{\\'e}pinard is now an associate professor at the University\nof Paris-Est Cr{\\'e}teil (UPEC). His research is focusing on playful learning\nand training through simulation. He is one of the founding members of the\nFrench simulation association.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 07:55:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["L\u00e9pinard", "Philippe", "", "IRG"], ["Loz\u00e9", "S\u00e9bastien", ""]]}, {"id": "1904.12117", "submitter": "Morad Behandish", "authors": "Saigopal Nelaturi, Morad Behandish, Amir M. Mirzendehdel, Johan de\n  Kleer", "title": "Automatic Support Removal for Additive Manufacturing Post Processing", "comments": "Special Issue on symposium on Solid and Physical Modeling (SPM'2019)", "journal-ref": "Journal of Computer-Aided Design, 2019", "doi": "10.1016/j.cad.2019.05.030", "report-no": null, "categories": "cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An additive manufacturing (AM) process often produces a {\\it near-net} shape\nthat closely conforms to the intended design to be manufactured. It sometimes\ncontains additional support structure (also called scaffolding), which has to\nbe removed in post-processing. We describe an approach to automatically\ngenerate process plans for support removal using a multi-axis machining\ninstrument. The goal is to fracture the contact regions between each support\ncomponent and the part, and to do it in the most cost-effective order while\navoiding collisions with evolving near-net shape, including the remaining\nsupport components. A recursive algorithm identifies a maximal collection of\nsupport components whose connection regions to the part are accessible as well\nas the orientations at which they can be removed at a given round. For every\nsuch region, the accessible orientations appear as a 'fiber' in the\ncollision-free space of the evolving near-net shape and the tool assembly. To\norder the removal of accessible supports, the algorithm constructs a search\ngraph whose edges are weighted by the Riemannian distance between the fibers.\nThe least expensive process plan is obtained by solving a traveling salesman\nproblem (TSP) over the search graph. The sequence of configurations obtained by\nsolving TSP is used as the input to a motion planner that finds collision free\npaths to visit all accessible features. The resulting part without the support\nstructure can then be finished using traditional machining to produce the\nintended design. The effectiveness of the method is demonstrated through\nbenchmark examples in 3D.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 06:45:42 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 03:33:32 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 22:37:55 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Nelaturi", "Saigopal", ""], ["Behandish", "Morad", ""], ["Mirzendehdel", "Amir M.", ""], ["de Kleer", "Johan", ""]]}, {"id": "1904.12225", "submitter": "Oh-Hyun Kwon", "authors": "Oh-Hyun Kwon and Kwan-Liu Ma", "title": "A Deep Generative Model for Graph Layout", "comments": "To appear in IEEE Transactions on Visualization and Computer\n  Graphics. In Proc. IEEE VIS 2019 (InfoVis)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934396", "report-no": null, "categories": "cs.SI cs.GR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different layouts can characterize different aspects of the same graph.\nFinding a \"good\" layout of a graph is thus an important task for graph\nvisualization. In practice, users often visualize a graph in multiple layouts\nby using different methods and varying parameter settings until they find a\nlayout that best suits the purpose of the visualization. However, this\ntrial-and-error process is often haphazard and time-consuming. To provide users\nwith an intuitive way to navigate the layout design space, we present a\ntechnique to systematically visualize a graph in diverse layouts using deep\ngenerative models. We design an encoder-decoder architecture to learn a model\nfrom a collection of example layouts, where the encoder represents training\nexamples in a latent space and the decoder produces layouts from the latent\nspace. In particular, we train the model to construct a two-dimensional latent\nspace for users to easily explore and generate various layouts. We demonstrate\nour approach through quantitative and qualitative evaluations of the generated\nlayouts. The results of our evaluations show that our model is capable of\nlearning and generalizing abstract concepts of graph layouts, not just\nmemorizing the training examples. In summary, this paper presents a\nfundamentally new approach to graph visualization where a machine learning\nmodel learns to visualize a graph from examples without manually-defined\nheuristics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 23:19:49 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 23:44:11 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 21:45:55 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 02:57:51 GMT"}, {"version": "v5", "created": "Thu, 29 Aug 2019 00:33:59 GMT"}, {"version": "v6", "created": "Mon, 2 Sep 2019 07:04:25 GMT"}, {"version": "v7", "created": "Tue, 15 Oct 2019 17:22:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kwon", "Oh-Hyun", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1904.12228", "submitter": "Tzu-Mao Li", "authors": "Tzu-Mao Li", "title": "Differentiable Visual Computing", "comments": "PhD Dissertation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives of computer graphics, image processing, and deep learning\nalgorithms have tremendous use in guiding parameter space searches, or solving\ninverse problems. As the algorithms become more sophisticated, we no longer\nonly need to differentiate simple mathematical functions, but have to deal with\ngeneral programs which encode complex transformations of data. This\ndissertation introduces three tools for addressing the challenges that arise\nwhen obtaining and applying the derivatives for complex graphics algorithms.\n  Traditionally, practitioners have been constrained to composing programs with\na limited set of operators, or hand-deriving derivatives. We extend the image\nprocessing language Halide with reverse-mode automatic differentiation, and the\nability to automatically optimize the gradient computations. This enables\nautomatic generation of the gradients of arbitrary Halide programs, at high\nperformance, with little programmer effort.\n  In 3D rendering, the gradient is required with respect to variables such as\ncamera parameters, geometry, and appearance. However, computing the gradient is\nchallenging because the rendering integral includes visibility terms that are\nnot differentiable. We introduce, to our knowledge, the first general-purpose\ndifferentiable ray tracer that solves the full rendering equation, while\ncorrectly taking the geometric discontinuities into account.\n  Finally, we demonstrate that the derivatives of light path throughput can\nalso be useful for guiding sampling in forward rendering. Simulating light\ntransport in the presence of multi-bounce glossy effects and motion in 3D\nrendering is challenging due to the hard-to-sample high-contribution areas. We\npresent a Markov Chain Monte Carlo rendering algorithm that extends Metropolis\nLight Transport by automatically and explicitly adapting to the local\nintegrand, thereby increasing sampling efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 23:43:07 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 11:34:03 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Li", "Tzu-Mao", ""]]}, {"id": "1904.12240", "submitter": "Francisca Gil-Ureta", "authors": "Francisca Gil-Ureta, Nico Pietroni, Denis Zorin", "title": "Structurally optimized shells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shells, i.e., objects made of a thin layer of material following a surface,\nare among the most common structures in use. They are highly efficient, in\nterms of material required to maintain strength, but also prone to deformation\nand failure. We introduce an efficient method for reinforcing shells, that is,\nadding material to the shell to increase its resilience to external loads. Our\ngoal is to produce a reinforcement structure of minimal weight. It has been\ndemonstrated that optimal reinforcement structures may be qualitatively\ndifferent, depending on external loads and surface shape. In some cases, it\nnaturally consists of discrete protruding ribs; in other cases, a smooth shell\nthickness variation allows to save more material.\n  Most previously proposed solutions, starting from classical Michell trusses,\nare not able to handle a full range of shells (e.g., are restricted to\nself-supporting structures) or are unable to reproduce this range of behaviors,\nresulting in suboptimal structures.\n  We propose a new method that works for any input surface with any load\nconfigurations, taking into account both in-plane (tensile/compression) and\nout-of-plane (bending) forces. By using a more precise volume model, we are\ncapable of producing optimized structures with the full range of qualitative\nbehaviors. Our method includes new algorithms for determining the layout of\nreinforcement structure elements, and an efficient algorithm to optimize their\nshape, minimizing a non-linear non-convex functional at a fraction of the cost\nand with better optimality compared to standard solvers.\n  We demonstrate the optimization results for a variety of shapes, and the\nimprovements it yields in the strength of 3D-printed objects.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 01:41:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Gil-Ureta", "Francisca", ""], ["Pietroni", "Nico", ""], ["Zorin", "Denis", ""]]}, {"id": "1904.12294", "submitter": "Kai Wang", "authors": "Kai Wang and Fuyuan Shi and Wenqi Wang and Yibing Nan and Shiguo Lian", "title": "Synthetic Data Generation and Adaption for Object Detection in Smart\n  Vending Machines", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an improved scheme for the generation and adaption of\nsynthetic images for the training of deep Convolutional Neural Networks(CNNs)\nto perform the object detection task in smart vending machines. While\ngenerating synthetic data has proved to be effective for complementing the\ntraining data in supervised learning methods, challenges still exist for\ngenerating virtual images which are similar to those of the complex real scenes\nand minimizing redundant training data. To solve these problems, we consider\nthe simulation of cluttered objects placed in a virtual scene and the\nwide-angle camera with distortions used to capture the whole scene in the data\ngeneration process, and post-processed the generated images with a\nelaborately-designed generative network to make them more similar to the real\nimages. Various experiments have been conducted to prove the efficiency of\nusing the generated virtual images to enhance the detection precision on\nexisting datasets with limited real training data and the generalization\nability of applying the trained network to datasets collected in new\nenvironment.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 10:16:04 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Wang", "Kai", ""], ["Shi", "Fuyuan", ""], ["Wang", "Wenqi", ""], ["Nan", "Yibing", ""], ["Lian", "Shiguo", ""]]}, {"id": "1904.12297", "submitter": "Enrique Rosales", "authors": "Enrique Rosales, Jafet Rodriguez, Alla Sheffer", "title": "SurfaceBrush: From Virtual Reality Drawings to Manifold Surfaces", "comments": "Accepted at SIGGRAPH 2019", "journal-ref": null, "doi": "10.1145/3306346.3322970", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Popular Virtual Reality (VR) tools allow users to draw varying-width,\nribbon-like 3D brush strokes by moving a hand-held controller in 3D space.\nArtists frequently use dense collections of such strokes to draw virtual 3D\nshapes. We propose SurfaceBrush, a surfacing method that converts such VR\ndrawings into user-intended manifold free-form 3D surfaces, providing a novel\napproach for modeling 3D shapes. The inputs to our method consist of dense\ncollections of artist-drawn stroke ribbons described by the positions and\nnormals of their central polylines, and ribbon widths. These inputs are highly\ndistinct from those handled by existing surfacing frameworks and exhibit\ndifferent sparsity and error patterns, necessitating a novel surfacing\napproach. We surface the input stroke drawings by identifying and leveraging\nlocal coherence between nearby artist strokes. In particular, we observe that\nstrokes intended to be adjacent on the artist imagined surface often have\nsimilar tangent directions along their respective polylines. We leverage this\nlocal stroke direction consistency by casting the computation of the\nuser-intended manifold surface as a constrained matching problem on stroke\npolyline vertices and edges. We first detect and smoothly connect adjacent\nsimilarly-directed sequences of stroke edges producing one or more manifold\npartial surfaces. We then complete the surfacing process by identifying and\nconnecting adjacent similarly directed edges along the borders of these partial\nsurfaces. We confirm the usability of the SurfaceBrush interface and the\nvalidity of our drawing analysis via an observational study. We validate our\nstroke surfacing algorithm by demonstrating an array of manifold surfaces\ncomputed by our framework starting from a range of inputs of varying\ncomplexity, and by comparing our outputs to reconstructions computed using\nalternative means.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 10:23:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rosales", "Enrique", ""], ["Rodriguez", "Jafet", ""], ["Sheffer", "Alla", ""]]}, {"id": "1904.12356", "submitter": "Justus Thies", "authors": "Justus Thies and Michael Zollh\\\"ofer and Matthias Nie{\\ss}ner", "title": "Deferred Neural Rendering: Image Synthesis using Neural Textures", "comments": "Video: https://youtu.be/z-pVip6WeyY SIGGRAPH 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern computer graphics pipeline can synthesize images at remarkable\nvisual quality; however, it requires well-defined, high-quality 3D content as\ninput. In this work, we explore the use of imperfect 3D content, for instance,\nobtained from photo-metric reconstructions with noisy and incomplete surface\ngeometry, while still aiming to produce photo-realistic (re-)renderings. To\naddress this challenging problem, we introduce Deferred Neural Rendering, a new\nparadigm for image synthesis that combines the traditional graphics pipeline\nwith learnable components. Specifically, we propose Neural Textures, which are\nlearned feature maps that are trained as part of the scene capture process.\nSimilar to traditional textures, neural textures are stored as maps on top of\n3D mesh proxies; however, the high-dimensional feature maps contain\nsignificantly more information, which can be interpreted by our new deferred\nneural rendering pipeline. Both neural textures and deferred neural renderer\nare trained end-to-end, enabling us to synthesize photo-realistic images even\nwhen the original 3D content was imperfect. In contrast to traditional,\nblack-box 2D generative neural networks, our 3D representation gives us\nexplicit control over the generated output, and allows for a wide range of\napplication domains. For instance, we can synthesize temporally-consistent\nvideo re-renderings of recorded 3D scenes as our representation is inherently\nembedded in 3D space. This way, neural textures can be utilized to coherently\nre-render or manipulate existing video content in both static and dynamic\nenvironments at real-time rates. We show the effectiveness of our approach in\nseveral experiments on novel view synthesis, scene editing, and facial\nreenactment, and compare to state-of-the-art approaches that leverage the\nstandard graphics pipeline as well as conventional generative neural networks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 18:00:08 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Thies", "Justus", ""], ["Zollh\u00f6fer", "Michael", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1904.12795", "submitter": "Anna Fr\\\"uhst\\\"uck", "authors": "Anna Fr\\\"uhst\\\"uck and Ibraheem Alhashim and Peter Wonka", "title": "TileGAN: Synthesis of Large-Scale Non-Homogeneous Textures", "comments": "Code is available at http://github.com/afruehstueck/tileGAN", "journal-ref": "ACM Transactions on Graphics (SIGGRAPH 2019) 38 (4)", "doi": "10.1145/3306346.3322993", "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We tackle the problem of texture synthesis in the setting where many input\nimages are given and a large-scale output is required. We build on recent\ngenerative adversarial networks and propose two extensions in this paper.\nFirst, we propose an algorithm to combine outputs of GANs trained on a smaller\nresolution to produce a large-scale plausible texture map with virtually no\nboundary artifacts. Second, we propose a user interface to enable artistic\ncontrol. Our quantitative and qualitative results showcase the generation of\nsynthesized high-resolution maps consisting of up to hundreds of megapixels as\na case in point.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 16:15:56 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Fr\u00fchst\u00fcck", "Anna", ""], ["Alhashim", "Ibraheem", ""], ["Wonka", "Peter", ""]]}, {"id": "1904.13041", "submitter": "Yifeng Jiang", "authors": "Yifeng Jiang, Tom Van Wouwe, Friedl De Groote, C. Karen Liu", "title": "Synthesis of Biologically Realistic Human Motion Using Joint Torque\n  Actuation", "comments": "SIGGRAPH 2019. 12 pages, 8 figures. Accompanying video:\n  https://youtu.be/3UxfF_BmDxY", "journal-ref": null, "doi": "10.1145/3306346.3322966", "report-no": null, "categories": "cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using joint actuators to drive the skeletal movements is a common practice in\ncharacter animation, but the resultant torque patterns are often unnatural or\ninfeasible for real humans to achieve. On the other hand, physiologically-based\nmodels explicitly simulate muscles and tendons and thus produce more human-like\nmovements and torque patterns. This paper introduces a technique to transform\nan optimal control problem formulated in the muscle-actuation space to an\nequivalent problem in the joint-actuation space, such that the solutions to\nboth problems have the same optimal value. By solving the equivalent problem in\nthe joint-actuation space, we can generate human-like motions comparable to\nthose generated by musculotendon models, while retaining the benefit of simple\nmodeling and fast computation offered by joint-actuation models. Our method\ntransforms constant bounds on muscle activations to nonlinear, state-dependent\ntorque limits in the joint-actuation space. In addition, the metabolic energy\nfunction on muscle activations is transformed to a nonlinear function of joint\ntorques, joint configuration and joint velocity. Our technique can also benefit\npolicy optimization using deep reinforcement learning approach, by providing a\nmore anatomically realistic action space for the agent to explore during the\nlearning process. We take the advantage of the physiologically-based simulator,\nOpenSim, to provide training data for learning the torque limits and the\nmetabolic energy function. Once trained, the same torque limits and the energy\nfunction can be applied to drastically different motor tasks formulated as\neither trajectory optimization or policy learning. Codebase:\nhttps://github.com/jyf588/lrle and https://github.com/jyf588/lrle-rl-examples\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:55:30 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 06:17:50 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Jiang", "Yifeng", ""], ["Van Wouwe", "Tom", ""], ["De Groote", "Friedl", ""], ["Liu", "C. Karen", ""]]}, {"id": "1904.13210", "submitter": "Morad Behandish", "authors": "Morad Behandish, Amir M. Mirzendehdel, Saigopal Nelaturi", "title": "A Classification of Topological Discrepancies in Additive Manufacturing", "comments": "Special Issue on symposium on Solid and Physical Modeling (SPM'2019)", "journal-ref": "Journal of Computer-Aided Design, 2019", "doi": "10.1016/j.cad.2019.05.032", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive manufacturing (AM) enables enormous freedom for design of complex\nstructures. However, the process-dependent limitations that result in\ndiscrepancies between as-designed and as-manufactured shapes are not fully\nunderstood. The tradeoffs between infinitely many different ways to approximate\na design by a manufacturable replica are even harder to characterize. To\nsupport design for AM (DfAM), one has to quantify local discrepancies\nintroduced by AM processes, identify the detrimental deviations (if any) to the\noriginal design intent, and prescribe modifications to the design and/or\nprocess parameters to countervail their effects. Our focus in this work will be\non topological analysis. There is ample evidence in many applications that\npreserving local topology (e.g., connectivity of beams in a lattice) is\nimportant even when slight geometric deviations can be tolerated. We first\npresent a generic method to characterize local topological discrepancies due to\nmaterial under- and over-deposition in AM, and show how it captures various\ntypes of defects in the as-manufactured structures. We use this information to\nsystematically modify the as-manufactured outcomes within the limitations of\navailable 3D printer resolution(s), which often comes at the expense of\nintroducing more geometric deviations (e.g., thickening a beam to avoid\ndisconnection). We validate the effectiveness of the method on 3D examples with\nnontrivial topologies such as lattice structures and foams.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 17:30:30 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 02:36:58 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 22:39:14 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Behandish", "Morad", ""], ["Mirzendehdel", "Amir M.", ""], ["Nelaturi", "Saigopal", ""]]}]