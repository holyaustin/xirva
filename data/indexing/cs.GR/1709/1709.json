[{"id": "1709.00184", "submitter": "Romeo Popa", "authors": "Romeo Traian Popa (1), Emilia-Cerna Mladin (1), Emil Petrescu (1),\n  Tudor Prisecaru (1) ((1) University Politehnica of Bucharest)", "title": "A simple en,ex marking rule for degenerate intersection points in 2D\n  polygon clipping", "comments": "Acknowledgements added, replaced contour orientation, ccw and cw with\n  respectively contour hand, left hand and right hand, corrected typos of v1 in\n  wording of Rule 1, Rule 2 and the RULE, 10 pages, 10 colored figures, 1 table\n  with colored symbols", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple en,ex rule to mark the intersection points of 2D input polygon\ncontours separating the polygon interior from its exterior in the vicinity of\nthe intersections is presented. Its form is close to the original Greiner &\nHormann algorithm rule but encompasses degenerate intersections that are not\nself-intersections. It only uses local geometric information once the hand of\nthe two input contours is known. The approach foundation is the distinction\nbetween two features of the studied intersections: the geometric intersection\npoint and the assembling/concatenation point of the result contour/border. No\nspecial form of the intersection finding procedure is required.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 07:29:06 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 14:58:29 GMT"}, {"version": "v3", "created": "Fri, 15 Sep 2017 10:09:27 GMT"}, {"version": "v4", "created": "Sun, 28 Jan 2018 22:53:09 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Popa", "Romeo Traian", "", "University Politehnica of Bucharest"], ["Mladin", "Emilia-Cerna", "", "University Politehnica of Bucharest"], ["Petrescu", "Emil", "", "University Politehnica of Bucharest"], ["Prisecaru", "Tudor", "", "University Politehnica of Bucharest"]]}, {"id": "1709.00643", "submitter": "Qifeng Chen", "authors": "Qifeng Chen, Jia Xu, Vladlen Koltun", "title": "Fast Image Processing with Fully-Convolutional Networks", "comments": "Published at the International Conference on Computer Vision (ICCV\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to accelerating a wide variety of image processing\noperators. Our approach uses a fully-convolutional network that is trained on\ninput-output pairs that demonstrate the operator's action. After training, the\noriginal operator need not be run at all. The trained network operates at full\nresolution and runs in constant time. We investigate the effect of network\narchitecture on approximation accuracy, runtime, and memory footprint, and\nidentify a specific architecture that balances these considerations. We\nevaluate the presented approach on ten advanced image processing operators,\nincluding multiple variational models, multiscale tone and detail manipulation,\nphotographic style transfer, nonlocal dehazing, and nonphotorealistic\nstylization. All operators are approximated by the same model. Experiments\ndemonstrate that the presented approach is significantly more accurate than\nprior approximation schemes. It increases approximation accuracy as measured by\nPSNR across the evaluated operators by 8.5 dB on the MIT-Adobe dataset (from\n27.5 to 36 dB) and reduces DSSIM by a multiplicative factor of 3 compared to\nthe most accurate prior approximation scheme, while being the fastest. We show\nthat our models generalize across datasets and across resolutions, and\ninvestigate a number of extensions of the presented approach. The results are\nshown in the supplementary video at https://youtu.be/eQyfHgLx8Dc\n", "versions": [{"version": "v1", "created": "Sat, 2 Sep 2017 22:38:13 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Chen", "Qifeng", ""], ["Xu", "Jia", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1709.00649", "submitter": "Sebastian Wagner-Carena", "authors": "Max Hopkins, Michael Mitzenmacher, and Sebastian Wagner-Carena", "title": "Simulated Annealing for JPEG Quantization", "comments": "Appendix not included in arXiv version due to size restrictions. For\n  full paper go to:\n  http://www.eecs.harvard.edu/~michaelm/SimAnneal/PAPER/simulated-annealing-jpeg.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JPEG is one of the most widely used image formats, but in some ways remains\nsurprisingly unoptimized, perhaps because some natural optimizations would go\noutside the standard that defines JPEG. We show how to improve JPEG compression\nin a standard-compliant, backward-compatible manner, by finding improved\ndefault quantization tables. We describe a simulated annealing technique that\nhas allowed us to find several quantization tables that perform better than the\nindustry standard, in terms of both compressed size and image fidelity.\nSpecifically, we derive tables that reduce the FSIM error by over 10% while\nimproving compression by over 20% at quality level 95 in our tests; we also\nprovide similar results for other quality levels. While we acknowledge our\napproach can in some images lead to visible artifacts under large\nmagnification, we believe use of these quantization tables, or additional\ntables that could be found using our methodology, would significantly reduce\nJPEG file sizes with improved overall image quality.\n", "versions": [{"version": "v1", "created": "Sun, 3 Sep 2017 01:10:18 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Hopkins", "Max", ""], ["Mitzenmacher", "Michael", ""], ["Wagner-Carena", "Sebastian", ""]]}, {"id": "1709.01221", "submitter": "Jieting Wu", "authors": "Jieting Wu, Jianping Zeng, Feiyu Zhu and Hongfeng Yu", "title": "MLSEB: Edge Bundling using Moving Least Squares Approximation", "comments": "Appears in the Proceedings of the 25th International Symposium on\n  Graph Drawing and Network Visualization (GD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge bundling methods can effectively alleviate visual clutter and reveal\nhigh-level graph structures in large graph visualization. Researchers have\ndevoted significant efforts to improve edge bundling according to different\nmetrics. As the edge bundling family evolve rapidly, the quality of edge\nbundles receives increasing attention in the literature accordingly. In this\npaper, we present MLSEB, a novel method to generate edge bundles based on\nmoving least squares (MLS) approximation. In comparison with previous edge\nbundling methods, we argue that our MLSEB approach can generate better results\nbased on a quantitative metric of quality, and also ensure scalability and the\nefficiency for visualizing large graphs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 03:08:01 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 21:26:02 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Wu", "Jieting", ""], ["Zeng", "Jianping", ""], ["Zhu", "Feiyu", ""], ["Yu", "Hongfeng", ""]]}, {"id": "1709.01250", "submitter": "Lin Gao", "authors": "Lin Gao and Yu-Kun Lai and Jie Yang and Ling-Xiao Zhang and Leif\n  Kobbelt and Shihong Xia", "title": "Sparse Data Driven Mesh Deformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Example-based mesh deformation methods are powerful tools for realistic shape\nediting. However, existing techniques typically combine all the example\ndeformation modes, which can lead to overfitting, i.e. using a overly\ncomplicated model to explain the user-specified deformation. This leads to\nimplausible or unstable deformation results, including unexpected global\nchanges outside the region of interest. To address this fundamental limitation,\nwe propose a sparse blending method that automatically selects a smaller number\nof deformation modes to compactly describe the desired deformation. This along\nwith a suitably chosen deformation basis including spatially localized\ndeformation modes leads to significant advantages, including more meaningful,\nreliable, and efficient deformations because fewer and localized deformation\nmodes are applied. To cope with large rotations, we develop a simple but\neffective representation based on polar decomposition of deformation gradients,\nwhich resolves the ambiguity of large global rotations using an\nas-consistent-as-possible global optimization. This simple representation has a\nclosed form solution for derivatives, making it efficient for sparse localized\nrepresentation and thus ensuring interactive performance. Experimental results\nshow that our method outperforms state-of-the-art data-driven mesh deformation\nmethods, for both quality of results and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 06:24:02 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Gao", "Lin", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jie", ""], ["Zhang", "Ling-Xiao", ""], ["Kobbelt", "Leif", ""], ["Xia", "Shihong", ""]]}, {"id": "1709.01295", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Isht Dwivedi, Abhijat Biswas, Sahil\n  Manocha, R. Venkatesh Babu", "title": "SketchParse : Towards Rich Descriptions for Poorly Drawn Sketches using\n  Multi-Task Hierarchical Deep Networks", "comments": "A shorter version of this submission was accepted at ACM Multimedia\n  (ACMMM) 2017. Code, annotated datasets and pre-trained models available at\n  https://github.com/val-iisc/sketch-parse", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to semantically interpret hand-drawn line sketches, although very\nchallenging, can pave way for novel applications in multimedia. We propose\nSketchParse, the first deep-network architecture for fully automatic parsing of\nfreehand object sketches. SketchParse is configured as a two-level fully\nconvolutional network. The first level contains shared layers common to all\nobject categories. The second level contains a number of expert sub-networks.\nEach expert specializes in parsing sketches from object categories which\ncontain structurally similar parts. Effectively, the two-level configuration\nenables our architecture to scale up efficiently as additional categories are\nadded. We introduce a router layer which (i) relays sketch features from shared\nlayers to the correct expert (ii) eliminates the need to manually specify\nobject category during inference. To bypass laborious part-level annotation, we\nsketchify photos from semantic object-part image datasets and use them for\ntraining. Our architecture also incorporates object pose prediction as a novel\nauxiliary task which boosts overall performance while providing supplementary\ninformation regarding the sketch. We demonstrate SketchParse's abilities (i) on\ntwo challenging large-scale sketch datasets (ii) in parsing unseen,\nsemantically related object categories (iii) in improving fine-grained\nsketch-based image retrieval. As a novel application, we also outline how\nSketchParse's output can be used to generate caption-style descriptions for\nhand-drawn sketches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:10:59 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Dwivedi", "Isht", ""], ["Biswas", "Abhijat", ""], ["Manocha", "Sahil", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1709.01597", "submitter": "Reza Parvaz", "authors": "M.Zarebnia, R.Kianfar, R.Parvaz", "title": "Multi-color image compression-encryption algorithm based on chaotic\n  system and fuzzy transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an algorithm for multi-color image compression-encryption is\nintroduced. For compression step fuzzy transform based on exponential b-spline\nfunction is used. In encryption step, a novel combination chaotic system based\non Sine and Tent systems is proposed. Also in the encryption algorithm, 3D\nshift based on chaotic system is introduced. The simulation results and\nsecurity analysis show that the proposed algorithm is secure and efficient.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 19:24:17 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Zarebnia", "M.", ""], ["Kianfar", "R.", ""], ["Parvaz", "R.", ""]]}, {"id": "1709.01638", "submitter": "Qiang Zhao", "authors": "Qiang Zhao, Liang Wan, Wei Feng, Jiawan Zhang and Tien-Tsin Wong", "title": "360 Panorama Cloning on Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address a novel problem of cloning a patch of the source\nspherical panoramic image to the target spherical panoramic image, which we\ncall 360 panorama cloning. Considering the sphere geometry constraint embedded\nin spherical panoramic images, we develop a coordinate-based method that\ndirectly clones in the spherical domain. Our method neither differentiates the\npolar regions and equatorial regions, nor identifies the boundaries in the\nunrolled planar-formatted panorama. We discuss in depth two unique issues in\npanorama cloning, i.e. preserving the patch's orientation, and handling the\nlarge-patch cloning (covering over 180 field of view) which may suffer from\ndiscoloration artifacts. As experimental results demonstrate, our method is\nable to get visually pleasing cloning results and achieve real time cloning\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Sep 2017 00:47:54 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Zhao", "Qiang", ""], ["Wan", "Liang", ""], ["Feng", "Wei", ""], ["Zhang", "Jiawan", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "1709.02721", "submitter": "Alexander Herega", "authors": "Alexander Herega", "title": "On the correlation between a level of structure order and properties of\n  composites. In Memory of Yu.L. Klimontovich", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": "878 N", "categories": "cs.GR cond-mat.mtrl-sci physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proposed the computerized method for calculating the relative level of order\ncomposites. Correlation between a level of structure order and properties of\nsolids is shown. Discussed the possibility of clarifying the terminology used\nin describing the structure.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 07:02:54 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 03:48:18 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Herega", "Alexander", ""]]}, {"id": "1709.02782", "submitter": "Majid Masoumi", "authors": "Majid Masoumi, A. Ben Hamza", "title": "Global spectral graph wavelet signature for surface analysis of carpal\n  bones", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.06250", "journal-ref": null, "doi": "10.1088/1361-6560/aaa71a", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a spectral graph wavelet approach for shape\nanalysis of carpal bones of human wrist. We apply a metric called global\nspectral graph wavelet signature for representation of cortical surface of the\ncarpal bone based on eigensystem of Laplace-Beltrami operator. Furthermore, we\npropose a heuristic and efficient way of aggregating local descriptors of a\ncarpal bone surface to global descriptor. The resultant global descriptor is\nnot only isometric invariant, but also much more efficient and requires less\nmemory storage. We perform experiments on shape of the carpal bones of ten\nwomen and ten men from a publicly-available database. Experimental results show\nthe excellency of the proposed GSGW compared to recent proposed GPS embedding\napproach for comparing shapes of the carpal bones across populations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 18:30:54 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Masoumi", "Majid", ""], ["Hamza", "A. Ben", ""]]}, {"id": "1709.04304", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Lin Gao, Yu-Kun Lai, Jie Yang and Shihong Xia", "title": "Mesh-based Autoencoders for Localized Deformation Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially localized deformation components are very useful for shape analysis\nand synthesis in 3D geometry processing. Several methods have recently been\ndeveloped, with an aim to extract intuitive and interpretable deformation\ncomponents. However, these techniques suffer from fundamental limitations\nespecially for meshes with noise or large-scale deformations, and may not\nalways be able to identify important deformation components. In this paper we\npropose a novel mesh-based autoencoder architecture that is able to cope with\nmeshes with irregular topology. We introduce sparse regularization in this\nframework, which along with convolutional operations, helps localize\ndeformations. Our framework is capable of extracting localized deformation\ncomponents from mesh data sets with large-scale deformations and is robust to\nnoise. It also provides a nonlinear approach to reconstruction of meshes using\nthe extracted basis, which is more effective than the current linear\ncombination approach. Extensive experiments show that our method outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 12:59:48 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 03:09:15 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Tan", "Qingyang", ""], ["Gao", "Lin", ""], ["Lai", "Yu-Kun", ""], ["Yang", "Jie", ""], ["Xia", "Shihong", ""]]}, {"id": "1709.04307", "submitter": "Qingyang Tan", "authors": "Qingyang Tan, Lin Gao, Yu-Kun Lai and Shihong Xia", "title": "Variational Autoencoders for Deforming 3D Mesh Models", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D geometric contents are becoming increasingly popular. In this paper, we\nstudy the problem of analyzing deforming 3D meshes using deep neural networks.\nDeforming 3D meshes are flexible to represent 3D animation sequences as well as\ncollections of objects of the same category, allowing diverse shapes with\nlarge-scale non-linear deformations. We propose a novel framework which we call\nmesh variational autoencoders (mesh VAE), to explore the probabilistic latent\nspace of 3D surfaces. The framework is easy to train, and requires very few\ntraining examples. We also propose an extended model which allows flexibly\nadjusting the significance of different latent variables by altering the prior\ndistribution. Extensive experiments demonstrate that our general framework is\nable to learn a reasonable representation for a collection of deformable\nshapes, and produce competitive results for a variety of applications,\nincluding shape generation, shape interpolation, shape space embedding and\nshape exploration, outperforming state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 13:09:22 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 03:16:42 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 03:27:39 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Tan", "Qingyang", ""], ["Gao", "Lin", ""], ["Lai", "Yu-Kun", ""], ["Xia", "Shihong", ""]]}, {"id": "1709.04752", "submitter": "Igor Sabo", "authors": "I.I. Sabo, H.R. Lagoda", "title": "The wave method of building color palette and its application in\n  computer graphics", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article describes a method of getting a harmonious combination of\ncolors, developed by us on the basis of the relationship of color and acoustic\nwaves. Presents a parallel between harmoniously matched colors and the concept\nof harmony in music theory (consonance). Describes the physical assumption of\nthe essence of the phenomenon of harmony (consonance). The article also\nprovides algorithm of implementation wave method for the sRGB color model.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 11:24:08 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Sabo", "I. I.", ""], ["Lagoda", "H. R.", ""]]}, {"id": "1709.04866", "submitter": "Ehsan Arbabi", "authors": "Ehsan Arbabi", "title": "Dynamic Network: Graphical Deformation of Penetrated Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a computer-based virtual environment, objects may collide with each other.\nTherefore, different algorithms are needed to detect the collision and perform\na correct action in order to avoid penetration. Based on the application and\nobjects physical characteristics, a correct action can include separating or\ndeforming the penetrated objects. In this article, by using the concepts of\ndynamic networks and simple physics, a method for deforming two penetrated 3D\nobjects is proposed. In this method, we consider each primitive of the objects\nas an element interacting with the other elements in a dynamic network. These\nkinds of interactions make the elements impose force on each other and change\ntheir position, until a force-balance happens. The proposed method is\nimplemented and tested on 3D sample objects, and the resulted deformation\nproved to be visually satisfying.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 16:36:56 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 06:07:53 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Arbabi", "Ehsan", ""]]}, {"id": "1709.05056", "submitter": "Marc Khoury", "authors": "Marc Khoury, Qian-Yi Zhou, Vladlen Koltun", "title": "Learning Compact Geometric Features", "comments": "International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to learning features that represent the local geometry\naround a point in an unstructured point cloud. Such features play a central\nrole in geometric registration, which supports diverse applications in robotics\nand 3D vision. Current state-of-the-art local features for unstructured point\nclouds have been manually crafted and none combines the desirable properties of\nprecision, compactness, and robustness. We show that features with these\nproperties can be learned from data, by optimizing deep networks that map\nhigh-dimensional histograms into low-dimensional Euclidean spaces. The\npresented approach yields a family of features, parameterized by dimension,\nthat are both more compact and more accurate than existing descriptors.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 04:44:28 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Khoury", "Marc", ""], ["Zhou", "Qian-Yi", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1709.05418", "submitter": "Brian McWilliams", "authors": "Simon Kallweit and Thomas M\\\"uller and Brian McWilliams and Markus\n  Gross and Jan Nov\\'ak", "title": "Deep Scattering: Rendering Atmospheric Clouds with Radiance-Predicting\n  Neural Networks", "comments": "ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia 2017)", "journal-ref": null, "doi": "10.1145/3130800.3130880", "report-no": null, "categories": "cs.LG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for efficiently synthesizing images of atmospheric\nclouds using a combination of Monte Carlo integration and neural networks. The\nintricacies of Lorenz-Mie scattering and the high albedo of cloud-forming\naerosols make rendering of clouds---e.g. the characteristic silverlining and\nthe \"whiteness\" of the inner body---challenging for methods based solely on\nMonte Carlo integration or diffusion theory. We approach the problem\ndifferently. Instead of simulating all light transport during rendering, we\npre-learn the spatial and directional distribution of radiant flux from tens of\ncloud exemplars. To render a new scene, we sample visible points of the cloud\nand, for each, extract a hierarchical 3D descriptor of the cloud geometry with\nrespect to the shading location and the light source. The descriptor is input\nto a deep neural network that predicts the radiance function for each shading\nconfiguration. We make the key observation that progressively feeding the\nhierarchical descriptor into the network enhances the network's ability to\nlearn faster and predict with high accuracy while using few coefficients. We\nalso employ a block design with residual connections to further improve\nperformance. A GPU implementation of our method synthesizes images of clouds\nthat are nearly indistinguishable from the reference solution within seconds\ninteractively. Our method thus represents a viable solution for applications\nsuch as cloud design and, thanks to its temporal stability, also for\nhigh-quality production of animated content.\n", "versions": [{"version": "v1", "created": "Fri, 15 Sep 2017 21:40:02 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Kallweit", "Simon", ""], ["M\u00fcller", "Thomas", ""], ["McWilliams", "Brian", ""], ["Gross", "Markus", ""], ["Nov\u00e1k", "Jan", ""]]}, {"id": "1709.07581", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Philip Marcus", "title": "Hierarchical Detail Enhancing Mesh-Based Shape Generation with 3D\n  Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic mesh-based shape generation is of great interest across a wide\nrange of disciplines, from industrial design to gaming, computer graphics and\nvarious other forms of digital art. While most traditional methods focus on\nprimitive based model generation, advances in deep learning made it possible to\nlearn 3-dimensional geometric shape representations in an end-to-end manner.\nHowever, most current deep learning based frameworks focus on the\nrepresentation and generation of voxel and point-cloud based shapes, making it\nnot directly applicable to design and graphics communities. This study\naddresses the needs for automatic generation of mesh-based geometries, and\npropose a novel framework that utilizes signed distance function representation\nthat generates detail preserving three-dimensional surface mesh by a deep\nlearning based approach.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 03:17:34 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Marcus", "Philip", ""]]}, {"id": "1709.07599", "submitter": "Zhen Li", "authors": "Xiaoguang Han, Zhen Li, Haibin Huang, Evangelos Kalogerakis, and\n  Yizhou Yu", "title": "High-Resolution Shape Completion Using Deep Neural Networks for Global\n  Structure and Local Geometry Inference", "comments": "8 pages paper, 11 pages supplementary material, ICCV spotlight paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven method for recovering miss-ing parts of 3D shapes.\nOur method is based on a new deep learning architecture consisting of two\nsub-networks: a global structure inference network and a local geometry\nrefinement network. The global structure inference network incorporates a long\nshort-term memorized context fusion module (LSTM-CF) that infers the global\nstructure of the shape based on multi-view depth information provided as part\nof the input. It also includes a 3D fully convolutional (3DFCN) module that\nfurther enriches the global structure representation according to volumetric\ninformation in the input. Under the guidance of the global structure network,\nthe local geometry refinement network takes as input lo-cal 3D patches around\nmissing regions, and progressively produces a high-resolution, complete surface\nthrough a volumetric encoder-decoder architecture. Our method jointly trains\nthe global structure inference and local geometry refinement networks in an\nend-to-end manner. We perform qualitative and quantitative evaluations on six\nobject categories, demonstrating that our method outperforms existing\nstate-of-the-art work on shape completion.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 05:16:17 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Han", "Xiaoguang", ""], ["Li", "Zhen", ""], ["Huang", "Haibin", ""], ["Kalogerakis", "Evangelos", ""], ["Yu", "Yizhou", ""]]}, {"id": "1709.08774", "submitter": "Zhutian Chen", "authors": "Zhutian Chen, Yifang Wang, Tianchen Sun, Xiang Gao, Wei Chen, Zhigeng\n  Pan, Huamin Qu, Yingcai Wu", "title": "Exploring the Design Space of Immersive Urban Analytics", "comments": "23 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid development and wide adoption of\nimmersive head-mounted devices, such as HTC VIVE, Oculus Rift, and Microsoft\nHoloLens. These immersive devices have the potential to significantly extend\nthe methodology of urban visual analytics by providing critical 3D context\ninformation and creating a sense of presence. In this paper, we propose an\ntheoretical model to characterize the visualizations in immersive urban\nanalytics. Further more, based on our comprehensive and concise model, we\ncontribute a typology of combination methods of 2D and 3D visualizations that\ndistinguish between linked views, embedded views, and mixed views. We also\npropose a supporting guideline to assist users in selecting a proper view under\ncertain circumstances by considering visual geometry and spatial distribution\nof the 2D and 3D visualizations. Finally, based on existing works, possible\nfuture research opportunities are explored and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 01:15:26 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Chen", "Zhutian", ""], ["Wang", "Yifang", ""], ["Sun", "Tianchen", ""], ["Gao", "Xiang", ""], ["Chen", "Wei", ""], ["Pan", "Zhigeng", ""], ["Qu", "Huamin", ""], ["Wu", "Yingcai", ""]]}, {"id": "1709.09602", "submitter": "Yuanming Hu", "authors": "Yuanming Hu, Hao He, Chenxi Xu, Baoyuan Wang, Stephen Lin", "title": "Exposure: A White-Box Photo Post-Processing Framework", "comments": "ACM Transaction on Graphics (Accepted with minor revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retouching can significantly elevate the visual appeal of photos, but many\ncasual photographers lack the expertise to do this well. To address this\nproblem, previous works have proposed automatic retouching systems based on\nsupervised learning from paired training images acquired before and after\nmanual editing. As it is difficult for users to acquire paired images that\nreflect their retouching preferences, we present in this paper a deep learning\napproach that is instead trained on unpaired data, namely a set of photographs\nthat exhibits a retouching style the user likes, which is much easier to\ncollect. Our system is formulated using deep convolutional neural networks that\nlearn to apply different retouching operations on an input image. Network\ntraining with respect to various types of edits is enabled by modeling these\nretouching operations in a unified manner as resolution-independent\ndifferentiable filters. To apply the filters in a proper sequence and with\nsuitable parameters, we employ a deep reinforcement learning approach that\nlearns to make decisions on what action to take next, given the current state\nof the image. In contrast to many deep learning systems, ours provides users\nwith an understandable solution in the form of conventional retouching edits,\nrather than just a \"black-box\" result. Through quantitative comparisons and\nuser studies, we show that this technique generates retouching results\nconsistent with the provided photo set.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 16:15:58 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 16:47:58 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Hu", "Yuanming", ""], ["He", "Hao", ""], ["Xu", "Chenxi", ""], ["Wang", "Baoyuan", ""], ["Lin", "Stephen", ""]]}, {"id": "1709.09701", "submitter": "Etienne Corman", "authors": "Etienne Corman and Maks Ovsjanikov", "title": "Functional Characterization of Deformation Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel representation for deformation fields of 3D\nshapes, by considering the induced changes in the underlying metric. In\nparticular, our approach allows to represent a deformation field in a\ncoordinate-free way as a linear operator acting on real-valued functions\ndefined on the shape. Such a representation both provides a way to relate\ndeformation fields to other classical functional operators and enables analysis\nand processing of deformation fields using standard linear-algebraic tools.\nThis opens the door to a wide variety of applications such as explicitly adding\nextrinsic information into the computation of functional maps, intrinsic shape\nsymmetrization, joint deformation design through precise control of metric\ndistortion, and coordinate-free deformation transfer without requiring\npointwise correspondences. Our method is applicable to both surface and\nvolumetric shape representations and we guarantee the equivalence between the\noperator-based and standard deformation field representation under mild\ngenericity conditions in the discrete setting. We demonstrate the utility of\nour approach by comparing it with existing techniques and show how our\nrepresentation provides a powerful toolbox for a wide variety of challenging\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 18:58:56 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Corman", "Etienne", ""], ["Ovsjanikov", "Maks", ""]]}, {"id": "1709.10214", "submitter": "Xuaner Zhang", "authors": "Xuaner Cecilia Zhang, Joon-Young Lee, Kalyan Sunkavalli, Zhaowen Wang", "title": "Photometric Stabilization for Fast-forward Videos", "comments": "9 pages, 11 figures, PG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Videos captured by consumer cameras often exhibit temporal variations in\ncolor and tone that are caused by camera auto-adjustments like white-balance\nand exposure. When such videos are sub-sampled to play fast-forward, as in the\nincreasingly popular forms of timelapse and hyperlapse videos, these temporal\nvariations are exacerbated and appear as visually disturbing high frequency\nflickering. Previous techniques to photometrically stabilize videos typically\nrely on computing dense correspondences between video frames, and use these\ncorrespondences to remove all color changes in the video sequences. However,\nthis approach is limited in fast-forward videos that often have large content\nchanges and also might exhibit changes in scene illumination that should be\npreserved. In this work, we propose a novel photometric stabilization algorithm\nfor fast-forward videos that is robust to large content-variation across\nframes. We compute pairwise color and tone transformations between neighboring\nframes and smooth these pair-wise transformations while taking in account the\npossibility of scene/content variations. This allows us to eliminate\nhigh-frequency fluctuations, while still adapting to real variations in scene\ncharacteristics. We evaluate our technique on a new dataset consisting of\ncontrolled synthetic and real videos, and demonstrate that our techniques\noutperforms the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 02:16:05 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Zhang", "Xuaner Cecilia", ""], ["Lee", "Joon-Young", ""], ["Sunkavalli", "Kalyan", ""], ["Wang", "Zhaowen", ""]]}]