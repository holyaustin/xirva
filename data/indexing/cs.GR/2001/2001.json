[{"id": "2001.00521", "submitter": "Kevin Karsch", "authors": "Brittany Factura, Laura LaPerche, Phil Reyneri, Brett Jones, Kevin\n  Karsch", "title": "Lightform: Procedural Effects for Projected AR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projected augmented reality, also called projection mapping or video mapping,\nis a form of augmented reality that uses projected light to directly augment 3D\nsurfaces, as opposed to using pass-through screens or headsets. The value of\nprojected AR is its ability to add a layer of digital content directly onto\nphysical objects or environments in a way that can be instantaneously viewed by\nmultiple people, unencumbered by a screen or additional setup.\n  Because projected AR typically involves projecting onto non-flat, textured\nobjects (especially those that are conventionally not used as projection\nsurfaces), the digital content needs to be mapped and aligned to precisely fit\nthe physical scene to ensure a compelling experience. Current projected AR\ntechniques require extensive calibration at the time of installation, which is\nnot conducive to iteration or change, whether intentional (the scene is\nreconfigured) or not (the projector is bumped or settles). The workflows are\nundefined and fragmented, thus making it confusing and difficult for many to\napproach projected AR. For example, a digital artist may have the software\nexpertise to create AR content, but could not complete an installation without\nexperience in mounting, blending, and realigning projector(s); the converse is\ntrue for many A/V installation teams/professionals. Projection mapping has\ntherefore been limited to high-end event productions, concerts, and films,\nbecause it requires expensive, complex tools, and skilled teams ($100K+\nbudgets).\n  Lightform provides a technology that makes projected AR approachable,\npractical, intelligent, and robust through integrated hardware and\ncomputer-vision software. Lightform brings together and unites a currently\nfragmented workflow into a single cohesive process that provides users with an\napproachable and robust method to create and control projected AR experiences.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 00:15:38 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Factura", "Brittany", ""], ["LaPerche", "Laura", ""], ["Reyneri", "Phil", ""], ["Jones", "Brett", ""], ["Karsch", "Kevin", ""]]}, {"id": "2001.00986", "submitter": "Kevin Karsch", "authors": "Kevin Karsch", "title": "Inverse Rendering Techniques for Physically Grounded Image Editing", "comments": "PhD thesis, Computer Science, University of Illinois at\n  Urbana-Champaign, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a single picture of a scene, people can typically grasp the spatial\nlayout immediately and even make good guesses at materials properties and where\nlight is coming from to illuminate the scene. For example, we can reliably tell\nwhich objects occlude others, what an object is made of and its rough shape,\nregions that are illuminated or in shadow, and so on. It is interesting how\nlittle is known about our ability to make these determinations; as such, we are\nstill not able to robustly \"teach\" computers to make the same high-level\nobservations as people. This document presents algorithms for understanding\nintrinsic scene properties from single images. The goal of these inverse\nrendering techniques is to estimate the configurations of scene elements\n(geometry, materials, luminaires, camera parameters, etc) using only\ninformation visible in an image. Such algorithms have applications in robotics\nand computer graphics. One such application is in physically grounded image\nediting: photo editing made easier by leveraging knowledge of the physical\nspace. These applications allow sophisticated editing operations to be\nperformed in a matter of seconds, enabling seamless addition, removal, or\nrelocation of objects in images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 04:01:34 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Karsch", "Kevin", ""]]}, {"id": "2001.01026", "submitter": "Amy Zhao", "authors": "Amy Zhao, Guha Balakrishnan, Kathleen M. Lewis, Fr\\'edo Durand, John\n  V. Guttag, Adrian V. Dalca", "title": "Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings", "comments": "10 pages, CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new video synthesis task: synthesizing time lapse videos\ndepicting how a given painting might have been created. Artists paint using\nunique combinations of brushes, strokes, and colors. There are often many\npossible ways to create a given painting. Our goal is to learn to capture this\nrich range of possibilities.\n  Creating distributions of long-term videos is a challenge for learning-based\nvideo synthesis methods. We present a probabilistic model that, given a single\nimage of a completed painting, recurrently synthesizes steps of the painting\nprocess. We implement this model as a convolutional neural network, and\nintroduce a novel training scheme to enable learning from a limited dataset of\npainting time lapses. We demonstrate that this model can be used to sample many\ntime steps, enabling long-term stochastic video synthesis. We evaluate our\nmethod on digital and watercolor paintings collected from video websites, and\nshow that human raters find our synthetic videos to be similar to time lapse\nvideos produced by real artists. Our code is available at\nhttps://xamyzhao.github.io/timecraft.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 03:12:38 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 22:20:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhao", "Amy", ""], ["Balakrishnan", "Guha", ""], ["Lewis", "Kathleen M.", ""], ["Durand", "Fr\u00e9do", ""], ["Guttag", "John V.", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2001.01129", "submitter": "Jiju Peethambaran Poovvancheri", "authors": "Aby Thomas, Adarsh Sunilkumar, Shankar Shylesh, Aby Abahai T.,\n  Subhasree Methirumangalath, Dong Chen and Jiju Peethambaran", "title": "TCM-ICP: Transformation Compatibility Measure for Registering Multiple\n  LIDAR Scans", "comments": "9 pages, 8 figures, submitted to IEEE GRSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Rigid registration of multi-view and multi-platform LiDAR scans is a\nfundamental problem in 3D mapping, robotic navigation, and large-scale urban\nmodeling applications. Data acquisition with LiDAR sensors involves scanning\nmultiple areas from different points of view, thus generating partially\noverlapping point clouds of the real world scenes. Traditionally, ICP\n(Iterative Closest Point) algorithm is used to register the acquired point\nclouds together to form a unique point cloud that captures the scanned real\nworld scene. Conventional ICP faces local minima issues and often needs a\ncoarse initial alignment to converge to the optimum. In this work, we present\nan algorithm for registering multiple, overlapping LiDAR scans. We introduce a\ngeometric metric called Transformation Compatibility Measure (TCM) which aids\nin choosing the most similar point clouds for registration in each iteration of\nthe algorithm. The LiDAR scan most similar to the reference LiDAR scan is then\ntransformed using simplex technique. An optimization of the transformation\nusing gradient descent and simulated annealing techniques are then applied to\nimprove the resulting registration. We evaluate the proposed algorithm on four\ndifferent real world scenes and experimental results shows that the\nregistration performance of the proposed method is comparable or superior to\nthe traditionally used registration methods. Further, the algorithm achieves\nsuperior registration results even when dealing with outliers.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 21:05:27 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 17:20:47 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Thomas", "Aby", ""], ["Sunilkumar", "Adarsh", ""], ["Shylesh", "Shankar", ""], ["T.", "Aby Abahai", ""], ["Methirumangalath", "Subhasree", ""], ["Chen", "Dong", ""], ["Peethambaran", "Jiju", ""]]}, {"id": "2001.01870", "submitter": "Haodi Hou", "authors": "Haodi Hou, Jing Huo, Jing Wu, Yu-Kun Lai, and Yang Gao", "title": "MW-GAN: Multi-Warping GAN for Caricature Generation with Multi-Style\n  Geometric Exaggeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an input face photo, the goal of caricature generation is to produce\nstylized, exaggerated caricatures that share the same identity as the photo. It\nrequires simultaneous style transfer and shape exaggeration with rich\ndiversity, and meanwhile preserving the identity of the input. To address this\nchallenging problem, we propose a novel framework called Multi-Warping GAN\n(MW-GAN), including a style network and a geometric network that are designed\nto conduct style transfer and geometric exaggeration respectively. We bridge\nthe gap between the style and landmarks of an image with corresponding latent\ncode spaces by a dual way design, so as to generate caricatures with arbitrary\nstyles and geometric exaggeration, which can be specified either through random\nsampling of latent code or from a given caricature sample. Besides, we apply\nidentity preserving loss to both image space and landmark space, leading to a\ngreat improvement in quality of generated caricatures. Experiments show that\ncaricatures generated by MW-GAN have better quality than existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 03:08:30 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hou", "Haodi", ""], ["Huo", "Jing", ""], ["Wu", "Jing", ""], ["Lai", "Yu-Kun", ""], ["Gao", "Yang", ""]]}, {"id": "2001.02600", "submitter": "Peng Xu", "authors": "Peng Xu, Timothy M. Hospedales, Qiyue Yin, Yi-Zhe Song, Tao Xiang,\n  Liang Wang", "title": "Deep Learning for Free-Hand Sketch: A Survey and A Toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-hand sketches are highly illustrative, and have been widely used by\nhumans to depict objects or stories from ancient times to the present. The\nrecent prevalence of touchscreen devices has made sketch creation a much easier\ntask than ever and consequently made sketch-oriented applications increasingly\npopular. The progress of deep learning has immensely benefited free-hand sketch\nresearch and applications. This paper presents a comprehensive survey of the\ndeep learning techniques oriented at free-hand sketch data, and the\napplications that they enable. The main contents of this survey include: (i) A\ndiscussion of the intrinsic traits and unique challenges of free-hand sketch,\nto highlight the essential differences between sketch data and other data\nmodalities, e.g., natural photos. (ii) A review of the developments of\nfree-hand sketch research in the deep learning era, by surveying existing\ndatasets, research topics, and the state-of-the-art methods through a detailed\ntaxonomy and experimental evaluation. (iii) Promotion of future work via a\ndiscussion of bottlenecks, open problems, and potential research directions for\nthe community. Finally, to support future sketch research and applications, we\ncontribute TorchSketch -- the first sketch-oriented open-source deep learning\nlibrary, which is built on PyTorch and available at\nhttps://github.com/PengBoXiangShang/torchsketch/.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:23:56 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 14:23:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Xu", "Peng", ""], ["Hospedales", "Timothy M.", ""], ["Yin", "Qiyue", ""], ["Song", "Yi-Zhe", ""], ["Xiang", "Tao", ""], ["Wang", "Liang", ""]]}, {"id": "2001.02620", "submitter": "Ingo Wald", "authors": "Ingo Wald, Bruce Cherniak, Will Usher, Carson Brownlee, Attila Afra,\n  Johannes Guenther, Jefferson Amstutz, Tim Rowley, Valerio Pascucci, Chris R\n  Johnson, Jim Jeffers", "title": "Digesting the Elephant -- Experiences with Interactive Production\n  Quality Path Tracing of the Moana Island Scene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New algorithmic and hardware developments over the past two decades have\nenabled interactive ray tracing of small to modest sized scenes, and are\nfinding growing popularity in scientific visualization and games. However,\ninteractive ray tracing has not been as widely explored in the context of\nproduction film rendering, where challenges due to the complexity of the models\nand, from a practical standpoint, their unavailability to the wider research\ncommunity, have posed significant challenges. The recent release of the Disney\nMoana Island Scene has made one such model available to the community for\nexperimentation. In this paper, we detail the challenges posed by this scene to\nan interactive ray tracer, and the solutions we have employed and developed to\nenable interactive path tracing of the scene with full geometric and shading\ndetail, with the goal of providing insight and guidance to other researchers.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:58:52 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Wald", "Ingo", ""], ["Cherniak", "Bruce", ""], ["Usher", "Will", ""], ["Brownlee", "Carson", ""], ["Afra", "Attila", ""], ["Guenther", "Johannes", ""], ["Amstutz", "Jefferson", ""], ["Rowley", "Tim", ""], ["Pascucci", "Valerio", ""], ["Johnson", "Chris R", ""], ["Jeffers", "Jim", ""]]}, {"id": "2001.03537", "submitter": "Chenhao Xie", "authors": "Chenhao Xie, Xin Fu, Mingsong Chen, Shuaiwen Leon Song", "title": "OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future\n  NUMA-Based Multi-GPU Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3307650.3322247", "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the strong computation capability, NUMA-based multi-GPU system is a\npromising candidate to provide sustainable and scalable performance for Virtual\nReality. However, the entire multi-GPU system is viewed as a single GPU which\nignores the data locality in VR rendering during the workload distribution,\nleading to tremendous remote memory accesses among GPU models. By conducting\ncomprehensive characterizations on different kinds of parallel rendering\nframeworks, we observe that distributing the rendering object along with its\nrequired data per GPM can reduce the inter-GPM memory accesses. However, this\nobject-level rendering still faces two major challenges in NUMA-based multi-GPU\nsystem: (1) the large data locality between the left and right views of the\nsame object and the data sharing among different objects and (2) the unbalanced\nworkloads induced by the software-level distribution and composition\nmechanisms. To tackle these challenges, we propose object-oriented VR rendering\nframework (OO-VR) that conducts the software and hardware co-optimization to\nprovide a NUMA friendly solution for VR multi-view rendering in NUMA-based\nmulti-GPU systems. We first propose an object-oriented VR programming model to\nexploit the data sharing between two views of the same object and group objects\ninto batches based on their texture sharing levels. Then, we design an object\naware runtime batch distribution engine and distributed hardware composition\nunit to achieve the balanced workloads among GPMs. Finally, evaluations on our\nVR featured simulator show that OO-VR provides 1.58x overall performance\nimprovement and 76% inter-GPM memory traffic reduction over the\nstate-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly\nperformance scalability for the future larger multi-GPU scenarios with ever\nincreasing asymmetric bandwidth between local and remote memory.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 19:44:51 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Xie", "Chenhao", ""], ["Fu", "Xin", ""], ["Chen", "Mingsong", ""], ["Song", "Shuaiwen Leon", ""]]}, {"id": "2001.03640", "submitter": "Omry Sendik", "authors": "Omry Sendik, Dani Lischinski, Daniel Cohen-Or", "title": "Unsupervised multi-modal Styled Content Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of deep generative models has recently enabled the automatic\ngeneration of massive amounts of graphical content, both in 2D and in 3D.\nGenerative Adversarial Networks (GANs) and style control mechanisms, such as\nAdaptive Instance Normalization (AdaIN), have proved particularly effective in\nthis context, culminating in the state-of-the-art StyleGAN architecture. While\nsuch models are able to learn diverse distributions, provided a sufficiently\nlarge training set, they are not well-suited for scenarios where the\ndistribution of the training data exhibits a multi-modal behavior. In such\ncases, reshaping a uniform or normal distribution over the latent space into a\ncomplex multi-modal distribution in the data domain is challenging, and the\ngenerator might fail to sample the target distribution well. Furthermore,\nexisting unsupervised generative models are not able to control the mode of the\ngenerated samples independently of the other visual attributes, despite the\nfact that they are typically disentangled in the training data.\n  In this paper, we introduce UMMGAN, a novel architecture designed to better\nmodel multi-modal distributions, in an unsupervised fashion. Building upon the\nStyleGAN architecture, our network learns multiple modes, in a completely\nunsupervised manner, and combines them using a set of learned weights. We\ndemonstrate that this approach is capable of effectively approximating a\ncomplex distribution as a superposition of multiple simple ones. We further\nshow that UMMGAN effectively disentangles between modes and style, thereby\nproviding an independent degree of control over the generated content.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 19:36:08 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:14:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sendik", "Omry", ""], ["Lischinski", "Dani", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2001.04528", "submitter": "Jorge Alberto Gutierrez Ortega", "authors": "Jorge Gutierrez, Julien Rabin, Bruno Galerne, Thomas Hurtut", "title": "On Demand Solid Texture Synthesis Using Deep 3D Networks", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13889", "report-no": null, "categories": "cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach for on demand volumetric texture\nsynthesis based on a deep learning framework that allows for the generation of\nhigh quality 3D data at interactive rates. Based on a few example images of\ntextures, a generative network is trained to synthesize coherent portions of\nsolid textures of arbitrary sizes that reproduce the visual characteristics of\nthe examples along some directions. To cope with memory limitations and\ncomputation complexity that are inherent to both high resolution and 3D\nprocessing on the GPU, only 2D textures referred to as \"slices\" are generated\nduring the training stage. These synthetic textures are compared to exemplar\nimages via a perceptual loss function based on a pre-trained deep network. The\nproposed network is very light (less than 100k parameters), therefore it only\nrequires sustainable training (i.e. few hours) and is capable of very fast\ngeneration (around a second for $256^3$ voxels) on a single GPU. Integrated\nwith a spatially seeded PRNG the proposed generator network directly returns an\nRGB value given a set of 3D coordinates. The synthesized volumes have good\nvisual results that are at least equivalent to the state-of-the-art patch based\napproaches. They are naturally seamlessly tileable and can be fully generated\nin parallel.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 20:59:14 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Gutierrez", "Jorge", ""], ["Rabin", "Julien", ""], ["Galerne", "Bruno", ""], ["Hurtut", "Thomas", ""]]}, {"id": "2001.04947", "submitter": "Lingjie Liu", "authors": "Lingjie Liu, Weipeng Xu, Marc Habermann, Michael Zollhoefer, Florian\n  Bernard, Hyeongwoo Kim, Wenping Wang, Christian Theobalt", "title": "Neural Human Video Rendering by Learning Dynamic Textures and\n  Rendering-to-Video Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic videos of humans using neural networks has been a\npopular alternative to the conventional graphics-based rendering pipeline due\nto its high efficiency. Existing works typically formulate this as an\nimage-to-image translation problem in 2D screen space, which leads to artifacts\nsuch as over-smoothing, missing body parts, and temporal instability of\nfine-scale detail, such as pose-dependent wrinkles in the clothing. In this\npaper, we propose a novel human video synthesis method that approaches these\nlimiting factors by explicitly disentangling the learning of time-coherent\nfine-scale details from the embedding of the human in 2D screen space. More\nspecifically, our method relies on the combination of two convolutional neural\nnetworks (CNNs). Given the pose information, the first CNN predicts a dynamic\ntexture map that contains time-coherent high-frequency details, and the second\nCNN conditions the generation of the final video on the temporally coherent\noutput of the first CNN. We demonstrate several applications of our approach,\nsuch as human reenactment and novel view synthesis from monocular video, where\nwe show significant improvement over the state of the art both qualitatively\nand quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:06:27 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 17:29:29 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 21:08:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Lingjie", ""], ["Xu", "Weipeng", ""], ["Habermann", "Marc", ""], ["Zollhoefer", "Michael", ""], ["Bernard", "Florian", ""], ["Kim", "Hyeongwoo", ""], ["Wang", "Wenping", ""], ["Theobalt", "Christian", ""]]}, {"id": "2001.05201", "submitter": "Wayne Wu", "authors": "Linsen Song, Wayne Wu, Chen Qian, Ran He, Chen Change Loy", "title": "Everybody's Talkin': Let Me Talk as You Want", "comments": "Technical report. Project page:\n  https://wywu.github.io/projects/EBT/EBT.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to edit a target portrait footage by taking a sequence of\naudio as input to synthesize a photo-realistic video. This method is unique\nbecause it is highly dynamic. It does not assume a person-specific rendering\nnetwork yet capable of translating arbitrary source audio into arbitrary video\noutput. Instead of learning a highly heterogeneous and nonlinear mapping from\naudio to the video directly, we first factorize each target video frame into\northogonal parameter spaces, i.e., expression, geometry, and pose, via\nmonocular 3D face reconstruction. Next, a recurrent network is introduced to\ntranslate source audio into expression parameters that are primarily related to\nthe audio content. The audio-translated expression parameters are then used to\nsynthesize a photo-realistic human subject in each video frame, with the\nmovement of the mouth regions precisely mapped to the source audio. The\ngeometry and pose parameters of the target human portrait are retained,\ntherefore preserving the context of the original video footage. Finally, we\nintroduce a novel video rendering network and a dynamic programming method to\nconstruct a temporally coherent and photo-realistic video. Extensive\nexperiments demonstrate the superiority of our method over existing approaches.\nOur method is end-to-end learnable and robust to voice variations in the source\naudio.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 09:54:23 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Song", "Linsen", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["He", "Ran", ""], ["Loy", "Chen Change", ""]]}, {"id": "2001.09352", "submitter": "Juuso Haavisto", "authors": "Juuso Haavisto and Jukka Riekki", "title": "Interoperable GPU Kernels as Latency Improver for MEC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality (MR) applications are expected to become common when 5G goes\nmainstream. However, the latency requirements are challenging to meet due to\nthe resources required by video-based remoting of graphics, that is, decoding\nvideo codecs. We propose an approach towards tackling this challenge: a\nclient-server implementation for transacting intermediate representation (IR)\nbetween a mobile UE and a MEC server instead of video codecs and this way\navoiding video decoding. We demonstrate the ability to address latency\nbottlenecks on edge computing workloads that transact graphics. We select\nSPIR-V compatible GPU kernels as the intermediate representation. Our approach\nrequires know-how in GPU architecture and GPU domain-specific languages (DSLs),\nbut compared to video-based edge graphics, it decreases UE device delay by\nsevenfold. Further, we find that due to low cold-start times on both UEs and\nMEC servers, application migration can happen in milliseconds. We imply that\ngraphics-based location-aware applications, such as MR, can benefit from this\nkind of approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 19:07:58 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Haavisto", "Juuso", ""], ["Riekki", "Jukka", ""]]}, {"id": "2001.09421", "submitter": "Xiaowei He", "authors": "Xiaowei He, Huamin Wang, Guoping Wang, Hongan Wang and Enhua Wu", "title": "A Variational Staggered Particle Framework for Incompressible\n  Free-Surface Flows", "comments": "15 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed particle hydrodynamics (SPH) has been extensively studied in\ncomputer graphics to animate fluids with versatile effects. However, SPH still\nsuffers from two numerical difficulties: the particle deficiency problem, which\nwill deteriorate the simulation accuracy, and the particle clumping problem,\nwhich usually leads to poor stability of particle simulations. We propose to\nsolve these two problems by developing an approximate projection method for\nincompressible free-surface flows under a variational staggered particle\nframework. After particle discretization, we first categorize all fluid\nparticles into four subsets. Then according to the classification, we propose\nto solve the particle deficiency problem by analytically imposing free surface\nboundary conditions on both the Laplacian operator and the source term. To\naddress the particle clumping problem, we propose to extend the Taylor-series\nconsistent pressure gradient model with kernel function correction and\nsemi-analytical boundary conditions. Compared to previous approximate\nprojection method [1], our incompressibility solver is stable under both\ncompressive and tensile stress states, no pressure clumping or iterative\ndensity correction (e.g., a density constrained pressure approach) is necessary\nto stabilize the solver anymore. Motivated by the Helmholtz free energy\nfunctional, we additionally introduce an iterative particle shifting algorithm\nto improve the accuracy. It significantly reduces particle splashes near the\nfree surface. Therefore, high-fidelity simulations of the formation and\nfragmentation of liquid jets and sheets are obtained for both the two-jets and\nmilk-crown examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 08:37:11 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["He", "Xiaowei", ""], ["Wang", "Huamin", ""], ["Wang", "Guoping", ""], ["Wang", "Hongan", ""], ["Wu", "Enhua", ""]]}, {"id": "2001.09792", "submitter": "Alexander Hirsch", "authors": "Alexander Hirsch, Peter Thoman", "title": "Running on Raygun", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the introduction of Nvidia RTX hardware, ray tracing is now viable as a\ngeneral real time rendering technique for complex 3D scenes. Leveraging this\nnew technology, we present Raygun, an open source rendering, simulation, and\ngame engine focusing on simplicity, expandability, and the topic of ray tracing\nrealized through Nvidia's Vulkan ray tracing extension.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 13:58:19 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hirsch", "Alexander", ""], ["Thoman", "Peter", ""]]}, {"id": "2001.10472", "submitter": "Yiqun Wang", "authors": "Yiqun Wang, Jing Ren, Dong-Ming Yan, Jianwei Guo, Xiaopeng Zhang,\n  Peter Wonka", "title": "MGCN: Descriptor Learning using Multiscale GCNs", "comments": "Accepted to SIGGRAPH 2020. (15 pages, 15 figures, 12 tables,\n  low-resolution version)", "journal-ref": "ACM Transactions on Graphics (Proceedings of SIGGRAPH), Vol. 39,\n  No. 4, Article 122. Publication date: July 2020", "doi": "10.1145/3386569.3392443", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for computing descriptors for characterizing\npoints on three-dimensional surfaces. First, we present a new non-learned\nfeature that uses graph wavelets to decompose the Dirichlet energy on a\nsurface. We call this new feature wavelet energy decomposition signature\n(WEDS). Second, we propose a new multiscale graph convolutional network (MGCN)\nto transform a non-learned feature to a more discriminative descriptor. Our\nresults show that the new descriptor WEDS is more discriminative than the\ncurrent state-of-the-art non-learned descriptors and that the combination of\nWEDS and MGCN is better than the state-of-the-art learned descriptors. An\nimportant design criterion for our descriptor is the robustness to different\nsurface discretizations including triangulations with varying numbers of\nvertices. Our results demonstrate that previous graph convolutional networks\nsignificantly overfit to a particular resolution or even a particular\ntriangulation, but MGCN generalizes well to different surface discretizations.\nIn addition, MGCN is compatible with previous descriptors and it can also be\nused to improve the performance of other descriptors, such as the heat kernel\nsignature, the wave kernel signature, or the local point signature.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:25:14 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 21:57:40 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 10:18:28 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wang", "Yiqun", ""], ["Ren", "Jing", ""], ["Yan", "Dong-Ming", ""], ["Guo", "Jianwei", ""], ["Zhang", "Xiaopeng", ""], ["Wonka", "Peter", ""]]}, {"id": "2001.10585", "submitter": "Duygu Sap", "authors": "Duygu Sap and Daniel P. Szabo", "title": "An Automated Approach for the Discovery of Interoperability", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present an automated approach that would test for and\ndiscover the interoperability of CAD systems based on the\napproximately-invariant shape properties of their models. We further show that\nexchanging models in standard format does not guarantee the preservation of\nshape properties. Our analysis is based on utilizing queries in deriving the\nshape properties and constructing the proxy models of the given CAD models [1].\nWe generate template files to accommodate the information necessary for the\nproperty computations and proxy model constructions, and implement an\ninteroperability discovery program called DTest to execute the interoperability\ntesting. We posit that our method could be extended to interoperability testing\non CAD-to-CAE and/or CAD-to-CAM interactions by modifying the set of property\nchecks and providing the additional requirements that may emerge in CAE or CAM\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 06:07:43 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Sap", "Duygu", ""], ["Szabo", "Daniel P.", ""]]}, {"id": "2001.11131", "submitter": "Jason R.C. Nurse Dr", "authors": "Meredydd Williams, Kelvin K. K. Yao, Jason R. C. Nurse", "title": "Developing an Augmented Reality Tourism App through User-Centred Design\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.GR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) bridges the gap between the physical and virtual\nworld. Through overlaying graphics on natural environments, users can immerse\nthemselves in a tailored environment. This offers great benefits to mobile\ntourism, where points of interest (POIs) can be annotated on a smartphone\nscreen. While a variety of apps currently exist, usability issues can\ndiscourage users from embracing AR. Interfaces can become cluttered with icons,\nwith POI occlusion posing further challenges. In this paper, we use\nuser-centred design (UCD) to develop an AR tourism app. We solicit requirements\nthrough a synthesis of domain analysis, tourist observation and semi-structured\ninterviews. Whereas previous user-centred work has designed mock-ups, we\niteratively develop a full Android app. This includes overhead maps and route\nnavigation, in addition to a detailed AR browser. The final product is\nevaluated by 20 users, who participate in a tourism task in a UK city. Users\nregard the system as usable and intuitive, and suggest the addition of further\ncustomisation. We finish by critically analysing the challenges of a\nuser-centred methodology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 23:35:32 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Williams", "Meredydd", ""], ["Yao", "Kelvin K. K.", ""], ["Nurse", "Jason R. C.", ""]]}]