[{"id": "1911.00189", "submitter": "Yi-Ling Qiao", "authors": "Yi-Ling Qiao, Lin Gao, Shu-Zhi Liu, Ligang Liu, Yu-Kun Lai, Xilin Chen", "title": "Learning-based Real-time Detection of Intrinsic Reflectional Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflectional symmetry is ubiquitous in nature. While extrinsic reflectional\nsymmetry can be easily parametrized and detected, intrinsic symmetry is much\nharder due to the high solution space. Previous works usually solve this\nproblem by voting or sampling, which suffer from high computational cost and\nrandomness. In this paper, we propose \\YL{a} learning-based approach to\nintrinsic reflectional symmetry detection. Instead of directly finding\nsymmetric point pairs, we parametrize this self-isometry using a functional map\nmatrix, which can be easily computed given the signs of Laplacian\neigenfunctions under the symmetric mapping. Therefore, we train a novel deep\nneural network to predict the sign of each eigenfunction under symmetry, which\nin addition takes the first few eigenfunctions as intrinsic features to\ncharacterize the mesh while avoiding coping with the connectivity explicitly.\nOur network aims at learning the global property of functions, and consequently\nconverts the problem defined on the manifold to the functional domain. By\ndisentangling the prediction of the matrix into separated basis, our method\ngeneralizes well to new shapes and is invariant under perturbation of\neigenfunctions. Through extensive experiments, we demonstrate the robustness of\nour method in challenging cases, including different topology and incomplete\nshapes with holes. By avoiding random sampling, our learning-based algorithm is\nover 100 times faster than state-of-the-art methods, and meanwhile, is more\nrobust, achieving higher correspondence accuracy in commonly used metrics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 02:58:45 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Qiao", "Yi-Ling", ""], ["Gao", "Lin", ""], ["Liu", "Shu-Zhi", ""], ["Liu", "Ligang", ""], ["Lai", "Yu-Kun", ""], ["Chen", "Xilin", ""]]}, {"id": "1911.00193", "submitter": "Mingliang Xu", "authors": "Chaochao Li, Pei Lv, Mingliang Xu, Xinyu Wang, Dinesh Manocha, Bing\n  Zhou, and Meng Wang", "title": "Personality-Aware Probabilistic Map for Trajectory Prediction of\n  Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel trajectory prediction algorithm for pedestrians based on a\npersonality-aware probabilistic feature map. This map is computed using a\nspatial query structure and each value represents the probability of the\npredicted pedestrian passing through various positions in the crowd space. We\nupdate this map dynamically based on the agents in the environment and prior\ntrajectory of a pedestrian. Furthermore, we estimate the personality\ncharacteristics of each pedestrian and use them to improve the prediction by\nestimating the shortest path in this map. Our approach is general and works\nwell on crowd videos with low and high pedestrian density. We evaluate our\nmodel on standard human-trajectory datasets. In practice, our prediction\nalgorithm improves the accuracy by 5-9% over prior algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 03:31:49 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Li", "Chaochao", ""], ["Lv", "Pei", ""], ["Xu", "Mingliang", ""], ["Wang", "Xinyu", ""], ["Manocha", "Dinesh", ""], ["Zhou", "Bing", ""], ["Wang", "Meng", ""]]}, {"id": "1911.00767", "submitter": "Shunsuke Saito", "authors": "Shichen Liu, Shunsuke Saito, Weikai Chen, Hao Li", "title": "Learning to Infer Implicit Surfaces without 3D Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in 3D deep learning have shown that it is possible to train\nhighly effective deep models for 3D shape generation, directly from 2D images.\nThis is particularly interesting since the availability of 3D models is still\nlimited compared to the massive amount of accessible 2D images, which is\ninvaluable for training. The representation of 3D surfaces itself is a key\nfactor for the quality and resolution of the 3D output. While explicit\nrepresentations, such as point clouds and voxels, can span a wide range of\nshape variations, their resolutions are often limited. Mesh-based\nrepresentations are more efficient but are limited by their ability to handle\nvarying topologies. Implicit surfaces, however, can robustly handle complex\nshapes, topologies, and also provide flexible resolution control. We address\nthe fundamental problem of learning implicit surfaces for shape inference\nwithout the need of 3D supervision. Despite their advantages, it remains\nnontrivial to (1) formulate a differentiable connection between implicit\nsurfaces and their 2D renderings, which is needed for image-based supervision;\nand (2) ensure precise geometric properties and control, such as local\nsmoothness. In particular, sampling implicit surfaces densely is also known to\nbe a computationally demanding and very slow operation. To this end, we propose\na novel ray-based field probing technique for efficient image-to-field\nsupervision, as well as a general geometric regularizer for implicit surfaces,\nwhich provides natural shape priors in unconstrained regions. We demonstrate\nthe effectiveness of our framework on the task of single-view image-based 3D\nshape digitization and show how we outperform state-of-the-art techniques both\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 19:05:23 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Liu", "Shichen", ""], ["Saito", "Shunsuke", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "1911.01911", "submitter": "Maximilian Denninger", "authors": "Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef\n  Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, Harinandan Katam", "title": "BlenderProc", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BlenderProc is a modular procedural pipeline, which helps in generating real\nlooking images for the training of convolutional neural networks. These can be\nused in a variety of use cases including segmentation, depth, normal and pose\nestimation and many others. A key feature of our extension of blender is the\nsimple to use modular pipeline, which was designed to be easily extendable. By\noffering standard modules, which cover a variety of scenarios, we provide a\nstarting point on which new modules can be created.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 16:53:12 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Denninger", "Maximilian", ""], ["Sundermeyer", "Martin", ""], ["Winkelbauer", "Dominik", ""], ["Zidan", "Youssef", ""], ["Olefir", "Dmitry", ""], ["Elbadrawy", "Mohamad", ""], ["Lodhi", "Ahsan", ""], ["Katam", "Harinandan", ""]]}, {"id": "1911.03117", "submitter": "Matthieu Heitz", "authors": "Matthieu Heitz, Nicolas Bonneel, David Coeurjolly, Marco Cuturi,\n  Gabriel Peyr\\'e", "title": "Ground Metric Learning on Graphs", "comments": "Fixed sign of gradient", "journal-ref": "Journal of Mathematical Imaging and Vision (2020): 1-19", "doi": "10.1007/s10851-020-00996-z", "report-no": null, "categories": "stat.ML cs.GR cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport (OT) distances between probability distributions are\nparameterized by the ground metric they use between observations. Their\nrelevance for real-life applications strongly hinges on whether that ground\nmetric parameter is suitably chosen. Selecting it adaptively and\nalgorithmically from prior knowledge, the so-called ground metric learning GML)\nproblem, has therefore appeared in various settings. We consider it in this\npaper when the learned metric is constrained to be a geodesic distance on a\ngraph that supports the measures of interest. This imposes a rich structure for\ncandidate metrics, but also enables far more efficient learning procedures when\ncompared to a direct optimization over the space of all metric matrices. We use\nthis setting to tackle an inverse problem stemming from the observation of a\ndensity evolving with time: we seek a graph ground metric such that the OT\ninterpolation between the starting and ending densities that result from that\nground metric agrees with the observed evolution. This OT dynamic framework is\nrelevant to model natural phenomena exhibiting displacements of mass, such as\nfor instance the evolution of the color palette induced by the modification of\nlighting and materials.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 08:27:19 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 12:14:11 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 15:17:39 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Heitz", "Matthieu", ""], ["Bonneel", "Nicolas", ""], ["Coeurjolly", "David", ""], ["Cuturi", "Marco", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1911.04446", "submitter": "Ufuk Celikcan", "authors": "Emre Avan, Ufuk Celikcan, Tolga K. Capin, and Hasmet Gurcay", "title": "Enhancing User Experience in Virtual Reality with Radial Basis Function\n  Interpolation Based Stereoscopic Camera Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing a depth-rich Virtual Reality (VR) experience to users without\ncausing discomfort remains to be a challenge with today's commercially\navailable head-mounted displays (HMDs), which enforce strict measures on\nstereoscopic camera parameters for the sake of keeping visual discomfort to a\nminimum. However, these measures often lead to an unimpressive VR experience\nwith shallow depth feeling. We propose the first method ready to be used with\nexisting consumer HMDs for automated stereoscopic camera control in virtual\nenvironments (VEs). Using radial basis function interpolation and projection\nmatrix manipulations, our method makes it possible to significantly enhance\nuser experience in terms of overall perceived depth while maintaining visual\ndiscomfort on a par with the default arrangement. In our implementation, we\nalso introduce the first immersive interface for authoring a unique 3D\nstereoscopic cinematography for any VE to be experienced with consumer HMDs. We\nconducted a user study that demonstrates the benefits of our approach in terms\nof superior picture quality and perceived depth. We also investigated the\neffects of using depth of field (DoF) in combination with our approach and\nobserved that the addition of our DoF implementation was seen as a degraded\nexperience, if not similar.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:49:49 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Avan", "Emre", ""], ["Celikcan", "Ufuk", ""], ["Capin", "Tolga K.", ""], ["Gurcay", "Hasmet", ""]]}, {"id": "1911.05204", "submitter": "Hsiao-Yu Chen", "authors": "Hsiao-yu Chen, Paul Kry, Etienne Vouga", "title": "Locking-free Simulation of Isometric Thin Plates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently simulate very thin, inextensible materials like cloth or\npaper, it is tempting to replace force-based thin-plate dynamics with hard\nisometry constraints. Unfortunately, naive formulations of the constraints\ninduce membrane locking---artificial stiffening of bending modes due to the\ninability of discrete kinematics to reproduce exact isometries. We propose a\nsimple set of meshless isometry constraints, based on moving-least-squares\naveraging of the strain tensor, which do not lock, and which can be easily\nincorporated into standard constrained Lagrangian dynamics integration.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 23:35:59 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Chen", "Hsiao-yu", ""], ["Kry", "Paul", ""], ["Vouga", "Etienne", ""]]}, {"id": "1911.05668", "submitter": "Teodoro Collin", "authors": "Teodoro Collin, Charisee Chiw, L. Ridgway Scott, John Reppy, and\n  Gordon L. Kindlmann", "title": "Point Movement in a DSL for Higher-Order FEM Visualization", "comments": "Appeared at IEEE Visualization 2019", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933623", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific visualization tools tend to be flexible in some ways (e.g., for\nexploring isovalues) while restricted in other ways, such as working only on\nregular grids, or only on unstructured meshes (as used in the finite element\nmethod, FEM). Our work seeks to expose the common structure of visualization\nmethods, apart from the specifics of how the fields being visualized are\nformed. Recognizing that previous approaches to FEM visualization depend on\nefficiently updating computed positions within a mesh, we took an existing\nvisualization domain-specific language, and added a mesh position type and\nassociated arithmetic operators. These are orthogonal to the visualization\nmethod itself, so existing programs for visualizing regular grid data work,\nwith minimal changes, on higher-order FEM data. We reproduce the efficiency\ngains of an earlier guided search method of mesh position update for computing\nstreamlines, and we demonstrate a novel ability to uniformly sample ridge\nsurfaces of higher-order FEM solutions defined on curved meshes.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 20:17:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Collin", "Teodoro", ""], ["Chiw", "Charisee", ""], ["Scott", "L. Ridgway", ""], ["Reppy", "John", ""], ["Kindlmann", "Gordon L.", ""]]}, {"id": "1911.05992", "submitter": "Sylvain Lefebvre", "authors": "Sylvain Lefebvre (MFX)", "title": "Efficient Direct Slicing Of Dilated And Eroded 3d Models For Additive\n  Manufacturing: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of additive manufacturing we present a novel technique for\ndirect slicing of a dilated or eroded volume, where the input volume boundary\nis a triangle mesh. Rather than computing a 3D model of the boundary of the\ndilated or eroded volume, our technique directly produces its slices. This\nleads to a computationally and memory efficient algorithm, which is\nembarrassingly parallel. Contours can be extracted under an arbitrary chord\nerror, non-uniform dilation or erosion are also possible. Finally, the scheme\nis simple and robust to implement.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:37:21 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Lefebvre", "Sylvain", "", "MFX"]]}, {"id": "1911.06001", "submitter": "Asbj{\\o}rn Engmark Espe", "authors": "Asbj{\\o}rn Engmark Espe, {\\O}ystein Gjermundnes, and Sverre Hendseth", "title": "Efficient Animation of Sparse Voxel Octrees for Real-Time Ray Tracing", "comments": "Preprint of an article submitted for review to IEEE Transactions on\n  Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable limitation of employing sparse voxels octrees (SVOs) as a\nmodel format for ray tracing has been that the octree data structure is\ninherently static. Due to traversal algorithms' dependence on the strict\nhierarchical structure of octrees, it has been challenging to achieve real-time\nperformance of SVO model animation in ray tracing since the octree data\nstructure would typically have to be regenerated every frame. Presented in this\narticle is a novel method for animation of models specified on the SVO format.\nThe method distinguishes itself by permitting model transformations such as\nrotation, translation, and anisotropic scaling, while preserving the\nhierarchical structure of SVO models so that they may be efficiently traversed.\nDue to its modest memory footprint and straightforward arithmetic operations,\nthe method is well-suited for implementation in hardware. A software ray\ntracing implementation of animated SVO models demonstrates real-time\nperformance on current-generation desktop GPUs, and shows that the animation\nmethod does not substantially slow down the rendering procedure compared to\nrendering static SVOs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:58:23 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 16:58:30 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Espe", "Asbj\u00f8rn Engmark", ""], ["Gjermundnes", "\u00d8ystein", ""], ["Hendseth", "Sverre", ""]]}, {"id": "1911.06144", "submitter": "Jisu Kim", "authors": "Jisu Kim, Young J. Kim", "title": "A Penetration Metric for Deforming Tetrahedra using Object Norm", "comments": "Published in IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel penetration metric, called deformable\npenetration depth PDd, to define a measure of inter-penetration between two\nlinearly deforming tetrahedra using the object norm. First of all, we show that\na distance metric for a tetrahedron deforming between two configurations can be\nfound in closed form based on object norm. Then, we show that the PDd between\nan intersecting pair of static and deforming tetrahedra can be found by solving\na quadratic programming (QP) problem in terms of the distance metric with\nnon-penetration constraints. We also show that the PDd between two,\nintersected, deforming tetrahedra can be found by solving a similar QP problem\nunder some assumption on penetrating directions, and it can be also accelerated\nby an order of magnitude using pre-calculated penetration direction. We have\nimplemented our algorithm on a standard PC platform using an off-the-shelf QP\noptimizer, and experimentally show that both the static/deformable and\ndeformable/deformable tetrahedra cases can be solvable in from a few to tens of\nmilliseconds. Finally, we demonstrate that our penetration metric is\nthree-times smaller (or tighter) than the classical, rigid penetration depth\nmetric in our experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 05:34:48 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Kim", "Jisu", ""], ["Kim", "Young J.", ""]]}, {"id": "1911.06245", "submitter": "Zhenyu Tang", "authors": "Zhenyu Tang, Nicholas J. Bryan, Dingzeyu Li, Timothy R. Langlois,\n  Dinesh Manocha", "title": "Scene-Aware Audio Rendering via Deep Acoustic Analysis", "comments": "Accepted to IEEE VR 2020 Journal Track (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.GR cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to capture the acoustic characteristics of real-world\nrooms using commodity devices, and use the captured characteristics to generate\nsimilar sounding sources with virtual models. Given the captured audio and an\napproximate geometric model of a real-world room, we present a novel\nlearning-based method to estimate its acoustic material properties. Our\napproach is based on deep neural networks that estimate the reverberation time\nand equalization of the room from recorded audio. These estimates are used to\ncompute material properties related to room reverberation using a novel\nmaterial optimization objective. We use the estimated acoustic material\ncharacteristics for audio rendering using interactive geometric sound\npropagation and highlight the performance on many real-world scenarios. We also\nperform a user study to evaluate the perceptual similarity between the recorded\nsounds and our rendered audio.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 17:04:00 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 19:20:58 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Tang", "Zhenyu", ""], ["Bryan", "Nicholas J.", ""], ["Li", "Dingzeyu", ""], ["Langlois", "Timothy R.", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1911.06464", "submitter": "Kaustav Chakraborty", "authors": "Michael Maring and Kaustav Chakraborty", "title": "Multiple Style-Transfer in Real-Time", "comments": "Authors agreed that there is not much novelty in the work so\n  presented", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer aims to combine the content of one image with the artistic\nstyle of another. It was discovered that lower levels of convolutional networks\ncaptured style information, while higher levels captures content information.\nThe original style transfer formulation used a weighted combination of VGG-16\nlayer activations to achieve this goal. Later, this was accomplished in\nreal-time using a feed-forward network to learn the optimal combination of\nstyle and content features from the respective images. The first aim of our\nproject was to introduce a framework for capturing the style from several\nimages at once. We propose a method that extends the original real-time style\ntransfer formulation by combining the features of several style images. This\nmethod successfully captures color information from the separate style images.\nThe other aim of our project was to improve the temporal style continuity from\nframe to frame. Accordingly, we have experimented with the temporal stability\nof the output images and discussed the various available techniques that could\nbe employed as alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 03:49:41 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 03:56:40 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Maring", "Michael", ""], ["Chakraborty", "Kaustav", ""]]}, {"id": "1911.06877", "submitter": "Zhenyi He", "authors": "Zhenyi He, Karl Rosenberg, Ken Perlin", "title": "Exploring Configurations for Multi-user Communication in Virtual Reality", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) enables users to collaborate while exploring scenarios\nnot realizable in the physical world. We propose CollabVR, a distributed\nmulti-user collaboration environment, to explore how digital content improves\nexpression and understanding of ideas among groups. To achieve this, we\ndesigned and examined three possible configurations for participants and shared\nmanipulable objects. In configuration (1), participants stand side-by-side. In\n(2), participants are positioned across from each other, mirrored face-to-face.\nIn (3), called \"eyes-free,\" participants stand side-by-side looking at a shared\ndisplay, and draw upon a horizontal surface. We also explored a \"telepathy\"\nmode, in which participants could see from each other's point of view. We\nimplemented \"3DSketch\" visual objects for participants to manipulate and move\nbetween virtual content boards in the environment. To evaluate the system, we\nconducted a study in which four people at a time used each of the three\nconfigurations to cooperate and communicate ideas with each other. We have\nprovided experimental results and interview responses.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:15:46 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["He", "Zhenyi", ""], ["Rosenberg", "Karl", ""], ["Perlin", "Ken", ""]]}, {"id": "1911.06906", "submitter": "Kinga Kruppa", "authors": "Kinga Kruppa", "title": "Applying Rational Envelope curves for skinning purposes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Special curves in the Minkowski space such as Minkowski Pythagorean\nhodographs play an important role in Computer Aided Geometric Design, and their\nusages have been thoroughly studied in the recent years. Also, several papers\nhave been published which describe methods for interpolating Hermite data in\nR2,1 by MPH curves. Bizzarri et al.introduced the class of RE curves and\npresented an interpolation method for G1 Hermite data, where the resulting RE\ncurve yields a rational boundary for the represented domain. We now propose a\nnew application area for RE curves: skinning of a discrete set of input\ncircles. We find the appropriate Hermite data to interpolate so that the\nobtained rational envelope curves touch each circle at previously defined\npoints of contact. This way we overcome the problematic scenarios when the\nlocation of the touching points would not be appropriate for skinning purposes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 23:17:09 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Kruppa", "Kinga", ""]]}, {"id": "1911.06971", "submitter": "Zhiqin Chen", "authors": "Zhiqin Chen and Andrea Tagliasacchi and Hao Zhang", "title": "BSP-Net: Generating Compact Meshes via Binary Space Partitioning", "comments": "CVPR 2020 Best Student Paper Award. Project page:\n  https://bsp-net.github.io, Code:\n  https://github.com/czq142857/BSP-NET-original", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polygonal meshes are ubiquitous in the digital 3D domain, yet they have only\nplayed a minor role in the deep learning revolution. Leading methods for\nlearning generative models of shapes rely on implicit functions, and generate\nmeshes only after expensive iso-surfacing routines. To overcome these\nchallenges, we are inspired by a classical spatial data structure from computer\ngraphics, Binary Space Partitioning (BSP), to facilitate 3D learning. The core\ningredient of BSP is an operation for recursive subdivision of space to obtain\nconvex sets. By exploiting this property, we devise BSP-Net, a network that\nlearns to represent a 3D shape via convex decomposition. Importantly, BSP-Net\nis unsupervised since no convex shape decompositions are needed for training.\nThe network is trained to reconstruct a shape using a set of convexes obtained\nfrom a BSP-tree built on a set of planes. The convexes inferred by BSP-Net can\nbe easily extracted to form a polygon mesh, without any need for iso-surfacing.\nThe generated meshes are compact (i.e., low-poly) and well suited to represent\nsharp geometry; they are guaranteed to be watertight and can be easily\nparameterized. We also show that the reconstruction quality by BSP-Net is\ncompetitive with state-of-the-art methods while using much fewer primitives.\nCode is available at https://github.com/czq142857/BSP-NET-original.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 06:25:26 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 07:07:37 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 17:58:07 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 23:41:51 GMT"}, {"version": "v5", "created": "Mon, 23 Nov 2020 22:12:27 GMT"}, {"version": "v6", "created": "Mon, 7 Dec 2020 20:02:19 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Zhiqin", ""], ["Tagliasacchi", "Andrea", ""], ["Zhang", "Hao", ""]]}, {"id": "1911.07913", "submitter": "Minchen Li", "authors": "Xinlei Wang, Minchen Li, Yu Fang, Xinxin Zhang, Ming Gao, Min Tang,\n  Danny M. Kaufman, Chenfanfu Jiang", "title": "Hierarchical Optimization Time Integration for CFL-rate MPM Stepping", "comments": null, "journal-ref": "ACM Transactions on Graphics 2020", "doi": "10.1145/3386760", "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Hierarchical Optimization Time Integration (HOT) for efficient\nimplicit time-stepping of the Material Point Method (MPM) irrespective of\nsimulated materials and conditions. HOT is an MPM-specialized hierarchical\noptimization algorithm that solves nonlinear time step problems for large-scale\nMPM systems near the CFL-limit. HOT provides convergent simulations\n\"out-of-the-box\" across widely varying materials and computational resolutions\nwithout parameter tuning. As an implicit MPM time stepper accelerated by a\ncustom-designed Galerkin multigrid wrapped in a quasi-Newton solver, HOT is\nboth highly parallelizable and robustly convergent. As we show in our analysis,\nHOT maintains consistent and efficient performance even as we grow stiffness,\nincrease deformation, and vary materials over a wide range of finite strain,\nelastodynamic and plastic examples. Through careful benchmark ablation studies,\nwe compare the effectiveness of HOT against seemingly plausible alternative\ncombinations of MPM with standard multigrid and other Newton-Krylov models. We\nshow how these alternative designs result in severe issues and poor\nperformance. In contrast, HOT outperforms the existing state-of-the-art,\nheavily optimized implicit MPM codes with an up to 10x performance speedup\nacross a wide range of challenging benchmark test simulations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:20:32 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 17:00:55 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 13:57:27 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Wang", "Xinlei", ""], ["Li", "Minchen", ""], ["Fang", "Yu", ""], ["Zhang", "Xinxin", ""], ["Gao", "Ming", ""], ["Tang", "Min", ""], ["Kaufman", "Danny M.", ""], ["Jiang", "Chenfanfu", ""]]}, {"id": "1911.08348", "submitter": "Oran Gafni", "authors": "Oran Gafni, Lior Wolf, Yaniv Taigman", "title": "Live Face De-Identification in Video", "comments": "ICCV 2019", "journal-ref": "Proceedings of the IEEE International Conference on Computer\n  Vision (2019) 9378--9387", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for face de-identification that enables fully automatic\nvideo modification at high frame rates. The goal is to maximally decorrelate\nthe identity, while having the perception (pose, illumination and expression)\nfixed. We achieve this by a novel feed-forward encoder-decoder network\narchitecture that is conditioned on the high-level representation of a person's\nfacial image. The network is global, in the sense that it does not need to be\nretrained for a given video or for a given identity, and it creates natural\nlooking image sequences with little distortion in time.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 15:28:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Gafni", "Oran", ""], ["Wolf", "Lior", ""], ["Taigman", "Yaniv", ""]]}, {"id": "1911.08591", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Alex Colburn, Gary Faigin, Linda Shapiro, and Barbara\n  Mones", "title": "Learning Stylized Character Expressions from Humans", "comments": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Women in Computer Vision (WiCV) Workshop Honolulu, Hawaii, USA, July\n  21st - July 26th, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DeepExpr, a novel expression transfer system from humans to\nmultiple stylized characters via deep learning. We developed : 1) a data-driven\nperceptual model of facial expressions, 2) a novel stylized character data set\nwith cardinal expression annotations : FERG (Facial Expression Research Group)\n- DB (added two new characters), and 3) . We evaluated our method on a set of\nretrieval tasks on our collected stylized character dataset of expressions. We\nhave also shown that the ranking order predicted by the proposed features is\nhighly correlated with the ranking order provided by a facial expression expert\nand Mechanical Turk (MT) experiments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:12:43 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Aneja", "Deepali", ""], ["Colburn", "Alex", ""], ["Faigin", "Gary", ""], ["Shapiro", "Linda", ""], ["Mones", "Barbara", ""]]}, {"id": "1911.09177", "submitter": "Ankit Desai", "authors": "Jekishan K. Parmar, Ankit Desai", "title": "Feature Extraction in Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmented Reality (AR) is used for various applications associated with the\nreal world. In this paper, first, describe characteristics and essential\nservices of AR. Brief history on Virtual Reality (VR) and AR is also mentioned\nin the introductory section. Then, AR Technologies along with its workflow is\ndepicted, which includes the complete AR Process consisting of the stages of\nImage Acquisition, Feature Extraction, Feature Matching, Geometric\nVerification, and Associated Information Retrieval. Feature extraction is the\nessence of AR hence its details are furnished in the paper.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 14:24:56 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Parmar", "Jekishan K.", ""], ["Desai", "Ankit", ""]]}, {"id": "1911.09204", "submitter": "Akshay Gadi Patil", "authors": "Jiongchao Jin, Akshay Gadi Patil, Zhang Xiong, Hao Zhang", "title": "DR-KFS: A Differentiable Visual Similarity Metric for 3D Shape\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a differential visual similarity metric to train deep neural\nnetworks for 3D reconstruction, aimed at improving reconstruction quality. The\nmetric compares two 3D shapes by measuring distances between multi-view images\ndifferentiably rendered from the shapes. Importantly, the image-space distance\nis also differentiable and measures visual similarity, rather than pixel-wise\ndistortion. Specifically, the similarity is defined by mean-squared errors over\nHardNet features computed from probabilistic keypoint maps of the compared\nimages. Our differential visual shape similarity metric can be easily plugged\ninto various 3D reconstruction networks, replacing their distortion-based\nlosses, such as Chamfer or Earth Mover distances, so as to optimize the network\nweights to produce reconstructions with better structural fidelity and visual\nquality. We demonstrate this both objectively, using well-known shape metrics\nfor retrieval and classification tasks that are independent from our new\nmetric, and subjectively through a perceptual study.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 22:57:51 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 21:34:27 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 20:29:58 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 18:16:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Jin", "Jiongchao", ""], ["Patil", "Akshay Gadi", ""], ["Xiong", "Zhang", ""], ["Zhang", "Hao", ""]]}, {"id": "1911.09267", "submitter": "Yujun Shen", "authors": "Ceyuan Yang, Yujun Shen, Bolei Zhou", "title": "Semantic Hierarchy Emerges in Deep Generative Representations for Scene\n  Synthesis", "comments": "15 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the success of Generative Adversarial Networks (GANs) in image\nsynthesis, there lacks enough understanding on what generative models have\nlearned inside the deep generative representations and how photo-realistic\nimages are able to be composed of the layer-wise stochasticity introduced in\nrecent GANs. In this work, we show that highly-structured semantic hierarchy\nemerges as variation factors from synthesizing scenes from the generative\nrepresentations in state-of-the-art GAN models, like StyleGAN and BigGAN. By\nprobing the layer-wise representations with a broad set of semantics at\ndifferent abstraction levels, we are able to quantify the causality between the\nactivations and semantics occurring in the output image. Such a quantification\nidentifies the human-understandable variation factors learned by GANs to\ncompose scenes. The qualitative and quantitative results further suggest that\nthe generative representations learned by the GANs with layer-wise latent codes\nare specialized to synthesize different hierarchical semantics: the early\nlayers tend to determine the spatial layout and configuration, the middle\nlayers control the categorical objects, and the later layers finally render the\nscene attributes as well as color scheme. Identifying such a set of\nmanipulatable latent variation factors facilitates semantic scene manipulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:26:15 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 02:12:56 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 05:49:15 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Yang", "Ceyuan", ""], ["Shen", "Yujun", ""], ["Zhou", "Bolei", ""]]}, {"id": "1911.10044", "submitter": "Christian Tominski", "authors": "Sven Kluge and Stefan Gladisch and Uwe Freiherr von Lukas and Oliver\n  Staadt and Christian Tominski", "title": "Virtual Lenses as Embodied Tools for Immersive Analytics", "comments": null, "journal-ref": "Proceedings of the GI VR/AR Workshop (VAR), 2020", "doi": "10.18420/vrar2020_8", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive lenses are useful tools for supporting the analysis of data in\ndifferent ways. Most existing lenses are designed for 2D visualization and are\noperated using standard mouse and keyboard interaction. On the other hand,\nresearch on virtual lenses for novel 3D immersive visualization environments is\nscarce. Our work aims to narrow this gap in the literature. We focus\nparticularly on the interaction with lenses. Inspired by natural interaction\nwith magnifying glasses in the real world, our lenses are designed as graspable\ntools that can be created and removed as needed, manipulated and parameterized\ndepending on the task, and even combined to flexibly create new views on the\ndata. We implemented our ideas in a system for the visual analysis of 3D sonar\ndata. Informal user feedback from more than a hundred people suggests that the\ndesigned lens interaction is easy to use for the task of finding a hidden wreck\nin sonar data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:53:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kluge", "Sven", ""], ["Gladisch", "Stefan", ""], ["von Lukas", "Uwe Freiherr", ""], ["Staadt", "Oliver", ""], ["Tominski", "Christian", ""]]}, {"id": "1911.10217", "submitter": "Jacopo Pantaleoni", "authors": "Jacopo Pantaleoni", "title": "Importance Sampling of Many Lights with Reinforcement Lightcuts Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we introduce a novel technique for sampling and\nintegrating direct illumination in the presence of many lights. Unlike previous\nwork, the presented technique importance samples the product distribution of\nradiance and visibility while using bounded memory footprint and very low\nsampling overhead. This is achieved by learning a compact approximation of the\ntarget distributions over both space and time, allowing to reuse and adapt the\nlearnt distributions both spatially, within a frame, and temporally, across\nmultiple frames. Finally, the technique is amenable to massive parallelization\non GPUs and suitable for both offline and real-time rendering.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 09:00:31 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 11:46:20 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Pantaleoni", "Jacopo", ""]]}, {"id": "1911.10274", "submitter": "Jacob Austin", "authors": "Jacob Austin, Rafael Corrales-Fatou, Sofia Wyetzner, Hod Lipson", "title": "Titan: A Parallel Asynchronous Library for Multi-Agent and Soft-Body\n  Robotics using NVIDIA CUDA", "comments": "7 pages, 8 figures, under submission for ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most robotics simulation libraries are built for low-dimensional and\nintrinsically serial tasks, soft-body and multi-agent robotics have created a\ndemand for simulation environments that can model many interacting bodies in\nparallel. Despite the increasing interest in these fields, no existing\nsimulation library addresses the challenge of providing a unified,\nhighly-parallelized, GPU-accelerated interface for simulating large robotic\nsystems. Titan is a versatile CUDA-based C++ robotics simulation library that\nemploys a novel asynchronous computing model for GPU-accelerated simulations of\nrobotics primitives. The innovative GPU architecture design permits\nsimultaneous optimization and control on the CPU while the GPU runs\nasynchronously, enabling rapid topology optimization and reinforcement learning\niterations. Kinematics are solved with a massively parallel integration scheme\nthat incorporates constraints and environmental forces. We report dramatically\nimproved performance over CPU-based baselines, simulating as many as 300\nmillion primitive updates per second, while allowing flexibility for a wide\nrange of research applications. We present several applications of Titan to\nhigh-performance simulations of soft-body and multi-agent robots.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 22:56:27 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Austin", "Jacob", ""], ["Corrales-Fatou", "Rafael", ""], ["Wyetzner", "Sofia", ""], ["Lipson", "Hod", ""]]}, {"id": "1911.10414", "submitter": "Matan Atzmon", "authors": "Matan Atzmon and Yaron Lipman", "title": "SAL: Sign Agnostic Learning of Shapes from Raw Data", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, neural networks have been used as implicit representations for\nsurface reconstruction, modelling, learning, and generation. So far, training\nneural networks to be implicit representations of surfaces required training\ndata sampled from a ground-truth signed implicit functions such as signed\ndistance or occupancy functions, which are notoriously hard to compute.\n  In this paper we introduce Sign Agnostic Learning (SAL), a deep learning\napproach for learning implicit shape representations directly from raw,\nunsigned geometric data, such as point clouds and triangle soups.\n  We have tested SAL on the challenging problem of surface reconstruction from\nan un-oriented point cloud, as well as end-to-end human shape space learning\ndirectly from raw scans dataset, and achieved state of the art reconstructions\ncompared to current approaches. We believe SAL opens the door to many geometric\ndeep learning applications with real-world data, alleviating the usual\npainstaking, often manual pre-process.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 20:18:29 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:50:00 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Atzmon", "Matan", ""], ["Lipman", "Yaron", ""]]}, {"id": "1911.10672", "submitter": "Dongxu Wei", "authors": "Dongxu Wei, Xiaowei Xu, Haibin Shen, Kejie Huang", "title": "GAC-GAN: A General Method for Appearance-Controllable Human Video Motion\n  Transfer", "comments": "paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human video motion transfer has a wide range of applications in multimedia,\ncomputer vision and graphics. Recently, due to the rapid development of\nGenerative Adversarial Networks (GANs), there has been significant progress in\nthe field. However, almost all existing GAN-based works are prone to address\nthe mapping from human motions to video scenes, with scene appearances are\nencoded individually in the trained models. Therefore, each trained model can\nonly generate videos with a specific scene appearance, new models are required\nto be trained to generate new appearances. Besides, existing works lack the\ncapability of appearance control. For example, users have to provide video\nrecords of wearing new clothes or performing in new backgrounds to enable\nclothes or background changing in their synthetic videos, which greatly limits\nthe application flexibility. In this paper, we propose GAC-GAN, a general\nmethod for appearance-controllable human video motion transfer. To enable\ngeneral-purpose appearance synthesis, we propose to include appearance\ninformation in the conditioning inputs. Thus, once trained, our model can\ngenerate new appearances by altering the input appearance information. To\nachieve appearance control, we first obtain the appearance-controllable\nconditioning inputs and then utilize a two-stage GAC-GAN to generate the\ncorresponding appearance-controllable outputs, where we utilize an ACGAN loss\nand a shadow extraction module for output foreground and background appearance\ncontrol respectively. We further build a solo dance dataset containing a large\nnumber of dance videos for training and evaluation. Experimental results show\nthat, our proposed GAC-GAN can not only support appearance-controllable human\nvideo motion transfer but also achieve higher video quality than state-of-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 02:56:47 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2020 15:52:47 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wei", "Dongxu", ""], ["Xu", "Xiaowei", ""], ["Shen", "Haibin", ""], ["Huang", "Kejie", ""]]}, {"id": "1911.10949", "submitter": "Rundi Wu", "authors": "Rundi Wu, Yixin Zhuang, Kai Xu, Hao Zhang, Baoquan Chen", "title": "PQ-NET: A Generative Part Seq2Seq Network for 3D Shapes", "comments": "Accepted to CVPR 2020. Code available at\n  https://github.com/ChrisWu1997/PQ-NET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PQ-NET, a deep neural network which represents and generates 3D\nshapes via sequential part assembly. The input to our network is a 3D shape\nsegmented into parts, where each part is first encoded into a feature\nrepresentation using a part autoencoder. The core component of PQ-NET is a\nsequence-to-sequence or Seq2Seq autoencoder which encodes a sequence of part\nfeatures into a latent vector of fixed size, and the decoder reconstructs the\n3D shape, one part at a time, resulting in a sequential assembly. The latent\nspace formed by the Seq2Seq encoder encodes both part structure and fine part\ngeometry. The decoder can be adapted to perform several generative tasks\nincluding shape autoencoding, interpolation, novel shape generation, and\nsingle-view 3D reconstruction, where the generated shapes are all composed of\nmeaningful parts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 14:43:05 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 10:16:05 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 01:38:40 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wu", "Rundi", ""], ["Zhuang", "Yixin", ""], ["Xu", "Kai", ""], ["Zhang", "Hao", ""], ["Chen", "Baoquan", ""]]}, {"id": "1911.11098", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka, Niloy Mitra,\n  Leonidas J. Guibas", "title": "StructEdit: Learning Structural Shape Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to encode differences in the geometry and (topological) structure of\nthe shapes of ordinary objects is key to generating semantically plausible\nvariations of a given shape, transferring edits from one shape to another, and\nmany other applications in 3D content creation. The common approach of encoding\nshapes as points in a high-dimensional latent feature space suggests treating\nshape differences as vectors in that space. Instead, we treat shape differences\nas primary objects in their own right and propose to encode them in their own\nlatent space. In a setting where the shapes themselves are encoded in terms of\nfine-grained part hierarchies, we demonstrate that a separate encoding of shape\ndeltas or differences provides a principled way to deal with inhomogeneities in\nthe shape space due to different combinatorial part structures, while also\nallowing for compactness in the representation, as well as edit abstraction and\ntransfer. Our approach is based on a conditional variational autoencoder for\nencoding and decoding shape deltas, conditioned on a source shape. We\ndemonstrate the effectiveness and robustness of our approach in multiple shape\nmodification and generation tasks, and provide comparison and ablation studies\non the PartNet dataset, one of the largest publicly available 3D datasets.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 18:08:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Mo", "Kaichun", ""], ["Guerrero", "Paul", ""], ["Yi", "Li", ""], ["Su", "Hao", ""], ["Wonka", "Peter", ""], ["Mitra", "Niloy", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1911.11380", "submitter": "Mathis Bode", "authors": "Mathis Bode, Michael Gauding, Zeyu Lian, Dominik Denker, Marco\n  Davidovic, Konstantin Kleinheinz, Jenia Jitsev, Heinz Pitsch", "title": "Using Physics-Informed Super-Resolution Generative Adversarial Networks\n  for Subgrid Modeling in Turbulent Reactive Flows", "comments": "Submitted to Combustion Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GR physics.comp-ph physics.flu-dyn stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turbulence is still one of the main challenges for accurately predicting\nreactive flows. Therefore, the development of new turbulence closures which can\nbe applied to combustion problems is essential. Data-driven modeling has become\nvery popular in many fields over the last years as large, often extensively\nlabeled, datasets became available and training of large neural networks became\npossible on GPUs speeding up the learning process tremendously. However, the\nsuccessful application of deep neural networks in fluid dynamics, for example\nfor subgrid modeling in the context of large-eddy simulations (LESs), is still\nchallenging. Reasons for this are the large amount of degrees of freedom in\nrealistic flows, the high requirements with respect to accuracy and error\nrobustness, as well as open questions, such as the generalization capability of\ntrained neural networks in such high-dimensional, physics-constrained\nscenarios. This work presents a novel subgrid modeling approach based on a\ngenerative adversarial network (GAN), which is trained with unsupervised deep\nlearning (DL) using adversarial and physics-informed losses. A two-step\ntraining method is used to improve the generalization capability, especially\nextrapolation, of the network. The novel approach gives good results in a\npriori as well as a posteriori tests with decaying turbulence including\nturbulent mixing. The applicability of the network in complex combustion\nscenarios is furthermore discussed by employing it to a reactive LES of the\nSpray A case defined by the Engine Combustion Network (ECN).\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:40:38 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Bode", "Mathis", ""], ["Gauding", "Michael", ""], ["Lian", "Zeyu", ""], ["Denker", "Dominik", ""], ["Davidovic", "Marco", ""], ["Kleinheinz", "Konstantin", ""], ["Jitsev", "Jenia", ""], ["Pitsch", "Heinz", ""]]}, {"id": "1911.11544", "submitter": "Rameen Abdal", "authors": "Rameen Abdal, Yipeng Qin, Peter Wonka", "title": "Image2StyleGAN++: How to Edit the Embedded Images?", "comments": "CVPR 2020 \" For the video, visit https://youtu.be/yd5WczbFt68 \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose Image2StyleGAN++, a flexible image editing framework with many\napplications. Our framework extends the recent Image2StyleGAN in three ways.\nFirst, we introduce noise optimization as a complement to the $W^+$ latent\nspace embedding. Our noise optimization can restore high-frequency features in\nimages and thus significantly improves the quality of reconstructed images,\ne.g. a big increase of PSNR from 20 dB to 45 dB. Second, we extend the global\n$W^+$ latent space embedding to enable local embeddings. Third, we combine\nembedding with activation tensor manipulation to perform high-quality local\nedits along with global semantic edits on images. Such edits motivate various\nhigh-quality image editing applications, e.g. image reconstruction, image\ninpainting, image crossover, local style transfer, image editing using\nscribbles, and attribute level feature transfer. Examples of the edited images\nare shown across the paper for visual inspection.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 14:08:28 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:38:59 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Wonka", "Peter", ""]]}, {"id": "1911.11758", "submitter": "Yuheng Li", "authors": "Yuheng Li, Krishna Kumar Singh, Utkarsh Ojha, Yong Jae Lee", "title": "MixNMatch: Multifactor Disentanglement and Encoding for Conditional\n  Image Generation", "comments": "CVPR 2020 camera ready", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MixNMatch, a conditional generative model that learns to\ndisentangle and encode background, object pose, shape, and texture from real\nimages with minimal supervision, for mix-and-match image generation. We build\nupon FineGAN, an unconditional generative model, to learn the desired\ndisentanglement and image generator, and leverage adversarial joint image-code\ndistribution matching to learn the latent factor encoders. MixNMatch requires\nbounding boxes during training to model background, but requires no other\nsupervision. Through extensive experiments, we demonstrate MixNMatch's ability\nto accurately disentangle, encode, and combine multiple factors for\nmix-and-match image generation, including sketch2color, cartoon2img, and\nimg2gif applications. Our code/models/demo can be found at\nhttps://github.com/Yuheng-Li/MixNMatch\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:49:39 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 06:17:57 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:56:13 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Li", "Yuheng", ""], ["Singh", "Krishna Kumar", ""], ["Ojha", "Utkarsh", ""], ["Lee", "Yong Jae", ""]]}, {"id": "1911.11999", "submitter": "Guoxian Song", "authors": "Guoxian Song, Jianmin Zheng, Jianfei Cai, Tat-Jen Cham", "title": "Recovering Facial Reflectance and Geometry from Multi-view Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the problem of estimating shapes and diffuse reflectances of human\nfaces from images has been extensively studied, there is relatively less work\ndone on recovering the specular albedo. This paper presents a lightweight\nsolution for inferring photorealistic facial reflectance and geometry. Our\nsystem processes video streams from two views of a subject, and outputs two\nreflectance maps for diffuse and specular albedos, as well as a vector map of\nsurface normals. A model-based optimization approach is used, consisting of the\nthree stages of multi-view face model fitting, facial reflectance inference and\nfacial geometry refinement. Our approach is based on a novel formulation built\nupon the 3D morphable model (3DMM) for representing 3D textured faces in\nconjunction with the Blinn-Phong reflection model. It has the advantage of\nrequiring only a simple setup with two video streams, and is able to exploit\nthe interaction between the diffuse and specular reflections across multiple\nviews as well as time frames. As a result, the method is able to reliably\nrecover high-fidelity facial reflectance and geometry, which facilitates\nvarious applications such as generating photorealistic facial images under new\nviewpoints or illumination conditions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:50:23 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Song", "Guoxian", ""], ["Zheng", "Jianmin", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""]]}, {"id": "1911.12070", "submitter": "Xiaopei Liu", "authors": "Daoming Liu, Chi Xiong, and Xiaopei Liu", "title": "Vectorizing Quantum Turbulence Vortex-Core Lines for Real-Time\n  Visualization", "comments": "14 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cond-mat.quant-gas hep-ph nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vectorizing vortex-core lines is crucial for high-quality visualization and\nanalysis of turbulence. While several techniques exist in the literature, they\ncan only be applied to classical fluids. Recently, quantum fluids with\nturbulence get more and more attention in physics. It is thus desirable that\nvortex-core lines can also be well extracted and visualized for quantum fluids.\nIn this paper, we aim for this goal and developed an efficient vortex-core line\nvectorization method for quantum fluids, which enables real-time visualization\nof high-resolution quantum turbulence structure. Given the datasets by\nsimulation, our technique is developed from the vortices identified by the\ncirculation-based method. To vectorize the vortex-core lines enclosed by those\nvortices, we propose a novel graph-based data structure, with iterative graph\nreduction and density-guided local optimization, to locate more precisely\nsub-grid-scale vortex-core line samples, which are then vectorized by\ncontinuous curves. This not only represents vortex-core line structures\ncontinuously, but also naturally preserves complex topology, such as branching\nduring reconnection. By vectorization, the memory consumption can be largely\nreduced by orders of magnitude, enabling real-time rendering performance.\nDifferent types of interactive visualizations are demonstrated to show the\neffectiveness of our technique, which could assist further research on quantum\nturbulence.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 10:41:48 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Liu", "Daoming", ""], ["Xiong", "Chi", ""], ["Liu", "Xiaopei", ""]]}, {"id": "1911.12327", "submitter": "Charalambos Poullis", "authors": "Yashas Joshi, Charalambos Poullis", "title": "Inattentional Blindness for Redirected Walking Using Dynamic Foveated\n  Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Redirected walking is a Virtual Reality(VR) locomotion technique which\nenables users to navigate virtual environments (VEs) that are spatially larger\nthan the available physical tracked space. In this work we present a novel\ntechnique for redirected walking in VR based on the psychological phenomenon of\ninattentional blindness. Based on the user's visual fixation points we divide\nthe user's view into zones. Spatially-varying rotations are applied according\nto the zone's importance and are rendered using foveated rendering. Our\ntechnique is real-time and applicable to small and large physical spaces.\nFurthermore, the proposed technique does not require the use of stimulated\nsaccades but rather takes advantage of naturally occurring saccades and blinks\nfor a complete refresh of the framebuffer. We performed extensive testing and\npresent the analysis of the results of three user studies conducted for the\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 18:08:21 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Joshi", "Yashas", ""], ["Poullis", "Charalambos", ""]]}, {"id": "1911.12383", "submitter": "Jiayi Xu", "authors": "Jiayi Xu, Soumya Dutta, Wenbin He, Joachim Moortgat, Han-Wei Shen", "title": "Geometry-Driven Detection, Tracking and Visual Analysis of Viscous and\n  Gravitational Fingers", "comments": "Published at IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3017568", "report-no": null, "categories": "cs.GR cs.CG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viscous and gravitational flow instabilities cause a displacement front to\nbreak up into finger-like fluids. The detection and evolutionary analysis of\nthese fingering instabilities are critical in multiple scientific disciplines\nsuch as fluid mechanics and hydrogeology. However, previous detection methods\nof the viscous and gravitational fingers are based on density thresholding,\nwhich provides limited geometric information of the fingers. The geometric\nstructures of fingers and their evolution are important yet little studied in\nthe literature. In this work, we explore the geometric detection and evolution\nof the fingers in detail to elucidate the dynamics of the instability. We\npropose a ridge voxel detection method to guide the extraction of finger cores\nfrom three-dimensional (3D) scalar fields. After skeletonizing finger cores\ninto skeletons, we design a spanning tree based approach to capture how fingers\nbranch spatially from the finger skeletons. Finally, we devise a novel\ngeometric-glyph augmented tracking graph to study how the fingers and their\nbranches grow, merge, and split over time. Feedback from earth scientists\ndemonstrates the usefulness of our approach to performing spatio-temporal\ngeometric analyses of fingers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:02:58 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 19:06:53 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 20:34:17 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 21:08:54 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Xu", "Jiayi", ""], ["Dutta", "Soumya", ""], ["He", "Wenbin", ""], ["Moortgat", "Joachim", ""], ["Shen", "Han-Wei", ""]]}, {"id": "1911.12861", "submitter": "Peihao Zhu", "authors": "Peihao Zhu, Rameen Abdal, Yipeng Qin, Peter Wonka", "title": "SEAN: Image Synthesis with Semantic Region-Adaptive Normalization", "comments": "Accepted as a CVPR 2020 oral paper. The interactive demo is available\n  at https://youtu.be/0Vbj9xFgoUw", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00515", "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose semantic region-adaptive normalization (SEAN), a simple but\neffective building block for Generative Adversarial Networks conditioned on\nsegmentation masks that describe the semantic regions in the desired output\nimage. Using SEAN normalization, we can build a network architecture that can\ncontrol the style of each semantic region individually, e.g., we can specify\none style reference image per region. SEAN is better suited to encode,\ntransfer, and synthesize style than the best previous method in terms of\nreconstruction quality, variability, and visual quality. We evaluate SEAN on\nmultiple datasets and report better quantitative metrics (e.g. FID, PSNR) than\nthe current state of the art. SEAN also pushes the frontier of interactive\nimage editing. We can interactively edit images by changing segmentation masks\nor the style for any given region. We can also interpolate styles from two\nreference images per region.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 20:54:35 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 13:47:09 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhu", "Peihao", ""], ["Abdal", "Rameen", ""], ["Qin", "Yipeng", ""], ["Wonka", "Peter", ""]]}, {"id": "1911.13032", "submitter": "Joaquim Jorge", "authors": "Maur\\'icio Sousa, Daniel Mendes, and Joaquim Jorge", "title": "Safe Walking In VR using Augmented Virtuality", "comments": "10 pages, 10 figures, VRCAI 2019 Poster; The Authors would like to\n  thank Francisco Venda for his work and contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New technologies allow ordinary people to access Virtual Reality at\naffordable prices in their homes. One of the most important tasks when\ninteracting with immersive Virtual Reality is to navigate the virtual\nenvironments (VEs). Arguably, the best methods to accomplish this use of direct\ncontrol interfaces. Among those, natural walking (NW) makes for enjoyable user\nexperience. However, common techniques to support direct control interfaces in\nVEs feature constraints that make it difficult to use those methods in cramped\nhome environments. Indeed, NW requires unobstructed and open space. To approach\nthis problem, we propose a new virtual locomotion technique, Combined Walking\nin Place (CWIP). CWIP allows people to take advantage of the available physical\nspace and empowers them to use NW to navigate in the virtual world. For longer\ndistances, we adopt Walking in Place (WIP) to enable them to move in the\nvirtual world beyond the confines of a cramped real room. However, roaming in\nimmersive alternate reality, while moving in the confines of a cluttered\nenvironment can lead people to stumble and fall. To approach these problems, we\ndeveloped Augmented Virtual Reality (AVR), to inform users about real-world\nhazards, such as chairs, drawers, walls via proxies and signs placed in the\nvirtual world. We propose thus CWIP-AVR as a way to safely explore VR in the\ncramped confines of your own home. To our knowledge, this is the first approach\nto combined different locomotion modalities in a safe manner. We evaluated it\nin a user study with 20 participants to validate their ability to navigate a\nvirtual world while walking in a confined and cluttered real space. Our results\nshow that CWIP-AVR allows people to navigate VR safely, switching between\nlocomotion modes flexibly while maintaining a good immersion.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 10:09:19 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sousa", "Maur\u00edcio", ""], ["Mendes", "Daniel", ""], ["Jorge", "Joaquim", ""]]}, {"id": "1911.13225", "submitter": "Shaohui Liu", "authors": "Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc Pollefeys,\n  Zhaopeng Cui", "title": "DIST: Rendering Deep Implicit Signed Distance Function with\n  Differentiable Sphere Tracing", "comments": "Camera-ready version to appear in CVPR 2020. Project page:\n  http://b1ueber2y.me/projects/DIST-Renderer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a differentiable sphere tracing algorithm to bridge the gap\nbetween inverse graphics methods and the recently proposed deep learning based\nimplicit signed distance function. Due to the nature of the implicit function,\nthe rendering process requires tremendous function queries, which is\nparticularly problematic when the function is represented as a neural network.\nWe optimize both the forward and backward passes of our rendering layer to make\nit run efficiently with affordable memory consumption on a commodity graphics\ncard. Our rendering method is fully differentiable such that losses can be\ndirectly computed on the rendered 2D observations, and the gradients can be\npropagated backwards to optimize the 3D geometry. We show that our rendering\nmethod can effectively reconstruct accurate 3D shapes from various inputs, such\nas sparse depth and multi-view images, through inverse optimization. With the\ngeometry based reasoning, our 3D shape prediction methods show excellent\ngeneralization capability and robustness against various noises.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 17:27:46 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 07:19:07 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Liu", "Shaohui", ""], ["Zhang", "Yinda", ""], ["Peng", "Songyou", ""], ["Shi", "Boxin", ""], ["Pollefeys", "Marc", ""], ["Cui", "Zhaopeng", ""]]}]