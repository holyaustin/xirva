[{"id": "2003.00410", "submitter": "Xinyi Le", "authors": "Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le", "title": "PF-Net: Point Fractal Network for 3D Point Cloud Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Point Fractal Network (PF-Net), a novel\nlearning-based approach for precise and high-fidelity point cloud completion.\nUnlike existing point cloud completion networks, which generate the overall\nshape of the point cloud from the incomplete point cloud and always change\nexisting points and encounter noise and geometrical loss, PF-Net preserves the\nspatial arrangements of the incomplete point cloud and can figure out the\ndetailed geometrical structure of the missing region(s) in the prediction. To\nsucceed at this task, PF-Net estimates the missing point cloud hierarchically\nby utilizing a feature-points-based multi-scale generating network. Further, we\nadd up multi-stage completion loss and adversarial loss to generate more\nrealistic missing region(s). The adversarial loss can better tackle multiple\nmodes in the prediction. Our experiments demonstrate the effectiveness of our\nmethod for several challenging point cloud completion tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 05:40:21 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Huang", "Zitian", ""], ["Yu", "Yikuan", ""], ["Xu", "Jiawen", ""], ["Ni", "Feng", ""], ["Le", "Xinyi", ""]]}, {"id": "2003.00792", "submitter": "Leonardo Fernandez-Jambrina", "authors": "Leonardo Fernandez-Jambrina", "title": "Characterisation of rational and NURBS developable surfaces in Computer\n  Aided Design", "comments": "Accepted for publication in Journal of Computational Mathematics. 18\n  pages, 8 figures, jcmlatex", "journal-ref": "Journal of Computational Mathematics 39, 550-568 (2021)", "doi": "10.4208/jcm.2003-m2019-0226", "report-no": null, "categories": "cs.GR cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a characterisation of rational developable surfaces\nin terms of the blossoms of the bounding curves and three rational functions\n$\\Lambda$, $M$, $\\nu$. Properties of developable surfaces are revised in this\nframework. In particular, a closed algebraic formula for the edge of regression\nof the surface is obtained in terms of the functions $\\Lambda$, $M$, $\\nu$,\nwhich are closely related to the ones that appear in the standard decomposition\nof the derivative of the parametrisation of one of the bounding curves in terms\nof the director vector of the rulings and its derivative. It is also shown that\nall rational developable surfaces can be described as the set of developable\nsurfaces which can be constructed with a constant $\\Lambda$, $M$, $\\nu$ . The\nresults are readily extended to rational spline developable surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 12:09:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Fernandez-Jambrina", "Leonardo", ""]]}, {"id": "2003.00902", "submitter": "Memo Akten", "authors": "Memo Akten, Rebecca Fiebrink, Mick Grierson", "title": "Learning to See: You Are What You See", "comments": "Presented as an Art Paper at SIGGRAPH 2019", "journal-ref": "ACM SIGGRAPH 2019 Art Gallery July 2019 Article No 13 Pages 1 to 6", "doi": "10.1145/3306211.3320143", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors present a visual instrument developed as part of the creation of\nthe artwork Learning to See. The artwork explores bias in artificial neural\nnetworks and provides mechanisms for the manipulation of specifically trained\nfor real-world representations. The exploration of these representations acts\nas a metaphor for the process of developing a visual understanding and/or\nvisual vocabulary of the world. These representations can be explored and\nmanipulated in real time, and have been produced in such a way so as to reflect\nspecific creative perspectives that call into question the relationship between\nhow both artificial neural networks and humans may construct meaning.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:12:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akten", "Memo", ""], ["Fiebrink", "Rebecca", ""], ["Grierson", "Mick", ""]]}, {"id": "2003.01061", "submitter": "Xiangyu Y Hu", "authors": "Zhe Ji and Lin Fu and Xiangyu Hu and Nikolaus Adams", "title": "A Feature-aware SPH for Isotropic Unstructured Mesh Generation", "comments": "54 pages and 22 figures", "journal-ref": null, "doi": "10.1016/j.cma.2020.113634", "report-no": null, "categories": "cs.GR physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a feature-aware SPH method for the concurrent and\nautomated isotropic unstructured mesh generation. Two additional objectives are\nachieved with the proposed method compared to the original SPH-based mesh\ngenerator (Fu et al., 2019). First, a feature boundary correction term is\nintroduced to address the issue of incomplete kernel support at the boundary\nvicinity. The mesh generation of feature curves, feature surfaces and volumes\ncan be handled concurrently without explicitly following a dimensional\nsequence. Second, a two-phase model is proposed to characterize the\nmesh-generation procedure by a feature-size-adaptation phase and a\nmesh-quality-optimization phase. By proposing a new error measurement criterion\nand an adaptive control system with two sets of simulation parameters, the\nobjectives of faster feature-size adaptation and local mesh-quality improvement\nare merged into a consistent framework. The proposed method is validated with a\nset of 2D and 3D numerical tests with different complexities and scales. The\nresults demonstrate that high-quality meshes are generated with a significant\nspeedup of convergence.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 13:35:07 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Ji", "Zhe", ""], ["Fu", "Lin", ""], ["Hu", "Xiangyu", ""], ["Adams", "Nikolaus", ""]]}, {"id": "2003.01215", "submitter": "Xuan Li", "authors": "Yue Li, Xuan Li, Minchen Li, Yixin Zhu, Bo Zhu, Chenfanfu Jiang", "title": "Lagrangian-Eulerian Multi-Density Topology Optimization with the\n  Material Point Method", "comments": "24 pages, 19 figures", "journal-ref": null, "doi": "10.1002/nme.6668", "report-no": null, "categories": "physics.comp-ph cs.CE cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a hybrid Lagrangian-Eulerian topology optimization (LETO)\nmethod is proposed to solve the elastic force equilibrium with the Material\nPoint Method (MPM). LETO transfers density information from freely movable\nLagrangian carrier particles to a fixed set of Eulerian quadrature points. This\ntransfer is based on a smooth radial kernel involved in the compliance\nobjective to avoid the artificial checkerboard pattern. The quadrature points\nact as MPM particles embedded in a lower-resolution grid and enable a sub-cell\nmulti-density resolution of intricate structures with a reduced computational\ncost. A quadrature-level connectivity graph-based method is adopted to avoid\nthe artificial checkerboard issues commonly existing in multi-resolution\ntopology optimization methods. Numerical experiments are provided to\ndemonstrate the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:05:45 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 05:20:09 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 22:04:10 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 02:45:13 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Yue", ""], ["Li", "Xuan", ""], ["Li", "Minchen", ""], ["Zhu", "Yixin", ""], ["Zhu", "Bo", ""], ["Jiang", "Chenfanfu", ""]]}, {"id": "2003.02874", "submitter": "Zhijing Li", "authors": "Zhijing Li, Christopher De Sa, Adrian Sampson", "title": "Optimizing JPEG Quantization for Classification Networks", "comments": "6 pages, 13 figures, Resource-Constrained Machine Learning (ReCoML)\n  Workshop of MLSys 2020 Conference, Austin, TX, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for computer vision depends on lossy image compression: it\nreduces the storage required for training and test data and lowers transfer\ncosts in deployment. Mainstream datasets and imaging pipelines all rely on\nstandard JPEG compression. In JPEG, the degree of quantization of frequency\ncoefficients controls the lossiness: an 8 by 8 quantization table (Q-table)\ndecides both the quality of the encoded image and the compression ratio. While\na long history of work has sought better Q-tables, existing work either seeks\nto minimize image distortion or to optimize for models of the human visual\nsystem. This work asks whether JPEG Q-tables exist that are \"better\" for\nspecific vision networks and can offer better quality--size trade-offs than\nones designed for human perception or minimal distortion. We reconstruct an\nImageNet test set with higher resolution to explore the effect of JPEG\ncompression under novel Q-tables. We attempt several approaches to tune a\nQ-table for a vision task. We find that a simple sorted random sampling method\ncan exceed the performance of the standard JPEG Q-table. We also use\nhyper-parameter tuning techniques including bounded random search, Bayesian\noptimization, and composite heuristic optimization methods. The new Q-tables we\nobtained can improve the compression rate by 10% to 200% when the accuracy is\nfixed, or improve accuracy up to $2\\%$ at the same compression rate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:13:06 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Li", "Zhijing", ""], ["De Sa", "Christopher", ""], ["Sampson", "Adrian", ""]]}, {"id": "2003.03040", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "DeProCams: Simultaneous Relighting, Compensation and Shape\n  Reconstruction for Projector-Camera Systems", "comments": "Source code and supplementary material at:\n  https://github.com/BingyaoHuang/DeProCams", "journal-ref": null, "doi": "10.1109/TVCG.2021.3067771", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based relighting, projector compensation and depth/normal\nreconstruction are three important tasks of projector-camera systems (ProCams)\nand spatial augmented reality (SAR). Although they share a similar pipeline of\nfinding projector-camera image mappings, in tradition, they are addressed\nindependently, sometimes with different prerequisites, devices and sampling\nimages. In practice, this may be cumbersome for SAR applications to address\nthem one-by-one. In this paper, we propose a novel end-to-end trainable model\nnamed DeProCams to explicitly learn the photometric and geometric mappings of\nProCams, and once trained, DeProCams can be applied simultaneously to the three\ntasks. DeProCams explicitly decomposes the projector-camera image mappings into\nthree subprocesses: shading attributes estimation, rough direct light\nestimation and photorealistic neural rendering. A particular challenge\naddressed by DeProCams is occlusion, for which we exploit epipolar constraint\nand propose a novel differentiable projector direct light mask. Thus, it can be\nlearned end-to-end along with the other modules. Afterwards, to improve\nconvergence, we apply photometric and geometric constraints such that the\nintermediate results are plausible. In our experiments, DeProCams shows clear\nadvantages over previous arts with promising quality and meanwhile being fully\ndifferentiable. Moreover, by solving the three tasks in a unified model,\nDeProCams waives the need for additional optical devices, radiometric\ncalibrations and structured light.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:49:16 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 09:19:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "2003.03473", "submitter": "Shashank Tripathi", "authors": "Shashank Tripathi, Siddhant Ranade, Ambrish Tyagi, Amit Agrawal", "title": "PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge\n  Distillation", "comments": "Accepted as Oral in 3DV 2020; supplementary material included; added\n  results on 3DPW dataset in revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recovering 3D human pose from 2D joints is a highly unconstrained problem. We\npropose a novel neural network framework, PoseNet3D, that takes 2D joints as\ninput and outputs 3D skeletons and SMPL body model parameters. By casting our\nlearning approach in a student-teacher framework, we avoid using any 3D data\nsuch as paired/unpaired 3D data, motion capture sequences, depth images or\nmulti-view images during training. We first train a teacher network that\noutputs 3D skeletons, using only 2D poses for training. The teacher network\ndistills its knowledge to a student network that predicts 3D pose in SMPL\nrepresentation. Finally, both the teacher and the student networks are jointly\nfine-tuned in an end-to-end manner using temporal, self-consistency and\nadversarial losses, improving the accuracy of each individual network. Results\non Human3.6M dataset for 3D human pose estimation demonstrate that our approach\nreduces the 3D joint prediction error by 18% compared to previous unsupervised\nmethods. Qualitative results on in-the-wild datasets show that the recovered 3D\nposes and meshes are natural, realistic, and flow smoothly over consecutive\nframes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 00:10:59 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 05:07:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tripathi", "Shashank", ""], ["Ranade", "Siddhant", ""], ["Tyagi", "Ambrish", ""], ["Agrawal", "Amit", ""]]}, {"id": "2003.03551", "submitter": "Aihua Mao", "authors": "Aihua Mao, Canglan Dai, Lin Gao, Ying He, Yong-jin Liu", "title": "STD-Net: Structure-preserving and Topology-adaptive Deformation Network\n  for 3D Reconstruction from a Single Image", "comments": "14 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from a single view image is a long-standing prob-lem in\ncomputer vision. Various methods based on different shape representations(such\nas point cloud or volumetric representations) have been proposed. However,the\n3D shape reconstruction with fine details and complex structures are still\nchal-lenging and have not yet be solved. Thanks to the recent advance of the\ndeepshape representations, it becomes promising to learn the structure and\ndetail rep-resentation using deep neural networks. In this paper, we propose a\nnovel methodcalled STD-Net to reconstruct the 3D models utilizing the mesh\nrepresentationthat is well suitable for characterizing complex structure and\ngeometry details.To reconstruct complex 3D mesh models with fine details, our\nmethod consists of(1) an auto-encoder network for recovering the structure of\nan object with bound-ing box representation from a single image, (2) a\ntopology-adaptive graph CNNfor updating vertex position for meshes of complex\ntopology, and (3) an unifiedmesh deformation block that deforms the structural\nboxes into structure-awaremeshed models. Experimental results on the images\nfrom ShapeNet show that ourproposed STD-Net has better performance than other\nstate-of-the-art methods onreconstructing 3D objects with complex structures\nand fine geometric details.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 11:02:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Mao", "Aihua", ""], ["Dai", "Canglan", ""], ["Gao", "Lin", ""], ["He", "Ying", ""], ["Liu", "Yong-jin", ""]]}, {"id": "2003.04187", "submitter": "Shao-Kui Zhang", "authors": "Yu He, Yun Cai, Yuan-Chen Guo, Zheng-Ning Liu, Shao-Kui Zhang,\n  Song-Hai Zhang, Hong-Bo Fu, Sheng-Yong Chen", "title": "Style-compatible Object Recommendation for Multi-room Indoor Scene\n  Synthesis", "comments": "11 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional indoor scene synthesis methods often take a two-step approach:\nobject selection and object arrangement. Current state-of-the-art object\nselection approaches are based on convolutional neural networks (CNNs) and can\nproduce realistic scenes for a single room. However, they cannot be directly\nextended to synthesize style-compatible scenes for multiple rooms with\ndifferent functions. To address this issue, we treat the object selection\nproblem as combinatorial optimization based on a Labeled LDA (L-LDA) model. We\nfirst calculate occurrence probability distribution of object categories\naccording to a topic model, and then sample objects from each category\nconsidering their function diversity along with style compatibility, while\nregarding not only separate rooms, but also associations among rooms. User\nstudy shows that our method outperforms the baselines by incorporating\nmulti-function and multi-room settings with style constraints, and sometimes\neven produces plausible scenes comparable to those produced by professional\ndesigners.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:04:25 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 07:47:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["He", "Yu", ""], ["Cai", "Yun", ""], ["Guo", "Yuan-Chen", ""], ["Liu", "Zheng-Ning", ""], ["Zhang", "Shao-Kui", ""], ["Zhang", "Song-Hai", ""], ["Fu", "Hong-Bo", ""], ["Chen", "Sheng-Yong", ""]]}, {"id": "2003.04583", "submitter": "Zhouyingcheng Liao", "authors": "Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll", "title": "TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape\n  and Garment Style", "comments": "Accepted to CVPR 2020. Chaitanya Patel and Zhouyingcheng Liao\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present TailorNet, a neural model which predicts clothing\ndeformation in 3D as a function of three factors: pose, shape and style\n(garment geometry), while retaining wrinkle detail. This goes beyond prior\nmodels, which are either specific to one style and shape, or generalize to\ndifferent shapes producing smooth results, despite being style specific. Our\nhypothesis is that (even non-linear) combinations of examples smooth out high\nfrequency components such as fine-wrinkles, which makes learning the three\nfactors jointly hard. At the heart of our technique is a decomposition of\ndeformation into a high frequency and a low frequency component. While the\nlow-frequency component is predicted from pose, shape and style parameters with\nan MLP, the high-frequency component is predicted with a mixture of shape-style\nspecific pose models. The weights of the mixture are computed with a narrow\nbandwidth kernel to guarantee that only predictions with similar high-frequency\npatterns are combined. The style variation is obtained by computing, in a\ncanonical pose, a subspace of deformation, which satisfies physical constraints\nsuch as inter-penetration, and draping on the body. TailorNet delivers 3D\ngarments which retain the wrinkles from the physics based simulations (PBS) it\nis learned from, while running more than 1000 times faster. In contrast to PBS,\nTailorNet is easy to use and fully differentiable, which is crucial for\ncomputer vision algorithms. Several experiments demonstrate TailorNet produces\nmore realistic results than prior work, and even generates temporally coherent\ndeformations on sequences of the AMASS dataset, despite being trained on static\nposes from a different dataset. To stimulate further research in this\ndirection, we will make a dataset consisting of 55800 frames, as well as our\nmodel publicly available at https://virtualhumans.mpi-inf.mpg.de/tailornet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:49:51 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 16:35:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Patel", "Chaitanya", ""], ["Liao", "Zhouyingcheng", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2003.04651", "submitter": "Michael Schelling", "authors": "Michael Schelling, Pedro Hermosilla, Pere-Pau Vazquez, Timo Ropinski", "title": "Enabling Viewpoint Learning through Dynamic Label Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal viewpoint prediction is an essential task in many computer graphics\napplications. Unfortunately, common viewpoint qualities suffer from two major\ndrawbacks: dependency on clean surface meshes, which are not always available,\nand the lack of closed-form expressions, which requires a costly search\ninvolving rendering. To overcome these limitations we propose to separate\nviewpoint selection from rendering through an end-to-end learning approach,\nwhereby we reduce the influence of the mesh quality by predicting viewpoints\nfrom unstructured point clouds instead of polygonal meshes. While this makes\nour approach insensitive to the mesh discretization during evaluation, it only\nbecomes possible when resolving label ambiguities that arise in this context.\nTherefore, we additionally propose to incorporate the label generation into the\ntraining procedure, making the label decision adaptive to the current network\npredictions. We show how our proposed approach allows for learning viewpoint\npredictions for models from different object categories and for different\nviewpoint qualities. Additionally, we show that prediction times are reduced\nfrom several minutes to a fraction of a second, as compared to state-of-the-art\n(SOTA) viewpoint quality evaluation. We will further release the code and\ntraining data, which will to our knowledge be the biggest viewpoint quality\ndataset available.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:49:27 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 14:35:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Schelling", "Michael", ""], ["Hermosilla", "Pedro", ""], ["Vazquez", "Pere-Pau", ""], ["Ropinski", "Timo", ""]]}, {"id": "2003.05471", "submitter": "Vage Egiazarian", "authors": "Vage Egiazarian, Oleg Voynov, Alexey Artemov, Denis Volkhonskiy,\n  Aleksandr Safin, Maria Taktasheva, Denis Zorin, Evgeny Burnaev", "title": "Deep Vectorization of Technical Drawings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58601-0_35", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for vectorization of technical line drawings, such as\nfloor plans, architectural drawings, and 2D CAD images. Our method includes (1)\na deep learning-based cleaning stage to eliminate the background and\nimperfections in the image and fill in missing parts, (2) a transformer-based\nnetwork to estimate vector primitives, and (3) optimization procedure to obtain\nthe final primitive configurations. We train the networks on synthetic data,\nrenderings of vector line drawings, and manually vectorized scans of line\ndrawings. Our method quantitatively and qualitatively outperforms a number of\nexisting techniques on a collection of representative technical drawings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:19:00 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:54:55 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 14:32:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Egiazarian", "Vage", ""], ["Voynov", "Oleg", ""], ["Artemov", "Alexey", ""], ["Volkhonskiy", "Denis", ""], ["Safin", "Aleksandr", ""], ["Taktasheva", "Maria", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2003.05863", "submitter": "Han Yang", "authors": "Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, Ping Luo", "title": "Towards Photo-Realistic Virtual Try-On by Adaptively\n  Generating$\\leftrightarrow$Preserving Image Content", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image visual try-on aims at transferring a target clothing image onto a\nreference person, and has become a hot topic in recent years. Prior arts\nusually focus on preserving the character of a clothing image (e.g. texture,\nlogo, embroidery) when warping it to arbitrary human pose. However, it remains\na big challenge to generate photo-realistic try-on images when large occlusions\nand human poses are presented in the reference person. To address this issue,\nwe propose a novel visual try-on network, namely Adaptive Content Generating\nand Preserving Network (ACGPN). In particular, ACGPN first predicts semantic\nlayout of the reference image that will be changed after try-on (e.g. long\nsleeve shirt$\\rightarrow$arm, arm$\\rightarrow$jacket), and then determines\nwhether its image content needs to be generated or preserved according to the\npredicted semantic layout, leading to photo-realistic try-on and rich clothing\ndetails. ACGPN generally involves three major modules. First, a semantic layout\ngeneration module utilizes semantic segmentation of the reference image to\nprogressively predict the desired semantic layout after try-on. Second, a\nclothes warping module warps clothing images according to the generated\nsemantic layout, where a second-order difference constraint is introduced to\nstabilize the warping process during training. Third, an inpainting module for\ncontent fusion integrates all information (e.g. reference image, semantic\nlayout, warped clothes) to adaptively produce each semantic part of human body.\nIn comparison to the state-of-the-art methods, ACGPN can generate\nphoto-realistic images with much better perceptual quality and richer\nfine-details.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:55:39 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yang", "Han", ""], ["Zhang", "Ruimao", ""], ["Guo", "Xiaobao", ""], ["Liu", "Wei", ""], ["Zuo", "Wangmeng", ""], ["Luo", "Ping", ""]]}, {"id": "2003.05938", "submitter": "Yamin Li", "authors": "Yamin Li, Dong He, Xiangyu Wang, Kai Tang", "title": "Geodesic Distance Field-based Curved Layer Volume Decomposition for\n  Multi-Axis Support-free Printing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new curved layer volume decomposition method for\nmulti-axis support-free printing of freeform solid parts. Given a solid model\nto be printed that is represented as a tetrahedral mesh, we first establish a\ngeodesic distance field embedded on the mesh, whose value at any vertex is the\ngeodesic distance to the base of the model. Next, the model is naturally\ndecomposed into curved layers by interpolating a number of iso-geodesic\ndistance surfaces (IGDSs). These IGDSs morph from bottom-up in an intrinsic and\nsmooth way owing to the nature of geodesics, which will be used as the curved\nprinting layers that are friendly to multi-axis printing. In addition, to cater\nto the collision-free requirement and to improve the printing efficiency, we\nalso propose a printing sequence optimization algorithm for determining the\nprinting order of the IGDSs, which helps reduce the air-move path length. Ample\nexperiments in both computer simulation and physical printing are performed,\nand the experimental results confirm the advantages of our method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 05:46:41 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Li", "Yamin", ""], ["He", "Dong", ""], ["Wang", "Xiangyu", ""], ["Tang", "Kai", ""]]}, {"id": "2003.06233", "submitter": "Chenyang Zhu", "authors": "Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu", "title": "Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online semantic 3D segmentation in company with real-time RGB-D\nreconstruction poses special challenges such as how to perform 3D convolution\ndirectly over the progressively fused 3D geometric data, and how to smartly\nfuse information from frame to frame. We propose a novel fusion-aware 3D point\nconvolution which operates directly on the geometric surface being\nreconstructed and exploits effectively the inter-frame correlation for high\nquality 3D feature learning. This is enabled by a dedicated dynamic data\nstructure which organizes the online acquired point cloud with global-local\ntrees. Globally, we compile the online reconstructed 3D points into an\nincrementally growing coordinate interval tree, enabling fast point insertion\nand neighborhood query. Locally, we maintain the neighborhood information for\neach point using an octree whose construction benefits from the fast query of\nthe global tree.Both levels of trees update dynamically and help the 3D\nconvolution effectively exploits the temporal coherence for effective\ninformation fusion across RGB-D frames.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 12:32:24 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 15:46:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhang", "Jiazhao", ""], ["Zhu", "Chenyang", ""], ["Zheng", "Lintao", ""], ["Xu", "Kai", ""]]}, {"id": "2003.06520", "submitter": "Zhelun Wu", "authors": "Zhelun Wu, Hongyan Jiang, Siyun He", "title": "Symmetry Detection of Occluded Point Cloud Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry detection has been a classical problem in computer graphics, many of\nwhich using traditional geometric methods. In recent years, however, we have\nwitnessed the arising deep learning changed the landscape of computer graphics.\nIn this paper, we aim to solve the symmetry detection of the occluded point\ncloud in a deep-learning fashion. To the best of our knowledge, we are the\nfirst to utilize deep learning to tackle such a problem. In such a deep\nlearning framework, double supervisions: points on the symmetry plane and\nnormal vectors are employed to help us pinpoint the symmetry plane. We\nconducted experiments on the YCB- video dataset and demonstrate the efficacy of\nour method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 00:23:58 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wu", "Zhelun", ""], ["Jiang", "Hongyan", ""], ["He", "Siyun", ""]]}, {"id": "2003.06659", "submitter": "Thomas Kerdreux", "authors": "Thomas Kerdreux and Louis Thiry and Erwan Kerdreux", "title": "Interactive Neural Style Transfer with Artists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present interactive painting processes in which a painter and various\nneural style transfer algorithms interact on a real canvas. Understanding what\nthese algorithms' outputs achieve is then paramount to describe the creative\nagency in our interactive experiments. We gather a set of paired\npainting-pictures images and present a new evaluation methodology based on the\npredictivity of neural style transfer algorithms. We point some algorithms'\ninstabilities and show that they can be used to enlarge the diversity and\npleasing oddity of the images synthesized by the numerous existing neural style\ntransfer algorithms. This diversity of images was perceived as a source of\ninspiration for human painters, portraying the machine as a computational\ncatalyst.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 15:27:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""], ["Kerdreux", "Erwan", ""]]}, {"id": "2003.07341", "submitter": "Mazlum Ferhat Arslan", "authors": "M. Ferhat Arslan (1), Sibel Tari (1) ((1) Middle East Technical\n  University)", "title": "Complexity of Shapes Embedded in ${\\mathbb Z^n}$ with a Bias Towards\n  Squares", "comments": "13 pages, 14 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Shape complexity is a hard-to-quantify quality, mainly due to its relative\nnature. Biased by Euclidean thinking, circles are commonly considered as the\nsimplest. However, their constructions as digital images are only\napproximations to the ideal form. Consequently, complexity orders computed in\nreference to circle are unstable. Unlike circles which lose their circleness in\ndigital images, squares retain their qualities. Hence, we consider squares\n(hypercubes in $\\mathbb Z^n$) to be the simplest shapes relative to which\ncomplexity orders are constructed. Using the connection between $L^\\infty$ norm\nand squares we effectively encode squareness-adapted simplification through\nwhich we obtain multi-scale complexity measure, where scale determines the\nlevel of interest to the boundary. The emergent scale above which the effect of\na boundary feature (appendage) disappears is related to the ratio of the\ncontacting width of the appendage to that of the main body. We discuss what\nzero complexity implies in terms of information repetition and constructibility\nand what kind of shapes in addition to squares have zero complexity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:24:22 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Arslan", "M. Ferhat", ""], ["Tari", "Sibel", ""]]}, {"id": "2003.07504", "submitter": "Wei Liu", "authors": "Wei Liu, Pingping Zhang, Xiaolin Huang, Jie Yang, Chunhua Shen and Ian\n  Reid", "title": "Real-time Image Smoothing via Iterative Least Squares", "comments": "24 pages, accepted to ACM Transactions on Graphics, presented at\n  SIGGRAPH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-preserving image smoothing is a fundamental procedure for many computer\nvision and graphic applications. There is a tradeoff between the smoothing\nquality and the processing speed: the high smoothing quality usually requires a\nhigh computational cost which leads to the low processing speed. In this paper,\nwe propose a new global optimization based method, named iterative least\nsquares (ILS), for efficient edge-preserving image smoothing. Our approach can\nproduce high-quality results but at a much lower computational cost.\nComprehensive experiments demonstrate that the propose method can produce\nresults with little visible artifacts. Moreover, the computation of ILS can be\nhighly parallel, which can be easily accelerated through either multi-thread\ncomputing or the GPU hardware. With the acceleration of a GTX 1080 GPU, it is\nable to process images of 1080p resolution ($1920\\times1080$) at the rate of\n20fps for color images and 47fps for gray images. In addition, the ILS is\nflexible and can be modified to handle more applications that require different\nsmoothing properties. Experimental results of several applications show the\neffectiveness and efficiency of the proposed method. The code is available at\n\\url{https://github.com/wliusjtu/Real-time-Image-Smoothing-via-Iterative-Least-Squares}\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:49:32 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 02:14:20 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Liu", "Wei", ""], ["Zhang", "Pingping", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "2003.08032", "submitter": "Carolyn Matl", "authors": "Carolyn Matl, Yashraj Narang, Ruzena Bajcsy, Fabio Ramos, Dieter Fox", "title": "Inferring the Material Properties of Granular Media for Robotic Tasks", "comments": "8 pages, 6 figures, appeared in ICRA 2020; fixed misplaced image in\n  figure 4; updated video link; fixed resolution for figure 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granular media (e.g., cereal grains, plastic resin pellets, and pills) are\nubiquitous in robotics-integrated industries, such as agriculture,\nmanufacturing, and pharmaceutical development. This prevalence mandates the\naccurate and efficient simulation of these materials. This work presents a\nsoftware and hardware framework that automatically calibrates a fast physics\nsimulator to accurately simulate granular materials by inferring material\nproperties from real-world depth images of granular formations (i.e., piles and\nrings). Specifically, coefficients of sliding friction, rolling friction, and\nrestitution of grains are estimated from summary statistics of grain formations\nusing likelihood-free Bayesian inference. The calibrated simulator accurately\npredicts unseen granular formations in both simulation and experiment;\nfurthermore, simulator predictions are shown to generalize to more complex\ntasks, including using a robot to pour grains into a bowl, as well as to create\na desired pattern of piles and rings. Visualizations of the framework and\nexperiments can be viewed at https://youtu.be/OBvV5h2NMKA\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 04:00:08 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 18:11:23 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 21:08:34 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2020 18:41:55 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Matl", "Carolyn", ""], ["Narang", "Yashraj", ""], ["Bajcsy", "Ruzena", ""], ["Ramos", "Fabio", ""], ["Fox", "Dieter", ""]]}, {"id": "2003.08124", "submitter": "Ziwei Liu", "authors": "Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, Xiaogang Wang", "title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation from\n  Single-View Images", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020. Code and models are available at:\n  https://github.com/Hangz-nju-cuhk/Rotate-and-Render", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though face rotation has achieved rapid progress in recent years, the lack of\nhigh-quality paired training data remains a great hurdle for existing methods.\nThe current generative models heavily rely on datasets with multi-view images\nof the same person. Thus, their generated results are restricted by the scale\nand domain of the data source. To overcome these challenges, we propose a novel\nunsupervised framework that can synthesize photo-realistic rotated faces using\nonly single-view image collections in the wild. Our key insight is that\nrotating faces in the 3D space back and forth, and re-rendering them to the 2D\nplane can serve as a strong self-supervision. We leverage the recent advances\nin 3D face modeling and high-resolution GAN to constitute our building blocks.\nSince the 3D rotation-and-render on faces can be applied to arbitrary angles\nwithout losing details, our approach is extremely suitable for in-the-wild\nscenarios (i.e. no paired data are available), where existing methods fall\nshort. Extensive experiments demonstrate that our approach has superior\nsynthesis quality as well as identity preservation over the state-of-the-art\nmethods, across a wide range of poses and domains. Furthermore, we validate\nthat our rotate-and-render framework naturally can act as an effective data\naugmentation engine for boosting modern face recognition systems even on strong\nbaseline models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 09:54:46 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Zhou", "Hang", ""], ["Liu", "Jihao", ""], ["Liu", "Ziwei", ""], ["Liu", "Yu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2003.08367", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan T.\n  Barron, Richard Tucker, Noah Snavely", "title": "Lighthouse: Predicting Lighting Volumes for Spatially-Coherent\n  Illumination", "comments": "CVPR 2020. Project page:\n  https://people.eecs.berkeley.edu/~pratul/lighthouse/ [Updates: typos\n  corrected]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning solution for estimating the incident illumination\nat any 3D location within a scene from an input narrow-baseline stereo image\npair. Previous approaches for predicting global illumination from images either\npredict just a single illumination for the entire scene, or separately estimate\nthe illumination at each 3D location without enforcing that the predictions are\nconsistent with the same 3D scene. Instead, we propose a deep learning model\nthat estimates a 3D volumetric RGBA model of a scene, including content outside\nthe observed field of view, and then uses standard volume rendering to estimate\nthe incident illumination at any 3D location within that volume. Our model is\ntrained without any ground truth 3D data and only requires a held-out\nperspective view near the input stereo pair and a spherical panorama taken\nwithin each scene as supervision, as opposed to prior methods for\nspatially-varying lighting estimation, which require ground truth scene\ngeometry for training. We demonstrate that our method can predict consistent\nspatially-varying lighting that is convincing enough to plausibly relight and\ninsert highly specular virtual objects into real images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:46:30 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 17:04:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Mildenhall", "Ben", ""], ["Tancik", "Matthew", ""], ["Barron", "Jonathan T.", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""]]}, {"id": "2003.08624", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, He Wang, Xinchen Yan, Leonidas J. Guibas", "title": "PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree\n  Conditions", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D generative shape modeling is a fundamental research area in computer\nvision and interactive computer graphics, with many real-world applications.\nThis paper investigates the novel problem of generating 3D shape point cloud\ngeometry from a symbolic part tree representation. In order to learn such a\nconditional shape generation procedure in an end-to-end fashion, we propose a\nconditional GAN \"part tree\"-to-\"point cloud\" model (PT2PC) that disentangles\nthe structural and geometric factors. The proposed model incorporates the part\ntree condition into the architecture design by passing messages top-down and\nbottom-up along the part tree hierarchy. Experimental results and user study\ndemonstrate the strengths of our method in generating perceptually plausible\nand diverse 3D point clouds, given the part tree condition. We also propose a\nnovel structural measure for evaluating if the generated shape point clouds\nsatisfy the part tree conditions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:27:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 01:28:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mo", "Kaichun", ""], ["Wang", "He", ""], ["Yan", "Xinchen", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2003.08723", "submitter": "Steffen Wiewel", "authors": "Steffen Wiewel, Byungsoo Kim, Vinicius C. Azevedo, Barbara\n  Solenthaler, Nils Thuerey", "title": "Latent Space Subdivision: Stable and Controllable Time Predictions for\n  Fluid Flow", "comments": "https://ge.in.tum.de/publications/latent-space-subdivision/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end trained neural networkarchitecture to robustly\npredict the complex dynamics of fluid flows with high temporal stability. We\nfocus on single-phase smoke simulations in 2D and 3D based on the\nincompressible Navier-Stokes (NS) equations, which are relevant for a wide\nrange of practical problems. To achieve stable predictions for long-term flow\nsequences, a convolutional neural network (CNN) is trained for spatial\ncompression in combination with a temporal prediction network that consists of\nstacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel\nlatent space subdivision (LSS) to separate the respective input quantities into\nindividual parts of the encoded latent space domain. This allows to\ndistinctively alter the encoded quantities without interfering with the\nremaining latent space values and hence maximizes external control. By\nselectively overwriting parts of the predicted latent space points, our\nproposed method is capable to robustly predict long-term sequences of complex\nphysics problems. In addition, we highlight the benefits of a recurrent\ntraining on the latent space creation, which is performed by the spatial\ncompression network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 12:38:52 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wiewel", "Steffen", ""], ["Kim", "Byungsoo", ""], ["Azevedo", "Vinicius C.", ""], ["Solenthaler", "Barbara", ""], ["Thuerey", "Nils", ""]]}, {"id": "2003.08758", "submitter": "Patricia Wollstadt", "authors": "Skylar Sible, Rodrigo Iza-Teran, Jochen Garcke, Nikola Aulig, Patricia\n  Wollstadt", "title": "A Compact Spectral Descriptor for Shape Deformations", "comments": "To be published in Proc. of the European Conference on Artificial\n  Intelligence 2020, 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern product design in the engineering domain is increasingly driven by\ncomputational analysis including finite-element based simulation, computational\noptimization, and modern data analysis techniques such as machine learning. To\napply these methods, suitable data representations for components under\ndevelopment as well as for related design criteria have to be found. While a\ncomponent's geometry is typically represented by a polygon surface mesh, it is\noften not clear how to parametrize critical design properties in order to\nenable efficient computational analysis. In the present work, we propose a\nnovel methodology to obtain a parameterization of a component's plastic\ndeformation behavior under stress, which is an important design criterion in\nmany application domains, for example, when optimizing the crash behavior in\nthe automotive context. Existing parameterizations limit computational analysis\nto relatively simple deformations and typically require extensive input by an\nexpert, making the design process time intensive and costly. Hence, we propose\na way to derive a compact descriptor of deformation behavior that is based on\nspectral mesh processing and enables a low-dimensional representation of also\ncomplex deformations.We demonstrate the descriptor's ability to represent\nrelevant deformation behavior by applying it in a nearest-neighbor search to\nidentify similar simulation results in a filtering task. The proposed\ndescriptor provides a novel approach to the parametrization of geometric\ndeformation behavior and enables the use of state-of-the-art data analysis\ntechniques such as machine learning to engineering tasks concerned with plastic\ndeformation behavior.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:34:30 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sible", "Skylar", ""], ["Iza-Teran", "Rodrigo", ""], ["Garcke", "Jochen", ""], ["Aulig", "Nikola", ""], ["Wollstadt", "Patricia", ""]]}, {"id": "2003.08934", "submitter": "Ben Mildenhall", "authors": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.\n  Barron, Ravi Ramamoorthi, Ren Ng", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "comments": "ECCV 2020 (oral). Project page with videos and code:\n  http://tancik.com/nerf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that achieves state-of-the-art results for synthesizing\nnovel views of complex scenes by optimizing an underlying continuous volumetric\nscene function using a sparse set of input views. Our algorithm represents a\nscene using a fully-connected (non-convolutional) deep network, whose input is\na single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing\ndirection $(\\theta, \\phi)$) and whose output is the volume density and\nview-dependent emitted radiance at that spatial location. We synthesize views\nby querying 5D coordinates along camera rays and use classic volume rendering\ntechniques to project the output colors and densities into an image. Because\nvolume rendering is naturally differentiable, the only input required to\noptimize our representation is a set of images with known camera poses. We\ndescribe how to effectively optimize neural radiance fields to render\nphotorealistic novel views of scenes with complicated geometry and appearance,\nand demonstrate results that outperform prior work on neural rendering and view\nsynthesis. View synthesis results are best viewed as videos, so we urge readers\nto view our supplementary video for convincing comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:57:23 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 22:17:31 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Tancik", "Matthew", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""]]}, {"id": "2003.09053", "submitter": "Dmitry Petrov", "authors": "Dmitry Petrov, Evangelos Kalogerakis", "title": "Cross-Shape Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that processes 3D point clouds by performing graph\nconvolution operations across shapes. In this manner, point descriptors are\nlearned by allowing interaction and propagation of feature representations\nwithin a shape collection. To enable this form of non-local, cross-shape graph\nconvolution, our method learns a pairwise point attention mechanism indicating\nthe degree of interaction between points on different shapes. Our method also\nlearns to create a graph over shapes of an input collection whose edges connect\nshapes deemed as useful for performing cross-shape convolution. The edges are\nalso equipped with learned weights indicating the compatibility of each shape\npair for cross-shape convolution. Our experiments demonstrate that this\ninteraction and propagation of point representations across shapes make them\nmore discriminative. In particular, our results show significantly improved\nperformance for 3D point cloud semantic segmentation compared to conventional\napproaches, especially in cases with the limited number of training examples.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 00:23:10 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 15:53:11 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 18:09:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Petrov", "Dmitry", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "2003.09178", "submitter": "Wenming Tang", "authors": "Wenming Tang, Yuanhao Gong, Kanglin Liu, Jun Liu, Wei Pan, Bozhi Liu,\n  and Guoping Qiu", "title": "Gaussian Curvature Filter on 3D Meshes", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing the Gaussian curvature of meshes can play a fundamental role in 3D\nmesh processing. However, there is a lack of computationally efficient and\nrobust Gaussian curvature optimization method. In this paper, we present a\nsimple yet effective method that can efficiently reduce Gaussian curvature for\n3D meshes. We first present the mathematical foundation of our method. Then, we\nintroduce a simple and robust implicit Gaussian curvature optimization method\nnamed Gaussian Curvature Filter (GCF). GCF implicitly minimizes Gaussian\ncurvature without the need to explicitly calculate the Gaussian curvature\nitself. GCF is highly efficient and this method can be used in a large range of\napplications that involve Gaussian curvature. We conduct extensive experiments\nto demonstrate that GCF significantly outperforms state-of-the-art methods in\nminimizing Gaussian curvature, and geometric feature preserving soothing on 3D\nmeshes. GCF program is available at https://github.com/tangwenming/GCF-filter.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:28:24 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 08:47:48 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Tang", "Wenming", ""], ["Gong", "Yuanhao", ""], ["Liu", "Kanglin", ""], ["Liu", "Jun", ""], ["Pan", "Wei", ""], ["Liu", "Bozhi", ""], ["Qiu", "Guoping", ""]]}, {"id": "2003.09725", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Chris H. Rycroft", "title": "Volumetric density-equalizing reference map with applications", "comments": null, "journal-ref": "Journal of Scientific Computing, 86(3), 41 (2021)", "doi": "10.1007/s10915-021-01411-4", "report-no": null, "categories": "math.NA cs.CG cs.GR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density-equalizing map, a technique developed for cartogram creation, has\nbeen widely applied to data visualization but only for 2D applications. In this\nwork, we propose a novel method called the volumetric density-equalizing\nreference map (VDERM) for computing density-equalizing map for volumetric\ndomains. Given a prescribed density distribution in a volumetric domain in\n$\\mathbb{R}^3$, the proposed method continuously deforms the domain, with\ndifferent volume elements enlarged or shrunk according to the density\ndistribution. With the aid of the proposed method, medical and sociological\ndata can be visualized via deformations of 3D objects. The method can also be\napplied to adaptive remeshing and shape modeling. Furthermore, by exploiting\nthe time-dependent nature of the proposed method, applications to shape\nmorphing can be easily achieved. Experimental results are presented to\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 18:58:51 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Rycroft", "Chris H.", ""]]}, {"id": "2003.09820", "submitter": "Martin Guay", "authors": "Dominik Borer, Lu Yuhang, Laura Wuelfroth, Jakob Buhmann, Martin Guay", "title": "Rig-space Neural Rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Movie productions use high resolution 3d characters with complex proprietary\nrigs to create the highest quality images possible for large displays.\nUnfortunately, these 3d assets are typically not compatible with real-time\ngraphics engines used for games, mixed reality and real-time pre-visualization.\nConsequently, the 3d characters need to be re-modeled and re-rigged for these\nnew applications, requiring weeks of work and artistic approval. Our solution\nto this problem is to learn a compact image-based rendering of the original 3d\ncharacter, conditioned directly on the rig parameters. Our idea is to render\nthe character in many different poses and views, and to train a deep neural\nnetwork to render high resolution images, from the rig parameters directly.\nMany neural rendering techniques have been proposed to render from 2d\nskeletons, or geometry and UV maps. However these require manual work, and to\ndo not remain compatible with the animator workflow of manipulating rig\nwidgets, as well as the real-time game engine pipeline of interpolating rig\nparameters. We extend our architecture to support dynamic re-lighting and\ncomposition with other 3d objects in the scene. We designed a network that\nefficiently generates multiple scene feature maps such as normals, depth,\nalbedo and mask, which are composed with other scene objects to form the final\nimage.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 06:28:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Borer", "Dominik", ""], ["Yuhang", "Lu", ""], ["Wuelfroth", "Laura", ""], ["Buhmann", "Jakob", ""], ["Guay", "Martin", ""]]}, {"id": "2003.09852", "submitter": "Lior Yariv", "authors": "Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen\n  Basri, Yaron Lipman", "title": "Multiview Neural Surface Reconstruction by Disentangling Geometry and\n  Appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the challenging problem of multiview 3D surface\nreconstruction. We introduce a neural network architecture that simultaneously\nlearns the unknown geometry, camera parameters, and a neural renderer that\napproximates the light reflected from the surface towards the camera. The\ngeometry is represented as a zero level-set of a neural network, while the\nneural renderer, derived from the rendering equation, is capable of\n(implicitly) modeling a wide set of lighting conditions and materials. We\ntrained our network on real world 2D images of objects with different material\nproperties, lighting conditions, and noisy camera initializations from the DTU\nMVS dataset. We found our model to produce state of the art 3D surface\nreconstructions with high fidelity, resolution and detail.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 10:20:13 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 14:43:32 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 10:30:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yariv", "Lior", ""], ["Kasten", "Yoni", ""], ["Moran", "Dror", ""], ["Galun", "Meirav", ""], ["Atzmon", "Matan", ""], ["Basri", "Ronen", ""], ["Lipman", "Yaron", ""]]}, {"id": "2003.10333", "submitter": "Difan Liu", "authors": "Difan Liu, Mohamed Nabail, Aaron Hertzmann, Evangelos Kalogerakis", "title": "Neural Contours: Learning to Draw Lines from 3D Shapes", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method for learning to generate line drawings from 3D\nmodels. Our architecture incorporates a differentiable module operating on\ngeometric features of the 3D model, and an image-based module operating on\nview-based shape representations. At test time, geometric and view-based\nreasoning are combined with the help of a neural module to create a line\ndrawing. The model is trained on a large number of crowdsourced comparisons of\nline drawings. Experiments demonstrate that our method achieves significant\nimprovements in line drawing over the state-of-the-art when evaluated on\nstandard benchmarks, resulting in drawings that are comparable to those\nproduced by experienced human artists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:37:49 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:20:05 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 03:22:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Difan", ""], ["Nabail", "Mohamed", ""], ["Hertzmann", "Aaron", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "2003.10435", "submitter": "Przemyslaw Musialski", "authors": "Kurt Leimer, Andreas Winkler, Stefan Ohrhallinger, Przemyslaw\n  Musialski", "title": "Pose to Seat: Automated Design of Body-Supporting Surfaces", "comments": "14 pages, 15 figures", "journal-ref": "Computer Aided Geometric Design Volume 79, May 2020, 101855", "doi": "10.1016/j.cagd.2020.101855", "report-no": null, "categories": "cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The design of functional seating furniture is a complicated process which\noften requires extensive manual design effort and empirical evaluation. We\npropose a computational design framework for pose-driven automated generation\nof body-supports which are optimized for comfort of sitting. Given a human body\nin a specified pose as input, our method computes an approximate pressure\ndistribution that also takes frictional forces and body torques into\nconsideration which serves as an objective measure of comfort. Utilizing this\ninformation to find out where the body needs to be supported in order to\nmaintain comfort of sitting, our algorithm can create a supporting mesh suited\nfor a person in that specific pose. This is done in an automated fitting\nprocess, using a template model capable of supporting a large variety of\nsitting poses. The results can be used directly or can be considered as a\nstarting point for further interactive design.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 21:08:52 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Leimer", "Kurt", ""], ["Winkler", "Andreas", ""], ["Ohrhallinger", "Stefan", ""], ["Musialski", "Przemyslaw", ""]]}, {"id": "2003.10558", "submitter": "Jakub Maksymilian Fober", "authors": "Jakub Maksymilian Fober", "title": "Perspective picture from Visual Sphere: a new approach to image\n  rasterization", "comments": "59 pages, 21 figures, 21 listings, 2 ancillary files, working paper\n  published January 7th 2020 at http://maxfober.space/research.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper alternative method for real-time 3D model rasterization is\ngiven. Surfaces are drawn in perspective-map space which acts as a virtual\ncamera lens. It can render single-pass 360{\\deg} angle of view (AOV) image of\nunlimited shape, view-directions count and unrestrained projection geometry\n(e.g. direct lens distortion, projection mapping, curvilinear perspective),\nnatively aliasing-free. In conjunction to perspective vector map, visual-sphere\nperspective model is proposed. A model capable of combining pictures from\nsources previously incompatible, like fish-eye camera and wide-angle lens\npicture. More so, method is proposed for measurement and simulation of a real\noptical system variable no-parallax point (NPP). This study also explores\nphilosophical and historical aspects of picture perception and presents a guide\nfor perspective design.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:45:26 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 04:22:16 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 23:49:26 GMT"}, {"version": "v4", "created": "Sun, 18 Oct 2020 22:17:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Fober", "Jakub Maksymilian", ""]]}, {"id": "2003.11025", "submitter": "Noura Hamze", "authors": "Noura Hamze, Lukas Nocker, Nikolaus Rauch, Markus Walzth\\\"oni, Fabio\n  Carrillo, Philipp F\\\"urnstahl, and Matthias Harders", "title": "Automatic Modelling of Human Musculoskeletal Ligaments -- Framework\n  Overview and Model Quality Evaluation", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of connective soft tissues is still a challenging task,\nwhich hinders the generation of corresponding geometric models for\nbiomechanical computations. Alternatively, one could predict ligament insertion\nsites and then approximate the shapes, based on anatomical knowledge and\nmorphological studies. Here, we describe a corresponding integrated framework\nfor the automatic modelling of human musculoskeletal ligaments. We combine\nstatistical shape modelling with geometric algorithms to automatically identify\ninsertion sites, based on which geometric surface and volume meshes are\ncreated. For demonstrating a clinical use case, the framework has been applied\nto generate models of the interosseous membrane in the forearm. For the\nadoption to the forearm anatomy, ligament insertion sites in the statistical\nmodel were defined according to anatomical predictions following an approach\nproposed in prior work. For evaluation we compared the generated sites, as well\nas the ligament shapes, to data obtained from a cadaveric study, involving five\nforearms with a total of 15 ligaments. Our framework permitted the creation of\n3D models approximating ligaments' shapes with good fidelity. However, we found\nthat the statistical model trained with the state-of-the-art prediction of the\ninsertion sites was not always reliable. Using that model, average mean square\nerrors as well as Hausdorff distances of the meshes increased by more than one\norder of magnitude, as compared to employing the known insertion locations of\nthe cadaveric study. Using the latter an average mean square error of 0.59 mm\nand an average Hausdorff distance of less than 7 mm resulted, for the complete\nset of ligaments. In conclusion, the presented approach for generating ligament\nshapes from insertion points appears to be feasible but the detection of the\ninsertion sites with a SSM is too inaccurate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:19:29 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Hamze", "Noura", ""], ["Nocker", "Lukas", ""], ["Rauch", "Nikolaus", ""], ["Walzth\u00f6ni", "Markus", ""], ["Carrillo", "Fabio", ""], ["F\u00fcrnstahl", "Philipp", ""], ["Harders", "Matthias", ""]]}, {"id": "2003.11038", "submitter": "Sunnie S. Y. Kim", "authors": "Sunnie S. Y. Kim, Nicholas Kolkin, Jason Salavon, Gregory\n  Shakhnarovich", "title": "Deformable Style Transfer", "comments": "ECCV 2020 (21 pages, 11 figures including the supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both geometry and texture are fundamental aspects of visual style. Existing\nstyle transfer methods, however, primarily focus on texture, almost entirely\nignoring geometry. We propose deformable style transfer (DST), an\noptimization-based approach that jointly stylizes the texture and geometry of a\ncontent image to better match a style image. Unlike previous geometry-aware\nstylization methods, our approach is neither restricted to a particular domain\n(such as human faces), nor does it require training sets of matching\nstyle/content pairs. We demonstrate our method on a diverse set of content and\nstyle images including portraits, animals, objects, scenes, and paintings. Code\nhas been made publicly available at https://github.com/sunniesuhyoung/DST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 18:00:18 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 02:50:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Sunnie S. Y.", ""], ["Kolkin", "Nicholas", ""], ["Salavon", "Jason", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "2003.11133", "submitter": "Tiago Novello", "authors": "Tiago Novello, Vinicius da Silva, Luiz Velho", "title": "Global Illumination of non-Euclidean spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a path tracer algorithm to compute the global\nillumination of non-Euclidean manifolds. We use the 3D torus as an example.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 22:21:28 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Novello", "Tiago", ""], ["da Silva", "Vinicius", ""], ["Velho", "Luiz", ""]]}, {"id": "2003.11148", "submitter": "Pekka Ruusuvuori", "authors": "Kaisa Liimatainen, Leena Latonen, Masi Valkonen, Kimmo Kartasalo,\n  Pekka Ruusuvuori", "title": "Virtual reality for 3D histology: multi-scale visualization of organs\n  with interactive feature exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) enables data visualization in an immersive and engaging\nmanner, and it can be used for creating ways to explore scientific data. Here,\nwe use VR for visualization of 3D histology data, creating a novel interface\nfor digital pathology. Our contribution includes 3D modeling of a whole organ\nand embedded objects of interest, fusing the models with associated\nquantitative features and full resolution serial section patches, and\nimplementing the virtual reality application. Our VR application is multi-scale\nin nature, covering two object levels representing different ranges of detail,\nnamely organ level and sub-organ level. In addition, the application includes\nseveral data layers, including the measured histology image layer and multiple\nrepresentations of quantitative features computed from the histology. In this\ninteractive VR application, the user can set visualization properties, select\ndifferent samples and features, and interact with various objects. In this\nwork, we used whole mouse prostates (organ level) with prostate cancer tumors\n(sub-organ objects of interest) as example cases, and included quantitative\nhistological features relevant for tumor biology in the VR model. Due to\nautomated processing of the histology data, our application can be easily\nadopted to visualize other organs and pathologies from various origins. Our\napplication enables a novel way for exploration of high-resolution,\nmultidimensional data for biomedical research purposes, and can also be used in\nteaching and researcher training.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 23:23:41 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liimatainen", "Kaisa", ""], ["Latonen", "Leena", ""], ["Valkonen", "Masi", ""], ["Kartasalo", "Kimmo", ""], ["Ruusuvuori", "Pekka", ""]]}, {"id": "2003.12227", "submitter": "Steven Gagniere", "authors": "Steven W. Gagniere, David A. B. Hyde, Alan Marquez-Razon, Chenfanfu\n  Jiang, Ziheng Ge, Xuchen Han, Qi Guo, Joseph Teran", "title": "A Hybrid Lagrangian/Eulerian Collocated Advection and Projection Method\n  for Fluid Simulation", "comments": "13 pages, 18 figures", "journal-ref": "Computer Graphics Forum, 39(8): 1-14 (2020)", "doi": "10.1111/cgf.14096", "report-no": null, "categories": "cs.GR cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid particle/grid approach for simulating incompressible\nfluids on collocated velocity grids. We interchangeably use particle and grid\nrepresentations of transported quantities to balance efficiency and accuracy. A\nnovel Backward Semi-Lagrangian method is derived to improve accuracy of grid\nbased advection. Our approach utilizes the implicit formula associated with\nsolutions of Burgers' equation. We solve this equation using Newton's method\nenabled by $C^1$ continuous grid interpolation. We enforce incompressibility\nover collocated, rather than staggered grids. Our projection technique is\nvariational and designed for B-spline interpolation over regular grids where\nmultiquadratic interpolation is used for velocity and multilinear interpolation\nfor pressure. Despite our use of regular grids, we extend the variational\ntechnique to allow for cut-cell definition of irregular flow domains for both\nDirichlet and free surface boundary conditions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 04:09:29 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Gagniere", "Steven W.", ""], ["Hyde", "David A. B.", ""], ["Marquez-Razon", "Alan", ""], ["Jiang", "Chenfanfu", ""], ["Ge", "Ziheng", ""], ["Han", "Xuchen", ""], ["Guo", "Qi", ""], ["Teran", "Joseph", ""]]}, {"id": "2003.12283", "submitter": "Antonio Norelli", "authors": "Luca Cosmo, Antonio Norelli, Oshri Halimi, Ron Kimmel, Emanuele\n  Rodol\\`a", "title": "LIMP: Learning Latent Shape Representations with Metric Preservation\n  Priors", "comments": "24 pages (main article 14 + main bibliography 3 + supplementary 6 +\n  supplementary bibliography 1)", "journal-ref": null, "doi": "10.1007/978-3-030-58580-8_2", "report-no": null, "categories": "cs.LG cs.CG cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advocate the adoption of metric preservation as a powerful\nprior for learning latent representations of deformable 3D shapes. Key to our\nconstruction is the introduction of a geometric distortion criterion, defined\ndirectly on the decoded shapes, translating the preservation of the metric on\nthe decoding to the formation of linear paths in the underlying latent space.\nOur rationale lies in the observation that training samples alone are often\ninsufficient to endow generative models with high fidelity, motivating the need\nfor large training datasets. In contrast, metric preservation provides a\nrigorous way to control the amount of geometric distortion incurring in the\nconstruction of the latent space, leading in turn to synthetic samples of\nhigher quality. We further demonstrate, for the first time, the adoption of\ndifferentiable intrinsic distances in the backpropagation of a geodesic loss.\nOur geometric priors are particularly relevant in the presence of scarce\ntraining data, where learning any meaningful latent structure can be especially\nchallenging. The effectiveness and potential of our generative model is\nshowcased in applications of style transfer, content generation, and shape\ncompletion.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 08:53:34 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 13:14:57 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cosmo", "Luca", ""], ["Norelli", "Antonio", ""], ["Halimi", "Oshri", ""], ["Kimmel", "Ron", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2003.12642", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, Ravi\n  Ramamoorthi", "title": "Deep 3D Capture: Geometry and Reflectance from Sparse Multi-View Images", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel learning-based method to reconstruct the high-quality\ngeometry and complex, spatially-varying BRDF of an arbitrary object from a\nsparse set of only six images captured by wide-baseline cameras under\ncollocated point lighting. We first estimate per-view depth maps using a deep\nmulti-view stereo network; these depth maps are used to coarsely align the\ndifferent views. We propose a novel multi-view reflectance estimation network\narchitecture that is trained to pool features from these coarsely aligned\nimages and predict per-view spatially-varying diffuse albedo, surface normals,\nspecular roughness and specular albedo. We do this by jointly optimizing the\nlatent space of our multi-view reflectance network to minimize the photometric\nerror between images rendered with our predictions and the input images. While\nprevious state-of-the-art methods fail on such sparse acquisition setups, we\ndemonstrate, via extensive experiments on synthetic and real data, that our\nmethod produces high-quality reconstructions that can be used to render\nphotorealistic images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:28:54 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 07:48:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Sunkavalli", "Kalyan", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2003.13326", "submitter": "Amir Hertz", "authors": "Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "PointGMM: a Neural GMM Network for Point Clouds", "comments": "CVPR 2020 -- final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are a popular representation for 3D shapes. However, they encode\na particular sampling without accounting for shape priors or non-local\ninformation. We advocate for the use of a hierarchical Gaussian mixture model\n(hGMM), which is a compact, adaptive and lightweight representation that\nprobabilistically defines the underlying 3D surface. We present PointGMM, a\nneural network that learns to generate hGMMs which are characteristic of the\nshape class, and also coincide with the input point cloud. PointGMM is trained\nover a collection of shapes to learn a class-specific prior. The hierarchical\nrepresentation has two main advantages: (i) coarse-to-fine learning, which\navoids converging to poor local-minima; and (ii) (an unsupervised) consistent\npartitioning of the input shape. We show that as a generative model, PointGMM\nlearns a meaningful latent space which enables generating consistent\ninterpolations between existing shapes, as well as synthesizing novel shapes.\nWe also present a novel framework for rigid registration using PointGMM, that\nlearns to disentangle orientation from structure of an input shape.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:34:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hertz", "Amir", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2003.13510", "submitter": "Yang-tian Sun", "authors": "Yang-Tian Sun, Qian-Cheng Fu, Yue-Ren Jiang, Zitao Liu, Yu-Kun Lai,\n  Hongbo Fu, Lin Gao", "title": "Human Motion Transfer with 3D Constraints and Detail Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for realistic human motion transfer using a\ngenerative adversarial network (GAN), which generates a motion video of a\ntarget character imitating actions of a source character, while maintaining\nhigh authenticity of the generated results. We tackle the problem by decoupling\nand recombining the posture information and appearance information of both the\nsource and target characters. The innovation of our approach lies in the use of\nthe projection of a reconstructed 3D human model as the condition of GAN to\nbetter maintain the structural integrity of transfer results in different\nposes. We further introduce a detail enhancement net to enhance the details of\ntransfer results by exploiting the details in real source frames. Extensive\nexperiments show that our approach yields better results both qualitatively and\nquantitatively than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:33:30 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 14:24:08 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 08:02:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sun", "Yang-Tian", ""], ["Fu", "Qian-Cheng", ""], ["Jiang", "Yue-Ren", ""], ["Liu", "Zitao", ""], ["Lai", "Yu-Kun", ""], ["Fu", "Hongbo", ""], ["Gao", "Lin", ""]]}, {"id": "2003.13834", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Aruni RoyChowdhury, Gopal Sharma, Evangelos\n  Kalogerakis, Liangliang Cao, Erik Learned-Miller, Rui Wang, Subhransu Maji", "title": "Label-Efficient Learning on Point Clouds using Approximate Convex\n  Decompositions", "comments": "First two authors had equal contribution. ECCV'20 version. 19 pages,\n  5 figures", "journal-ref": "16th European Conference on Computer Vision (ECCV 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems of shape classification and part segmentation from 3D point\nclouds have garnered increasing attention in the last few years. Both of these\nproblems, however, suffer from relatively small training sets, creating the\nneed for statistically efficient methods to learn 3D shape representations. In\nthis paper, we investigate the use of Approximate Convex Decompositions (ACD)\nas a self-supervisory signal for label-efficient learning of point cloud\nrepresentations. We show that using ACD to approximate ground truth\nsegmentation provides excellent self-supervision for learning 3D point cloud\nrepresentations that are highly effective on downstream tasks. We report\nimprovements over the state-of-the-art for unsupervised representation learning\non the ModelNet40 shape classification dataset and significant gains in\nfew-shot part segmentation on the ShapeNetPart dataset.Code available at\nhttps://github.com/matheusgadelha/PointCloudLearningACD\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:44:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 21:01:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Gadelha", "Matheus", ""], ["RoyChowdhury", "Aruni", ""], ["Sharma", "Gopal", ""], ["Kalogerakis", "Evangelos", ""], ["Cao", "Liangliang", ""], ["Learned-Miller", "Erik", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2003.13845", "submitter": "Alexandros Lattas", "authors": "Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos\n  Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, Stefanos Zafeiriou", "title": "AvatarMe: Realistically Renderable 3D Facial Reconstruction\n  \"in-the-wild\"", "comments": "Accepted to CVPR2020. Project page: github.com/lattas/AvatarMe with\n  high resolution results, data and more. 10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, with the advent of Generative Adversarial Networks\n(GANs), many face analysis tasks have accomplished astounding performance, with\napplications including, but not limited to, face generation and 3D face\nreconstruction from a single \"in-the-wild\" image. Nevertheless, to the best of\nour knowledge, there is no method which can produce high-resolution\nphotorealistic 3D faces from \"in-the-wild\" images and this can be attributed to\nthe: (a) scarcity of available data for training, and (b) lack of robust\nmethodologies that can successfully be applied on very high-resolution data. In\nthis paper, we introduce AvatarMe, the first method that is able to reconstruct\nphotorealistic 3D faces from a single \"in-the-wild\" image with an increasing\nlevel of detail. To achieve this, we capture a large dataset of facial shape\nand reflectance and build on a state-of-the-art 3D texture and shape\nreconstruction method and successively refine its results, while generating the\nper-pixel diffuse and specular components that are required for realistic\nrendering. As we demonstrate in a series of qualitative and quantitative\nexperiments, AvatarMe outperforms the existing arts by a significant margin and\nreconstructs authentic, 4K by 6K-resolution 3D faces from a single\nlow-resolution image that, for the first time, bridges the uncanny valley.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:17:54 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lattas", "Alexandros", ""], ["Moschoglou", "Stylianos", ""], ["Gecer", "Baris", ""], ["Ploumpis", "Stylianos", ""], ["Triantafyllou", "Vasileios", ""], ["Ghosh", "Abhijeet", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2003.13912", "submitter": "C.-H. Huck Yang", "authors": "Hao-Hsiang Yang, Chao-Han Huck Yang, Yi-Chang James Tsai", "title": "Y-net: Multi-scale feature aggregation network with wavelet structure\n  similarity loss function for single image dehazing", "comments": "Accepted to IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image dehazing is the ill-posed two-dimensional signal reconstruction\nproblem. Recently, deep convolutional neural networks (CNN) have been\nsuccessfully used in many computer vision problems. In this paper, we propose a\nY-net that is named for its structure. This network reconstructs clear images\nby aggregating multi-scale features maps. Additionally, we propose a Wavelet\nStructure SIMilarity (W-SSIM) loss function in the training step. In the\nproposed loss function, discrete wavelet transforms are applied repeatedly to\ndivide the image into differently sized patches with different frequencies and\nscales. The proposed loss function is the accumulation of SSIM loss of various\npatches with respective ratios. Extensive experimental results demonstrate that\nthe proposed Y-net with the W-SSIM loss function restores high-quality clear\nimages and outperforms state-of-the-art algorithms. Code and models are\navailable at https://github.com/dectrfov/Y-net.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:07:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Yang", "Chao-Han Huck", ""], ["Tsai", "Yi-Chang James", ""]]}]