[{"id": "1312.0042", "submitter": "Bojian Xu", "authors": "Bojian Xu", "title": "Boosting the Basic Counting on Distributed Streams", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the classic basic counting problem in the distributed streaming\nmodel that was studied by Gibbons and Tirthapura (GT). In the solution for\nmaintaining an $(\\epsilon,\\delta)$-estimate, as what GT's method does, we make\nthe following new contributions: (1) For a bit stream of size $n$, where each\nbit has a probability at least $\\gamma$ to be 1, we exponentially reduced the\naverage total processing time from GT's $\\Theta(n \\log(1/\\delta))$ to\n$O((1/(\\gamma\\epsilon^2))(\\log^2 n) \\log(1/\\delta))$, thus providing the first\nsublinear-time streaming algorithm for this problem. (2) In addition to an\noverall much faster processing speed, our method provides a new tradeoff that a\nlower accuracy demand (a larger value for $\\epsilon$) promises a faster\nprocessing speed, whereas GT's processing speed is $\\Theta(n \\log(1/\\delta))$\nin any case and for any $\\epsilon$. (3) The worst-case total time cost of our\nmethod matches GT's $\\Theta(n\\log(1/\\delta))$, which is necessary but rarely\noccurs in our method. (4) The space usage overhead in our method is a lower\norder term compared with GT's space usage and occurs only $O(\\log n)$ times\nduring the stream processing and is too negligible to be detected by the\noperating system in practice. We further validate these solid theoretical\nresults with experiments on both real-world and synthetic data, showing that\nour method is faster than GT's by a factor of several to several thousands\ndepending on the stream size and accuracy demands, without any detectable space\nusage overhead. Our method is based on a faster sampling technique that we\ndesign for boosting GT's method and we believe this technique can be of other\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 23:56:52 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Xu", "Bojian", ""]]}, {"id": "1312.0086", "submitter": "Filomena Ferrucci", "authors": "Filomena Ferrucci, M-Tahar Kechadi, Pasquale Salza, Federica Sarro", "title": "A Framework for Genetic Algorithms Based on Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GAs) are powerful metaheuristic techniques mostly used in\nmany real-world applications. The sequential execution of GAs requires\nconsiderable computational power both in time and resources. Nevertheless, GAs\nare naturally parallel and accessing a parallel platform such as Cloud is easy\nand cheap. Apache Hadoop is one of the common services that can be used for\nparallel applications. However, using Hadoop to develop a parallel version of\nGAs is not simple without facing its inner workings. Even though some\nsequential frameworks for GAs already exist, there is no framework supporting\nthe development of GA applications that can be executed in parallel. In this\npaper is described a framework for parallel GAs on the Hadoop platform,\nfollowing the paradigm of MapReduce. The main purpose of this framework is to\nallow the user to focus on the aspects of GA that are specific to the problem\nto be addressed, being sure that this task is going to be correctly executed on\nthe Cloud with a good performance. The framework has been also exploited to\ndevelop an application for Feature Subset Selection problem. A preliminary\nanalysis of the performance of the developed GA application has been performed\nusing three datasets and shown very promising performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 10:41:29 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 23:01:10 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Ferrucci", "Filomena", ""], ["Kechadi", "M-Tahar", ""], ["Salza", "Pasquale", ""], ["Sarro", "Federica", ""]]}, {"id": "1312.0114", "submitter": "Mamoon Rashid", "authors": "Mamoon Rashid, Er. Rishma Chawla", "title": "Extended Role Based Access Control with Blob Service on Cloud", "comments": "6 page and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Role-based access control (RBAC) models have generated a great interest in\nthe security community as a powerful and generalized approach to security\nmanagement and ability to model organizational structure and their capability\nto reduce administrative expenses. In this paper, we highlight the drawbacks of\nlatest developed RBAC models in terms of access control and authorization and\nlater provide a more viable extended-RBAC model, which enhances and extends its\npowers to make any system more secure by adding valuable constraints. Later the\nBlobs are stored on cloud server which is then accessed by the end users via\nthis Extended RBAC model.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 15:06:21 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Rashid", "Mamoon", ""], ["Chawla", "Er. Rishma", ""]]}, {"id": "1312.0193", "submitter": "Hyokun Yun", "authors": "Hyokun Yun, Hsiang-Fu Yu, Cho-Jui Hsieh, S.V.N. Vishwanathan, Inderjit\n  Dhillon", "title": "NOMAD: Non-locking, stOchastic Multi-machine algorithm for Asynchronous\n  and Decentralized matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient parallel distributed algorithm for matrix completion,\nnamed NOMAD (Non-locking, stOchastic Multi-machine algorithm for Asynchronous\nand Decentralized matrix completion). NOMAD is a decentralized algorithm with\nnon-blocking communication between processors. One of the key features of NOMAD\nis that the ownership of a variable is asynchronously transferred between\nprocessors in a decentralized fashion. As a consequence it is a lock-free\nparallel algorithm. In spite of being an asynchronous algorithm, the variable\nupdates of NOMAD are serializable, that is, there is an equivalent update\nordering in a serial implementation. NOMAD outperforms synchronous algorithms\nwhich require explicit bulk synchronization after every iteration: our\nextensive empirical evaluation shows that not only does our algorithm perform\nwell in distributed setting on commodity hardware, but also outperforms\nstate-of-the-art algorithms on a HPC cluster both in multi-core and distributed\nmemory settings.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 08:56:36 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 09:59:12 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Yun", "Hyokun", ""], ["Yu", "Hsiang-Fu", ""], ["Hsieh", "Cho-Jui", ""], ["Vishwanathan", "S. V. N.", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "1312.0249", "submitter": "Mohamed Gharzouli Dr", "authors": "Mohamed Gharzouli", "title": "Reuse of existing applications during the development of Enterprise\n  Portals integrating Web Services", "comments": "06 pages, 4 figures, 10th Maghrebian Conference on Information\n  Technologies MCSEAI08, April 28-30 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During these last years, the use of the web technologies in the enterprises\nbecomes an essential factor to define a new business model. Among these\ntechnologies, web services and enterprise portals have gathered to integrate\nexisting heterogeneous systems such as e-commerce, eservices hub and\ne-learning. However, the design and the modeling of the portals, on the one\nhand, and their integration with the existing applications, on the other hand,\nare still two points open for discussion. The first problem is related to the\ndevelopment of the lifecycle that can be used for designing and modeling the\nenterprise portals. For the second problem, which is the integration of the\nexisting applications, the discussion is intended towards the use of\ntechnologies based on web services. In This paper, we present a software\nengineering solution for the development of Web services-based enterprise\nportals.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 17:09:44 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Gharzouli", "Mohamed", ""]]}, {"id": "1312.0466", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov", "title": "Intensional Cyberforensics", "comments": "412 pages, 94 figures, 18 tables, 19 algorithms and listings; PhD\n  thesis; v2 corrects some typos and refs; also available on Spectrum at\n  http://spectrum.library.concordia.ca/977460/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LO cs.NI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the application of intensional logic to cyberforensic\nanalysis and its benefits and difficulties are compared with the\nfinite-state-automata approach. This work extends the use of the intensional\nprogramming paradigm to the modeling and implementation of a cyberforensics\ninvestigation process with backtracing of event reconstruction, in which\nevidence is modeled by multidimensional hierarchical contexts, and proofs or\ndisproofs of claims are undertaken in an eductive manner of evaluation. This\napproach is a practical, context-aware improvement over the finite state\nautomata (FSA) approach we have seen in previous work. As a base implementation\nlanguage model, we use in this approach a new dialect of the Lucid programming\nlanguage, called Forensic Lucid, and we focus on defining hierarchical contexts\nbased on intensional logic for the distributed evaluation of cyberforensic\nexpressions. We also augment the work with credibility factors surrounding\ndigital evidence and witness accounts, which have not been previously modeled.\nThe Forensic Lucid programming language, used for this intensional\ncyberforensic analysis, formally presented through its syntax and operational\nsemantics. In large part, the language is based on its predecessor and\ncodecessor Lucid dialects, such as GIPL, Indexical Lucid, Lucx, Objective\nLucid, and JOOIP bound by the underlying intensional programming paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 14:16:48 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 19:06:53 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Mokhov", "Serguei A.", ""]]}, {"id": "1312.0499", "submitter": "Christoph Neumann", "authors": "Nicolas Le Scouarnec, Christoph Neumann, Gilles Straub", "title": "Cache policies for cloud-based systems: To keep or not to keep", "comments": "Proceedings of IEEE International Conference on Cloud Computing 2014\n  (CLOUD 14)", "journal-ref": null, "doi": "10.1109/CLOUD.2014.11", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study cache policies for cloud-based caching. Cloud-based\ncaching uses cloud storage services such as Amazon S3 as a cache for data items\nthat would have been recomputed otherwise. Cloud-based caching departs from\nclassical caching: cloud resources are potentially infinite and only paid when\nused, while classical caching relies on a fixed storage capacity and its main\nmonetary cost comes from the initial investment. To deal with this new context,\nwe design and evaluate a new caching policy that minimizes the overall cost of\na cloud-based system. The policy takes into account the frequency of\nconsumption of an item and the cloud cost model. We show that this policy is\neasier to operate, that it scales with the demand and that it outperforms\nclassical policies managing a fixed capacity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 16:21:25 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 08:05:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Scouarnec", "Nicolas Le", ""], ["Neumann", "Christoph", ""], ["Straub", "Gilles", ""]]}, {"id": "1312.0510", "submitter": "Andrey Demichev", "authors": "A. Demichev, V. Ilyin, A. Kryukov and S. Polyakov", "title": "Fault Tolerance of Small-World Regular and Stochastic Interconnection\n  Networks", "comments": "9 pages, 5 figures; LaTeX; typos corrected, figures improved,\n  acknowledgements added", "journal-ref": "Vychisl. Metody Programm (Numerical Methods and Programming)\n  vol.15 (2014) 36 - 48 (in Russian)", "doi": null, "report-no": null, "categories": "cs.SI cs.DC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resilience of the most important properties of stochastic and regular\n(deterministic) small-world interconnection networks is studied. It is shown\nthat in the broad range of values of the fraction of faulty nodes the networks\nunder consideration possess high fault tolerance, the deterministic networks\nbeing slightly better than the stochastic ones.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 13:53:10 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 07:09:35 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Demichev", "A.", ""], ["Ilyin", "V.", ""], ["Kryukov", "A.", ""], ["Polyakov", "S.", ""]]}, {"id": "1312.0742", "submitter": "Leandro Pacheco", "authors": "Leandro Pacheco, Daniele Sciascia, Fernando Pedone", "title": "Parallel Deferred Update Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deferred update replication (DUR) is an established approach to implementing\nhighly efficient and available storage. While the throughput of read-only\ntransactions scales linearly with the number of deployed replicas in DUR, the\nthroughput of update transactions experiences limited improvements as replicas\nare added. This paper presents Parallel Deferred Update Replication (P-DUR), a\nvariation of classical DUR that scales both read-only and update transactions\nwith the number of cores available in a replica. In addition to introducing the\nnew approach, we describe its full implementation and compare its performance\nto classical DUR and to Berkeley DB, a well-known standalone database.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 09:17:22 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Pacheco", "Leandro", ""], ["Sciascia", "Daniele", ""], ["Pedone", "Fernando", ""]]}, {"id": "1312.0910", "submitter": "Derek Groen", "authors": "Derek Groen, Steven Rieder and Simon Portegies Zwart", "title": "MPWide: a light-weight library for efficient message passing over wide\n  area networks", "comments": "accepted by the Journal Of Open Research Software, 13 pages, 4\n  figures, 1 table", "journal-ref": "Journal of Open Research Software 1(1):e9, 2013", "doi": "10.5334/jors.ah", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present MPWide, a light weight communication library which allows\nefficient message passing over a distributed network. MPWide has been designed\nto connect application running on distributed (super)computing resources, and\nto maximize the communication performance on wide area networks for those\nwithout administrative privileges. It can be used to provide message-passing\nbetween application, move files, and make very fast connections in\nclient-server environments. MPWide has already been applied to enable\ndistributed cosmological simulations across up to four supercomputers on two\ncontinents, and to couple two different bloodflow simulations to form a\nmultiscale simulation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 19:17:57 GMT"}], "update_date": "2014-01-06", "authors_parsed": [["Groen", "Derek", ""], ["Rieder", "Steven", ""], ["Zwart", "Simon Portegies", ""]]}, {"id": "1312.0917", "submitter": "Peter Wittich", "authors": "A. Gianelle, S. Amerio, D. Bastieri, M. Corvo, W. Ketchum, T. Liu, A.\n  Lonardo, D. Lucchesi, S. Poprocki, R. Rivera, L. Tosoratto, P. Vicini, P.\n  Wittich", "title": "Applications of Many-Core Technologies to On-line Event Reconstruction\n  in High Energy Physics Experiments", "comments": "Proceedings for 2013 IEEE NSS/MIC conference; fixed author list\n  omission", "journal-ref": null, "doi": "10.1109/NSSMIC.2013.6829552", "report-no": null, "categories": "physics.ins-det cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in many-core architectures applied to real time selections is\ngrowing in High Energy Physics (HEP) experiments. In this paper we describe\nperformance measurements of many-core devices when applied to a typical HEP\nonline task: the selection of events based on the trajectories of charged\nparticles. We use as benchmark a scaled-up version of the algorithm used at CDF\nexperiment at Tevatron for online track reconstruction - the SVT algorithm - as\na realistic test-case for low-latency trigger systems using new computing\narchitectures for LHC experiment. We examine the complexity/performance\ntrade-off in porting existing serial algorithms to many-core devices. We\nmeasure performance of different architectures (Intel Xeon Phi and AMD GPUs, in\naddition to NVidia GPUs) and different software environments (OpenCL, in\naddition to NVidia CUDA). Measurements of both data processing and data\ntransfer latency are shown, considering different I/O strategies to/from the\nmany-core devices.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 19:55:16 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 11:50:21 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Gianelle", "A.", ""], ["Amerio", "S.", ""], ["Bastieri", "D.", ""], ["Corvo", "M.", ""], ["Ketchum", "W.", ""], ["Liu", "T.", ""], ["Lonardo", "A.", ""], ["Lucchesi", "D.", ""], ["Poprocki", "S.", ""], ["Rivera", "R.", ""], ["Tosoratto", "L.", ""], ["Vicini", "P.", ""], ["Wittich", "P.", ""]]}, {"id": "1312.1031", "submitter": "Tianbao Yang", "authors": "Tianbao Yang, Shenghuo Zhu, Rong Jin, Yuanqing Lin", "title": "Analysis of Distributed Stochastic Dual Coordinate Ascent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\citep{Yangnips13}, the author presented distributed stochastic dual\ncoordinate ascent (DisDCA) algorithms for solving large-scale regularized loss\nminimization. Extraordinary performances have been observed and reported for\nthe well-motivated updates, as referred to the practical updates, compared to\nthe naive updates. However, no serious analysis has been provided to understand\nthe updates and therefore the convergence rates. In the paper, we bridge the\ngap by providing a theoretical analysis of the convergence rates of the\npractical DisDCA algorithm. Our analysis helped by empirical studies has shown\nthat it could yield an exponential speed-up in the convergence by increasing\nthe number of dual updates at each iteration. This result justifies the\nsuperior performances of the practical DisDCA as compared to the naive variant.\nAs a byproduct, our analysis also reveals the convergence behavior of the\none-communication DisDCA.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 05:48:30 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 22:13:17 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Yang", "Tianbao", ""], ["Zhu", "Shenghuo", ""], ["Jin", "Rong", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1312.1085", "submitter": "Franck Iutzeler", "authors": "Franck Iutzeler, Pascal Bianchi, Philippe Ciblat, and Walid Hachem", "title": "Explicit Convergence Rate of a Distributed Alternating Direction Method\n  of Multipliers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of N agents seeking to solve distributively the minimization\nproblem $\\inf_{x} \\sum_{n = 1}^N f_n(x)$ where the convex functions $f_n$ are\nlocal to the agents. The popular Alternating Direction Method of Multipliers\nhas the potential to handle distributed optimization problems of this kind. We\nprovide a general reformulation of the problem and obtain a class of\ndistributed algorithms which encompass various network architectures. The rate\nof convergence of our method is considered. It is assumed that the infimum of\nthe problem is reached at a point $x_\\star$, the functions $f_n$ are twice\ndifferentiable at this point and $\\sum \\nabla^2 f_n(x_\\star) > 0$ in the\npositive definite ordering of symmetric matrices. With these assumptions, it is\nshown that the convergence to the consensus $x_\\star$ is linear and the exact\nrate is provided. Application examples where this rate can be optimized with\nrespect to the ADMM free parameter $\\rho$ are also given.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 09:53:48 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 09:59:14 GMT"}, {"version": "v3", "created": "Sun, 28 Dec 2014 14:15:27 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Iutzeler", "Franck", ""], ["Bianchi", "Pascal", ""], ["Ciblat", "Philippe", ""], ["Hachem", "Walid", ""]]}, {"id": "1312.1187", "submitter": "Volker Weinberg", "authors": "Bruce D. Scott, Volker Weinberg, Olivier Hoenen, Anupam Karmakar, Luis\n  Fazendeiro", "title": "Scalability of the plasma physics code GEM", "comments": "9 pages, 6 figures, PRACE Whitepaper", "journal-ref": null, "doi": null, "report-no": "WP125", "categories": "cs.DC cs.PF physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a detailed weak scaling analysis of GEM, a 3D MPI-parallelised\ngyrofluid code used in theoretical plasma physics at the Max Planck Institute\nof Plasma Physics, IPP at Garching b. M\\\"unchen, Germany. Within a PRACE\nPreparatory Access Project various versions of the code have been analysed on\nthe HPC systems SuperMUC at LRZ and JUQUEEN at J\\\"ulich Supercomputing Centre\n(JSC) to improve the parallel scalability of the application. The diagnostic\ntool Scalasca has been used to filter out suboptimal routines. The code uses\nthe electromagnetic gyrofluid model which is a superset of magnetohydrodynamic\nand drift-Alfv\\'en microturbulance and also includes several relevant kinetic\nprocesses. GEM can be used with different geometries depending on the targeted\nuse case, and has been proven to show good scalability when the computational\ndomain is distributed amongst two dimensions. Such a distribution allows grids\nwith sufficient size to describe small scale tokamak devices. In order to\nenable simulation of very large tokamaks (such as the next generation nuclear\nfusion device ITER in Cadarache, France) the third dimension has been\nparallelised and weak scaling has been achieved for significantly larger grids.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 14:43:50 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 10:34:45 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Scott", "Bruce D.", ""], ["Weinberg", "Volker", ""], ["Hoenen", "Olivier", ""], ["Karmakar", "Anupam", ""], ["Fazendeiro", "Luis", ""]]}, {"id": "1312.1225", "submitter": "Alasdair Armstrong", "authors": "Alasdair Armstrong, Victor B. F. Gomes, Georg Struth", "title": "Algebraic Principles for Rely-Guarantee Style Concurrency Verification\n  Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple equational principles for deriving rely-guarantee-style\ninference rules and refinement laws based on idempotent semirings. We link the\nalgebraic layer with concrete models of programs based on languages and\nexecution traces. We have implemented the approach in Isabelle/HOL as a\nlightweight concurrency verification tool that supports reasoning about the\ncontrol and data flow of concurrent programs with shared variables at different\nlevels of abstraction. This is illustrated on two simple verification examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 16:03:39 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Armstrong", "Alasdair", ""], ["Gomes", "Victor B. F.", ""], ["Struth", "Georg", ""]]}, {"id": "1312.1961", "submitter": "Christian  Lavault", "authors": "Marc Bui (CHART), Franck Butelle (LIPN), Christian Lavault (LIPN)", "title": "A Distributed Algorithm for Constructing a Minimum Diameter Spanning\n  Tree", "comments": "Comments: 11 pages LaTeX, 2 figures; International Journal with\n  referees article; New version (full paper design): results added in Section\n  2.2 and 2.2; typos removed", "journal-ref": "Journal of Parallel and Distributed Computing 64, 5 (2004) 571-577", "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm, which solves the problem of distributively\nfinding a minimum diameter spanning tree of any (non-negatively) real-weighted\ngraph $G = (V,E,\\omega)$. As an intermediate step, we use a new, fast,\nlinear-time all-pairs shortest paths distributed algorithm to find an absolute\ncenter of $G$. The resulting distributed algorithm is asynchronous, it works\nfor named asynchronous arbitrary networks and achieves $\\mathcal{O}(|V|)$ time\ncomplexity and $\\mathcal{O}\\left(|V|\\,|E|\\right)$\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 19:04:02 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 08:09:33 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Bui", "Marc", "", "CHART"], ["Butelle", "Franck", "", "LIPN"], ["Lavault", "Christian", "", "LIPN"]]}, {"id": "1312.2074", "submitter": "Ranjan Kumar", "authors": "Ranjan Kumar and G.Sahoo", "title": "Load Balancing using Ant Colony in Cloud Computing", "comments": "5 pages, 1 figure, 1 table", "journal-ref": "International Journal of Information Technology Convergence and\n  Services (IJITCS) Vol.3, No.5, Pp- 01-05, October 2013", "doi": "10.2013/VOL3/NO.5", "report-no": null, "categories": "cs.DC cs.CY cs.SY", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Ants are very small insects.They are capable to find food even they are\ncomplete blind. The ants lives in their nest and their job is to search food\nwhile they get hungry. We are not interested in their living style, such as how\nthey live, how they sleep. But we are interested in how they search for food,\nand how they find the shortest path. The technique for finding the shortest\npath are now applying in cloud computing. The Ant Colony approach towards Cloud\nComputing gives better performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 09:10:16 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Kumar", "Ranjan", ""], ["Sahoo", "G.", ""]]}, {"id": "1312.2501", "submitter": "Jesper Larsson Tr\\\"aff", "authors": "Martin Wimmer and Daniel Cederman and Francesco Versaci and Jesper\n  Larsson Tr\\\"aff and Philippas Tsigas", "title": "Data Structures for Task-based Priority Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many task-parallel applications can benefit from attempting to execute tasks\nin a specific order, as for instance indicated by priorities associated with\nthe tasks. We present three lock-free data structures for priority scheduling\nwith different trade-offs on scalability and ordering guarantees. First we\npropose a basic extension to work-stealing that provides good scalability, but\ncannot provide any guarantees for task-ordering in-between threads. Next, we\npresent a centralized priority data structure based on $k$-fifo queues, which\nprovides strong (but still relaxed with regard to a sequential specification)\nguarantees. The parameter $k$ allows to dynamically configure the trade-off\nbetween scalability and the required ordering guarantee. Third, and finally, we\ncombine both data structures into a hybrid, $k$-priority data structure, which\nprovides scalability similar to the work-stealing based approach for larger\n$k$, while giving strong ordering guarantees for smaller $k$. We argue for\nusing the hybrid data structure as the best compromise for generic,\npriority-based task-scheduling.\n  We analyze the behavior and trade-offs of our data structures in the context\nof a simple parallelization of Dijkstra's single-source shortest path\nalgorithm. Our theoretical analysis and simulations show that both the\ncentralized and the hybrid $k$-priority based data structures can give strong\nguarantees on the useful work performed by the parallel Dijkstra algorithm. We\nsupport our results with experimental evidence on an 80-core Intel Xeon system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 16:27:10 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Wimmer", "Martin", ""], ["Cederman", "Daniel", ""], ["Versaci", "Francesco", ""], ["Tr\u00e4ff", "Jesper Larsson", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1312.2570", "submitter": "Christian Lavault", "authors": "Sidi Mohamed Sedjelmaci (LIPN), Christian Lavault (LIPN)", "title": "A New Modular Division Algorithm and Applications", "comments": "12 pages", "journal-ref": "International Conference on Theoretical Computer Science\n  (ICTCS98), Pisa : Italy (1998)", "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper proposes a new parallel algorithm for the modular division\n$u/v\\bmod \\beta^s$, where $u,\\; v,\\; \\beta$ and $s$ are positive integers\n$(\\beta\\ge 2)$. The algorithm combines the classical add-and-shift\nmultiplication scheme with a new propagation carry technique. This \"Pen and\nPaper Inverse\" ({\\em PPI}) algorithm, is better suited for systolic\nparallelization in a \"least-significant digit first\" pipelined manner. Although\nit is equivalent to Jebelean's modular division algorithm~\\cite{jeb2} in terms\nof performance (time complexity, work, efficiency), the linear parallelization\nof the {\\em PPI} algorithm improves on the latter when the input size is large.\nThe parallelized versions of the {\\em PPI} algorithm leads to various\napplications, such as the exact division and the digit modulus operation (dmod)\nof two long integers. It is also applied to the determination of the periods of\nrational numbers as well as their $p$-adic expansion in any radix $\\beta \\ge\n2$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 20:34:42 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Sedjelmaci", "Sidi Mohamed", "", "LIPN"], ["Lavault", "Christian", "", "LIPN"]]}, {"id": "1312.2628", "submitter": "Ibrahim Umar", "authors": "Ibrahim Umar, Otto Anshus, Phuong Ha (University of Troms{\\o})", "title": "DeltaTree: A Practical Locality-aware Concurrent Search Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": "IFI-UIT Technical Report 2013-74", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As other fundamental programming abstractions in energy-efficient computing,\nsearch trees are expected to support both high parallelism and data locality.\nHowever, existing highly-concurrent search trees such as red-black trees and\nAVL trees do not consider data locality while existing locality-aware search\ntrees such as those based on the van Emde Boas layout (vEB-based trees), poorly\nsupport concurrent (update) operations.\n  This paper presents DeltaTree, a practical locality-aware concurrent search\ntree that combines both locality-optimisation techniques from vEB-based trees\nand concurrency-optimisation techniques from non-blocking highly-concurrent\nsearch trees. DeltaTree is a $k$-ary leaf-oriented tree of DeltaNodes in which\neach DeltaNode is a size-fixed tree-container with the van Emde Boas layout.\nThe expected memory transfer costs of DeltaTree's Search, Insert, and Delete\noperations are $O(\\log_B N)$, where $N, B$ are the tree size and the unknown\nmemory block size in the ideal cache model, respectively. DeltaTree's Search\noperation is wait-free, providing prioritised lanes for Search operations, the\ndominant operation in search trees. Its Insert and {\\em Delete} operations are\nnon-blocking to other Search, Insert, and Delete operations, but they may be\noccasionally blocked by maintenance operations that are sometimes triggered to\nkeep DeltaTree in good shape. Our experimental evaluation using the latest\nimplementation of AVL, red-black, and speculation friendly trees from the\nSynchrobench benchmark has shown that DeltaTree is up to 5 times faster than\nall of the three concurrent search trees for searching operations and up to 1.6\ntimes faster for update operations when the update contention is not too high.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 23:30:02 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Umar", "Ibrahim", "", "University of Troms\u00f8"], ["Anshus", "Otto", "", "University of Troms\u00f8"], ["Ha", "Phuong", "", "University of Troms\u00f8"]]}, {"id": "1312.2698", "submitter": "EPTCS", "authors": "Luca Padovani (Dipartimento di Informatica, Universit\\`a di Torino)", "title": "From Lock Freedom to Progress Using Session Types", "comments": "In Proceedings PLACES 2013, arXiv:1312.2218", "journal-ref": "EPTCS 137, 2013, pp. 3-19", "doi": "10.4204/EPTCS.137.2", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by Kobayashi's type system for lock freedom, we define a behavioral\ntype system for ensuring progress in a language of binary sessions. The key\nidea is to annotate actions in session types with priorities representing the\nurgency with which such actions must be performed and to verify that processes\nperform such actions with the required priority. Compared to related systems\nfor session-based languages, the presented type system is relatively simpler\nand establishes progress for a wider range of processes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:03:18 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Padovani", "Luca", "", "Dipartimento di Informatica, Universit\u00e0 di Torino"]]}, {"id": "1312.2701", "submitter": "EPTCS", "authors": "Laura Bocchi (Imperial College, London), Romain Demangeon (Imperial\n  College, London)", "title": "Embedding Session Types in HML", "comments": "In Proceedings PLACES 2013, arXiv:1312.2218", "journal-ref": "EPTCS 137, 2013, pp. 53-62", "doi": "10.4204/EPTCS.137.5", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on the enhancement of multiparty session types with logical\nannotations enable the effective verification of properties on (1) the\nstructure of the conversations, (2) the sorts of the messages, and (3) the\nactual values exchanged. In [3] we extend this work to enable the specification\nand verification of mutual effects of multiple cross-session interactions. Here\nwe give a sound and complete embedding into the Hennessy-Milner logic to\njustify the expressiveness of the approach in [3] and to provide it with a\nlogical background that will enable us to compare it with similar approaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:03:49 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Bocchi", "Laura", "", "Imperial College, London"], ["Demangeon", "Romain", "", "Imperial\n  College, London"]]}, {"id": "1312.2703", "submitter": "EPTCS", "authors": "Ashkan Tousimojarad, Wim Vanderbauwhede", "title": "The Glasgow Parallel Reduction Machine: Programming Shared-memory\n  Many-core Systems using Parallel Task Composition", "comments": "In Proceedings PLACES 2013, arXiv:1312.2218", "journal-ref": "EPTCS 137, 2013, pp. 79-94", "doi": "10.4204/EPTCS.137.7", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Glasgow Parallel Reduction Machine (GPRM), a novel, flexible\nframework for parallel task-composition based many-core programming. We allow\nthe programmer to structure programs into task code, written as C++ classes,\nand communication code, written in a restricted subset of C++ with functional\nsemantics and parallel evaluation. In this paper we discuss the GPRM, the\nvirtual machine framework that enables the parallel task composition approach.\nWe focus the discussion on GPIR, the functional language used as the\nintermediate representation of the bytecode running on the GPRM. Using examples\nin this language we show the flexibility and power of our task composition\nframework. We demonstrate the potential using an implementation of a merge sort\nalgorithm on a 64-core Tilera processor, as well as on a conventional Intel\nquad-core processor and an AMD 48-core processor system. We also compare our\nframework with OpenMP tasks in a parallel pointer chasing algorithm running on\nthe Tilera processor. Our results show that the GPRM programs outperform the\ncorresponding OpenMP codes on all test platforms, and can greatly facilitate\nwriting of parallel programs, in particular non-data parallel algorithms such\nas reductions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:04:07 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""]]}, {"id": "1312.2705", "submitter": "EPTCS", "authors": "Eduardo R. B. Marques (LASIGE/FCUL, Universidade of Lisbon), Francisco\n  Martins (LASIGE/FCUL, Universidade of Lisbon), Vasco T. Vasconcelos\n  (LASIGE/FCUL, Universidade of Lisbon), Nicholas Ng (Imperial College London),\n  Nuno Martins (LASIGE/FCUL, Universidade of Lisbon)", "title": "Towards deductive verification of MPI programs against session types", "comments": "In Proceedings PLACES 2013, arXiv:1312.2218", "journal-ref": "EPTCS 137, 2013, pp. 103-113", "doi": "10.4204/EPTCS.137.9", "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Message Passing Interface (MPI) is the de facto standard message-passing\ninfrastructure for developing parallel applications. Two decades after the\nfirst version of the library specification, MPI-based applications are nowadays\nroutinely deployed on super and cluster computers. These applications, written\nin C or Fortran, exhibit intricate message passing behaviours, making it hard\nto statically verify important properties such as the absence of deadlocks. Our\nwork builds on session types, a theory for describing protocols that provides\nfor correct-by-construction guarantees in this regard. We annotate MPI\nprimitives and C code with session type contracts, written in the language of a\nsoftware verifier for C. Annotated code is then checked for correctness with\nthe software verifier. We present preliminary results and discuss the\nchallenges that lie ahead for verifying realistic MPI program compliance\nagainst session types.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:04:28 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Marques", "Eduardo R. B.", "", "LASIGE/FCUL, Universidade of Lisbon"], ["Martins", "Francisco", "", "LASIGE/FCUL, Universidade of Lisbon"], ["Vasconcelos", "Vasco T.", "", "LASIGE/FCUL, Universidade of Lisbon"], ["Ng", "Nicholas", "", "Imperial College London"], ["Martins", "Nuno", "", "LASIGE/FCUL, Universidade of Lisbon"]]}, {"id": "1312.2706", "submitter": "EPTCS", "authors": "Romain Demeyer, Wim Vanhoof", "title": "Static Application-Level Race Detection in STM Haskell using Contracts", "comments": "In Proceedings PLACES 2013, arXiv:1312.2218. rde@info.fundp.ac.be;\n  wim.vanhoof@unamur.be", "journal-ref": "EPTCS 137, 2013, pp. 115-134", "doi": "10.4204/EPTCS.137.10", "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing concurrent programs is a hard task, even when using high-level\nsynchronization primitives such as transactional memories together with a\nfunctional language with well-controlled side-effects such as Haskell, because\nthe interferences generated by the processes to each other can occur at\ndifferent levels and in a very subtle way. The problem occurs when a thread\nleaves or exposes the shared data in an inconsistent state with respect to the\napplication logic or the real meaning of the data. In this paper, we propose to\nassociate contracts to transactions and we define a program transformation that\nmakes it possible to extend static contract checking in the context of STM\nHaskell. As a result, we are able to check statically that each transaction of\na STM Haskell program handles the shared data in a such way that a given\nconsistency property, expressed in the form of a user-defined boolean function,\nis preserved. This ensures that bad interference will not occur during the\nexecution of the concurrent program.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:04:37 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Demeyer", "Romain", ""], ["Vanhoof", "Wim", ""]]}, {"id": "1312.2806", "submitter": "Susumu Matsumae", "authors": "Susumu Matsumae, Fukuhito Ooshita", "title": "Hierarchical Low Power Consumption Technique with Location Information\n  for Sensor Networks", "comments": "6 pages, 9 figures, 2 tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 4 Issue 4, 2013", "doi": "10.14569/IJACSA.2013.040412", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wireless sensor networks composed of battery-powered sensor nodes, one\nof the main issues is how to save power consumption at each node. The usual\napproach to this problem is to activate only necessary nodes (e.g., those nodes\nwhich compose a backbone network), and to put other nodes to sleep. One such\nalgorithm using location information is GAF (Geographical Adaptive Fidelity),\nand the GAF is enhanced to HGAF (Hierarchical Geographical Adaptive Fidelity).\nIn this paper, we show that we can further improve the energy efficiency of\nHGAF by modifying the manner of dividing sensor-field. We also provide a\ntheoretical bound on this problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 14:13:08 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Matsumae", "Susumu", ""], ["Ooshita", "Fukuhito", ""]]}, {"id": "1312.2807", "submitter": "Susumu Matsumae", "authors": "Susumu Matsumae", "title": "Polylogarithmic Gap between Meshes with Reconfigurable Row/Column Buses\n  and Meshes with Statically Partitioned Buses", "comments": "6 pages, 5 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 3 Issue 2, 2012", "doi": "10.14569/IJACSA.2012.030216", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the difference in computational power between the\nmesh-connected parallel computers equipped with dynamically reconfigurable bus\nsystems and those with static ones. The mesh with separable buses (MSB) is the\nmesh-connected parallel computer with dynamically reconfigurable row/column\nbuses. The broadcast buses of the MSB can be dynamically sectioned into smaller\nbus segments by program control. We show that the MSB of size $n \\times n$ can\nwork with$O(\\log^2 n)$ step even if its dynamic reconfigurable function is\ndisabled. Here, we assume the word-model broadcast buses, and use the relation\nbetween the word-model bus and the bit-model bus.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 14:18:46 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Matsumae", "Susumu", ""]]}, {"id": "1312.2813", "submitter": "Susumu Matsumae", "authors": "Susumu Matsumae", "title": "Energy-Efficient Cell Partition of 3D Space for Sensor Networks with\n  Location Information", "comments": "14 pages, 9 figures, 2 tables", "journal-ref": "International Journal of Network Protocols and Algorithms, Vol. 1,\n  No. 2 (2009)", "doi": "10.5296/npa.v1i2.270", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wireless sensor networks composed of battery-powered sensor nodes, one\nof the main issues is how to save power consumption on each node. The usual\napproach to this problem is to activate only necessary nodes (e.g., those nodes\nwhich compose a backbone network), and to put other nodes to sleep. One such\nalgorithm using location information is GAF (Geographical Adaptive Fidelity),\nand GAF is enhanced to HGAF (Hierarchical Geographical Adaptive Fidelity). In\nthis paper, we study the energy-efficient partition of a 3 dimensional sensor\nfield into cells. Further, we give a theoretical upper bound on cell size for\nthis problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 14:34:44 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Matsumae", "Susumu", ""]]}, {"id": "1312.2998", "submitter": "Ashkan Paya Mr.", "authors": "Dan C. Marinescu and Ashkan Paya and John P. Morrison and Philip Healy", "title": "An Auction-driven Self-organizing Cloud Delivery Model", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three traditional cloud delivery models -- IaaS, PaaS, and SaaS --\nconstrain access to cloud resources by hiding their raw functionality and\nforcing us to use them indirectly via a restricted set of actions. Can we\nintroduce a new delivery model, and, at the same time, support improved\nsecurity, a higher degree of assurance, find relatively simple solutions to the\nhard cloud resource management problems, eliminate some of the inefficiencies\nrelated to resource virtualization, allow the assembly of clouds of clouds,\nand, last but not least, minimize the number of interoperability standards?\n  We sketch a self-organizing architecture for very large compute clouds\ncomposed of many-core processors and heterogeneous coprocessors. We discuss how\nself-organization will address each of the challenges described above. The\napproach is {\\em bid-centric}. The system of heterogeneous cloud resources is\ndynamically, and autonomically, configured to bid to meet the needs identified\nin a high-level task or service specification. When the task is completed, or\nthe service is retired, the resources are released for subsequent reuse.\n  Our approach mimics the process followed by individual researchers who, in\nresponse to a call for proposals released by a funding agency, organize\nthemselves in groups of various sizes and specialities. If the bid is\nsuccessful, then the group carries out the proposed work and releases the\nresults. After the work is completed, individual researchers in the group\ndisperse, possibly joining other groups or submitting individual bids in\nresponse to other proposals. Similar protocols are common to other human\nactivities such as procurement management.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 23:39:15 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Marinescu", "Dan C.", ""], ["Paya", "Ashkan", ""], ["Morrison", "John P.", ""], ["Healy", "Philip", ""]]}, {"id": "1312.3018", "submitter": "Abdullah Gharaibeh", "authors": "Abdullah Gharaibeh, Tahsin Reza, Elizeu Santos-Neto, Lauro Beltrao\n  Costa, Scott Sallinen, Matei Ripeanu", "title": "Efficient Large-Scale Graph Processing on Hybrid CPU and GPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing scale and wealth of inter-connected data, such as those\naccrued by social network applications, demand the design of new techniques and\nplatforms to efficiently derive actionable knowledge from large-scale graphs.\nHowever, real-world graphs are famously difficult to process efficiently. Not\nonly they have a large memory footprint, but also most graph algorithms entail\nmemory access patterns with poor locality, data-dependent parallelism and a low\ncompute-to-memory access ratio. Moreover, most real-world graphs have a highly\nheterogeneous node degree distribution, hence partitioning these graphs for\nparallel processing and simultaneously achieving access locality and\nload-balancing is difficult.\n  This work starts from the hypothesis that hybrid platforms (e.g.,\nGPU-accelerated systems) have both the potential to cope with the heterogeneous\nstructure of real graphs and to offer a cost-effective platform for\nhigh-performance graph processing. This work assesses this hypothesis and\npresents an extensive exploration of the opportunity to harness hybrid systems\nto process large-scale graphs efficiently. In particular, (i) we present a\nperformance model that estimates the achievable performance on hybrid\nplatforms; (ii) informed by the performance model, we design and develop TOTEM\n- a processing engine that provides a convenient environment to implement graph\nalgorithms on hybrid platforms; (iii) we show that further performance gains\ncan be extracted using partitioning strategies that aim to produce partitions\nthat each matches the strengths of the processing element it is allocated to,\nfinally, (iv) we demonstrate the performance advantages of the hybrid system\nthrough a comprehensive evaluation that uses real and synthetic workloads (as\nlarge as 16 billion edges), multiple graph algorithms that stress the system in\nvarious ways, and a variety of hardware configurations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 01:40:29 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 18:49:41 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Gharaibeh", "Abdullah", ""], ["Reza", "Tahsin", ""], ["Santos-Neto", "Elizeu", ""], ["Costa", "Lauro Beltrao", ""], ["Sallinen", "Scott", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1312.3020", "submitter": "Huasha Zhao Mr", "authors": "Huasha Zhao, John Canny", "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large datasets exhibit power-law statistics: The web graph, social\nnetworks, text data, click through data etc. Their adjacency graphs are termed\nnatural graphs, and are known to be difficult to partition. As a consequence\nmost distributed algorithms on these graphs are communication intensive. Many\nalgorithms on natural graphs involve an Allreduce: a sum or average of\npartitioned data which is then shared back to the cluster nodes. Examples\ninclude PageRank, spectral partitioning, and many machine learning algorithms\nincluding regression, factor (topic) models, and clustering. In this paper we\ndescribe an efficient and scalable Allreduce primitive for power-law data. We\npoint out scaling problems with existing butterfly and round-robin networks for\nSparse Allreduce, and show that a hybrid approach improves on both.\nFurthermore, we show that Sparse Allreduce stages should be nested instead of\ncascaded (as in the dense case). And that the optimum throughput Allreduce\nnetwork should be a butterfly of heterogeneous degree where degree decreases\nwith depth into the network. Finally, a simple replication scheme is introduced\nto deal with node failures. We present experiments showing significant\nimprovements over existing systems such as PowerGraph and Hadoop.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 02:33:45 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Zhao", "Huasha", ""], ["Canny", "John", ""]]}, {"id": "1312.3300", "submitter": "Nathalie Revol", "authors": "Nathalie Revol (Inria Grenoble Rh\\^one-Alpes / LIP Laboratoire de\n  l'Informatique du Parall\\'elisme), Philippe Th\\'eveny (Inria Grenoble\n  Rh\\^one-Alpes / LIP Laboratoire de l'Informatique du Parall\\'elisme, LIP)", "title": "Numerical Reproducibility and Parallel Computations: Issues for Interval\n  Algorithms", "comments": "submitted to IEEE Transactions on Computers", "journal-ref": "IEEE Transactions on Computers (2014)", "doi": "10.1109/TC.2014.2322593", "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is called \"numerical reproducibility\" is the problem of getting the same\nresult when the scientific computation is run several times, either on the same\nmachine or on different machines, with different types and numbers of\nprocessing units, execution environments, computational loads etc. This problem\nis especially stringent for HPC numerical simulations. In what follows, the\nfocus is on parallel implementations of interval arithmetic using\nfloating-point arithmetic. For interval computations, numerical reproducibility\nis of course an issue for testing and debugging purposes. However, as long as\nthe computed result encloses the exact and unknown result, the inclusion\nproperty, which is the main property of interval arithmetic, is satisfied and\ngetting bit for bit identical results may not be crucial. Still, implementation\nissues may invalidate the inclusion property. Several ways to preserve the\ninclusion property are presented, on the example of the product of matrices\nwith interval coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 20:09:33 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Revol", "Nathalie", "", "Inria Grenoble Rh\u00f4ne-Alpes / LIP Laboratoire de\n  l'Informatique du Parall\u00e9lisme"], ["Th\u00e9veny", "Philippe", "", "Inria Grenoble\n  Rh\u00f4ne-Alpes / LIP Laboratoire de l'Informatique du Parall\u00e9lisme, LIP"]]}, {"id": "1312.3303", "submitter": "Christian Lavault", "authors": "Franck Butelle (LIPN), Christian Lavault (LIPN), Marc Bui (CHART)", "title": "A Uniform Self-Stabilizing Minimum Diameter Spanning Tree Algorithm", "comments": "14 pages; International conf\\'erence; Uniform self-stabilizing\n  variant of the problem, 9th International Workshop on Distributed Algorithms\n  (WDAG'95), Mont-Saint-Michel : France (1995)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a uniform self-stabilizing algorithm, which solves the problem of\ndistributively finding a minimum diameter spanning tree of an arbitrary\npositively real-weighted graph. Our algorithm consists in two stages of\nstabilizing protocols. The first stage is a uniform randomized stabilizing {\\em\nunique naming} protocol, and the second stage is a stabilizing {\\em MDST}\nprotocol, designed as a {\\em fair composition} of Merlin--Segall's stabilizing\nprotocol and a distributed deterministic stabilizing protocol solving the\n(MDST) problem. The resulting randomized distributed algorithm presented herein\nis a composition of the two stages; it stabilizes in $O(n\\Delta+{\\cal D}^2 + n\n\\log\\log n)$ expected time, and uses $O(n^2\\log n + n \\log W)$ memory bits\n(where $n$ is the order of the graph, $\\Delta$ is the maximum degree of the\nnetwork, $\\cal D$ is the diameter in terms of hops, and $W$ is the largest edge\nweight). To our knowledge, our protocol is the very first distributed algorithm\nfor the (MDST) problem. Moreover, it is fault-tolerant and works for any\nanonymous arbitrary network.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 20:13:48 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Butelle", "Franck", "", "LIPN"], ["Lavault", "Christian", "", "LIPN"], ["Bui", "Marc", "", "CHART"]]}, {"id": "1312.3347", "submitter": "Ousmane Thiare OT", "authors": "M. Naimi, O. Thiare", "title": "A Distributed Deadlock Free Quorum Based Algorithm for Mutual Exclusion", "comments": "7 pages, 9 figures", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 11, No. 8, August 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Quorum based mutual exclusion algorithms enjoy many advantages such as low\nmessage complexity and high failure resiliency. The use of quorums is a well\nknown approach to achieving mutual exclusion in distributed environments.\nSeveral distributed based quorum mutual exclusion was presented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 21:14:44 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Naimi", "M.", ""], ["Thiare", "O.", ""]]}, {"id": "1312.3455", "submitter": "Hong Xu", "authors": "Shuhao Liu, Hong Xu, Zhiping Cai", "title": "Low Latency Datacenter Networking: A Short Survey", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datacenters are the cornerstone of the big data infrastructure supporting\nnumerous online services. The demand for interactivity, which significantly\nimpacts user experience and provider revenue, is translated into stringent\ntiming requirements for flows in datacenter networks. Thus low latency\nnetworking is becoming a major concern of both industry and academia.\n  We provide a short survey of recent progress made by the networking community\nfor low latency datacenter networks. We propose a taxonomy to categorize\nexisting work based on four main techniques, reducing queue length,\naccelerating retransmissions, prioritizing mice flows, and exploiting\nmulti-path. Then we review select papers, highlight the principal ideas, and\ndiscuss their pros and cons. We also present our perspectives of the research\nchallenges and opportunities, hoping to aspire more future work in this space.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 11:50:23 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 08:36:28 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Liu", "Shuhao", ""], ["Xu", "Hong", ""], ["Cai", "Zhiping", ""]]}, {"id": "1312.3491", "submitter": "Lennaert van Veen", "authors": "Dhavide Aruliah, Lennaert van Veen, Alex Dubitski", "title": "PAMPAC: A Parallel Adaptive Method for Pseudo-Arclength Continuation", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-arclength continuation is a well-established method for generating a\nnumerical curve approximating the solution of an underdetermined system of\nnonlinear equations. It is an inherently sequential predictor-corrector method\nin which new approximate solutions are extrapolated from previously converged\nresults and then iteratively refined. Convergence of the iterative corrections\nis guaranteed only for sufficiently small prediction steps. In high-dimensional\nsystems, corrector steps are extremely costly to compute and the prediction\nstep-length must be adapted carefully to avoid failed steps or unnecessarily\nslow progress. We describe a parallel method for adapting the step-length\nemploying several predictor-corrector sequences of different step lengths\ncomputed concurrently. In addition, the algorithm permits intermediate results\nof unconverged correction sequences to seed new predictions. This strategy\nresults in an aggressive optimization of the step length at the cost of\nredundancy in the concurrent computation. We present two examples of convoluted\nsolution curves of high-dimensional systems showing that speed-up by a factor\nof two can be attained on a multi-core CPU while a factor of three is\nattainable on a small cluster.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 14:06:12 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Aruliah", "Dhavide", ""], ["van Veen", "Lennaert", ""], ["Dubitski", "Alex", ""]]}, {"id": "1312.3504", "submitter": "Warren Smith", "authors": "Warren Smith and Shava Smallen", "title": "Building An Information System for a Distributed Testbed", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an information system designed to support the large\nvolume of monitoring information generated by a distributed testbed. This\nmonitoring information is produced by several subsystems and consists of status\nand performance data that needs to be federated, distributed, and stored in a\ntimely and easy to use manner. Our approach differs from existing approaches\nbecause it federates and distributes information at a low architectural level\nvia messaging; a natural match to many of the producers and consumers of\ninformation. In addition, a database is easily layered atop the messaging layer\nfor consumers that want to query and search the information. Finally, a common\nlanguage to represent information in all layers of the information system makes\nit significantly easier for users to consume information. Performance data\nshows that this approach meets the significant needs of FutureGrid and would\nmeet the needs of an experimental infrastructure twice the size of FutureGrid.\nIn addition, this design also meets the needs of existing distributed\nscientific infrastructures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 14:47:50 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Smith", "Warren", ""], ["Smallen", "Shava", ""]]}, {"id": "1312.3613", "submitter": "Jean-Baptiste  Tristan", "authors": "Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock,\n  Stephen J. Green, Guy L. Steele Jr", "title": "Augur: a Modeling Language for Data-Parallel Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is time-consuming and error-prone to implement inference procedures for\neach new probabilistic model. Probabilistic programming addresses this problem\nby allowing a user to specify the model and having a compiler automatically\ngenerate an inference procedure for it. For this approach to be practical, it\nis important to generate inference code that has reasonable performance. In\nthis paper, we present a probabilistic programming language and compiler for\nBayesian networks designed to make effective use of data-parallel architectures\nsuch as GPUs. Our language is fully integrated within the Scala programming\nlanguage and benefits from tools such as IDE support, type-checking, and code\ncompletion. We show that the compiler can generate data-parallel inference code\nscalable to thousands of GPU cores by making use of the conditional\nindependence relationships in the Bayesian network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 20:23:20 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 19:53:09 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Tristan", "Jean-Baptiste", ""], ["Huang", "Daniel", ""], ["Tassarotti", "Joseph", ""], ["Pocock", "Adam", ""], ["Green", "Stephen J.", ""], ["Steele", "Guy L.", "Jr"]]}, {"id": "1312.3938", "submitter": "Jiajun Cao", "authors": "Jiajun Cao, Gregory Kerr, Kapil Arya, Gene Cooperman", "title": "Transparent Checkpoint-Restart over InfiniBand", "comments": "22 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  InfiniBand is widely used for low-latency, high-throughput cluster computing.\nSaving the state of the InfiniBand network as part of distributed checkpointing\nhas been a long-standing challenge for researchers. Because of a lack of a\nsolution, typical MPI implementations have included custom checkpoint-restart\nservices that \"tear down\" the network, checkpoint each node as if the node were\na standalone computer, and then re-connect the network again. We present the\nfirst example of transparent, system-initiated checkpoint-restart that directly\nsupports InfiniBand. The new approach is independent of any particular Linux\nkernel, thus simplifying the current practice of using a kernel-based module,\nsuch as BLCR. This direct approach results in checkpoints that are found to be\nfaster than with the use of a checkpoint-restart service. The generality of\nthis approach is shown not only by checkpointing an MPI computation, but also a\nnative UPC computation (Berkeley Unified Parallel C), which does not use MPI.\nScalability is shown by checkpointing 2,048 MPI processes across 128 nodes\n(with 16 cores per node). In addition, a cost-effective debugging approach is\nalso enabled, in which a checkpoint image from an InfiniBand-based production\ncluster is copied to a local Ethernet-based cluster, where it can be restarted\nand an interactive debugger can be attached to it. This work is based on a\nplugin that extends the DMTCP (Distributed MultiThreaded CheckPointing)\ncheckpoint-restart package.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 20:53:39 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2013 07:41:11 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2014 21:39:39 GMT"}], "update_date": "2014-02-03", "authors_parsed": [["Cao", "Jiajun", ""], ["Kerr", "Gregory", ""], ["Arya", "Kapil", ""], ["Cooperman", "Gene", ""]]}, {"id": "1312.4108", "submitter": "F. Ozgur Catak", "authors": "Ferhat \\\"Ozg\\\"ur \\c{C}atak, Mehmet Erdal Balaban", "title": "A MapReduce based distributed SVM algorithm for binary classification", "comments": "19 Pages. arXiv admin note: text overlap with arXiv:1301.0082", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Support Vector Machine (SVM) algorithm has a high generalization\nproperty to classify for unseen examples after training phase and it has small\nloss value, the algorithm is not suitable for real-life classification and\nregression problems. SVMs cannot solve hundreds of thousands examples in\ntraining dataset. In previous studies on distributed machine learning\nalgorithms, SVM is trained over a costly and preconfigured computer\nenvironment. In this research, we present a MapReduce based distributed\nparallel SVM training algorithm for binary classification problems. This work\nshows how to distribute optimization problem over cloud computing systems with\nMapReduce technique. In the second step of this work, we used statistical\nlearning theory to find the predictive hypothesis that minimize our empirical\nrisks from hypothesis spaces that created with reduce function of MapReduce.\nThe results of this research are important for training of big datasets for SVM\nalgorithm based classification problems. We provided that iterative training of\nsplit dataset with MapReduce technique; accuracy of the classifier function\nwill converge to global optimal classifier function's accuracy in finite\niteration size. The algorithm performance was measured on samples from letter\nrecognition and pen-based recognition of handwritten digits dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 05:42:51 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["\u00c7atak", "Ferhat \u00d6zg\u00fcr", ""], ["Balaban", "Mehmet Erdal", ""]]}, {"id": "1312.4176", "submitter": "Gabriele Oliva", "authors": "Gabriele Oliva, Roberto Setola, and Christoforos N. Hadjicostis", "title": "Distributed k-means algorithm", "comments": "preprint submitted to IEEE transactions on mobile computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a fully distributed implementation of the k-means\nclustering algorithm, intended for wireless sensor networks where each agent is\nendowed with a possibly high-dimensional observation (e.g., position, humidity,\ntemperature, etc.) The proposed algorithm, by means of one-hop communication,\npartitions the agents into measure-dependent groups that have small in-group\nand large out-group \"distances\". Since the partitions may not have a relation\nwith the topology of the network--members of the same clusters may not be\nspatially close--the algorithm is provided with a mechanism to compute the\nclusters'centroids even when the clusters are disconnected in several\nsub-clusters.The results of the proposed distributed algorithm coincide, in\nterms of minimization of the objective function, with the centralized k-means\nalgorithm. Some numerical examples illustrate the capabilities of the proposed\nsolution.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 18:08:27 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 22:38:49 GMT"}, {"version": "v3", "created": "Mon, 10 Nov 2014 13:36:34 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Oliva", "Gabriele", ""], ["Setola", "Roberto", ""], ["Hadjicostis", "Christoforos N.", ""]]}, {"id": "1312.4188", "submitter": "Ankit Tharwani", "authors": "Kamal Chandra Reddy, Ankit Tharwani, Ch.Vamshi Krishna,\n  Lakshminarayanan.V", "title": "Parallel Firewalls on General-Purpose Graphics Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firewalls use a rule database to decide which packets will be allowed from\none network onto another thereby implementing a security policy. In high-speed\nnetworks as the inter-arrival rate of packets decreases, the latency incurred\nby a firewall increases. In such a scenario, a single firewall become a\nbottleneck and reduces the overall throughput of the network.A firewall with\nheavy load, which is supposed to be a first line of defense against attacks,\nbecomes susceptible to Denial of Service (DoS) attacks. Many works are being\ndone to optimize firewalls.This paper presents our implementation of different\nparallel firewall models on General-Purpose Graphics Processing Unit (GPGPU).\nWe implemented the parallel firewall architecture proposed in and introduced a\nnew model that can effectively exploit the massively parallel computing\ncapabilities of GPGPU.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 20:23:12 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Reddy", "Kamal Chandra", ""], ["Tharwani", "Ankit", ""], ["Krishna", "Ch. Vamshi", ""], ["V", "Lakshminarayanan.", ""]]}, {"id": "1312.4203", "submitter": "Georgios Zois", "authors": "Dimitrios Fotakis, Ioannis Milis, Emmanouil Zampetakis, Georgios Zois", "title": "Scheduling MapReduce Jobs and Data Shuffle on Unrelated Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose constant approximation algorithms for generalizations of the\nFlexible Flow Shop (FFS) problem which form a realistic model for\nnon-preemptive scheduling in MapReduce systems. Our results concern the\nminimization of the total weighted completion time of a set of MapReduce jobs\non unrelated processors and improve substantially on the model proposed by\nMoseley et al. (SPAA 2011) in two directions. First, we consider each job\nconsisting of multiple Map and Reduce tasks, as this is the key idea behind\nMapReduce computations, and we propose a constant approximation algorithm.\nThen, we introduce into our model the crucial cost of data shuffle phase, i.e.,\nthe cost for the transmission of intermediate data from Map to Reduce tasks. In\nfact, we model this phase by an additional set of Shuffle tasks for each job\nand we manage to keep the same approximation ratio when they are scheduled on\nthe same processors with the corresponding Reduce tasks and to provide also a\nconstant ratio when they are scheduled on different processors. This is the\nmost general setting of the FFS problem (with a special third stage) for which\na constant approximation ratio is known.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 23:10:27 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 20:00:24 GMT"}, {"version": "v3", "created": "Tue, 24 Jun 2014 14:45:53 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Fotakis", "Dimitrios", ""], ["Milis", "Ioannis", ""], ["Zampetakis", "Emmanouil", ""], ["Zois", "Georgios", ""]]}, {"id": "1312.4333", "submitter": "Marius Buliga", "authors": "Marius Buliga and Louis H. Kauffman", "title": "GLC actors, artificial chemical connectomes, topological issues and\n  knots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.GT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on graphic lambda calculus, we propose a program for a new model of\nasynchronous distributed computing, inspired from Hewitt Actor Model, as well\nas several investigation paths, concerning how one may graft lambda calculus\nand knot diagrammatics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 12:40:25 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Buliga", "Marius", ""], ["Kauffman", "Louis H.", ""]]}, {"id": "1312.4415", "submitter": "Zaid Towfic", "authors": "Zaid J. Towfic and Ali H. Sayed", "title": "Adaptive Penalty-Based Distributed Stochastic Convex Optimization", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": "10.1109/TSP.2014.2331615", "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the task of distributed optimization over a network of\nlearners in which each learner possesses a convex cost function, a set of\naffine equality constraints, and a set of convex inequality constraints. We\npropose a fully-distributed adaptive diffusion algorithm based on penalty\nmethods that allows the network to cooperatively optimize the global cost\nfunction, which is defined as the sum of the individual costs over the network,\nsubject to all constraints. We show that when small constant step-sizes are\nemployed, the expected distance between the optimal solution vector and that\nobtained at each node in the network can be made arbitrarily small. Two\ndistinguishing features of the proposed solution relative to other related\napproaches is that the developed strategy does not require the use of\nprojections and is able to adapt to and track drifts in the location of the\nminimizer due to changes in the constraints or in the aggregate cost itself.\nThe proposed strategy is also able to cope with changing network topology, is\nrobust to network disruptions, and does not require global information or rely\non central processors.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 16:01:08 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Towfic", "Zaid J.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.4508", "submitter": "Christian Lavault", "authors": "Gabriel Paillard (LIPN), Christian Lavault (LIPN), Felipe Franca\n  (PESC)", "title": "A distributed prime sieving algorithm based on Scheduling by Multiple\n  Edge Reversal", "comments": "11 pages. Special issue : Selected papers from the ISPDC'05\n  Conference (4th International Symposium on Parallel and Distributed\n  Computing); 4th International Symposium on Parallel and Distributed\n  Computing, Lille : France (2005)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new distributed approach for generating all prime\nnumbers in a given interval of integers. From Eratosthenes, who elaborated the\nfirst prime sieve (more than 2000 years ago), to the current generation of\nparallel computers, which have permitted to reach larger bounds on the interval\nor to obtain previous results in a shorter time, prime numbers generation still\nrepresents an attractive domain of research and plays a central role in\ncryptography. We propose a fully distributed algorithm for finding all primes\nin the interval $[2\\ldots, n]$, based on the \\emph{wheel sieve} and the SMER\n(\\emph{Scheduling by Multiple Edge Reversal}) multigraph dynamics. Given a\nmultigraph $\\mathcal{M}$ of arbitrary topology, having $N$ nodes, a SMER-driven\nsystem is defined by the number of directed edges (arcs) between any two nodes\nof $\\mathcal{M}$, and by the global period length of all \"arc reversals\" in\n$\\mathcal{M}$. The new prime number generation method inherits the distributed\nand parallel nature of SMER and requires at most $n + \\lfloor \\sqrt{n}\\rfloor$\ntime steps. The message complexity achieves at most $n\\Delta_N + \\lfloor\n\\sqrt{n}\\rfloor \\Delta_N$, where $1\\le \\Delta_N\\le N - 1$ is the maximal\nmultidegree of $\\mathcal{M}$, and the maximal amount of memory space required\nper process is $\\mathcal{O}(n)$ bits.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 20:30:57 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Paillard", "Gabriel", "", "LIPN"], ["Lavault", "Christian", "", "LIPN"], ["Franca", "Felipe", "", "PESC"]]}, {"id": "1312.4605", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David B. Dunson", "title": "Parallelizing MCMC via Weierstrass Sampler", "comments": "The original Algorithm 1 removed. Provided some theoretical\n  justification for refinement sampling (Theorem 2). Added a new algorithm in\n  addition to the rejection sampling for handling dimensionality curse. New\n  simulations and graphs (with new colors and designs). A real data analysis is\n  also provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing scales of statistical problems, subset based\ncommunication-free parallel MCMC methods are a promising future for large scale\nBayesian analysis. In this article, we propose a new Weierstrass sampler for\nparallel MCMC based on independent subsets. The new sampler approximates the\nfull data posterior samples via combining the posterior draws from independent\nsubset MCMC chains, and thus enjoys a higher computational efficiency. We show\nthat the approximation error for the Weierstrass sampler is bounded by some\ntuning parameters and provide suggestions for choice of the values. Simulation\nstudy shows the Weierstrass sampler is very competitive compared to other\nmethods for combining MCMC chains generated for subsets, including averaging\nand kernel smoothing.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 01:43:39 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 18:46:47 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David B.", ""]]}, {"id": "1312.4722", "submitter": "Marcos Assuncao", "authors": "Marcos D. Assuncao, Rodrigo N. Calheiros, Silvia Bianchi, Marco A. S.\n  Netto, Rajkumar Buyya", "title": "Big Data Computing and Clouds: Trends and Future Directions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2014.08.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses approaches and environments for carrying out analytics\non Clouds for Big Data applications. It revolves around four important areas of\nanalytics and Big Data, namely (i) data management and supporting\narchitectures; (ii) model development and scoring; (iii) visualisation and user\ninteraction; and (iv) business models. Through a detailed survey, we identify\npossible gaps in technology and provide recommendations for the research\ncommunity on future directions on Cloud-supported Big Data computing and\nanalytics solutions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 11:18:32 GMT"}, {"version": "v2", "created": "Fri, 22 Aug 2014 08:10:35 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Assuncao", "Marcos D.", ""], ["Calheiros", "Rodrigo N.", ""], ["Bianchi", "Silvia", ""], ["Netto", "Marco A. S.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1312.4853", "submitter": "Philip Healy", "authors": "Philip Healy, Stefan Meyer, John Morrison, Theo Lynn, Ashkan Paya, Dan\n  C. Marinescu", "title": "Bid-Centric Cloud Service Provisioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bid-centric service descriptions have the potential to offer a new cloud\nservice provisioning model that promotes portability, diversity of choice and\ndifferentiation between providers. A bid matching model based on requirements\nand capabilities is presented that provides the basis for such an approach. In\norder to facilitate the bidding process, tenders should be specified as\nabstractly as possible so that the solution space is not needlessly restricted.\nTo this end, we describe how partial TOSCA service descriptions allow for a\nrange of diverse solutions to be proposed by multiple providers in response to\ntenders. Rather than adopting a lowest common denominator approach, true\nportability should allow for the relative strengths and differentiating\nfeatures of cloud service providers to be applied to bids. With this in mind,\nwe describe how TOSCA service descriptions could be augmented with additional\ninformation in order to facilitate heterogeneity in proposed solutions, such as\nthe use of coprocessors and provider-specific services.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 16:33:14 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Healy", "Philip", ""], ["Meyer", "Stefan", ""], ["Morrison", "John", ""], ["Lynn", "Theo", ""], ["Paya", "Ashkan", ""], ["Marinescu", "Dan C.", ""]]}, {"id": "1312.4892", "submitter": "Matt Wytock", "authors": "Matt Wytock and J. Zico Kolter", "title": "A Fast Algorithm for Sparse Controller Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of designing sparse control laws for large-scale systems\nby directly minimizing an infinite horizon quadratic cost with an $\\ell_1$\npenalty on the feedback controller gains. Our focus is on an improved algorithm\nthat allows us to scale to large systems (i.e. those where sparsity is most\nuseful) with convergence times that are several orders of magnitude faster than\nexisting algorithms. In particular, we develop an efficient proximal Newton\nmethod which minimizes per-iteration cost with a coordinate descent active set\napproach and fast numerical solutions to the Lyapunov equations. Experimentally\nwe demonstrate the appeal of this approach on synthetic examples and real power\nnetworks significantly larger than those previously considered in the\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 18:53:36 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Wytock", "Matt", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1312.4993", "submitter": "Herv\\'e Paulino", "authors": "Herv\\'e Paulino and Eduardo Marques", "title": "Heterogeneous Programming with Single Operation Multiple Data", "comments": "28 pages, Preprint of the article accepted for publication in a\n  special issue of Journal of Computer and System Sciences{\\dag} dedicated to\n  the 14th IEEE International Conference on High Performance Computing and\n  Communication (HPCC 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneity is omnipresent in today's commodity computational systems,\nwhich comprise at least one multi-core Central Processing Unit (CPU) and one\nGraphics Processing Unit (GPU). Nonetheless, all this computing power is not\nbeing exploited in mainstream computing, as the programming of these systems\nentails many details of the underlying architecture and of its distinct\nexecution models. Current research on parallel programming is addressing these\nissues but, still, the systems' heterogeneity is exposed at language level.\nThis paper proposes a uniform framework, grounded on the Single Operation\nMultiple Data model, for the programming of such heterogeneous systems. The\nmodel is declarative, empowering the compiler to generate code for multiple\narchitectures from the same source. To this extent, we designed a simple\nextension of the Java programming language that embodies the model, and\ndeveloped a compiler that generates code for both multi-core CPUs and GPUs. A\nperformance evaluation attests the validity of the approach that, despite being\nbased on a simple programming model, is able to deliver performance gains on\npar with hand-tuned data parallel multi-threaded Java applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 22:43:59 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2014 17:27:22 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Paulino", "Herv\u00e9", ""], ["Marques", "Eduardo", ""]]}, {"id": "1312.5469", "submitter": "Anjali Pp", "authors": "PP Anjali, A Binu", "title": "Network Traffic Analysis:Hadoop Pig vs Typical MapReduce", "comments": "7 pages, 5 figures, Conference - ITCSE 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analysis has become much popular in the present day scenario and the\nmanipulation of big data has gained the keen attention of researchers in the\nfield of data analytics. Analysis of big data is currently considered as an\nintegral part of many computational and statistical departments. As a result,\nnovel approaches in data analysis are evolving on a daily basis. Thousands of\ntransaction requests are handled and processed everyday by different websites\nassociated with e-commerce, e-banking, e-shopping carts etc. The network\ntraffic and weblog analysis comes to play a crucial role in such situations\nwhere Hadoop can be suggested as an efficient solution for processing the\nNetflow data collected from switches as well as website access-logs during\nfixed intervals.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 10:32:54 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Anjali", "PP", ""], ["Binu", "A", ""]]}, {"id": "1312.5691", "submitter": "Wei Zhang", "authors": "Wei Zhang, Olivier Tardieu, David Grove, Benjamin Herta, Tomio Kamada,\n  Vijay Saraswat, Mikio Takeuchi", "title": "GLB: Lifeline-based Global Load Balancing library in X10", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GLB, a programming model and an associated implementation that can\nhandle a wide range of irregular paral- lel programming problems running over\nlarge-scale distributed systems. GLB is applicable both to problems that are\neasily load-balanced via static scheduling and to problems that are hard to\nstatically load balance. GLB hides the intricate syn- chronizations (e.g.,\ninter-node communication, initialization and startup, load balancing,\ntermination and result collection) from the users. GLB internally uses a\nversion of the lifeline graph based work-stealing algorithm proposed by\nSaraswat et al. Users of GLB are simply required to write several pieces of\nsequential code that comply with the GLB interface. GLB then schedules and\norchestrates the parallel execution of the code correctly and efficiently at\nscale. We have applied GLB to two representative benchmarks: Betweenness\nCentrality (BC) and Unbalanced Tree Search (UTS). Among them, BC can be\nstatically load-balanced whereas UTS cannot. In either case, GLB scales well--\nachieving nearly linear speedup on different computer architectures (Power,\nBlue Gene/Q, and K) -- up to 16K cores.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 18:46:00 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Zhang", "Wei", ""], ["Tardieu", "Olivier", ""], ["Grove", "David", ""], ["Herta", "Benjamin", ""], ["Kamada", "Tomio", ""], ["Saraswat", "Vijay", ""], ["Takeuchi", "Mikio", ""]]}, {"id": "1312.5799", "submitter": "Peter Richtarik", "authors": "Olivier Fercoq and Peter Richt\\'arik", "title": "Accelerated, Parallel and Proximal Coordinate Descent", "comments": "25 pages, 2 algorithms, 6 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new stochastic coordinate descent method for minimizing the sum\nof convex functions each of which depends on a small number of coordinates\nonly. Our method (APPROX) is simultaneously Accelerated, Parallel and PROXimal;\nthis is the first time such a method is proposed. In the special case when the\nnumber of processors is equal to the number of coordinates, the method\nconverges at the rate $2\\bar{\\omega}\\bar{L} R^2/(k+1)^2 $, where $k$ is the\niteration counter, $\\bar{\\omega}$ is an average degree of separability of the\nloss function, $\\bar{L}$ is the average of Lipschitz constants associated with\nthe coordinates and individual functions in the sum, and $R$ is the distance of\nthe initial point from the minimizer. We show that the method can be\nimplemented without the need to perform full-dimensional vector operations,\nwhich is the major bottleneck of existing accelerated coordinate descent\nmethods. The fact that the method depends on the average degree of\nseparability, and not on the maximum degree of separability, can be attributed\nto the use of new safe large stepsizes, leading to improved expected separable\noverapproximation (ESO). These are of independent interest and can be utilized\nin all existing parallel stochastic coordinate descent algorithms based on the\nconcept of ESO.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 03:00:15 GMT"}, {"version": "v2", "created": "Sat, 1 Mar 2014 17:04:59 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Fercoq", "Olivier", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1312.6029", "submitter": "Geza Odor", "authors": "G\\'eza \\'Odor, Jeffrey Kelling and Sibylle Gemming", "title": "Ageing of the 2+1 dimensional Kardar-Parisi-Zhang model", "comments": "6 pages, 5 figs, 1 table, accepted version in PRE", "journal-ref": "Phys. Rev. E 89, 032146 (2014)", "doi": "10.1103/PhysRevE.89.032146", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.mtrl-sci cs.DC nlin.CG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended dynamical simulations have been performed on a 2+1 dimensional\ndriven dimer lattice gas model to estimate ageing properties. The\nauto-correlation and the auto-response functions are determined and the\ncorresponding scaling exponents are tabulated. Since this model can be mapped\nonto the 2+1 dimensional Kardar-Parisi-Zhang surface growth model, our results\ncontribute to the understanding of the universality class of that basic system.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 16:42:03 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2014 20:38:43 GMT"}, {"version": "v3", "created": "Mon, 17 Mar 2014 13:39:44 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["\u00d3dor", "G\u00e9za", ""], ["Kelling", "Jeffrey", ""], ["Gemming", "Sibylle", ""]]}, {"id": "1312.6170", "submitter": "Rajiv Ranjan Dr.", "authors": "Khalid Alhamazani, (University of New South Wales, Australia) and\n  Rajiv Ranjan, (CSIRO, Australia) and Karan Mitra, (CSIRO, Australia) and\n  Fethi Rabhi, (University of New South Wales, Australia) and Samee Ullah Khan,\n  (North Dakota State University, USA) and Adnene Guabtni, (NICTA, Australia)\n  and Vasudha Bhatnagar (University of Delhi, India)", "title": "An Overview of the Commercial Cloud Monitoring Tools: Research\n  Dimensions, Design Issues, and State-of-the-Art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud monitoring activity involves dynamically tracking the Quality of\nService (QoS) parameters related to virtualized resources (e.g., VM, storage,\nnetwork, appliances, etc.), the physical resources they share, the applications\nrunning on them and data hosted on them. Applications and resources\nconfiguration in cloud computing environment is quite challenging considering a\nlarge number of heterogeneous cloud resources. Further, considering the fact\nthat at each point of time, there will be a different and specific cloud\nservice which may be massively required. Hence, cloud monitoring tools can\nassist a cloud providers or application developers in: (i) keeping their\nresources and applications operating at peak efficiency; (ii) detecting\nvariations in resource and application performance; (iii) accounting the\nService Level Agreement (SLA) violations of certain QoS parameters; and (iv)\ntracking the leave and join operations of cloud resources due to failures and\nother dynamic configuration changes.\n  In this paper, we identify and discuss the major research dimensions and\ndesign issues related to engineering cloud monitoring tools. We further discuss\nhow aforementioned research dimensions and design issues are handled by current\nacademic research as well as by commercial monitoring tools.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 22:54:23 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Alhamazani", "Khalid", "", "University of Delhi, India"], ["Ranjan", "Rajiv", "", "University of Delhi, India"], ["Mitra", "Karan", "", "University of Delhi, India"], ["Rabhi", "Fethi", "", "University of Delhi, India"], ["Khan", "Samee Ullah", "", "University of Delhi, India"], ["Guabtni", "Adnene", "", "University of Delhi, India"], ["Bhatnagar", "Vasudha", "", "University of Delhi, India"]]}, {"id": "1312.6186", "submitter": "Tom Paine", "authors": "Thomas Paine, Hailin Jin, Jianchao Yang, Zhe Lin, Thomas Huang", "title": "GPU Asynchronous Stochastic Gradient Descent to Speed Up Neural Network\n  Training", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to train large-scale neural networks has resulted in\nstate-of-the-art performance in many areas of computer vision. These results\nhave largely come from computational break throughs of two forms: model\nparallelism, e.g. GPU accelerated training, which has seen quick adoption in\ncomputer vision circles, and data parallelism, e.g. A-SGD, whose large scale\nhas been used mostly in industry. We report early experiments with a system\nthat makes use of both model parallelism and data parallelism, we call GPU\nA-SGD. We show using GPU A-SGD it is possible to speed up training of large\nconvolutional neural networks useful for computer vision. We believe GPU A-SGD\nwill make it possible to train larger networks on larger training sets in a\nreasonable amount of time.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 00:56:56 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Paine", "Thomas", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""], ["Lin", "Zhe", ""], ["Huang", "Thomas", ""]]}, {"id": "1312.6246", "submitter": "John Levine", "authors": "John Levine, Graeme Ritchie, Alastair Andrew and Simon Gates", "title": "New Results for the Heterogeneous Multi-Processor Scheduling Problem\n  using a Fast, Effective Local Search and Random Disruption", "comments": "6 pages. Proceedings of PlanSIG 2012, Teeside University, December\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient scheduling of independent computational tasks in a\nheterogeneous computing environment is an important problem that occurs in\ndomains such as Grid and Cloud computing. Finding optimal schedules is an\nNP-hard problem in general, so we have to rely on approximate algorithms to\ncome up schedules that are as near to optimal as possible. In our previous work\non this problem, we applied a fast, effective local search to generate\nreasonably good schedules in a short amount of time and used ant colony\noptimisation (ACO) to incrementally improve those schedules over a longer time\nperiod. In this work, we replace the ACO component with a random disruption\nalgorithm and find that this produces results which are competitive with the\ncurrent state of the art over a 90 second execution time. We also ran our\nalgorithm for a longer time period on 12 well-known benchmark instances and as\na result provide new upper bounds for these instances.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 11:52:21 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Levine", "John", ""], ["Ritchie", "Graeme", ""], ["Andrew", "Alastair", ""], ["Gates", "Simon", ""]]}, {"id": "1312.6273", "submitter": "Minyar Sassi", "authors": "Sonia Alouane-Ksouri, Minyar Sassi-Hidri, Kamel Barkaoui", "title": "Parallel architectures for fuzzy triadic similarity learning", "comments": null, "journal-ref": "International Conference on Control, Engineering & Information\n  Technology (CEIT), Proceedings Engineering & Technology, Vol. 1, pp. 121-126,\n  2013", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context of document co-clustering, we define a new similarity measure\nwhich iteratively computes similarity while combining fuzzy sets in a\nthree-partite graph. The fuzzy triadic similarity (FT-Sim) model can deal with\nuncertainty offers by the fuzzy sets. Moreover, with the development of the Web\nand the high availability of storage spaces, more and more documents become\naccessible. Documents can be provided from multiple sites and make similarity\ncomputation an expensive processing. This problem motivated us to use parallel\ncomputing. In this paper, we introduce parallel architectures which are able to\ntreat large and multi-source data sets by a sequential, a merging or a\nsplitting-based process. Then, we proceed to a local and a central (or global)\ncomputing using the basic FT-Sim measure. The idea behind these architectures\nis to reduce both time and space complexities thanks to parallel computation.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 16:51:26 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Alouane-Ksouri", "Sonia", ""], ["Sassi-Hidri", "Minyar", ""], ["Barkaoui", "Kamel", ""]]}, {"id": "1312.6293", "submitter": "Jerome Darmont", "authors": "Jaume Ferrarons (ERIC), Mulu Adhana (ERIC), Carlos Colmenares (ERIC),\n  Sandra Pietrowska (ERIC), Fadila Bentayeb (ERIC), J\\'er\\^ome Darmont (ERIC)", "title": "PRIMEBALL: a Parallel Processing Framework Benchmark for Big Data\n  Applications in the Cloud", "comments": "5th TPC Technology Conference on Performance Evaluation and\n  Benchmarking (VLDB/TPCTC 13), Riva del Garda : Italy (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we draw the specifications of a novel benchmark for comparing\nparallel processing frameworks in the context of big data applications hosted\nin the cloud. We aim at filling several gaps in already existing cloud data\nprocessing benchmarks, which lack a real-life context for their processes, thus\nlosing relevance when trying to assess performance for real applications.\nHence, we propose a fictitious news site hosted in the cloud that is to be\nmanaged by the framework under analysis, together with several objective use\ncase scenarios and measures for evaluating system performance. The main\nstrengths of our benchmark are parallelization capabilities supporting cloud\nfeatures and big data properties.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 19:15:22 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Ferrarons", "Jaume", "", "ERIC"], ["Adhana", "Mulu", "", "ERIC"], ["Colmenares", "Carlos", "", "ERIC"], ["Pietrowska", "Sandra", "", "ERIC"], ["Bentayeb", "Fadila", "", "ERIC"], ["Darmont", "J\u00e9r\u00f4me", "", "ERIC"]]}, {"id": "1312.6485", "submitter": "Zheng Li", "authors": "Zheng Li and Mingfei Liang and Liam O'Brien and He Zhang", "title": "The Cloud's Cloudy Moment: A Systematic Survey of Public Cloud Service\n  Outage", "comments": "11 pages", "journal-ref": "International Journal of Cloud Computing and Services Science\n  (IJ-CLOSER), vol. 2, no. 5, 2013", "doi": "10.11591/closer.v2i5.5125", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inadequate service availability is the top concern when employing Cloud\ncomputing. It has been recognized that zero downtime is impossible for\nlarge-scale Internet services. By learning from the previous and others'\nmistakes, nevertheless, it is possible for Cloud vendors to minimize the risk\nof future downtime or at least keep the downtime short. To facilitate\nsummarizing lessons for Cloud providers, we performed a systematic survey of\npublic Cloud service outage events. This paper reports the result of this\nsurvey. In addition to a set of findings, our work generated a lessons\nframework by classifying the outage root causes. The framework can in turn be\nused to arrange outage lessons for reference by Cloud providers. By including\npotentially new root causes, this lessons framework will be smoothly expanded\nin our future work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 08:53:23 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Li", "Zheng", ""], ["Liang", "Mingfei", ""], ["O'Brien", "Liam", ""], ["Zhang", "He", ""]]}, {"id": "1312.6488", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Rajiv Ranjan and Miranda Zhang", "title": "Early Observations on Performance of Google Compute Engine for\n  Scientific Computing", "comments": "Proceedings of the 5th International Conference on Cloud Computing\n  Technologies and Science (CloudCom 2013), pp. 1-8, Bristol, UK, December 2-5,\n  2013", "journal-ref": null, "doi": "10.1109/CloudCom.2013.7", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Cloud computing emerged for business applications in industry,\npublic Cloud services have been widely accepted and encouraged for scientific\ncomputing in academia. The recently available Google Compute Engine (GCE) is\nclaimed to support high-performance and computationally intensive tasks, while\nlittle evaluation studies can be found to reveal GCE's scientific capabilities.\nConsidering that fundamental performance benchmarking is the strategy of\nearly-stage evaluation of new Cloud services, we followed the Cloud Evaluation\nExperiment Methodology (CEEM) to benchmark GCE and also compare it with Amazon\nEC2, to help understand the elementary capability of GCE for dealing with\nscientific problems. The experimental results and analyses show both potential\nadvantages of, and possible threats to applying GCE to scientific computing.\nFor example, compared to Amazon's EC2 service, GCE may better suit applications\nthat require frequent disk operations, while it may not be ready yet for single\nVM-based parallel computing. Following the same evaluation methodology,\ndifferent evaluators can replicate and/or supplement this fundamental\nevaluation of GCE. Based on the fundamental evaluation results, suitable GCE\nenvironments can be further established for case studies of solving real\nscience problems.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 09:12:09 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Ranjan", "Rajiv", ""], ["Zhang", "Miranda", ""]]}, {"id": "1312.6723", "submitter": "Bruce Berriman", "authors": "G. Bruce Berriman, Ewa Deelman, John Good, Gideon Juve, Jamie Kinney,\n  Ann Merrihew, and Mats Rynge", "title": "Creating A Galactic Plane Atlas With Amazon Web Services", "comments": "7 pages, 1 table, 2 figures. Submitted to IEEE Special Edition on\n  Computing in Astronomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes by example how astronomers can use cloud-computing\nresources offered by Amazon Web Services (AWS) to create new datasets at scale.\nWe have created from existing surveys an atlas of the Galactic Plane at 16\nwavelengths from 1 {\\mu}m to 24 {\\mu}m with pixels co-registered at spatial\nsampling of 1 arcsec. We explain how open source tools support management and\noperation of a virtual cluster on AWS platforms to process data at scale, and\ndescribe the technical issues that users will need to consider, such as\noptimization of resources, resource costs, and management of virtual machine\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 00:10:27 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Berriman", "G. Bruce", ""], ["Deelman", "Ewa", ""], ["Good", "John", ""], ["Juve", "Gideon", ""], ["Kinney", "Jamie", ""], ["Merrihew", "Ann", ""], ["Rynge", "Mats", ""]]}, {"id": "1312.7217", "submitter": "Venkatesan Chakaravarthy", "authors": "Archita Agarwal and Venkatesan T. Chakaravarthy and Anamitra R.\n  Choudhury and Sambuddha Roy and Yogish Sabharwal", "title": "Distributed and Parallel Algorithms for Set Cover Problems with Small\n  Neighborhood Covers", "comments": "Full version of FSTTCS'13 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study a class of set cover problems that satisfy a special\nproperty which we call the {\\em small neighborhood cover} property. This class\nencompasses several well-studied problems including vertex cover, interval\ncover, bag interval cover and tree cover. We design unified distributed and\nparallel algorithms that can handle any set cover problem falling under the\nabove framework and yield constant factor approximations. These algorithms run\nin polylogarithmic communication rounds in the distributed setting and are in\nNC, in the parallel setting.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 09:03:42 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Agarwal", "Archita", ""], ["Chakaravarthy", "Venkatesan T.", ""], ["Choudhury", "Anamitra R.", ""], ["Roy", "Sambuddha", ""], ["Sabharwal", "Yogish", ""]]}, {"id": "1312.7606", "submitter": "Sergio Valcarcel Macua", "authors": "Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "comments": "36 pages, 4 figures, accepted for publication on IEEE Transactions on\n  Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply diffusion strategies to develop a fully-distributed cooperative\nreinforcement learning algorithm in which agents in a network communicate only\nwith their immediate neighbors to improve predictions about their environment.\nThe algorithm can also be applied to off-policy learning, meaning that the\nagents can predict the response to a behavior different from the actual\npolicies they are following. The proposed distributed strategy is efficient,\nwith linear complexity in both computation time and memory footprint. We\nprovide a mean-square-error performance analysis and establish convergence\nunder constant step-size updates, which endow the network with continuous\nlearning capabilities. The results show a clear gain from cooperation: when the\nindividual agents can estimate the solution, cooperation increases stability\nand reduces bias and variance of the prediction error; but, more importantly,\nthe network is able to approach the optimal solution even when none of the\nindividual agents can (e.g., when the individual behavior policies restrict\neach agent to sample a small portion of the state space).\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 00:16:34 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 19:50:03 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Macua", "Sergio Valcarcel", ""], ["Chen", "Jianshu", ""], ["Zazo", "Santiago", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.7626", "submitter": "Amer Mouawad", "authors": "Faisal N. Abu-Khzam, Khuzaima Daudjee, Amer E. Mouawad, Naomi\n  Nishimura", "title": "An Easy-to-use Scalable Framework for Parallel Recursive Backtracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercomputers are equipped with an increasingly large number of cores to use\ncomputational power as a way of solving problems that are otherwise\nintractable. Unfortunately, getting serial algorithms to run in parallel to\ntake advantage of these computational resources remains a challenge for several\napplication domains. Many parallel algorithms can scale to only hundreds of\ncores. The limiting factors of such algorithms are usually communication\noverhead and poor load balancing. Solving NP-hard graph problems to optimality\nusing exact algorithms is an example of an area in which there has so far been\nlimited success in obtaining large scale parallelism. Many of these algorithms\nuse recursive backtracking as their core solution paradigm. In this paper, we\npropose a lightweight, easy-to-use, scalable framework for transforming almost\nany recursive backtracking algorithm into a parallel one. Our framework incurs\nminimal communication overhead and guarantees a load-balancing strategy that is\nimplicit, i.e., does not require any problem-specific knowledge. The key idea\nbehind this framework is the use of an indexed search tree approach that is\noblivious to the problem being solved. We test our framework with parallel\nimplementations of algorithms for the well-known Vertex Cover and Dominating\nSet problems. On sufficiently hard instances, experimental results show linear\nspeedups for thousands of cores, reducing running times from days to just a few\nminutes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 04:50:35 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Abu-Khzam", "Faisal N.", ""], ["Daudjee", "Khuzaima", ""], ["Mouawad", "Amer E.", ""], ["Nishimura", "Naomi", ""]]}, {"id": "1312.7685", "submitter": "Oshri Naparstek", "authors": "Oshri Naparstek (Student Member, IEEE) and Amir Leshem (Senior Member,\n  IEEE)", "title": "Fully distributed optimal channel assignment for open spectrum access", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2285512", "report-no": null, "categories": "cs.DC cs.IT cs.NI math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of fully distributed assignment of users\nto sub-bands such that the sum-rate of the system is maximized. We introduce a\nmodified auction algorithm that can be applied in a fully distributed way using\nan opportunistic CSMA assignment scheme and is $\\epsilon$ optimal. We analyze\nthe expected time complexity of the algorithm and suggest a variant to the\nalgorithm that has lower expected complexity. We then show that in the case of\ni.i.d Rayleigh channels a simple greedy scheme is asymptotically optimal as\n$\\SNR$ increases or as the number of users is increased to infinity. We\nconclude by providing simulated results of the suggested algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 11:43:16 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Naparstek", "Oshri", "", "Student Member, IEEE"], ["Leshem", "Amir", "", "Senior Member,\n  IEEE"]]}, {"id": "1312.7688", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Pavel A. Smirnov, Konstantin V. Knyazkov,\n  Alexander S. Zagarskikh, Alexander V. Boukhanovsky", "title": "Knowledge-based Expressive Technologies within Cloud Computing\n  Environments", "comments": "Proceedings of the 8th International Conference on Intelligent\n  Systems and Knowledge Engineering (ISKE2013). 2013", "journal-ref": null, "doi": "10.1007/978-3-642-54927-4_1", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented paper describes the development of comprehensive approach for\nknowledge processing within e-Sceince tasks. Considering the task solving\nwithin a simulation-driven approach a set of knowledge-based procedures for\ntask definition and composite application processing can be identified. This\nprocedures could be supported by the use of domain-specific knowledge being\nformalized and used for automation purpose. Within this work the developed\nconceptual and technological knowledge-based toolbox for complex\nmultidisciplinary task solv-ing support is proposed. Using CLAVIRE cloud\ncomputing environment as a core platform a set of interconnected expressive\ntechnologies were developed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 11:47:36 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Smirnov", "Pavel A.", ""], ["Knyazkov", "Konstantin V.", ""], ["Zagarskikh", "Alexander S.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1312.7869", "submitter": "Jinliang Wei", "authors": "Jinliang Wei, Wei Dai, Abhimanu Kumar, Xun Zheng, Qirong Ho and Eric\n  P. Xing", "title": "Consistent Bounded-Asynchronous Parameter Servers for Distributed ML", "comments": "Corrected Title", "journal-ref": null, "doi": null, "report-no": "CMU-ML-13-115", "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed ML applications, shared parameters are usually replicated\namong computing nodes to minimize network overhead. Therefore, proper\nconsistency model must be carefully chosen to ensure algorithm's correctness\nand provide high throughput. Existing consistency models used in\ngeneral-purpose databases and modern distributed ML systems are either too\nloose to guarantee correctness of the ML algorithms or too strict and thus fail\nto fully exploit the computing power of the underlying distributed system.\n  Many ML algorithms fall into the category of \\emph{iterative convergent\nalgorithms} which start from a randomly chosen initial point and converge to\noptima by repeating iteratively a set of procedures. We've found that many such\nalgorithms are to a bounded amount of inconsistency and still converge\ncorrectly. This property allows distributed ML to relax strict consistency\nmodels to improve system performance while theoretically guarantees algorithmic\ncorrectness. In this paper, we present several relaxed consistency models for\nasynchronous parallel computation and theoretically prove their algorithmic\ncorrectness. The proposed consistency models are implemented in a distributed\nparameter server and evaluated in the context of a popular ML application:\ntopic modeling.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:53:09 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2013 22:07:17 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Wei", "Jinliang", ""], ["Dai", "Wei", ""], ["Kumar", "Abhimanu", ""], ["Zheng", "Xun", ""], ["Ho", "Qirong", ""], ["Xing", "Eric P.", ""]]}]