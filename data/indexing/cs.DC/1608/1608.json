[{"id": "1608.00039", "submitter": "Chung-Kai Yu", "authors": "Chung-Kai Yu and Mihaela van der Schaar and Ali H. Sayed", "title": "Distributed Learning for Stochastic Generalized Nash Equilibrium\n  Problems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2695451", "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work examines a stochastic formulation of the generalized Nash\nequilibrium problem (GNEP) where agents are subject to randomness in the\nenvironment of unknown statistical distribution. We focus on fully-distributed\nonline learning by agents and employ penalized individual cost functions to\ndeal with coupled constraints. Three stochastic gradient strategies are\ndeveloped with constant step-sizes. We allow the agents to use heterogeneous\nstep-sizes and show that the penalty solution is able to approach the Nash\nequilibrium in a stable manner within $O(\\mu_\\text{max})$, for small step-size\nvalue $\\mu_\\text{max}$ and sufficiently large penalty parameters. The operation\nof the algorithm is illustrated by considering the network Cournot competition\nproblem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 22:11:43 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 15:47:09 GMT"}, {"version": "v3", "created": "Thu, 6 Apr 2017 19:37:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Yu", "Chung-Kai", ""], ["van der Schaar", "Mihaela", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1608.00066", "submitter": "Peng Hao", "authors": "Hao Peng, Rongke Liu, Yi Hou, Ling Zhao", "title": "A Gb/s Parallel Block-based Viterbi Decoder for Convolutional Codes on\n  GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a parallel block-based Viterbi decoder (PBVD) on\nthe graphic processing unit (GPU) platform for the decoding of convolutional\ncodes. The decoding procedure is simplified and parallelized, and the\ncharacteristic of the trellis is exploited to reduce the metric computation.\nBased on the compute unified device architecture (CUDA), two kernels with\ndifferent parallelism are designed to map two decoding phases. Moreover, the\noptimal design of data structures for several kinds of intermediate information\nare presented, to improve the efficiency of internal memory transactions.\nExperimental results demonstrate that the proposed decoder achieves high\nthroughput of 598Mbps on NVIDIA GTX580 and 1802Mbps on GTX980 for the 64-state\nconvolutional code, which are 1.5 times speedup compared to the existing\nfastest works on GPUs.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jul 2016 03:21:34 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Peng", "Hao", ""], ["Liu", "Rongke", ""], ["Hou", "Yi", ""], ["Zhao", "Ling", ""]]}, {"id": "1608.00214", "submitter": "Joel Rybicki", "authors": "Christoph Lenzen and Joel Rybicki", "title": "Near-Optimal Self-Stabilising Counting and Firing Squads", "comments": "1+30 pages, 6 figures, extended and revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a fully-connected synchronous distributed system consisting of $n$\nnodes, where up to $f$ nodes may be faulty and every node starts in an\narbitrary initial state. In the synchronous $C$-counting problem, all nodes\nneed to eventually agree on a counter that is increased by one modulo $C$ in\neach round for given $C>1$. In the self-stabilising firing squad problem, the\ntask is to eventually guarantee that all non-faulty nodes have simultaneous\nresponses to external inputs: if a subset of the correct nodes receive an\nexternal \"go\" signal as input, then all correct nodes should agree on a round\n(in the not-too-distant future) in which to jointly output a \"fire\" signal.\nMoreover, no node should generate a \"fire\" signal without some correct node\nhaving previously received a \"go\" signal as input.\n  We present a framework reducing both tasks to binary consensus at very small\ncost. For example, we obtain a deterministic algorithm for self-stabilising\nByzantine firing squads with optimal resilience $f<n/3$, asymptotically optimal\nstabilisation and response time $O(f)$, and message size $O(\\log f)$. As our\nframework does not restrict the type of consensus routines used, we also obtain\nefficient randomised solutions, and it is straightforward to adapt our\nframework for other types of permanent faults.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 12:57:13 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 13:13:15 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Lenzen", "Christoph", ""], ["Rybicki", "Joel", ""]]}, {"id": "1608.00249", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Comments on the parallelization efficiency of the Sunway TaihuLight\n  supercomputer", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world of supercomputers, the large number of processors requires to\nminimize the inefficiencies of parallelization, which appear as a sequential\npart of the program from the point of view of Amdahl's law. The recently\nsuggested new figure of merit is applied to the recently presented\nsupercomputer, and the timeline of \"Top 500\" supercomputers is scrutinized\nusing the metric. It is demonstrated, that in addition to the computing\nperformance and power consumption, the new supercomputer is also excellent in\nthe efficiency of parallelization. Based on the suggested merit, a \"Moore-law\"\nlike observation is derived for the timeline of parallelization efficacy of\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2016 19:02:34 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1608.00307", "submitter": "Boya Di", "authors": "Boya Di, Tianyu Wang, Lingyang Song, and Zhu Han", "title": "Collaborative Smartphone Sensing using Overlapping Coalition Formation\n  Games", "comments": null, "journal-ref": null, "doi": "10.1109/TMC.2016.2538224", "report-no": null, "categories": "cs.DC cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of sensor technology, smartphone sensing has become an\neffective approach to improve the quality of smartphone applications. However,\ndue to time-varying wireless channels and lack of incentives for the users to\nparticipate, the quality and quantity of the data uploaded by the smartphone\nusers are not always satisfying. In this paper, we consider a smartphone\nsensing system in which a platform publicizes multiple tasks, and the\nsmartphone users choose a set of tasks to participate in. In the traditional\nnon-cooperative approach with incentives, each smartphone user gets rewards\nfrom the platform as an independent individual and the limit of the wireless\nchannel resources is often omitted. To tackle this problem, we introduce a\nnovel cooperative approach with an overlapping coalition formation game\n(OCF-game) model, in which the smartphone users can cooperate with each other\nto form the overlapping coalitions for different sensing tasks. We also utilize\na centralized case to describe the upper bound of the system sensing\nperformance. Simulation results show that the cooperative approach achieves a\nbetter performance than the non-cooperative one in various situations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 03:30:09 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Di", "Boya", ""], ["Wang", "Tianyu", ""], ["Song", "Lingyang", ""], ["Han", "Zhu", ""]]}, {"id": "1608.00361", "submitter": "Reza Arablouei", "authors": "Reza Arablouei, Ethan Goan, Stephen Gensemer, and Branislav Kusy", "title": "Fast and robust pushbroom hyperspectral imaging via DMD-based scanning", "comments": null, "journal-ref": null, "doi": "10.1117/12.2239107", "report-no": null, "categories": "cs.CV cs.DC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new pushbroom hyperspectral imaging device that has no macro\nmoving part. The main components of the proposed hyperspectral imager are a\ndigital micromirror device (DMD), a CMOS image sensor with no filter as the\nspectral sensor, a CMOS color (RGB) image sensor as the auxiliary image sensor,\nand a diffraction grating. Using the image sensor pair, the device can\nsimultaneously capture hyperspectral data as well as RGB images of the scene.\nThe RGB images captured by the auxiliary image sensor can facilitate geometric\nco-registration of the hyperspectral image slices captured by the spectral\nsensor. In addition, the information discernible from the RGB images can lead\nto capturing the spectral data of only the regions of interest within the\nscene. The proposed hyperspectral imaging architecture is cost-effective, fast,\nand robust. It also enables a trade-off between resolution and speed. We have\nbuilt an initial prototype based on the proposed design. The prototype can\ncapture a hyperspectral image datacube with a spatial resolution of 192x192\npixels and a spectral resolution of 500 bands in less than thirty seconds.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 09:18:38 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 03:26:49 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Arablouei", "Reza", ""], ["Goan", "Ethan", ""], ["Gensemer", "Stephen", ""], ["Kusy", "Branislav", ""]]}, {"id": "1608.00406", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Ozgur Akgun, Ian Miguel, Long Thai and Adam Barker", "title": "Cloud Benchmarking For Maximising Performance of Scientific Applications", "comments": "14 pages, accepted to the IEEE Transactions on Cloud Computing on 31\n  July 2016", "journal-ref": null, "doi": "10.1109/TCC.2016.2603476", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can applications be deployed on the cloud to achieve maximum performance?\nThis question is challenging to address with the availability of a wide variety\nof cloud Virtual Machines (VMs) with different performance capabilities. The\nresearch reported in this paper addresses the above question by proposing a six\nstep benchmarking methodology in which a user provides a set of weights that\nindicate how important memory, local communication, computation and storage\nrelated operations are to an application. The user can either provide a set of\nfour abstract weights or eight fine grain weights based on the knowledge of the\napplication. The weights along with benchmarking data collected from the cloud\nare used to generate a set of two rankings - one based only on the performance\nof the VMs and the other takes both performance and costs into account. The\nrankings are validated on three case study applications using two validation\ntechniques. The case studies on a set of experimental VMs highlight that\nmaximum performance can be achieved by the three top ranked VMs and maximum\nperformance in a cost-effective manner is achieved by at least one of the top\nthree ranked VMs produced by the methodology.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 12:41:28 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Varghese", "Blesson", ""], ["Akgun", "Ozgur", ""], ["Miguel", "Ian", ""], ["Thai", "Long", ""], ["Barker", "Adam", ""]]}, {"id": "1608.00492", "submitter": "K\\'evin Atighehchi", "authors": "Kevin Atighehchi", "title": "Some observations on the optimization of a parallel SHAKE function using\n  Sakura", "comments": "Addition of some comments (and an illustration) in Section 3.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some parallel constructions of a SHAKE hash function using Sakura coding are\nintroduced, whose basic operation is the Keccak's permutation. For each\nproposed tree-based algorithm, observations are made on both its parallel\nrunning time (depth) and the required number of processors to reach it. This\npreliminary work makes the assumption that the tree-level chaining value length\nis equal to the capacity of the underlying sponge construction, as recommended\nin the Sakura paper.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 16:44:26 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 15:24:12 GMT"}, {"version": "v3", "created": "Sat, 13 Aug 2016 23:00:21 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Atighehchi", "Kevin", ""]]}, {"id": "1608.00509", "submitter": "Mahdi Zamani", "authors": "Mahdi Zamani, Jared Saia, Jedidiah Crandall", "title": "TorBricks: Blocking-Resistant Tor Bridge Distribution", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tor is currently the most popular network for anonymous Internet access. It\ncritically relies on volunteer nodes called bridges for relaying Internet\ntraffic when a user's ISP blocks connections to Tor. Unfortunately, current\nmethods for distributing bridges are vulnerable to malicious users who obtain\nand block bridge addresses. In this paper, we propose TorBricks, a protocol for\ndistributing Tor bridges to n users, even when an unknown number t < n of these\nusers are controlled by a malicious adversary. TorBricks distributes O(tlog(n))\nbridges and guarantees that all honest users can connect to Tor with high\nprobability after O(log(t)) rounds of communication with the distributor.\n  We also extend our algorithm to perform privacy-preserving bridge\ndistribution when run among multiple untrusted distributors. This not only\nprevents the distributors from learning bridge addresses and bridge assignment\ninformation, but also provides resistance against malicious attacks from a m/3\nfraction of the distributors, where m is the number of distributors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 17:58:37 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Zamani", "Mahdi", ""], ["Saia", "Jared", ""], ["Crandall", "Jedidiah", ""]]}, {"id": "1608.00571", "submitter": "Daniel Sorin", "authors": "Blake A. Hechtman, Andrew D. Hilton, and Daniel J. Sorin", "title": "TREES: A CPU/GPU Task-Parallel Runtime with Explicit Epoch\n  Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a task-parallel runtime system, called TREES, that is\ndesigned for high performance on CPU/GPU platforms. On platforms with multiple\nCPUs, Cilk's \"work-first\" principle underlies how task-parallel applications\ncan achieve performance, but work-first is a poor fit for GPUs. We build upon\nwork-first to create the \"work-together\" principle that addresses the specific\nstrengths and weaknesses of GPUs. The work-together principle extends\nwork-first by stating that (a) the overhead on the critical path should be paid\nby the entire system at once and (b) work overheads should be paid\nco-operatively. We have implemented the TREES runtime in OpenCL, and we\nexperimentally evaluate TREES applications on a CPU/GPU platform.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 15:33:14 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Hechtman", "Blake A.", ""], ["Hilton", "Andrew D.", ""], ["Sorin", "Daniel J.", ""]]}, {"id": "1608.00636", "submitter": "Florian Frank", "authors": "Max Grossman, Christopher Thiele, Mauricio Araya-Polo, Florian Frank,\n  Faruk O. Alpak, Vivek Sarkar", "title": "A survey of sparse matrix-vector multiplication performance on large\n  matrices", "comments": "Rice Oil & Gas High Performance Computing Workshop. March 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute a third-party survey of sparse matrix-vector (SpMV) product\nperformance on industrial-strength, large matrices using: (1) The SpMV\nimplementations in Intel MKL, the Trilinos project (Tpetra subpackage), the\nCUSPARSE library, and the CUSP library, each running on modern architectures.\n(2) NVIDIA GPUs and Intel multi-core CPUs (supported by each software package).\n(3) The CSR, BSR, COO, HYB, and ELL matrix formats (supported by each software\npackage).\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2016 22:52:28 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Grossman", "Max", ""], ["Thiele", "Christopher", ""], ["Araya-Polo", "Mauricio", ""], ["Frank", "Florian", ""], ["Alpak", "Faruk O.", ""], ["Sarkar", "Vivek", ""]]}, {"id": "1608.00656", "submitter": "EPTCS", "authors": "Camille Coti (LIPN, CNRS UMR 7030, Universit\\'e Paris 13)", "title": "Parametric, Probabilistic, Timed Resource Discovery System", "comments": "In Proceedings Cassting'16/SynCoP'16, arXiv:1608.00177", "journal-ref": "EPTCS 220, 2016, pp. 53-62", "doi": "10.4204/EPTCS.220.5", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully distributed resource discovery and reservation\nsystem. Verification of such a system is important to ensure the execution of\ndistributed applications on a set of resources in appropriate conditions. A\nsemi-formal model for his system is presented using probabilistic timed\nautomata. This model is timed, parametric and probabilistic, making it a\nchallenge to the parameter synthesis community.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 00:37:01 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Coti", "Camille", "", "LIPN, CNRS UMR 7030, Universit\u00e9 Paris 13"]]}, {"id": "1608.00687", "submitter": "Xibo Jin", "authors": "Xibo Jin, Fa Zhang, Athanasios V. Vasilakos, and Zhiyong Liu", "title": "Green Data Centers: A Survey, Perspectives, and Future Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, a major concern regarding data centers is their extremely high\nenergy consumption and carbon dioxide emissions. However, because of the\nover-provisioning of resources, the utilization of existing data centers is, in\nfact, remarkably low, leading to considerable energy waste. Therefore, over the\npast few years, many research efforts have been devoted to increasing\nefficiency for the construction of green data centers. The goal of these\nefforts is to efficiently utilize available resources and to reduce energy\nconsumption and thermal cooling costs. In this paper, we provide a survey of\nthe state-of-the-art research on green data center techniques, including energy\nefficiency, resource management, thermal control and green metrics.\nAdditionally, we present a detailed comparison of the reviewed proposals. We\nfurther discuss the key challenges for future research and highlight some\nfuture research issues for addressing the problem of building green data\ncenters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 03:28:30 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Jin", "Xibo", ""], ["Zhang", "Fa", ""], ["Vasilakos", "Athanasios V.", ""], ["Liu", "Zhiyong", ""]]}, {"id": "1608.00695", "submitter": "Eduardo Castell\\'o Ferrer", "authors": "Eduardo Castell\\'o Ferrer", "title": "The blockchain: a new framework for robotic swarm systems", "comments": "12 pages, 13 figures", "journal-ref": null, "doi": "10.1007/978-3-030-02683-7_77", "report-no": null, "categories": "cs.RO cs.DC cs.ET cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Swarms of robots will revolutionize many industrial applications, from\ntargeted material delivery to precision farming. However, several of the\nheterogeneous characteristics that make them ideal for certain future\napplications --- robot autonomy, decentralized control, collective emergent\nbehavior, etc. --- hinder the evolution of the technology from academic\ninstitutions to real-world problems. Blockchain, an emerging technology\noriginated in the Bitcoin field, demonstrates that by combining peer-to-peer\nnetworks with cryptographic algorithms a group of agents can reach an agreement\non a particular state of affairs and record that agreement without the need for\na controlling authority. The combination of blockchain with other distributed\nsystems, such as robotic swarm systems, can provide the necessary capabilities\nto make robotic swarm operations more secure, autonomous, flexible and even\nprofitable. This work explains how blockchain technology can provide innovative\nsolutions to four emergent issues in the swarm robotics research field. New\nsecurity, decision making, behavior differentiation and business models for\nswarm robotic systems are described by providing case scenarios and examples.\nFinally, limitations and possible future problems that arise from the\ncombination of these two technologies are described.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 05:09:43 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 05:17:03 GMT"}, {"version": "v3", "created": "Thu, 25 Aug 2016 17:39:10 GMT"}, {"version": "v4", "created": "Sun, 25 Jun 2017 00:00:46 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Ferrer", "Eduardo Castell\u00f3", ""]]}, {"id": "1608.00726", "submitter": "Sebastien Tixeuil", "authors": "Dianne Foreback, Mikhail Nesterenko, S\\'ebastien Tixeuil (NPA, IUF,\n  LINCS)", "title": "Infinite Unlimited Churn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study unlimited infinite churn in peer-to-peer overlay networks. Under\nthis churn, arbitrary many peers may concurrently request to join or leave the\noverlay network; moreover these requests may never stop coming. We prove that\nunlimited adversarial churn, where processes may just exit the overlay network,\nis unsolvable. We focus on cooperative churn where exiting processes\nparticipate in the churn handling algorithm. We define the problem of unlimited\ninfinite churn in this setting. We distinguish the fair version of the problem,\nwhere each request is eventually satisfied, from the unfair version that just\nguarantees progress. We focus on local solutions to the problem, and prove that\na local solution to the Fair Infinite Unlimited Churn is impossible. We then\npresent and prove correct an algorithm UIUC that solves the Unfair Infinite\nUnlimited Churn Problem for a linearized peer-to-peer overlay network. We\nextend this solution to skip lists and skip graphs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 08:25:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Foreback", "Dianne", "", "NPA, IUF,\n  LINCS"], ["Nesterenko", "Mikhail", "", "NPA, IUF,\n  LINCS"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF,\n  LINCS"]]}, {"id": "1608.00781", "submitter": "Edward J. Yoon", "authors": "Edward J. Yoon", "title": "Horn: A System for Parallel Training and Regularizing of Large-Scale\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": "EP-909420F9A6E94B3691E5EE413DAD353E", "categories": "cs.DC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  I introduce a new distributed system for effective training and regularizing\nof Large-Scale Neural Networks on distributed computing architectures. The\nexperiments demonstrate the effectiveness of flexible model partitioning and\nparallelization strategies based on neuron-centric computation model, with an\nimplementation of the collective and parallel dropout neural networks training.\nExperiments are performed on MNIST handwritten digits classification including\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 11:57:09 GMT"}, {"version": "v2", "created": "Sun, 26 Feb 2017 22:01:12 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Yoon", "Edward J.", ""]]}, {"id": "1608.00849", "submitter": "Nikos Chondros", "authors": "Nikos Chondros, Bingsheng Zhang, Thomas Zacharias, Panos\n  Diamantopoulos, Stathis Maneas, Christos Patsonakis, Alex Delis, Aggelos\n  Kiayias, Mema Roussopoulos", "title": "Distributed, End-to-end Verifiable, and Privacy-Preserving Internet\n  Voting Systems", "comments": "arXiv admin note: text overlap with arXiv:1507.06812", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-voting systems are a powerful technology for improving democracy.\nUnfortunately, prior voting systems have single points-of-failure, which may\ncompromise availability, privacy, or integrity of the election results.\n  We present the design, implementation, security analysis, and evaluation of\nthe D-DEMOS suite of distributed, privacy-preserving, and end-to-end verifiable\ne-voting systems. We present two systems: one asynchronous and one with minimal\ntiming assumptions but better performance. Our systems include a distributed\nvote collection subsystem that does not require cryptographic operations on\nbehalf of the voter. We also include a distributed, replicated and\nfault-tolerant Bulletin Board component, that stores all necessary\nelection-related information, and allows any party to read and verify the\ncomplete election process. Finally, we incorporate trustees, who control result\nproduction while guaranteeing privacy and end-to-end-verifiability as long as\ntheir strong majority is honest.\n  Our suite of e-voting systems are the first whose voting operation is human\nverifiable, i.e., a voter can vote over the web, even when her web client stack\nis potentially unsafe, without sacrificing her privacy, and still be assured\nher vote was recorded as cast. Additionally, a voter can outsource election\nauditing to third parties, still without sacrificing privacy.\n  We provide a model and security analysis of the systems, implement complete\nprototypes, measure their performance experimentally, and demonstrate their\nability to handle large-scale elections. Finally, we demonstrate the\nperformance trade-offs between the two versions of the system. A preliminary\nversion of our system was used to conduct exit-polls at three voting sites for\ntwo national-level elections and is being adopted for use by the largest civil\nunion of workers in Greece, consisting of over a half million members.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 14:49:51 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Chondros", "Nikos", ""], ["Zhang", "Bingsheng", ""], ["Zacharias", "Thomas", ""], ["Diamantopoulos", "Panos", ""], ["Maneas", "Stathis", ""], ["Patsonakis", "Christos", ""], ["Delis", "Alex", ""], ["Kiayias", "Aggelos", ""], ["Roussopoulos", "Mema", ""]]}, {"id": "1608.00944", "submitter": "Mohsen Mosleh", "authors": "Mohsen Mosleh, Kia Dalili and Babak Heydari", "title": "Distributed or Monolithic? A Computational Architecture Decision\n  Framework", "comments": "12 pages", "journal-ref": "IEEE Systems Journal, Early View, 2016", "doi": "10.1109/JSYST.2016.2594290", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed architectures have become ubiquitous in many complex technical\nand socio-technical systems because of their role in improving uncertainty\nmanagement, accommodating multiple stakeholders, and increasing scalability and\nevolvability. This departure from monolithic architectures provides a system\nwith more flexibility and robustness in response to uncertainties that it may\nconfront during its lifetime. Distributed architecture does not provide\nbenefits only, as it can increase cost and complexity of the system and result\nin potential instabilities. The mechanisms behind this trade-off, however, are\nanalogous to those of the widely-studied transition from integrated to modular\narchitectures. In this paper, we use a conceptual decision framework that\nunifies modularity and distributed architecture on a five-stage systems\narchitecture spectrum. We add an extensive computational layer to the framework\nand explain how this can enhance decision making about the level of modularity\nof the architecture. We then apply it to a simplified demonstration of the\nDefense Advanced Research Projects Agency (DARPA) fractionated satellite\nprogram. Through simulation, we calculate the net value that is gained (or\nlost) by migrating from a monolithic architecture to a distributed architecture\nand show how this value changes as a function of uncertainties in the\nenvironment and various system parameters. Additionally, we use Value at Risk\nas a measure for the risk of losing the value of distributed architecture,\ngiven its inherent uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 19:24:27 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Mosleh", "Mohsen", ""], ["Dalili", "Kia", ""], ["Heydari", "Babak", ""]]}, {"id": "1608.01009", "submitter": "Sergey Bastrakov", "authors": "Igor Surmin, Sergey Bastrakov, Zakhar Matveev, Evgeny Efimenko, Arkady\n  Gonoskov, Iosif Meyerov", "title": "Co-design of a particle-in-cell plasma simulation code for Intel Xeon\n  Phi: a first look at Knights Landing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional particle-in-cell laser-plasma simulation is an important\narea of computational physics. Solving state-of-the-art problems requires\nlarge-scale simulation on a supercomputer using specialized codes. A growing\ndemand in computational resources inspires research in improving efficiency and\nco-design for supercomputers based on many-core architectures. This paper\npresents first performance results of the particle-in-cell plasma simulation\ncode PICADOR on the recently introduced Knights Landing generation of Intel\nXeon Phi. A straightforward rebuilding of the code yields a 2.43 x speedup\ncompared to the previous Knights Corner generation. Further code optimization\nresults in an additional 1.89 x speedup. The optimization performed is\nbeneficial not only for Knights Landing, but also for high-end CPUs and Knights\nCorner. The optimized version achieves 100 GFLOPS double precision performance\non a Knights Landing device with the speedups of 2.35 x compared to a 14-core\nHaswell CPU and 3.47 x compared to a 61-core Knights Corner Xeon Phi.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 21:24:55 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Surmin", "Igor", ""], ["Bastrakov", "Sergey", ""], ["Matveev", "Zakhar", ""], ["Efimenko", "Evgeny", ""], ["Gonoskov", "Arkady", ""], ["Meyerov", "Iosif", ""]]}, {"id": "1608.01019", "submitter": "Mihai Gramada", "authors": "Mihai Gramada", "title": "The Entity Registry System. From concept to deployment", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entity registry system (ERS) is a decentralized entity registry that can\nbe used to replace the Web as a platform for publishing linked data when the\nlatter is not available. In developing countries, where off-line is the default\nmode of operation, centralized linked data solutions fail to address the needs\nof the communities. Although the features are mostly completed, the system is\nnot yet ready for deployment. This project aims to provide extensive tests and\nscalability investigations that would make it ready for a real scenario.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2016 22:35:17 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Gramada", "Mihai", ""]]}, {"id": "1608.01362", "submitter": "Yunming Zhang", "authors": "Yunming Zhang, Vladimir Kiriansky, Charith Mendis, Matei Zaharia,\n  Saman Amarasinghe", "title": "Making Caches Work for Graph Analytics", "comments": "paper accepted at IEEE BigData 17, Best Student Paper", "journal-ref": "IEEE BigData 2017", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware systems are heavily underutilized when running large-scale\ngraph applications. While many in-memory graph frameworks have made substantial\nprogress in optimizing these applications, we show that it is still possible to\nachieve up to 4 $\\times$ speedups over the fastest frameworks by greatly\nimproving cache utilization. Previous systems have applied out-of-core\nprocessing techniques from the memory/disk boundary to the cache/DRAM boundary.\nHowever, we find that blindly applying such techniques is ineffective because\nof the much smaller performance gap between DRAM and cache. We present two\ntechniques that take advantage of the cache with minimal or no instruction\noverhead. The first, frequency based clustering, groups together frequently\naccessed vertices to improve the utilization of each cache line with no runtime\noverhead. The second, CSR segmenting, partitions the graph to restrict all\nrandom accesses to the cache, makes all DRAM access sequential, and merges\npartition results using a very low overhead cache-aware merge. Both techniques\ncan be easily implemented on top of optimized graph frameworks. Our techniques\ncombined give speedups of up to 4 $\\times$ for PageRank, Label Propagation and\nCollaborative Filtering, and 2 $\\times$ for Betweenness Centrality over the\nbest published results\n", "versions": [{"version": "v1", "created": "Wed, 3 Aug 2016 21:23:07 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 00:44:41 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 14:29:01 GMT"}, {"version": "v4", "created": "Fri, 12 Jan 2018 15:34:06 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Zhang", "Yunming", ""], ["Kiriansky", "Vladimir", ""], ["Mendis", "Charith", ""], ["Zaharia", "Matei", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1608.01499", "submitter": "Ricardo Rocha", "authors": "Jo\\~ao Santos and Ricardo Rocha", "title": "On the Implementation of an Or-Parallel Prolog System for Clusters of\n  Multicores", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, clusters of multicores are becoming the norm and, although, many\nor-parallel Prolog systems have been developed in the past, to the best of our\nknowledge, none of them was specially designed to explore the combination of\nshared and distributed memory architectures. In recent work, we have proposed a\nnovel computational model specially designed for such combination which\nintroduces a layered model with two scheduling levels, one for workers sharing\nmemory resources, which we named a team of workers, and another for teams of\nworkers (not sharing memory resources). In this work, we present a first\nimplementation of such model and for that we revive and extend the YapOr system\nto exploit or-parallelism between teams of workers. We also propose a new set\nof built-in predicates that constitute the syntax to interact with an\nor-parallel engine in our platform. Experimental results show that our\nimplementation is able to increase speedups as we increase the number of\nworkers per team, thus taking advantage of the maximum number of cores in a\nmachine, and to increase speedups as we increase the number of teams, thus\ntaking advantage of adding more computer nodes to a cluster. We thus argue that\nour platform is an efficient and viable alternative for exploiting implicit\nor-parallelism in the currently available clusters of low cost multicore\narchitectures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 11:32:45 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Santos", "Jo\u00e3o", ""], ["Rocha", "Ricardo", ""]]}, {"id": "1608.01537", "submitter": "Rajrup Ghosh", "authors": "Rajrup Ghosh, Yogesh Simmhan", "title": "Distributed Scheduling of Event Analytics across Edge and Cloud", "comments": "29 pages, 6 figures, Journal", "journal-ref": "ACM Transactions on Cyber-Physical Systems, Volume 2 Issue 4,\n  September 2018", "doi": "10.1145/3140256", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) domains generate large volumes of high velocity\nevent streams from sensors, which need to be analyzed with low latency to drive\ndecisions. Complex Event Processing (CEP) is a Big Data technique to enable\nsuch analytics, and is traditionally performed on Cloud Virtual Machines (VM).\nLeveraging captive IoT edge resources in combination with Cloud VMs can offer\nbetter performance, flexibility and monetary costs for CEP. Here, we formulate\nan optimization problem for energy-aware placement of CEP queries, composed as\nan analytics dataflow, across a collection of edge and Cloud resources, with\nthe goal of minimizing the end-to-end latency for the dataflow. We propose a\nGenetic Algorithm (GA) meta-heuristic to solve this problem, and compare it\nagainst a brute-force optimal algorithm (BF). We perform detailed real-world\nbenchmarks on the compute, network and energy capacity of edge and Cloud\nresources. These results are used to define a realistic and comprehensive\nsimulation study that validates the BF and GA solutions for 45 diverse CEP\ndataflows, LAN and WAN setup, and different edge resource availability. We\ncompare the GA and BF solutions against random and Cloud-only baselines for\ndifferent configurations, for a total of 1764 simulation runs. Our study shows\nthat GA is within 97% of the optimal BF solution that takes hours, maps\ndataflows with 4 - 50 queries in 1 - 26 secs, and only fails to offer a\nfeasible solution <= 20% of the time.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 14:07:39 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:32:44 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 11:14:41 GMT"}, {"version": "v4", "created": "Sat, 9 Dec 2017 17:22:52 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ghosh", "Rajrup", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1608.01689", "submitter": "Gregory Schwartzman", "authors": "Keren Censor-Hillel, Merav Parter, Gregory Schwartzman", "title": "Derandomizing Local Distributed Algorithms under Bandwidth Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the cornerstone family of \\emph{local problems} in\ndistributed computing, and investigates the curious gap between randomized and\ndeterministic solutions under bandwidth restrictions.\n  Our main contribution is in providing tools for derandomizing solutions to\nlocal problems, when the $n$ nodes can only send $O(\\log n)$-bit messages in\neach round of communication. We combine bounded independence, which we show to\nbe sufficient for some algorithms, with the method of conditional expectations\nand with additional machinery, to obtain the following results.\n  Our techniques give a deterministic maximal independent set (MIS) algorithm\nin the CONGEST model, where the communication graph is identical to the input\ngraph, in $O(D\\log^2 n)$ rounds, where $D$ is the diameter of the graph. The\nbest known running time in terms of $n$ alone is $2^{O(\\sqrt{\\log n})}$, which\nis super-polylogarithmic, and requires large messages. For the CONGEST model,\nthe only known previous solution is a coloring-based $O(\\Delta + \\log^*\nn)$-round algorithm, where $\\Delta$ is the maximal degree in the graph.\n  On the way to obtaining the above, we show that in the \\emph{Congested\nClique} model, which allows all-to-all communication, there is a deterministic\nMIS algorithm that runs in $O(\\log \\Delta \\log n)$ rounds.%, where $\\Delta$ is\nthe maximum degree. When $\\Delta=O(n^{1/3})$, the bound improves to $O(\\log\n\\Delta)$ and holds also for $(\\Delta+1)$-coloring.\n  In addition, we deterministically construct a $(2k-1)$-spanner with\n$O(kn^{1+1/k}\\log n)$ edges in $O(k \\log n)$ rounds. For comparison, in the\nmore stringent CONGEST model, the best deterministic algorithm for constructing\na $(2k-1)$-spanner with $O(kn^{1+1/k})$ edges runs in $O(n^{1-1/k})$ rounds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 20:16:18 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Parter", "Merav", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1608.01712", "submitter": "Victor Yodaiken", "authors": "Victor Yodaiken", "title": "State machines for large scale computer software and systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for specifying the behavior and architecture of discrete state\nsystems such as digital electronic devices and software using deterministic\nstate machines and automata products. The state machines are represented by\nsequence maps $f:A^*\\to X$ where $f(s)=x$ indicates that the output of the\nsystem is $x$ in the state reached by following the sequence of events $s$ from\nthe initial state. Examples provided include counters, networks, reliable\nmessage delivery, real-time analysis of gates and latches, and\nproducer/consumer. Techniques for defining, parameterizing, characterizing\nabstract properties, and connecting sequence functions are developed. Sequence\nfunctions are shown to represent (possibly non-finite) Moore type state\nmachines and general products of state machines. The method draws on state\nmachine theory, automata products, and recursive functions and is ordinary\nworking mathematics, not involving formal methods or any foundational or\nmeta-mathematical techniques. Systems in which there are levels of components\nthat may operate in parallel or concurrently are specified in terms of function\ncomposition.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2016 22:16:50 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 17:29:22 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 05:37:25 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 01:18:12 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yodaiken", "Victor", ""]]}, {"id": "1608.02274", "submitter": "Liang Yu", "authors": "Liang Yu, Tao Jiang, and Yulong Zou", "title": "Distributed Real-Time Energy Management in Data Center Microgrids", "comments": "To be published in IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data center operators are typically faced with three significant problems\nwhen running their data centers, i.e., rising electricity bills, growing carbon\nfootprints and unexpected power outages. To mitigate these issues, running data\ncenters in microgrids is a good choice since microgrids can enhance the energy\nefficiency, sustainability and reliability of electrical services. Thus, in\nthis paper, we investigate the problem of energy management for multiple data\ncenter microgrids. Specifically, we intend to minimize the long-term\noperational cost of data center microgrids by taking into account the\nuncertainties in electricity prices, renewable outputs and data center\nworkloads. We first formulate a stochastic programming problem with the\nconsiderations of many factors, e.g., providing heterogeneous service delay\nguarantees for batch workloads, interactive workload allocation, batch workload\nshedding, electricity buying/selling, battery charging/discharging efficiency,\nand the ramping constraints of backup generators. Then, we design a realtime\nand distributed algorithm for the formulated problem based on Lyapunov\noptimization technique and a variant of alternating direction method of\nmultipliers (ADMM). Moreover, the performance guarantees provided by the\nproposed algorithm are analyzed. Extensive simulation results indicate the\neffectiveness of the proposed algorithm in operational cost reduction for data\ncenter microgrids.\n", "versions": [{"version": "v1", "created": "Sun, 7 Aug 2016 22:18:33 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 08:55:46 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 04:52:08 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Yu", "Liang", ""], ["Jiang", "Tao", ""], ["Zou", "Yulong", ""]]}, {"id": "1608.02432", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, Kaushik Mondal, H. Ramesh and Partha Sarathi\n  Mandal", "title": "Fault-Tolerant Gathering of Mobile Robots with Weak Multiplicity\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a wide interest in designing distributed algorithms for tiny\nrobots. In particular, it has been shown that the robots can complete certain\ntasks even in the presence of faulty robots. In this paper, we focus on\ngathering of all non-faulty robots at a single point in presence of faulty\nrobots. We propose a wait-free algorithm (i.e., no robot waits for other robot\nand algorithm instructs each robot to move in every step, unless it is already\nat the gathering location), that gathers all non-faulty robots in\nsemi-synchronous model without any agreement in the coordinate system and with\nweak multiplicity detection (i.e., a robot can only detect that either there is\none or more robot at a location) in the presence of at most $n-1$ faulty robots\nfor $n\\geqslant 3$. We show that the required capability for gathering robots\nis minimal in the above model, since relaxing it further makes gathering\nimpossible to solve.\n  Also, we introduce an intermediate scheduling model \\textit{ASYNC$_{IC}$}\nbetween the asynchronous ( i.e., no instantaneous movement or computation) and\nthe semi-synchronous (i.e., both instantaneous movement and computation) as the\nasynchronous model with instantaneous computation. Then we propose another\nalgorithm in \\textit{ASYNC$_{IC}$} model for gathering all non-faulty robots\nwith weak multiplicity detection without any agreement on the coordinate system\nin the presence of at most $\\lfloor n/2\\rfloor-2$ faulty robots for $n\\geqslant\n7$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 13:49:18 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Mondal", "Kaushik", ""], ["Ramesh", "H.", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1608.02442", "submitter": "Niklas Ekstr\\\"om", "authors": "Niklas Ekstr\\\"om and Seif Haridi", "title": "A Fault-Tolerant Sequentially Consistent DSM With a Compositional\n  Correctness Proof", "comments": "Paper presented at the 4th Edition of The International Conference on\n  NETworked sYStems, May 18-20, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the SC-ABD algorithm that implements sequentially consistent\ndistributed shared memory (DSM). The algorithm tolerates that less than half of\nthe processes are faulty (crash-stop). Compared to the multi-writer ABD\nalgorithm, SC-ABD requires one instead of two round-trips of communication to\nperform a write operation, and an equal number of round-trips (two) to perform\na read operation. Although sequential consistency is not a compositional\nconsistency condition, the provided correctness proof is compositional.\n", "versions": [{"version": "v1", "created": "Mon, 8 Aug 2016 14:08:23 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Ekstr\u00f6m", "Niklas", ""], ["Haridi", "Seif", ""]]}, {"id": "1608.02674", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall", "title": "Further Algebraic Algorithms in the Congested Clique Model and\n  Applications to Graph-Theoretic Problems", "comments": "29 pages; accepted to DISC'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Censor-Hillel et al. [PODC'15] recently showed how to efficiently implement\ncentralized algebraic algorithms for matrix multiplication in the congested\nclique model, a model of distributed computing that has received increasing\nattention in the past few years. This paper develops further algebraic\ntechniques for designing algorithms in this model. We present deterministic and\nrandomized algorithms, in the congested clique model, for efficiently computing\nmultiple independent instances of matrix products, computing the determinant,\nthe rank and the inverse of a matrix, and solving systems of linear equations.\nAs applications of these techniques, we obtain more efficient algorithms for\nthe computation, again in the congested clique model, of the all-pairs shortest\npaths and the diameter in directed and undirected graphs with small weights,\nimproving over Censor-Hillel et al.'s work. We also obtain algorithms for\nseveral other graph-theoretic problems such as computing the number of edges in\na maximum matching and the Gallai-Edmonds decomposition of a simple graph, and\ncomputing a minimum vertex cover of a bipartite graph.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 02:08:28 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1608.02707", "submitter": "Minxian Xu", "authors": "Minxian Xu, Amir Vahid Dastjerdi, Rajkumar Buyya", "title": "Energy Efficient Scheduling of Cloud Application Components with\n  Brownout", "comments": "14 pages, 10 figures, 8 tables", "journal-ref": null, "doi": "10.1109/TSUSC.2017.2661339", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for cloud data centers meeting unexpected loads like request\nbursts, which may lead to overloaded situation and performance degradation.\nDynamic Voltage Frequency Scaling and VM consolidation have been proved\neffective to manage overloads. However, they cannot function when the whole\ndata center is overloaded. Brownout provides a promising direction to avoid\noverloads through configuring applications to temporarily degrade user\nexperience. Additionally, brownout can also be applied to reduce data center\nenergy consumption. As a complementary option for Dynamic Voltage Frequency\nScaling and VM consolidation, our combined brownout approach reduces energy\nconsumption through selectively and dynamically deactivating application\noptional components, which can also be applied to self-contained microservices.\nThe results show that our approach can save more than 20% energy consumption\nand there are trade-offs between energy saving and discount offered to users.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 07:26:16 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 08:50:53 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 02:01:16 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Xu", "Minxian", ""], ["Dastjerdi", "Amir Vahid", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1608.02780", "submitter": "Pedro Lopez-Garcia", "authors": "Pedro Lopez-Garcia and Maximiliano Klemen and Umer Liqat and Manuel V.\n  Hermenegildo", "title": "A General Framework for Static Profiling of Parametric Resource Usage", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 22 pages,\n  LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional static resource analyses estimate the total resource usage of a\nprogram, without executing it. In this paper we present a novel resource\nanalysis whose aim is instead the static profiling of accumulated cost, i.e.,\nto discover, for selected parts of the program, an estimate or bound of the\nresource usage accumulated in each of those parts. Traditional resource\nanalyses are parametric in the sense that the results can be functions on input\ndata sizes. Our static profiling is also parametric, i.e., our accumulated cost\nestimates are also parameterized by input data sizes. Our proposal is based on\nthe concept of cost centers and a program transformation that allows the static\ninference of functions that return bounds on these accumulated costs depending\non input data sizes, for each cost center of interest. Such information is much\nmore useful to the software developer than the traditional resource usage\nfunctions, as it allows identifying the parts of a program that should be\noptimized, because of their greater impact on the total cost of program\nexecutions. We also report on our implementation of the proposed technique\nusing the CiaoPP program analysis framework, and provide some experimental\nresults. This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 12:13:02 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 10:44:20 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Lopez-Garcia", "Pedro", ""], ["Klemen", "Maximiliano", ""], ["Liqat", "Umer", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1608.02797", "submitter": "Sharon Lee", "authors": "Sharon X Lee, Kaleb L Leemaqz, Geoffrey J McLachlan", "title": "A block EM algorithm for multivariate skew normal and skew t-mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of skew distributions provide a flexible tool for modelling\nheterogeneous data with asymmetric distributional features. However, parameter\nestimation via the Expectation-Maximization (EM) algorithm can become very\ntime-consuming due to the complicated expressions involved in the E-step that\nare numerically expensive to evaluate. A more time-efficient implementation of\nthe EM algorithm was recently proposed which allows each component of the\nmixture model to be evaluated in parallel. In this paper, we develop a block\nimplementation of the EM algorithm that facilitates the calculations in the E-\nand M-steps to be spread across a larger number of threads. We focus on the\nfitting of finite mixtures of multivariate skew normal and skew\nt-distributions, and show that both the E- and M-steps in the EM algorithm can\nbe modified to allow the data to be split into blocks. The approach can be\neasily implemented for use by multicore and multi-processor machines. It can\nalso be applied concurrently with the recently proposed multithreaded EM\nalgorithm to achieve further reduction in computation time. The improvement in\ntime performance is illustrated on some real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 13:28:38 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Lee", "Sharon X", ""], ["Leemaqz", "Kaleb L", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1608.03044", "submitter": "Jianbin Fang", "authors": "Zhaokui Li, Jianbin Fang, Tao Tang, Xuhao Chen and Canqun Yang", "title": "Streaming Applications on Heterogeneous Platforms", "comments": "Accepted in The 13th IFIP International Conference on Network and\n  Parallel Computing (NPC'16). arXiv admin note: text overlap with\n  arXiv:1603.08619", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using multiple streams can improve the overall system performance by\nmitigating the data transfer overhead on heterogeneous systems. Currently, very\nfew cases have been streamed to demonstrate the streaming performance impact\nand a systematic investigation of streaming necessity and how-to over a large\nnumber of test cases remains a gap. In this paper, we use a total of 56\nbenchmarks to build a statistical view of the data transfer overhead, and give\nan in-depth analysis of the impacting factors. Among the heterogeneous codes,\nwe identify two types of non-streamable codes and three types of streamable\ncodes, for which a streaming approach has been proposed. Our experimental\nresults on the CPU-MIC platform show that, with multiple streams, we can\nimprove the application performance by up 90%. Our work can serve as a generic\nflow of using multiple streams on heterogeneous platforms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 04:45:12 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Li", "Zhaokui", ""], ["Fang", "Jianbin", ""], ["Tang", "Tao", ""], ["Chen", "Xuhao", ""], ["Yang", "Canqun", ""]]}, {"id": "1608.03125", "submitter": "EPTCS", "authors": "Eduard Baranov (Ecole polytechnique f\\'ed\\'erale de Lausanne), Simon\n  Bliudze (Ecole polytechnique f\\'ed\\'erale de Lausanne)", "title": "A Note on the Expressiveness of BIP", "comments": "In Proceedings EXPRESS/SOS 2016, arXiv:1608.02692", "journal-ref": "EPTCS 222, 2016, pp. 1-14", "doi": "10.4204/EPTCS.222.1", "report-no": null, "categories": "cs.DC cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our previous algebraic formalisation of the notion of\ncomponent-based framework in order to formally define two forms, strong and\nweak, of the notion of full expressiveness. Our earlier result shows that the\nBIP (Behaviour-Interaction-Priority) framework does not possess the strong full\nexpressiveness. In this paper, we show that BIP has the weak form of this\nnotion and provide results detailing weak and strong full expressiveness for\nclassical BIP and several modifications, obtained by relaxing the constraints\nimposed on priority models.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 11:10:33 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Baranov", "Eduard", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne"], ["Bliudze", "Simon", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne"]]}, {"id": "1608.03175", "submitter": "Vincent T. Lee", "authors": "Vincent T. Lee, Justin Kotalik, Carlo C. Del Mundo, Armin Alaghi, Luis\n  Ceze, Mark Oskin", "title": "Similarity Search on Automata Processors", "comments": "12 pages, 11 figures, accepted to International Parallel and\n  Distribution Processing Symposium (IPDPS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a critical primitive for a wide variety of applications\nincluding natural language processing, content-based search, machine learning,\ncomputer vision, databases, robotics, and recommendation systems. At its core,\nsimilarity search is implemented using the k-nearest neighbors (kNN) algorithm,\nwhere computation consists of highly parallel distance calculations and a\nglobal top-k sort. In contemporary von-Neumann architectures, kNN is\nbottlenecked by data movement which limits throughput and latency. In this\npaper, we present and evaluate a novel automata-based algorithm for kNN on the\nMicron Automata Processor (AP), which is a non-von Neumann near-data processing\narchitecture. By employing near-data processing, the AP minimizes the data\nmovement bottleneck and is able to achieve better performance. Unlike prior\nwork in the automata processing space, our work combines temporal encodings\nwith automata design to augment the space of applications for the AP. We\nevaluate our design's performance on the AP and compare to state-of-the-art\nCPU, GPU, and FPGA implementations; we show that the current generation of AP\nhardware can achieve over 50x speedup over CPUs while maintaining competitive\nenergy efficiency gains. We also propose several automata optimization\ntechniques and simple architectural extensions that highlight the potential of\nthe AP hardware.\n", "versions": [{"version": "v1", "created": "Tue, 9 Aug 2016 17:27:12 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 18:53:02 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Lee", "Vincent T.", ""], ["Kotalik", "Justin", ""], ["Del Mundo", "Carlo C.", ""], ["Alaghi", "Armin", ""], ["Ceze", "Luis", ""], ["Oskin", "Mark", ""]]}, {"id": "1608.03313", "submitter": "Atri Rudra", "authors": "Arkadev Chattopadhyay and Michael Langberg and Shi Li and Atri Rudra", "title": "Tight Network Topology Dependent Bounds on Rounds of Communication", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove tight network topology dependent bounds on the round complexity of\ncomputing well studied $k$-party functions such as set disjointness and element\ndistinctness. Unlike the usual case in the CONGEST model in distributed\ncomputing, we fix the function and then vary the underlying network topology.\nThis complements the recent such results on total communication that have\nreceived some attention. We also present some applications to distributed graph\ncomputation problems.\n  Our main contribution is a proof technique that allows us to reduce the\nproblem on a general graph topology to a relevant two-party communication\ncomplexity problem. However, unlike many previous works that also used the same\nhigh level strategy, we do not reason about a two-party communication problem\nthat is induced by a cut in the graph. To `stitch' back the various lower\nbounds from the two party communication problems, we use the notion of timed\ngraph that has seen prior use in network coding. Our reductions use some tools\nfrom Steiner tree packing and multi-commodity flow problems that have a delay\nconstraint.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2016 23:57:44 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2016 15:05:03 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Chattopadhyay", "Arkadev", ""], ["Langberg", "Michael", ""], ["Li", "Shi", ""], ["Rudra", "Atri", ""]]}, {"id": "1608.03321", "submitter": "EPTCS", "authors": "Simon Fowler (The University of Edinburgh, Edinburgh, UK)", "title": "An Erlang Implementation of Multiparty Session Actors", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 36-50", "doi": "10.4204/EPTCS.223.3", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By requiring co-ordination to take place using explicit message passing\ninstead of relying on shared memory, actor-based programming languages have\nbeen shown to be effective tools for building reliable and fault-tolerant\ndistributed systems. Although naturally communication-centric, communication\npatterns in actor-based applications remain informally specified, meaning that\nerrors in communication are detected late, if at all.\n  Multiparty session types are a formalism to describe, at a global level, the\ninteractions between multiple communicating entities. This article describes\nthe implementation of a prototype framework for monitoring Erlang/OTP\ngen_server applications against multiparty session types, showing how previous\nwork on multiparty session actors can be adapted to a purely actor-based\nlanguage, and how monitor violations and termination of session participants\ncan be reported in line with the Erlang mantra of \"let it fail\". Finally, the\nframework is used to implement two case studies: an adaptation of a\nfreely-available DNS server, and a chat server.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:25:48 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Fowler", "Simon", "", "The University of Edinburgh, Edinburgh, UK"]]}, {"id": "1608.03322", "submitter": "EPTCS", "authors": "Keyvan Azadbakht (Centrum Wiskunde en Informatica), Frank S. de Boer\n  (Centrum Wiskunde en Informatica), Vlad Serbanescu (Centrum Wiskunde en\n  Informatica)", "title": "Multi-Threaded Actors", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 51-66", "doi": "10.4204/EPTCS.223.4", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new programming model of multi-threaded actors\nwhich feature the parallel processing of their messages. In this model an actor\nconsists of a group of active objects which share a message queue. We provide a\nformal operational semantics, and a description of a Java-based implementation\nfor the basic programming abstractions describing multi-threaded actors.\nFinally, we evaluate our proposal by means of an example application.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:25:56 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Azadbakht", "Keyvan", "", "Centrum Wiskunde en Informatica"], ["de Boer", "Frank S.", "", "Centrum Wiskunde en Informatica"], ["Serbanescu", "Vlad", "", "Centrum Wiskunde en\n  Informatica"]]}, {"id": "1608.03326", "submitter": "EPTCS", "authors": "Seyed Hossein Haeri (Universite catholique de Louvain, Belgium), Peter\n  Van Roy (Universite catholique de Louvain, Belgium), Carlos Baquero\n  (Universidade do Minho, Portugal), Christopher Meiklejohn (Universite\n  catholique de Louvain, Belgium)", "title": "Worlds of Events: Deduction with Partial Knowledge about Causality", "comments": "In Proceedings ICE 2016, arXiv:1608.03131", "journal-ref": "EPTCS 223, 2016, pp. 113-127", "doi": "10.4204/EPTCS.223.8", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between internet users are mediated by their devices and the\ncommon support infrastructure in data centres. Keeping track of causality\namongst actions that take place in this distributed system is key to provide a\nseamless interaction where effects follow causes. Tracking causality in large\nscale interactions is difficult due to the cost of keeping large quantities of\nmetadata; even more challenging when dealing with resource-limited devices. In\nthis paper, we focus on keeping partial knowledge on causality and address\ndeduction from that knowledge.\n  We provide the first proof-theoretic causality modelling for distributed\npartial knowledge. We prove computability and consistency results. We also\nprove that the partial knowledge gives rise to a weaker model than classical\ncausality. We provide rules for offline deduction about causality and refute\nsome related folklore. We define two notions of forward and backward\nbisimilarity between devices, using which we prove two important results.\nNamely, no matter the order of addition/removal, two devices deduce similarly\nabout causality so long as: (1) the same causal information is fed to both. (2)\nthey start bisimilar and erase the same causal information. Thanks to our\nestablishment of forward and backward bisimilarity, respectively, proofs of the\nlatter two results work by simple induction on length.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 00:26:34 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Haeri", "Seyed Hossein", "", "Universite catholique de Louvain, Belgium"], ["Van Roy", "Peter", "", "Universite catholique de Louvain, Belgium"], ["Baquero", "Carlos", "", "Universidade do Minho, Portugal"], ["Meiklejohn", "Christopher", "", "Universite\n  catholique de Louvain, Belgium"]]}, {"id": "1608.03545", "submitter": "James Ross", "authors": "James Ross and David Richie", "title": "An OpenSHMEM Implementation for the Adapteva Epiphany Coprocessor", "comments": "14 pages, 9 figures, OpenSHMEM 2016: Third workshop on OpenSHMEM and\n  Related Technologies", "journal-ref": null, "doi": "10.1007/978-3-319-50995-2_10", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the implementation and performance evaluation of the\nOpenSHMEM 1.3 specification for the Adapteva Epiphany architecture within the\nParallella single-board computer. The Epiphany architecture exhibits massive\nmany-core scalability with a physically compact 2D array of RISC CPU cores and\na fast network-on-chip (NoC). While fully capable of MPMD execution, the\nphysical topology and memory-mapped capabilities of the core and network\ntranslate well to Partitioned Global Address Space (PGAS) programming models\nand SPMD execution with SHMEM.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 17:42:16 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Ross", "James", ""], ["Richie", "David", ""]]}, {"id": "1608.03549", "submitter": "James Ross", "authors": "David Richie and James Ross", "title": "OpenCL + OpenSHMEM Hybrid Programming Model for the Adapteva Epiphany\n  Architecture", "comments": "12 pages, 5 figures, OpenSHMEM 2016: Third workshop on OpenSHMEM and\n  Related Technologies", "journal-ref": null, "doi": "10.1007/978-3-319-50995-2_12", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is interest in exploring hybrid OpenSHMEM + X programming models to\nextend the applicability of the OpenSHMEM interface to more hardware\narchitectures. We present a hybrid OpenCL + OpenSHMEM programming model for\ndevice-level programming for architectures like the Adapteva Epiphany many-core\nRISC array processor. The Epiphany architecture comprises a 2D array of\nlow-power RISC cores with minimal uncore functionality connected by a 2D mesh\nNetwork-on-Chip (NoC). The Epiphany architecture offers high computational\nenergy efficiency for integer and floating point calculations as well as\nparallel scalability. The Epiphany-III is available as a coprocessor in\nplatforms that also utilize an ARM CPU host. OpenCL provides good functionality\nfor supporting a co-design programming model in which the host CPU offloads\nparallel work to a coprocessor. However, the OpenCL memory model is\ninconsistent with the Epiphany memory architecture and lacks support for\ninter-core communication. We propose a hybrid programming model in which\nOpenSHMEM provides a better solution by replacing the non-standard OpenCL\nextensions introduced to achieve high performance with the Epiphany\narchitecture. We demonstrate the proposed programming model for matrix-matrix\nmultiplication based on Cannon's algorithm showing that the hybrid model\naddresses the deficiencies of using OpenCL alone to achieve good benchmark\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 17:50:49 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Richie", "David", ""], ["Ross", "James", ""]]}, {"id": "1608.03630", "submitter": "Andreas Mang", "authors": "Andreas Mang and Amir Gholami and George Biros", "title": "Distributed-memory large deformation diffeomorphic 3D image registration", "comments": "accepted for publication at SC16 in Salt Lake City, Utah, USA;\n  November 2016", "journal-ref": "Proceedings of the International Conference for High Performance\n  Computing, Networking, Storage and Analysis, Article No. 72, 2016", "doi": "10.1109/SC.2016.71", "report-no": null, "categories": "cs.DC cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel distributed-memory algorithm for large deformation\ndiffeomorphic registration of volumetric images that produces large isochoric\ndeformations (locally volume preserving). Image registration is a key\ntechnology in medical image analysis. Our algorithm uses a partial differential\nequation constrained optimal control formulation. Finding the optimal\ndeformation map requires the solution of a highly nonlinear problem that\ninvolves pseudo-differential operators, biharmonic operators, and pure\nadvection operators both forward and back- ward in time. A key issue is the\ntime to solution, which poses the demand for efficient optimization methods as\nwell as an effective utilization of high performance computing resources. To\naddress this problem we use a preconditioned, inexact, Gauss-Newton- Krylov\nsolver. Our algorithm integrates several components: a spectral discretization\nin space, a semi-Lagrangian formulation in time, analytic adjoints, different\nregularization functionals (including volume-preserving ones), a spectral\npreconditioner, a highly optimized distributed Fast Fourier Transform, and a\ncubic interpolation scheme for the semi-Lagrangian time-stepping. We\ndemonstrate the scalability of our algorithm on images with resolution of up to\n$1024^3$ on the \"Maverick\" and \"Stampede\" systems at the Texas Advanced\nComputing Center (TACC). The critical problem in the medical imaging\napplication domain is strong scaling, that is, solving registration problems of\na moderate size of $256^3$---a typical resolution for medical images. We are\nable to solve the registration problem for images of this size in less than\nfive seconds on 64 x86 nodes of TACC's \"Maverick\" system.\n", "versions": [{"version": "v1", "created": "Thu, 11 Aug 2016 22:52:27 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Mang", "Andreas", ""], ["Gholami", "Amir", ""], ["Biros", "George", ""]]}, {"id": "1608.03866", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Distributed Optimization for Client-Server Architecture with Negative\n  Gradient Weights", "comments": "[Submitted 12 Aug., 2016. Revised 18 Dec.,2016.] Added Section 3.1,\n  added additional discussion to Section 5, added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of both massive datasets and computing resources have made\nmachine learning and predictive analytics extremely pervasive. In this work we\npresent a synchronous algorithm and architecture for distributed optimization\nmotivated by privacy requirements posed by applications in machine learning. We\npresent an algorithm for the recently proposed multi-parameter-server\narchitecture. We consider a group of parameter servers that learn a model based\non randomized gradients received from clients. Clients are computational\nentities with private datasets (inducing a private objective function), that\nevaluate and upload randomized gradients to the parameter servers. The\nparameter servers perform model updates based on received gradients and share\nthe model parameters with other servers. We prove that the proposed algorithm\ncan optimize the overall objective function for a very general architecture\ninvolving $C$ clients connected to $S$ parameter servers in an arbitrary time\nvarying topology and the parameter servers forming a connected network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Aug 2016 18:34:06 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 15:19:25 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1608.03960", "submitter": "Martin Kleppmann", "authors": "Martin Kleppmann, Alastair R. Beresford", "title": "A Conflict-Free Replicated JSON Datatype", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2017.2697382", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications model their data in a general-purpose storage format such\nas JSON. This data structure is modified by the application as a result of user\ninput. Such modifications are well understood if performed sequentially on a\nsingle copy of the data, but if the data is replicated and modified\nconcurrently on multiple devices, it is unclear what the semantics should be.\nIn this paper we present an algorithm and formal semantics for a JSON data\nstructure that automatically resolves concurrent modifications such that no\nupdates are lost, and such that all replicas converge towards the same state (a\nconflict-free replicated datatype or CRDT). It supports arbitrarily nested list\nand map types, which can be modified by insertion, deletion and assignment. The\nalgorithm performs all merging client-side and does not depend on ordering\nguarantees from the network, making it suitable for deployment on mobile\ndevices with poor network connectivity, in peer-to-peer networks, and in\nmessaging systems with end-to-end encryption.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 09:48:35 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 10:32:25 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 15:00:24 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Kleppmann", "Martin", ""], ["Beresford", "Alastair R.", ""]]}, {"id": "1608.04030", "submitter": "Tao Chen", "authors": "Tao Chen", "title": "Self-Aware and Self-Adaptive Autoscaling for Cloud Based Services", "comments": "PhD thesis, University of Birmingham", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Internet services are increasingly leveraging on cloud computing for\nflexible, elastic and on-demand provision. Typically, Quality of Service (QoS)\nof cloud-based services can be tuned using different underlying cloud\nconfigurations and resources, e.g., number of threads, CPU and memory etc.,\nwhich are shared, leased and priced as utilities. This benefit is fundamentally\ngrounded by autoscaling: an automatic and elastic process that adapts cloud\nconfigurations on-demand according to time-varying workloads. This thesis\nproposes a holistic cloud autoscaling framework to effectively and seamlessly\naddress existing challenges related to different logical aspects of\nautoscaling, including architecting autoscaling system, modelling the QoS of\ncloud-based service, determining the granularity of control and deciding\ntrade-off autoscaling decisions. The framework takes advantages of the\nprinciples of self-awareness and the related algorithms to adaptively handle\nthe dynamics, uncertainties, QoS interference and trade-offs on objectives that\nare exhibited in the cloud. The major benefit is that, by leveraging the\nframework, cloud autoscaling can be effectively achieved without heavy human\nanalysis and design time knowledge. Through conducting various experiments\nusing RUBiS benchmark and realistic workload on real cloud setting, this thesis\nevaluates the effectiveness of the framework based on various quality\nindicators and compared with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 13 Aug 2016 21:46:40 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Chen", "Tao", ""]]}, {"id": "1608.04174", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Gianluca De Marco and Muhammed Talo", "title": "Naming a Channel with Beeps", "comments": null, "journal-ref": "Fundamenta Informatica, 153 (3) : 199 - 219, 2017", "doi": "10.3233/FI-2017-1537", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a communication channel in which the only possible communication\nmode is transmitting beeps, which reach all the nodes instantaneously. Nodes\nare anonymous, in that they do not have any individual identifiers. The\nalgorithmic goal is to randomly assign names to the nodes in such a manner that\nthe names make a contiguous segment of positive integers starting from $1$. We\ngive a Las Vegas naming algorithm for the case when the number of nodes $n$ is\nknown, and a Monte Carlo algorithm for the case when the number of nodes $n$ is\nnot known. The algorithms are provably optimal with respect to the expected\ntime $O(n\\log n)$, the number of used random bits $O(n\\log n)$, and the\nprobability of error.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 03:17:37 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 17:11:10 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["De Marco", "Gianluca", ""], ["Talo", "Muhammed", ""]]}, {"id": "1608.04329", "submitter": "Alexander Terenin", "authors": "Alexander Terenin, Shawfeng Dong, David Draper", "title": "GPU-accelerated Gibbs sampling: a case study of the Horseshoe Probit\n  model", "comments": null, "journal-ref": "Statistics and Computing 29(2):301-310, 2019", "doi": "10.1007/s11222-018-9809-3", "report-no": null, "categories": "stat.CO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gibbs sampling is a widely used Markov chain Monte Carlo (MCMC) method for\nnumerically approximating integrals of interest in Bayesian statistics and\nother mathematical sciences. Many implementations of MCMC methods do not extend\neasily to parallel computing environments, as their inherently sequential\nnature incurs a large synchronization cost. In the case study illustrated by\nthis paper, we show how to do Gibbs sampling in a fully data-parallel manner on\na graphics processing unit, for a large class of exchangeable models that admit\nlatent variable representations. Our approach takes a systems perspective, with\nemphasis placed on efficient use of compute hardware. We demonstrate our method\non a Horseshoe Probit regression model and find that our implementation scales\neffectively to thousands of predictors and millions of data points\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:02:41 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 21:02:46 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 07:04:13 GMT"}, {"version": "v4", "created": "Sat, 29 Jul 2017 16:47:51 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 23:48:33 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Terenin", "Alexander", ""], ["Dong", "Shawfeng", ""], ["Draper", "David", ""]]}, {"id": "1608.04336", "submitter": "Daniel Graziotin", "authors": "Dirk Pfl\\\"uger, Miriam Mehl, Julian Valentin, Florian Lindner, David\n  Pfander, Stefan Wagner, Daniel Graziotin, Yang Wang", "title": "The Scalability-Efficiency/Maintainability-Portability Trade-off in\n  Simulation Software Engineering: Examples and a Preliminary Systematic\n  Literature Review", "comments": "9 pages, 2 figures. Accepted for presentation at the Fourth\n  International Workshop on Software Engineering for High Performance Computing\n  in Computational Science and Engineering (SEHPCCSE 2016)", "journal-ref": "2016 Fourth International Workshop on Software Engineering for\n  High Performance Computing in Computational Science and Engineering\n  (SE-HPCCSE), Salt Lake City, UT, 2016, pp. 26-34", "doi": "10.1109/SE-HPCCSE.2016.008", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale simulations play a central role in science and the industry.\nSeveral challenges occur when building simulation software, because simulations\nrequire complex software developed in a dynamic construction process. That is\nwhy simulation software engineering (SSE) is emerging lately as a research\nfocus. The dichotomous trade-off between scalability and efficiency (SE) on the\none hand and maintainability and portability (MP) on the other hand is one of\nthe core challenges. We report on the SE/MP trade-off in the context of an\nongoing systematic literature review (SLR). After characterizing the issue of\nthe SE/MP trade-off using two examples from our own research, we (1) review the\n33 identified articles that assess the trade-off, (2) summarize the proposed\nsolutions for the trade-off, and (3) discuss the findings for SSE and future\nwork. Overall, we see evidence for the SE/MP trade-off and first solution\napproaches. However, a strong empirical foundation has yet to be established;\ngeneral quantitative metrics and methods supporting software developers in\naddressing the trade-off have to be developed. We foresee considerable future\nwork in SSE across scientific communities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 17:31:46 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 07:26:35 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 07:40:22 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Pfl\u00fcger", "Dirk", ""], ["Mehl", "Miriam", ""], ["Valentin", "Julian", ""], ["Lindner", "Florian", ""], ["Pfander", "David", ""], ["Wagner", "Stefan", ""], ["Graziotin", "Daniel", ""], ["Wang", "Yang", ""]]}, {"id": "1608.04431", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Parallel Non-divergent Flow Accumulation For Trillion Cell Digital\n  Elevation Models On Desktops Or Clusters", "comments": "23 pages (double-spaced), 4 figures, 2 tables. arXiv admin note:\n  substantial text overlap with arXiv:1606.06204", "journal-ref": "2017. Environmental Modelling & Software 92, 202-212", "doi": "10.1016/j.envsoft.2017.02.022", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continent-scale datasets challenge hydrological algorithms for processing\ndigital elevation models. Flow accumulation is an important input for many such\nalgorithms; here, I parallelize its calculation. The new algorithm works on one\nor many cores, or multiple machines, and can take advantage of large memories\nor cope with small ones. Unlike previous parallel algorithms, the new algorithm\nguarantees a fixed number of memory access and communication events per raster\ncell. In testing, the new algorithm ran faster and used fewer resources than\nprevious algorithms, exhibiting ~30% strong and weak scaling efficiencies up to\n48 cores and linear scaling across datasets ranging over three orders of\nmagnitude. The largest dataset tested had two trillion (2*10^12) cells. With 48\ncores, processing required 24 minutes wall-time (14.5 compute-hours). This test\nis three orders of magnitude larger than any previously performed in the\nliterature. Complete, well-commented source code and correctness tests are\navailable on Github.\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2016 22:50:48 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 01:50:55 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1608.04647", "submitter": "Mihai Capot\\u{a}", "authors": "Michael J. Anderson, Mihai Capot\\u{a}, Javier S. Turek, Xia Zhu,\n  Theodore L. Willke, Yida Wang, Po-Hsuan Chen, Jeremy R. Manning, Peter J.\n  Ramadge, Kenneth A. Norman", "title": "Enabling Factor Analysis on Thousand-Subject Neuroimaging Datasets", "comments": null, "journal-ref": null, "doi": "10.1109/BigData.2016.7840719", "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scale of functional magnetic resonance image data is rapidly increasing\nas large multi-subject datasets are becoming widely available and\nhigh-resolution scanners are adopted. The inherent low-dimensionality of the\ninformation in this data has led neuroscientists to consider factor analysis\nmethods to extract and analyze the underlying brain activity. In this work, we\nconsider two recent multi-subject factor analysis methods: the Shared Response\nModel and Hierarchical Topographic Factor Analysis. We perform analytical,\nalgorithmic, and code optimization to enable multi-node parallel\nimplementations to scale. Single-node improvements result in 99x and 1812x\nspeedups on these two methods, and enables the processing of larger datasets.\nOur distributed implementations show strong scaling of 3.3x and 5.5x\nrespectively with 20 nodes on real datasets. We also demonstrate weak scaling\non a synthetic dataset with 1024 subjects, on up to 1024 nodes and 32,768\ncores.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2016 16:05:14 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 02:30:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Anderson", "Michael J.", ""], ["Capot\u0103", "Mihai", ""], ["Turek", "Javier S.", ""], ["Zhu", "Xia", ""], ["Willke", "Theodore L.", ""], ["Wang", "Yida", ""], ["Chen", "Po-Hsuan", ""], ["Manning", "Jeremy R.", ""], ["Ramadge", "Peter J.", ""], ["Norman", "Kenneth A.", ""]]}, {"id": "1608.04841", "submitter": "Isaac Sheff", "authors": "Isaac Sheff, Tom Magrino, Jed Liu, Andrew C. Myers, Robbert van\n  Renesse", "title": "Safe Serializable Secure Scheduling: Transactions and the Trade-off\n  Between Security and Consistency", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications often operate on data in multiple administrative domains.\nIn this federated setting, participants may not fully trust each other. These\ndistributed applications use transactions as a core mechanism for ensuring\nreliability and consistency with persistent data. However, the coordination\nmechanisms needed for transactions can both leak confidential information and\nallow unauthorized influence.\n  By implementing a simple attack, we show these side channels can be\nexploited. However, our focus is on preventing such attacks. We explore secure\nscheduling of atomic, serializable transactions in a federated setting. While\nwe prove that no protocol can guarantee security and liveness in all settings,\nwe establish conditions for sets of transactions that can safely complete under\nsecure scheduling. Based on these conditions, we introduce staged commit, a\nsecure scheduling protocol for federated transactions. This protocol avoids\ninsecure information channels by dividing transactions into distinct stages. We\nimplement a compiler that statically checks code to ensure it meets our\nconditions, and a system that schedules these transactions using the staged\ncommit protocol. Experiments on this implementation demonstrate that realistic\nfederated transactions can be scheduled securely, atomically, and efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 03:11:07 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2016 20:35:08 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Sheff", "Isaac", ""], ["Magrino", "Tom", ""], ["Liu", "Jed", ""], ["Myers", "Andrew C.", ""], ["van Renesse", "Robbert", ""]]}, {"id": "1608.04967", "submitter": "Manuel Mazzara", "authors": "Lukas Breitwieser, Roman Bauer, Alberto Di Meglio, Leonard Johard,\n  Marcus Kaiser, Marco Manca, Manuel Mazzara, Fons Rademakers, Max Talanov", "title": "The BioDynaMo Project: Creating a Platform for Large-Scale Reproducible\n  Biological Simulations", "comments": "4th Workshop on Sustainable Software for Science: Practice and\n  Experiences (WSSSPE4), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations have become a very powerful tool for scientific\nresearch. In order to facilitate research in computational biology, the\nBioDynaMo project aims at a general platform for biological computer\nsimulations, which should be executable on hybrid cloud computing systems. This\npaper describes challenges and lessons learnt during the early stages of the\nsoftware development process, in the context of implementation issues and the\ninternational nature of the collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 17 Aug 2016 13:55:43 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Breitwieser", "Lukas", ""], ["Bauer", "Roman", ""], ["Di Meglio", "Alberto", ""], ["Johard", "Leonard", ""], ["Kaiser", "Marcus", ""], ["Manca", "Marco", ""], ["Mazzara", "Manuel", ""], ["Rademakers", "Fons", ""], ["Talanov", "Max", ""]]}, {"id": "1608.05138", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi and Rong Zhou", "title": "Hybrid CPU-GPU Framework for Network Motifs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel architectures such as the GPU are becoming increasingly\nimportant due to the recent proliferation of data. In this paper, we propose a\nkey class of hybrid parallel graphlet algorithms that leverages multiple CPUs\nand GPUs simultaneously for computing k-vertex induced subgraph statistics\n(called graphlets). In addition to the hybrid multi-core CPU-GPU framework, we\nalso investigate single GPU methods (using multiple cores) and multi-GPU\nmethods that leverage all available GPUs simultaneously for computing induced\nsubgraph statistics. Both methods leverage GPU devices only, whereas the hybrid\nmulti-core CPU-GPU framework leverages all available multi-core CPUs and\nmultiple GPUs for computing graphlets in large networks. Compared to recent\napproaches, our methods are orders of magnitude faster, while also more cost\neffective enjoying superior performance per capita and per watt. In particular,\nthe methods are up to 300 times faster than the recent state-of-the-art method.\nTo the best of our knowledge, this is the first work to leverage multiple CPUs\nand GPUs simultaneously for computing induced subgraph statistics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 00:29:21 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 01:55:07 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""]]}, {"id": "1608.05174", "submitter": "Cory James Kleinheksel", "authors": "Cory J. Kleinheksel and Arun K. Somani", "title": "Scaling Distributed All-Pairs Algorithms: Manage Computation and Limit\n  Data Replication with Quorums", "comments": "Chapter Information Science and Applications (ICISA) 2016 Volume 376\n  of the series Lecture Notes in Electrical Engineering pp 247-257 Date: 16\n  February 2016", "journal-ref": "Kleinheksel, Cory J., and Arun K. Somani. \"Scaling Distributed\n  All-Pairs Algorithms.\" Information Science and Applications (ICISA) 2016.\n  Springer Singapore, 2016. 247-257", "doi": "10.1007/978-981-10-0557-2_25", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose and prove that cyclic quorum sets can efficiently\nmanage all-pairs computations and data replication. The quorums are\nO(N/sqrt(P)) in size, up to 50% smaller than the dual N/sqrt(P) array\nimplementations, and significantly smaller than solutions requiring all data.\nImplementation evaluation demonstrated scalability on real datasets with a 7x\nspeed up on 8 nodes with 1/3rd the memory usage per process. The all-pairs\nproblem requires all data elements to be paired with all other data elements.\nThese all-pair problems occur in many science fields, which has led to their\ncontinued interest. Additionally, as datasets grow in size, new methods like\nthese that can reduce memory footprints and distribute work equally across\ncompute nodes will be demanded.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 04:51:38 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Kleinheksel", "Cory J.", ""], ["Somani", "Arun K.", ""]]}, {"id": "1608.05265", "submitter": "Miguel Tasende", "authors": "Miguel Tasende", "title": "Generation of the Single Precision BLAS library for the Parallella\n  platform, with Epiphany co-processor acceleration, using the BLIS framework", "comments": "8 pages, 9 figures, conference manuscript for IEEE DataCom 2016\n  (Auckland, New Zealand)", "journal-ref": null, "doi": "10.1109/DASC-PICom-DataCom-CyberSciTec.2016.154", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Parallella is a hybrid computing platform that came into existence as the\nresult of a Kickstarter project by Adapteva. It is composed of the high\nperformance, energy-efficient, manycore architecture, Epiphany chip (used as\nco-processor) and one Zynq-7000 series chip, which normally runs a regular\nLinux OS version, serves as the main processor, and implements \"glue logic\" in\nits internal FPGA to communicate with the many interfaces in the Parallella. In\nthis paper an Epiphany-accelerated BLAS library for the Parallella platform was\ncreated (which could be suitable, also, for similar hybrid platforms that\ninclude the Epiphany chip as a coprocessor). For the actual instantiation of\nthe BLAS, the BLIS framework was used. There have been previous implementations\nof Matrix-Matrix multiplication, on this platform, that achieved very good\nperformances inside the Epiphany chip (up to 85% of peak), but not so good ones\nfor the complete Parallella platform (due to inter-chip data transfer bandwidth\nlimitations). The main purpose of this work was to get closer to practical\nLinear Algebra aplications for the entire Parallella platform, with scientific\ncomputing in view.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 14:01:48 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Tasende", "Miguel", ""]]}, {"id": "1608.05288", "submitter": "Ferdinando Fioretto", "authors": "Ferdinando Fioretto, Enrico Pontelli, William Yeoh, Rina Dechter", "title": "Accelerating Exact and Approximate Inference for (Distributed) Discrete\n  Optimization with GPUs", "comments": null, "journal-ref": "Constraints (2018) 23: 1", "doi": "10.1007/s10601-017-9274-1", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete optimization is a central problem in artificial intelligence. The\noptimization of the aggregated cost of a network of cost functions arises in a\nvariety of problems including (W)CSP, DCOP, as well as optimization in\nstochastic variants such as the tasks of finding the most probable explanation\n(MPE) in belief networks. Inference-based algorithms are powerful techniques\nfor solving discrete optimization problems, which can be used independently or\nin combination with other techniques. However, their applicability is often\nlimited by their compute intensive nature and their space requirements. This\npaper proposes the design and implementation of a novel inference-based\ntechnique, which exploits modern massively parallel architectures, such as\nthose found in Graphical Processing Units (GPUs), to speed up the resolution of\nexact and approximated inference-based algorithms for discrete optimization.\nThe paper studies the proposed algorithm in both centralized and distributed\noptimization contexts. The paper demonstrates that the use of GPUs provides\nsignificant advantages in terms of runtime and scalability, achieving up to two\norders of magnitude in speedups and showing a considerable reduction in\nexecution time (up to 345 times faster) with respect to a sequential version.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 15:14:37 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 02:14:06 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Pontelli", "Enrico", ""], ["Yeoh", "William", ""], ["Dechter", "Rina", ""]]}, {"id": "1608.05315", "submitter": "Hamid Reza Hassanzadeh", "authors": "Reihaneh Hassanzadeh and Ali Movaghar and Hamid Reza Hassanzadeh", "title": "A Multi-Dimensional Fairness Combinatorial Double-Sided Auction Model in\n  Cloud Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud investment markets, consumers are looking for the lowest cost and a\ndesirable fairness while providers are looking for strategies to achieve the\nhighest possible profit and return. Most existing models for auction-based\nresource allocation in cloud environments only consider the overall profit\nincrease and ignore the profit of each participant individually or the\ndifference between the rich and the poor participants. This paper proposes a\nmulti-dimensional fairness combinatorial double auction (MDFCDA) model which\nstrikes a balance between the revenue and the fairness among participants. We\nsolve a winner determination problem (WDP) through integer programming which\nincorporates the fairness attribute based on the history of participants which\nis stored in a repository. Our evaluation results show that the proposed model\nincreases the willingness of participants to take part in the next auction\nrounds. Moreover, the average percentage of resource utilization is increased.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 16:11:07 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 15:01:24 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Hassanzadeh", "Reihaneh", ""], ["Movaghar", "Ali", ""], ["Hassanzadeh", "Hamid Reza", ""]]}, {"id": "1608.05327", "submitter": "Igor Konnov", "authors": "Igor Konnov, Marijana Lazic, Helmut Veith and Josef Widder", "title": "A Short Counterexample Property for Safety and Liveness Verification of\n  Fault-tolerant Distributed Algorithms", "comments": "16 pages, 11 pages appendix", "journal-ref": null, "doi": "10.1145/3009837.3009860", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed algorithms have many mission-critical applications ranging from\nembedded systems and replicated databases to cloud computing. Due to\nasynchronous communication, process faults, or network failures, these\nalgorithms are difficult to design and verify. Many algorithms achieve fault\ntolerance by using threshold guards that, for instance, ensure that a process\nwaits until it has received an acknowledgment from a majority of its peers.\nConsequently, domain-specific languages for fault-tolerant distributed systems\noffer language support for threshold guards.\n  We introduce an automated method for model checking of safety and liveness of\nthreshold-guarded distributed algorithms in systems where the number of\nprocesses and the fraction of faulty processes are parameters. Our method is\nbased on a short counterexample property: if a distributed algorithm violates a\ntemporal specification (in a fragment of LTL), then there is a counterexample\nwhose length is bounded and independent of the parameters. We prove this\nproperty by (i) characterizing executions depending on the structure of the\ntemporal formula, and (ii) using commutativity of transitions to accelerate and\nshorten executions. We extended the ByMC toolset (Byzantine Model Checker) with\nour technique, and verified liveness and safety of 10 prominent fault-tolerant\ndistributed algorithms, most of which were out of reach for existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 16:43:03 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 10:37:16 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Konnov", "Igor", ""], ["Lazic", "Marijana", ""], ["Veith", "Helmut", ""], ["Widder", "Josef", ""]]}, {"id": "1608.05401", "submitter": "Shripad Gade", "authors": "Shripad Gade and Nitin H. Vaidya", "title": "Distributed Optimization of Convex Sum of Non-Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed solution to optimizing a convex function composed of\nseveral non-convex functions. Each non-convex function is privately stored with\nan agent while the agents communicate with neighbors to form a network. We show\nthat coupled consensus and projected gradient descent algorithm proposed in [1]\ncan optimize convex sum of non-convex functions under an additional assumption\non gradient Lipschitzness. We further discuss the applications of this analysis\nin improving privacy in distributed optimization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 19:57:41 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Gade", "Shripad", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1608.05634", "submitter": "Timo Bingmann", "authors": "Timo Bingmann, Michael Axtmann, Emanuel J\\\"obstl, Sebastian Lamm,\n  Huyen Chau Nguyen, Alexander Noe, Sebastian Schlag, Matthias Stumpp, Tobias\n  Sturm, and Peter Sanders", "title": "Thrill: High-Performance Algorithmic Distributed Batch Data Processing\n  with C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and a first performance evaluation of Thrill -- a\nprototype of a general purpose big data processing framework with a convenient\ndata-flow style programming interface. Thrill is somewhat similar to Apache\nSpark and Apache Flink with at least two main differences. First, Thrill is\nbased on C++ which enables performance advantages due to direct native code\ncompilation, a more cache-friendly memory layout, and explicit memory\nmanagement. In particular, Thrill uses template meta-programming to compile\nchains of subsequent local operations into a single binary routine without\nintermediate buffering and with minimal indirections. Second, Thrill uses\narrays rather than multisets as its primary data structure which enables\nadditional operations like sorting, prefix sums, window scans, or combining\ncorresponding fields of several arrays (zipping). We compare Thrill with Apache\nSpark and Apache Flink using five kernels from the HiBench suite. Thrill is\nconsistently faster and often several times faster than the other frameworks.\nAt the same time, the source codes have a similar level of simplicity and\nabstraction\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 15:13:31 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Bingmann", "Timo", ""], ["Axtmann", "Michael", ""], ["J\u00f6bstl", "Emanuel", ""], ["Lamm", "Sebastian", ""], ["Nguyen", "Huyen Chau", ""], ["Noe", "Alexander", ""], ["Schlag", "Sebastian", ""], ["Stumpp", "Matthias", ""], ["Sturm", "Tobias", ""], ["Sanders", "Peter", ""]]}, {"id": "1608.05743", "submitter": "Songze Li", "authors": "Songze Li, Qian Yu, Mohammad Ali Maddah-Ali, A. Salman Avestimehr", "title": "A Scalable Framework for Wireless Distributed Computing", "comments": "To appear in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a wireless distributed computing system, in which multiple mobile\nusers, connected wirelessly through an access point, collaborate to perform a\ncomputation task. In particular, users communicate with each other via the\naccess point to exchange their locally computed intermediate computation\nresults, which is known as data shuffling. We propose a scalable framework for\nthis system, in which the required communication bandwidth for data shuffling\ndoes not increase with the number of users in the network. The key idea is to\nutilize a particular repetitive pattern of placing the dataset (thus a\nparticular repetitive pattern of intermediate computations), in order to\nprovide coding opportunities at both the users and the access point, which\nreduce the required uplink communication bandwidth from users to access point\nand the downlink communication bandwidth from access point to users by factors\nthat grow linearly with the number of users. We also demonstrate that the\nproposed dataset placement and coded shuffling schemes are optimal (i.e.,\nachieve the minimum required shuffling load) for both a centralized setting and\na decentralized setting, by developing tight information-theoretic lower\nbounds.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2016 21:48:19 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 22:30:16 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Li", "Songze", ""], ["Yu", "Qian", ""], ["Maddah-Ali", "Mohammad Ali", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "1608.05766", "submitter": "Jinshan Zeng", "authors": "Jinshan Zeng and Wotao Yin", "title": "On Nonconvex Decentralized Gradient Descent", "comments": "This version is an extended one of the previous version. In the\n  current version, we study the conververgence of DGD and Prox-DGD using both\n  constant and diminishing step sizes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus optimization has received considerable attention in recent years. A\nnumber of decentralized algorithms have been proposed for {convex} consensus\noptimization. However, to the behaviors or consensus \\emph{nonconvex}\noptimization, our understanding is more limited.\n  When we lose convexity, we cannot hope our algorithms always return global\nsolutions though they sometimes still do sometimes. Somewhat surprisingly, the\ndecentralized consensus algorithms, DGD and Prox-DGD, retain most other\nproperties that are known in the convex setting. In particular, when\ndiminishing (or constant) step sizes are used, we can prove convergence to a\n(or a neighborhood of) consensus stationary solution under some regular\nassumptions. It is worth noting that Prox-DGD can handle nonconvex nonsmooth\nfunctions if their proximal operators can be computed. Such functions include\nSCAD and $\\ell_q$ quasi-norms, $q\\in[0,1)$. Similarly, Prox-DGD can take the\nconstraint to a nonconvex set with an easy projection.\n  To establish these properties, we have to introduce a completely different\nline of analysis, as well as modify existing proofs that were used the convex\nsetting.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 01:12:44 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 12:25:20 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 09:07:47 GMT"}, {"version": "v4", "created": "Fri, 26 Jan 2018 02:51:30 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zeng", "Jinshan", ""], ["Yin", "Wotao", ""]]}, {"id": "1608.05794", "submitter": "Kyle Niemeyer", "authors": "Christopher P. Stone and Andrew T. Alferman and Kyle E. Niemeyer", "title": "Accelerating finite-rate chemical kinetics with coprocessors: comparing\n  vectorization methods on GPUs, MICs, and CPUs", "comments": "32 pages, 11 figures", "journal-ref": "Comput. Phys. Comm. 226 (2018) 18-29", "doi": "10.1016/j.cpc.2018.01.015", "report-no": null, "categories": "physics.comp-ph cs.DC physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient ordinary differential equation solvers for chemical kinetics must\ntake into account the available thread and instruction-level parallelism of the\nunderlying hardware, especially on many-core coprocessors, as well as the\nnumerical efficiency. A stiff Rosenbrock and nonstiff Runge-Kutta solver are\nimplemented using the single instruction, multiple thread (SIMT) and single\ninstruction, multiple data (SIMD) paradigms with OpenCL. The performances of\nthese parallel implementations were measured with three chemical kinetic models\nacross several multicore and many-core platforms. Two runtime benchmarks were\nconducted to clearly determine any performance advantage offered by either\nmethod: evaluating the right-hand-side source terms in parallel, and\nintegrating a series of constant-pressure homogeneous reactors using the\nRosenbrock and Runge-Kutta solvers. The right-hand-side evaluations with SIMD\nparallelism on the host multicore Xeon CPU and many-core Xeon Phi co-processor\nperformed approximately three times faster than the baseline multithreaded\ncode. The SIMT model on the host and Phi was 13-35% slower than the baseline\nwhile the SIMT model on the GPU provided approximately the same performance as\nthe SIMD model on the Phi. The runtimes for both ODE solvers decreased 2.5-2.7x\nwith the SIMD implementations on the host CPU and 4.7-4.9x with the Xeon Phi\ncoprocessor compared to the baseline parallel code. The SIMT implementations on\nthe GPU ran 1.4-1.6 times faster than the baseline multithreaded CPU code;\nhowever, this was significantly slower than the SIMD versions on the host CPU\nor the Xeon Phi. The performance difference between the three platforms was\nattributed to thread divergence caused by the adaptive step-sizes within the\nODE integrators. Analysis showed that the wider vector width of the GPU incurs\na higher level of divergence than the narrower Sandy Bridge or Xeon Phi.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 07:22:22 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 00:27:55 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Stone", "Christopher P.", ""], ["Alferman", "Andrew T.", ""], ["Niemeyer", "Kyle E.", ""]]}, {"id": "1608.05839", "submitter": "Michael McGarry", "authors": "Salvador Melendez and Michael P. McGarry", "title": "Computation Offloading Decisions for Reducing Completion Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the conditions in which offloading computation reduces completion\ntime. We extend the existing literature by deriving an inequality (Eq. 4) that\nrelates computation offloading system parameters to the bits per instruction\nratio of a computational job. This ratio is the inverse of the arithmetic\nintensity. We then discuss how this inequality can be used to determine the\ncomputations that can benefit from offloading as well as the computation\noffloading systems required to make offloading beneficial for particular\ncomputations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 15:20:26 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Melendez", "Salvador", ""], ["McGarry", "Michael P.", ""]]}, {"id": "1608.05844", "submitter": "Christophe Guyeux", "authors": "Jacques Bahi and Wiem Elghazel and Christophe Guyeux and Mohammed\n  Haddad and Mourad Hakem and Kamal Medjaher and Nourredine Zerhouni", "title": "Resiliency in Distributed Sensor Networks for PHM of the Monitoring\n  Targets", "comments": "The Computer Journal (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In condition-based maintenance, real-time observations are crucial for\non-line health assessment. When the monitoring system is a wireless sensor\nnetwork, data loss becomes highly probable and this affects the quality of the\nremaining useful life prediction. In this paper, we present a fully distributed\nalgorithm that ensures fault tolerance and recovers data loss in wireless\nsensor networks. We first theoretically analyze the algorithm and give\ncorrectness proofs, then provide simulation results and show that the algorithm\nis (i) able to ensure data recovery with a low failure rate and (ii) preserves\nthe overall energy for dense networks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 15:51:05 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bahi", "Jacques", ""], ["Elghazel", "Wiem", ""], ["Guyeux", "Christophe", ""], ["Haddad", "Mohammed", ""], ["Hakem", "Mourad", ""], ["Medjaher", "Kamal", ""], ["Zerhouni", "Nourredine", ""]]}, {"id": "1608.05866", "submitter": "Marius Poke", "authors": "Marius Poke, Torsten Hoefler, Colin W. Glass", "title": "AllConcur: Leaderless Concurrent Atomic Broadcast (Extended Version)", "comments": "Overview: 18 pages, 7 sections, 10 figures, 3 tables. Modifications\n  from previous version: added Figure 4; added, in Section 4.4, a paragraph\n  describing the construction of Gs digraphs; added Section 4.5, a theoretical\n  comparison between AllConcur and leader-based agreement; added, in Section 5,\n  a comparison to unreliable agreement; rephrased several paragraphs to improve\n  clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed systems require coordination between the components\ninvolved. With the steady growth of such systems, the probability of failures\nincreases, which necessitates scalable fault-tolerant agreement protocols. The\nmost common practical agreement protocol, for such scenarios, is leader-based\natomic broadcast. In this work, we propose AllConcur, a distributed system that\nprovides agreement through a leaderless concurrent atomic broadcast algorithm,\nthus, not suffering from the bottleneck of a central coordinator. In AllConcur,\nall components exchange messages concurrently through a logical overlay network\nthat employs early termination to minimize the agreement latency. Our\nimplementation of AllConcur supports standard sockets-based TCP as well as\nhigh-performance InfiniBand Verbs communications. AllConcur can handle up to\n135 million requests per second and achieves 17x higher throughput than today's\nstandard leader-based protocols, such as Libpaxos. Thus, AllConcur is highly\ncompetitive with regard to existing solutions and, due to its decentralized\napproach, enables hitherto unattainable system designs in a variety of fields.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 19:53:45 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 13:41:39 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Poke", "Marius", ""], ["Hoefler", "Torsten", ""], ["Glass", "Colin W.", ""]]}, {"id": "1608.05874", "submitter": "Giulio Masetti", "authors": "Silvano Chiaradonna, Felicita Di Giandomenico and Giulio Masetti", "title": "Efficient non-anonymous composition operator for modeling complex\n  dependable systems", "comments": "Editor: Gilles Tredan. 12th European Dependable Computing Conference\n  (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden. Fast Abstracts\n  Proceedings- EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model composer is proposed to automatically generate non-anonymous\nmodel replicas in the context of performability and dependability evaluation.\nIt is a state-sharing composer that extends the standard anonymous replication\ncomposer in order to share the state of a replica among a set of other specific\nreplicas or among the eplica and another external model. This new composition\noperator aims to improve expressiveness and performance with respect to the\nstandard anonymous replicator, namely the one adopted by the M{\\\"o}bius\nmodeling framework.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2016 22:14:52 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 11:29:05 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Chiaradonna", "Silvano", ""], ["Di Giandomenico", "Felicita", ""], ["Masetti", "Giulio", ""]]}, {"id": "1608.05917", "submitter": "Tao Chen", "authors": "Tao Chen, Rami Bahsoon", "title": "Self-Adaptive Trade-off Decision Making for Autoscaling Cloud-Based\n  Services", "comments": "published in IEEE Transactions on Services Computing 2015", "journal-ref": null, "doi": "10.1109/TSC.2015.2499770", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elasticity in the cloud is often achieved by on-demand autoscaling. In such\ncontext, the goal is to optimize the Quality of Service (QoS) and cost\nobjectives for the cloud-based services. However, the difficulty lies in the\nfacts that these objectives, e.g., throughput and cost, can be naturally\nconflicted, and the QoS of cloud-based services often interfere due to the\nshared infrastructure in cloud. Consequently, dynamic and effective trade-off\ndecision making of autoscaling in the cloud is necessary, yet challenging. In\nparticular, it is even harder to achieve well-compromised trade-offs, where the\ndecision largely improves the majority of the objectives, while causing\nrelatively small degradations to others. In this paper, we present a\nself-adaptive decision making approach for autoscaling in the cloud. It is\ncapable to adaptively produce autoscaling decisions that lead to\nwell-compromised trade-offs without heavy human intervention. We leverage on\nant colony inspired multi-objective optimization for searching and optimizing\nthe trade-offs decisions, the result is then filtered by compromise-dominance,\na mechanism that extracts the decisions with balanced improvements in the\ntrade-offs. We experimentally compare our approach to four state-of-the-arts\nautoscaling approaches: rule, heuristic, randomized and multi-objective genetic\nalgorithm based solutions. The results reveal the effectiveness of our approach\nover the others, including better quality of trade-offs and significantly\nsmaller violation of the requirements.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 11:21:02 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chen", "Tao", ""], ["Bahsoon", "Rami", ""]]}, {"id": "1608.05936", "submitter": "Christophe Guyeux", "authors": "Jacques M. Bahi and Christophe Guyeux and Abdallah Makhoul", "title": "Two Security Layers for Hierarchical Data Aggregation in Sensor Networks", "comments": null, "journal-ref": "International Journal of Autonomous and Adaptive Communications\n  Systems. 7(3), 239-270, 2014", "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to resource restricted sensor nodes, it is important to minimize the\namount of data transmission among sensor networks. To reduce the amount of\nsending data, an aggregation approach can be applied along the path from\nsensors to the sink. However, as sensor networks are often deployed in\nuntrusted and even hostile environments, sensor nodes are prone to node\ncompromise attacks. Hence, an end-to-end secure aggregation approach is\nrequired to ensure a healthy data reception. In this paper, we propose two\nlayers for secure data aggregation in sensor networks. Firstly, we provide an\nend-to-end encryption scheme that supports operations over cypher-text. It is\nbased on elliptic curve cryptography that exploits a smaller key size, allows\nthe use of higher number of operations on cypher-texts, and prevents the\ndistinction between two identical texts from their cryptograms. Secondly, we\npropose a new watermarking-based authentication that enables sensor nodes to\nensure the identity of other nodes they are communicating with. Our experiments\nshow that our hybrid approach of secure data aggregation enhances the security,\nsignificantly reduces computation and communication overhead, and can be\npractically implemented in on-the-shelf sensor platforms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 13:11:56 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bahi", "Jacques M.", ""], ["Guyeux", "Christophe", ""], ["Makhoul", "Abdallah", ""]]}, {"id": "1608.05951", "submitter": "Christophe Guyeux", "authors": "Jacques M. Bahi, Christophe Guyeux, Mourad Hakem, Abdallah Makhoul", "title": "Epidemiological Approach for Data Survivability in Unattended Wireless\n  Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unattended Wireless Sensor Networks (UWSNs) are Wireless Sensor Networks\ncharacterized by sporadic sink presence and operation in hostile settings. The\nabsence of the sink for period of time, prevents sensor nodes to offload data\nin real time and offer greatly increased opportunities for attacks resulting in\nerasure, modification, or disclosure of sensor-collected data. In this paper,\nwe focus on UWSNs where sensor nodes collect and store data locally and try to\nupload all the information once the sink becomes available. One of the most\nrelevant issues pertaining UWSNs is to guarantee a certain level of information\nsurvivability in an unreliable network and even in presence of a powerful\nattackers. In this paper, we first introduce an epidemic-domain inspired\napproach to model the information survivability in UWSN. Next, we derive a\nfully distributed algorithm that supports these models and give the correctness\nproofs.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 15:16:12 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Bahi", "Jacques M.", ""], ["Guyeux", "Christophe", ""], ["Hakem", "Mourad", ""], ["Makhoul", "Abdallah", ""]]}, {"id": "1608.06002", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, Kaushik Mondal, Partha Sarathi Mandal and Stefan\n  Schmid", "title": "Convergence of Even Simpler Robots without Location Information", "comments": "15 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of distributed gathering and convergence algorithms for tiny\nrobots has recently received much attention. In particular, it has been shown\nthat convergence problems can even be solved for very weak, \\emph{oblivious}\nrobots: robots which cannot maintain state from one round to the next. The\noblivious robot model is hence attractive from a self-stabilization\nperspective, where state is subject to adversarial manipulation. However, to\nthe best of our knowledge, all existing robot convergence protocols rely on the\nassumption that robots, despite being \"weak\", can measure distances.\n  We in this paper initiate the study of convergence protocols for even simpler\nrobots, called \\emph{monoculus robots}: robots which cannot measure distances.\nIn particular, we introduce two natural models which relax the assumptions on\nthe robots' cognitive capabilities: (1) a Locality Detection ($\\mathcal{LD}$)\nmodel in which a robot can only detect whether another robot is closer than a\ngiven constant distance or not, (2) an Orthogonal Line Agreement\n($\\mathcal{OLA}$) model in which robots only agree on a pair of orthogonal\nlines (say North-South and West-East, but without knowing which is which).\n  The problem turns out to be non-trivial, and simple median and angle\nbisection strategies can easily increase the distances among robots (e.g., the\narea of the enclosing convex hull) over time. Our main contribution are\ndeterministic self-stabilizing convergence algorithms for these two models,\ntogether with a complexity analysis. We also show that in some sense, the\nassumptions made in our models are minimal: by relaxing the assumptions on the\n\\textit{monoculus robots} further, we run into impossibility results.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2016 21:18:25 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Mondal", "Kaushik", ""], ["Mandal", "Partha Sarathi", ""], ["Schmid", "Stefan", ""]]}, {"id": "1608.06033", "submitter": "Philipp Woelfel", "authors": "George Giakkoupis, Maryam Helmi, Lisa Higham, Philipp Woelfel", "title": "Deterministic and Fast Randomized Test-and-Set in Optimal Space", "comments": "The results in this paper combine and elaborate on previous work by\n  the same authors that appeared in the 2015 ACM Symposium on Theory of\n  Computing and the 2013 International Symposium on Distributed Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test-and-set object is a fundamental synchronization primitive for shared\nmemory systems. A test-and-set object stores a bit, initialized to 0, and\nsupports one operation, test&set(), which sets the bit's value to 1 and returns\nits previous value. This paper studies the number of atomic registers required\nto implement a test-and-set object in the standard asynchronous shared memory\nmodel with n processes. The best lower bound is log(n)-1 for obstruction-free\n(Giakkoupis and Woelfel, 2012) and deadlock-free (Styer and Peterson, 1989)\nimplementations. Recently a deterministic obstruction-free implementation using\nO(sqrt(n)) registers was presented (Giakkoupis, Helmi, Higham, and Woelfel,\n2013). This paper closes the gap between these known upper and lower bounds by\npresenting a deterministic obstruction-free implementation of a test-and-set\nobject from Theta(log n) registers of size Theta(log n) bits. We also provide a\ntechnique to transform any deterministic obstruction-free algorithm, in which,\nfrom any configuration, any process can finish if it runs for b steps without\ninterference, into a randomized wait-free algorithm for the oblivious\nadversary, in which the expected step complexity is polynomial in n and b. This\ntransformation allows us to combine our obstruction-free algorithm with the\nrandomized test-and-set algorithm by Giakkoupis and Woelfel (2012), to obtain a\nrandomized wait-free test-and-set algorithm from Theta(log n) registers, with\nexpected step-complexity Theta(log* n) against the oblivious adversary.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 02:18:34 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Giakkoupis", "George", ""], ["Helmi", "Maryam", ""], ["Higham", "Lisa", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1608.06103", "submitter": "Bj\\\"orn B\\\"onninghoff", "authors": "Bj\\\"orn B\\\"onninghoff and Horst Schirmeier", "title": "Estimating Maximum Error Impact in Dynamic Data-driven Applications for\n  Resource-aware Adaption of Software-based Fault-Tolerance", "comments": "Editor: Gilles Tredan. 12th European Dependable Computing Conference\n  (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden. Fast Abstracts\n  Proceedings- EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of transient faults in modern hardware requires system designers to\nconsider errors occurring at runtime. Both hardware- and software-based error\nhandling must be deployed to meet application reliability requirements. The\nlevel of required reliability can vary for system components and depend on\ninput and state, so that a selective use of resilience methods is advised,\nespecially for resource-constrained platforms as found in embedded systems. If\nan error occurring at runtime can be classified as having negligible or\ntolerable impact, less effort can be spent on correcting it. As the actual\nimpact of an error is often dependent on the state of a system at time of\noccurrence, it can not be determined precisely for highly dynamic workloads in\ndata-driven applications. We present a concept to estimate error propagation in\nsets of tasks with variable data dependencies. This allows for a coarse grained\nanalysis of the impact a failed task may have on the overall output. As an\napplication example, we demonstrate our method for a typical dynamic embedded\napplication, namely a decoder for the H.264 video format.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 09:55:11 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["B\u00f6nninghoff", "Bj\u00f6rn", ""], ["Schirmeier", "Horst", ""]]}, {"id": "1608.06171", "submitter": "Alcides Fonseca", "authors": "Alcides Fonseca and Raul Barbosa", "title": "MISO: An intermediate language to express parallel and dependable\n  programs", "comments": "Editor: Gilles Tredan. 12th European Dependable Computing Conference\n  (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden. Fast Abstracts\n  Proceedings- EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to write fast programs is to explore the potential parallelism and\ntake advantage of the high number of cores available in microprocessors. This\ncan be achieved by manually specifying which code executes on which thread, by\nusing compiler parallelization hints (such as OpenMP or Cilk), or by using a\nparallel programming language (such as X10, Chapel or Aeminium. Regardless of\nthe approach, all of these programs are compiled to an intermediate lower-level\nlanguage that is sequential, thus preventing the backend compiler from\noptimizing the program and observing its parallel nature. This paper presents\nMISO, an intermediate language that expresses the parallel nature of programs\nand that can be targeted by front-end compilers. The language defines 'cells',\nwhich are composed by a state and a transition function from one state to the\nnext. This language can express both sequential and parallel programs, and\nprovides information for a backend- compiler to generate efficient parallel\nprograms. Moreover, MISO can be used to automatically add redundancy to a\nprogram, by replicating the state or by taking advantage of different processor\ncores, in order to provide fault tolerance for programs running on unreliable\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 14:09:33 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Fonseca", "Alcides", ""], ["Barbosa", "Raul", ""]]}, {"id": "1608.06310", "submitter": "Renato Luiz de Freitas Cunha", "authors": "Renato L. F. Cunha, Eduardo R. Rodrigues, Leonardo P. Tizzei, Marco A.\n  S. Netto (IBM Research)", "title": "Job Placement Advisor Based on Turnaround Predictions for HPC Hybrid\n  Clouds", "comments": "14 pages, 7 figures, accepted for publication at Future Generation\n  Computer Systems (FGCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several companies and research institutes are moving their CPU-intensive\napplications to hybrid High Performance Computing (HPC) cloud environments.\nSuch a shift depends on the creation of software systems that help users decide\nwhere a job should be placed considering execution time and queue wait time to\naccess on-premise clusters. Relying blindly on turnaround prediction techniques\nwill affect negatively response times inside HPC cloud environments. This paper\nintroduces a tool to make job placement decisions in HPC hybrid cloud\nenvironments taking into account the inaccuracy of execution and waiting time\npredictions. We used job traces from real supercomputing centers to run our\nexperiments, and compared the performance between environments using real\nspeedup curves. We also extended a state-of-the-art machine learning based\npredictor to work with data from the cluster scheduler. Our main findings are:\n(i) depending on workload characteristics, there is a turning point where\npredictions should be disregarded in favor of a more conservative decision to\nminimize job turnaround times and (ii) scheduler data plays a key role in\nimproving predictions generated with machine learning using job trace\ndata---our experiments showed around 20% prediction accuracy improvements.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 20:43:46 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 11:31:21 GMT"}, {"version": "v3", "created": "Fri, 26 Aug 2016 21:25:12 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Cunha", "Renato L. F.", "", "IBM Research"], ["Rodrigues", "Eduardo R.", "", "IBM Research"], ["Tizzei", "Leonardo P.", "", "IBM Research"], ["Netto", "Marco A. S.", "", "IBM Research"]]}, {"id": "1608.06347", "submitter": "Shikai Jin", "authors": "Shikai Jin, Yuxuan Cui, Chunli Yu", "title": "A New Parallelization Method for K-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is a popular clustering method used in data mining area. To work with\nlarge datasets, researchers propose PKMeans, which is a parallel k-means on\nMapReduce. However, the existing k-means parallelization methods including\nPKMeans have many limitations. PKMeans can't finish all its iterations in one\nMapReduce job, so it has to repeat cascading MapReduce jobs in a loop until\nconvergence. On the most popular MapReduce platform, Hadoop, every MapReduce\njob introduces significant I/O overheads and extra execution time at stages of\njob start-up and shuffling. Even worse, it has been proved that in the worst\ncase, k-means needs $2^{{\\Omega}(n)}$ MapReduce jobs to converge, where n is\nthe number of data instances, which means huge overheads for large datasets.\nAdditionally, in PKMeans, at most one reducer can be assigned to and update\neach centroid, so PKMeans can only make use of limited number of parallel\nreducers. In this paper, we propose an improved parallel method for k-means,\nIPKMeans, which has a parallel preprocessing stage using k-d tree and can\nfinish k-means in one single MapReduce job with much more reducers working in\nparallel and lower I/O overheads than PKMeans and has a fast post-processing\nstage generating the final result. In our method, both k-d tree and the new\nimproved parallel k-means are implemented using MapReduce and tested on Hadoop.\nOur experiments show that with same dataset and initial centroids, our method\nhas up to 2/3 lower I/O overheads and consumes less amount of time than PKMeans\nto get a very close clustering result.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 00:35:10 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 21:11:34 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Jin", "Shikai", ""], ["Cui", "Yuxuan", ""], ["Yu", "Chunli", ""]]}, {"id": "1608.06491", "submitter": "Zhihao Shang Zhihao Shang", "authors": "Zhihao Shang, Katinka Wolter", "title": "Delay Evaluation of OpenFlow Network Based on Queueing Model", "comments": "Editor: Hans-Peter Schwefel. 12th European Dependable Computing\n  Conference (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden. Proceedings\n  of Student Forum - EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most popular south-bound protocol of software-defined\nnetworking(SDN), OpenFlow decouples the network control from forwarding\ndevices. It offers flexible and scalable functionality for networks. These\nadvantages may cause performance issues since there are performance penalties\nin terms of packet processing speed. It is important to understand the\nperformance of OpenFlow switches and controllers for its deployments. In this\npaper we model the packet processing time of OpenFlow switches and controllers.\nWe mainly analyze how the probability of packet-in messages impacts the\nperformance of switches and controllers. Our results show that there is a\nperformance penalty in OpenFlow networks. However, the penalty is not much when\nprobability of packet-in messages is low. This model can be used for a network\ndesigner to approximate the performance of her deployments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 12:56:40 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Shang", "Zhihao", ""], ["Wolter", "Katinka", ""]]}, {"id": "1608.06510", "submitter": "Mohammed Kemal Mr", "authors": "Mohammed S. Kemal and Rasmus L. Olsen", "title": "Adaptive Data Collection Mechanisms for Smart Monitoring of Distribution\n  Grids", "comments": "5 pages, 2 figures , Hans-Peter Schwefel. 12th European Dependable\n  Computing Conference (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden.\n  Proceedings of Student Forum - EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Grid systems not only transport electric energy but also information\nwhich will be active part of the electricity supply system. This has led to the\nintroduction of intelligent components on all layers of the electrical grid in\npower generation, transmission, distribution and consumption units. For\nelectric distribution systems, Information from Smart Meters can be utilized to\nmonitor and control the state of the grid. Hence, it is indeed inherent that\ndata from Smart Meters should be collected in a resilient, reliable, secure and\ntimely manner fulfilling all the communication requirements and standards. This\npaper presents a proposal for smart data collection mechanisms to monitor\nelectrical grids with adaptive smart metering infrastructures. A general\noverview of a platform is given for testing, evaluating and implementing\nmechanisms to adapt Smart Meter data aggregation. Three main aspects of\nadaptiveness of the system are studied, adaptiveness to smart metering\napplication needs, adaptiveness to changing communication network dynamics and\nadaptiveness to security attacks. Execution of tests will be conducted in real\nfield experimental set-up and in an advanced hardware in the loop test-bed with\npower and communication co-simulation for validation purposes.\n", "versions": [{"version": "v1", "created": "Mon, 22 Aug 2016 08:11:06 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Kemal", "Mohammed S.", ""], ["Olsen", "Rasmus L.", ""]]}, {"id": "1608.06559", "submitter": "Jose Luis Nunes", "authors": "Jose Luis Nunes", "title": "Improving FPGA resilience through Partial Dynamic Reconfiguration", "comments": "Editor: Hans-Peter Schwefel. 12th European Dependable Computing\n  Conference (EDCC 2016), September 5-9, 2016, Gothenburg, Sweden. Proceedings\n  of Student Forum - EDCC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores advances in reconfiguration properties of SRAM-based\nFPGAs, namely Partial Dynamic Reconfiguration, to improve the resilience of\ncritical systems that take advantage of this technology. Commercial\nof-the-shelf state-of-the-art FPGA devices use SRAM cells for the configuration\nmemory, which allow an increase in both performance and capacity. The fast\naccess times and unlimited number of writes of this technology, reduces\nreconfiguration delays and extends the device lifetime but, at the same time,\nmakes them more sensitive to radiation effects, in the form of Single Event\nUpsets. To overcome this limitation, manufacturers have proposed a few fault\ntolerant approaches, which rely on space/time redundancy and configuration\nmemory content recovery - scrubbing. In this paper, we first present radiation\neffects on these devices and investigate the applicability of the most commonly\nused fault tolerant approaches, and then propose an approach to improve FPGA\nresilience, through the use of a less intrusive failure prediction scrubbing.\nIt is expected that this approach relieves the system designer from\ndependability concerns and reduces both time intrusiveness and overall power\nconsumption.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 15:58:47 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Nunes", "Jose Luis", ""]]}, {"id": "1608.06696", "submitter": "Heidi Howard", "authors": "Heidi Howard, Dahlia Malkhi, Alexander Spiegelman", "title": "Flexible Paxos: Quorum intersection revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed consensus is integral to modern distributed systems. The widely\nadopted Paxos algorithm uses two phases, each requiring majority agreement, to\nreliably reach consensus. In this paper, we demonstrate that Paxos, which lies\nat the foundation of many production systems, is conservative. Specifically, we\nobserve that each of the phases of Paxos may use non-intersecting quorums.\nMajority quorums are not necessary as intersection is required only across\nphases.\n  Using this weakening of the requirements made in the original formulation, we\npropose Flexible Paxos, which generalizes over the Paxos algorithm to provide\nflexible quorums. We show that Flexible Paxos is safe, efficient and easy to\nutilize in existing distributed systems. We conclude by discussing the wide\nreaching implications of this result. Examples include improved availability\nfrom reducing the size of second phase quorums by one when the number of\nacceptors is even and utilizing small disjoint phase-2 quorums to speed up the\nsteady-state.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 03:13:17 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Howard", "Heidi", ""], ["Malkhi", "Dahlia", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "1608.06861", "submitter": "Jun Yue", "authors": "Xia Yue, Wang Man, Jun Yue, Guangcao Liu", "title": "Parallel K-Medoids++ Spatial Clustering Algorithm Based on MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering analysis has received considerable attention in spatial data\nmining for several years. With the rapid development of the geospatial\ninformation technologies, the size of spatial information data is growing\nexponentially which makes clustering massive spatial data a challenging task.\nIn order to improve the efficiency of spatial clustering for large scale data,\nmany researchers proposed several efficient clustering algorithms in parallel.\nIn this paper, a new K-Medoids++ spatial clustering algorithm based on\nMapReduce for clustering massive spatial data is proposed. The initialization\nalgorithm to decrease the number of iterations is combined with the MapReduce\nframework. Comparative Experiments conducted over different dataset and\ndifferent number of nodes indicate that the proposed K-Medoids spatial\nclustering algorithm provides better efficiency than traditional K-Medoids and\nscales well while processing massive spatial data on commodity hardware.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 15:21:24 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Yue", "Xia", ""], ["Man", "Wang", ""], ["Yue", "Jun", ""], ["Liu", "Guangcao", ""]]}, {"id": "1608.06972", "submitter": "Sourav Das", "authors": "Sourav Das, Janardhan Rao Doppa, Partha Pratim Pande, Krishnendu\n  Chakrabarty", "title": "Design-Space Exploration and Optimization of an Energy-Efficient and\n  Reliable 3D Small-world Network-on-Chip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A three-dimensional (3D) Network-on-Chip (NoC) enables the design of high\nperformance and low power many-core chips. Existing 3D NoCs are inadequate for\nmeeting the ever-increasing performance requirements of many-core processors\nsince they are simple extensions of regular 2D architectures and they do not\nfully exploit the advantages provided by 3D integration. Moreover, the\nanticipated performance gain of a 3D NoC-enabled many-core chip may be\ncompromised due to the potential failures of through-silicon-vias (TSVs) that\nare predominantly used as vertical interconnects in a 3D IC. To address these\nproblems, we propose a machine-learning-inspired predictive design methodology\nfor energy-efficient and reliable many-core architectures enabled by 3D\nintegration. We demonstrate that a small-world network-based 3D NoC (3D SWNoC)\nperforms significantly better than its 3D MESH-based counterparts. On average,\nthe 3D SWNoC shows 35% energy-delay-product (EDP) improvement over 3D MESH for\nthe PARSEC and SPLASH2 benchmarks considered in this work. To improve the\nreliability of 3D NoC, we propose a computationally efficient spare-vertical\nlink (sVL) allocation algorithm based on a state-space search formulation. Our\nresults show that the proposed sVL allocation algorithm can significantly\nimprove the reliability as well as the lifetime of 3D SWNoC.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 21:24:02 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["Das", "Sourav", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Chakrabarty", "Krishnendu", ""]]}, {"id": "1608.07155", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "A new kind of parallelism and its programming in the Explicitly\n  Many-Processor Approach", "comments": "13 pages. 7 figures, 2 tables, 5 program listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The processor accelerators are effective because they are working not\n(completely) on principles of stored program computers. They use some kind of\nparallelism, and it is rather hard to program them effectively: a parallel\narchitecture by means of (and thinking in) sequential programming. The recently\nintroduced EMPA architecture uses a new kind of parallelism, which offers the\npotential of reaching higher degree of parallelism, and also provides extra\npossibilities and challenges. It not only provides synchronization and inherent\nparallelization, but also takes over some duties typically offered by the OS,\nand even opens the till now closed machine instructions for the end-user. A\ntoolchain for EMPA architecture with Y86 cores has been prepared, including an\nassembler and a cycle-accurate simulator. The assembler is equipped with some\nmeta-instructions, which allow to use all advanced possibilities of the EMPA\narchitecture, and at the same time provide a (nearly) conventional-style\nprogramming. The cycle accurate simulator is able to execute the EMPA-aware\nobject code, and is a good tool for developing algorithms for EMPA\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2016 18:47:03 GMT"}], "update_date": "2016-08-26", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "1608.07200", "submitter": "Jan-Willem Buurlage", "authors": "Jan-Willem Buurlage, Tom Bannink, Abe Wits", "title": "Bulk-synchronous pseudo-streaming algorithms for many-core accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bulk-synchronous parallel (BSP) model provides a framework for writing\nparallel programs with predictable performance. In this paper we extend the BSP\nmodel to support what we will call pseudo-streaming algorithms for\naccelerators. We also generalize the BSP cost function to these algorithms, so\nthat it is possible to predict the running time for programs targeting\nmany-core accelerators and to identify possible bottlenecks. Several examples\nof algorithms within this new framework will be explored. We extend the BSPlib\nstandard by proposing a small number of new BSP primitives to create and use\nstreams in a portable way. We will introduce a software library called Epiphany\nBSP that implements these ideas for the Parallella development board. Finally\nwe will give experimental results for pseudo-streaming algorithms on the\nParallella platform.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 15:38:47 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 12:39:47 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Buurlage", "Jan-Willem", ""], ["Bannink", "Tom", ""], ["Wits", "Abe", ""]]}, {"id": "1608.07249", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Qiang Wang, Pengfei Xu, Xiaowen Chu", "title": "Benchmarking State-of-the-Art Deep Learning Software Tools", "comments": "Revision history: 1. Revise ResNet-50 configuration in MXNet. 2. Add\n  faster implementation of ResNet-56 in TensorFlow with multiple GPUs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been shown as a successful machine learning method for a\nvariety of tasks, and its popularity results in numerous open-source deep\nlearning software tools. Training a deep network is usually a very\ntime-consuming process. To address the computational challenge in deep\nlearning, many tools exploit hardware features such as multi-core CPUs and\nmany-core GPUs to shorten the training time. However, different tools exhibit\ndifferent features and running performance when training different types of\ndeep networks on different hardware platforms, which makes it difficult for end\nusers to select an appropriate pair of software and hardware. In this paper, we\naim to make a comparative study of the state-of-the-art GPU-accelerated deep\nlearning software tools, including Caffe, CNTK, MXNet, TensorFlow, and Torch.\nWe first benchmark the running performance of these tools with three popular\ntypes of neural networks on two CPU platforms and three GPU platforms. We then\nbenchmark some distributed versions on multiple GPUs. Our contribution is\ntwo-fold. First, for end users of deep learning tools, our benchmarking results\ncan serve as a guide to selecting appropriate hardware platforms and software\ntools. Second, for software developers of deep learning tools, our in-depth\nanalysis points out possible future directions to further optimize the running\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2016 18:48:16 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:25:05 GMT"}, {"version": "v3", "created": "Sat, 3 Sep 2016 16:40:32 GMT"}, {"version": "v4", "created": "Sun, 11 Sep 2016 06:13:13 GMT"}, {"version": "v5", "created": "Mon, 19 Sep 2016 07:09:07 GMT"}, {"version": "v6", "created": "Wed, 25 Jan 2017 09:27:52 GMT"}, {"version": "v7", "created": "Fri, 17 Feb 2017 11:02:08 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Shi", "Shaohuai", ""], ["Wang", "Qiang", ""], ["Xu", "Pengfei", ""], ["Chu", "Xiaowen", ""]]}, {"id": "1608.07443", "submitter": "Christophe Guyeux", "authors": "Abdallah Makhoul and Christophe Guyeux and Mourad Hakem and Jacques M.\n  Bahi", "title": "Using an epidemiological approach to maximize data survival in the\n  internet of things", "comments": "arXiv admin note: text overlap with arXiv:1608.05951", "journal-ref": "ACM Transactions on Internet Technology. 16 (1), 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internet of things (IoT) has gained worldwide attention in recent years.\nIt transforms the everyday objects that surround us into proactive actors of\nthe Internet, generating and consuming information. An important issue related\nto the appearance of such large-scale self-coordinating IoT is the reliability\nand the collaboration between the objects in the presence of environmental\nhazards. High failure rates lead to significant loss of data. Therefore, data\nsurvivability is a main challenge of the IoT. In this paper, we have developed\na compartmental e-Epidemic SIR (Susceptible-Infectious-Recovered) model to save\nthe data in the network and let it survive after attacks. Furthermore, our\nmodel takes into account the dynamic topology of the network where natural\ndeath (crashing nodes) and birth are defined and analyzed. Theoretical methods\nand simulations are employed to solve and simulate the system of equations\ndeveloped and to analyze the model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 12:50:04 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Makhoul", "Abdallah", ""], ["Guyeux", "Christophe", ""], ["Hakem", "Mourad", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1608.07573", "submitter": "Garth Wells", "authors": "Jack S. Hale, Lizao Li, Chris N. Richardson and Garth N. Wells", "title": "Containers for portable, productive and performant scientific computing", "comments": null, "journal-ref": null, "doi": "10.1109/MCSE.2017.2421459", "report-no": null, "categories": "cs.DC cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containers are an emerging technology that hold promise for improving\nproductivity and code portability in scientific computing. We examine Linux\ncontainer technology for the distribution of a non-trivial scientific computing\nsoftware stack and its execution on a spectrum of platforms from laptop\ncomputers through to high performance computing (HPC) systems. We show on a\nworkstation and a leadership-class HPC system that when deployed appropriately\nthere are no performance penalties running scientific programs inside\ncontainers. For Python code run on large parallel computers, the run time is\nreduced inside a container due to faster library imports. The software\ndistribution approach and data that we present will help developers and users\ndecide on whether container technology is appropriate for them. We also provide\nguidance for the vendors of HPC systems that rely on proprietary libraries for\nperformance on what they can do to make containers work seamlessly and without\nperformance penalty.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2016 11:58:00 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 18:21:56 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Hale", "Jack S.", ""], ["Li", "Lizao", ""], ["Richardson", "Chris N.", ""], ["Wells", "Garth N.", ""]]}, {"id": "1608.07781", "submitter": "Chinmay Chandak", "authors": "Chinmay Chandak, Hrishikesh Vaidya, Sathya Peri", "title": "Multiversion Altruistic Locking", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds on altruistic locking which is an extension of 2PL. It\nallows more relaxed rules as compared to 2PL. But altruistic locking too\nenforces some rules which disallow some valid schedules (present in VSR and\nCSR) to be passed by AL. This paper proposes a multiversion variant of AL which\nsolves this problem. The report also discusses the relationship or comparison\nbetween different protocols such as MAL and MV2PL, MAL and AL, MAL and 2PL and\nso on. This paper also discusses the caveats involved in MAL and where it lies\nin the Venn diagram of multiversion serializable schedule protocols. Finally,\nthe possible use of MAL in hybrid protocols and the parameters involved in\nmaking MAL successful are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 28 Aug 2016 08:01:22 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 10:45:08 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 17:15:57 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Chandak", "Chinmay", ""], ["Vaidya", "Hrishikesh", ""], ["Peri", "Sathya", ""]]}, {"id": "1608.07923", "submitter": "Lewis Tseng", "authors": "Lewis Tseng", "title": "Recent Results on Fault-Tolerant Consensus in Message-Passing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault-tolerant consensus has been studied extensively in the literature,\nbecause it is one of the most important distributed primitives and has wide\napplications in practice. This paper surveys important results on\nfault-tolerant consensus in message-passing networks, and the focus is on\nresults from the past decade. Particularly, we categorize the results into two\ngroups: new problem formulations and practical applications. In the first part,\nwe discuss new ways to define the consensus problem, which includes larger\ninput domains, link fault models, different network models . . . etc, and\nbriefly discuss the important techniques. In the second part, we focus on Crash\nFault-Tolerant (CFT) systems that use Paxos or Raft, and Byzantine\nFault-Tolerant (BFT) systems. We also discuss Bitcoin, which can be related to\nsolving Byzantine consensus in anonymous systems, and compare Bitcoin with BFT\nsystems and Byzantine consensus.\n", "versions": [{"version": "v1", "created": "Mon, 29 Aug 2016 06:29:53 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Tseng", "Lewis", ""]]}, {"id": "1608.08521", "submitter": "Mojgan Khaledi", "authors": "Mojgan Khaledi, Mehrdad khaledi, Sneha Kumar Kasera", "title": "Profitable Task Allocation in Mobile Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a game theoretic framework for task allocation in mobile cloud\ncomputing that corresponds to offloading of compute tasks to a group of nearby\nmobile devices. Specifically, in our framework, a distributor node holds a\nmultidimensional auction for allocating the tasks of a job among nearby mobile\nnodes based on their computational capabilities and also the cost of\ncomputation at these nodes, with the goal of reducing the overall job\ncompletion time. Our proposed auction also has the desired incentive\ncompatibility property that ensures that mobile devices truthfully reveal their\ncapabilities and costs and that those devices benefit from the task allocation.\nTo deal with node mobility, we perform multiple auctions over adaptive time\nintervals. We develop a heuristic approach to dynamically find the best time\nintervals between auctions to minimize unnecessary auctions and the\naccompanying overheads. We evaluate our framework and methods using both real\nworld and synthetic mobility traces. Our evaluation results show that our game\ntheoretic framework improves the job completion time by a factor of 2-5 in\ncomparison to the time taken for executing the job locally, while minimizing\nthe number of auctions and the accompanying overheads. Our approach is also\nprofitable for the nearby nodes that execute the distributor's tasks with these\nnodes receiving a compensation higher than their actual costs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:52:48 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Khaledi", "Mojgan", ""], ["khaledi", "Mehrdad", ""], ["Kasera", "Sneha Kumar", ""]]}, {"id": "1608.08522", "submitter": "Alessio Arleo", "authors": "Alessio Arleo, Walter Didimo, Giuseppe Liotta, Fabrizio Montecchiani", "title": "A Distributed Multilevel Force-directed Algorithm", "comments": "Appears in the Proceedings of the 24th International Symposium on\n  Graph Drawing and Network Visualization (GD 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide availability of powerful and inexpensive cloud computing services\nnaturally motivates the study of distributed graph layout algorithms, able to\nscale to very large graphs. Nowadays, to process Big Data, companies are\nincreasingly relying on PaaS infrastructures rather than buying and maintaining\ncomplex and expensive hardware. So far, only a few examples of basic\nforce-directed algorithms that work in a distributed environment have been\ndescribed. Instead, the design of a distributed multilevel force-directed\nalgorithm is a much more challenging task, not yet addressed. We present the\nfirst multilevel force-directed algorithm based on a distributed vertex-centric\nparadigm, and its implementation on Giraph, a popular platform for distributed\ngraph algorithms. Experiments show the effectiveness and the scalability of the\napproach. Using an inexpensive cloud computing service of Amazon, we draw\ngraphs with ten million edges in about 60 minutes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2016 15:57:01 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 14:50:59 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Arleo", "Alessio", ""], ["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}]