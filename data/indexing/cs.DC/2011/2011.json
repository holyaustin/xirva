[{"id": "2011.00071", "submitter": "Arissa Wongpanich", "authors": "Arissa Wongpanich, Hieu Pham, James Demmel, Mingxing Tan, Quoc Le,\n  Yang You, Sameer Kumar", "title": "Training EfficientNets at Supercomputer Scale: 83% ImageNet Top-1\n  Accuracy in One Hour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EfficientNets are a family of state-of-the-art image classification models\nbased on efficiently scaled convolutional neural networks. Currently,\nEfficientNets can take on the order of days to train; for example, training an\nEfficientNet-B0 model takes 23 hours on a Cloud TPU v2-8 node. In this paper,\nwe explore techniques to scale up the training of EfficientNets on TPU-v3 Pods\nwith 2048 cores, motivated by speedups that can be achieved when training at\nsuch scales. We discuss optimizations required to scale training to a batch\nsize of 65536 on 1024 TPU-v3 cores, such as selecting large batch optimizers\nand learning rate schedules as well as utilizing distributed evaluation and\nbatch normalization techniques. Additionally, we present timing and performance\nbenchmarks for EfficientNet models trained on the ImageNet dataset in order to\nanalyze the behavior of EfficientNets at scale. With our optimizations, we are\nable to train EfficientNet on ImageNet to an accuracy of 83% in 1 hour and 4\nminutes.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 19:27:11 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 02:17:22 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Wongpanich", "Arissa", ""], ["Pham", "Hieu", ""], ["Demmel", "James", ""], ["Tan", "Mingxing", ""], ["Le", "Quoc", ""], ["You", "Yang", ""], ["Kumar", "Sameer", ""]]}, {"id": "2011.00087", "submitter": "Canran Wang", "authors": "Canran Wang and Netanel Raviv", "title": "Low Latency Cross-Shard Transactions in Coded Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although blockchain, the supporting technology of Bitcoin and various\ncryptocurrencies, has offered a potentially effective framework for numerous\napplications, it still suffers from the adverse affects of the impossibility\ntriangle. Performance, security, and decentralization of blockchains normally\ndo not scale simultaneously with the number of participants in the network. The\nrecent introduction of error correcting codes in sharded blockchain by Li et\nal. partially settles this trilemma, boosting throughput without compromising\nsecurity and decentralization. In this paper, we improve the coded sharding\nscheme in three ways. First, we propose a novel 2-Dimensional Sharding\nstrategy, which inherently supports cross-shard transactions, alleviating the\nneed for complicated inter-shard communication protocols. Second, we employ\ndistributed storage techniques in the propagation of blocks, improving latency\nunder restricted bandwidth. Finally, we incorporate polynomial cryptographic\nprimitives of low degree, which brings coded blockchain techniques into the\nrealm of feasible real-world parameters.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 20:12:05 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Canran", ""], ["Raviv", "Netanel", ""]]}, {"id": "2011.00236", "submitter": "Khaled Alanezi", "authors": "Khaled Alanezi and Shivakant Mishra", "title": "An edge-based architecture to support the execution of ambience\n  intelligence tasks using the IoP paradigm", "comments": "Journal paper", "journal-ref": "Elsevier Future Generation Computer Systems 2020", "doi": "10.1016/j.future.2020.08.001", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an IoP environment, edge computing has been proposed to address the\nproblems of resource limitations of edge devices such as smartphones as well as\nthe high-latency, user privacy exposure and network bottleneck that the cloud\ncomputing platform solutions incur. This paper presents a context management\nframework comprised of sensors, mobile devices such as smartphones and an edge\nserver to enable high performance, context-aware computing at the edge. Key\nfeatures of this architecture include energy-efficient discovery of available\nsensors and edge services for the client, an automated mechanism for task\nplanning and execution on the edge server, and a dynamic environment where new\nsensors and services may be added to the framework. A prototype of this\narchitecture has been implemented, and an experimental evaluation using two\ncomputer vision tasks as example services is presented. Performance measurement\nshows that the execution of the example tasks performs quite well and the\nproposed framework is well suited for an edge-computing environment.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 10:48:44 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Alanezi", "Khaled", ""], ["Mishra", "Shivakant", ""]]}, {"id": "2011.00243", "submitter": "Georg Hager", "authors": "Ayesha Afzal and Georg Hager and Gerhard Wellein", "title": "An analytic performance model for overlapping execution of memory-bound\n  loop kernels on multicore CPUs", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex applications running on multicore processors show a rich performance\nphenomenology. The growing number of cores per ccNUMA domain complicates\nperformance analysis of memory-bound code since system noise, load imbalance,\nor task-based programming models can lead to thread desynchronization. Hence,\nthe simplifying assumption that all cores execute the same loop can not be\nupheld. Motivated by observations on plain and modified versions of the HPCG\nbenchmark, we construct a performance model of execution of memory-bound loop\nkernels. It can predict the memory bandwidth share per kernel on a memory\ncontention domain depending on the number of active cores and which other\nworkload the kernel is paired with. The only code features required are the\nsingle-thread cache line access frequency per kernel, which is directly related\nto the single-thread memory bandwidth, and its saturated bandwidth. It can\neither be measured directly or predicted using the Execution-Cache-Memory (ECM)\nperformance model. The computational intensity of the kernels and the detailed\nstructure of the code is of no significance. We validate our model on Intel\nBroadwell, Intel Cascade Lake, and AMD Rome processors pairing various\nstreaming and stencil kernels. The error in predicting the bandwidth share per\nkernel is less than 8%.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 11:08:49 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2011.00330", "submitter": "Brijen Thananjeyan", "authors": "Brijen Thananjeyan, Kirthevasan Kandasamy, Ion Stoica, Michael I.\n  Jordan, Ken Goldberg, Joseph E. Gonzalez", "title": "Resource Allocation in Multi-armed Bandit Exploration: Overcoming\n  Sublinear Scaling with Adaptive Parallelism", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exploration in stochastic multi-armed bandits when we have access to\na divisible resource that can be allocated in varying amounts to arm pulls. We\nfocus in particular on the allocation of distributed computing resources, where\nwe may obtain results faster by allocating more resources per pull, but might\nhave reduced throughput due to nonlinear scaling. For example, in\nsimulation-based scientific studies, an expensive simulation can be sped up by\nrunning it on multiple cores. This speed-up however, is partly offset by the\ncommunication among cores, which results in lower throughput than if fewer\ncores were allocated per trial to run more trials in parallel. In this paper,\nwe explore these trade-offs in two settings. First, in a fixed confidence\nsetting, we need to find the best arm with a given target success probability\nas quickly as possible. We propose an algorithm which trades off between\ninformation accumulation and throughput and show that the time taken can be\nupper bounded by the solution of a dynamic program whose inputs are the gaps\nbetween the sub-optimal and optimal arms. We also prove a matching hardness\nresult. Second, we present an algorithm for a fixed deadline setting, where we\nare given a time deadline and need to maximize the probability of finding the\nbest arm. We corroborate our theoretical insights with simulation experiments\nthat show that the algorithms consistently match or outperform baseline\nalgorithms on a variety of problem instances.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2020 18:19:29 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 19:25:31 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Thananjeyan", "Brijen", ""], ["Kandasamy", "Kirthevasan", ""], ["Stoica", "Ion", ""], ["Jordan", "Michael I.", ""], ["Goldberg", "Ken", ""], ["Gonzalez", "Joseph E.", ""]]}, {"id": "2011.00443", "submitter": "Varun Behera", "authors": "Ashish Ranjan, Varun Nagesh Jolly Behera, Motahar Reza", "title": "A Parallel Approach for Real-Time Face Recognition from a Large Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new facial recognition system, capable of identifying a person,\nprovided their likeness has been previously stored in the system, in real time.\nThe system is based on storing and comparing facial embeddings of the subject,\nand identifying them later within a live video feed. This system is highly\naccurate, and is able to tag people with their ID in real time. It is able to\ndo so, even when using a database containing thousands of facial embeddings, by\nusing a parallelized searching technique. This makes the system quite fast and\nallows it to be highly scalable.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 07:40:10 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ranjan", "Ashish", ""], ["Behera", "Varun Nagesh Jolly", ""], ["Reza", "Motahar", ""]]}, {"id": "2011.00644", "submitter": "Sheik Mohammad Mostakim Fattah", "authors": "Sheik Mohammad Mostakim Fattah, Athman Bouguettaya, and Sajib Mistry", "title": "Long-term IaaS Selection using Performance Discovery", "comments": "14 pages, accepted and to appear in IEEE Ttransactions on Services\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel framework to select IaaS providers according to a\nconsumer's long-term performance requirements. The proposed framework leverages\nfree short-term trials to discover the unknown QoS performance of IaaS\nproviders. We design a temporal skyline-based filtering method to select\ncandidate IaaS providers for the short-term trials. A novel cooperative\nlong-term QoS prediction approach is developed that utilizes past trial\nexperiences of similar consumers using a workload replay technique. We propose\na new trial workload generation model that estimates a provider's long-term\nperformance in the absence of past trial experiences. The confidence of the\nprediction is measured based on the trial experience of the consumer. A set of\nexperiments are conducted based on real-world datasets to evaluate the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2020 23:31:31 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Fattah", "Sheik Mohammad Mostakim", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2011.00656", "submitter": "Giulia Guidi", "authors": "Giulia Guidi, Marquita Ellis, Aydin Buluc, Katherine Yelick, David\n  Culler", "title": "10 Years Later: Cloud Computing is Closing the Performance Gap", "comments": null, "journal-ref": "Companion of the 2021 ACM/SPEC International Conference on\n  Performance Engineering (ICPE21 Companion)", "doi": "10.1145/3447545.3451183", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can cloud computing infrastructures provide HPC-competitive performance for\nscientific applications broadly? Despite prolific related literature, this\nquestion remains open. Answers are crucial for designing future systems and\ndemocratizing high-performance computing. We present a multi-level approach to\ninvestigate the performance gap between HPC and cloud computing, isolating\ndifferent variables that contribute to this gap. Our experiments are divided\ninto (i) hardware and system microbenchmarks and (ii) user application proxies.\nThe results show that today's high-end cloud computing can deliver\nHPC-competitive performance not only for computationally intensive applications\nbut also for memory- and communication-intensive applications - at least at\nmodest scales - thanks to the high-speed memory systems and interconnects and\ndedicated batch scheduling now available on some cloud platforms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 00:35:38 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:57:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Guidi", "Giulia", ""], ["Ellis", "Marquita", ""], ["Buluc", "Aydin", ""], ["Yelick", "Katherine", ""], ["Culler", "David", ""]]}, {"id": "2011.00667", "submitter": "Guannan Liang", "authors": "Qianqian Tong, Guannan Liang, Xingyu Cai, Chunjiang Zhu and Jinbo Bi", "title": "Asynchronous Parallel Stochastic Quasi-Newton Methods", "comments": "Accepted by Parallel Computing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although first-order stochastic algorithms, such as stochastic gradient\ndescent, have been the main force to scale up machine learning models, such as\ndeep neural nets, the second-order quasi-Newton methods start to draw attention\ndue to their effectiveness in dealing with ill-conditioned optimization\nproblems. The L-BFGS method is one of the most widely used quasi-Newton\nmethods. We propose an asynchronous parallel algorithm for stochastic\nquasi-Newton (AsySQN) method. Unlike prior attempts, which parallelize only the\ncalculation for gradient or the two-loop recursion of L-BFGS, our algorithm is\nthe first one that truly parallelizes L-BFGS with a convergence guarantee.\nAdopting the variance reduction technique, a prior stochastic L-BFGS, which has\nnot been designed for parallel computing, reaches a linear convergence rate. We\nprove that our asynchronous parallel scheme maintains the same linear\nconvergence rate but achieves significant speedup. Empirical evaluations in\nboth simulations and benchmark datasets demonstrate the speedup in comparison\nwith the non-parallel stochastic L-BFGS, as well as the better performance than\nfirst-order methods in solving ill-conditioned problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 01:20:00 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Tong", "Qianqian", ""], ["Liang", "Guannan", ""], ["Cai", "Xingyu", ""], ["Zhu", "Chunjiang", ""], ["Bi", "Jinbo", ""]]}, {"id": "2011.00715", "submitter": "Richard Mills", "authors": "Richard Tran Mills, Mark F. Adams, Satish Balay, Jed Brown, Alp Dener,\n  Matthew Knepley, Scott E. Kruger, Hannah Morgan, Todd Munson, Karl Rupp,\n  Barry F. Smith, Stefano Zampini, Hong Zhang, Junchao Zhang", "title": "Toward Performance-Portable PETSc for GPU-based Exascale Systems", "comments": "14 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9401-1020", "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Portable Extensible Toolkit for Scientific computation (PETSc) library\ndelivers scalable solvers for nonlinear time-dependent differential and\nalgebraic equations and for numerical optimization.The PETSc design for\nperformance portability addresses fundamental GPU accelerator challenges and\nstresses flexibility and extensibility by separating the programming model used\nby the application from that used by the library, and it enables application\ndevelopers to use their preferred programming model, such as Kokkos, RAJA,\nSYCL, HIP, CUDA, or OpenCL, on upcoming exascale systems. A blueprint for using\nGPUs from PETSc-based codes is provided, and case studies emphasize the\nflexibility and high performance achieved on current GPU-based systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 04:05:29 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mills", "Richard Tran", ""], ["Adams", "Mark F.", ""], ["Balay", "Satish", ""], ["Brown", "Jed", ""], ["Dener", "Alp", ""], ["Knepley", "Matthew", ""], ["Kruger", "Scott E.", ""], ["Morgan", "Hannah", ""], ["Munson", "Todd", ""], ["Rupp", "Karl", ""], ["Smith", "Barry F.", ""], ["Zampini", "Stefano", ""], ["Zhang", "Hong", ""], ["Zhang", "Junchao", ""]]}, {"id": "2011.00892", "submitter": "Yuting Fu", "authors": "Yuting Fu, Andrei Terechko, Jan Friso Groote, Arash Khabbaz Saberi", "title": "A Formally Verified Fail-Operational Safety Concept for Automated\n  Driving", "comments": "11 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.FL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Automated Driving (AD) systems rely on safety measures to handle\nfaults and to bring vehicle to a safe state. To eradicate lethal road\naccidents, car manufacturers are constantly introducing new perception as well\nas control systems. Contemporary automotive design and safety engineering best\npractices are suitable for analyzing system components in isolation, whereas\ntoday's highly complex and interdependent AD systems require novel approach to\nensure resilience to multi-point failures. We present a holistic safety concept\nunifying advanced safety measures for handling multiple-point faults. Our\nproposed approach enables designers to focus on more pressing issues such as\nhandling fault-free hazardous behavior associated with system performance\nlimitations. To verify our approach, we developed an executable model of the\nsafety concept in the formal specification language mCRL2. The model behavior\nis governed by a four-mode degradation policy controlling distributed\nprocessors, redundant communication networks, and virtual machines. To keep the\nvehicle as safe as possible our degradation policy can reduce driving comfort\nor AD system's availability using additional low-cost driving channels. We\nformalized five safety requirements in the modal mu-calculus and proved them\nagainst our mCRL2 model, which is intractable to accomplish exhaustively using\ntraditional road tests or simulation techniques. In conclusion, our formally\nproven safety concept defines a holistic design pattern for designing AD\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:05:09 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 16:21:26 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Fu", "Yuting", ""], ["Terechko", "Andrei", ""], ["Groote", "Jan Friso", ""], ["Saberi", "Arash Khabbaz", ""]]}, {"id": "2011.01040", "submitter": "Sebastian Frischbier", "authors": "Sebastian Frischbier, Mario Paic, Alexander Echler, Christian Roth", "title": "Poster: A Real-World Distributed Infrastructure for Processing Financial\n  Data at Scale", "comments": "Authors' version of the accepted submission; final version published\n  by ACM as part of the proceedings of DEBS '19: The 13th ACM International\n  Conference on Distributed and Event-based Systems (DEBS '19); 2 pages, 1\n  figure; vwd Vereinigte Wirtschaftsdienste GmbH is by now known as Infront\n  Financial Technology GmbH (part of the Infront group)", "journal-ref": null, "doi": "10.1145/3328905.3332513", "report-no": null, "categories": "cs.DC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are event- and data-driven to an extremely high degree. For\nmaking decisions and triggering actions stakeholders require notifications\nabout significant events and reliable background information that meet their\nindividual requirements in terms of timeliness, accuracy, and completeness. As\none of Europe's leading providers of financial data and regulatory solutions\nvwd processes an average of 18 billion event notifications from 500+ data\nsources for 30 million symbols per day. Our large-scale distributed event-based\nsystems handle daily peak rates of 1+ million event notifications per second\nand additional load generated by singular pivotal events with global impact. In\nthis poster we give practical insights into our IT systems. We outline the\ninfrastructure we operate and the event-driven architecture we apply at vwd. In\nparticular we showcase the (geo)distributed publish/subscribe broker network we\noperate across locations and countries to provide market data to our customers\nwith varying quality of information (QoI) properties.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 13:14:20 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Frischbier", "Sebastian", ""], ["Paic", "Mario", ""], ["Echler", "Alexander", ""], ["Roth", "Christian", ""]]}, {"id": "2011.01057", "submitter": "Thomas Schl\\\"ogl", "authors": "Thomas Schl\\\"ogl (1) and Ulrich Schmid (1) and Roman Kuznets (1) ((1)\n  TU Wien, Vienna, Austria)", "title": "The Persistence of False Memory: Brain in a Vat Despite Perfect Clocks", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a detailed epistemic reasoning framework for multi-agent systems\nwith byzantine faulty asynchronous agents and possibly unreliable communication\nwas introduced. We have developed a modular extension framework implemented on\ntop of it, which allows to encode and safely combine additional system\nassumptions commonly used in the modeling and analysis of fault-tolerant\ndistributed systems, like reliable communication, time-bounded communication,\nmulticasting, synchronous and lock-step synchronous agents and even agents with\ncoordinated actions. We use this extension framework for analyzing basic\nproperties of synchronous and lock-step synchronous agents, such as the agents'\nlocal and global fault detection abilities. Moreover, we show that even the\nperfectly synchronized clocks available in lock-step synchronous systems cannot\nbe used to avoid \"brain-in-a-vat\" scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 15:40:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Schl\u00f6gl", "Thomas", ""], ["Schmid", "Ulrich", ""], ["Kuznets", "Roman", ""]]}, {"id": "2011.01096", "submitter": "Javad Zarrin", "authors": "Javad Zarrin, Phang Hao Wen, Lakshmi Babu Saheer, Bahram Zarrin", "title": "Blockchain for Decentralization of Internet: Prospects, Trends, and\n  Challenges", "comments": "Under Review in Cluster Computing, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has made an impact on today's technology by revolutionizing the\nfinancial industry in its utilization on cryptocurrency and the features it\nprovided on decentralization. With the current trend of pursuing the\ndecentralized Internet, many methods have been proposed to achieve\ndecentralization considering different aspects of the current Internet model\nranging from infrastructure and protocols to services and applications. This\npaper focuses on using Blockchain to provide a robust and secure decentralized\ncomputing system. The paper conducts a literature review on Blockchain-based\nmethods capable for the decentralization of the future Internet. To achieve\nthat decentralization, two research aspects of Blockchain have been\ninvestigated that are highly relevant in realizing the decentralized Internet.\nThe first aspect is the consensus algorithms, which are vital components for\ndecentralization of Blockchain. We have identified three consensus algorithms\nbeing PoP, Paxos, and PoAH to be more adequate for reaching consensus in\nBlockchain-enabled Internet architecture. The second aspect that we\ninvestigated is the impact of future Internet technologies on Blockchain, where\ntheir combinations with Blockchain would help to make it overcome its\nestablished flaws and be more optimized and applicable for Internet\ndecentralization.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 16:31:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zarrin", "Javad", ""], ["Wen", "Phang Hao", ""], ["Saheer", "Lakshmi Babu", ""], ["Zarrin", "Bahram", ""]]}, {"id": "2011.01207", "submitter": "Jean-Philip Piquemal", "authors": "Olivier Adjoua, Louis Lagard\\`ere, Luc-Henri Jolly, Arnaud Durocher,\n  Thibaut Very, Isabelle Dupays, Zhi Wang, Th\\'eo Jaffrelot Inizan,\n  Fr\\'ed\\'eric C\\'elerse, Pengyu Ren, Jay W. Ponder, Jean-Philip Piquemal", "title": "Tinker-HP : Accelerating Molecular Dynamics Simulations of Large Complex\n  Systems with Advanced Point Dipole Polarizable Force Fields using GPUs and\n  Multi-GPUs systems", "comments": null, "journal-ref": "Journal of Chemical Theory and Computation, 2021, 17, 4, 2034-2053", "doi": "10.1021/acs.jctc.0c01164", "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the extension of the Tinker-HP package (Lagard\\`ere et al., Chem.\nSci., 2018,9, 956-972) to the use of Graphics Processing Unit (GPU) cards to\naccelerate molecular dynamics simulations using polarizable many-body force\nfields. The new high-performance module allows for an efficient use of single-\nand multi-GPU architectures ranging from research laboratories to modern\nsupercomputer centers. After detailing an analysis of our general scalable\nstrategy that relies on OpenACC and CUDA, we discuss the various capabilities\nof the package. Among them, the multi-precision possibilities of the code are\ndiscussed. If an efficient double precision implementation is provided to\npreserve the possibility of fast reference computations, we show that a lower\nprecision arithmetic is preferred providing a similar accuracy for molecular\ndynamics while exhibiting superior performances. As Tinker-HP is mainly\ndedicated to accelerate simulations using new generation point dipole\npolarizable force field, we focus our study on the implementation of the AMOEBA\nmodel. Testing various NVIDIA platforms including 2080Ti, 3090, V100 and A100\ncards, we provide illustrative benchmarks of the code for single- and\nmulti-cards simulations on large biosystems encompassing up to millions of\natoms. The new code strongly reduces time to solution and offers the best\nperformances to date obtained using the AMOEBA polarizable force field.\nPerspectives toward the strong-scaling performance of our multi-node massive\nparallelization strategy, unsupervised adaptive sampling and large scale\napplicability of the Tinker-HP code in biophysics are discussed. The present\nsoftware has been released in phase advance on GitHub in link with the High\nPerformance Computing community COVID-19 research efforts and is free for\nAcademics (see https://github.com/TinkerTools/tinker-hp).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:50:39 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:08:19 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 16:51:06 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 20:01:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Adjoua", "Olivier", ""], ["Lagard\u00e8re", "Louis", ""], ["Jolly", "Luc-Henri", ""], ["Durocher", "Arnaud", ""], ["Very", "Thibaut", ""], ["Dupays", "Isabelle", ""], ["Wang", "Zhi", ""], ["Inizan", "Th\u00e9o Jaffrelot", ""], ["C\u00e9lerse", "Fr\u00e9d\u00e9ric", ""], ["Ren", "Pengyu", ""], ["Ponder", "Jay W.", ""], ["Piquemal", "Jean-Philip", ""]]}, {"id": "2011.01334", "submitter": "Dane Taylor", "authors": "Bao Huynh, Haimonti Dutta, Dane Taylor", "title": "Impact of Community Structure on Consensus Machine Learning", "comments": "9 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus dynamics support decentralized machine learning for data that is\ndistributed across a cloud compute cluster or across the internet of things. In\nthese and other settings, one seeks to minimize the time $\\tau_\\epsilon$\nrequired to obtain consensus within some $\\epsilon>0$ margin of error.\n$\\tau_\\epsilon$ typically depends on the topology of the underlying\ncommunication network, and for many algorithms $\\tau_\\epsilon$ depends on the\nsecond-smallest eigenvalue $\\lambda_2\\in[0,1]$ of the network's normalized\nLaplacian matrix: $\\tau_\\epsilon\\sim\\mathcal{O}(\\lambda_2^{-1})$. Here, we\nanalyze the effect on $\\tau_\\epsilon$ of network community structure, which can\narise when compute nodes/sensors are spatially clustered, for example. We study\nconsensus machine learning over networks drawn from stochastic block models,\nwhich yield random networks that can contain heterogeneous communities with\ndifferent sizes and densities. Using random matrix theory, we analyze the\neffects of communities on $\\lambda_2$ and consensus, finding that $\\lambda_2$\ngenerally increases (i.e., $\\tau_\\epsilon$ decreases) as one decreases the\nextent of community structure. We further observe that there exists a critical\nlevel of community structure at which $\\tau_\\epsilon$ reaches a lower bound and\nis no longer limited by the presence of communities. We support our findings\nwith empirical experiments for decentralized support vector machines.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 21:41:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Huynh", "Bao", ""], ["Dutta", "Haimonti", ""], ["Taylor", "Dane", ""]]}, {"id": "2011.01349", "submitter": "Kailai Xu", "authors": "Kailai Xu, Weiqiang Zhu, Eric Darve", "title": "Distributed Machine Learning for Computational Engineering using MPI", "comments": "24 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for training neural networks that are coupled with\npartial differential equations (PDEs) in a parallel computing environment.\nUnlike most distributed computing frameworks for deep neural networks, our\nfocus is to parallelize both numerical solvers and deep neural networks in\nforward and adjoint computations. Our parallel computing model views data\ncommunication as a node in the computational graph for numerical simulations.\nThe advantage of our model is that data communication and computing are cleanly\nseparated and thus provide better flexibility, modularity, and testability. We\ndemonstrate using various large-scale problems that we can achieve substantial\nacceleration by using parallel solvers for PDEs in training deep neural\nnetworks that are coupled with PDEs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:19:02 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 08:19:41 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Xu", "Kailai", ""], ["Zhu", "Weiqiang", ""], ["Darve", "Eric", ""]]}, {"id": "2011.01367", "submitter": "Narasimha Veeraragavan", "authors": "Narasimha Raghavan Veeraragavan and Kaiwen Zhang", "title": "A position paper on GDPR compliance in sharded blockchains: rehash of\n  old ideas or new interesting challenges?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharding has emerged as one of the common techniques to address the\nscalability problems of blockchain systems. To this end, various sharding\ntechniques for blockchain systems have been proposed in the literature. When\nsharded blockchains process personal data, the data controllers and the data\nprocessors associated with the sharded blockchains need to be compliant with\nthe General Data Protection Regulation (GDPR). To this end, this article makes\nthe first attempt to address the following key question: to what extent the\nexisting techniques developed by different communities such as the distributed\ncomputing community, the distributed systems community, the database community,\nidentity and access control community and the dependability community can be\nused by the data controllers and data processors for complying with the GDPR\nrequirements of data subject rights in sharded blockchains? As part of\nanswering this question, this article argues that there is a need for\ncross-disciplinary research towards finding optimal solutions for implementing\nthe data subject rights in sharded blockchains.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 22:48:48 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Veeraragavan", "Narasimha Raghavan", ""], ["Zhang", "Kaiwen", ""]]}, {"id": "2011.01383", "submitter": "Pratik Fegade", "authors": "Pratik Fegade, Tianqi Chen, Phillip B. Gibbons, Todd C. Mowry", "title": "Cortex: A Compiler for Recursive Deep Learning Models", "comments": "11 pages, 12 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing deep learning models is generally performed in two steps: (i)\nhigh-level graph optimizations such as kernel fusion and (ii) low level kernel\noptimizations such as those found in vendor libraries. This approach often\nleaves significant performance on the table, especially for the case of\nrecursive deep learning models. In this paper, we present Cortex, a\ncompiler-based approach to generate highly-efficient code for recursive models\nfor low latency inference. Our compiler approach and low reliance on vendor\nlibraries enables us to perform end-to-end optimizations, leading to up to 14X\nlower inference latencies over past work, across different backends.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 23:35:14 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:37:53 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Fegade", "Pratik", ""], ["Chen", "Tianqi", ""], ["Gibbons", "Phillip B.", ""], ["Mowry", "Todd C.", ""]]}, {"id": "2011.01410", "submitter": "Hai Zhou", "authors": "Hai Zhou, Dan Feng, Yuchong Hu", "title": "Multi-level Forwarding and Scheduling Recovery Algorithm in\n  Rapidly-changing Network for Erasure-coded Clusters", "comments": "We have modified the algorithm of this article, and a submission\n  meeting is required. However, the meeting request cannot be published, so we\n  apply to withdraw the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key design goal of erasure-coded clusters is to reduce the repair time. The\nexisting Erasure-coded data repair schemes are roughly classified into two\ncategories: 1. Designing rapid data repair (e.g., PPR) in a homogeneous\nenvironment. 2. Constructing data repair (e.g., PPT) based on bandwidth in a\nheterogeneous environment. However, these solutions are difficult to cope with\nthe heterogeneous and Rapidly-changing network in erasure-coded clusters. To\naddress this problem, a bandwidth-aware multi-level forwarding repair\nalgorithm, called BMFRepair, is proposed. BMFRepair monitors the network\nbandwidth in real time when data is forwarded, and selects idle nodes with\nhigh-bandwidth links to assist in forwarding. Thus, it can reduce the time\nbottleneck caused by low link transmission. At the same time, multi-node repair\nbecomes very complicated when the bandwidth changes drastically. A multi-node\nscheduling repairing algorithm, called MSRepair, is proposed for multi-node\nrepairing problems, which can repair multiple failed blocks in parallel by\nscheduling node resources. The two algorithms can flexibly adapt to the rapidly\nchanging network environment and make full use of the bandwidth resources of\nidle nodes. Most importantly, algorithms can continuously adjust the repair\nplan according to the bandwidth change in fast and dynamic network. The\nalgorithms have been evaluated by both simulations on Mininet and real\nexperiments on Aliyun cloud platform ECS. Results show that compared with the\nstate-of-the-art repair schemes PPR and PPT, the algorithms can significantly\nreduce the repair time in rapidly-changing network.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 01:36:52 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 06:02:03 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Zhou", "Hai", ""], ["Feng", "Dan", ""], ["Hu", "Yuchong", ""]]}, {"id": "2011.01419", "submitter": "Weidong Wang", "authors": "Weidong Wang and Wangda Luo", "title": "Heartbeat Diagnosis of Performance Anomaly in OpenMP Multi-Threaded\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel heartbeat diagnosis regarding performance anomaly\nfor OpenMP multi-threaded applications. First, we design injected heartbeat\nAPIs for OpenMP multi-threaded applications. Then, we leverage the heartbeat\nsequences to extract features of previously-observed anomalies. Finally, we\nadopt a tree-based algorithm, namely HSA, to identify the features that are\nrequired to diagnose anomalies. To evaluate our framework, the NAS Parallel NPB\nbenchmark, EPCC OpenMP micro-benchmark suite, and Jacobi benchmark are used to\ntest the performance of our approach proposed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 02:00:19 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Weidong", ""], ["Luo", "Wangda", ""]]}, {"id": "2011.01439", "submitter": "Erik Li", "authors": "Xiaoyi Li", "title": "A Scenario-Based Development Framework for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article summarizes the research progress of scenario-based testing and\ndevelopment technology for autonomous vehicles. We systematically analyzed\nprevious research works and proposed the definition of scenario, the elements\nof the scenario ontology, the data source of the scenario, the processing\nmethod of the scenario data, and scenario-based V-Model. Moreover, we\nsummarized the automated test scenario construction method by random scenario\ngeneration and dangerous scenario generation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:06:48 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 20:09:36 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Li", "Xiaoyi", ""]]}, {"id": "2011.01441", "submitter": "Yuan Tang", "authors": "Yuan Tang, Weiguo Gao", "title": "Balanced Partitioning of Several Cache-Oblivious Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frigo et al. proposed an ideal cache model and a recursive technique to\ndesign sequential cache-efficient algorithms in a cache-oblivious fashion.\nBallard et al. pointed out that it is a fundamental open problem to extend the\ntechnique to an arbitrary architecture. Ballard et al. raised another open\nquestion on how to parallelize Strassen's algorithm exactly and efficiently on\nan arbitrary number of processors.\n  We propose a novel way of partitioning a cache-oblivious algorithm to achieve\nperfect strong scaling on an arbitrary number, even a prime number, of\nprocessors within a certain range in a shared-memory setting. Our approach is\nProcessor-Aware but Cache-Oblivious (PACO). We demonstrate our approach on\nseveral important cache-oblivious algorithms, including LCS, 1D, GAP, classic\nrectangular matrix multiplication on a semiring, and Strassen's algorithm. We\ndiscuss how to extend our approach to a distributed-memory architecture, or\neven a heterogeneous computing system. Hence, our work may provide a new\nperspective on the fundamental open problem of extending the recursive\ncache-oblivious technique to an arbitrary architecture. We provide an almost\nexact solution to the open problem on parallelizing Strassen. Our approach may\nprovide a new perspective on extending the recursive cache-oblivious technique\nto an arbitrary architecture. All our algorithms demonstrate better scalability\nor better overall parallel cache complexities than the best known algorithms.\nPreliminary experiments justify our theoretical prediction that the PACO\nalgorithms can outperform significantly state-of-the-art Processor-Oblivious\n(PO) and Processor-Aware (PA) counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 03:13:29 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Tang", "Yuan", ""], ["Gao", "Weiguo", ""]]}, {"id": "2011.01671", "submitter": "Christian Berger", "authors": "Christian Berger, Hans P. Reiser, Jo\\~ao Sousa, Alysson Bessani", "title": "AWARE: Adaptive Wide-Area Replication for Fast and Resilient Byzantine\n  Consensus", "comments": "This paper consists of 16 pages in total. This paper is the accepted\n  version to be published in IEEE Transactions on Dependable and Secure\n  Computing (2020). For the published version refer to DOI\n  https://doi.org/10.1109/TDSC.2020.3030605", "journal-ref": null, "doi": "10.1109/TDSC.2020.3030605", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With upcoming blockchain infrastructures, world-spanning Byzantine consensus\nis getting practical and necessary. In geographically distributed systems, the\npace at which consensus is achieved is limited by the heterogenous latencies of\nconnections between replicas. If deployed on a wide-area network,\nconsensus-based systems benefit from weighted replication, an approach that\nutilizes extra replicas and assigns higher voting power to well connected\nreplicas. This enables more choice in quorum formation and replicas can\nleverage proportionally smaller quorums to advance, thus decreasing consensus\nlatency. However, the system needs a solution to autonomously adjust to its\nenvironment if network conditions change or faults occur. We present Adaptive\nWide-Area REplication (AWARE), a mechanism which improves the geographical\nscalability of consensus with nodes being widely spread across the world.\nEssentially, AWARE is an automated and dynamic voting weight tuning and leader\npositioning scheme, which supports the emergence of fast quorums in the system.\nIt employs a reliable self-monitoring process and provides a prediction model\nseeking to minimize the system's consensus latency. In experiments using\nseveral AWS EC2 regions, AWARE dynamically optimizes consensus latency by\nself-reliantly finding a fast weight configuration yielding latency gains\nobserved by clients located across the globe.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:58:39 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Berger", "Christian", ""], ["Reiser", "Hans P.", ""], ["Sousa", "Jo\u00e3o", ""], ["Bessani", "Alysson", ""]]}, {"id": "2011.01740", "submitter": "Ferenc Heged\\H{u}s Dr.", "authors": "D\\'aniel Nagy and Lambert Plavecz and Ferenc Heged\\H{u}s", "title": "Solving large number of non-stiff, low-dimensional ordinary differential\n  equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT\n  and DifferentialEquations.jl", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the performance characteristics of different solution\ntechniques and program packages to solve a large number of independent ordinary\ndifferential equation systems is examined. The employed hardware are an Intel\nCore i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores\nand an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak\ndouble-precision performance. The tested systems (Lorenz equation,\nKeller--Miksis equation and a pressure relief valve model) are non-stiff and\nhave low dimension. Thus, the performance of the codes are not limited by\nmemory bandwidth, and Runge--Kutta type solvers are efficient and suitable\nchoices. The tested program packages are MPGOS written in C++ and specialised\nonly for GPUs; ODEINT implemented in C++, which supports execution on both CPUs\nand GPUs; finally, DifferentialEquations.jl written in Julia that also supports\nexecution on both CPUs and GPUs. Using GPUs, the program package MPGOS is\nsuperior. For CPU computations, the ODEINT program package has the best\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:40:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nagy", "D\u00e1niel", ""], ["Plavecz", "Lambert", ""], ["Heged\u0171s", "Ferenc", ""]]}, {"id": "2011.01783", "submitter": "Tiansheng Huang", "authors": "Tiansheng Huang, Weiwei Lin, Wentai Wu, Ligang He, Keqin Li and Albert\n  Y.Zomaya", "title": "An Efficiency-boosting Client Selection Scheme for Federated Learning\n  with Fairness Guarantee", "comments": "Accepted by IEEE TPDS. DOI: 10.1109/TPDS.2020.3040887", "journal-ref": null, "doi": "10.1109/TPDS.2020.3040887", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of potential privacy leakage during centralized AI's model training\nhas drawn intensive concern from the public. A Parallel and Distributed\nComputing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new\nparadigm to cope with the privacy issue by allowing clients to perform model\ntraining locally, without the necessity to upload their personal sensitive\ndata. In FL, the number of clients could be sufficiently large, but the\nbandwidth available for model distribution and re-upload is quite limited,\nmaking it sensible to only involve part of the volunteers to participate in the\ntraining process. The client selection policy is critical to an FL process in\nterms of training efficiency, the final model's quality as well as fairness. In\nthis paper, we will model the fairness guaranteed client selection as a\nLyapunov optimization problem and then a C2MAB-based method is proposed for\nestimation of the model exchange time between each client and the server, based\non which we design a fairness guaranteed algorithm termed RBCS-F for\nproblem-solving. The regret of RBCS-F is strictly bounded by a finite constant,\njustifying its theoretical feasibility. Barring the theoretical results, more\nempirical data can be derived from our real training experiments on public\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:27:02 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 15:28:10 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 06:27:51 GMT"}, {"version": "v4", "created": "Fri, 21 May 2021 03:36:54 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Huang", "Tiansheng", ""], ["Lin", "Weiwei", ""], ["Wu", "Wentai", ""], ["He", "Ligang", ""], ["Li", "Keqin", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "2011.01814", "submitter": "Charilaos Tzovas", "authors": "Charilaos Tzovas, Maria Predari, Henning Meyerhenke", "title": "Distributing Sparse Matrix/Graph Applications in Heterogeneous Clusters\n  -- an Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in scientific and engineering applications contain sparse\nmatrices or graphs as main input objects, e.g. numerical simulations on meshes.\nLarge inputs are abundant these days and require parallel processing for memory\nsize and speed. To optimize the execution of such simulations on cluster\nsystems, the input problem needs to be distributed suitably onto the processing\nunits (PUs). More and more frequently, such clusters contain different CPUs or\na combination of CPUs and GPUs. This heterogeneity makes the load distribution\nproblem quite challenging. Our study is motivated by the observation that\nestablished partitioning tools do not handle such heterogeneous distribution\nproblems as well as homogeneous ones.\n  In this paper, we first formulate the problem of balanced load distribution\nfor heterogeneous architectures as a multi-objective, single-constraint\noptimization problem. We then split the problem into two phases and propose a\ngreedy approach to determine optimal block sizes for each PU. These block sizes\nare then fed into numerous existing graph partitioners, for us to examine how\nwell they handle the above problem. One of the tools we consider is an\nextension of our own previous work (von Looz et al, ICPP'18) called Geographer.\nOur experiments on well-known benchmark meshes indicate that only two tools\nunder consideration are able to yield good quality. These two are Parmetis\n(both the geometric and the combinatorial variant) and Geographer. While\nParmetis is faster, Geographer yields better quality on average.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 16:12:35 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 08:41:23 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 18:05:32 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Tzovas", "Charilaos", ""], ["Predari", "Maria", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "2011.01871", "submitter": "Xiang Li", "authors": "Xiang Li", "title": "FASTCloud: A framework of assessment and selection for trustworthy cloud\n  service based on QoS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By virtue of technology and benefit advantages, cloud computing has\nincreasingly attracted a large number of potential cloud consumers (PCC) plan\nto migrate the traditional business to the cloud service. However, trust has\nbecome one of the most challenging issues that prevent the PCC from adopting\ncloud services, especially in trustworthy cloud service selection. Besides, due\nto the diversity and dynamic of quality of service (QoS) in the cloud\nenvironment, the existing trust assessment methods based on the single constant\nvalue of QoS attribute and the subjective weight assignment are not good enough\nto provide an effective solution for PCCs to identify and select a trustworthy\ncloud service among a wide range of functionally-equivalent cloud service\nproviders (CSPs). To address the challenge, a novel assessment and selection\nframework for trustworthy cloud service, FASTCloud, is proposed in this study.\nThis framework facilitates PCCs to select a trustworthy cloud service based on\ntheir actual QoS requirements. In order to accurately and efficiently assess\nthe trust level of cloud services, a QoS-based trust assessment model is\nproposed. This model represents a trust level assessment method based on the\ninterval multiple attributes with an objective weight assignment method based\non the deviation maximization to adaptively determine the trust level of\ndifferent cloud services provisioned by candidate CSPs. The advantage of the\nproposed trust level assessment method in time complexity is demonstrated by\nthe performance analysis and comparison. The experimental result of a case\nstudy with an open-source dataset shows that the trust model is efficient in\ncloud service trust assessment and the FASTCloud can effectively help PCCs\nselect a trustworthy cloud service.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 01:18:05 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 07:49:30 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Li", "Xiang", ""]]}, {"id": "2011.01922", "submitter": "Baturalp Buyukates", "authors": "Baturalp Buyukates and Emre Ozfatura and Sennur Ulukus and Deniz\n  Gunduz", "title": "Gradient Coding with Dynamic Clustering for Straggler Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed synchronous gradient descent (GD) the main performance\nbottleneck for the per-iteration completion time is the slowest\n\\textit{straggling} workers. To speed up GD iterations in the presence of\nstragglers, coded distributed computation techniques are implemented by\nassigning redundant computations to workers. In this paper, we propose a novel\ngradient coding (GC) scheme that utilizes dynamic clustering, denoted by GC-DC,\nto speed up the gradient calculation. Under time-correlated straggling\nbehavior, GC-DC aims at regulating the number of straggling workers in each\ncluster based on the straggler behavior in the previous iteration. We\nnumerically show that GC-DC provides significant improvements in the average\ncompletion time (of each iteration) with no increase in the communication load\ncompared to the original GC scheme.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 18:52:15 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Buyukates", "Baturalp", ""], ["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2011.02046", "submitter": "Helen Xu", "authors": "Shahin Kamali, Helen Xu", "title": "Beyond Worst-case Analysis of Multicore Caching Strategies", "comments": "22 pages, short version to appear in APOCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every processor with multiple cores sharing a cache needs to implement a\ncache-replacement algorithm. Previous work demonstrated that the competitive\nratio of a large class of online algorithms, including Least-Recently-Used\n(LRU), grows with the length of the input. Furthermore, even offline algorithms\nlike Furthest-In-Future, the optimal algorithm in single-core caching, cannot\ncompete in the multicore setting. These negative results motivate a more\nin-depth comparison of multicore caching algorithms via alternative analysis\nmeasures. Specifically, the power of the adversary to adapt to online\nalgorithms suggests the need for a direct comparison of online algorithms to\neach other.\n  In this paper, we introduce cyclic analysis, a generalization of bijective\nanalysis introduced by Angelopoulos and Schweitzer [JACM'13]. Cyclic analysis\ncaptures the advantages of bijective analysis while offering flexibility that\nmakes it more useful for comparing algorithms for a variety online problems. In\nparticular, we take the first steps beyond worst-case analysis for analysis of\nmulticore caching algorithms. We use cyclic analysis to establish relationships\nbetween multicore caching algorithms, including the advantage of LRU over all\nother multicore caching algorithms in the presence of locality of reference.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 22:41:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Kamali", "Shahin", ""], ["Xu", "Helen", ""]]}, {"id": "2011.02084", "submitter": "Michael Lui", "authors": "Michael Lui, Yavuz Yetim, \\\"Ozg\\\"ur \\\"Ozkan, Zhuoran Zhao, Shin-Yeh\n  Tsai, Carole-Jean Wu, and Mark Hempstead", "title": "Understanding Capacity-Driven Scale-Out Neural Recommendation Inference", "comments": "16 pages + references, 16 Figures. Additive revision to clarify\n  distinction between this work and other DLRM-like models and add\n  Acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation models have grown to the terabyte scale.\nTraditional serving schemes--that load entire models to a single server--are\nunable to support this scale. One approach to support this scale is with\ndistributed serving, or distributed inference, which divides the memory\nrequirements of a single large model across multiple servers.\n  This work is a first-step for the systems research community to develop novel\nmodel-serving solutions, given the huge system design space. Large-scale deep\nrecommender systems are a novel workload and vital to study, as they consume up\nto 79% of all inference cycles in the data center. To that end, this work\ndescribes and characterizes scale-out deep learning recommendation inference\nusing data-center serving infrastructure. This work specifically explores\nlatency-bounded inference systems, compared to the throughput-oriented training\nsystems of other recent works. We find that the latency and compute overheads\nof distributed inference are largely a result of a model's static embedding\ntable distribution and sparsity of input inference requests. We further\nevaluate three embedding table mapping strategies of three DLRM-like models and\nspecify challenging design trade-offs in terms of end-to-end latency, compute\noverhead, and resource efficiency. Overall, we observe only a marginal latency\noverhead when the data-center scale recommendation models are served with the\ndistributed inference manner--P99 latency is increased by only 1% in the best\ncase configuration. The latency overheads are largely a result of the commodity\ninfrastructure used and the sparsity of embedding tables. Even more\nencouragingly, we also show how distributed inference can account for\nefficiency improvements in data-center scale recommendation serving.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 00:51:40 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 16:31:05 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lui", "Michael", ""], ["Yetim", "Yavuz", ""], ["\u00d6zkan", "\u00d6zg\u00fcr", ""], ["Zhao", "Zhuoran", ""], ["Tsai", "Shin-Yeh", ""], ["Wu", "Carole-Jean", ""], ["Hempstead", "Mark", ""]]}, {"id": "2011.02190", "submitter": "Elad Michael Schiller (PhD)", "authors": "Zacharias Georgiou and Chryssis Georgiou and George Pallis and Elad\n  Michael Schiller and Demetris Trihinas", "title": "A Self-stabilizing Control Plane for the Edge and Fog Ecosystems", "comments": "This version was submitted on 2020/05/14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog Computing is now emerging as the dominating paradigm bridging the compute\nand connectivity gap between sensing devices (a.k.a. \"things\") and\nlatency-sensitive services. However, as fog deployments scale by accumulating\nnumerous devices interconnected over highly dynamic and volatile network\nfabrics, the need for self-configuration and self-healing in the presence of\nfailures is more evident now than ever. Using the prevailing methodology of\nself-stabilization, we propose a fault-tolerant framework for distributed\ncontrol planes that enables fog services to cope and recover from a very broad\nfault model. Specifically, our model considers network uncertainties, packet\ndrops, node fail-stop failures, and violations of the assumptions according to\nwhich the system was designed to operate, such as an arbitrary corruption of\nthe system state. Our self-stabilizing algorithms guarantee automatic recovery\nwithin a constant number of communication rounds without the need for external\n(human) intervention. To showcase the framework's effectiveness, the\ncorrectness proof of the proposed self-stabilizing algorithmic process is\naccompanied by a comprehensive evaluation featuring an open and reproducible\ntestbed utilizing real-world data from the intelligent transportation domain.\nResults show that our framework ensures a fog ecosystem recovery from faults in\nconstant time, analytics are computed correctly, while the overhead to the\nsystem's control plane scales linearly towards the IoT load.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 09:23:12 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Georgiou", "Zacharias", ""], ["Georgiou", "Chryssis", ""], ["Pallis", "George", ""], ["Schiller", "Elad Michael", ""], ["Trihinas", "Demetris", ""]]}, {"id": "2011.02224", "submitter": "Jiaqi Chen", "authors": "Jiaqi Chen, Shlomi Dolev, Shay Kutten", "title": "Reactive Proof Labeling Schemes for Distributed Decision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the definition of Proof Labeling Schemes to reactive systems,\nthat is, systems where the configuration is supposed to keep changing forever.\nAs an example, we address the main classical test case of reactive tasks,\nnamely, the task of token passing. Different RPLSs are given for the cases that\nthe network is assumed to be a tree or an anonymous ring, or a general graph,\nand the sizes of RPLSs' labels are analyzed. We also address the question of\nwhether an RPLS exists. First, on the positive side, we show that there exists\nan RPLS for any distributed task for a family of graphs with unique identities.\nFor the case of anonymous networks (even for the special case of rings),\ninterestingly, it is known that no token passing algorithm is possible even if\nthe number n of nodes is known. Nevertheless, we show that an RPLS is possible.\nOn the negative side, we show that if one drops the assumption that n is known,\nthen the construction becomes impossible.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 11:01:55 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chen", "Jiaqi", ""], ["Dolev", "Shlomi", ""], ["Kutten", "Shay", ""]]}, {"id": "2011.02367", "submitter": "Jihong Park", "authors": "Hyowoon Seo, Jihong Park, Seungeun Oh, Mehdi Bennis, Seong-Lyun Kim", "title": "Federated Knowledge Distillation", "comments": "30 pages, 12 figures, 2 tables; This chapter is written for the\n  forthcoming book, Machine Learning and Wireless Communications (Cambridge\n  University Press), edited by H. V. Poor, D. Gunduz, A. Goldsmith, and Y.\n  Eldar", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning frameworks often rely on exchanging model parameters\nacross workers, instead of revealing their raw data. A prime example is\nfederated learning that exchanges the gradients or weights of each neural\nnetwork model. Under limited communication resources, however, such a method\nbecomes extremely costly particularly for modern deep neural networks having a\nhuge number of model parameters. In this regard, federated distillation (FD) is\na compelling distributed learning solution that only exchanges the model\noutputs whose dimensions are commonly much smaller than the model sizes (e.g.,\n10 labels in the MNIST dataset). The goal of this chapter is to provide a deep\nunderstanding of FD while demonstrating its communication efficiency and\napplicability to a variety of tasks. To this end, towards demystifying the\noperational principle of FD, the first part of this chapter provides a novel\nasymptotic analysis for two foundational algorithms of FD, namely knowledge\ndistillation (KD) and co-distillation (CD), by exploiting the theory of neural\ntangent kernel (NTK). Next, the second part elaborates on a baseline\nimplementation of FD for a classification task, and illustrates its performance\nin terms of accuracy and communication efficiency compared to FL. Lastly, to\ndemonstrate the applicability of FD to various distributed learning tasks and\nenvironments, the third part presents two selected applications, namely FD over\nasymmetric uplink-and-downlink wireless channels and FD for reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:56:13 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Seo", "Hyowoon", ""], ["Park", "Jihong", ""], ["Oh", "Seungeun", ""], ["Bennis", "Mehdi", ""], ["Kim", "Seong-Lyun", ""]]}, {"id": "2011.02368", "submitter": "Nilanjan Goswami", "authors": "Nilanjan Goswami, Amer Qouneh, Chao Li, Tao Li", "title": "An Empirical-cum-Statistical Approach to Power-Performance\n  Characterization of Concurrent GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Growing deployment of power and energy efficient throughput accelerators\n(GPU) in data centers demands enhancement of power-performance co-optimization\ncapabilities of GPUs. Realization of exascale computing using accelerators\nrequires further improvements in power efficiency. With hardwired kernel\nconcurrency enablement in accelerators, inter- and intra-workload simultaneous\nkernels computation predicts increased throughput at lower energy budget. To\nimprove Performance-per-Watt metric of the architectures, a systematic\nempirical study of real-world throughput workloads (with concurrent kernel\nexecution) is required. To this end, we propose a multi-kernel throughput\nworkload generation framework that will facilitate aggressive energy and\nperformance management of exascale data centers and will stimulate synergistic\npower-performance co-optimization of throughput architectures. Also, we\ndemonstrate a multi-kernel throughput benchmark suite based on the framework\nthat encapsulates symmetric, asymmetric and co-existing (often appears\ntogether) kernel based workloads. On average, our analysis reveals that spatial\nand temporal concurrency within kernel execution in throughput architectures\nsaves energy consumption by 32%, 26% and 33% in GTX470, Tesla M2050 and Tesla\nK20 across 12 benchmarks. Concurrency and enhanced utilization are often\ncorrelated but do not imply significant deviation in power dissipation.\nDiversity analysis of proposed multi-kernels confirms characteristic variation\nand power-profile diversity within the suite. Besides, we explain several\nfindings regarding power-performance co-optimization of concurrent throughput\nworkloads.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 15:58:54 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 03:25:55 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Goswami", "Nilanjan", ""], ["Qouneh", "Amer", ""], ["Li", "Chao", ""], ["Li", "Tao", ""]]}, {"id": "2011.02379", "submitter": "Mathieu Even", "authors": "Mathieu Even, Hadrien Hendrikx, Laurent Massouli\\'e", "title": "Asynchrony and Acceleration in Gossip Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the minimization of a sum of smooth and strongly convex\nfunctions dispatched over the nodes of a communication network. Previous works\non the subject either focus on synchronous algorithms, which can be heavily\nslowed down by a few slow nodes (the straggler problem), or consider a model of\nasynchronous operation (Boyd et al., 2006) in which adjacent nodes communicate\nat the instants of Poisson point processes. We have two main contributions. 1)\nWe propose CACDM (a Continuously Accelerated Coordinate Dual Method), and for\nthe Poisson model of asynchronous operation, we prove CACDM to converge to\noptimality at an accelerated convergence rate in the sense of Nesterov et\nStich, 2017. In contrast, previously proposed asynchronous algorithms have not\nbeen proven to achieve such accelerated rate. While CACDM is based on discrete\nupdates, the proof of its convergence crucially depends on a continuous time\nanalysis. 2) We introduce a new communication scheme based on Loss-Networks,\nthat is programmable in a fully asynchronous and decentralized way, unlike the\nPoisson model of asynchronous operation that does not capture essential aspects\nof asynchrony such as non-instantaneous communications and computations. Under\nthis Loss-Network model of asynchrony, we establish for CDM (a Coordinate Dual\nMethod) a rate of convergence in terms of the eigengap of the Laplacian of the\ngraph weighted by local effective delays. We believe this eigengap to be a\nfundamental bottleneck for convergence rates of asynchronous optimization.\nFinally, we verify empirically that CACDM enjoys an accelerated convergence\nrate in the Loss-Network model of asynchrony.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:15:32 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 11:26:03 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Even", "Mathieu", ""], ["Hendrikx", "Hadrien", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2011.02412", "submitter": "Bryan Ford", "authors": "Bryan Ford", "title": "Identity and Personhood in Digital Democracy: Evaluating Inclusion,\n  Equality, Security, and Privacy in Pseudonym Parties and Other Proofs of\n  Personhood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital identity seems like a prerequisite for digital democracy: how can we\nensure \"one person, one vote\" online without identifying voters? But digital\nidentity solutions - ID checking, biometrics, self-sovereign identity, and\ntrust networks - all present flaws, leaving users vulnerable to exclusion,\nidentity loss or theft, and coercion. These flaws may be insurmountable because\ndigital identity is a cart pulling the horse. We cannot achieve digital\nidentity secure enough for the weight of digital democracy, until we build it\non a solid foundation of \"digital personhood.\" While identity is about\ndistinguishing one person from another through attributes or affiliations,\npersonhood is about giving all real people inalienable digital participation\nrights independent of identity, including protection against erosion of their\ndemocratic rights through identity loss, theft, coercion, or fakery.\n  We explore and analyze alternative approaches to \"proof of personhood\" that\nmay provide this missing foundation. Pseudonym parties marry the transparency\nof periodic physical-world events with the power of digital tokens between\nevents. These tokens represent limited-term but renewable claims usable for\npurposes such as online voting or liquid democracy, sampled juries or\ndeliberative polls, abuse-resistant social communication, or minting universal\nbasic income in a permissionless cryptocurrency. Enhancing pseudonym parties to\nprovide participants a moment of enforced physical security and privacy can\naddress coercion and vote-buying risks that plague today's E-voting systems. We\nalso examine other proposed approaches to proof of personhood, some of which\noffer conveniences such as all-online participation. These alternatives\ncurrently fall short of satisfying all the key digital personhood goals,\nunfortunately, but offer valuable insights into the challenges we face.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 17:08:54 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Ford", "Bryan", ""]]}, {"id": "2011.02422", "submitter": "Jiawei Shao", "authors": "Jiawei Shao, Haowei Zhang, Yuyi Mao, Jun Zhang", "title": "Branchy-GNN: a Device-Edge Co-Inference Framework for Efficient Point\n  Cloud Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advancements of three-dimensional (3D) data acquisition devices\nhave spurred a new breed of applications that rely on point cloud data\nprocessing. However, processing a large volume of point cloud data brings a\nsignificant workload on resource-constrained mobile devices, prohibiting from\nunleashing their full potentials. Built upon the emerging paradigm of\ndevice-edge co-inference, where an edge device extracts and transmits the\nintermediate feature to an edge server for further processing, we propose\nBranchy-GNN for efficient graph neural network (GNN) based point cloud\nprocessing by leveraging edge computing platforms. In order to reduce the\non-device computational cost, the Branchy-GNN adds branch networks for early\nexiting. Besides, it employs learning-based joint source-channel coding (JSCC)\nfor the intermediate feature compression to reduce the communication overhead.\nOur experimental results demonstrate that the proposed Branchy-GNN secures a\nsignificant latency reduction compared with several benchmark methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 01:02:52 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Shao", "Jiawei", ""], ["Zhang", "Haowei", ""], ["Mao", "Yuyi", ""], ["Zhang", "Jun", ""]]}, {"id": "2011.02455", "submitter": "Butler Lampson", "authors": "Butler Lampson", "title": "Hints and Principles for Computer System Design", "comments": "There is also a short version of this paper, about half the length of\n  this one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GL cs.OS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This new long version of my 1983 paper suggests the goals you might have for\nyour system -- Simple, Timely, Efficient, Adaptable, Dependable, Yummy (STEADY)\n-- and techniques for achieving them -- Approximate, Incremental, Divide &\nConquer (AID). It also gives some principles for system design that are more\nthan just hints, and many examples of how to apply the ideas.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:40:36 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 19:09:21 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 15:45:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lampson", "Butler", ""]]}, {"id": "2011.02600", "submitter": "Kenneth Duru", "authors": "Kenneth Duru, Frederick Fung, Christopher Williams", "title": "Upwind summation by parts finite difference methods for large scale\n  elastic wave simulations in 3D complex geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order accurate summation-by-parts (SBP) finite difference (FD) methods\nconstitute efficient numerical methods for simulating large-scale hyperbolic\nwave propagation problems. Traditional SBP FD operators that approximate\nfirst-order spatial derivatives with central-difference stencils often have\nspurious unresolved numerical wave-modes in their computed solutions. Recently\nderived high order accurate upwind SBP operators based upwind FD stencils have\nthe potential to suppress these poisonous spurious wave-modes on marginally\nresolved computational grids. In this paper, we demonstrate that not all high\norder upwind SBP FD operators are applicable. Numerical dispersion relation\nanalysis shows that odd-order upwind SBP FD operators also support spurious\nunresolved high-frequencies on marginally resolved meshes. Meanwhile,\neven-order upwind SBP FD operators (of order 2, 4, 6) do not support spurious\nunresolved high frequency wave modes and also have better numerical dispersion\nproperties. We discretise the three space dimensional (3D) elastic wave\nequation on boundary-conforming curvilinear meshes. Using the energy method we\nprove that the semi-discrete approximation is stable and energy-conserving. We\nderive a priori error estimate and prove the convergence of the numerical\nerror. Numerical experiments for the 3D elastic wave equation in complex\ngeometries corroborate the theoretical analysis. Numerical simulations of the\n3D elastic wave equation in heterogeneous media with complex non-planar free\nsurface topography are given, including numerical simulations of community\ndeveloped seismological benchmark problems. Computational results show that\neven-order upwind SBP FD operators are more efficient, robust and less prone to\nnumerical dispersion errors on marginally resolved meshes when compared to the\nodd-order upwind and traditional SBP FD operators.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 01:10:56 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 13:19:15 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 00:23:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Duru", "Kenneth", ""], ["Fung", "Frederick", ""], ["Williams", "Christopher", ""]]}, {"id": "2011.02617", "submitter": "Huda Ibeid", "authors": "Gen Xu, Huda Ibeid, Xin Jiang, Vjekoslav Svilan, and Zhaojuan Bian", "title": "Simulation-Based Performance Prediction of HPC Applications: A Case\n  Study of HPL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simulation-based approach for performance modeling of parallel\napplications on high-performance computing platforms. Our approach enables\nfull-system performance modeling: (1) the hardware platform is represented by\nan abstract yet high-fidelity model; (2) the computation and communication\ncomponents are simulated at a functional level, where the simulator allows the\nuse of the components native interface; this results in a (3) fast and accurate\nsimulation of full HPC applications with minimal modifications to the\napplication source code. This hardware/software hybrid modeling methodology\nallows for low overhead, fast, and accurate exascale simulation and can be\neasily carried out on a standard client platform (desktop or laptop). We\ndemonstrate the capability and scalability of our approach with High\nPerformance LINPACK (HPL), the benchmark used to rank supercomputers in the\nTOP500 list. Our results show that our modeling approach can accurately and\nefficiently predict the performance of HPL at the scale of the TOP500 list\nsupercomputers. For instance, the simulation of HPL on Frontera takes less than\nfive hours with an error rate of four percent.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:18:04 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Xu", "Gen", ""], ["Ibeid", "Huda", ""], ["Jiang", "Xin", ""], ["Svilan", "Vjekoslav", ""], ["Bian", "Zhaojuan", ""]]}, {"id": "2011.02653", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Prithwish Basu, Don Towsley, Ananthram Swami and\n  Kin K. Leung", "title": "On the Analysis of Spatially Constrained Power of Two Choice Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of power of two choice based assignment policies for\nallocating users to servers, where both users and servers are located on a\ntwo-dimensional Euclidean plane. In this framework, we investigate the inherent\ntradeoff between the communication cost, and load balancing performance of\ndifferent allocation policies. To this end, we first design and evaluate a\nSpatial Power of two (sPOT) policy in which each user is allocated to the least\nloaded server among its two geographically nearest servers sequentially. When\nservers are placed on a two-dimensional square grid, sPOT maps to the classical\nPower of two (POT) policy on the Delaunay graph associated with the Voronoi\ntessellation of the set of servers. We show that the associated Delaunay graph\nis 4-regular and provide expressions for asymptotic maximum load using results\nfrom the literature. For uniform placement of servers, we map sPOT to a\nclassical balls and bins allocation policy with bins corresponding to the\nVoronoi regions associated with the second order Voronoi diagram of the set of\nservers. We provide expressions for the lower bound on the asymptotic expected\nmaximum load on the servers and prove that sPOT does not achieve POT load\nbalancing benefits. However, experimental results suggest the efficacy of sPOT\nwith respect to expected communication cost. Finally, we propose two\nnon-uniform server sampling based POT policies that achieve the best of both\nthe performance metrics. Experimental results validate the effctiveness of our\nproposed policies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 04:19:17 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Basu", "Prithwish", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""], ["Leung", "Kin K.", ""]]}, {"id": "2011.02694", "submitter": "Aftab Alam Mr.", "authors": "Aftab Alam, Shah Khalid, Muhammad Numan Khan, Tariq Habib Afridi,\n  Irfan Ullah, and Young-Koo Lee", "title": "Video Big Data Analytics in the Cloud: Research Issues and Challenges", "comments": "15 pages, 3 Figures, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the rise of distributed computing technologies, video big data analytics\nin the cloud have attracted researchers and practitioners' attention. The\ncurrent technology and market trends demand an efficient framework for video\nbig data analytics. However, the current work is too limited to provide an\narchitecture on video big data analytics in the cloud, including managing and\nanalyzing video big data, the challenges, and opportunities. This study\nproposes a service-oriented layered reference architecture for intelligent\nvideo big data analytics in the cloud. Finally, we identify and articulate\nseveral open research issues and challenges, which have been raised by the\ndeployment of big data technologies in the cloud for video big data analytics.\nThis paper provides the research studies and technologies advancing video\nanalyses in the era of big data and cloud computing. This is the first study\nthat presents the generalized view of the video big data analytics in the cloud\nto the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 07:58:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Alam", "Aftab", ""], ["Khalid", "Shah", ""], ["Khan", "Muhammad Numan", ""], ["Afridi", "Tariq Habib", ""], ["Ullah", "Irfan", ""], ["Lee", "Young-Koo", ""]]}, {"id": "2011.02749", "submitter": "Busra Tegin", "authors": "Busra Tegin, Eduin E. Hernandez, Stefano Rini, Tolga M. Duman", "title": "Straggler Mitigation through Unequal Error Protection for Distributed\n  Matrix Multiplication", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning and data mining methods routinely distribute\ncomputations across multiple agents to parallelize processing. The time\nrequired for computation at the agents is affected by the availability of local\nresources giving rise to the \"straggler problem\" in which the computation\nresults are held back by unresponsive agents. For this problem, linear coding\nof the matrix sub-blocks can be used to introduce resilience toward straggling.\nThe Parameter Server (PS) utilizes a channel code and distributes the matrices\nto the workers for multiplication. It then produces an approximation to the\ndesired matrix multiplication using the results of the computations received at\na given deadline. In this paper, we propose to employ Unequal Error Protection\n(UEP) codes to alleviate the straggler problem. The resiliency level of each\nsub-block is chosen according to its norm as blocks with larger norms have\nhigher effects on the result of the matrix multiplication. We validate the\neffectiveness of our scheme both theoretically and through numerical\nevaluations. We derive a theoretical characterization of the performance of UEP\nusing random linear codes, and compare it the case of equal error protection.\nWe also apply the proposed coding strategy to the computation of the\nback-propagation step in the training of a Deep Neural Network (DNN), for which\nwe investigate the fundamental trade-off between precision and the time\nrequired for the computations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 10:43:32 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:24:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Tegin", "Busra", ""], ["Hernandez", "Eduin E.", ""], ["Rini", "Stefano", ""], ["Duman", "Tolga M.", ""]]}, {"id": "2011.02849", "submitter": "Fabian Knorr", "authors": "Fabian Knorr, Peter Thoman and Thomas Fahringer", "title": "Datasets for Benchmarking Floating-Point Compressors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression of floating-point data, both lossy and lossless, is a topic of\nincreasing interest in scientific computing. Developing and evaluating suitable\ncompression algorithms requires representative samples of data from real-world\napplications. We present a collection of publicly accessible sources for volume\nand time series data as well as a list of concrete datasets that form an\nadequate basis for compressor benchmarking.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 14:21:39 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Knorr", "Fabian", ""], ["Thoman", "Peter", ""], ["Fahringer", "Thomas", ""]]}, {"id": "2011.02914", "submitter": "Weidong Wang", "authors": "Weidong Wang and Wangda Luo", "title": "Machine Learning Framwork for Performance Anomaly in OpenMP\n  Multi-Threaded Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.01419", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some OpenMP multi-threaded applications increasingly suffer from performance\nanomaly owning to shared resource contention as well as software- and\nhardware-related problems. Such performance anomaly can result in failure and\ninefficiencies, and are among the main challenges in system resiliency. To\nminimize the impact of performance anomaly, one must quickly and accurately\ndetect and diagnose the performance anomalies that cause the failures. However,\nit is difficult to identify anomalies in the dynamic and noisy data collected\nby OpenMP multi-threaded monitoring infrastructures. This paper presents a\nnovel machine learning framework for performance anomaly in OpenMP\nmulti-threaded systems. To evaluate our framework, the NAS Parallel NPB\nbenchmark, EPCC OpenMP micro-benchmark suite, and Jacobi benchmark are used to\ntest the performance of our framework proposed. The experimental results\ndemonstrate that our framework successfully identifies 90.3\\% of injected\nanomalies of OpenMP multi-threaded applications.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:01:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Wang", "Weidong", ""], ["Luo", "Wangda", ""]]}, {"id": "2011.02999", "submitter": "Kiwan Maeng", "authors": "Kiwan Maeng, Shivam Bharuka, Isabel Gao, Mark C. Jeffrey, Vikram\n  Saraph, Bor-Yiing Su, Caroline Trippel, Jiyan Yang, Mike Rabbat, Brandon\n  Lucia, Carole-Jean Wu", "title": "CPR: Understanding and Improving Failure Tolerant Training for Deep\n  Learning Recommendation with Partial Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes and optimizes a partial recovery training system, CPR, for\nrecommendation models. CPR relaxes the consistency requirement by enabling\nnon-failed nodes to proceed without loading checkpoints when a node fails\nduring training, improving failure-related overheads. The paper is the first to\nthe extent of our knowledge to perform a data-driven, in-depth analysis of\napplying partial recovery to recommendation models and identified a trade-off\nbetween accuracy and performance. Motivated by the analysis, we present CPR, a\npartial recovery training system that can reduce the training time and maintain\nthe desired level of model accuracy by (1) estimating the benefit of partial\nrecovery, (2) selecting an appropriate checkpoint saving interval, and (3)\nprioritizing to save updates of more frequently accessed parameters. Two\nvariants of CPR, CPR-MFU and CPR-SSU, reduce the checkpoint-related overhead\nfrom 8.2-8.5% to 0.53-0.68% compared to full recovery, on a configuration\nemulating the failure pattern and overhead of a production-scale cluster. While\nreducing overhead significantly, CPR achieves model quality on par with the\nmore expensive full recovery scheme, training the state-of-the-art\nrecommendation model using Criteo's Ads CTR dataset. Our preliminary results\nalso suggest that CPR can speed up training on a real production-scale cluster,\nwithout notably degrading the accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:54:35 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Maeng", "Kiwan", ""], ["Bharuka", "Shivam", ""], ["Gao", "Isabel", ""], ["Jeffrey", "Mark C.", ""], ["Saraph", "Vikram", ""], ["Su", "Bor-Yiing", ""], ["Trippel", "Caroline", ""], ["Yang", "Jiyan", ""], ["Rabbat", "Mike", ""], ["Lucia", "Brandon", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2011.03135", "submitter": "Xuhao Chen", "authors": "Xuhao Chen, Roshan Dathathri, Gurbinder Gill, Loc Hoang, Keshav\n  Pingali", "title": "Sandslash: A Two-Level Framework for Efficient Graph Pattern Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph pattern mining (GPM) is used in diverse application areas including\nsocial network analysis, bioinformatics, and chemical engineering. Existing GPM\nframeworks either provide high-level interfaces for productivity at the cost of\nexpressiveness or provide low-level interfaces that can express a wide variety\nof GPM algorithms at the cost of increased programming complexity. Moreover,\nexisting systems lack the flexibility to explore combinations of optimizations\nto achieve performance competitive with hand-optimized applications.\n  We present Sandslash, an in-memory Graph Pattern Mining (GPM) framework that\nuses a novel programming interface to support productive, expressive, and\nefficient GPM on large graphs. Sandslash provides a high-level API that needs\nonly a specification of the GPM problem, and it implements fast subgraph\nenumeration, provides efficient data structures, and applies high-level\noptimizations automatically. To achieve performance competitive with\nexpert-optimized implementations, Sandslash also provides a low-level API that\nallows users to express algorithm-specific optimizations. This enables\nSandslash to support both high-productivity and high-efficiency without losing\nexpressiveness. We evaluate Sandslash on shared-memory machines using five GPM\napplications and a wide range of large real-world graphs. Experimental results\ndemonstrate that applications written using Sandslash high-level or low-level\nAPI outperforms state-of-the-art GPM systems AutoMine, Pangolin, and Peregrine\non average by 13.8x, 7.9x, and 5.4x, respectively. We also show that these\nSandslash applications outperform expert-optimized GPM implementations by 2.3x\non average with less programming effort.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 23:13:29 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Chen", "Xuhao", ""], ["Dathathri", "Roshan", ""], ["Gill", "Gurbinder", ""], ["Hoang", "Loc", ""], ["Pingali", "Keshav", ""]]}, {"id": "2011.03180", "submitter": "Ali Abedi", "authors": "Ali Abedi and Shehroz S. Khan", "title": "FedSL: Federated Split Learning on Distributed Sequential Data in\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) and Split Learning (SL) are privacy-preserving\nMachine-Learning (ML) techniques that enable training ML models over data\ndistributed among clients without requiring direct access to their raw data.\nExisting FL and SL approaches work on horizontally or vertically partitioned\ndata and cannot handle sequentially partitioned data where segments of\nmultiple-segment sequential data are distributed across clients. In this paper,\nwe propose a novel federated split learning framework, FedSL, to train models\non distributed sequential data. The most common ML models to train on\nsequential data are Recurrent Neural Networks (RNNs). Since the proposed\nframework is privacy preserving, segments of multiple-segment sequential data\ncannot be shared between clients or between clients and server. To circumvent\nthis limitation, we propose a novel SL approach tailored for RNNs. A RNN is\nsplit into sub-networks, and each sub-network is trained on one client\ncontaining single segments of multiple-segment training sequences. During local\ntraining, the sub-networks on different clients communicate with each other to\ncapture latent dependencies between consecutive segments of multiple-segment\nsequential data on different clients, but without sharing raw data or complete\nmodel parameters. After training local sub-networks with local sequential data\nsegments, all clients send their sub-networks to a federated server where\nsub-networks are aggregated to generate a global model. The experimental\nresults on simulated and real-world datasets demonstrate that the proposed\nmethod successfully train models on distributed sequential data, while\npreserving privacy, and outperforms previous FL and centralized learning\napproaches in terms of achieving higher accuracy in fewer communication rounds.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:00:39 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Abedi", "Ali", ""], ["Khan", "Shehroz S.", ""]]}, {"id": "2011.03190", "submitter": "Linghao Song", "authors": "Linghao Song, Fan Chen, Xuehai Qian, Hai Li, Yiran Chen", "title": "Low-Cost Floating-Point Processing in ReRAM for Scientific Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ReFloat, a data format and an accelerator architecture, for\nlow-cost and high-performance floating-point processing in ReRAM for scientific\ncomputing. ReFloat reduces bit number for floating-point representation and\nprocessing to achieve a smaller number ofReRAM crossbars and processing cycles\nfor floating-point matrix-vector multiplication. In the ReFloat data format,\nfor scalars of a matrix block, an exponent offset is derived from an optimized\nbase, and then an exponent offset is re-served for each scalar and fraction bit\nnumbers are reduced. The exponent base optimization is enabled by taking\nadvantage of the existence of value locality in real-world matrices. After\ndefining the ReFloat data format, we develop the conversion scheme from default\ndouble-precision floating-point format to ReFloat format, the computation\nprocedure, and the low-cost high-performance floating-point processing\narchitecture in ReRAM. With ReFloat, we find that for all 12 matrices only 3\nbits for matrix exponent, matrix fraction and vector exponent, and 8 or 16 bits\nfor vector fraction are sufficient to ensure convergence on solvers CG and\nBiCGSTAB. It translates to an average speedup of20.10x/24.59x onCG / BiCGSTAB\ncompared with a GPU baseline and an average speedup of18.86x/54.00x on CG /\nBiCGSTAB compared with a state-of-the-art ReRAM-based accelerator for\nscientific computing.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 04:59:25 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 19:35:25 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 22:16:28 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 00:33:28 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Song", "Linghao", ""], ["Chen", "Fan", ""], ["Qian", "Xuehai", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2011.03196", "submitter": "Seonmyeong Bak", "authors": "Seonmyeong Bak, Oscar Hernandez, Mark Gates, Piotr Luszczek and Vivek\n  Sarkar", "title": "Task-Graph Scheduling Extensions for Efficient Synchronization and\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task graphs have been studied for decades as a foundation for scheduling\nirregular parallel applications and incorporated in programming models such as\nOpenMP. While many high-performance parallel libraries are based on task\ngraphs, they also have additional scheduling requirements, such as\nsynchronization from inner levels of data parallelism and internal blocking\ncommunications. In this paper, we extend task-graph scheduling to support\nefficient synchronization and communication within tasks. Our scheduler avoids\ndeadlock and oversubscription of worker threads, and refines victim selection\nto increase the overlap of sibling tasks. To the best of our knowledge, our\napproach is the first to combine gang-scheduling and work-stealing in a single\nruntime. Our approach has been evaluated on the SLATE highperformance linear\nalgebra library. Relative to the LLVM OMP runtime, our runtime demonstrates\nperformance improvements of up to 13.82%, 15.2%, and 36.94% for LU, QR, and\nCholesky, respectively, evaluated across different configurations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 05:40:28 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bak", "Seonmyeong", ""], ["Hernandez", "Oscar", ""], ["Gates", "Mark", ""], ["Luszczek", "Piotr", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2011.03204", "submitter": "Thomas Uram", "authors": "Rafael Vescovi and Hanyu Li and Jeffery Kinnison and Murat Keceli and\n  Misha Salim and Narayanan Kasthuri and Thomas D. Uram and Nicola Ferrier", "title": "Toward an Automated HPC Pipeline for Processing Large Scale Electron\n  Microscopy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a fully modular and scalable software pipeline for processing\nelectron microscope (EM) images of brain slices into 3D visualization of\nindividual neurons and demonstrate an end-to-end segmentation of a large EM\nvolume using a supercomputer. Our pipeline scales multiple packages used by the\nEM community with minimal changes to the original source codes. We tested each\nstep of the pipeline individually, on a workstation, a cluster, and a\nsupercomputer. Furthermore, we can compose workflows from these operations\nusing a Balsam database that can be triggered during the data acquisition or\nwith the use of different front ends and control the granularity of the\npipeline execution. We describe the implementation of our pipeline and\nmodifications required to integrate and scale up existing codes. The modular\nnature of our environment enables diverse research groups to contribute to the\npipeline without disrupting the workflow, i.e. new individual codes can be\neasily integrated for each step on the pipeline.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:13:54 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Vescovi", "Rafael", ""], ["Li", "Hanyu", ""], ["Kinnison", "Jeffery", ""], ["Keceli", "Murat", ""], ["Salim", "Misha", ""], ["Kasthuri", "Narayanan", ""], ["Uram", "Thomas D.", ""], ["Ferrier", "Nicola", ""]]}, {"id": "2011.03206", "submitter": "Gautham Krishna Gudur", "authors": "Gautham Krishna Gudur, Bala Shyamala Balaji, Satheesh K. Perepu", "title": "Resource-Constrained Federated Learning with Heterogeneous Labels and\n  Models", "comments": "6 pages, 5 figures, ACM KDD 2020 (The 3rd International Workshop on\n  Artificial Intelligence of Things - AIoT'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various IoT applications demand resource-constrained machine learning\nmechanisms for different applications such as pervasive healthcare, activity\nmonitoring, speech recognition, real-time computer vision, etc. This\nnecessitates us to leverage information from multiple devices with few\ncommunication overheads. Federated Learning proves to be an extremely viable\noption for distributed and collaborative machine learning. Particularly,\non-device federated learning is an active area of research, however, there are\na variety of challenges in addressing statistical (non-IID data) and model\nheterogeneities. In addition, in this paper we explore a new challenge of\ninterest -- to handle label heterogeneities in federated learning. To this end,\nwe propose a framework with simple $\\alpha$-weighted federated aggregation of\nscores which leverages overlapping information gain across labels, while saving\nbandwidth costs in the process. Empirical evaluation on Animals-10 dataset\n(with 4 labels for effective elucidation of results) indicates an average\ndeterministic accuracy increase of at least ~16.7%. We also demonstrate the\non-device capabilities of our proposed framework by experimenting with\nfederated learning and inference across different iterations on a Raspberry Pi\n2, a single-board computing platform.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:23:47 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Gudur", "Gautham Krishna", ""], ["Balaji", "Bala Shyamala", ""], ["Perepu", "Satheesh K.", ""]]}, {"id": "2011.03208", "submitter": "Leye Wang", "authors": "Leye Wang, Han Yu, Xiao Han", "title": "Federated Crowdsensing: Framework and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsensing is a promising sensing paradigm for smart city applications\n(e.g., traffic and environment monitoring) with the prevalence of smart mobile\ndevices and advanced network infrastructure. Meanwhile, as tasks are performed\nby individuals, privacy protection is one of the key issues in crowdsensing\nsystems. Traditionally, to alleviate users' privacy concerns, noises are added\nto participants' sensitive data (e.g., participants' locations) through\ntechniques such as differential privacy. However, this inevitably results in\nquality loss to the crowdsensing task. Recently, federated learning paradigm\nhas been proposed, which aims to achieve privacy preservation in machine\nlearning while ensuring that the learning quality suffers little or no loss.\nInspired by the federated learning paradigm, this article studies how federated\nlearning may benefit crowdsensing applications. In particular, we first propose\na federated crowdsensing framework, which analyzes the privacy concerns of each\ncrowdsensing stage (i.e., task creation, task assignment, task execution, and\ndata aggregation) and discuss how federated learning techniques may take\neffect. Finally, we summarize key challenges and opportunities in federated\ncrowdsensing.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 06:49:11 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Wang", "Leye", ""], ["Yu", "Han", ""], ["Han", "Xiao", ""]]}, {"id": "2011.03248", "submitter": "Chaochao Chen", "authors": "Longfei Zheng, Jun Zhou, Chaochao Chen, Bingzhe Wu, Li Wang, Benyu\n  Zhang", "title": "ASFGNN: Automated Separated-Federated Graph Neural Network", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have achieved remarkable performance by taking\nadvantage of graph data. The success of GNN models always depends on rich\nfeatures and adjacent relationships. However, in practice, such data are\nusually isolated by different data owners (clients) and thus are likely to be\nNon-Independent and Identically Distributed (Non-IID). Meanwhile, considering\nthe limited network status of data owners, hyper-parameters optimization for\ncollaborative learning approaches is time-consuming in data isolation\nscenarios. To address these problems, we propose an Automated\nSeparated-Federated Graph Neural Network (ASFGNN) learning paradigm. ASFGNN\nconsists of two main components, i.e., the training of GNN and the tuning of\nhyper-parameters. Specifically, to solve the data Non-IID problem, we first\npropose a separated-federated GNN learning model, which decouples the training\nof GNN into two parts: the message passing part that is done by clients\nseparately, and the loss computing part that is learnt by clients federally. To\nhandle the time-consuming parameter tuning problem, we leverage Bayesian\noptimization technique to automatically tune the hyper-parameters of all the\nclients. We conduct experiments on benchmark datasets and the results\ndemonstrate that ASFGNN significantly outperforms the naive federated GNN, in\nterms of both accuracy and parameter-tuning efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 09:21:34 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Zheng", "Longfei", ""], ["Zhou", "Jun", ""], ["Chen", "Chaochao", ""], ["Wu", "Bingzhe", ""], ["Wang", "Li", ""], ["Zhang", "Benyu", ""]]}, {"id": "2011.03262", "submitter": "Akash Kumar", "authors": "Behnaz Ranjbar, Tuan D.A.Nguyen, Alireza Ejlali, and Akash Kumar", "title": "Power-Aware Run-Time Scheduler for Mixed-Criticality Systems on\n  Multi-Core Platform", "comments": null, "journal-ref": null, "doi": "10.1109/TCAD.2020.3033374", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In modern multi-core Mixed-Criticality (MC) systems, a rise in peak power\nconsumption due to parallel execution of tasks with maximum frequency,\nspecially in the overload situation, may lead to thermal issues, which may\naffect the reliability and timeliness of MC systems. Therefore, managing peak\npower consumption has become imperative in multi-core MC systems. In this\nregard, we propose an online peak power and thermal management heuristic for\nmulti-core MC systems. This heuristic reduces the peak power consumption of the\nsystem as much as possible during runtime by exploiting dynamic slack and\nper-cluster Dynamic Voltage and Frequency Scaling (DVFS). Specifically, our\napproach examines multiple tasks ahead to determine the most appropriate one\nfor slack assignment, that has the most impact on the system peak power and\ntemperature. However, changing the frequency and selecting a proper task for\nslack assignment and a proper core for task re-mapping at runtime can be\ntime-consuming and may cause deadline violation which is not admissible for\nhigh-criticality tasks. Therefore, we analyze and then optimize our run-time\nscheduler and evaluate it for various platforms. The proposed approach is\nexperimentally validated on the ODROID-XU3 (DVFS-enabled heterogeneous\nmulti-core platform) with various embedded real-time benchmarks. Results show\nthat our heuristic achieves up to 5.25% reduction in system peak power and\n20.33\\% reduction in maximum temperature compared to an existing method while\nmeeting deadline constraints in different criticality modes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 10:12:57 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Ranjbar", "Behnaz", ""], ["Nguyen", "Tuan D. A.", ""], ["Ejlali", "Alireza", ""], ["Kumar", "Akash", ""]]}, {"id": "2011.03479", "submitter": "Christian B\\\"ohm", "authors": "Christian B\\\"ohm, Claudia Plant", "title": "Massively Parallel Graph Drawing and Representation Learning", "comments": null, "journal-ref": "IEEE BigData 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fully exploit the performance potential of modern multi-core processors,\nmachine learning and data mining algorithms for big data must be parallelized\nin multiple ways. Today's CPUs consist of multiple cores, each following an\nindependent thread of control, and each equipped with multiple arithmetic units\nwhich can perform the same operation on a vector of multiple data objects.\nGraph embedding, i.e. converting the vertices of a graph into numerical vectors\nis a data mining task of high importance and is useful for graph drawing\n(low-dimensional vectors) and graph representation learning (high-dimensional\nvectors). In this paper, we propose MulticoreGEMPE (Graph Embedding by\nMinimizing the Predictive Entropy), an information-theoretic method which can\ngenerate low and high-dimensional vectors. MulticoreGEMPE applies MIMD\n(Multiple Instructions Multiple Data, using OpenMP) and SIMD (Single\nInstructions Multiple Data, using AVX-512) parallelism. We propose general\nideas applicable in other graph-based algorithms like \\emph{vectorized hashing}\nand \\emph{vectorized reduction}. Our experimental evaluation demonstrates the\nsuperiority of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 17:18:14 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["B\u00f6hm", "Christian", ""], ["Plant", "Claudia", ""]]}, {"id": "2011.03602", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Study of Automatic GPU Offloading Method from Various Language\n  Applications", "comments": "6 pages, 1 figure, in Japanese", "journal-ref": null, "doi": null, "report-no": "IEICE Technical Report, SC2020-13, Nov. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, utilization of heterogeneous hardware other than small core\nCPU such as GPU, FPGA or many core CPU is increasing. However, when using\nheterogeneous hardware, barriers of technical skills such as CUDA are high.\nBased on that, I have proposed environment-adaptive software that enables\nautomatic conversion, configuration, and high performance operation of once\nwritten code, according to the hardware to be placed. However, the source\nlanguage for offloading was mainly C/C++ language applications currently, and\nthere was no research for common offloading for various language applications.\nIn this paper, I study a common method for automatically offloading for various\nlanguage applications not only in C language but also in Python and Java.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:28:48 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2011.03605", "submitter": "Sameer Kumar", "authors": "Sameer Kumar and Norm Jouppi", "title": "Highly Available Data Parallel ML training on Mesh Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallel ML models can take several days or weeks to train on several\naccelerators. The long duration of training relies on the cluster of resources\nto be available for the job to keep running for the entire duration. On a mesh\nnetwork this is challenging because failures will create holes in the mesh.\nPackets must be routed around the failed chips for full connectivity. In this\npaper, we present techniques to route gradient summation allreduce traffic\naround failed chips on 2-D meshes. We evaluate performance of our fault\ntolerant allreduce techniques via the MLPerf-v0.7 ResNet-50 and BERT\nbenchmarks. Performance results show minimal impact to training throughput on\n512 and 1024 TPU-v3 chips.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 21:36:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Kumar", "Sameer", ""], ["Jouppi", "Norm", ""]]}, {"id": "2011.03641", "submitter": "Sameer Kumar", "authors": "Sameer Kumar and James Bradbury and Cliff Young and Yu Emma Wang and\n  Anselm Levskaya and Blake Hechtman and Dehao Chen and HyoukJoong Lee and\n  Mehmet Deveci and Naveen Kumar and Pankaj Kanwar and Shibo Wang and Skye\n  Wanderman-Milne and Steve Lacy and Tao Wang and Tayo Oguntebi and Yazhou Zu\n  and Yuanzhong Xu and Andy Swing", "title": "Exploring the limits of Concurrency in ML Training on Google TPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results in language understanding using neural networks have required\ntraining hardware of unprecedentedscale, with thousands of chips cooperating on\na single training run. This paper presents techniques to scaleML models on the\nGoogle TPU Multipod, a mesh with 4096 TPU-v3 chips. We discuss model\nparallelism toovercome scaling limitations from the fixed batch size in data\nparallelism, communication/collective optimizations,distributed evaluation of\ntraining metrics, and host input processing scaling optimizations. These\ntechniques aredemonstrated in both the TensorFlow and JAX programming\nframeworks. We also present performance resultsfrom the recent Google\nsubmission to the MLPerf-v0.7 benchmark contest, achieving record training\ntimes from16 to 28 seconds in four MLPerf models on the Google TPU-v3 Multipod\nmachine.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:18:43 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 02:42:48 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 19:33:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kumar", "Sameer", ""], ["Bradbury", "James", ""], ["Young", "Cliff", ""], ["Wang", "Yu Emma", ""], ["Levskaya", "Anselm", ""], ["Hechtman", "Blake", ""], ["Chen", "Dehao", ""], ["Lee", "HyoukJoong", ""], ["Deveci", "Mehmet", ""], ["Kumar", "Naveen", ""], ["Kanwar", "Pankaj", ""], ["Wang", "Shibo", ""], ["Wanderman-Milne", "Skye", ""], ["Lacy", "Steve", ""], ["Wang", "Tao", ""], ["Oguntebi", "Tayo", ""], ["Zu", "Yazhou", ""], ["Xu", "Yuanzhong", ""], ["Swing", "Andy", ""]]}, {"id": "2011.03649", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager, Kotagiri Ramamohanarao, Rajkumar Buyya", "title": "Thermal Prediction for Efficient Energy Management of Clouds using\n  Machine Learning", "comments": "Under submission at IEEE Transactions on Parallel and Distributed\n  Systems (TPDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal management in the hyper-scale cloud data centers is a critical\nproblem. Increased host temperature creates hotspots which significantly\nincreases cooling cost and affects reliability. Accurate prediction of host\ntemperature is crucial for managing the resources effectively. Temperature\nestimation is a non-trivial problem due to thermal variations in the data\ncenter. Existing solutions for temperature estimation are inefficient due to\ntheir computational complexity and lack of accurate prediction. However,\ndata-driven machine learning methods for temperature prediction is a promising\napproach. In this regard, we collect and study data from a private cloud and\nshow the presence of thermal variations. We investigate several machine\nlearning models to accurately predict the host temperature. Specifically, we\npropose a gradient boosting machine learning model for temperature prediction.\nThe experiment results show that our model accurately predicts the temperature\nwith the average RMSE value of 0.05 or an average prediction error of 2.38\ndegree Celsius, which is 6 degree Celsius less as compared to an existing\ntheoretical model. In addition, we propose a dynamic scheduling algorithm to\nminimize the peak temperature of hosts. The results show that our algorithm\nreduces the peak temperature by 6.5 degree Celsius and consumes 34.5% less\nenergy as compared to the baseline algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 00:55:47 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 03:57:01 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 01:14:59 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Ilager", "Shashikant", ""], ["Ramamohanarao", "Kotagiri", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2011.04050", "submitter": "Nader Bouacida", "authors": "Nader Bouacida, Jiahui Hou, Hui Zang and Xin Liu", "title": "Adaptive Federated Dropout: Improving Communication Efficiency and\n  Generalization for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more regulations tackling users' privacy-sensitive data protection in\nrecent years, access to such data has become increasingly restricted and\ncontroversial. To exploit the wealth of data generated and located at\ndistributed entities such as mobile phones, a revolutionary decentralized\nmachine learning setting, known as Federated Learning, enables multiple clients\nlocated at different geographical locations to collaboratively learn a machine\nlearning model while keeping all their data on-device. However, the scale and\ndecentralization of federated learning present new challenges. Communication\nbetween the clients and the server is considered a main bottleneck in the\nconvergence time of federated learning.\n  In this paper, we propose and study Adaptive Federated Dropout (AFD), a novel\ntechnique to reduce the communication costs associated with federated learning.\nIt optimizes both server-client communications and computation costs by\nallowing clients to train locally on a selected subset of the global model. We\nempirically show that this strategy, combined with existing compression\nmethods, collectively provides up to 57x reduction in convergence time. It also\noutperforms the state-of-the-art solutions for communication efficiency.\nFurthermore, it improves model generalization by up to 1.7%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 18:41:44 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Bouacida", "Nader", ""], ["Hou", "Jiahui", ""], ["Zang", "Hui", ""], ["Liu", "Xin", ""]]}, {"id": "2011.04511", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari and Fabian Kuhn", "title": "Deterministic Distributed Vertex Coloring: Simpler, Faster, and without\n  Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple deterministic distributed algorithm that computes a\n$(\\Delta+1)$-vertex coloring in $O(\\log^2 \\Delta \\cdot \\log n)$ rounds, in any\ngraph with at most $n$ nodes and maximum degree at most $\\Delta$. The algorithm\ncan be implemented with $O(\\log n)$-bit messages. It can also be extended to\nthe more general $(degree+1)$-list coloring problem.\n  Obtaining a polylogarithmic-time deterministic algorithm for\n$(\\Delta+1)$-vertex coloring had remained a central open question in the area\nof distributed graph algorithms since the 1980s, until a recent network\ndecomposition algorithm of Rozho\\v{n} and Ghaffari [STOC '20]. The current\nstate of the art is based on an improved variant of their decomposition, which\nleads to an $O(\\log^5 n)$-round algorithm for $(\\Delta+1)$-vertex coloring.\n  Our coloring algorithm is completely different and considerably simpler and\nfaster. It solves the coloring problem in a direct way, without using network\ndecomposition, by gradually rounding a certain fractional color assignment\nuntil reaching an integral color assignment. Moreover, via the approach of\nChang, Li, and Pettie [STOC '18], this improved deterministic algorithm also\nleads to an improvement in the complexity of randomized algorithms for\n$(\\Delta+1)$-coloring, now reaching the bound of $O(\\log^3\\log n)$ rounds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:42:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2011.04719", "submitter": "Alexander Spiegelman", "authors": "Guy Goren, Yoram Moses, and Alexander Spiegelman", "title": "Probabilistic Indistinguishability and the Quality of Validity in\n  Byzantine Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lower bounds and impossibility results in distributed computing are both\nintellectually challenging and practically important. Hundreds if not thousands\nof proofs appear in the literature, but surprisingly, the vast majority of them\napply to deterministic algorithms only. Probabilistic distributed problems have\nbeen around for at least four decades and receive a lot of attention with the\nemergence of blockchain systems. Nonetheless, we are aware of only a handful of\nrandomized lower bounds.\n  In this paper we provide a formal framework to reason about randomized\ndistributed algorithms. We generalize the notion of indistinguishability, the\nmost useful tool in deterministic lower bounds, to apply to a probabilistic\nsetting. The power of this framework is applied to completely characterize the\nquality of decisions in the randomized multi-valued Consensus problem in an\nasynchronous environment with Byzantine faults. That is, we provide a tight\nbound on the probability of honest parties deciding on a possibly bogus value\nand prove that, in a precise sense, no algorithm can do better.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 19:50:09 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Goren", "Guy", ""], ["Moses", "Yoram", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "2011.04726", "submitter": "Pedro Mendes", "authors": "Pedro Mendes, Maria Casimiro, Paolo Romano, David Garlan", "title": "TrimTuner: Efficient Optimization of Machine Learning Jobs in the Cloud\n  via Sub-Sampling", "comments": "Mascots 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work introduces TrimTuner, the first system for optimizing machine\nlearning jobs in the cloud to exploit sub-sampling techniques to reduce the\ncost of the optimization process while keeping into account user-specified\nconstraints. TrimTuner jointly optimizes the cloud and application-specific\nparameters and, unlike state of the art works for cloud optimization, eschews\nthe need to train the model with the full training set every time a new\nconfiguration is sampled. Indeed, by leveraging sub-sampling techniques and\ndata-sets that are up to 60x smaller than the original one, we show that\nTrimTuner can reduce the cost of the optimization process by up to 50x.\nFurther, TrimTuner speeds-up the recommendation process by 65x with respect to\nstate of the art techniques for hyper-parameter optimization that use\nsub-sampling techniques. The reasons for this improvement are twofold: i) a\nnovel domain specific heuristic that reduces the number of configurations for\nwhich the acquisition function has to be evaluated; ii) the adoption of an\nensemble of decision trees that enables boosting the speed of the\nrecommendation process by one additional order of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:06:28 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Mendes", "Pedro", ""], ["Casimiro", "Maria", ""], ["Romano", "Paolo", ""], ["Garlan", "David", ""]]}, {"id": "2011.04727", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "von Neumann's missing \"Second Draft\": what it should contain", "comments": "5 pages, 4 figures. Accepted to 2020 International Conference on\n  Computational Science and Computational Intelligence, Las Vegas, US, as paper\n  CSCI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing science is based on a computing paradigm that is not valid anymore\nfor today's technological conditions. The reason is that the transmission time\neven inside the processor chip, but especially between the components of the\nsystem, is not negligible anymore. The paper introduces a quantitative measure\nfor dispersion, which is vital for both computing performance and energy\nconsumption, and demonstrates how its value increased with the changing\ntechnology. The temporal behavior (including the dispersion of the commonly\nused synchronization clock time) of computing components has a critical impact\non the system's performance at all levels, as demonstrated from gate-level\noperation to supercomputing. The same effect limits the utility of the\nresearched new materials/effects if the related transfer time cannot be\nproportionally mitigated. von Neumann's model is perfect, but now it is used\noutside of its range of validity. The correct procedure to consider the\ntransfer time for the present technological background is also derived.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 20:07:53 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2011.04893", "submitter": "Nitish K Panigrahy", "authors": "Nitish K. Panigrahy, Prithwish Basu, Philippe Nain, Don Towsley,\n  Ananthram Swami, Kevin S. Chan and Kin K. Leung", "title": "Resource Allocation in One-dimensional Distributed Service Networks with\n  Applications", "comments": "arXiv admin note: text overlap with arXiv:1901.02414", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider assignment policies that allocate resources to users, where both\nresources and users are located on a one-dimensional line. First, we consider\nunidirectional assignment policies that allocate resources only to users\nlocated to their left. We propose the Move to Right (MTR) policy, which scans\nfrom left to right assigning nearest rightmost available resource to a user,\nand contrast it to the Unidirectional Gale-Shapley (UGS) matching policy. While\nboth policies among all unidirectional policies, minimize the expected distance\ntraveled by a request (request distance), MTR is fairer. Moreover, we show that\nwhen user and resource locations are modeled by statistical point processes,\nand resources are allowed to satisfy more than one user, the spatial system\nunder unidirectional policies can be mapped into bulk service queueing systems,\nthus allowing the application of many queueing theory results that yield closed\nform expressions. As we consider a case where different resources can satisfy\ndifferent numbers of users, we also generate new results for bulk service\nqueues. We also consider bidirectional policies where there are no directional\nrestrictions on resource allocation and develop an algorithm for computing the\noptimal assignment which is more efficient than known algorithms in the\nliterature when there are more resources than users. Numerical evaluation of\nperformance of unidirectional and bidirectional allocation schemes yields\ndesign guidelines beneficial for resource placement. \\np{Finally, we present a\nheuristic algorithm, which leverages the optimal dynamic programming scheme for\none-dimensional inputs to obtain approximate solutions to the optimal\nassignment problem for the two-dimensional scenario and empirically yields\nrequest distances within a constant factor of the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 03:35:00 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Panigrahy", "Nitish K.", ""], ["Basu", "Prithwish", ""], ["Nain", "Philippe", ""], ["Towsley", "Don", ""], ["Swami", "Ananthram", ""], ["Chan", "Kevin S.", ""], ["Leung", "Kin K.", ""]]}, {"id": "2011.04902", "submitter": "Maxwell Young", "authors": "William C. Anderton and Trisha Chakraborty and Maxwell Young", "title": "Windowed Backoff Algorithms for WiFi: Theory and Performance under\n  Batched Arrivals", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.09271", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary exponential backoff (BEB) is a decades-old algorithm for coordinating\naccess to a shared channel. In modern networks, BEB plays an important role in\nWiFi (IEEE 802.11) and other wireless communication standards.\n  Despite this track record, well-known theoretical results indicate that under\nbursty traffic BEB yields poor makespan, and superior algorithms are possible.\nTo date, the degree to which these findings impact performance in wireless\nnetworks has not been examined.\n  To address this issue, we investigate one of the strongest cases against BEB:\na single burst batch of packets that simultaneously contend for access to a\nwireless channel. Using Network Simulator 3, we incorporate into IEEE 802.11g\nseveral newer algorithms that, while inspired by BEB, possess makespan\nguarantees that are theoretically superior. Surprisingly, we discover that\nthese newer algorithms underperform BEB.\n  Investigating further, we identify as the culprit a common abstraction\nregarding the cost of collisions. Our experimental results are complemented by\nanalytical arguments that the number of collisions - and not solely makespan -\nis an important metric to optimize. We argue that these findings have\nimplications for the design of backoff algorithms in wireless networks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 22:57:08 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 16:57:49 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Anderton", "William C.", ""], ["Chakraborty", "Trisha", ""], ["Young", "Maxwell", ""]]}, {"id": "2011.04919", "submitter": "Chunchi Liu", "authors": "Chunchi Liu, Minghui Xu, Hechuan Guo, Xiuzhen Cheng, Yinhao Xiao,\n  Dongxiao Yu, Bei Gong, Arkady Yerukhimovich, Shengling Wang and Weifeng Lv", "title": "Tokoin: A Coin-Based Accountable Access Control Scheme for Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of Internet of Things (IoT) applications, IoT devices\ninteract closely with our surrounding environments, bringing us unparalleled\nsmartness and convenience. However, the development of secure IoT solutions is\ngetting a long way lagged behind, making us exposed to common unauthorized\naccesses that may bring malicious attacks and unprecedented danger to our daily\nlife. Overprivilege attack, a widely reported phenomenon in IoT that accesses\nunauthorized or excessive resources, is notoriously hard to prevent, trace and\nmitigate. To tackle this challenge, we propose Tokoin-Based Access Control\n(TBAC), an accountable access control model enabled by blockchain and Trusted\nExecution Environment (TEE) technologies, to offer fine-graininess, strong\nauditability, and access procedure control for IoT. TBAC materializes the\nvirtual access power into a definite-amount and secure cryptographic coin\ntermed \"tokoin\" (token+coin), and manages it using atomic and accountable\nstate-transition functions in a blockchain. We also realize access procedure\ncontrol by mandating every tokoin a fine-grained access policy defining who is\nallowed to do what at when in where by how. The tokoin is peer-to-peer\ntransferable, and can be modified only by the resource owner when necessary. We\nfully implement TBAC with well-studied cryptographic primitives and blockchain\nplatforms and present a readily available APP for regular users. We also\npresent a case study to demonstrate how TBAC is employed to enable autonomous\nin-home cargo delivery while guaranteeing the access policy compliance and home\nowner's physical security by regulating the physical behaviors of the\ndeliveryman.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 05:56:36 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Liu", "Chunchi", ""], ["Xu", "Minghui", ""], ["Guo", "Hechuan", ""], ["Cheng", "Xiuzhen", ""], ["Xiao", "Yinhao", ""], ["Yu", "Dongxiao", ""], ["Gong", "Bei", ""], ["Yerukhimovich", "Arkady", ""], ["Wang", "Shengling", ""], ["Lv", "Weifeng", ""]]}, {"id": "2011.04931", "submitter": "Ang Li", "authors": "Cheng Tan, Chenhao Xie, Tong Geng, Andres Marquez, Antonino Tumeo,\n  Kevin Barker, Ang Li", "title": "ARENA: Asynchronous Reconfigurable Accelerator Ring to Enable\n  Data-Centric Parallel Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The next generation HPC and data centers are likely to be reconfigurable and\ndata-centric due to the trend of hardware specialization and the emergence of\ndata-driven applications. In this paper, we propose ARENA -- an asynchronous\nreconfigurable accelerator ring architecture as a potential scenario on how the\nfuture HPC and data centers will be like. Despite using the coarse-grained\nreconfigurable arrays (CGRAs) as the substrate platform, our key contribution\nis not only the CGRA-cluster design itself, but also the ensemble of a new\narchitecture and programming model that enables asynchronous tasking across a\ncluster of reconfigurable nodes, so as to bring specialized computation to the\ndata rather than the reverse. We presume distributed data storage without\nasserting any prior knowledge on the data distribution. Hardware specialization\noccurs at runtime when a task finds the majority of data it requires are\navailable at the present node. In other words, we dynamically generate\nspecialized CGRA accelerators where the data reside. The asynchronous tasking\nfor bringing computation to data is achieved by circulating the task token,\nwhich describes the data-flow graphs to be executed for a task, among the CGRA\ncluster connected by a fast ring network. Evaluations on a set of HPC and\ndata-driven applications across different domains show that ARENA can provide\nbetter parallel scalability with reduced data movement (53.9%). Compared with\ncontemporary compute-centric parallel models, ARENA can bring on average 4.37x\nspeedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm^2\nchip area under 45nm process technology and can run at 800MHz with on average\n759.8mW power consumption. ARENA also supports the concurrent execution of\nmulti-applications, offering ideal architectural support for future\nhigh-performance parallel computing and data analytics systems.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 06:25:28 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 05:48:46 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tan", "Cheng", ""], ["Xie", "Chenhao", ""], ["Geng", "Tong", ""], ["Marquez", "Andres", ""], ["Tumeo", "Antonino", ""], ["Barker", "Kevin", ""], ["Li", "Ang", ""]]}, {"id": "2011.04981", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Exploring the acceleration of Nekbone on reconfigurable architectures", "comments": "Pre-print of paper accepted to IEEE/ACM International Workshop on\n  Heterogeneous High-performance Reconfigurable Computing (H2RC)", "journal-ref": null, "doi": "10.1109/H2RC51942.2020.00008", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware technological advances are struggling to match scientific ambition,\nand a key question is how we can use the transistors that we already have more\neffectively. This is especially true for HPC, where the tendency is often to\nthrow computation at a problem whereas codes themselves are commonly bound,\nat-least to some extent, by other factors. By redesigning an algorithm and\nmoving from a Von Neumann to dataflow style, then potentially there is more\nopportunity to address these bottlenecks on reconfigurable architectures,\ncompared to more general-purpose architectures.\n  In this paper we explore the porting of Nekbone's AX kernel, a widely popular\nHPC mini-app, to FPGAs using High Level Synthesis via Vitis. Whilst computation\nis an important part of this code, it is also memory bound on CPUs, and a key\nquestion is whether one can ameliorate this by leveraging FPGAs. We first\nexplore optimisation strategies for obtaining good performance, with over a\n4000 times runtime difference between the first and final version of our kernel\non FPGAs. Subsequently, performance and power efficiency of our approach on an\nAlveo U280 are compared against a 24 core Xeon Platinum CPU and NVIDIA V100\nGPU, with the FPGA outperforming the CPU by around four times, achieving almost\nthree quarters the GPU performance, and significantly more power efficient than\nboth. The result of this work is a comparison and set of techniques that both\napply to Nekbone on FPGAs specifically and are also of interest more widely in\naccelerating HPC codes on reconfigurable architectures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:00:23 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2011.04983", "submitter": "Nick Brown", "authors": "Maurice Jamieson, Nick Brown", "title": "Benchmarking micro-core architectures for detecting disasters at the\n  edge", "comments": "Preprint of paper accepted to IEEE/ACM Second International Workshop\n  on the use of HPC for Urgent Decision Making (UrgentHPC)", "journal-ref": null, "doi": "10.1109/UrgentHPC51945.2020.00009", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging real-time data to detect disasters such as wildfires, extreme\nweather, earthquakes, tsunamis, human health emergencies, or global diseases is\nan important opportunity. However, much of this data is generated in the field\nand the volumes involved mean that it is impractical for transmission back to a\ncentral data-centre for processing. Instead, edge devices are required to\ngenerate insights from sensor data streaming in, but an important question\ngiven the severe performance and power constraints that these must operate\nunder is that of the most suitable CPU architecture. One class of device that\nwe believe has a significant role to play here is that of micro-cores, which\ncombine many simple low-power cores in a single chip. However, there are many\nto choose from, and an important question is which is most suited to what\nsituation.\n  This paper presents the Eithne framework, designed to simplify benchmarking\nof micro-core architectures. Three benchmarks, LINPACK, DFT and FFT, have been\nimplemented atop of this framework and we use these to explore the key\ncharacteristics and concerns of common micro-core designs within the context of\noperating on the edge for disaster detection. The result of this work is an\nextensible framework that the community can use help develop and test these\ndevices in the future.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 09:04:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Jamieson", "Maurice", ""], ["Brown", "Nick", ""]]}, {"id": "2011.05022", "submitter": "Xiatian Zhang", "authors": "Xiatian Zhang, Xunshi He, Nan Wang and Rong Chen", "title": "Distributed Learning with Low Communication Cost via Gradient Boosting\n  Untrained Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high-dimensional data, there are huge communication costs for distributed\nGBDT because the communication volume of GBDT is related to the number of\nfeatures. To overcome this problem, we propose a novel gradient boosting\nalgorithm, the Gradient Boosting Untrained Neural Network(GBUN). GBUN ensembles\nthe untrained randomly generated neural network that softly distributes data\nsamples to multiple neuron outputs and dramatically reduces the communication\ncosts for distributed learning. To avoid creating huge neural networks for\nhigh-dimensional data, we extend Simhash algorithm to mimic forward calculation\nof the neural network. Our experiments on multiple public datasets show that\nGBUN is as good as conventional GBDT in terms of prediction accuracy and much\nbetter than it in scaling property for distributed learning. Comparing to\nconventional GBDT varieties, GBUN speeds up the training process up to 13 times\non the cluster with 64 machines, and up to 4614 times on the cluster with\n100KB/s network bandwidth. Therefore, GBUN is not only an efficient distributed\nlearning algorithm but also has great potentials for federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:26:57 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhang", "Xiatian", ""], ["He", "Xunshi", ""], ["Wang", "Nan", ""], ["Chen", "Rong", ""]]}, {"id": "2011.05042", "submitter": "Luan Teylo", "authors": "Luan Teylo, Luciana Arantes, Pierre Sens, L\\'ucia Maria de A. Drummond", "title": "Scheduling Bag-of-Tasks in Clouds using Spot and Burstable Virtual\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Leading Cloud providers offer several types of Virtual Machines (VMs) in\ndiverse contract models, with different guarantees in terms of availability and\nreliability. Among them, the most popular contract models are the on-demand and\nthe spot models. In the former, on-demand VMs are allocated for a fixed cost\nper time unit, and their availability is ensured during the whole execution. On\nthe other hand, in the spot market, VMs are offered with a huge discount when\ncompared to the on-demand VMs, but their availability fluctuates according to\nthe cloud's current demand that can terminate or hibernate a spot VM at any\ntime. Furthermore, in order to cope with workload variations, cloud providers\nhave also introduced the concept of burstable VMs which are able to burst up\ntheir respective baseline CPU performance during a limited period of time with\nan up to 20% discount when compared to an equivalent non-burstable on-demand\nVMs. In the current work, we present the Burst Hibernation-Aware Dynamic\nScheduler (Burst-HADS), a framework that schedules and executes tasks of\nBag-of-Tasks applications with deadline constraints by exploiting spot and\non-demand burstable VMs, aiming at minimizing both the monetary cost and the\nexecution time. Based on ILS metaheuristics, Burst-HADS defines an initial\nscheduling map of tasks to VMs which can then be dynamically altered by\nmigrating tasks of a hibernated spot VM or by performing work-stealing when VMs\nbecome idle. Performance results on Amazon EC2 cloud with different\napplications show that, when compared to a solution that uses only regular\non-demand instances, Burst-HADS reduces the monetary cost of the execution and\nmeet the application deadline even in scenarios with high spot hibernation\nrates. It also reduces the total execution time when compared to a solution\nthat uses only spot and non-burstable on-demand instances.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 11:08:47 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Teylo", "Luan", ""], ["Arantes", "Luciana", ""], ["Sens", "Pierre", ""], ["Drummond", "L\u00facia Maria de A.", ""]]}, {"id": "2011.05066", "submitter": "Yuval Efron", "authors": "Bertie Ancona, Keren Censor-Hillel, Mina Dalirrooyfard, Yuval Efron,\n  Virginia Vassilevska Williams", "title": "Distributed Distance Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diameter, radius and eccentricities are fundamental graph parameters, which\nare extensively studied in various computational settings. Typically, computing\napproximate answers can be much more efficient compared with computing exact\nsolutions. In this paper, we give a near complete characterization of the\ntrade-offs between approximation ratios and round complexity of distributed\nalgorithms for approximating these parameters, with a focus on the weighted and\ndirected variants.\n  Furthermore, we study \\emph{bi-chromatic} variants of these parameters\ndefined on a graph whose vertices are colored either red or blue, and one\nfocuses only on distances for pairs of vertices that are colored differently.\nMotivated by applications in computational geometry, bi-chromatic diameter,\nradius and eccentricities have been recently studied in the sequential setting\n[Backurs et al. STOC'18, Dalirrooyfard et al. ICALP'19]. We provide the first\ndistributed upper and lower bounds for such problems.\n  Our technical contributions include introducing the notion of\n\\emph{approximate pseudo-center}, which extends the \\emph{pseudo-centers} of\n[Choudhary and Gold SODA'20], and presenting an efficient distributed algorithm\nfor computing approximate pseudo-centers. On the lower bound side, our\nconstructions introduce the usage of new functions into the framework of\nreductions from 2-party communication complexity to distributed algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 12:09:00 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 23:44:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ancona", "Bertie", ""], ["Censor-Hillel", "Keren", ""], ["Dalirrooyfard", "Mina", ""], ["Efron", "Yuval", ""], ["Williams", "Virginia Vassilevska", ""]]}, {"id": "2011.05160", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi, Fabrizio Petrini, Hongbo Rong, Andrei Valentin,\n  Carl Ebeling", "title": "Mapping Stencils on Coarse-grained Reconfigurable Spatial Architecture", "comments": "9 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencils represent a class of computational patterns where an output grid\npoint depends on a fixed shape of neighboring points in an input grid. Stencil\ncomputations are prevalent in scientific applications engaging a significant\nportion of supercomputing resources. Therefore, it has been always important to\noptimize stencil programs for the best performance. A rich body of research has\nfocused on optimizing stencil computations on almost all parallel\narchitectures. Stencil applications have regular dependency patterns, inherent\npipeline-parallelism, and plenty of data reuse. This makes these applications a\nperfect match for a coarse-grained reconfigurable spatial architecture (CGRA).\nA CGRA consists of many simple, small processing elements (PEs) connected with\nan on-chip network. Each PE can be configured to execute part of a stencil\ncomputation and all PEs run in parallel; the network can also be configured so\nthat data loaded can be passed from a PE to a neighbor PE directly and thus\nreused by many PEs without register spilling and memory traffic. How to\nefficiently map a stencil computation to a CGRA is the key to performance. In\nthis paper, we show a few unique and generalizable ways of mapping one- and\nmultidimensional stencil computations to a CGRA, fully exploiting the data\nreuse opportunities and parallelism. Our simulation experiments demonstrate\nthat these mappings are efficient and enable the CGRA to outperform\nstate-of-the-art GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:51:20 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:31:46 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""], ["Rong", "Hongbo", ""], ["Valentin", "Andrei", ""], ["Ebeling", "Carl", ""]]}, {"id": "2011.05277", "submitter": "Aymeric Vie", "authors": "Aymeric Vie", "title": "Qualities, challenges and future of genetic algorithms: a literature\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms, computer programs that simulate natural evolution, are\nincreasingly applied across many disciplines. They have been used to solve\nvarious optimisation problems from neural network architecture search to\nstrategic games, and to model phenomena of adaptation and learning. Expertise\non the qualities and drawbacks of this technique is largely scattered across\nthe literature or former, motivating an compilation of this knowledge at the\nlight of the most recent developments of the field. In this review, we present\ngenetic algorithms, their qualities, limitations and challenges, as well as\nsome future development perspectives. Genetic algorithms are capable of\nexploring large and complex spaces of possible solutions, to quickly locate\npromising elements, and provide an adequate modelling tool to describe\nevolutionary systems, from games to economies. They however suffer from high\ncomputation costs, difficult parameter configuration, and crucial\nrepresentation of the solutions. Recent developments such as GPU, parallel and\nquantum computing, conception of powerful parameter control methods, and novel\napproaches in representation strategies, may be keys to overcome those\nlimitations. This compiling review aims at informing practitioners and\nnewcomers in the field alike in their genetic algorithm research, and at\noutlining promising avenues for future research. It highlights the potential\nfor interdisciplinary research associating genetic algorithms to pulse original\ndiscoveries in social sciences, open ended evolution, artificial life and AI.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 17:53:33 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 13:10:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vie", "Aymeric", ""]]}, {"id": "2011.05383", "submitter": "Meghana Madhyastha", "authors": "Meghana Madhyastha, Kunal Lillaney, James Browne, Joshua Vogelstein,\n  Randal Burns", "title": "PACSET (Packed Serialized Trees): Reducing Inference Latency for Tree\n  Ensemble Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present methods to serialize and deserialize tree ensembles that optimize\ninference latency when models are not already loaded into memory. This arises\nwhenever models are larger than memory, but also systematically when models are\ndeployed on low-resource devices, such as in the Internet of Things, or run as\nWeb micro-services where resources are allocated on demand. Our packed\nserialized trees (PACSET) encode reference locality in the layout of a tree\nensemble using principles from external memory algorithms. The layout\ninterleaves correlated nodes across multiple trees, uses leaf cardinality to\ncollocate the nodes on the most popular paths and is optimized for the I/O\nblocksize. The result is that each I/O yields a higher fraction of useful data,\nleading to a 2-6 times reduction in classification latency for interactive\nworkloads.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 20:32:11 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Madhyastha", "Meghana", ""], ["Lillaney", "Kunal", ""], ["Browne", "James", ""], ["Vogelstein", "Joshua", ""], ["Burns", "Randal", ""]]}, {"id": "2011.05422", "submitter": "Steven Kommrusch", "authors": "Steve Kommrusch, Marcos Horro, Louis-No\\\"el Pouchet, Gabriel\n  Rodr\\'iguez, Juan Touri\\~no", "title": "Coherence Traffic in Manycore Processors with Opaque Distributed\n  Directories", "comments": "17 pages, 13 figures, submitted to IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manycore processors feature a high number of general-purpose cores designed\nto work in a multithreaded fashion. Recent manycore processors are kept\ncoherent using scalable distributed directories. A paramount example is the\nIntel Mesh interconnect, which consists of a network-on-chip interconnecting\n\"tiles\", each of which contains computation cores, local caches, and coherence\nmasters. The distributed coherence subsystem must be queried for every\nout-of-tile access, imposing an overhead on memory latency. This paper studies\nthe physical layout of an Intel Knights Landing processor, with a particular\nfocus on the coherence subsystem, and uncovers the pseudo-random mapping\nfunction of physical memory blocks across the pieces of the distributed\ndirectory. Leveraging this knowledge, candidate optimizations to improve memory\nlatency through the minimization of coherence traffic are studied. Although\nthese optimizations do improve memory throughput, ultimately this does not\ntranslate into performance gains due to inherent overheads stemming from the\ncomputational complexity of the mapping functions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 22:10:10 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kommrusch", "Steve", ""], ["Horro", "Marcos", ""], ["Pouchet", "Louis-No\u00ebl", ""], ["Rodr\u00edguez", "Gabriel", ""], ["Touri\u00f1o", "Juan", ""]]}, {"id": "2011.05630", "submitter": "Zihan Liu", "authors": "Zihan Liu and Jingwen Leng and Quan Chen and Chao Li and Wenli Zheng\n  and Li Li and Minyi Guo", "title": "DLFusion: An Auto-Tuning Compiler for Layer Fusion on Deep Neural\n  Network Accelerator", "comments": "To be appeared in ISPA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many hardware vendors have introduced specialized deep neural networks (DNN)\naccelerators owing to their superior performance and efficiency. As such, how\nto generate and optimize the code for the hardware accelerator becomes an\nimportant yet less explored problem. In this paper, we perform the\ncompiler-stage optimization study using a novel and representative Cambricon\nDNN accelerator and demonstrate that the code optimization knobs play an\nimportant role in unleashing the potential of hardware computational\nhorsepower. However, even only two studied code optimization knobs, namely the\nnumber of cores and layer fusion scheme, present an enormous search space that\nprevents the naive brute-force search. This work introduces a joint,\nauto-tuning optimization framework to address this challenge. We first use a\nset of synthesized DNN layers to study the interplay between the hardware\nperformance and layer characteristics. Based on the insights, we extract the\noperation count and feature map channel size as each layer's characteristics\nand derive a joint optimization strategy to decide the performance-optimal core\nnumber and fusion scheme. We evaluate the performance of the proposed approach\nusing a set of representative DNN models and show that it achieves the minimal\nof 3.6x and the maximal of 7.9x performance speedup compared to no optimization\nbaseline. We also show that the achieved speedup is close to the oracle case\nthat is based on a reduced brute-force search but with much less search time.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 08:39:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Liu", "Zihan", ""], ["Leng", "Jingwen", ""], ["Chen", "Quan", ""], ["Li", "Chao", ""], ["Zheng", "Wenli", ""], ["Li", "Li", ""], ["Guo", "Minyi", ""]]}, {"id": "2011.05755", "submitter": "Szu-Chi Chung", "authors": "Szu-Chi Chung, Cheng-Yu Hung, Huei-Lun Siao, Hung-Yi Wu, Wei-Hau Chang\n  and I-Ping Tu", "title": "Cryo-RALib -- a modular library for accelerating alignment in cryo-EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.DC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Thanks to automated cryo-EM and GPU-accelerated processing, single-particle\ncryo-EM has become a rapid structure determination method that permits capture\nof dynamical structures of molecules in solution, which has been recently\ndemonstrated by the determination of COVID-19 spike protein in March, shortly\nafter its breakout in late January 2020. This rapidity is critical for vaccine\ndevelopment in response to emerging pandemic. This explains why a 2D\nclassification approach based on multi-reference alignment (MRA) is not as\npopular as the Bayesian-based approach despite that the former has advantage in\ndifferentiating structural variations under low signal-to-noise ratio. This is\nperhaps because that MRA is a time-consuming process and a modular\nGPU-acceleration library for MRA is lacking. Here, we introduce a library\ncalled Cryo-RALib that expands the functionality of CUDA library used by GPU\nISAC. It contains a GPU-accelerated MRA routine for accelerating MRA-based\nclassification algorithms. In addition, we connect the cryo-EM image analysis\nwith the python data science stack so as to make it easier for users to perform\ndata analysis and visualization. Benchmarking on the TaiWan Computing Cloud\n(TWCC) container shows that our implementation can accelerate the computation\nby one order of magnitude. The library is available at\nhttps://github.com/phonchi/Cryo-RAlib.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 13:15:22 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 05:32:16 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 02:28:45 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 08:48:26 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chung", "Szu-Chi", ""], ["Hung", "Cheng-Yu", ""], ["Siao", "Huei-Lun", ""], ["Wu", "Hung-Yi", ""], ["Chang", "Wei-Hau", ""], ["Tu", "I-Ping", ""]]}, {"id": "2011.05772", "submitter": "Volker Turau", "authors": "Volker Turau", "title": "Synchronous Concurrent Broadcasts for Intermittent Channels with Bounded\n  Capacities", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we extend the recently proposed synchronous broadcast algorithm\namnesiac flooding to the case of intermittent communication channels. In\namnesiac flooding a node forwards a received message in the subsequent round.\nThere are several reasons that render an immediate forward of a message\nimpossible: Higher priority traffic, overloaded channels, etc. We show that\npostponing the forwarding for one or more rounds prevents termination. Our\nextension overcomes this shortcoming while retaining the advantages of the\nalgorithm: Nodes don't need to memorize the reception of a message to guarantee\ntermination and messages are sent at most twice per edge. This extension allows\nto solve more general broadcast tasks such as multi-source broadcasts and\nconcurrent broadcasts for systems with bounded channel capacities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 13:44:50 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Turau", "Volker", ""]]}, {"id": "2011.05925", "submitter": "Saurabh Bagchi", "authors": "Shikhar Suryavansh, Chandan Bothra, Kwang Taik Kim, Mung Chiang,\n  Chunyi Peng, Saurabh Bagchi", "title": "I-BOT: Interference-Based Orchestration of Tasks for Dynamic Unmanaged\n  Edge Computing", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, edge computing has become a popular choice for\nlatency-sensitive applications like facial recognition and augmented reality\nbecause it is closer to the end users compared to the cloud. Although\ninfrastructure providers are working toward creating managed edge networks,\npersonal devices such as laptops and tablets, which are widely available and\nare underutilized, can also be used as potential edge devices. We call such\ndevices Unmanaged Edge Devices (UEDs). Scheduling application tasks on such an\nunmanaged edge system is not straightforward because of three fundamental\nreasons-heterogeneity in the computational capacity of the UEDs, uncertainty in\nthe availability of the UEDs (due to devices leaving the system), and\ninterference among multiple tasks sharing a UED. In this paper, we present\nI-BOT, an interference-based orchestration scheme for latency-sensitive tasks\non an Unmanaged Edge Platform (UEP). It minimizes the completion time of\napplications and is bandwidth efficient. I-BOT brings forth three innovations.\nFirst, it profiles and predicts the interference patterns of the tasks to make\nscheduling decisions. Second, it uses a feedback mechanism to adjust for\nchanges in the computational capacity of the UEDs and a prediction mechanism to\nhandle their sporadic exits. Third, it accounts for input dependence of tasks\nin its scheduling decision (such as, two tasks requiring the same input data).\nTo evaluate I-BOT, we run end-to-end simulations with applications representing\nautonomous driving, composed of multiple tasks. We compare to two basic\nbaselines (random and round-robin) and two state-of-the-arts, Lavea [SEC-2017]\nand Petrel [MSN-2018]. Compared to these baselines, I-BOT significantly reduces\nthe average service time of application tasks. This reduction is more\npronounced in dynamic heterogeneous environments, which would be the case in a\nUEP.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:34:18 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Suryavansh", "Shikhar", ""], ["Bothra", "Chandan", ""], ["Kim", "Kwang Taik", ""], ["Chiang", "Mung", ""], ["Peng", "Chunyi", ""], ["Bagchi", "Saurabh", ""]]}, {"id": "2011.05954", "submitter": "Dibakar Saha", "authors": "Dibakar Saha and Partha Sarathi Mandal", "title": "A Distributed Algorithm for Overlapped Community Detection in\n  Large-Scale Networks", "comments": null, "journal-ref": "2021 International Conference on COMmunication Systems & NETworkS\n  (COMSNETS)", "doi": "10.1109/COMSNETS51098.2021.9352856", "report-no": "10.1109/COMSNETS51098.2021.9352856", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapped community detection in social networks has become an important\nresearch area with the increasing popularity and complexity of the networks.\nMost of the existing solutions are either centralized or parallel algorithms,\nwhich are computationally intensive - require complete knowledge of the entire\nnetworks. But it isn't easy to collect entire network data because the size of\nthe actual networks may be prohibitively large. This may be a result of either\nprivacy concerns or technological impediments. Performing in-network\ncomputation solves both problems utilizing the computational capability of the\nindividual nodes of the network. Simultaneously, nodes communicate and share\ndata with their neighbors via message passing, which may go a long way toward\nmitigating individual nodes' privacy concerns in the network. All the\naforementioned concerns motivated us to design a decentralized or distributed\ntechnique to detect overlapped communities in a large-scale network. It is\ndesirable because this technique does not offer a single point of failure, and\nthe system as a whole can continue to function even when many of the nodes\nfail. In this paper, we address the overlapped community detection problem for\nlarge-scale networks. We present an efficient distributed algorithm, named\nDOCD, to identify the overlapped communities in the network. The DOCD\nalgorithm's efficiency is verified with extensive simulation study on real\nnetwork data such as Dolphin, Zachary karate club, Football club, and Facebook\nego networks. We show that DOCD algorithm is capable of keeping the\nasymptotically same results with the existing classical centralized algorithms\nin terms of community modularity and the number of identified communities. The\nDOCD algorithm can also efficiently identify the overlapped nodes and\noverlapped communities with a small number of rounds of communication and\ncomputation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:18:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Saha", "Dibakar", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "2011.06103", "submitter": "Viska Wei", "authors": "Viska Wei, Nikita Ivkin, Vladimir Braverman, Alexander Szalay", "title": "Sketch and Scale: Geo-distributed tSNE and UMAP", "comments": "IEEE BigData2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.SR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running machine learning analytics over geographically distributed datasets\nis a rapidly arising problem in the world of data management policies ensuring\nprivacy and data security. Visualizing high dimensional data using tools such\nas t-distributed Stochastic Neighbor Embedding (tSNE) and Uniform Manifold\nApproximation and Projection (UMAP) became common practice for data scientists.\nBoth tools scale poorly in time and memory. While recent optimizations showed\nsuccessful handling of 10,000 data points, scaling beyond million points is\nstill challenging. We introduce a novel framework: Sketch and Scale (SnS). It\nleverages a Count Sketch data structure to compress the data on the edge nodes,\naggregates the reduced size sketches on the master node, and runs vanilla tSNE\nor UMAP on the summary, representing the densest areas, extracted from the\naggregated sketch. We show this technique to be fully parallel, scale linearly\nin time, logarithmically in memory, and communication, making it possible to\nanalyze datasets with many millions, potentially billions of data points,\nspread across several data centers around the globe. We demonstrate the power\nof our method on two mid-size datasets: cancer data with 52 million 35-band\npixels from multiple images of tumor biopsies; and astrophysics data of 100\nmillion stars with multi-color photometry from the Sloan Digital Sky Survey\n(SDSS).\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 22:32:21 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Wei", "Viska", ""], ["Ivkin", "Nikita", ""], ["Braverman", "Vladimir", ""], ["Szalay", "Alexander", ""]]}, {"id": "2011.06159", "submitter": "Amani Abusafia", "authors": "Amani Abusafia and Athman Bouguettaya", "title": "Reliability Model for Incentive-Driven IoT Energy Services", "comments": "10 pages, 10 figures, This paper is accepted in the 2020 EAI\n  International Conference on Mobile and Ubiquitous Systems: Computing,\n  Networking and Services (EAI MobiQuitous 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel reliability model for composing energy service requests.\nThe proposed model is based on consumers' behavior and history of energy\nrequests. The reliability model ensures the maximum incentives to providers.\nIncentives are used as a green solution to increase IoT users' participation in\na crowdsourced energy sharing environment. Additionally, adaptive and priority\nscheduling compositions are proposed to compose the most reliable energy\nrequests while maximizing providers' incentives. A set of experiments is\nconducted to evaluate the proposed approaches. Experimental results prove the\nefficiency of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 01:55:34 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 09:09:52 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 01:11:59 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Abusafia", "Amani", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2011.06180", "submitter": "Eric Peterson", "authors": "Eric C. Peterson, Peter J. Karalekas", "title": "aether: Distributed system emulation in Common Lisp", "comments": "Final, for-publication version", "journal-ref": "Proceedings of the 14th European Lisp Symposium (2021)", "doi": "10.5281/zenodo.4713971", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Common Lisp package suitable for the high-level design,\nspecification, simulation, and instrumentation of real-time distributed\nalgorithms and hardware on which to run them. We discuss various design\ndecisions around the package structure, and we explore their consequences with\nsmall examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 03:23:40 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 07:28:54 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Peterson", "Eric C.", ""], ["Karalekas", "Peter J.", ""]]}, {"id": "2011.06201", "submitter": "V Lalitha", "authors": "Divija Swetha Gadiraju, V. Lalitha and Vaneet Aggarwal", "title": "Secure Regenerating Codes for Reducing Storage and Bootstrap Costs in\n  Sharded Blockchains", "comments": "8 pages, accepted for publication in IEEE Blockchain 2020. arXiv\n  admin note: text overlap with arXiv:1906.12140 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a distributed ledger with wide applications. Due to the\nincreasing storage requirement for blockchains, the computation can be afforded\nby only a few miners. Sharding has been proposed to scale blockchains so that\nstorage and transaction efficiency of the blockchain improves at the cost of\nsecurity guarantee. This paper aims to consider a new protocol,\nSecure-Repair-Blockchain (SRB), which aims to decrease the storage cost at the\nminers. In addition, SRB also decreases the bootstrapping cost, which allows\nfor new miners to easily join a sharded blockchain. In order to reduce storage,\ncoding-theoretic techniques are used in SRB. In order to decrease the amount of\ndata that is transferred to the new node joining a shard, the concept of exact\nrepair secure regenerating codes is used. The proposed blockchain protocol\nachieves lower storage than those that do not use coding, and achieves lower\nbootstrapping cost as compared to the different baselines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 17:23:08 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Gadiraju", "Divija Swetha", ""], ["Lalitha", "V.", ""], ["Aggarwal", "Vaneet", ""]]}, {"id": "2011.06223", "submitter": "Saurav Prakash", "authors": "Saurav Prakash, Sagar Dhakal, Mustafa Akdeniz, Yair Yona, Shilpa\n  Talwar, Salman Avestimehr, Nageen Himayat", "title": "Coded Computing for Low-Latency Federated Learning over Wireless Edge\n  Networks", "comments": "Final version to appear in the first issue of the IEEE JSAC Series on\n  Machine Learning for Communications and Networks", "journal-ref": null, "doi": "10.1109/JSAC.2020.3036961", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables training a global model from data located at the\nclient nodes, without data sharing and moving client data to a centralized\nserver. Performance of federated learning in a multi-access edge computing\n(MEC) network suffers from slow convergence due to heterogeneity and stochastic\nfluctuations in compute power and communication link qualities across clients.\nWe propose a novel coded computing framework, CodedFedL, that injects\nstructured coding redundancy into federated learning for mitigating stragglers\nand speeding up the training procedure. CodedFedL enables coded computing for\nnon-linear federated learning by efficiently exploiting distributed kernel\nembedding via random Fourier features that transforms the training task into\ncomputationally favourable distributed linear regression. Furthermore, clients\ngenerate local parity datasets by coding over their local datasets, while the\nserver combines them to obtain the global parity dataset. Gradient from the\nglobal parity dataset compensates for straggling gradients during training, and\nthereby speeds up convergence. For minimizing the epoch deadline time at the\nMEC server, we provide a tractable approach for finding the amount of coding\nredundancy and the number of local data points that a client processes during\ntraining, by exploiting the statistical properties of compute as well as\ncommunication delays. We also characterize the leakage in data privacy when\nclients share their local parity datasets with the server. We analyze the\nconvergence rate and iteration complexity of CodedFedL under simplifying\nassumptions, by treating CodedFedL as a stochastic gradient descent algorithm.\nFurthermore, we conduct numerical experiments using practical network\nparameters and benchmark datasets, where CodedFedL speeds up the overall\ntraining time by up to $15\\times$ in comparison to the benchmark schemes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 06:21:59 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 19:46:31 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Prakash", "Saurav", ""], ["Dhakal", "Sagar", ""], ["Akdeniz", "Mustafa", ""], ["Yona", "Yair", ""], ["Talwar", "Shilpa", ""], ["Avestimehr", "Salman", ""], ["Himayat", "Nageen", ""]]}, {"id": "2011.06283", "submitter": "Dipankar Sarkar", "authors": "Dipankar Sarkar, Ankur Narang, Sumit Rai", "title": "Fed-Focal Loss for imbalanced data classification in Federated Learning", "comments": "Accepted for the Workshop on Federated Learning for Data Privacy and\n  Confidentiality in Conjunction with IJCAI 2020 (FL-IJCAI'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Federated Learning setting has a central server coordinating the training\nof a model on a network of devices. One of the challenges is variable training\nperformance when the dataset has a class imbalance. In this paper, we address\nthis by introducing a new loss function called Fed-Focal Loss. We propose to\naddress the class imbalance by reshaping cross-entropy loss such that it\ndown-weights the loss assigned to well-classified examples along the lines of\nfocal loss. Additionally, by leveraging a tunable sampling framework, we take\ninto account selective client model contributions on the central server to\nfurther focus the detector during training and hence improve its robustness.\nUsing a detailed experimental analysis with the VIRTUAL (Variational Federated\nMulti-Task Learning) approach, we demonstrate consistently superior performance\nin both the balanced and unbalanced scenarios for MNIST, FEMNIST, VSN and HAR\nbenchmarks. We obtain a more than 9% (absolute percentage) improvement in the\nunbalanced MNIST benchmark. We further show that our technique can be adopted\nacross multiple Federated Learning algorithms to get improvements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 09:52:14 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Sarkar", "Dipankar", ""], ["Narang", "Ankur", ""], ["Rai", "Sumit", ""]]}, {"id": "2011.06391", "submitter": "Md. Khaledur Rahman", "authors": "Md. Khaledur Rahman, Majedul Haque Sujon and Ariful Azad", "title": "FusedMM: A Unified SDDMM-SpMM Kernel for Graph Embedding and Graph\n  Neural Networks", "comments": "11 pages, Under review in IPDPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a fused matrix multiplication kernel that unifies sampled\ndense-dense matrix multiplication and sparse-dense matrix multiplication under\na single operation called FusedMM. By using user-defined functions, FusedMM can\ncapture almost all computational patterns needed by popular graph embedding and\nGNN approaches. FusedMM is an order of magnitude faster than its equivalent\nkernels in Deep Graph Library. The superior performance of FusedMM comes from\nthe low-level vectorized kernels, a suitable load balancing scheme and an\nefficient utilization of the memory bandwidth. FusedMM can tune its performance\nusing a code generator and perform equally well on Intel, AMD and ARM\nprocessors. FusedMM speeds up an end-to-end graph embedding algorithm by up to\n28x on different processors.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 18:06:57 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Rahman", "Md. Khaledur", ""], ["Sujon", "Majedul Haque", ""], ["Azad", "Ariful", ""]]}, {"id": "2011.06488", "submitter": "Florian Jacob", "authors": "Florian Jacob, Carolin Beer, Norbert Henze, Hannes Hartenstein", "title": "Analysis of the Matrix Event Graph Replicated Data Type", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3058576", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matrix is a new kind of decentralized, topic-based publish-subscribe\nmiddleware for communication and data storage that is getting popular\nparticularly as a basis for secure instant messaging. In comparison to\ntraditional decentralized communication systems, Matrix replaces pure message\npassing with a replicated data structure. This data structure, which we extract\nand call the Matrix Event Graph (MEG), depicts the causal history of messages.\nWe show that this MEG represents an interesting and important replicated data\ntype for general decentralized applications that are based on causal histories\nof publish-subscribe events: we show that a MEG possesses strong properties\nwith respect to consistency, byzantine attackers, and scalability. First, we\nshow that the MEG provides Strong Eventual Consistency (SEC), and that it is\navailable under partition, by proving that the MEG is a Conflict-Free\nReplicated Data Type for causal histories. While strong consistency is\nimpossible here as shown by the famous CAP theorem, SEC is among the best known\nachievable trade-offs. Second, we discuss the implications of byzantine\nattackers on the data type's properties. We note that the MEG, as it does not\nstrive for consensus, can cope with $n > f$ environments with $n$ total\nparticipants of which $f$ show byzantine faults. Furthermore, we analyze\nscalability: Using Markov chains we study the width of the MEG, defined as the\nnumber of forward extremities, over time and observe an almost optimal\nevolution. We conjecture that this property is inherent to the underlying\nspatially inhomogeneous random walk.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:45:02 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jacob", "Florian", ""], ["Beer", "Carolin", ""], ["Henze", "Norbert", ""], ["Hartenstein", "Hannes", ""]]}, {"id": "2011.06495", "submitter": "Kerem \\\"Ozfatura", "authors": "Kerem Ozfatura and Emre Ozfatura and Deniz Gunduz", "title": "Distributed Sparse SGD with Majority Voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning, particularly variants of distributed stochastic\ngradient descent (DSGD), are widely employed to speed up training by leveraging\ncomputational resources of several workers. However, in practise, communication\ndelay becomes a bottleneck due to the significant amount of information that\nneeds to be exchanged between the workers and the parameter server. One of the\nmost efficient strategies to mitigate the communication bottleneck is top-K\nsparsification. However, top-K sparsification requires additional communication\nload to represent the sparsity pattern, and the mismatch between the sparsity\npatterns of the workers prevents exploitation of efficient communication\nprotocols. To address these issues, we introduce a novel majority voting based\nsparse communication strategy, in which the workers first seek a consensus on\nthe structure of the sparse representation. This strategy provides a\nsignificant reduction in the communication load and allows using the same\nsparsity level in both communication directions. Through extensive simulations\non the CIFAR-10 dataset, we show that it is possible to achieve up to x4000\ncompression without any loss in the test accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:06:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ozfatura", "Kerem", ""], ["Ozfatura", "Emre", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2011.06684", "submitter": "Joseph Schuchart", "authors": "Joseph Schuchart and Christoph Niethammer and Jos\\'e Gracia", "title": "Fibers are not (P)Threads: The Case for Loose Coupling of Asynchronous\n  Programming Models and MPI Through Continuations", "comments": "12 pages, 7 figures Published in proceedings of EuroMPI/USA '20,\n  September 21-24, 2020, Austin, TX, USA", "journal-ref": null, "doi": "10.1145/3416315.3416320", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous programming models (APM) are gaining more and more traction,\nallowing applications to expose the available concurrency to a runtime system\ntasked with coordinating the execution. While MPI has long provided support for\nmulti-threaded communication and non-blocking operations, it falls short of\nadequately supporting APMs as correctly and efficiently handling MPI\ncommunication in different models is still a challenge. Meanwhile, new\nlow-level implementations of light-weight, cooperatively scheduled execution\ncontexts (fibers, aka user-level threads (ULT)) are meant to serve as a basis\nfor higher-level APMs and their integration in MPI implementations has been\nproposed as a replacement for traditional POSIX thread support to alleviate\nthese challenges.\n  In this paper, we first establish a taxonomy in an attempt to clearly\ndistinguish different concepts in the parallel software stack. We argue that\nthe proposed tight integration of fiber implementations with MPI is neither\nwarranted nor beneficial and instead is detrimental to the goal of MPI being a\nportable communication abstraction. We propose MPI Continuations as an\nextension to the MPI standard to provide callback-based notifications on\ncompleted operations, leading to a clear separation of concerns by providing a\nloose coupling mechanism between MPI and APMs. We show that this interface is\nflexible and interacts well with different APMs, namely OpenMP detached tasks,\nOmpSs-2, and Argobots.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:21:16 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Schuchart", "Joseph", ""], ["Niethammer", "Christoph", ""], ["Gracia", "Jos\u00e9", ""]]}, {"id": "2011.06762", "submitter": "Yang Wang", "authors": "Xu Jiang, Nan Guan, Maolin Yang, Yue Tang, Wang Yi", "title": "Schedulability Bounds for Parallel Real-Time Tasks under Global\n  Rate-Monotonic Scheduling", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schedulability bounds not only serve as efficient tests to decide\nschedulability of real-time task systems but also reveal insights about the\nworst-case performance of scheduling algorithms. Different from sequential\nreal-time task systems for which utilization is a suitable metric to develop\nschedulability bounds, schedulability of parallel real-time tasks depends on\nnot only utilization but also the workload graph structure of tasks, which can\nbe well represented by the tensity metric. In this paper, we develop new\nanalysis techniques for parallel real-time task systems under Global\nRate-Monotonic (G-RM) scheduling and obtain new results on schedulability\nbounds based on these two metrics: utilization and tensity. First, we develop\nthe first utilization-tensity bound for G-RM. Second, we improve the capacity\naugmentation bound of G-RM from the best known value 3.73 to 3.18. These\nschedulability bounds not only provide theoretical insights about real-time\nperformance of G-RM, but also serve as highly efficient schedulability tests,\nwhich are particularly suitable to design scenarios in which detailed task\ngraph structures are unknown or may change at run-time. Experiments with\nrandomly generated task sets show that our new results consistently outperform\nthe state-of-the-art with a significant margin under different parameter\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 05:02:43 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Jiang", "Xu", ""], ["Guan", "Nan", ""], ["Yang", "Maolin", ""], ["Tang", "Yue", ""], ["Yi", "Wang", ""]]}, {"id": "2011.06771", "submitter": "Abdallah Lakhdari", "authors": "Abdallah Lakhdari, Athman Bouguettaya, Sajib Mistry, Azadeh Ghari\n  Neiat, Basem Suleiman", "title": "Elastic Composition of Crowdsourced IoT Energy Services", "comments": "10 pages, 8 Figures. This paper is accepted in the 2020 EAI\n  International Conference on Mobile and Ubiquitous Systems: Computing,\n  Networking and Services (EAI MobiQuitous 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel type of service composition, called elastic composition\nwhich provides a reliable framework in a highly fluctuating IoT energy\nprovisioning settings. We rely on crowdsourcing IoT energy (e.g., wearables) to\nprovide wireless energy to nearby devices. We introduce the concepts of soft\ndeadline and hard deadline as key criteria to cater for an elastic composition\nframework. We conduct a set of experiments on real-world datasets to assess the\nefficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 05:57:59 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Lakhdari", "Abdallah", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""], ["Neiat", "Azadeh Ghari", ""], ["Suleiman", "Basem", ""]]}, {"id": "2011.06830", "submitter": "Jiyue Huang", "authors": "Jiyue Huang, Rania Talbi, Zilong Zhao, Sara Boucchenak, Lydia Y. Chen,\n  Stefanie Roos", "title": "An Exploratory Analysis on Users' Contributions in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated Learning is an emerging distributed collaborative learning paradigm\nadopted by many of today's applications, e.g., keyboard prediction and object\nrecognition. Its core principle is to learn from large amount of users data\nwhile preserving data privacy by design as collaborative users only need to\nshare the machine learning models and keep data locally. The main challenge for\nsuch systems is to provide incentives to users to contribute high-quality\nmodels trained from their local data. In this paper, we aim to answer how well\nincentives recognize (in)accurate local models from honest and malicious users,\nand perceive their impacts on the model accuracy of federated learning systems.\nWe first present a thorough survey on two contrasting perspectives: incentive\nmechanisms to measure the contribution of local models by honest users, and\nmalicious users to deliberately degrade the overall model. We conduct\nsimulation experiments to empirically demonstrate if existing contribution\nmeasurement schemes can disclose low-quality models from malicious users. Our\nresults show there exists a clear tradeoff among measurement schemes in terms\nof the computational efficiency and effectiveness to distill the impact of\nmalicious participants. We conclude this paper by discussing the research\ndirections to design resilient contribution incentives.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 09:40:54 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Huang", "Jiyue", ""], ["Talbi", "Rania", ""], ["Zhao", "Zilong", ""], ["Boucchenak", "Sara", ""], ["Chen", "Lydia Y.", ""], ["Roos", "Stefanie", ""]]}, {"id": "2011.06892", "submitter": "Ahmet M. Elbir", "authors": "Ahmet M. Elbir, Sinem Coleri, Kumar Vijay Mishra", "title": "Hybrid Federated and Centralized Learning", "comments": "5pages4figures. This work has been submitted to the IEEE for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the machine learning (ML) tasks are focused on centralized learning\n(CL), which requires the transmission of local datasets from the clients to a\nparameter server (PS) leading to a huge communication overhead. Federated\nlearning (FL) overcomes this issue by allowing the clients to send only the\nmodel updates to the PS instead of the whole dataset. In this way, FL brings\nthe learning to edge level, wherein powerful computational resources are\nrequired on the client side. This requirement may not always be satisfied\nbecause of diverse computational capabilities of edge devices. We address this\nthrough a novel hybrid federated and centralized learning (HFCL) framework to\neffectively train a learning model by exploiting the computational capability\nof the clients. In HFCL, only the clients who have sufficient resources employ\nFL; the remaining clients resort to CL by transmitting their local dataset to\nPS. This allows all the clients to collaborate on the learning process\nregardless of their computational resources. We also propose a sequential data\ntransmission approach with HFCL (HFCL-SDT) to reduce the training duration. The\nproposed HFCL frameworks outperform previously proposed non-hybrid FL (CL)\nbased schemes in terms of learning accuracy (communication overhead) since all\nthe clients collaborate on the learning process with their datasets regardless\nof their computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:11:04 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 20:28:58 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Elbir", "Ahmet M.", ""], ["Coleri", "Sinem", ""], ["Mishra", "Kumar Vijay", ""]]}, {"id": "2011.07179", "submitter": "Huiwen Wu", "authors": "Huiwen Wu and Cen Chen and Li Wang", "title": "A Theoretical Perspective on Differentially Private Federated Multi-task\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of big data, the need to expand the amount of data through data\nsharing to improve model performance has become increasingly compelling. As a\nresult, effective collaborative learning models need to be developed with\nrespect to both privacy and utility concerns. In this work, we propose a new\nfederated multi-task learning method for effective parameter transfer with\ndifferential privacy to protect gradients at the client level. Specifically,\nthe lower layers of the networks are shared across all clients to capture\ntransferable feature representation, while top layers of the network are\ntask-specific for on-client personalization. Our proposed algorithm naturally\nresolves the statistical heterogeneity problem in federated networks. We are,\nto the best of knowledge, the first to provide both privacy and utility\nguarantees for such a proposed federated algorithm. The convergences are proved\nfor the cases with Lipschitz smooth objective functions under the non-convex,\nconvex, and strongly convex settings. Empirical experiment results on different\ndatasets have been conducted to demonstrate the effectiveness of the proposed\nalgorithm and verify the implications of the theoretical findings.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 00:53:16 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wu", "Huiwen", ""], ["Chen", "Cen", ""], ["Wang", "Li", ""]]}, {"id": "2011.07229", "submitter": "Dipankar Sarkar", "authors": "Dipankar Sarkar, Sumit Rai, Ankur Narang", "title": "CatFedAvg: Optimising Communication-efficiency and Classification\n  Accuracy in Federated Learning", "comments": "Supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Federated learning has allowed the training of statistical models over remote\ndevices without the transfer of raw client data. In practice, training in\nheterogeneous and large networks introduce novel challenges in various aspects\nlike network load, quality of client data, security and privacy. Recent works\nin FL have worked on improving communication efficiency and addressing uneven\nclient data distribution independently, but none have provided a unified\nsolution for both challenges. We introduce a new family of Federated Learning\nalgorithms called CatFedAvg which not only improves the communication\nefficiency but improves the quality of learning using a category coverage\nmaximization strategy.\n  We use the FedAvg framework and introduce a simple and efficient step every\nepoch to collect meta-data about the client's training data structure which the\ncentral server uses to request a subset of weight updates. We explore two\ndistinct variations which allow us to further explore the tradeoffs between\ncommunication efficiency and model accuracy. Our experiments based on a vision\nclassification task have shown that an increase of 10% absolute points in\naccuracy using the MNIST dataset with 70% absolute points lower network\ntransfer over FedAvg. We also run similar experiments with Fashion MNIST,\nKMNIST-10, KMNIST-49 and EMNIST-47. Further, under extreme data imbalance\nexperiments for both globally and individual clients, we see the model\nperforming better than FedAvg. The ablation study further explores its\nbehaviour under varying data and client parameter conditions showcasing the\nrobustness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 06:52:02 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Sarkar", "Dipankar", ""], ["Rai", "Sumit", ""], ["Narang", "Ankur", ""]]}, {"id": "2011.07392", "submitter": "Przemys{\\l}aw Uzna\\'nski", "authors": "Leszek G\\k{a}sieniec, Grzegorz Stachowiak, Przemys{\\l}aw Uzna\\'nski", "title": "Time and Space Optimal Exact Majority Population Protocols", "comments": "We combined this paper with arXiv:2012.15800 and have a new updated\n  version at arXiv:2106.10201", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study population protocols governed by the {\\em random\nscheduler}, which uniformly at random selects pairwise interactions between $n$\nagents. The main result of this paper is the first time and space optimal {\\em\nexact majority population protocol} which also works with high probability. The\nnew protocol operates in the optimal {\\em parallel time} $O(\\log n),$ which is\nequivalent to $O(n\\log n)$ sequential {\\em pairwise interactions}, where each\nagent utilises the optimal number of $O(\\log n)$ states.\n  The time optimality of the new majority protocol is possible thanks to the\nnovel concept of fixed-resolution phase clocks introduced and analysed in this\npaper. The new phase clock allows to count approximately constant parallel time\nin population protocols.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 21:23:47 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 15:03:08 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["G\u0105sieniec", "Leszek", ""], ["Stachowiak", "Grzegorz", ""], ["Uzna\u0144ski", "Przemys\u0142aw", ""]]}, {"id": "2011.07405", "submitter": "Dean Leitersdorf", "authors": "Keren Censor-Hillel, Yi-Jun Chang, Fran\\c{c}ois Le Gall, Dean\n  Leitersdorf", "title": "Tight Distributed Listing of Cliques", "comments": "21 pages. To appear in SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much progress has recently been made in understanding the complexity\nlandscape of subgraph finding problems in the CONGEST model of distributed\ncomputing. However, so far, very few tight bounds are known in this area. For\ntriangle (i.e., 3-clique) listing, an optimal $\\tilde{O}(n^{1/3})$-round\ndistributed algorithm has been constructed by Chang et al.~[SODA 2019, PODC\n2019]. Recent works of Eden et al.~[DISC 2019] and of Censor-Hillel et\nal.~[PODC 2020] have shown sublinear algorithms for $K_p$-listing, for each $p\n\\geq 4$, but still leaving a significant gap between the upper bounds and the\nknown lower bounds of the problem.\n  In this paper, we completely close this gap. We show that for each $p \\geq\n4$, there is an $\\tilde{O}(n^{1 - 2/p})$-round distributed algorithm that lists\nall $p$-cliques $K_p$ in the communication network. Our algorithm is\n\\emph{optimal} up to a polylogarithmic factor, due to the $\\tilde{\\Omega}(n^{1\n- 2/p})$-round lower bound of Fischer et al.~[SPAA 2018], which holds even in\nthe CONGESTED CLIQUE model. Together with the triangle-listing algorithm by\nChang et al.~[SODA 2019, PODC 2019], our result thus shows that the round\ncomplexity of $K_p$-listing, for all $p$, is the same in both the CONGEST and\nCONGESTED CLIQUE models, at $\\tilde{\\Theta}(n^{1 - 2/p})$ rounds.\n  For $p=4$, our result additionally matches the $\\tilde{\\Omega}(n^{1/2})$\nlower bound for $K_4$-\\emph{detection} by Czumaj and Konrad [DISC 2018],\nimplying that the round complexities for detection and listing of $K_4$ are\nequivalent in the CONGEST model.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2020 22:44:01 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Chang", "Yi-Jun", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Leitersdorf", "Dean", ""]]}, {"id": "2011.07429", "submitter": "Anbu Huang", "authors": "Anbu Huang", "title": "Dynamic backdoor attacks against federated learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a new machine learning framework, which enables\nmillions of participants to collaboratively train machine learning model\nwithout compromising data privacy and security. Due to the independence and\nconfidentiality of each client, FL does not guarantee that all clients are\nhonest by design, which makes it vulnerable to adversarial attack naturally. In\nthis paper, we focus on dynamic backdoor attacks under FL setting, where the\ngoal of the adversary is to reduce the performance of the model on targeted\ntasks while maintaining a good performance on the main task, current existing\nstudies are mainly focused on static backdoor attacks, that is the poison\npattern injected is unchanged, however, FL is an online learning framework, and\nadversarial targets can be changed dynamically by attacker, traditional\nalgorithms require learning a new targeted task from scratch, which could be\ncomputationally expensive and require a large number of adversarial training\nexamples, to avoid this, we bridge meta-learning and backdoor attacks under FL\nsetting, in which case we can learn a versatile model from previous\nexperiences, and fast adapting to new adversarial tasks with a few of examples.\nWe evaluate our algorithm on different datasets, and demonstrate that our\nalgorithm can achieve good results with respect to dynamic backdoor attacks. To\nthe best of our knowledge, this is the first paper that focus on dynamic\nbackdoor attacks research under FL setting.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 01:32:58 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Huang", "Anbu", ""]]}, {"id": "2011.07447", "submitter": "Qinzi Zhang", "authors": "Qinzi Zhang, Lewis Tseng", "title": "Echo-CGC: A Communication-Efficient Byzantine-tolerant Distributed\n  Machine Learning Algorithm in Single-Hop Radio Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on a popular DML framework -- the parameter server\ncomputation paradigm and iterative learning algorithms that proceed in rounds.\nWe aim to reduce the communication complexity of Byzantine-tolerant DML\nalgorithms in the single-hop radio network. Inspired by the CGC filter\ndeveloped by Gupta and Vaidya, PODC 2020, we propose a gradient descent-based\nalgorithm, Echo-CGC. Our main novelty is a mechanism to utilize the broadcast\nproperties of the radio network to avoid transmitting the raw gradients (full\n$d$-dimensional vectors). In the radio network, each worker is able to overhear\nprevious gradients that were transmitted to the parameter server. Roughly\nspeaking, in Echo-CGC, if a worker \"agrees\" with a combination of prior\ngradients, it will broadcast the \"echo message\" instead of the its raw local\ngradient. The echo message contains a vector of coefficients (of size at most\n$n$) and the ratio of the magnitude between two gradients (a float). In\ncomparison, the traditional approaches need to send $n$ local gradients in each\nround, where each gradient is typically a vector in an ultra-high dimensional\nspace ($d\\gg n$). The improvement on communication complexity of our algorithm\ndepends on multiple factors, including number of nodes, number of faulty\nworkers in an execution, and the cost function. We numerically analyze the\nimprovement, and show that with a large number of nodes, Echo-CGC reduces\n$80\\%$ of the communication under standard assumptions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 04:35:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Zhang", "Qinzi", ""], ["Tseng", "Lewis", ""]]}, {"id": "2011.07516", "submitter": "Jonathan Passerat-Palmbach", "authors": "Harry Cai and Daniel Rueckert and Jonathan Passerat-Palmbach", "title": "2CP: Decentralized Protocols to Transparently Evaluate Contributivity in\n  Blockchain Federated Learning Environments", "comments": null, "journal-ref": "IEEE 2nd International Workshop on Advances in Artificial\n  Intelligence for Blockchain (AIChain 2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Federated Learning harnesses data from multiple sources to build a single\nmodel. While the initial model might belong solely to the actor bringing it to\nthe network for training, determining the ownership of the trained model\nresulting from Federated Learning remains an open question. In this paper we\nexplore how Blockchains (in particular Ethereum) can be used to determine the\nevolving ownership of a model trained with Federated Learning.\n  Firstly, we use the step-by-step evaluation metric to assess the relative\ncontributivities of participants in a Federated Learning process. Next, we\nintroduce 2CP, a framework comprising two novel protocols for Blockchained\nFederated Learning, which both reward contributors with shares in the final\nmodel based on their relative contributivity. The Crowdsource Protocol allows\nan actor to bring a model forward for training, and use their own data to\nevaluate the contributions made to it. Potential trainers are guaranteed a fair\nshare of the resulting model, even in a trustless setting. The Consortium\nProtocol gives trainers the same guarantee even when no party owns the initial\nmodel and no evaluator is available.\n  We conduct experiments with the MNIST dataset that reveal sound\ncontributivity scores resulting from both Protocols by rewarding larger\ndatasets with greater shares in the model. Our experiments also showed the\nnecessity to pair 2CP with a robust model aggregation mechanism to discard low\nquality inputs coming from model poisoning attacks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 12:59:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cai", "Harry", ""], ["Rueckert", "Daniel", ""], ["Passerat-Palmbach", "Jonathan", ""]]}, {"id": "2011.07622", "submitter": "Adam Morrison", "authors": "Daniel Katzan and Adam Morrison", "title": "Recoverable, Abortable, and Adaptive Mutual Exclusion with\n  Sublogarithmic RMR Complexity", "comments": "Full version of OPODIS 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the first recoverable mutual exclusion (RME) algorithm that is\nsimultaneously abortable, adaptive to point contention, and with sublogarithmic\nRMR complexity. Our algorithm has $O(\\min(K,\\log_W N))$ RMR passage complexity\nand $O(F + \\min(K,\\log_W N))$ RMR super-passage complexity, where $K$ is the\nnumber of concurrent processes (point contention), $W$ is the size (in bits) of\nregisters, and $F$ is the number of crashes in a super-passage. Under the\nstandard assumption that $W=\\Theta(\\log N)$, these bounds translate to\nworst-case $O(\\frac{\\log N}{\\log \\log N})$ passage complexity and $O(F +\n\\frac{\\log N}{\\log \\log N})$ super-passage complexity. Our key building blocks\nare:\n  * A $D$-process abortable RME algorithm, for $D \\leq W$, with $O(1)$ passage\ncomplexity and $O(1+F)$ super-passage complexity. We obtain this algorithm by\nusing the Fetch-And-Add (FAA) primitive, unlike prior work on RME that uses\nFetch-And-Store (FAS/SWAP).\n  * A generic transformation that transforms any abortable RME algorithm with\npassage complexity of $B < W$, into an abortable RME lock with passage\ncomplexity of $O(\\min(K,B))$.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 20:24:57 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:58:36 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Katzan", "Daniel", ""], ["Morrison", "Adam", ""]]}, {"id": "2011.07650", "submitter": "Amani Abusafia", "authors": "Abdallah Lakhdari, Amani Abusafia, Athman Bouguettaya", "title": "Crowdsharing Wireless Energy Services", "comments": "7 pages, 4 figures. This paper will be presented in the IEEE\n  International Conference on Collaboration and Internet Computing (IEEE CIC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel self-sustained ecosystem for energy sharing in the IoT\nenvironment. We leverage energy harvesting, wireless power transfer, and\ncrowdsourcing that facilitate the development of an energy crowdsharing\nframework to charge IoT devices. The ubiquity of IoT devices, coupled with the\npotential ability for sharing energy, provides new and exciting opportunities\nto crowdsource wireless energy, thus enabling a green alternative for powering\nIoT devices anytime and anywhere. We discuss the crowdsharing of energy\nservices, open challenges, and proposed solutions.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:01:29 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 09:17:00 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Lakhdari", "Abdallah", ""], ["Abusafia", "Amani", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2011.07764", "submitter": "Navod Thilakarathne", "authors": "N.N.Thilakarathne, Dilani Wickramaaarachchi", "title": "Improved hierarchical role based access control model for cloud\n  computing", "comments": null, "journal-ref": "International Research Conference on Smart Computing and Systems\n  Engineering ( 2018 )", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is considered as the one of the most dominant paradigm in\nfield of information technology which offers on demand cost effective services\nsuch as Software as a service (SAAS), Infrastructure as a service (IAAS) and\nPlatform as a service (PAAS).Promising all these services as it is, this cloud\ncomputing paradigm still associates number of challenges such as data security,\nabuse of cloud services, malicious insider and cyber-attacks. Among all these\nsecurity requirements of cloud computing access control is the one of the\nfundamental requirement in order to avoid unauthorized access to a system and\norganizational assets. Main purpose of this research is to review the existing\nmethods of cloud access control models and their variants pros and cons and to\nidentify further related research directions for developing an improved access\ncontrol model for public cloud data storage. We have presented detailed access\ncontrol requirement analysis for cloud computing and have identified important\ngaps, which are not fulfilled by conventional access control models. As the\noutcome of the study we have come up with an improved access control model with\nhybrid cryptographic schema and hybrid cloud architecture and practical\nimplementation of it. We have tested our model for security implications,\nperformance, functionality and data integrity to prove the validity. We have\nused AES and RSA cryptographic algorithms to implement the cryptographic schema\nand used public and private cloud to enforce our access control security and\nreliability.By validating and testing we have proved that our model can\nwithstand against most of the cyber attacks in real cloud environment. Hence it\nhas improved capabilities compared with other previous access control models\nthat we have reviewed through literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 07:49:32 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Thilakarathne", "N. N.", ""], ["Wickramaaarachchi", "Dilani", ""]]}, {"id": "2011.07807", "submitter": "Aftab Alam Mr.", "authors": "Aftab Alam, Irfan Ullah, Young-Koo Lee", "title": "Video Big Data Analytics in the Cloud: A Reference Architecture, Survey,\n  Opportunities, and Open Research Issues", "comments": null, "journal-ref": "A. Alam, I. Ullah and Y. -K. Lee, \"Video Big Data Analytics in the\n  Cloud: A Reference Architecture, Survey, Opportunities, and Open Research\n  Issues,\" in IEEE Access, vol. 8, pp. 152377-152422, 2020", "doi": "10.1109/ACCESS.2020.3017135", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The proliferation of multimedia devices over the Internet of Things (IoT)\ngenerates an unprecedented amount of data. Consequently, the world has stepped\ninto the era of big data. Recently, on the rise of distributed computing\ntechnologies, video big data analytics in the cloud has attracted the attention\nof researchers and practitioners. The current technology and market trends\ndemand an efficient framework for video big data analytics. However, the\ncurrent work is too limited to provide a complete survey of recent research\nwork on video big data analytics in the cloud, including the management and\nanalysis of a large amount of video data, the challenges, opportunities, and\npromising research directions. To serve this purpose, we present this study,\nwhich conducts a broad overview of the state-of-the-art literature on video big\ndata analytics in the cloud. It also aims to bridge the gap among large-scale\nvideo analytics challenges, big data solutions, and cloud computing. In this\nstudy, we clarify the basic nomenclatures that govern the video analytics\ndomain and the characteristics of video big data while establishing its\nrelationship with cloud computing. We propose a service-oriented layered\nreference architecture for intelligent video big data analytics in the cloud.\nThen, a comprehensive and keen review has been conducted to examine\ncutting-edge research trends in video big data analytics. Finally, we identify\nand articulate several open research issues and challenges, which have been\nraised by the deployment of big data technologies in the cloud for video big\ndata analytics. To the best of our knowledge, this is the first study that\npresents the generalized view of the video big data analytics in the cloud.\nThis paper provides the research studies and technologies advancing video\nanalyses in the era of big data and cloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 09:21:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Alam", "Aftab", ""], ["Ullah", "Irfan", ""], ["Lee", "Young-Koo", ""]]}, {"id": "2011.07846", "submitter": "Na Young Ahn", "authors": "N. Y. Ahn, D.H. Lee", "title": "Secure Vehicle Communications Using Proof-of-Nonce Blockchain", "comments": "6 pages, 4 figures, IEEE Communication Magazine (Submitted on Nov.\n  16, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an autonomous driving that achieves physical layer\nsecurity. Proposed vehicle communication is implemented based on Proof-of-Nonce\n(PoN) blockchain algorithm. PoN blockchain algorithm is a consensus algorithm\nthat can be implemented in light weight. We propose a more secure vehicle\ncommunication scheme while achieving physical layer security by defecting PoN\nalgorithm and secrecy capacity. By generating a block only when secrecy\ncapacity is greater than or equal to the reference value, traffic information\ncan be provided only to vehicles with physical layer security. This vehicle\ncommunication scheme can secure sufficient safety even from hackers based on\nquantum computing.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 10:31:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ahn", "N. Y.", ""], ["Lee", "D. H.", ""]]}, {"id": "2011.07863", "submitter": "Harel Levin", "authors": "Leonid Barenboim and Harel Levin", "title": "Secured Distributed Algorithms Without Hardness Assumptions", "comments": "26 pages. 1 appendix. Short version of the paper was accepted to\n  OPODIS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study algorithms in the distributed message-passing model that produce\nsecured output, for an input graph $G$. Specifically, each vertex computes its\npart in the output, the entire output is correct, but each vertex cannot\ndiscover the output of other vertices, with a certain probability. This is\nmotivated by high-performance processors that are embedded nowadays in a large\nvariety of devices. In such situations, it no longer makes sense, and in many\ncases it is not feasible, to leave the whole processing task to a single\ncomputer or even a group of central computers. As the extensive research in the\ndistributed algorithms field yielded efficient decentralized algorithms for\nmany classic problems, the discussion about the security of distributed\nalgorithms was somewhat neglected. Nevertheless, many protocols and algorithms\nwere devised in the research area of secure multi-party computation problem\n(MPC or SMC). However, the notions and terminology of these protocols are quite\ndifferent than in classic distributed algorithms. As a consequence, the focus\nin those protocols was to work for every function $f$ at the expense of\nincreasing the round complexity, or the necessity of several computational\nassumptions. In this work, we present a novel approach, which rather than\nturning existing algorithms into secure ones, identifies and develops those\nalgorithms that are inherently secure (which means they do not require any\nfurther constructions). This approach yields efficient secure algorithms for\nvarious locality problems, such as coloring, network decomposition, forest\ndecomposition, and a variety of additional labeling problems. Remarkably, our\napproach does not require any hardness assumption, but only a private\nrandomness generator in each vertex. This is in contrast to previously known\ntechniques in this setting that are based on public-key encryption schemes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:01:21 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 08:14:50 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 19:08:53 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Barenboim", "Leonid", ""], ["Levin", "Harel", ""]]}, {"id": "2011.07965", "submitter": "Jonathan Will", "authors": "Jonathan Will, Jonathan Bader, Lauritz Thamsen", "title": "Towards Collaborative Optimization of Cluster Configurations for\n  Distributed Dataflow Jobs", "comments": "6 pages, 7 figures, 1 table; Associated experiment results:\n  https://github.com/dos-group/c3o-experiments ; Appearence in the Proceedings\n  of the 2020 IEEE International Conference on Big Data (Big Data);\n  Presentation at the 4th International Workshop on Benchmarking, Performance\n  Tuning and Optimization for Big Data Applications (BPOD). IEEE. 2020", "journal-ref": "IEEE BigData (2020) 2851-2856", "doi": "10.1109/BigData50022.2020.9377994", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing large datasets with distributed dataflow systems requires the use\nof clusters. Public cloud providers offer a large variety and quantity of\nresources that can be used for such clusters. However, picking the appropriate\nresources in both type and number can often be challenging, as the selected\nconfiguration needs to match a distributed dataflow job's resource demands and\naccess patterns. A good cluster configuration avoids hardware bottlenecks and\nmaximizes resource utilization, avoiding costly overprovisioning.\n  We propose a collaborative approach for finding optimal cluster\nconfigurations based on sharing and learning from historical runtime data of\ndistributed dataflow jobs. Collaboratively shared data can be utilized to\npredict runtimes of future job executions through the use of specialized\nregression models. However, training prediction models on historical runtime\ndata that were produced by different users and in diverse contexts requires the\nmodels to take these contexts into account.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:56:09 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:27:56 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Will", "Jonathan", ""], ["Bader", "Jonathan", ""], ["Thamsen", "Lauritz", ""]]}, {"id": "2011.08250", "submitter": "Tim Hellemans", "authors": "Tim Hellemans and Benny Van Houdt", "title": "Improved Load Balancing in Large Scale Systems using Attained Service\n  Time Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our interest lies in load balancing jobs in large scale systems consisting of\nmultiple dispatchers and FCFS servers. In the absence of any information on job\nsizes, dispatchers typically use queue length information reported by the\nservers to assign incoming jobs. When job sizes are highly variable, using only\nqueue length information is clearly suboptimal and performance can be improved\nif some indication can be provided to the dispatcher about the size of an\nongoing job. In a FCFS server measuring the attained service time of the\nongoing job is easy and servers can therefore report this attained service time\ntogether with the queue length when queried by a dispatcher.\n  In this paper we propose and analyse a variety of load balancing policies\nthat exploit both the queue length and attained service time to assign jobs, as\nwell as policies for which only the attained service time of the job in service\nis used. We present a unified analysis for all these policies in a large scale\nsystem under the usual asymptotic independence assumptions. The accuracy of the\nproposed analysis is illustrated using simulation.\n  We present extensive numerical experiments which clearly indicate that a\nsignificant improvement in waiting (and thus also in response) time may be\nachieved by using the attained service time information on top of the queue\nlength of a server. Moreover, the policies which do not make use of the queue\nlength still provide an improved waiting time for moderately loaded systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:04:21 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 12:58:09 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hellemans", "Tim", ""], ["Van Houdt", "Benny", ""]]}, {"id": "2011.08253", "submitter": "Isaac Sheff", "authors": "Isaac Sheff, Xinwen Wang, Robbert van Renesse, Andrew C. Myers", "title": "Heterogeneous Paxos: Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In distributed systems, a group of $\\textit{learners}$ achieve\n$\\textit{consensus}$ when, by observing the output of some\n$\\textit{acceptors}$, they all arrive at the same value. Consensus is crucial\nfor ordering transactions in failure-tolerant systems. Traditional consensus\nalgorithms are homogeneous in three ways:\n  - all learners are treated equally,\n  - all acceptors are treated equally, and\n  - all failures are treated equally.\n  These assumptions, however, are unsuitable for cross-domain applications,\nincluding blockchains, where not all acceptors are equally trustworthy, and not\nall learners have the same assumptions and priorities. We present the first\nconsensus algorithm to be heterogeneous in all three respects. Learners set\ntheir own mixed failure tolerances over differently trusted sets of acceptors.\nWe express these assumptions in a novel $\\textit{Learner Graph}$, and\ndemonstrate sufficient conditions for consensus. We present\n$\\textit{Heterogeneous Paxos}$: an extension of Byzantine Paxos. Heterogeneous\nPaxos achieves consensus for any viable Learner Graph in best-case three\nmessage sends, which is optimal. We present a proof-of-concept implementation,\nand demonstrate how tailoring for heterogeneous scenarios can save resources\nand latency.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 20:16:34 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 22:40:14 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Sheff", "Isaac", ""], ["Wang", "Xinwen", ""], ["van Renesse", "Robbert", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2011.08281", "submitter": "Aditya Devarakonda", "authors": "Aditya Devarakonda, James Demmel", "title": "Avoiding Communication in Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent (SGD) is one of the most widely used optimization\nmethods for solving various machine learning problems. SGD solves an\noptimization problem by iteratively sampling a few data points from the input\ndata, computing gradients for the selected data points, and updating the\nsolution. However, in a parallel setting, SGD requires interprocess\ncommunication at every iteration. We introduce a new communication-avoiding\ntechnique for solving the logistic regression problem using SGD. This technique\nre-organizes the SGD computations into a form that communicates every $s$\niterations instead of every iteration, where $s$ is a tuning parameter. We\nprove theoretical flops, bandwidth, and latency upper bounds for SGD and its\nnew communication-avoiding variant. Furthermore, we show experimental results\nthat illustrate that the new Communication-Avoiding SGD (CA-SGD) method can\nachieve speedups of up to $4.97\\times$ on a high-performance Infiniband cluster\nwithout altering the convergence behavior or accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:14:39 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Devarakonda", "Aditya", ""], ["Demmel", "James", ""]]}, {"id": "2011.08366", "submitter": "Hiroto Yasumi", "authors": "Hiroto Yasumi, Fukuhito Ooshita, Michiko Inoue, S\\'ebastien Tixeuil", "title": "Uniform Bipartition in the Population Protocol Model with Arbitrary\n  Communication Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the uniform bipartition problem in the population\nprotocol model. This problem aims to divide a population into two groups of\nequal size. In particular, we consider the problem in the context of\n\\emph{arbitrary} communication graphs. As a result, we clarify the solvability\nof the uniform bipartition problem with arbitrary communication graphs when\nagents in the population have designated initial states, under various\nassumptions such as the existence of a base station, symmetry of the protocol,\nand fairness of the execution. When the problem is solvable, we present\nprotocols for uniform bipartition. When global fairness is assumed, the space\ncomplexity of our solutions is tight.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:06:21 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 02:18:22 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Yasumi", "Hiroto", ""], ["Ooshita", "Fukuhito", ""], ["Inoue", "Michiko", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "2011.08373", "submitter": "Gautam Muduganti", "authors": "Saurabh Joshi and Gautam Muduganti", "title": "GPURepair: Automated Repair of GPU Kernels", "comments": "19 pages, 1 algorithm, 3 figures, 22nd International Conference on\n  Verification Model Checking and Abstract Interpretation (VMCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tool for repairing errors in GPU kernels written in\nCUDA or OpenCL due to data races and barrier divergence. Our novel extension to\nprior work can also remove barriers that are deemed unnecessary for\ncorrectness. We implement these ideas in our tool called GPURepair, which uses\nGPUVerify as the verification oracle for GPU kernels. We also extend GPUVerify\nto support CUDA Cooperative Groups, allowing GPURepair to perform inter-block\nsynchronization for CUDA kernels. To the best of our knowledge, GPURepair is\nthe only tool that can propose a fix for intra-block data races and barrier\ndivergence errors for both CUDA and OpenCL kernels and the only tool that fixes\ninter-block data races for CUDA kernels. We perform extensive experiments on\nabout 750 kernels and provide a comparison with prior work. We demonstrate the\nsuperiority of GPURepair through its capability to fix more kernels and its\nunique ability to remove redundant barriers and handle inter-block data races.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:28:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Joshi", "Saurabh", ""], ["Muduganti", "Gautam", ""]]}, {"id": "2011.08381", "submitter": "Minoo Hosseinzadeh", "authors": "Minoo Hosseinzadeh, Andrew Wachal, Hana Khamfroush, Daniel E. Lucani", "title": "Optimal Accuracy-Time Trade-off for Deep Learning Services in Edge\n  Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing demand for computationally intensive services like deep\nlearning tasks, emerging distributed computing platforms such as edge computing\n(EC) systems are becoming more popular. Edge computing systems have shown\npromising results in terms of latency reduction compared to the traditional\ncloud systems. However, their limited processing capacity imposes a trade-off\nbetween the potential latency reduction and the achieved accuracy in\ncomputationally-intensive services such as deep learning-based services. In\nthis paper, we focus on finding the optimal accuracy-time trade-off for running\ndeep learning services in a three-tier EC platform where several deep learning\nmodels with different accuracy levels are available. Specifically, we cast the\nproblem as an Integer Linear Program, where optimal task scheduling decisions\nare made to maximize overall user satisfaction in terms of accuracy-time\ntrade-off. We prove that our problem is NP-hard and then provide a polynomial\nconstant-time greedy algorithm, called GUS, that is shown to attain\nnear-optimal results. Finally, upon vetting our algorithmic solution through\nnumerical experiments and comparison with a set of heuristics, we deploy it on\na test-bed implemented to measure for real-world results. The results of both\nnumerical analysis and real-world implementation show that GUS can outperform\nthe baseline heuristics in terms of the average percentage of satisfied users\nby a factor of at least 50%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 02:39:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hosseinzadeh", "Minoo", ""], ["Wachal", "Andrew", ""], ["Khamfroush", "Hana", ""], ["Lucani", "Daniel E.", ""]]}, {"id": "2011.08451", "submitter": "Vignesh Balaji", "authors": "Vignesh Balaji, Brandon Lucia", "title": "Optimizing Graph Processing and Preprocessing with Hardware Assisted\n  Propagation Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extensive prior research has focused on alleviating the characteristic poor\ncache locality of graph analytics workloads. However, graph pre-processing\ntasks remain relatively unexplored. In many important scenarios, graph\npre-processing tasks can be as expensive as the downstream graph analytics\nkernel. We observe that Propagation Blocking (PB), a software optimization\ndesigned for SpMV kernels, generalizes to many graph analytics kernels as well\nas common pre-processing tasks. In this work, we identify the lingering\ninefficiencies of a PB execution on conventional multicores and propose\narchitecture support to eliminate PB's bottlenecks, further improving the\nperformance gains from PB. Our proposed architecture -- COBRA -- optimizes the\nPB execution of both graph processing and pre-processing alike to provide\nend-to-end speedups of up to 4.6x (3.5x on average).\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:13:52 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Balaji", "Vignesh", ""], ["Lucia", "Brandon", ""]]}, {"id": "2011.08455", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Revising the classic computing paradigm and its technological\n  implementations", "comments": "12 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2006.01128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.ET cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's computing is told to be based on the classic paradigm, proposed by\nvon Neumann, a three-quarter century ago. However, that paradigm was justified\n(for the timing relations of) vacuum tubes only. The technological development\ninvalidated the classic paradigm (but not the model!) and led to catastrophic\nperformance losses in computing systems, from operating gate level to large\nnetworks, including the neuromorphic ones. The paper reviews the critical\npoints of the classic paradigm and scrutinizes the confusion made around it. It\ndiscusses some of the consequences of improper technological implementation,\nfrom the shared media to the parallelized operation. The model is perfect, but\nit is applied outside of its range of validity. The paradigm is extended by\nproviding the \"procedure\" that enables computing science to work with cases\nwhere the transfer time is not negligible apart from processing time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 05:37:26 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2011.08474", "submitter": "Honglin Yuan", "authors": "Honglin Yuan, Manzil Zaheer, Sashank Reddi", "title": "Federated Composite Optimization", "comments": "Accepted to ICML 2021. Code repository see\n  https://github.com/hongliny/FCO-ICML21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a distributed learning paradigm that scales\non-device learning collaboratively and privately. Standard FL algorithms such\nas FedAvg are primarily geared towards smooth unconstrained settings. In this\npaper, we study the Federated Composite Optimization (FCO) problem, in which\nthe loss function contains a non-smooth regularizer. Such problems arise\nnaturally in FL applications that involve sparsity, low-rank, monotonicity, or\nmore general constraints. We first show that straightforward extensions of\nprimal algorithms such as FedAvg are not well-suited for FCO since they suffer\nfrom the \"curse of primal averaging,\" resulting in poor convergence. As a\nsolution, we propose a new primal-dual algorithm, Federated Dual Averaging\n(FedDualAvg), which by employing a novel server dual averaging procedure\ncircumvents the curse of primal averaging. Our theoretical analysis and\nempirical experiments demonstrate that FedDualAvg outperforms the other\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:54:06 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 18:46:15 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 06:32:27 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yuan", "Honglin", ""], ["Zaheer", "Manzil", ""], ["Reddi", "Sashank", ""]]}, {"id": "2011.08538", "submitter": "Faheem Zafar", "authors": "Faheem Zafar, Abid Khan, Saif Ur Rehman Malik, Adeel Anjum, Mansoor\n  Ahmed", "title": "MobChain: Three-Way Collusion Resistance in Witness-Oriented Location\n  Proof Systems Using Distributed Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart devices have accentuated the importance of geolocation information.\nGeolocation identification using smart devices has paved the path for\nincentive-based location-based services (LBS). A location proof is a digital\ncertificate of the geographical location of a user, which can be used to access\nvarious LBS. However, a user full control over a device allows the tampering of\nlocation proof. Initially, to resist false proofs, two-party trusted\ncentralized location proof systems (LPS) were introduced to aid the users in\ngenerating secure location proofs mutually. However, two-party protocols\nsuffered from the collusion attacks by the participants of the protocol.\nConsequently, many witness-oriented LPS have emerged to mitigate collusion\nattacks in two-party protocols. However, witness-oriented LPS presented the\npossibility of three-way collusion attacks (involving the user, location\nauthority, and the witness). The three-way collusion attacks are inevitable in\nall existing witness-oriented schemes. To mitigate the inability to resist\nthree-way collusion of existing schemes, in this paper, we introduce a\ndecentralized consensus protocol called as MobChain, where the selection of a\nwitness and location authority is achieved through a distributed consensus of\nnodes in an underlying P2P network of a private blockchain. The persistent\nprovenance data over the blockchain provides strong security guarantees, as a\nresult, the forging and manipulation become impractical. MobChain provides\nsecure location provenance architecture, relying on decentralized decision\nmaking for the selection of participants of the protocol to resist three-way\ncollusion problem. Our prototype implementation and comparison with the\nstate-of-the-art solutions show that MobChain is computationally efficient,\nhighly available while improving the security of LPS.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 10:12:59 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Zafar", "Faheem", ""], ["Khan", "Abid", ""], ["Malik", "Saif Ur Rehman", ""], ["Anjum", "Adeel", ""], ["Ahmed", "Mansoor", ""]]}, {"id": "2011.08697", "submitter": "Hanqi Guo", "authors": "Hanqi Guo, David Lenz, Jiayi Xu, Xin Liang, Wenbin He, Iulian R.\n  Grindeanu, Han-Wei Shen, Tom Peterka, Todd Munson, Ian Foster", "title": "FTK: A Simplicial Spacetime Meshing Framework for Robust and Scalable\n  Feature Tracking", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2021.3073399", "report-no": "ANL/MCS-P9423-1120", "categories": "cs.GR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Feature Tracking Kit (FTK), a framework that simplifies,\nscales, and delivers various feature-tracking algorithms for scientific data.\nThe key of FTK is our high-dimensional simplicial meshing scheme that\ngeneralizes both regular and unstructured spatial meshes to spacetime while\ntessellating spacetime mesh elements into simplices. The benefits of using\nsimplicial spacetime meshes include (1) reducing ambiguity cases for feature\nextraction and tracking, (2) simplifying the handling of degeneracies using\nsymbolic perturbations, and (3) enabling scalable and parallel processing. The\nuse of simplicial spacetime meshing simplifies and improves the implementation\nof several feature-tracking algorithms for critical points, quantum vortices,\nand isosurfaces. As a software framework, FTK provides end users with\nVTK/ParaView filters, Python bindings, a command line interface, and\nprogramming interfaces for feature-tracking applications. We demonstrate use\ncases as well as scalability studies through both synthetic data and scientific\napplications including Tokamak, fluid dynamics, and superconductivity\nsimulations. We also conduct end-to-end performance studies on the Summit\nsupercomputer. FTK is open-sourced under the MIT license:\nhttps://github.com/hguo/ftk\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 15:24:17 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 03:41:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Guo", "Hanqi", ""], ["Lenz", "David", ""], ["Xu", "Jiayi", ""], ["Liang", "Xin", ""], ["He", "Wenbin", ""], ["Grindeanu", "Iulian R.", ""], ["Shen", "Han-Wei", ""], ["Peterka", "Tom", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""]]}, {"id": "2011.08756", "submitter": "Tiansheng Huang", "authors": "Tiansheng Huang, Weiwei Lin, Li Shen, Keqin Li, and Albert Y. Zomaya", "title": "Stochastic Client Selection for Federated Learning with Volatile Clients", "comments": "20 pages, 7 figures. Under review by IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL), arising as a novel secure learning paradigm, has\nreceived notable attention from the public. In each round of synchronous FL\ntraining, only a fraction of available clients are chosen to participate and\nthe selection decision might have a significant effect on the training\nefficiency, as well as the final model performance. In this paper, we\ninvestigate the client selection problem under a volatile context, in which the\nlocal training of heterogeneous clients is likely to fail due to various kinds\nof reasons and in different levels of frequency. Intuitively, too much training\nfailure might potentially reduce the training efficiency, while too much\nselection on clients with greater stability might introduce bias, and thereby\nresult in degradation of the training effectiveness. To tackle this tradeoff,\nwe in this paper formulate the client selection problem under joint\nconsideration of effective participation and fairness. Further, we propose\nE3CS, a stochastic client selection scheme on the basis of an adversarial\nbandit solution, and we further corroborate its effectiveness by conducting\nreal data-based experiments. According to the experimental results, our\nproposed selection scheme is able to achieve up to 2x faster convergence to a\nfixed model accuracy while maintaining the same level of final model accuracy,\nin comparison to the vanilla selection scheme in FL.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 16:35:24 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 03:09:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Huang", "Tiansheng", ""], ["Lin", "Weiwei", ""], ["Shen", "Li", ""], ["Li", "Keqin", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "2011.08879", "submitter": "Terry Cojean", "authors": "Terry Cojean, Yu-Hsiang \"Mike\" Tsai, Hartwig Anzt", "title": "Ginkgo -- A Math Library designed for Platform Portability", "comments": "Submitted to Parallel Computing Journal (PARCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first associations to software sustainability might be the existence of a\ncontinuous integration (CI) framework; the existence of a testing framework\ncomposed of unit tests, integration tests, and end-to-end tests; and also the\nexistence of software documentation. However, when asking what is a common\ndeathblow for a scientific software product, it is often the lack of platform\nand performance portability. Against this background, we designed the Ginkgo\nlibrary with the primary focus on platform portability and the ability to not\nonly port to new hardware architectures, but also achieve good performance. In\nthis paper we present the Ginkgo library design, radically separating\nalgorithms from hardware-specific kernels forming the distinct hardware\nexecutors, and report our experience when adding execution backends for NVIDIA,\nAMD, and Intel GPUs. We also comment on the different levels of performance\nportability, and the performance we achieved on the distinct hardware backends.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:10:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Cojean", "Terry", ""], ["Tsai", "Yu-Hsiang \"Mike\"", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2011.09017", "submitter": "Dingwen Tao", "authors": "Sian Jin, Guanpeng Li, Shuaiwen Leon Song, Dingwen Tao", "title": "A Novel Memory-Efficient Deep Learning Training Framework via\n  Error-Bounded Lossy Compression", "comments": "11 pages, 11 figures, 1 table, accepted by PPoPP '21 as a poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks (DNNs) are becoming increasingly deeper, wider, and\nnon-linear due to the growing demands on prediction accuracy and analysis\nquality. When training a DNN model, the intermediate activation data must be\nsaved in the memory during forward propagation and then restored for backward\npropagation. However, state-of-the-art accelerators such as GPUs are only\nequipped with very limited memory capacities due to hardware design\nconstraints, which significantly limits the maximum batch size and hence\nperformance speedup when training large-scale DNNs.\n  In this paper, we propose a novel memory-driven high performance DNN training\nframework that leverages error-bounded lossy compression to significantly\nreduce the memory requirement for training in order to allow training larger\nnetworks. Different from the state-of-the-art solutions that adopt image-based\nlossy compressors such as JPEG to compress the activation data, our framework\npurposely designs error-bounded lossy compression with a strict\nerror-controlling mechanism. Specifically, we provide theoretical analysis on\nthe compression error propagation from the altered activation data to the\ngradients, and then empirically investigate the impact of altered gradients\nover the entire training process. Based on these analyses, we then propose an\nimproved lossy compressor and an adaptive scheme to dynamically configure the\nlossy compression error-bound and adjust the training batch size to further\nutilize the saved memory space for additional speedup. We evaluate our design\nagainst state-of-the-art solutions with four popular DNNs and the ImageNet\ndataset. Results demonstrate that our proposed framework can significantly\nreduce the training memory consumption by up to 13.5x and 1.8x over the\nbaseline training and state-of-the-art framework with compression,\nrespectively, with little or no accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 00:47:21 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jin", "Sian", ""], ["Li", "Guanpeng", ""], ["Song", "Shuaiwen Leon", ""], ["Tao", "Dingwen", ""]]}, {"id": "2011.09067", "submitter": "M. Ali Vosoughi", "authors": "M. Ali Vosoughi", "title": "Distributed Injection-Locking in Analog Ising Machines to Solve\n  Combinatorial Optimizations", "comments": "5 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.DC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The oscillator-based Ising machine (OIM) is a network of coupled CMOS\noscillators that solves combinatorial optimization problems. In this paper, the\ndistribution of the injection-locking oscillations throughout the circuit is\nproposed to accelerate the phase-locking of the OIM. The implications of the\nproposed technique theoretically investigated and verified by extensive\nsimulations in EDA tools with a $130~nm$ PTM model. By distributing the\ninjective signal of the super-harmonic oscillator, the speed is increased by\n$219.8\\%$ with negligible increase in the power dissipation and phase-locking\nerror of the device due to the distributed technique.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:34:34 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 05:46:27 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Vosoughi", "M. Ali", ""]]}, {"id": "2011.09073", "submitter": "Masudul Quraishi", "authors": "Masudul Hassan Quraishi, Erfan Bank Tavakoli, Fengbo Ren", "title": "A Survey of System Architectures and Techniques for FPGA Virtualization", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  FPGA accelerators are gaining increasing attention in both cloud and edge\ncomputing because of their hardware flexibility, high computational throughput,\nand low power consumption. However, the design flow of FPGAs often requires\nspecific knowledge of the underlying hardware, which hinders the wide adoption\nof FPGAs by application developers. Therefore, the virtualization of FPGAs\nbecomes extremely important to create a useful abstraction of the hardware\nsuitable for application developers. Such abstraction also enables the sharing\nof FPGA resources among multiple users and accelerator applications, which is\nimportant because, traditionally, FPGAs have been mostly used in single-user,\nsingle-embedded-application scenarios. There are many works in the field of\nFPGA virtualization covering different aspects and targeting different\napplication areas. In this survey, we review the system architectures used in\nthe literature for FPGA virtualization. In addition, we identify the primary\nobjectives of FPGA virtualization, based on which we summarize the techniques\nfor realizing FPGA virtualization. This survey helps researchers to efficiently\nlearn about FPGA virtualization research by providing a comprehensive review of\nthe existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 03:48:42 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 05:17:42 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 04:38:31 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Quraishi", "Masudul Hassan", ""], ["Tavakoli", "Erfan Bank", ""], ["Ren", "Fengbo", ""]]}, {"id": "2011.09107", "submitter": "Levente Csikor PhD", "authors": "Levente Csikor, Vipul Ujawane, Dinil Mon Divakaran", "title": "On the Feasibility and Enhancement of the Tuple Space Explosion Attack\n  against Open vSwitch", "comments": "13 pages + bios in IEEE two-column journal style Submitted only to\n  arXiv!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being a crucial part of networked systems, packet classification has to be\nhighly efficient; however, software switches in cloud environments still face\nperformance challenges. The recently proposed Tuple Space Explosion (TSE)\nattack exploits an algorithmic deficiency in Open vSwitch (OVS). In TSE,\nlegitimate low-rate attack traffic makes the cardinal linear search algorithm\nin the Tuple Space Search (TSS) algorithm to spend an unaffordable time for\nclassifying each packet resulting in a denial-of-service (DoS) for the rest of\nthe users. In this paper, we investigate the feasibility of TSE from multiple\nperspectives. Besides showing that TSE is still efficient in the newer version\nof OVS, we show that when the kernel datapath is compiled from a different\nsource, it can degrade its performance to ~1% of its baseline with less than 1\nMbps attack rate. Finally, we show that TSE is much less effective against\nOVS-DPDK with userspace datapath due to the enhanced ranking process in its TSS\nimplementation. Therefore, we propose TSE 2.0 to defeat the ranking process and\nachieve a complete DoS against OVS-DPDK. Furthermore, we present TSE 2.1, which\nachieves the same goal against OVS-DPDK running on multiple cores without\nsignificantly increasing the attack rate.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 06:13:08 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Csikor", "Levente", ""], ["Ujawane", "Vipul", ""], ["Divakaran", "Dinil Mon", ""]]}, {"id": "2011.09208", "submitter": "Xianyan Jia", "authors": "Ang Wang, Xianyan Jia, Le Jiang, Jie Zhang, Yong Li, Wei Lin", "title": "Whale: A Unified Distributed Training Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data parallelism (DP) has been a common practice to speed up the training\nworkloads for a long time. However, with the increase of data size and model\nsize, DP has become less optimal for most distributed training workloads.\nMoreover, it does not work on models whose parameter size cannot fit into a\nsingle GPU's device memory. To enable and further improve the industrial-level\ngiant model training, we present Whale, a unified distributed training\nframework. It provides comprehensive parallel strategies including data\nparallelism, model parallelism, operator sharding, pipeline, hybrid strategy,\nand automatic parallel strategy. To express complex training strategies\neffectively and efficiently in one framework, Whale IR is designed as the basic\nunit to explore and implement different distributed strategies. Moreover, Whale\nenables automatic parallelism upon using a meta-driven cost model. Whale is\ncompatible with TensorFlow and can easily distribute training tasks by adding a\nfew code lines without changing user model code. To the best of our knowledge,\nWhale is the first work that can support various hybrid distributed strategies\nwithin one framework. In our experiment of Bert Large model, Whale pipeline\nstrategy is 2.32 times faster than Horovod data parallelism (HDP) on 64 GPUs.\nIn a large-scale image classification task (100,000 classes), Whale hybrid\nstrategy, which consists of operator sharding and DP, is 14.8 times faster than\nHDP on 64 GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 10:54:31 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Wang", "Ang", ""], ["Jia", "Xianyan", ""], ["Jiang", "Le", ""], ["Zhang", "Jie", ""], ["Li", "Yong", ""], ["Lin", "Wei", ""]]}, {"id": "2011.09337", "submitter": "Matin Hashemi", "authors": "Alireza Mohammadidoost, Matin Hashemi", "title": "High-Throughput and Memory-Efficient Parallel Viterbi Decoder for\n  Convolutional Codes on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a parallel implementation of Viterbi decoding algorithm.\nViterbi decoder is widely used in many state-of-the-art wireless systems. The\nproposed solution optimizes both throughput and memory usage by applying\noptimizations such as unified kernel implementation and parallel traceback.\nExperimental evaluations show that the proposed solution achieves higher\nthroughput compared to previous GPU-accelerated solutions.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:21:01 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Mohammadidoost", "Alireza", ""], ["Hashemi", "Matin", ""]]}, {"id": "2011.09359", "submitter": "Nicolas Kourtellis Ph.D.", "authors": "Nicolas Kourtellis and Kleomenis Katevas and Diego Perino", "title": "FLaaS: Federated Learning as a Service", "comments": "7 pages, 4 figures, 7 subfigures, 34 references", "journal-ref": "In 1st Workshop on Distributed Machine Learning\n  (DistributedML'20), Dec. 1, 2020, Barcelona, Spain. ACM, New York, NY, USA, 7\n  pages", "doi": "10.1145/3426745.3431337", "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated Learning (FL) is emerging as a promising technology to build\nmachine learning models in a decentralized, privacy-preserving fashion. Indeed,\nFL enables local training on user devices, avoiding user data to be transferred\nto centralized servers, and can be enhanced with differential privacy\nmechanisms. Although FL has been recently deployed in real systems, the\npossibility of collaborative modeling across different 3rd-party applications\nhas not yet been explored. In this paper, we tackle this problem and present\nFederated Learning as a Service (FLaaS), a system enabling different scenarios\nof 3rd-party application collaborative model building and addressing the\nconsequent challenges of permission and privacy management, usability, and\nhierarchical model training. FLaaS can be deployed in different operational\nenvironments. As a proof of concept, we implement it on a mobile phone setting\nand discuss practical implications of results on simulated and real devices\nwith respect to on-device training CPU cost, memory footprint and power\nconsumed per FL model round. Therefore, we demonstrate FLaaS's feasibility in\nbuilding unique or joint FL models across applications for image object\ndetection in a few hours, across 100 devices.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:56:22 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Katevas", "Kleomenis", ""], ["Perino", "Diego", ""]]}, {"id": "2011.09655", "submitter": "Di Chai", "authors": "Di Chai and Leye Wang and Kai Chen and Qiang Yang", "title": "FedEval: A Benchmark System with a Comprehensive Evaluation Model for\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an innovative solution for privacy-preserving machine learning (ML),\nfederated learning (FL) is attracting much attention from research and industry\nareas. While new technologies proposed in the past few years do evolve the FL\narea, unfortunately, the evaluation results presented in these works fall short\nin integrity and are hardly comparable because of the inconsistent evaluation\nmetrics and the lack of a common platform. In this paper, we propose a\ncomprehensive evaluation framework for FL systems. Specifically, we first\nintroduce the ACTPR model, which defines five metrics that cannot be excluded\nin FL evaluation, including Accuracy, Communication, Time efficiency, Privacy,\nand Robustness. Then we design and implement a benchmarking system called\nFedEval, which enables the systematic evaluation and comparison of existing\nworks under consistent experimental conditions. We then provide an in-depth\nbenchmarking study between the two most widely-used FL mechanisms, FedSGD and\nFedAvg. The benchmarking results show that FedSGD and FedAvg both have\nadvantages and disadvantages under the ACTPR model. For example, FedSGD is\nbarely influenced by the none independent and identically distributed (non-IID)\ndata problem, but FedAvg suffers from a decline in accuracy of up to 9% in our\nexperiments. On the other hand, FedAvg is more efficient than FedSGD regarding\ntime consumption and communication. Lastly, we excavate a set of take-away\nconclusions, which are very helpful for researchers in the FL area.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:59:51 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 16:08:13 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Chai", "Di", ""], ["Wang", "Leye", ""], ["Chen", "Kai", ""], ["Yang", "Qiang", ""]]}, {"id": "2011.09676", "submitter": "Benjamin Berg", "authors": "Benjamin Berg, Rein Vesilo, Mor Harchol-Balter", "title": "heSRPT: Parallel Scheduling to Minimize Mean Slowdown", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.09346", "journal-ref": "Performance Evaluation (2020) 102147", "doi": "10.1016/j.peva.2020.102147", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data centers serve workloads which are capable of exploiting\nparallelism. When a job parallelizes across multiple servers it will complete\nmore quickly, but jobs receive diminishing returns from being allocated\nadditional servers. Because allocating multiple servers to a single job is\ninefficient, it is unclear how best to allocate a fixed number of servers\nbetween many parallelizable jobs. This paper provides the first optimal\nallocation policy for minimizing the mean slowdown of parallelizable jobs of\nknown size when all jobs are present at time 0. Our policy provides a simple\nclosed form formula for the optimal allocations at every moment in time.\nMinimizing mean slowdown usually requires favoring short jobs over long ones\n(as in the SRPT policy). However, because parallelizable jobs have sublinear\nspeedup functions, system efficiency is also an issue. System efficiency is\nmaximized by giving equal allocations to all jobs and thus competes with the\ngoal of prioritizing small jobs. Our optimal policy, high-efficiency SRPT\n(heSRPT), balances these competing goals. heSRPT completes jobs according to\ntheir size order, but maintains overall system efficiency by allocating some\nservers to each job at every moment in time. Our results generalize to also\nprovide the optimal allocation policy with respect to mean flow time. Finally,\nwe consider the online case where jobs arrive to the system over time. While\noptimizing mean slowdown in the online setting is even more difficult, we find\nthat heSRPT provides an excellent heuristic policy for the online setting. In\nfact, our simulations show that heSRPT significantly outperforms\nstate-of-the-art allocation policies for parallelizable jobs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 18:56:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Berg", "Benjamin", ""], ["Vesilo", "Rein", ""], ["Harchol-Balter", "Mor", ""]]}, {"id": "2011.09753", "submitter": "Rachid Zennou", "authors": "Rachid Zennou, Ranadeep Biswas, Ahmed Bouajjani, Constantin Enea,\n  Mohammed Erradi", "title": "Checking Causal Consistency of Distributed Databases", "comments": "Extended version of the paper <Checking Causal Consistency of\n  Distributed Databases>. It has been published in the Special issue of\n  NETYS2019 by Computing Journal\n  (https://link.springer.com/article/10.1007/s00607-021-00911-3). It is\n  extended with more than 30% of novel contribution,\n  CFP:https://www.springer.com/journal/607/updates/17646200 . Computing is\n  abstracted and indexed by SCOPUS and DBLP", "journal-ref": null, "doi": "10.1007/s00607-021-00911-3", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The CAP Theorem shows that (strong) Consistency, Availability, and Partition\ntolerance are impossible to be ensured together. Causal consistency is one of\nthe weak consistency models that can be implemented to ensure availability and\npartition tolerance in distributed systems. In this work, we propose a tool to\ncheck automatically the conformance of distributed/concurrent systems\nexecutions to causal consistency models. Our approach consists in reducing the\nproblem of checking if an execution is causally consistent to solving Datalog\nqueries. The reduction is based on complete characterizations of the executions\nviolating causal consistency in terms of the existence of cycles in suitably\ndefined relations between the operations occurring in these executions. We have\nimplemented the reduction in a testing tool for distributed databases, and\ncarried out several experiments on real case studies, showing the efficiency of\nthe suggested approach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 10:10:21 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 16:45:56 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2021 23:40:44 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 09:02:57 GMT"}, {"version": "v5", "created": "Tue, 16 Feb 2021 08:35:45 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zennou", "Rachid", ""], ["Biswas", "Ranadeep", ""], ["Bouajjani", "Ahmed", ""], ["Enea", "Constantin", ""], ["Erradi", "Mohammed", ""]]}, {"id": "2011.09849", "submitter": "Ihab Mohammed", "authors": "Ihab Mohammed, Shadha Tabatabai, Ala Al-Fuqaha, Faissal El Bouanani,\n  Junaid Qadir, Basheer Qolomany, Mohsen Guizani", "title": "Budgeted Online Selection of Candidate IoT Clients to Participate in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML), and Deep Learning (DL) in particular, play a vital\nrole in providing smart services to the industry. These techniques however\nsuffer from privacy and security concerns since data is collected from clients\nand then stored and processed at a central location. Federated Learning (FL),\nan architecture in which model parameters are exchanged instead of client data,\nhas been proposed as a solution to these concerns. Nevertheless, FL trains a\nglobal model by communicating with clients over communication rounds, which\nintroduces more traffic on the network and increases the convergence time to\nthe target accuracy. In this work, we solve the problem of optimizing accuracy\nin stateful FL with a budgeted number of candidate clients by selecting the\nbest candidate clients in terms of test accuracy to participate in the training\nprocess. Next, we propose an online stateful FL heuristic to find the best\ncandidate clients. Additionally, we propose an IoT client alarm application\nthat utilizes the proposed heuristic in training a stateful FL global model\nbased on IoT device type classification to alert clients about unauthorized IoT\ndevices in their environment. To test the efficiency of the proposed online\nheuristic, we conduct several experiments using a real dataset and compare the\nresults against state-of-the-art algorithms. Our results indicate that the\nproposed heuristic outperforms the online random algorithm with up to 27% gain\nin accuracy. Additionally, the performance of the proposed online heuristic is\ncomparable to the performance of the best offline algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 06:32:31 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Mohammed", "Ihab", ""], ["Tabatabai", "Shadha", ""], ["Al-Fuqaha", "Ala", ""], ["Bouanani", "Faissal El", ""], ["Qadir", "Junaid", ""], ["Qolomany", "Basheer", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2011.10014", "submitter": "Fabian Kuhn", "authors": "Salwa Faour and Fabian Kuhn", "title": "Approximate Bipartite Vertex Cover in the CONGEST Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give efficient distributed algorithms for the minimum vertex cover problem\nin bipartite graphs in the CONGEST model. From K\\H{o}nig's theorem, it is well\nknown that in bipartite graphs the size of a minimum vertex cover is equal to\nthe size of a maximum matching. We first show that together with an existing\n$O(n\\log n)$-round algorithm for computing a maximum matching, the constructive\nproof of K\\H{o}nig's theorem directly leads to a deterministic $O(n\\log\nn)$-round CONGEST algorithm for computing a minimum vertex cover. We then show\nthat by adapting the construction, we can also convert an \\emph{approximate}\nmaximum matching into an \\emph{approximate} minimum vertex cover. Given a\n$(1-\\delta)$-approximate matching for some $\\delta>1$, we show that a\n$(1+O(\\delta))$-approximate vertex cover can be computed in time\n$O(D+\\mathrm{poly}(\\frac{\\log n}{\\delta}))$, where $D$ is the diameter of the\ngraph. When combining with known graph clustering techniques, for any\n$\\varepsilon\\in(0,1]$, this leads to a $\\mathrm{poly}(\\frac{\\log\nn}{\\varepsilon})$-time deterministic and also to a slightly faster and simpler\nrandomized $O(\\frac{\\log n}{\\varepsilon^3})$-round CONGEST algorithm for\ncomputing a $(1+\\varepsilon)$-approximate vertex cover in bipartite graphs. For\nconstant $\\varepsilon$, the randomized time complexity matches the $\\Omega(\\log\nn)$ lower bound for computing a $(1+\\varepsilon)$-approximate vertex cover in\nbipartite graphs even in the LOCAL model. Our results are also in contrast to\nthe situation in general graphs, where it is known that computing an optimal\nvertex cover requires $\\tilde{\\Omega}(n^2)$ rounds in the CONGEST model and\nwhere it is not even known how to compute any $(2-\\varepsilon)$-approximation\nin time $o(n^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:28:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Faour", "Salwa", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2011.10170", "submitter": "Dingwen Tao", "authors": "Chengming Zhang, Geng Yuan, Wei Niu, Jiannan Tian, Sian Jin, Donglin\n  Zhuang, Zhe Jiang, Yanzhi Wang, Bin Ren, Shuaiwen Leon Song, Dingwen Tao", "title": "ClickTrain: Efficient and Accurate End-to-End Deep Learning Training via\n  Fine-Grained Architecture-Preserving Pruning", "comments": "12 pages, 15 figures, 2 tables, published by ICS'21", "journal-ref": null, "doi": "10.1145/3447818.3459988", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are becoming increasingly deeper, wider,\nand non-linear because of the growing demand on prediction accuracy and\nanalysis quality. The wide and deep CNNs, however, require a large amount of\ncomputing resources and processing time. Many previous works have studied model\npruning to improve inference performance, but little work has been done for\neffectively reducing training cost. In this paper, we propose ClickTrain: an\nefficient and accurate end-to-end training and pruning framework for CNNs.\nDifferent from the existing pruning-during-training work, ClickTrain provides\nhigher model accuracy and compression ratio via fine-grained\narchitecture-preserving pruning. By leveraging pattern-based pruning with our\nproposed novel accurate weight importance estimation, dynamic pattern\ngeneration and selection, and compiler-assisted computation optimizations,\nClickTrain generates highly accurate and fast pruned CNN models for direct\ndeployment without any extra time overhead, compared with the baseline\ntraining. ClickTrain also reduces the end-to-end time cost of the\npruning-after-training method by up to 2.3X with comparable accuracy and\ncompression ratio. Moreover, compared with the state-of-the-art\npruning-during-training approach, ClickTrain provides significant improvements\nboth accuracy and compression ratio on the tested CNN models and datasets,\nunder similar limited training time.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 01:46:56 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 00:43:17 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 18:55:15 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 03:33:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Chengming", ""], ["Yuan", "Geng", ""], ["Niu", "Wei", ""], ["Tian", "Jiannan", ""], ["Jin", "Sian", ""], ["Zhuang", "Donglin", ""], ["Jiang", "Zhe", ""], ["Wang", "Yanzhi", ""], ["Ren", "Bin", ""], ["Song", "Shuaiwen Leon", ""], ["Tao", "Dingwen", ""]]}, {"id": "2011.10436", "submitter": "Armando Casta\\~neda", "authors": "Hagit Attiya, Armando Casta\\~neda, Sergio Rajsbaum", "title": "Locally Solvable Tasks and the Limitations of Valency Arguments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An elegant strategy for proving impossibility results in distributed\ncomputing was introduced in the celebrated FLP consensus impossibility proof.\nThis strategy is local in nature as at each stage, one configuration of a\nhypothetical protocol for consensus is considered, together with future\nvalencies of possible extensions. This proof strategy has been used in numerous\nsituations related to consensus, leading one to wonder why it has not been used\nin impossibility results of two other well-known tasks: set agreement and\nrenaming. This paper provides an explanation of why impossibility proofs of\nthese tasks have been of a global nature. It shows that a protocol can always\nsolve such tasks locally, in the following sense. Given a configuration and all\nits future valencies, if a single successor configuration is selected, then the\nprotocol can reveal all decisions in this branch of executions, satisfying the\ntask specification. This result is shown for both set agreement and renaming,\nimplying that there are no local impossibility proofs for these tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 14:59:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 21:32:28 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 01:53:03 GMT"}, {"version": "v4", "created": "Wed, 28 Jul 2021 14:29:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Attiya", "Hagit", ""], ["Casta\u00f1eda", "Armando", ""], ["Rajsbaum", "Sergio", ""]]}, {"id": "2011.10643", "submitter": "Abolfazl Hashemi", "authors": "Abolfazl Hashemi, Anish Acharya, Rudrajit Das, Haris Vikalo, Sujay\n  Sanghavi, Inderjit Dhillon", "title": "On the Benefits of Multiple Gossip Steps in Communication-Constrained\n  Decentralized Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In decentralized optimization, it is common algorithmic practice to have\nnodes interleave (local) gradient descent iterations with gossip (i.e.\naveraging over the network) steps. Motivated by the training of large-scale\nmachine learning models, it is also increasingly common to require that\nmessages be {\\em lossy compressed} versions of the local parameters. In this\npaper, we show that, in such compressed decentralized optimization settings,\nthere are benefits to having {\\em multiple} gossip steps between subsequent\ngradient iterations, even when the cost of doing so is appropriately accounted\nfor e.g. by means of reducing the precision of compressed information. In\nparticular, we show that having $O(\\log\\frac{1}{\\epsilon})$ gradient iterations\n{with constant step size} - and $O(\\log\\frac{1}{\\epsilon})$ gossip steps\nbetween every pair of these iterations - enables convergence to within\n$\\epsilon$ of the optimal value for smooth non-convex objectives satisfying\nPolyak-\\L{}ojasiewicz condition. This result also holds for smooth strongly\nconvex objectives. To our knowledge, this is the first work that derives\nconvergence results for nonconvex optimization under arbitrary communication\ncompression.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:17:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Hashemi", "Abolfazl", ""], ["Acharya", "Anish", ""], ["Das", "Rudrajit", ""], ["Vikalo", "Haris", ""], ["Sanghavi", "Sujay", ""], ["Dhillon", "Inderjit", ""]]}, {"id": "2011.10645", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Study of Resource Amount Configuration for Automatic Application\n  Offloading", "comments": "6 pages, 1 figure, in Japanese", "journal-ref": null, "doi": null, "report-no": "IEICE Technical Report, SWIM2020-11, Nov. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, utilization of heterogeneous hardware other than small core\nCPU such as GPU, FPGA or many core CPU is increasing. However, when using\nheterogeneous hardware, barriers of technical skills such as OpenMP, CUDA and\nOpenCL are high. Based on that, I have proposed environment-adaptive software\nthat enables automatic conversion, configuration, and high performance\noperation of once written code, according to the hardware to be placed.\nHowever, although the conversion of the code according to the migration\ndestination environment has been studied so far, there has been no research to\nproperly set the resource amount. In this paper, as a new element of\nenvironment adaptive software, in order to operate the application with high\ncost performance, I study a method to optimize the resource amount of CPUs and\noffload devices.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:20:03 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2011.10894", "submitter": "Jaeyoung Song Dr.", "authors": "Jaeyoung Song and Marios Kountouris", "title": "Wireless Distributed Edge Learning: How Many Edge Devices Do We Need?", "comments": "IEEE Journal on Selected Areas in Communications (JSAC), accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed machine learning at the wireless edge, where a\nparameter server builds a global model with the help of multiple wireless edge\ndevices that perform computations on local dataset partitions. Edge devices\ntransmit the result of their computations (updates of current global model) to\nthe server using a fixed rate and orthogonal multiple access over an error\nprone wireless channel. In case of a transmission error, the undelivered packet\nis retransmitted until successfully decoded at the receiver. Leveraging on the\nfundamental tradeoff between computation and communication in distributed\nsystems, our aim is to derive how many edge devices are needed to minimize the\naverage completion time while guaranteeing convergence. We provide upper and\nlower bounds for the average completion and we find a necessary condition for\nadding edge devices in two asymptotic regimes, namely the large dataset and the\nhigh accuracy regime. Conducted experiments on real datasets and numerical\nresults confirm our analysis and substantiate our claim that the number of edge\ndevices should be carefully selected for timely distributed edge learning.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 00:14:42 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Song", "Jaeyoung", ""], ["Kountouris", "Marios", ""]]}, {"id": "2011.10896", "submitter": "Masudul Quraishi", "authors": "Michael Riera, Erfan Bank Tavakoli, Masudul Hassan Quraishi, Fengbo\n  Ren", "title": "HALO 1.0: A Hardware-agnostic Accelerator Orchestration Framework for\n  Enabling Hardware-agnostic Programming with True Performance Portability for\n  Heterogeneous HPC", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents HALO 1.0, an open-ended extensible multi-agent software\nframework that implements a set of proposed hardware-agnostic accelerator\norchestration (HALO) principles. HALO implements a novel compute-centric\nmessage passing interface (C^2MPI) specification for enabling the\nperformance-portable execution of a hardware-agnostic host application across\nheterogeneous accelerators. The experiment results of evaluating eight widely\nused HPC subroutines based on Intel Xeon E5-2620 CPUs, Intel Arria 10 GX FPGAs,\nand NVIDIA GeForce RTX 2080 Ti GPUs show that HALO 1.0 allows for a unified\ncontrol flow for host programs to run across all the computing devices with a\nconsistently top performance portability score, which is up to five orders of\nmagnitude higher than the OpenCL-based solution.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 00:25:55 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 04:27:01 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 00:56:23 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Riera", "Michael", ""], ["Tavakoli", "Erfan Bank", ""], ["Quraishi", "Masudul Hassan", ""], ["Ren", "Fengbo", ""]]}, {"id": "2011.10981", "submitter": "Pratik Ratadiya", "authors": "Pratik Ratadiya, Khushi Asawa, Omkar Nikhal", "title": "A decentralized aggregation mechanism for training deep learning models\n  using smart contract system for bank loan prediction", "comments": "Accepted at the Workshop on AI and Blockchains at the 29th\n  International Joint Conference on Artificial Intelligence (IJCAI-PRICAI),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data privacy and sharing has always been a critical issue when trying to\nbuild complex deep learning-based systems to model data. Facilitation of a\ndecentralized approach that could take benefit from data across multiple nodes\nwhile not needing to merge their data contents physically has been an area of\nactive research. In this paper, we present a solution to benefit from a\ndistributed data setup in the case of training deep learning architectures by\nmaking use of a smart contract system. Specifically, we propose a mechanism\nthat aggregates together the intermediate representations obtained from local\nANN models over a blockchain. Training of local models takes place on their\nrespective data. The intermediate representations derived from them, when\ncombined and trained together on the host node, helps to get a more accurate\nsystem. While federated learning primarily deals with the same features of data\nwhere the number of samples being distributed on multiple nodes, here we are\ndealing with the same number of samples but with their features being\ndistributed on multiple nodes. We consider the task of bank loan prediction\nwherein the personal details of an individual and their bank-specific details\nmay not be available at the same place. Our aggregation mechanism helps to\ntrain a model on such existing distributed data without having to share and\nconcatenate together the actual data values. The obtained performance, which is\nbetter than that of individual nodes, and is at par with that of a centralized\ndata setup makes a strong case for extending our technique across other\narchitectures and tasks. The solution finds its application in organizations\nthat want to train deep learning models on vertically partitioned data.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 10:47:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ratadiya", "Pratik", ""], ["Asawa", "Khushi", ""], ["Nikhal", "Omkar", ""]]}, {"id": "2011.11012", "submitter": "Mohammad Reza Samsami", "authors": "Mohammad Reza Samsami, Hossein Alimadad", "title": "Distributed Deep Reinforcement Learning: An Overview", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning (DRL) is a very active research area. However,\nseveral technical and scientific issues require to be addressed, amongst which\nwe can mention data inefficiency, exploration-exploitation trade-off, and\nmulti-task learning. Therefore, distributed modifications of DRL were\nintroduced; agents that could be run on many machines simultaneously. In this\narticle, we provide a survey of the role of the distributed approaches in DRL.\nWe overview the state of the field, by studying the key research works that\nhave a significant impact on how we can use distributed methods in DRL. We\nchoose to overview these papers, from the perspective of distributed learning,\nand not the aspect of innovations in reinforcement learning algorithms. Also,\nwe evaluate these methods on different tasks and compare their performance with\neach other and with single actor and learner agents.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 13:24:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Samsami", "Mohammad Reza", ""], ["Alimadad", "Hossein", ""]]}, {"id": "2011.11082", "submitter": "Gerald Pao", "authors": "Wassapon Watanakeesuntorn (1), Keichi Takahashi (1), Kohei Ichikawa\n  (1), Joseph Park (2), George Sugihara (3), Ryousei Takano (4), Jason Haga\n  (4), Gerald M. Pao (5) ((1) Nara Institute of Science and Technology, Nara,\n  Japan, (2) U.S. Department of the Interior, Florida, USA, (3) University of\n  California San Diego, California, USA, (4) National Institute of Advanced\n  Industrial Science and Technology, Tsukuba, Japan,(5) Salk Institute for\n  Biological Studies, California, USA)", "title": "Massively Parallel Causal Inference of Whole Brain Dynamics at Single\n  Neuron Resolution", "comments": "10 pges, 10 figures, accepted at IEEE International Conference on\n  Parallel and Distributed Systems (ICPADS)2020, corresponding authors: Keichi\n  Takahashi, Gerald M Pao", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Dynamic Modeling (EDM) is a nonlinear time series causal inference\nframework. The latest implementation of EDM, cppEDM, has only been used for\nsmall datasets due to computational cost. With the growth of data collection\ncapabilities, there is a great need to identify causal relationships in large\ndatasets. We present mpEDM, a parallel distributed implementation of EDM\noptimized for modern GPU-centric supercomputers. We improve the original\nalgorithm to reduce redundant computation and optimize the implementation to\nfully utilize hardware resources such as GPUs and SIMD units. As a use case, we\nrun mpEDM on AI Bridging Cloud Infrastructure (ABCI) using datasets of an\nentire animal brain sampled at single neuron resolution to identify dynamical\ncausation patterns across the brain. mpEDM is 1,530 X faster than cppEDM and a\ndataset containing 101,729 neuron was analyzed in 199 seconds on 512 nodes.\nThis is the largest EDM causal inference achieved to date.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 18:33:31 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Watanakeesuntorn", "Wassapon", ""], ["Takahashi", "Keichi", ""], ["Ichikawa", "Kohei", ""], ["Park", "Joseph", ""], ["Sugihara", "George", ""], ["Takano", "Ryousei", ""], ["Haga", "Jason", ""], ["Pao", "Gerald M.", ""]]}, {"id": "2011.11097", "submitter": "Songze Li", "authors": "Songze Li, David Tse", "title": "TaiJi: Longest Chain Availability with BFT Fast Confirmation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state machine replication protocols are either based on the 40-years-old\nByzantine Fault Tolerance (BFT) theory or the more recent Nakamoto's longest\nchain design. Longest chain protocols, designed originally in the Proof-of-Work\n(PoW) setting, are available under dynamic participation, but has probabilistic\nconfirmation with long latency dependent on the security parameter. BFT\nprotocols, designed for the permissioned setting, has fast deterministic\nconfirmation, but assume a fixed number of nodes always online. We present a\nnew construction which combines a longest chain protocol and a BFT protocol to\nget the best of both worlds. Using this construction, we design TaiJi, the\nfirst dynamically available PoW protocol which has almost deterministic\nconfirmation with latency independent of the security parameter. In contrast to\nprevious hybrid approaches which use a single longest chain to sample\nparticipants to run a BFT protocol, our native PoW construction uses many\nindependent longest chains to sample propose actions and vote actions for the\nBFT protocol. This design enables TaiJi to inherit the full dynamic\navailability of Bitcoin, as well as its full unpredictability, making it secure\nagainst fully-adaptive adversaries with up to 50% of online hash power.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 20:21:20 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Li", "Songze", ""], ["Tse", "David", ""]]}, {"id": "2011.11118", "submitter": "Wayes Tushar", "authors": "Wayes Tushar, Chau Yuen, Tapan Saha, Thomas Morstyn, Archie Chapman,\n  M. Jan E Alam, Sarmad Hanif, and H. Vincent Poor", "title": "Peer-to-Peer Energy Systems for Connected Communities: A Review of\n  Recent Advances and Emerging Challenges", "comments": "33 pages, 6 figures, 4 tables", "journal-ref": "Applied Energy 2021", "doi": "10.1016/j.apenergy.2020.116131", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  After a century of relative stability of the electricity industry, extensive\ndeployment of distributed energy resources and recent advances in computation\nand communication technologies have changed the nature of how we consume,\ntrade, and apply energy. The power system is facing a transition from its\ntraditional hierarchical structure to a more deregulated model by introducing\nnew energy distribution models such as peer-to-peer sharing for connected\ncommunities. The proven effectiveness of P2P sharing in benefiting both\nprosumers and the grid has been demonstrated in many studies and pilot\nprojects. However, there is still no extensive implementation of such sharing\nmodels in today's electricity markets. This paper aims to shed some light on\nthis gap through a comprehensive overview of recent advances in the P2P energy\nsystem and an insightful discussion of the challenges that need to be addressed\nin order to establish P2P sharing as a viable energy management option in\ntoday's electricity market. To this end, in this article, we provide some\nbackground on different aspects of P2P sharing. Then, we discuss advances in\nP2P sharing through a systematic domain-based classification. We also review\ndifferent pilot projects on P2P sharing across the globe. Finally, we identify\nand discuss a number of challenges that need to be addressed for scaling up P2P\nsharing in the electricity market followed by concluding remarks at the end of\nthe paper.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 22:02:49 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Tushar", "Wayes", ""], ["Yuen", "Chau", ""], ["Saha", "Tapan", ""], ["Morstyn", "Thomas", ""], ["Chapman", "Archie", ""], ["Alam", "M. Jan E", ""], ["Hanif", "Sarmad", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2011.11160", "submitter": "Hong Lin", "authors": "Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu", "title": "LINDT: Tackling Negative Federated Learning with Local Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a promising distributed learning paradigm, which\nallows a number of data owners (also called clients) to collaboratively learn a\nshared model without disclosing each client's data. However, FL may fail to\nproceed properly, amid a state that we call negative federated learning (NFL).\nThis paper addresses the problem of negative federated learning. We formulate a\nrigorous definition of NFL and analyze its essential cause. We propose a novel\nframework called LINDT for tackling NFL in run-time. The framework can\npotentially work with any neural-network-based FL systems for NFL detection and\nrecovery. Specifically, we introduce a metric for detecting NFL from the\nserver. On occasion of NFL recovery, the framework makes adaptation to the\nfederated model on each client's local data by learning a Layer-wise\nIntertwined Dual-model. Experiment results show that the proposed approach can\nsignificantly improve the performance of FL on local data in various scenarios\nof NFL.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 01:31:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lin", "Hong", ""], ["Shou", "Lidan", ""], ["Chen", "Ke", ""], ["Chen", "Gang", ""], ["Wu", "Sai", ""]]}, {"id": "2011.11223", "submitter": "Cheng Cheng", "authors": "Nazar Emirov, Cheng Cheng, Qiyu Sun and Zhihua Qu", "title": "Distributed algorithms to determine eigenvectors of matrices on\n  spatially distributed networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Eigenvectors of matrices on a network have been used for understanding\nspectral clustering and influence of a vertex. For matrices with small\ngeodesic-width, we propose a distributed iterative algorithm in this letter to\nfind eigenvectors associated with their given eigenvalues. We also consider the\nimplementation of the proposed algorithm at the vertex/agent level in a\nspatially distributed network.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 05:46:27 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Emirov", "Nazar", ""], ["Cheng", "Cheng", ""], ["Sun", "Qiyu", ""], ["Qu", "Zhihua", ""]]}, {"id": "2011.11266", "submitter": "Miao Yang", "authors": "Miao Yang, Akitanoshou Wong, Hongbin Zhu, Haifeng Wang, Hua Qian", "title": "Federated learning with class imbalance reduction", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a promising technique that enables a large amount\nof edge computing devices to collaboratively train a global learning model. Due\nto privacy concerns, the raw data on devices could not be available for\ncentralized server. Constrained by the spectrum limitation and computation\ncapacity, only a subset of devices can be engaged to train and transmit the\ntrained model to centralized server for aggregation. Since the local data\ndistribution varies among all devices, class imbalance problem arises along\nwith the unfavorable client selection, resulting in a slow converge rate of the\nglobal model. In this paper, an estimation scheme is designed to reveal the\nclass distribution without the awareness of raw data. Based on the scheme, a\ndevice selection algorithm towards minimal class imbalance is proposed, thus\ncan improve the convergence performance of the global model. Simulation results\ndemonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 08:13:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Yang", "Miao", ""], ["Wong", "Akitanoshou", ""], ["Zhu", "Hongbin", ""], ["Wang", "Haifeng", ""], ["Qian", "Hua", ""]]}, {"id": "2011.11325", "submitter": "Jiahua Xu", "authors": "Jiahua Xu and Damien Ackerer and Alevtina Dubovitskaya", "title": "A Game-Theoretic Analysis of Cross-Chain Atomic Swaps with HTLCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To achieve interoperability between unconnected ledgers, hash time lock\ncontracts (HTLCs) are commonly used for cross-chain asset exchange. The\nsolution tolerates transaction failure, and can \"make the best out of worst\" by\nallowing transacting agents to at least keep their original assets in case of\nan abort. Nonetheless, as an undesired outcome, reoccurring transaction\nfailures prompt a critical and analytical examination of the protocol. In this\nstudy, we propose a game-theoretic framework to study the strategic behaviors\nof agents taking part in cross-chain atomic swaps implemented with HTLCs. We\nstudy the success rate of the transaction as a function of the exchange rate of\nthe swap, the token price and its volatility, among other variables. We\ndemonstrate that in an attempt to maximize one's own utility as asset price\nchanges, either agent might withdraw from the swap. An extension of our model\nconfirms that collateral deposits can improve the transaction success rate,\nmotivating further research towards collateralization without a trusted third\nparty. A second model variation suggests that a swap is more likely to succeed\nwhen agents dynamically adjust the exchange rate in response to price\nfluctuations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 11:02:12 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 20:55:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xu", "Jiahua", ""], ["Ackerer", "Damien", ""], ["Dubovitskaya", "Alevtina", ""]]}, {"id": "2011.11450", "submitter": "Petr Hn\\v{e}tynka", "authors": "Lubom\\'ir Bulej, Tom\\'a\\v{s} Bure\\v{s}, Adam Filandr, Petr\n  Hn\\v{e}tynka, Iveta Hn\\v{e}tynkova, Jan Pacovsk\\'y, Gabor Sandor, Ilias\n  Gerostathopoulos", "title": "Managing Latency in Edge-Cloud Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern Cyber-physical Systems (CPS) include applications like smart traffic,\nsmart agriculture, smart power grid, etc. Commonly, these systems are\ndistributed and composed of end-user applications and microservices that\ntypically run in the cloud. The connection with the physical world, which is\ninherent to CPS, brings the need to operate and respond in real-time. As the\ncloud becomes part of the computation loop, the real-time requirements have to\nbe also reflected by the cloud. In this paper, we present an approach that\nprovides soft real-time guarantees on the response time of services running in\ncloud and edge-cloud (i.e., cloud geographically close to the end-user), where\nthese services are developed in high-level programming languages. In\nparticular, we elaborate a method that allows us to predict the upper bound of\nthe response time of a service when sharing the same computer with other\nservices. Importantly, as our approach focuses on minimizing the impact on the\ndeveloper of such services, it does not require any special programming model\nnor limits usage of common libraries, etc.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 14:52:43 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Bulej", "Lubom\u00edr", ""], ["Bure\u0161", "Tom\u00e1\u0161", ""], ["Filandr", "Adam", ""], ["Hn\u011btynka", "Petr", ""], ["Hn\u011btynkova", "Iveta", ""], ["Pacovsk\u00fd", "Jan", ""], ["Sandor", "Gabor", ""], ["Gerostathopoulos", "Ilias", ""]]}, {"id": "2011.11635", "submitter": "Sebastian Friedemann", "authors": "Sebastian Friedemann (DATAMOVE), Bruno Raffin (DATAMOVE)", "title": "An elastic framework for ensemble-based large-scale data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of chaotic systems relies on a floating fusion of sensor data\n(observations) with a numerical model to decide on a good system trajectory and\nto compensate nonlinear feedback effects. Ensemble-based data assimilation (DA)\nis a major method for this concern depending on propagating an ensemble of\nperturbed model realizations.In this paper we develop an elastic, online,\nfault-tolerant and modular framework called Melissa-DA for large-scale\nensemble-based DA. Melissa-DA allows elastic addition or removal of compute\nresources for state propagation at runtime. Dynamic load balancing based on\nlist scheduling ensuresefficient execution. Online processing of the data\nproduced by ensemble members enables to avoid the I/O bottleneck of file-based\napproaches. Our implementation embeds the PDAF parallel DA engine, enabling the\nuse of various DA methods. Melissa-DA can support extra ensemble-based\nDAmethods by implementing the transformation of member background states into\nanalysis states. Experiments confirm the excellent scalability of Melissa-DA,\nrunning on up to 16,240 cores, to propagate 16,384 members for a regional\nhydrological critical zone assimilation relying on theParFlow model on a domain\nwith about 4 M grid cells.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 11:23:43 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 08:23:29 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Friedemann", "Sebastian", "", "DATAMOVE"], ["Raffin", "Bruno", "", "DATAMOVE"]]}, {"id": "2011.11711", "submitter": "Chavit Denninnart", "authors": "Chavit Denninnart", "title": "Cost- and QoS-Efficient Serverless Cloud Computing", "comments": "PhD thesis, University of Louisiana at Lafayette (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud-based serverless computing systems, either public or privately\nprovisioned, aim to provide the illusion of infinite resources and abstract\nusers from details of the allocation decisions. With the goal of providing a\nlow cost and a high QoS, the serverless computing paradigm offers opportunities\nthat can be harnessed to attain the goals. Specifically, our strategy in this\ndissertation is to avoid redundant computing, in cases where independent task\nrequests are similar to each other and for tasks that are pointless to process.\nWe explore two main approaches to (A) reuse part of computation needed to\nprocess the services and (B) proactively pruning tasks with a low chance of\nsuccess to improve the overall QoS of the system. For the first approach, we\npropose a mechanism to identify various types of \"mergeable\" tasks, which can\nbenefit from computational reuse if they are executed together as a group. To\nevaluate the task merging configurations extensively, we quantify the\nresource-saving magnitude and then leveraging the experimental data to create a\nresource-saving predictor. We investigate multiple tasks merging approaches\nthat suit different workload scenarios to determine when it is appropriate to\naggregate tasks and how to allocate them so that the QoS of other tasks is\nminimally affected. For the second approach, we developed the mechanisms to\nskip tasks whose chance of completing on time is not worth pursuing by drop or\ndefer them. We determined the minimum chance of success thresholds for tasks to\npass to get scheduled and executed. We dynamically adjust such thresholds based\non multiple characteristics of the arriving workload and the system's\nconditions. We employed approximate computing to reduce the pruning mechanism's\ncomputational overheads and ensure that the mechanism can be used practically.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:30:25 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Denninnart", "Chavit", ""]]}, {"id": "2011.11744", "submitter": "Anshuman Misra", "authors": "Anshuman Misra and Ajay D. Kshemkalyani", "title": "The Bloom Clock for Causality Testing", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-65621-8_1", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Testing for causality between events in distributed executions is a\nfundamental problem. Vector clocks solve this problem but do not scale well.\nThe probabilistic Bloom clock can determine causality between events with lower\nspace, time, and message-space overhead than vector clock; however, predictions\nsuffer from false positives. We give the protocol for the Bloom clock based on\nCounting Bloom filters and study its properties including the probabilities of\na positive outcome and a false positive. We show the results of extensive\nexperiments to determine how these above probabilities vary as a function of\nthe Bloom timestamps of the two events being tested, and to determine the\naccuracy, precision, and false positive rate of a slice of the execution\ncontaining events in the temporal proximity of each other. Based on these\nexperiments, we make recommendations for the setting of the Bloom clock\nparameters. We postulate the causality spread hypothesis from the application's\nperspective to indicate whether Bloom clocks will be suitable for correct\npredictions with high confidence. The Bloom clock design can serve as a viable\nspace-, time-, and message-space-efficient alternative to vector clocks if\nfalse positives can be tolerated by an application.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 21:43:27 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Misra", "Anshuman", ""], ["Kshemkalyani", "Ajay D.", ""]]}, {"id": "2011.11762", "submitter": "Emanuel Rubensson", "authors": "Emanuel H. Rubensson, Elias Rudberg, Anastasia Kruchinina, Anton G.\n  Artemov", "title": "The Chunks and Tasks Matrix Library 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a C++ header-only parallel sparse matrix library, based on sparse\nquadtree representation of matrices using the Chunks and Tasks programming\nmodel. The library implements a number of sparse matrix algorithms for\ndistributed memory parallelization that are able to dynamically exploit data\nlocality to avoid movement of data. This is demonstrated for the example of\nblock-sparse matrix-matrix multiplication applied to three sequences of\nmatrices with different nonzero structure, using the CHT-MPI 2.0 runtime\nlibrary implementation of the Chunks and Tasks model. The runtime library\nsucceeds to dynamically load balance the calculation regardless of the sparsity\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:04:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Rubensson", "Emanuel H.", ""], ["Rudberg", "Elias", ""], ["Kruchinina", "Anastasia", ""], ["Artemov", "Anton G.", ""]]}, {"id": "2011.12431", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Proposal of Automatic Offloading Method in Mixed Offloading Destination\n  Environment", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "The Eighth International Symposium on Computing and Networking\n  (CANDAR 2020), pp.460-464, Nov. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using heterogeneous hardware, barriers of technical skills such as\nOpenMP, CUDA and OpenCL are high. Based on that, I have proposed\nenvironment-adaptive software that enables automatic conversion, configuration.\nHowever, including existing technologies, there has been no research to\nproperly and automatically offload the mixed offloading destination environment\nsuch as GPU, FPGA and many core CPU. In this paper, as a new element of\nenvironment-adaptive software, I study a method for offloading applications\nproperly and automatically in the environment where the offloading destination\nis mixed with GPU, FPGA and many core CPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 22:28:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2011.12469", "submitter": "Minh N. H. Nguyen Dr.", "authors": "Minh N. H. Nguyen, Nguyen H. Tran, Yan Kyaw Tun, Zhu Han, Choong Seon\n  Hong", "title": "Toward Multiple Federated Learning Services Resource Sharing in Mobile\n  Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Federated Learning is a new learning scheme for collaborative training a\nshared prediction model while keeping data locally on participating devices. In\nthis paper, we study a new model of multiple federated learning services at the\nmulti-access edge computing server. Accordingly, the sharing of CPU resources\namong learning services at each mobile device for the local training process\nand allocating communication resources among mobile devices for exchanging\nlearning information must be considered. Furthermore, the convergence\nperformance of different learning services depends on the hyper-learning rate\nparameter that needs to be precisely decided. Towards this end, we propose a\njoint resource optimization and hyper-learning rate control problem, namely\nMS-FEDL, regarding the energy consumption of mobile devices and overall\nlearning time. We design a centralized algorithm based on the block coordinate\ndescent method and a decentralized JP-miADMM algorithm for solving the MS-FEDL\nproblem. Different from the centralized approach, the decentralized approach\nrequires many iterations to obtain but it allows each learning service to\nindependently manage the local resource and learning process without revealing\nthe learning service information. Our simulation results demonstrate the\nconvergence performance of our proposed algorithms and the superior performance\nof our proposed algorithms compared to the heuristic strategy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:29:41 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Nguyen", "Minh N. H.", ""], ["Tran", "Nguyen H.", ""], ["Tun", "Yan Kyaw", ""], ["Han", "Zhu", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2011.12511", "submitter": "Sen Lin", "authors": "Sen Lin, Li Yang, Zhezhi He, Deliang Fan, Junshan Zhang", "title": "MetaGater: Fast Learning of Conditional Channel Gated Networks via\n  Federated Meta-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has achieved phenomenal successes in many AI\napplications, its enormous model size and intensive computation requirements\npose a formidable challenge to the deployment in resource-limited nodes. There\nhas recently been an increasing interest in computationally-efficient learning\nmethods, e.g., quantization, pruning and channel gating. However, most existing\ntechniques cannot adapt to different tasks quickly. In this work, we advocate a\nholistic approach to jointly train the backbone network and the channel gating\nwhich enables dynamical selection of a subset of filters for more efficient\nlocal computation given the data input. Particularly, we develop a federated\nmeta-learning approach to jointly learn good meta-initializations for both\nbackbone networks and gating modules, by making use of the model similarity\nacross learning tasks on different nodes. In this way, the learnt meta-gating\nmodule effectively captures the important filters of a good meta-backbone\nnetwork, based on which a task-specific conditional channel gated network can\nbe quickly adapted, i.e., through one-step gradient descent, from the\nmeta-initializations in a two-stage procedure using new samples of that task.\nThe convergence of the proposed federated meta-learning algorithm is\nestablished under mild conditions. Experimental results corroborate the\neffectiveness of our method in comparison to related work.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 04:26:23 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 16:29:36 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Lin", "Sen", ""], ["Yang", "Li", ""], ["He", "Zhezhi", ""], ["Fan", "Deliang", ""], ["Zhang", "Junshan", ""]]}, {"id": "2011.12623", "submitter": "Hangyu Zhu", "authors": "Hangyu Zhu, Rui Wang, Yaochu Jin, Kaitai Liang and Jianting Ning", "title": "Distributed Additive Encryption and Quantization for Privacy Preserving\n  Federated Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homomorphic encryption is a very useful gradient protection technique used in\nprivacy preserving federated learning. However, existing encrypted federated\nlearning systems need a trusted third party to generate and distribute key\npairs to connected participants, making them unsuited for federated learning\nand vulnerable to security risks. Moreover, encrypting all model parameters is\ncomputationally intensive, especially for large machine learning models such as\ndeep neural networks. In order to mitigate these issues, we develop a\npractical, computationally efficient encryption based protocol for federated\ndeep learning, where the key pairs are collaboratively generated without the\nhelp of a third party. By quantization of the model parameters on the clients\nand an approximated aggregation on the server, the proposed method avoids\nencryption and decryption of the entire model. In addition, a threshold based\nsecret sharing technique is designed so that no one can hold the global private\nkey for decryption, while aggregated ciphertexts can be successfully decrypted\nby a threshold number of clients even if some clients are offline. Our\nexperimental results confirm that the proposed method significantly reduces the\ncommunication costs and computational complexity compared to existing encrypted\nfederated learning without compromising the performance and security.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:23:42 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Zhu", "Hangyu", ""], ["Wang", "Rui", ""], ["Jin", "Yaochu", ""], ["Liang", "Kaitai", ""], ["Ning", "Jianting", ""]]}, {"id": "2011.12633", "submitter": "Matan Kraus", "authors": "Stav Ben-Nun, Tsvi Kopelowitz, Matan Kraus, Ely Porat", "title": "An $O(\\log^{3/2}n)$ Parallel Time Population Protocol for Majority with\n  $O(\\log n)$ States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population protocols, the underlying distributed network consists of $n$\nnodes (or agents), denoted by $V$, and a scheduler that continuously selects\nuniformly random pairs of nodes to interact. When two nodes interact, their\nstates are updated by applying a state transition function that depends only on\nthe states of the two nodes prior to the interaction. The efficiency of a\npopulation protocol is measured in terms of both time (which is the number of\ninteractions until the nodes collectively have a valid output) and the number\nof possible states of nodes used by the protocol. By convention, we consider\nthe parallel time cost, which is the time divided by $n$.\n  In this paper we consider the majority problem, where each node receives as\ninput a color that is either black or white, and the goal is to have all of the\nnodes output the color that is the majority of the input colors. We design a\npopulation protocol that solves the majority problem in $O(\\log^{3/2}n)$\nparallel time, both with high probability and in expectation, while using\n$O(\\log n)$ states. Our protocol improves on a recent protocol of Berenbrink et\nal. that runs in $O(\\log^{5/3}n)$ parallel time, both with high probability and\nin expectation, using $O(\\log n)$ states.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 10:48:05 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Ben-Nun", "Stav", ""], ["Kopelowitz", "Tsvi", ""], ["Kraus", "Matan", ""], ["Porat", "Ely", ""]]}, {"id": "2011.12691", "submitter": "Yong Xiao", "authors": "Yong Xiao and Yingyu Li and Guangming Shi and H. Vincent Poor", "title": "Optimizing Resource-Efficiency for Federated Edge Intelligence in IoT\n  Networks", "comments": "Accepted at International Conference on Wireless Communications and\n  Signal Processing (WCSP), Nanjing, China, October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies an edge intelligence-based IoT network in which a set of\nedge servers learn a shared model using federated learning (FL) based on the\ndatasets uploaded from a multi-technology-supported IoT network. The data\nuploading performance of IoT network and the computational capacity of edge\nservers are entangled with each other in influencing the FL model training\nprocess. We propose a novel framework, called federated edge intelligence\n(FEI), that allows edge servers to evaluate the required number of data samples\naccording to the energy cost of the IoT network as well as their local data\nprocessing capacity and only request the amount of data that is sufficient for\ntraining a satisfactory model. We evaluate the energy cost for data uploading\nwhen two widely-used IoT solutions: licensed band IoT (e.g., 5G NB-IoT) and\nunlicensed band IoT (e.g., Wi-Fi, ZigBee, and 5G NR-U) are available to each\nIoT device. We prove that the cost minimization problem of the entire IoT\nnetwork is separable and can be divided into a set of subproblems, each of\nwhich can be solved by an individual edge server. We also introduce a mapping\nfunction to quantify the computational load of edge servers under different\ncombinations of three key parameters: size of the dataset, local batch size,\nand number of local training passes. Finally, we adopt an Alternative Direction\nMethod of Multipliers (ADMM)-based approach to jointly optimize energy cost of\nthe IoT network and average computing resource utilization of edge servers. We\nprove that our proposed algorithm does not cause any data leakage nor disclose\nany topological information of the IoT network. Simulation results show that\nour proposed framework significantly improves the resource efficiency of the\nIoT network and edge servers with only a limited sacrifice on the model\nconvergence performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 12:51:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Xiao", "Yong", ""], ["Li", "Yingyu", ""], ["Shi", "Guangming", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2011.12719", "submitter": "Michael Luo Zhiyu", "authors": "Eric Liang, Zhanghao Wu, Michael Luo, Sven Mika, Ion Stoica", "title": "RLlib Flow: Distributed Reinforcement Learning is a Dataflow Problem", "comments": "9 pages, 1 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Researchers and practitioners in the field of reinforcement learning (RL)\nfrequently leverage parallel computation, which has led to a plethora of new\nalgorithms and systems in the last few years. In this paper, we re-examine the\nchallenges posed by distributed RL and try to view it through the lens of an\nold idea: distributed dataflow. We show that viewing RL as a dataflow problem\nleads to highly composable and performant implementations. We propose RLlib\nflow, a hybrid actor-dataflow programming model for distributed RL, and\nvalidate its practicality by porting the full suite of algorithms in RLlib, a\nwidely-adopted distributed RL library.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 13:28:16 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 04:55:36 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 01:10:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liang", "Eric", ""], ["Wu", "Zhanghao", ""], ["Luo", "Michael", ""], ["Mika", "Sven", ""], ["Stoica", "Ion", ""]]}, {"id": "2011.12729", "submitter": "Vladimir Yussupov", "authors": "Vladimir Yussupov and Ghareeb Falazi and Uwe Breitenb\\\"ucher and Frank\n  Leymann", "title": "On the Serverless Nature of Blockchains and Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although historically the term serverless was also used in the context of\npeer-to-peer systems, it is more frequently associated with the architectural\nstyle for developing cloud-native applications. From the developer's\nperspective, serverless architectures allow reducing management efforts since\napplications are composed using provider-managed components, e.g.,\nDatabase-as-a-Service (DBaaS) and Function-as-a-Service (FaaS) offerings.\nBlockchains are distributed systems designed to enable collaborative scenarios\ninvolving multiple untrusted parties. It seems that the decentralized\npeer-to-peer nature of blockchains makes it interesting to consider them in\nserverless architectures, since resource allocation and management tasks are\nnot required to be performed by users. Moreover, considering their useful\nproperties of ensuring transaction's immutability and facilitating accountable\ninteractions, blockchains might enhance the overall guarantees and capabilities\nof serverless architectures. Therefore, in this work, we analyze how the\nblockchain technology and smart contracts fit into the serverless picture and\nderive a set of scenarios in which they act as different component types in\nserverless architectures. Furthermore, we formulate the implementation\nrequirements that have to be fulfilled to successfully use blockchains and\nsmart contracts in these scenarios. Finally, we investigate which existing\ntechnologies enable these scenarios, and analyze their readiness and\nsuitability to fulfill the formulated requirements.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 10:51:56 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Yussupov", "Vladimir", ""], ["Falazi", "Ghareeb", ""], ["Breitenb\u00fccher", "Uwe", ""], ["Leymann", "Frank", ""]]}, {"id": "2011.12875", "submitter": "Aidan Thompson", "authors": "Rahulkumar Gayatri, Stan Moore, Evan Weinberg, Nicholas Lubbers, Sarah\n  Anderson, Jack Deslippe, Danny Perez, and Aidan P. Thompson", "title": "Rapid Exploration of Optimization Strategies on Advanced Architectures\n  using TestSNAP and LAMMPS", "comments": "Submitted to IPDPS 2021, October 19, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exascale race is at an end with the announcement of the Aurora and\nFrontier machines. This next generation of supercomputers utilize diverse\nhardware architectures to achieve their compute performance, providing an added\nonus on the performance portability of applications. An expanding fragmentation\nof programming models would provide a compounding optimization challenge were\nit not for the evolution of performance-portable frameworks, providing unified\nmodels for mapping abstract hierarchies of parallelism to diverse\narchitectures. A solution to this challenge is the evolution of\nperformance-portable frameworks, providing unified models for mapping abstract\nhierarchies of parallelism to diverse architectures. Kokkos is one such\nperformance portable programming model for C++ applications, providing back-end\nimplementations for each major HPC platform. Even with a performance portable\nframework, restructuring algorithms to expose higher degrees of parallelism is\nnon-trivial. The Spectral Neighbor Analysis Potential (SNAP) is a\nmachine-learned inter-atomic potential utilized in cutting-edge molecular\ndynamics simulations. Previous implementations of the SNAP calculation showed a\ndownward trend in their performance relative to peak on newer-generation CPUs\nand low performance on GPUs. In this paper we describe the restructuring and\noptimization of SNAP as implemented in the Kokkos CUDA backend of the LAMMPS\nmolecular dynamics package, benchmarked on NVIDIA GPUs. We identify novel\npatterns of hierarchical parallelism, facilitating a minimization of memory\naccess overheads and pushing the implementation into a compute-saturated\nregime. Our implementation via Kokkos enables recompile-and-run efficiency on\nupcoming architectures. We find a $\\sim$22x time-to-solution improvement\nrelative to an existing implementation as measured on an NVIDIA Tesla V100-16GB\nfor an important benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:52:08 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Gayatri", "Rahulkumar", ""], ["Moore", "Stan", ""], ["Weinberg", "Evan", ""], ["Lubbers", "Nicholas", ""], ["Anderson", "Sarah", ""], ["Deslippe", "Jack", ""], ["Perez", "Danny", ""], ["Thompson", "Aidan P.", ""]]}, {"id": "2011.12879", "submitter": "Philippe Qu\\'einnec", "authors": "Adam Shimi, Aur\\'elie Hurault, Philippe Queinnec", "title": "Characterization and Derivation of Heard-Of Predicates for Asynchronous\n  Message-Passing Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In distributed computing, multiple processes interact to solve a problem\ntogether. The main model of interaction is the message-passing model, where\nprocesses communicate by exchanging messages. Nevertheless, there are several\nmodels varying along important dimensions: degree of synchrony, kinds of\nfaults, number of faults... This variety is compounded by the lack of a general\nformalism in which to abstract these models. One way to bring order is to\nconstrain these models to communicate in rounds. This is the setting of the\nHeard-Of model, which captures many models through predicates on the messages\nsent in a round and received on time. Yet, it is not easy to define the\npredicate that captures a given operational model. The question is even harder\nfor the asynchronous case, as unbounded message delay means the implementation\nof rounds must depend on details of the model. This paper shows that\ncharacterising asynchronous models by heard-of predicates is indeed meaningful.\nThis characterization relies on delivered predicates, an intermediate\nabstraction between the informal operational model and the heard-of predicates.\nOur approach splits the problem into two steps: first extract the delivered\nmodel capturing the informal model, and then characterize the heard-of\npredicates that are generated by this delivered model. For the first part, we\nprovide examples of delivered predicates, and an approach to derive more. It\nuses the intuition that complex models are a composition of simpler models. We\ndefine operations like union, succession or repetition that make it easier to\nderive complex delivered predicates from simple ones while retaining\nexpressivity. For the second part, we formalize and study strategies for when\nto change rounds. Intuitively, the characterizing predicate of a model is the\none generated by a strategy that waits for as much messages as possible,\nwithout blocking forever.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:57:11 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:28:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Shimi", "Adam", ""], ["Hurault", "Aur\u00e9lie", ""], ["Queinnec", "Philippe", ""]]}, {"id": "2011.12958", "submitter": "Zhenlong Li Dr.", "authors": "Zhenlong Li, Xiao Huang, Xinyue Ye, Xiaoming Li", "title": "ODT Flow Explorer: Extract, Query, and Visualize Human Mobility", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding human mobility dynamics among places provides fundamental\nknowledge regarding their interactive gravity, benefiting a wide range of\napplications in need of prior knowledge in human spatial interactions. The\nongoing COVID-19 pandemic uniquely highlights the need for monitoring and\nmeasuring fine-scale human spatial interactions. In response to the soaring\nneeds of human mobility data under the pandemic, we developed an interactive\ngeospatial web portal by extracting worldwide daily population flows from\nbillions of geotagged tweets and United States (U.S.) population flows from\nSafeGraph mobility data. The web portal is named ODT (Origin-Destination-Time)\nFlow Explorer. At the core of the explorer is an ODT data cube coupled with a\nbig data computing cluster to efficiently manage, query, and aggregate billions\nof OD flows at different spatial and temporal scales. Although the explorer is\nstill in its early developing stage, the rapidly generated mobility flow data\ncan benefit a wide range of domains that need timely access to the fine-grained\nhuman mobility records. The ODT Flow Explorer can be accessed via\nhttp://gis.cas.sc.edu/GeoAnalytics/od.html.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 21:24:15 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Li", "Zhenlong", ""], ["Huang", "Xiao", ""], ["Ye", "Xinyue", ""], ["Li", "Xiaoming", ""]]}, {"id": "2011.12984", "submitter": "Cody Balos", "authors": "Cody J. Balos and David J. Gardner and Carol S. Woodward and Daniel R.\n  Reynolds", "title": "Enabling GPU Accelerated Computing in the SUNDIALS Time Integration\n  Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the Exascale Computing Project (ECP), a recent focus of\ndevelopment efforts for the SUite of Nonlinear and DIfferential/ALgebraic\nequation Solvers (SUNDIALS) has been to enable GPU-accelerated time integration\nin scientific applications at extreme scales. This effort has resulted in\nseveral new GPU-enabled implementations of core SUNDIALS data structures,\nsupport for programming paradigms which are aware of the heterogeneous\narchitectures, and the introduction of utilities to provide new points of\nflexibility. In this paper, we discuss our considerations, both internal and\nexternal, when designing these new features and present the features\nthemselves. We also present performance results for several of the features on\nthe Summit supercomputer and early access hardware for the Frontier\nsupercomputer, which demonstrate negligible performance overhead resulting from\nthe additional infrastructure and significant speedups when using both NVIDIA\nand AMD GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:09:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Balos", "Cody J.", ""], ["Gardner", "David J.", ""], ["Woodward", "Carol S.", ""], ["Reynolds", "Daniel R.", ""]]}, {"id": "2011.13219", "submitter": "Qingbiao Li", "authors": "Qingbiao Li, Weizhe Lin, Zhe Liu, Amanda Prorok", "title": "Message-Aware Graph Attention Networks for Large-Scale Multi-Robot Path\n  Planning", "comments": "This work has been accepted to the IEEE Robotics and Automation\n  Letters (RA-L) for publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC cs.LG cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domains of transport and logistics are increasingly relying on autonomous\nmobile robots for the handling and distribution of passengers or resources. At\nlarge system scales, finding decentralized path planning and coordination\nsolutions is key to efficient system performance. Recently, Graph Neural\nNetworks (GNNs) have become popular due to their ability to learn communication\npolicies in decentralized multi-agent systems. Yet, vanilla GNNs rely on\nsimplistic message aggregation mechanisms that prevent agents from prioritizing\nimportant information. To tackle this challenge, in this paper, we extend our\nprevious work that utilizes GNNs in multi-agent path planning by incorporating\na novel mechanism to allow for message-dependent attention. Our Message-Aware\nGraph Attention neTwork (MAGAT) is based on a key-query-like mechanism that\ndetermines the relative importance of features in the messages received from\nvarious neighboring robots. We show that MAGAT is able to achieve a performance\nclose to that of a coupled centralized expert algorithm. Further, ablation\nstudies and comparisons to several benchmark models show that our attention\nmechanism is very effective across different robot densities and performs\nstably in different constraints in communication bandwidth. Experiments\ndemonstrate that our model is able to generalize well in previously unseen\nproblem instances, and that it achieves a 47\\% improvement over the benchmark\nsuccess rate, even in very large-scale instances that are $\\times$100 larger\nthan the training instances.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 10:37:13 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 11:40:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Qingbiao", ""], ["Lin", "Weizhe", ""], ["Liu", "Zhe", ""], ["Prorok", "Amanda", ""]]}, {"id": "2011.13234", "submitter": "Morteza Mohaqeqi", "authors": "Wang Yi, Morteza Mohaqeqi, Susanne Graf", "title": "MIMOS: A Deterministic Model for the Design and Update of Real-Time\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the pioneering work of Gilles Kahn on concurrent systems, we\npropose to model timed systems as a network of software components (implemented\nas real-time processes or tasks), each of which is specified to compute a\ncollection of functions according to given timing constraints. We present a\nfixed-point semantics for this model which shows that each system function of\nsuch a network computes for a given set of (timed) input streams, a\ndeterministic (timed) output stream. As a desired feature, such a network model\ncan be modified by integrating new components for adding new system functions\nwithout changing the existing ones. Additionally, existing components may be\nreplaced also by new ones fulfilling given requirements. Thanks to the\ndeterministic semantics, a model-based approach is enabled for not only\nbuilding systems but also updating them after deployment, allowing for\nefficient analysis techniques such as model-in-the-loop simulation to verify\nthe complete behaviour of the updated system.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:05:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yi", "Wang", ""], ["Mohaqeqi", "Morteza", ""], ["Graf", "Susanne", ""]]}, {"id": "2011.13579", "submitter": "Matin Hashemi", "authors": "Alireza Mohammadidoost, Matin Hashemi", "title": "High-Throughput Parallel Viterbi Decoder on GPU Tensor Cores", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.09337", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many research works have been performed on implementation of Vitrerbi\ndecoding algorithm on GPU instead of FPGA because this platform provides\nconsiderable flexibility in addition to great performance. Recently, the\nrecently-introduced Tensor cores in modern GPU architectures provide incredible\ncomputing capability. This paper proposes a novel parallel implementation of\nViterbi decoding algorithm based on Tensor cores in modern GPU architectures.\nThe proposed parallel algorithm is optimized to efficiently utilize the\ncomputing power of Tensor cores. Experiments show considerable throughput\nimprovements in comparison with previous works.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 06:44:47 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Mohammadidoost", "Alireza", ""], ["Hashemi", "Matin", ""]]}, {"id": "2011.13630", "submitter": "Susumu Nishimura", "authors": "Koki Yagi, Susumu Nishimura", "title": "Logical Obstruction to Set Agreement Tasks for Superset-Closed\n  Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In their recent paper (GandALF 2018), Goubault, Ledent, and Rajsbaum provided\na formal epistemic model for distributed computing. Their logical model, as an\nalternative to the well-studied topological model, provides an attractive\nframework for refuting the solvability of a given distributed task by means of\nlogical obstruction: One just needs to devise a formula, in the formal language\nof epistemic logic, that describes a discrepancy between the model of\ncomputation and that of the task. However, few instances of logical obstruction\nwere presented in their paper and specifically logical obstruction to the\nwait-free 2-set agreement task was left as an open problem. Soon later, Nishida\naffirmatively answered to the problem by providing inductively defined logical\nobstruction formulas to the wait-free $k$-set agreement tasks.\n  The present paper refines Nishida's work and devises logical obstruction\nformulas to $k$-set agreement tasks for superset-closed adversaries, which\nsupersede the wait-free model. These instances of logical obstruction formulas\nexemplify that the logical framework can provide yet another feasible method\nfor showing impossibility of distributed tasks, though it is currently being\nconfined to one-round distributed protocols. The logical method has an\nadvantage over the topological method that it enjoys a self-contained,\nelementary induction proof. This is in contrast to topological methods, in\nwhich sophisticated topological tools, such as Nerve lemma, are often assumed\nas granted.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 09:37:15 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 09:51:09 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Yagi", "Koki", ""], ["Nishimura", "Susumu", ""]]}, {"id": "2011.13743", "submitter": "Pegah Rokhforoz", "authors": "Pegah Rokhforoz, Olga Fink", "title": "Distributed joint dynamic maintenance and production scheduling in\n  manufacturing systems: Framework based on model predictive control and\n  Benders decomposition", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scheduling the maintenance based on the condition, respectively the\ndegradation level of the system leads to improved system's reliability while\nminimizing the maintenance cost. Since the degradation level changes\ndynamically during the system's operation, we face a dynamic maintenance\nscheduling problem. In this paper, we address the dynamic maintenance\nscheduling of manufacturing systems based on their degradation level. The\nmanufacturing system consists of several units with a defined capacity and an\nindividual dynamic degradation model, seeking to optimize their reward. The\nunits sell their production capacity, while maintaining the systems based on\nthe degradation state to prevent failures. The manufacturing units are jointly\nresponsible for fulfilling the demand of the system. This induces a coupling\nconstraint among the agents. Hence, we face a large-scale mixed-integer dynamic\nmaintenance scheduling problem. In order to handle the dynamic model of the\nsystem and large-scale optimization, we propose a distributed algorithm using\nmodel predictive control (MPC) and Benders decomposition method. In the\nproposed algorithm, first, the master problem obtains the maintenance\nscheduling for all the agents, and then based on this data, the agents obtain\ntheir optimal production using the distributed MPC method which employs the\ndual decomposition approach to tackle the coupling constraints among the\nagents. The effectiveness of the proposed method is investigated on a case\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 14:10:55 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rokhforoz", "Pegah", ""], ["Fink", "Olga", ""]]}, {"id": "2011.13823", "submitter": "Aleix Roca Nonell", "authors": "Aleix Roca Nonell, Vicen\\c{c} Beltran Querol, Sergi Mateo Bellido", "title": "Introducing the Task-Aware Storage I/O (TASIO) Library", "comments": "16 pages, 17 figures, conference", "journal-ref": "OpenMP: Conquering the Full Hardware Spectrum 15 (2019) 274-288", "doi": "10.1007/978-3-030-28596-8_19", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based programming models are excellent tools to parallelize and\nseamlessly load balance an application workload. However, the integration of\nI/O intensive applications and task-based programming models is lacking.\nTypically, I/O operations stall the requesting thread until the data is\nserviced by the backing device. Because the core where the thread was running\nbecomes idle, it should be possible to overlap the data query operation with\neither computation workloads or even more I/O operations. Nonetheless,\noverlapping I/O tasks with other tasks entails an extra degree of complexity\ncurrently not managed by programming models' runtimes. In this work, we focus\non integrating storage I/O into the tasking model by introducing the Task-Aware\nStorage I/O (TASIO) library. We test TASIO extensively with a custom benchmark\nfor a number of configurations and conclude that it is able to achieve speedups\nup to 2x depending on the workload, although it might lead to slowdowns if not\nused with the right settings.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 16:41:07 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Nonell", "Aleix Roca", ""], ["Querol", "Vicen\u00e7 Beltran", ""], ["Bellido", "Sergi Mateo", ""]]}, {"id": "2011.14243", "submitter": "Liang Luo", "authors": "Liang Luo, Peter West, Arvind Krishnamurthy, Luis Ceze", "title": "Srift: Swift and Thrift Cloud-Based Distributed Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cost-efficiency and training time are primary concerns in cloud-based\ndistributed training today. With many VM configurations to choose from, given a\ntime constraint, what configuration achieves the lowest cost? Or, given a cost\nbudget, which configuration leads to the highest throughput? We present a\ncomprehensive throughput and cost-efficiency study across a wide array of\ninstance choices in the cloud. With the insights from this study, we build\nSrift, a system that combines runtime instrumentation and learned performance\nmodels to accurately predict training performance and find the best choice of\nVMs to improve throughput and lower cost while satisfying user constraints.\nWith Pytorch and EC2, we show Srift's choices of VM instances can lead to up to\n2x better throughput and 1.6x lower cost per iteration compared to baseline\nchoices across various DNN models in real-world scenarios, leveraging\nheterogeneous setups and spot instances.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 00:43:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Luo", "Liang", ""], ["West", "Peter", ""], ["Krishnamurthy", "Arvind", ""], ["Ceze", "Luis", ""]]}, {"id": "2011.14416", "submitter": "Juan Isern MSc", "authors": "Juan Isern, Francisco Barranco, Daniel Deniz, Juho Lesonen, Jari\n  Hannuksela, Richard R. Carrillo", "title": "Reconfigurable Cyber-Physical System for Critical Infrastructure\n  Protection in Smart Cities via Smart Video-Surveillance", "comments": "13 pages, 8 figures and 5 tables", "journal-ref": "Pattern Recognition Letters Volume 140, December 2020, Pages\n  303-309", "doi": "10.1016/j.patrec.2020.11.004", "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surveillance is essential for the protection of Critical\nInfrastructures (CIs) in future Smart Cities. The dynamic environments and\nbandwidth requirements demand systems that adapt themselves to react when\nevents of interest occur. We present a reconfigurable Cyber Physical System for\nthe protection of CIs using distributed cloud-edge smart video surveillance.\nOur local edge nodes perform people detection via Deep Learning. Processing is\nembedded in high performance SoCs (System-on-Chip) achieving real-time\nperformance ($\\approx$ 100 fps - frames per second) which enables efficiently\nmanaging video streams of more cameras source at lower frame rate. Cloud server\ngathers results from nodes to carry out biometric facial identification,\ntracking, and perimeter monitoring. A Quality and Resource Management module\nmonitors data bandwidth and triggers reconfiguration adapting the transmitted\nvideo resolution. This also enables a flexible use of the network by multiple\ncameras while maintaining the accuracy of biometric identification. A\nreal-world example shows a reduction of $\\approx$ 75\\% bandwidth use with\nrespect to the no-reconfiguration scenario.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 18:43:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Isern", "Juan", ""], ["Barranco", "Francisco", ""], ["Deniz", "Daniel", ""], ["Lesonen", "Juho", ""], ["Hannuksela", "Jari", ""], ["Carrillo", "Richard R.", ""]]}, {"id": "2011.14499", "submitter": "Navid Rezazadeh", "authors": "Navid Rezazadeh, Solmaz S. Kia", "title": "Multi-Agent Maximization of a Monotone Submodular Function via Maximum\n  Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained submodular set function maximization problems often appear in\nmulti-agent decision-making problems with a discrete feasible set. A prominent\nexample is the problem of multi-agent mobile sensor placement over a discrete\ndomain. However, submodular set function optimization problems are known to be\nNP-hard. In this paper, we consider a class of submodular optimization problems\nthat consists of maximization of a monotone and submodular set function subject\nto a uniform matroid constraint over a group of networked agents that\ncommunicate over a connected undirected graph. Our objective is to obtain a\ndistributed suboptimal polynomial-time algorithm that enables each agent to\nobtain its respective policy via local interactions with its neighboring\nagents. Our solution is a fully distributed gradient-based algorithm using the\nmultilinear extension of the submodular set functions and exploiting a maximum\nconsensus scheme. This algorithm results in a policy set that when the team\nobjective function is evaluated at worst case the objective function value is\nin $1-1/e-O(1/T)$ of the optimal solution. An example demonstrates our results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 01:51:48 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Rezazadeh", "Navid", ""], ["Kia", "Solmaz S.", ""]]}, {"id": "2011.14706", "submitter": "Mostafa Haghi Kashani", "authors": "Mostafa Haghi Kashani, Ahmad Ahmadzadeh, Ebrahim Mahdipour", "title": "Load balancing mechanisms in fog computing: A systematic review", "comments": "19 pages, 9 figures, 11 tables, 94 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, fog computing has been introduced as a modern distributed paradigm\nand complement to cloud computing to provide services. Fog system extends\nstoring and computing to the edge of the network, which can solve the problem\nabout service computing of the delay-sensitive applications remarkably besides\nenabling the location awareness and mobility support. Load balancing is an\nimportant aspect of fog networks that avoids a situation with some under-loaded\nor overloaded fog nodes. Quality of Service (QoS) parameters such as resource\nutilization, throughput, cost, response time, performance, and energy\nconsumption can be improved with load balancing. In recent years, some\nresearches in load balancing techniques in fog networks have been carried out,\nbut there is no systematic review to consolidate these studies. This article\nreviews the load-balancing mechanisms systematically in fog computing in four\nclassifications, including approximate, exact, fundamental, and hybrid methods\n(published between 2013 and August 2020). Also, this article investigates load\nbalancing metrics with all advantages and disadvantages related to chosen load\nbalancing mechanisms in fog networks. The evaluation techniques and tools\napplied for each reviewed study are explored as well. Additionally, the\nessential open challenges and future trends of these mechanisms are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:33:40 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 11:26:44 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kashani", "Mostafa Haghi", ""], ["Ahmadzadeh", "Ahmad", ""], ["Mahdipour", "Ebrahim", ""]]}, {"id": "2011.14732", "submitter": "Mostafa Haghi Kashani", "authors": "Maryam Songhorabadi, Morteza Rahimi, Amir Mahdi Moghaddam Farid,\n  Mostafa Haghi Kashani", "title": "Fog Computing Approaches in Smart Cities: A State-of-the-Art Review", "comments": "19pages, 8 figures, 9 tables, 97 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These days, the development of smart cities, specifically in location-aware,\nlatency-sensitive, and security-crucial applications (such as emergency fire\nevents, patient health monitoring, or real-time manufacturing) heavily depends\non a more advance computing paradigms that can address these requirements. In\nthis regard, fog computing, a robust cloud computing complement, plays a\npreponderant role by virtue of locating closer to the end-devices. Nonetheless,\nutilized approaches in smart cities are frequently cloud-based, which causes\nnot only the security and time-sensitive services to suffer but also its\nflexibility and reliability to be restricted. So as to obviate the limitations\nof cloud and other related computing paradigms such as edge computing, this\npaper proposes a systematic literature review (SLR) for the state-of-the-art\nfog-based approaches in smart cities. Furthermore, according to the content of\nthe reviewed researches, a taxonomy is proposed, falls into three classes,\nincluding service-based, resource-based, and application-based. This SLR also\ninvestigates the evaluation factors, used tools, evaluation methods, merits,\nand demerits of each class. Types of proposed algorithms in each class are\nmentioned as well. Above all else, by taking various perspectives into account,\ncomprehensive and distinctive open issues and challenges are provided via\nclassifying future trends and issues into practical sub-classes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 12:22:18 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 11:29:08 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Songhorabadi", "Maryam", ""], ["Rahimi", "Morteza", ""], ["Farid", "Amir Mahdi Moghaddam", ""], ["Kashani", "Mostafa Haghi", ""]]}, {"id": "2011.14789", "submitter": "Josef Widder", "authors": "Igor Konnov and Marijana Lazi\\'c and Ilina Stoilkovska and Josef\n  Widder", "title": "Survey on Parameterized Verification with Threshold Automata and the\n  Byzantine Model Checker", "comments": "This is an extended version of our paper: Igor Konnov, Marijana\n  Lazi\\'c, Ilina Stoilkovska, Josef Widder. Tutorial: Parameterized\n  Verification with Byzantine Model Checker. FORTE 2020: 189-207", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Threshold guards are a basic primitive of many fault-tolerant algorithms that\nsolve classical problems of distributed computing, such as reliable broadcast,\ntwo-phase commit, and consensus. Moreover, threshold guards can be found in\nrecent blockchain algorithms such as Tendermint consensus. In this article, we\ngive an overview of the techniques implemented in Byzantine Model Checker\n(ByMC). ByMC implements several techniques for automatic verification of\nthreshold-guarded distributed algorithms. These algorithms have the following\nfeatures: (1) up to $t$ of processes may crash or behave Byzantine; (2) the\ncorrect processes count messages and make progress when they receive\nsufficiently many messages, e.g., at least $t+1$; (3) the number $n$ of\nprocesses in the system is a parameter, as well as $t$; (4) and the parameters\nare restricted by a resilience condition, e.g., $n > 3t$. Traditionally, these\nalgorithms were implemented in distributed systems with up to ten participating\nprocesses. Nowadays, they are implemented in distributed systems that involve\nhundreds or thousands of processes. To make sure that these algorithms are\nstill correct for that scale, it is imperative to verify them for all possible\nvalues of the parameters.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:46:45 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Konnov", "Igor", ""], ["Lazi\u0107", "Marijana", ""], ["Stoilkovska", "Ilina", ""], ["Widder", "Josef", ""]]}, {"id": "2011.14816", "submitter": "Christian Cachin", "authors": "Ignacio Amores-Sesar, Christian Cachin, Jovana Mi\\'ci\\'c", "title": "Security Analysis of Ripple Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ripple network is one of the most prominent blockchain platforms and its\nnative XRP token currently has one of the highest cryptocurrency market\ncapitalizations. The Ripple consensus protocol powers this network and is\ngenerally considered to a Byzantine fault-tolerant agreement protocol, which\ncan reach consensus in the presence of faulty or malicious nodes. In contrast\nto traditional Byzantine agreement protocols, there is no global knowledge of\nall participating nodes in Ripple consensus; instead, each node declares a list\nof other nodes that it trusts and from which it considers votes.\n  Previous work has brought up concerns about the liveness and safety of the\nconsensus protocol under the general assumptions stated initially by Ripple,\nand there is currently no appropriate understanding of its workings and its\nproperties in the literature. This paper closes this gap and makes two\ncontributions. It first provides a detailed, abstract description of the\nprotocol, which has been derived from the source code. Second, the paper points\nout that the abstract protocol may violate safety and liveness in several\nsimple executions under relatively benign network assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 14:11:55 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Amores-Sesar", "Ignacio", ""], ["Cachin", "Christian", ""], ["Mi\u0107i\u0107", "Jovana", ""]]}, {"id": "2011.14995", "submitter": "Edgar Fajardo", "authors": "Edgar Fajardo, Frank Wuerthwein, Brian Bockelman, Miron Livny, Greg\n  Thain, James Alexander Clark, Peter Couvares and Josh Willis", "title": "Adapting LIGO workflows to run in the Open Science Grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the first observation run the LIGO collaboration needed to offload\nsome of its most, intense CPU workflows from its dedicated computing sites to\nopportunistic resources. Open Science Grid enabled LIGO to run PyCbC, RIFT and\nBayeswave workflows to seamlessly run in a combination of owned and\nopportunistic resources. One of the challenges is enabling the workflows to use\nseveral heterogeneous resources in a coordinated and effective way.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:02:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Fajardo", "Edgar", ""], ["Wuerthwein", "Frank", ""], ["Bockelman", "Brian", ""], ["Livny", "Miron", ""], ["Thain", "Greg", ""], ["Clark", "James Alexander", ""], ["Couvares", "Peter", ""], ["Willis", "Josh", ""]]}, {"id": "2011.15013", "submitter": "Brijesh Dongol", "authors": "Eleni Bila, John Derrick, Simon Doherty, Brijesh Dongol, Gerhard\n  Schellhorn, and Heike Wehrheim", "title": "Modularising Verification Of Durable Opacity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-volatile memory (NVM), also known as persistent memory, is an emerging\nparadigm for memory that preserves its contents even after power loss. NVM is\nwidely expected to become ubiquitous, and hardware architectures are already\nproviding support for NVM programming. This has stimulated interest in the\ndesign of novel concepts ensuring correctness of concurrent programming\nabstractions in the face of persistency and in the development of associated\nverification approaches.\n  Software transactional memory (STM) is a key programming abstraction that\nsupports concurrent access to shared state. In a fashion similar to\nlinearizability as the correctness condition for concurrent data structures,\nthere is an established notion of correctness for STMs known as opacity. We\nhave recently proposed durable opacity as the natural extension of opacity to a\nsetting with non-volatile memory. Together with this novel correctness\ncondition, we designed a verification technique based on refinement. In this\npaper, we extend this work in two directions.\n  First, we develop a durably opaque version of NOrec (no ownership records),\nan existing STM algorithm proven to be opaque. Second, we modularize our\nexisting verification approach by separating the proof of durability of memory\naccesses from the proof of opacity. For NOrec, this allows us to re-use an\nexisting opacity proof and complement it with a proof of the durability of\naccesses to shared state.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:15:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Bila", "Eleni", ""], ["Derrick", "John", ""], ["Doherty", "Simon", ""], ["Dongol", "Brijesh", ""], ["Schellhorn", "Gerhard", ""], ["Wehrheim", "Heike", ""]]}, {"id": "2011.15028", "submitter": "G\\'abor Sz\\'arnyas", "authors": "Alexandru Iosup, Ahmed Musaafir, Alexandru Uta, Arnau Prat P\\'erez,\n  G\\'abor Sz\\'arnyas, Hassan Chafi, Ilie Gabriel T\\u{a}nase, Lifeng Nai,\n  Michael Anderson, Mihai Capot\\u{a}, Narayanan Sundaram, Peter Boncz,\n  Siegfried Depner, Stijn Heldens, Thomas Manhardt, Tim Hegeman, Wing Lung\n  Ngai, Yinglong Xia", "title": "The LDBC Graphalytics Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we describe LDBC Graphalytics, an industrial-grade\nbenchmark for graph analysis platforms. The main goal of Graphalytics is to\nenable the fair and objective comparison of graph analysis platforms. Due to\nthe diversity of bottlenecks and performance issues such platforms need to\naddress, Graphalytics consists of a set of selected deterministic algorithms\nfor full-graph analysis, standard graph datasets, synthetic dataset generators,\nand reference output for validation purposes. Its test harness produces deep\nmetrics that quantify multiple kinds of systems scalability, weak and strong,\nand robustness, such as failures and performance variability. The benchmark\nalso balances comprehensiveness with runtime necessary to obtain the deep\nmetrics. The benchmark comes with open-source software for generating\nperformance data, for validating algorithm results, for monitoring and sharing\nperformance data, and for obtaining the final benchmark result as a standard\nperformance report.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 17:34:37 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 13:59:29 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 18:37:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Iosup", "Alexandru", ""], ["Musaafir", "Ahmed", ""], ["Uta", "Alexandru", ""], ["P\u00e9rez", "Arnau Prat", ""], ["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Chafi", "Hassan", ""], ["T\u0103nase", "Ilie Gabriel", ""], ["Nai", "Lifeng", ""], ["Anderson", "Michael", ""], ["Capot\u0103", "Mihai", ""], ["Sundaram", "Narayanan", ""], ["Boncz", "Peter", ""], ["Depner", "Siegfried", ""], ["Heldens", "Stijn", ""], ["Manhardt", "Thomas", ""], ["Hegeman", "Tim", ""], ["Ngai", "Wing Lung", ""], ["Xia", "Yinglong", ""]]}]