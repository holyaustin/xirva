[{"id": "1204.0334", "submitter": "Francis Lau C.M.", "authors": "Yue Zhao and Francis C. M. Lau", "title": "Implementation Of Decoders for LDPC Block Codes and LDPC Convolutional\n  Codes Based on GPUs", "comments": "9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the use of belief propagation (BP) decoding algorithm, low-density\nparity-check (LDPC) codes can achieve near-Shannon limit performance. In order\nto evaluate the error performance of LDPC codes, simulators running on CPUs are\ncommonly used. However, the time taken to evaluate LDPC codes with very good\nerror performance is excessive. In this paper, efficient LDPC block-code\ndecoders/simulators which run on graphics processing units (GPUs) are proposed.\nWe also implement the decoder for the LDPC convolutional code (LDPCCC). The\nLDPCCC is derived from a pre-designed quasi-cyclic LDPC block code with good\nerror performance. Compared to the decoder based on the randomly constructed\nLDPCCC code, the complexity of the proposed LDPCCC decoder is reduced due to\nthe periodicity of the derived LDPCCC and the properties of the quasi-cyclic\nstructure. In our proposed decoder architecture, $\\Gamma$ (a multiple of a\nwarp) codewords are decoded together and hence the messages of $\\Gamma$\ncodewords are also processed together. Since all the $\\Gamma$ codewords share\nthe same Tanner graph, messages of the $\\Gamma$ distinct codewords\ncorresponding to the same edge can be grouped into one package and stored\nlinearly. By optimizing the data structures of the messages used in the\ndecoding process, both the read and write processes can be performed in a\nhighly parallel manner by the GPUs. In addition, a thread hierarchy minimizing\nthe divergence of the threads is deployed, and it can maximize the efficiency\nof the parallel execution. With the use of a large number of cores in the GPU\nto perform the simple computations simultaneously, our GPU-based LDPC decoder\ncan obtain hundreds of times speedup compared with a serial CPU-based simulator\nand over 40 times speedup compared with an 8-thread CPU-based simulator.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 07:48:02 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2012 02:16:11 GMT"}], "update_date": "2012-07-30", "authors_parsed": [["Zhao", "Yue", ""], ["Lau", "Francis C. M.", ""]]}, {"id": "1204.0414", "submitter": "Samuel Mimram", "authors": "Lisbeth Fajstrup, Eric Goubault (CEA LIST), Emmanuel Haucourt (CEA\n  LIST), Samuel Mimram (CEA LIST), Martin Raussen", "title": "Trace Spaces: an Efficient New Technique for State-Space Reduction", "comments": null, "journal-ref": "ESOP - 21st European Symposium on Programming 7211 (2012) 274-294", "doi": "10.1007/978-3-642-28869-2_14", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space reduction techniques, used primarily in model-checkers, all rely\non the idea that some actions are independent, hence could be taken in any\n(respective) order while put in parallel, without changing the semantics. It is\nthus not necessary to consider all execution paths in the interleaving\nsemantics of a concurrent program, but rather some equivalence classes. The\npurpose of this paper is to describe a new algorithm to compute such\nequivalence classes, and a representative per class, which is based on ideas\noriginating in algebraic topology. We introduce a geometric semantics of\nconcurrent languages, where programs are interpreted as directed topological\nspaces, and study its properties in order to devise an algorithm for computing\ndihomotopy classes of execution paths. In particular, our algorithm is able to\ncompute a control-flow graph for concurrent programs, possibly containing\nloops, which is \"as reduced as possible\" in the sense that it generates traces\nmodulo equivalence. A preliminary implementation was achieved, showing\npromising results towards efficient methods to analyze concurrent programs,\nwith very promising results compared to partial-order reduction techniques.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 14:24:18 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Fajstrup", "Lisbeth", "", "CEA LIST"], ["Goubault", "Eric", "", "CEA LIST"], ["Haucourt", "Emmanuel", "", "CEA\n  LIST"], ["Mimram", "Samuel", "", "CEA LIST"], ["Raussen", "Martin", ""]]}, {"id": "1204.0479", "submitter": "Tobias Buer", "authors": "Tobias Buer and J\\\"org Homberger and Hermann Gehring", "title": "A collaborative ant colony metaheuristic for distributed multi-level\n  lot-sizing", "comments": null, "journal-ref": "International Journal of Production Research 51 (17) 2013,\n  5253-5270", "doi": "10.1080/00207543.2013.802822", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an ant colony optimization metaheuristic for collaborative\nplanning. Collaborative planning is used to coordinate individual plans of\nself-interested decision makers with private information in order to increase\nthe overall benefit of the coalition. The method consists of a new search graph\nbased on encoded solutions. Distributed and private information is integrated\nvia voting mechanisms and via a simple but effective collaborative local search\nprocedure. The approach is applied to a distributed variant of the multi-level\nlot-sizing problem and evaluated by means of 352 benchmark instances from the\nliterature. The proposed approach clearly outperforms existing approaches on\nthe sets of medium and large sized instances. While the best method in the\nliterature so far achieves an average deviation from the best known\nnon-distributed solutions of 46 percent for the set of the largest instances,\nfor example, the presented approach reduces the average deviation to only 5\npercent.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 17:44:45 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Buer", "Tobias", ""], ["Homberger", "J\u00f6rg", ""], ["Gehring", "Hermann", ""]]}, {"id": "1204.0641", "submitter": "Peter Robinson", "authors": "Martin Biely, Peter Robinson, Ulrich Schmid", "title": "Agreement in Directed Dynamic Networks", "comments": "A preliminary version of this paper appeared in SIROCCO 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed computation in synchronous dynamic networks where an\nomniscient adversary controls the unidirectional communication links. Its\nbehavior is modeled as a sequence of directed graphs representing the active\n(i.e. timely) communication links per round. We prove that consensus is\nimpossible under some natural weak connectivity assumptions, and introduce\nvertex-stable root components as a means for circumventing this impossibility.\nEssentially, we assume that there is a short period of time during which an\narbitrary part of the network remains strongly connected, while its\ninterconnect topology may keep changing continuously. We present a consensus\nalgorithm that works under this assumption, and prove its correctness. Our\nalgorithm maintains a local estimate of the communication graphs, and applies\ntechniques for detecting stable network properties and univalent system\nconfigurations. Our possibility results are complemented by several\nimpossibility results and lower bounds for consensus and other distributed\ncomputing problems like leader election, revealing that our algorithm is\nasymptotically optimal.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 10:00:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 01:23:53 GMT"}, {"version": "v3", "created": "Fri, 18 May 2012 07:46:43 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2012 18:45:37 GMT"}, {"version": "v5", "created": "Wed, 8 May 2013 07:54:28 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Biely", "Martin", ""], ["Robinson", "Peter", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1204.0939", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy, Anne Benoit, Fanny Dufoss\\'e and Yves Robert", "title": "Reclaiming the energy of a schedule: models and algorithms", "comments": "A two-page extended abstract of this work appeared as a short\n  presentation in SPAA'2011, while the long version has been accepted for\n  publication in \"Concurrency and Computation: Practice and Experience\"", "journal-ref": null, "doi": "10.1002/cpe.2889", "report-no": "INRIA Research report 7598", "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a task graph to be executed on a set of processors. We assume\nthat the mapping is given, say by an ordered list of tasks to execute on each\nprocessor, and we aim at optimizing the energy consumption while enforcing a\nprescribed bound on the execution time. While it is not possible to change the\nallocation of a task, it is possible to change its speed. Rather than using a\nlocal approach such as backfilling, we consider the problem as a whole and\nstudy the impact of several speed variation models on its complexity. For\ncontinuous speeds, we give a closed-form formula for trees and series-parallel\ngraphs, and we cast the problem into a geometric programming problem for\ngeneral directed acyclic graphs. We show that the classical dynamic voltage and\nfrequency scaling (DVFS) model with discrete modes leads to a NP-complete\nproblem, even if the modes are regularly distributed (an important particular\ncase in practice, which we analyze as the incremental model). On the contrary,\nthe VDD-hopping model leads to a polynomial solution. Finally, we provide an\napproximation algorithm for the incremental model, which we extend for the\ngeneral DVFS model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 12:53:48 GMT"}], "update_date": "2012-08-03", "authors_parsed": [["Aupy", "Guillaume", ""], ["Benoit", "Anne", ""], ["Dufoss\u00e9", "Fanny", ""], ["Robert", "Yves", ""]]}, {"id": "1204.1106", "submitter": "Matt Kraning", "authors": "Matt Kraning and Eric Chu and Javad Lavaei and Stephen Boyd", "title": "Message Passing for Dynamic Network Energy Management", "comments": "Submitted to IEEE Transactions on Smart grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a network of devices, such as generators, fixed loads, deferrable\nloads, and storage devices, each with its own dynamic constraints and\nobjective, connected by lossy capacitated lines. The problem is to minimize the\ntotal network objective subject to the device and line constraints, over a\ngiven time horizon. This is a large optimization problem, with variables for\nconsumption or generation in each time period for each device. In this paper we\ndevelop a decentralized method for solving this problem. The method is\niterative: At each step, each device exchanges simple messages with its\nneighbors in the network and then solves its own optimization problem,\nminimizing its own objective function, augmented by a term determined by the\nmessages it has received. We show that this message passing method converges to\na solution when the device objective and constraints are convex. The method is\ncompletely decentralized, and needs no global coordination other than\nsynchronizing iterations; the problems to be solved by each device can\ntypically be solved extremely efficiently and in parallel. The method is fast\nenough that even a serial implementation can solve substantial problems in\nreasonable time frames. We report results for several numerical experiments,\ndemonstrating the method's speed and scaling, including the solution of a\nproblem instance with over 30 million variables in 52 minutes for a serial\nimplementation; with decentralized computing, the solve time would be less than\none second.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 01:45:40 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Kraning", "Matt", ""], ["Chu", "Eric", ""], ["Lavaei", "Javad", ""], ["Boyd", "Stephen", ""]]}, {"id": "1204.1140", "submitter": "Sasko Ristov", "authors": "Sasko Ristov, Marjan Gusev and Magdalena Kostoska", "title": "Cloud Computing Security in Business Information Systems", "comments": null, "journal-ref": "S. Ristov, M. Gusev, and M. Kostoska, \"Cloud computing security in\n  business information systems,\" International Journal of Network Security &\n  Its Applications (IJNSA), vol. 4, no. 2, pp. 75-93, 2012", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing providers' and customers' services are not only exposed to\nexisting security risks, but, due to multi-tenancy, outsourcing the application\nand data, and virtualization, they are exposed to the emergent, as well.\nTherefore, both the cloud providers and customers must establish information\nsecurity system and trustworthiness each other, as well as end users. In this\npaper we analyze main international and industrial standards targeting\ninformation security and their conformity with cloud computing security\nchallenges. We evaluate that almost all main cloud service providers (CSPs) are\nISO 27001:2005 certified, at minimum. As a result, we propose an extension to\nthe ISO 27001:2005 standard with new control objective about virtualization, to\nretain generic, regardless of company's type, size and nature, that is, to be\napplicable for cloud systems, as well, where virtualization is its baseline. We\nalso define a quantitative metric and evaluate the importance factor of ISO\n27001:2005 control objectives if customer services are hosted on-premise or in\ncloud. The conclusion is that obtaining the ISO 27001:2005 certificate (or if\nalready obtained) will further improve CSP and CC information security systems,\nand introduce mutual trust in cloud services but will not cover all relevant\nissues. In this paper we also continue our efforts in business continuity\ndetriments cloud computing produces, and propose some solutions that mitigate\nthe risks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 08:08:04 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Ristov", "Sasko", ""], ["Gusev", "Marjan", ""], ["Kostoska", "Magdalena", ""]]}, {"id": "1204.1178", "submitter": "Atsushi Inoie", "authors": "Tomoyuki Ishii and Atsushi Inoie", "title": "An initial peer configuration algorithm for multi-streaming peer-to-peer\n  networks", "comments": "10 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of the Internet technology enables us to use network applications\nfor streaming audio and video. Especially, real-time streaming services using\npeer-to-peer (P2P) technology are currently emerging. An important issue on P2P\nstreaming is how to construct a logical network (overlay network) on a physical\nnetwork (IP network). In this paper, we propose an initial peer configuration\nalgorithm for a multi-streaming peer-to-peer network. The proposed algorithm is\nbased on a mesh-pull approach where any node has multiple parent and child\nnodes as neighboring nodes, and content transmitted between these neighboring\nnodes depends on their parent-child relationships. Our simulation experiments\nshow that the proposed algorithm improves the number of joining node and\ntraffic load.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 10:58:12 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Ishii", "Tomoyuki", ""], ["Inoie", "Atsushi", ""]]}, {"id": "1204.1225", "submitter": "Nasrin Jaberi", "authors": "Masnida Emami, Ali Setayesh, Nasrin Jaberi", "title": "Distributed computing of Seismic Imaging Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary use of technical computing in the oil and gas industries is for\nseismic imaging of the earth's subsurface, driven by the business need for\nmaking well-informed drilling decisions during petroleum exploration and\nproduction. Since each oil/gas well in exploration areas costs several tens of\nmillions of dollars, producing high-quality seismic images in a reasonable time\ncan significantly reduce the risk of drilling a \"dry hole\". Similarly, these\nimages are important as they can improve the position of wells in a\nbillion-dollar producing oil field. However seismic imaging is very data- and\ncompute-intensive which needs to process terabytes of data and require\nGflop-years of computation (using \"flop\" to mean floating point operation per\nsecond). Due to the data/computing intensive nature of seismic imaging,\nparallel computing are used to process data to reduce the time compilation.\n  With introducing of Cloud computing, MapReduce programming model has been\nattracted a lot of attention in parallel and distributed systems [1, 2] to\nexecute massive processing algorithms such as Bioinformatics[3], Astronomy[4],\nGeology[5] and so on. In this report, we will investigate and discuss current\napproaches to fit seismic algorithms to MapReduce programming model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 13:43:23 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Emami", "Masnida", ""], ["Setayesh", "Ali", ""], ["Jaberi", "Nasrin", ""]]}, {"id": "1204.1373", "submitter": "Paulo Jesus", "authors": "Miguel Borges, Paulo Jesus, Carlos Baquero, Paulo S\\'ergio Almeida", "title": "Spectra: Robust Estimation of Distribution Functions in Networks", "comments": "Full version of the paper published at 12th IFIP International\n  Conference on Distributed Applications and Interoperable Systems (DAIS),\n  Stockholm (Sweden), June 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed aggregation allows the derivation of a given global aggregate\nproperty from many individual local values in nodes of an interconnected\nnetwork system. Simple aggregates such as minima/maxima, counts, sums and\naverages have been thoroughly studied in the past and are important tools for\ndistributed algorithms and network coordination. Nonetheless, this kind of\naggregates may not be comprehensive enough to characterize biased data\ndistributions or when in presence of outliers, making the case for richer\nestimates of the values on the network. This work presents Spectra, a\ndistributed algorithm for the estimation of distribution functions over large\nscale networks. The estimate is available at all nodes and the technique\ndepicts important properties, namely: robust when exposed to high levels of\nmessage loss, fast convergence speed and fine precision in the estimate. It can\nalso dynamically cope with changes of the sampled local property, not requiring\nalgorithm restarts, and is highly resilient to node churn. The proposed\napproach is experimentally evaluated and contrasted to a competing state of the\nart distribution aggregation technique.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 22:53:35 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Borges", "Miguel", ""], ["Jesus", "Paulo", ""], ["Baquero", "Carlos", ""], ["Almeida", "Paulo S\u00e9rgio", ""]]}, {"id": "1204.1516", "submitter": "Gaurav Sharma", "authors": "Rajesh Kumar Bawa, Gaurav Sharma", "title": "Reliable Resource Selection in Grid Environment", "comments": "IJGCA, March 2012, Volume 3, Number 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary concern in area of computational grid is security and resources.\nMost of the existing grids address this problem by authenticating the users,\nhosts and their interactions in an appropriate manner. A secured system is\ncompulsory for the efficient utilization of grid services. The high degree of\nstrangeness has been identified as the problem factors in the secured selection\nof grid. Without the assurance of a higher degree of trust relationship,\ncompetent resource selection and utilization cannot be achieved. In this paper\nwe proposed an approach which is providing reliability and reputation aware\nsecurity for resource selection in grid environment. In this approach, the\nself-protection capability and reputation weightage is utilized to obtain the\nReliability Factor (RF) value. Therefore jobs are allocated to the resources\nthat posses higher RF values. Extensive experimental evaluation shows that as\nhigher trust and reliable nodes are selected the chances of failure decreased\ndrastically.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 17:09:53 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Bawa", "Rajesh Kumar", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1204.1710", "submitter": "Dhyanendra  Jain", "authors": "Dhyanendra Jain", "title": "Hiding Sensitive Association Rules without Altering the Support of\n  Sensitive Item(s)", "comments": "10 pages", "journal-ref": null, "doi": "10.5121/ijaia.2012.3207", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Association rule mining is an important data-mining technique that finds\ninteresting association among a large set of data items. Since it may disclose\npatterns and various kinds of sensitive knowledge that are difficult to find\notherwise, it may pose a threat to the privacy of discovered confidential\ninformation. Such information is to be protected against unauthorized access.\nMany strategies had been proposed to hide the information. Some use distributed\ndatabases over several sites, data perturbation, clustering, and data\ndistortion techniques. Hiding sensitive rules problem, and still not\nsufficiently investigated, is the requirement to balance the confidentiality of\nthe disclosed data with the legitimate needs of the user. The proposed approach\nuses the data distortion technique where the position of the sensitive items is\naltered but its support is never changed. The size of the database remains the\nsame. It uses the idea of representative rules to prune the rules first and\nthen hides the sensitive rules. Advantage of this approach is that it hides\nmaximum number of rules however, the existing approaches fail to hide all the\ndesired rules, which are supposed to be hidden in minimum number of passes. The\npaper also compares of the proposed approach with existing ones.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2012 04:52:49 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Jain", "Dhyanendra", ""]]}, {"id": "1204.1754", "submitter": "Anish Das Sarma", "authors": "Foto N. Afrati, Anish Das Sarma, Semih Salihoglu, Jeffrey D. Ullman", "title": "Vision Paper: Towards an Understanding of the Limits of Map-Reduce\n  Computation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of recent research work has addressed the problem of\nsolving various data management problems in the cloud. The major algorithmic\nchallenges in map-reduce computations involve balancing a multitude of factors\nsuch as the number of machines available for mappers/reducers, their memory\nrequirements, and communication cost (total amount of data sent from mappers to\nreducers). Most past work provides custom solutions to specific problems, e.g.,\nperforming fuzzy joins in map-reduce, clustering, graph analyses, and so on.\nWhile some problems are amenable to very efficient map-reduce algorithms, some\nother problems do not lend themselves to a natural distribution, and have\nprovable lower bounds. Clearly, the ease of \"map-reducability\" is closely\nrelated to whether the problem can be partitioned into independent pieces,\nwhich are distributed across mappers/reducers. What makes a problem\ndistributable? Can we characterize general properties of problems that\ndetermine how easy or hard it is to find efficient map-reduce algorithms?\n  This is a vision paper that attempts to answer the questions described above.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2012 19:10:07 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Afrati", "Foto N.", ""], ["Sarma", "Anish Das", ""], ["Salihoglu", "Semih", ""], ["Ullman", "Jeffrey D.", ""]]}, {"id": "1204.1939", "submitter": "Colin Cooper", "authors": "Petra Berenbrink, Colin Cooper, Tom Friedetzky", "title": "Random walks which prefer unvisited edges. Exploring high girth even\n  degree expanders in linear time", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a modified random walk which uses unvisited edges whenever\npossible, and makes a simple random walk otherwise. We call such a walk an\nedge-process. We assume there is a rule A, which tells the walk which unvisited\nedge to use whenever there is a choice. In the simplest case, A is a uniform\nrandom choice over unvisited edges incident with the current walk position.\nHowever we do not exclude arbitrary choices of rule A. For example, the rule\ncould be determined on-line by an adversary, or could vary from vertex to\nvertex.\n  For even degree expander graphs, of bounded maximum degree, we have the\nfollowing result. Let G be an n vertex even degree expander graph, for which\nevery vertex is in at least one vertex induced cycle of length L. Any\nedge-process on G has cover time (n+ (n log n)/L). This result is independent\nof the rule A used to select the order of the unvisited edges, which can be\nchosen on-line by an adversary.\n  As an example, With high probability, random r-regular graphs, (r at least 4,\neven), are expanders for which L = Omega(log n). Thus, for almost all such\ngraphs, the vertex cover time of the edge-process is Theta(n). This improves\nthe vertex cover time of such graphs by a factor of log n, compared to the\nOmega(n log n) cover time of any weighted random walk.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 17:52:17 GMT"}, {"version": "v2", "created": "Sat, 5 May 2012 08:36:13 GMT"}, {"version": "v3", "created": "Sun, 27 May 2012 11:31:11 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Berenbrink", "Petra", ""], ["Cooper", "Colin", ""], ["Friedetzky", "Tom", ""]]}, {"id": "1204.2204", "submitter": "Rajiv Ranjan Dr.", "authors": "Rajiv Ranjan and Boualem Benatallah", "title": "Programming Cloud Resource Orchestration Framework: Operations and\n  Research Challenges", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of cloud computing over the past five years is potentially one\nof the breakthrough advances in the history of computing. It delivers hardware\nand software resources as virtualization-enabled services and in which\nadministrators are free from the burden of worrying about the low level\nimplementation or system administration details. Although cloud computing\noffers considerable opportunities for the users (e.g. application developers,\ngovernments, new startups, administrators, consultants, scientists, business\nanalyst, etc.) such as no up-front investment, lowering operating cost, and\ninfinite scalability, it has many unique research challenges that need to be\ncarefully addressed in the future. In this paper, we present a survey on key\ncloud computing concepts, resource abstractions, and programming operations for\norchestrating resources and associated research challenges, wherever\napplicable.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 16:06:46 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 08:27:57 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2012 05:29:55 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Ranjan", "Rajiv", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1204.2280", "submitter": "Jeroen B\\'edorf", "authors": "Jeroen B\\'edorf and Evghenii Gaburov and Simon Portegies Zwart", "title": "Bonsai: A GPU Tree-Code", "comments": "5 pages, 2 figures. Proceedings of \"Advances in Computational\n  Astrophysics: methods, tools and outcomes\", June 13-17, 2011, Cefalu, Sicily,\n  Italy, eds. Capuzzo Dolcetta, Limongi, Tornambe and Giobbi", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a gravitational hierarchical N-body code that is designed to run\nefficiently on Graphics Processing Units (GPUs). All parts of the algorithm are\nexecuted on the GPU which eliminates the need for data transfer between the\nCentral Processing Unit (CPU) and the GPU. Our tests indicate that the\ngravitational tree-code outperforms tuned CPU code for all parts of the\nalgorithm and show an overall performance improvement of more than a factor 20,\nresulting in a processing rate of more than 2.8 million particles per second.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 20:36:14 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["B\u00e9dorf", "Jeroen", ""], ["Gaburov", "Evghenii", ""], ["Zwart", "Simon Portegies", ""]]}, {"id": "1204.2320", "submitter": "Muhammad Abdullah Adnan", "authors": "Muhammad Abdullah Adnan, Ryo Sugihara and Rajesh Gupta", "title": "Energy Efficient Geographical Load Balancing via Dynamic Deferral of\n  Workload", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of Cloud computing and Mobile computing,\nindividuals, enterprises and research centers have started outsourcing their IT\nand computational needs to on-demand cloud services. Recently geographical load\nbalancing techniques have been suggested for data centers hosting cloud\ncomputation in order to reduce energy cost by exploiting the electricity price\ndifferences across regions. However, these algorithms do not draw distinction\namong diverse requirements for responsiveness across various workloads. In this\npaper, we use the flexibility from the Service Level Agreements (SLAs) to\ndifferentiate among workloads under bounded latency requirements and propose a\nnovel approach for cost savings for geographical load balancing. We investigate\nhow much workload to be executed in each data center and how much workload to\nbe delayed and migrated to other data centers for energy saving while meeting\ndeadlines. We present an offline formulation for geographical load balancing\nproblem with dynamic deferral and give online algorithms to determine the\nassignment of workload to the data centers and the migration of workload\nbetween data centers in order to adapt with dynamic electricity price changes.\nWe compare our algorithms with the greedy approach and show that significant\ncost savings can be achieved by migration of workload and dynamic deferral with\nfuture electricity price prediction. We validate our algorithms on MapReduce\ntraces and show that geographic load balancing with dynamic deferral can\nprovide 20-30% cost-savings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 02:20:37 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Adnan", "Muhammad Abdullah", ""], ["Sugihara", "Ryo", ""], ["Gupta", "Rajesh", ""]]}, {"id": "1204.2536", "submitter": "Hans P. Reiser", "authors": "Benedikt H\\\"ofling, Hans P. Reiser", "title": "SecureSMART: A Security Architecture for BFT Replication Libraries", "comments": "2 pages, EDCC 2012 fast abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several research projects have shown that Byzantine fault tolerance (BFT) is\npractical today in terms of performance. Deficiencies in other aspects might\nstill be an obstacle to a more wide-spread deployment in real-world\napplications. One of these aspects is an over-all security architecture beyond\nthe low-level protocol. This paper proposes the security architecture\nSecureSMART, which provides dynamic key distribution, internal and external\nintegrity and confidentiality measures, as well as mechanisms for availability\nand access control. For this purpose, it implements security mechanism among\nclients, nodes and an external trust center.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 19:54:30 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["H\u00f6fling", "Benedikt", ""], ["Reiser", "Hans P.", ""]]}, {"id": "1204.2566", "submitter": "Julien Lange", "authors": "Julien Lange and Emilio Tuosto", "title": "Synthesising Choreographies from Local Session Types (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and analysing multiparty distributed interactions can be achieved\neither by means of a global view (e.g. in choreography-based approaches) or by\ncomposing available computational entities (e.g. in service orchestration).\nThis paper proposes a typing systems which allows, under some conditions, to\nsynthesise a choreography (i.e. a multiparty global type) from a set of local\nsession types which describe end-point behaviours (i.e. local types).\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 20:52:29 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Lange", "Julien", ""], ["Tuosto", "Emilio", ""]]}, {"id": "1204.3052", "submitter": "Vasanth Raja Chittampally", "authors": "Chittampally Vasanth Raja, Srinivas Balasubramanian, Prakash S\n  Raghavendra", "title": "Heterogeneous Highly Parallel Implementation of Matrix Exponentiation\n  Using GPU", "comments": "15 pages, 12 figures, International Journal of Distributed and\n  Parallel systems (IJDPS) ISSN : 0976 - 9757 [Online] ; 2229 - 3957 [Print]", "journal-ref": "International Journal of Distributed and Parallel Systems, Vol 3,\n  No. 2, March 2012 issue", "doi": "10.5121/ijdps.2012.3209", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The vision of super computer at every desk can be realized by powerful and\nhighly parallel CPUs or GPUs or APUs. Graphics processors once specialized for\nthe graphics applications only, are now used for the highly computational\nintensive general purpose applications. Very expensive GFLOPs and TFLOP\nperformance has become very cheap with the GPGPUs. Current work focuses mainly\non the highly parallel implementation of Matrix Exponentiation. Matrix\nExponentiation is widely used in many areas of scientific community ranging\nfrom highly critical flight, CAD simulations to financial, statistical\napplications. Proposed solution for Matrix Exponentiation uses OpenCL for\nexploiting the hyper parallelism offered by the many core GPGPUs. It employs\nmany general GPU optimizations and architectural specific optimizations. This\nexperimentation covers the optimizations targeted specific to the Scientific\nGraphics cards (Tesla-C2050). Heterogeneous Highly Parallel Matrix\nExponentiation method has been tested for matrices of different sizes and with\ndifferent powers. The devised Kernel has shown 1000X speedup and 44 fold\nspeedup with the naive GPU Kernel.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 17:13:50 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Raja", "Chittampally Vasanth", ""], ["Balasubramanian", "Srinivas", ""], ["Raghavendra", "Prakash S", ""]]}, {"id": "1204.3058", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Paola Flocchini, Bernard Mans, Nicola Santoro", "title": "Building Fastest Broadcast Trees in Periodically-Varying Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delay-tolerant networks (DTNs) are characterized by a possible absence of\nend-to-end communication routes at any instant. Still, connectivity can\ngenerally be established over time and space. The optimality of a temporal path\n(journey) in this context can be defined in several terms, including\ntopological (e.g. {\\em shortest} in hops) and temporal (e.g. {\\em fastest,\nforemost}). The combinatorial problem of computing shortest, foremost, and\nfastest journeys {\\em given full knowledge} of the network schedule was\naddressed a decade ago (Bui-Xuan {\\it et al.}, 2003). A recent line of research\nhas focused on the distributed version of this problem, where foremost,\nshortest or fastest {\\em broadcast} are performed without knowing the schedule\nbeforehand. In this paper we show how to build {\\em fastest} broadcast trees\n(i.e., trees that minimize the global duration of the broadcast, however late\nthe departure is) in Time-Varying Graphs where intermittent edges are available\nperiodically (it is known that the problem is infeasible in the general case\neven if various parameters of the graph are know). We address the general case\nwhere contacts between nodes can have arbitrary durations and thus fastest\nroutes may consist of a mixture of {\\em continuous} and {\\em discontinuous}\nsegments (a more complex scenario than when contacts are {\\em punctual} and\nthus routes are only discontinuous). Using the abstraction of \\tclocks to\ncompute the temporal distances, we solve the fastest broadcast problem by first\nlearning, at the emitter, what is its time of {\\em minimum temporal\neccentricity} (i.e. the fastest time to reach all the other nodes), and second\nby building a {\\em foremost} broadcast tree relative to this particular\nemission date.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2012 17:25:28 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Flocchini", "Paola", ""], ["Mans", "Bernard", ""], ["Santoro", "Nicola", ""]]}, {"id": "1204.3471", "submitter": "Arockia Anand Raj", "authors": "Arockia Anand Raj and T. Mala", "title": "Cloudpress 2.0: A MapReduce Approach for News Retrieval on the Cloud", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of the Internet, the amount of news articles added every minute\nof everyday is humongous. As a result of this explosive amount of news\narticles, news retrieval systems are required to process the news articles\nfrequently and intensively. The news retrieval systems that are in-use today\nare not capable of coping up with these data-intensive computations. Cloudpress\n2.0 presented here, is designed and implemented to be scalable, robust and\nfault tolerant. It is designed in such a way that, all the processes involved\nin news retrieval such as fetching, pre-processing, indexing, storing and\nsummarizing, exploit MapReduce paradigm and use the power of the Cloud\ncomputing. It uses novel approaches for parallel processing, for storing the\nnews articles in a distributed database and for visualizing them as a 3D\nvisual. It uses Lucene-based indexing for efficient and faster retrieval. It\nalso includes a novel query expansion feature for searching the news articles.\nCloudpress 2.0 also allows on-the-fly, extractive summarization of news\narticles based on the input query.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 13:06:59 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Raj", "Arockia Anand", ""], ["Mala", "T.", ""]]}, {"id": "1204.3494", "submitter": "Ian Kash", "authors": "Ian A. Kash, Eric J. Friedman, Joseph Y. Halpern", "title": "Optimizing Scrip Systems: Crashes, Altruists, Hoarders, Sybils and\n  Collusion", "comments": "24 pages. Forthcoming in Distributed Computing. Premliminary versions\n  of this material appeared in the Proceedings of the 7th and 8th ACM\n  Conferences on Electronic Commerce [14,26] and the Proceedings of the First\n  Conference on Auctions, Market Mechanisms and Their Applications [27]. These\n  are also available as arXiv:0705.4094, arXiv:0705.4110, and arXiv:0903.2278", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scrip, or artificial currency, is a useful tool for designing systems that\nare robust to selfish behavior by users. However, it also introduces problems\nfor a system designer, such as how the amount of money in the system should be\nset. In this paper, the effect of varying the total amount of money in a scrip\nsystem on efficiency (i.e., social welfare---the total utility of all the\nagents in the system) is analyzed, and it is shown that by maintaining the\nappropriate ratio between the total amount of money and the number of agents,\nefficiency is maximized. This ratio can be found by increasing the money supply\nto just below the point that the system would experience a \"monetary crash,\"\nwhere money is sufficiently devalued that no agent is willing to perform a\nservice. The implications of the presence of altruists, hoarders, sybils, and\ncollusion on the performance of the system are examined. Approaches are\ndiscussed to identify the strategies and types of agents.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 14:20:20 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Kash", "Ian A.", ""], ["Friedman", "Eric J.", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "1204.3553", "submitter": "Lawrence Bryan", "authors": "B. N. Lawrence, V. Bennett, J. Churchill, M. Juckes, P. Kershaw, P.\n  Oliver, M. Pritchard, and A. Stephens", "title": "The JASMIN super-data-cluster", "comments": "Submitted to Supercomputing 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The JASMIN super-data-cluster is being deployed to support the data analysis\nrequirements of the UK and European climate and earth system modelling\ncommunity. Physical colocation of the core JASMIN resource with significant\ncomponents of the facility for Climate and Environmental Monitoring from Space\n(CEMS) provides additional support for the earth observation community, as well\nas facilitating further comparison and evaluation of models with data. JASMIN\nand CEMS together centrally deploy 9.3 PB of storage - 4.6 PB of Panasas fast\ndisk storage alongside the STFC Atlas Tape Store. Over 370 computing cores\nprovide local computation. Remote JASMIN resources at Bristol, Leeds and\nReading provide additional distributed storage and compute configured to\nsupport local workflow as a stepping stone to using the central JASMIN system.\nFast network links from JASMIN provide reliable communication between the UK\nsupercomputers MONSooN (at the Met Office) and HECToR (at the University of\nEdinburgh). JASMIN also supports European users via a light path to KNMI in the\nNetherlands. The functional components of the JASMIN infrastructure have been\ndesigned to support and integrate workflows for three main goals: (1) the\nefficient operation of data curation and facilitation at the STFC Centre for\nEnvironmental Data Archival; (2) efficient data analysis by the UK and European\nclimate and earth system science communities, and; (3) flexible access for the\nclimate impacts and earth observation communities to complex data and\nconcomitant services.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 16:17:22 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Lawrence", "B. N.", ""], ["Bennett", "V.", ""], ["Churchill", "J.", ""], ["Juckes", "M.", ""], ["Kershaw", "P.", ""], ["Oliver", "P.", ""], ["Pritchard", "M.", ""], ["Stephens", "A.", ""]]}, {"id": "1204.4044", "submitter": "Xavier Vila\\c{c}a", "authors": "Xavier Vila\\c{c}a, Oksana Denysyuk, Lu\\'is Rodrigues", "title": "Asynchrony and Collusion in the N-party BAR Transfer Problem", "comments": "13 pages, 3 algorithms, to appear in Proceedings of the 19th\n  International Colloquium on Structural Information and Communication\n  Complexity (SIROCCO 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of reliably transferring data from a set of $N_P$ producers to a\nset of $N_C$ consumers in the BAR model, named N-party BAR Transfer (NBART), is\nan important building block for volunteer computing systems. An algorithm to\nsolve this problem in synchronous systems, which provides a Nash equilibrium,\nhas been presented in previous work. In this paper, we propose an NBART\nalgorithm for asynchronous systems. Furthermore, we also address the\npossibility of collusion among the Rational processes. Our game theoretic\nanalysis shows that the proposed algorithm tolerates certain degree of\narbitrary collusion, while still fulfilling the NBART properties.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 11:01:42 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Vila\u00e7a", "Xavier", ""], ["Denysyuk", "Oksana", ""], ["Rodrigues", "Lu\u00eds", ""]]}, {"id": "1204.4086", "submitter": "Jose Gracia", "authors": "Jose Gracia, Christoph Niethammer, Manuel Hasert, Steffen Brinkmann,\n  Rainer Keller, Colin W. Glass", "title": "Hybrid MPI/StarSs - a case study", "comments": "8 pages, accepted for publication in ISPA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid parallel programming models combining distributed and shared memory\nparadigms are well established in high-performance computing. The classical\nprototype of hybrid programming in HPC is MPI/OpenMP, but many other\ncombinations are being investigated. Recently, the data-dependency driven, task\nparallel model for shared memory parallelisation named StarSs has been\nsuggested for usage in combination with MPI. In this paper we apply hybrid\nMPI/StarSs to a Lattice-Boltzmann code. In particular, we present the hybrid\nprogramming model, the benefits we expect, the challenges in porting, and\nfinally a comparison of the performance of MPI/StarSs hybrid, MPI/OpenMP hybrid\nand the original MPI-only versions of the same code.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 14:06:14 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Gracia", "Jose", ""], ["Niethammer", "Christoph", ""], ["Hasert", "Manuel", ""], ["Brinkmann", "Steffen", ""], ["Keller", "Rainer", ""], ["Glass", "Colin W.", ""]]}, {"id": "1204.4176", "submitter": "David Doty", "authors": "Ho-Lin Chen, David Doty, David Soloveichik", "title": "Deterministic Function Computation with Chemical Reaction Networks", "comments": "fixed errors in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical reaction networks (CRNs) formally model chemistry in a well-mixed\nsolution. CRNs are widely used to describe information processing occurring in\nnatural cellular regulatory networks, and with upcoming advances in synthetic\nbiology, CRNs are a promising language for the design of artificial molecular\ncontrol circuitry. Nonetheless, despite the widespread use of CRNs in the\nnatural sciences, the range of computational behaviors exhibited by CRNs is not\nwell understood.\n  CRNs have been shown to be efficiently Turing-universal when allowing for a\nsmall probability of error. CRNs that are guaranteed to converge on a correct\nanswer, on the other hand, have been shown to decide only the semilinear\npredicates. We introduce the notion of function, rather than predicate,\ncomputation by representing the output of a function f:N^k --> N^l by a count\nof some molecular species, i.e., if the CRN starts with n_1,...,n_k molecules\nof some \"input\" species X1,...,Xk, the CRN is guaranteed to converge to having\nf(n_1,...,n_k) molecules of the \"output\" species Y1,...,Yl. We show that a\nfunction f:N^k --> N^l is deterministically computed by a CRN if and only if\nits graph {(x,y) | f(x) = y} is a semilinear set. Furthermore, each semilinear\nfunction f can be computed on input x in expected time O(polylog(|x|)).\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 19:57:14 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2012 21:25:31 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2013 08:18:05 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Chen", "Ho-Lin", ""], ["Doty", "David", ""], ["Soloveichik", "David", ""]]}, {"id": "1204.4475", "submitter": "James Swift Ph.D.", "authors": "John M. Neuberger, Nandor Sieben and James W. Swift", "title": "An MPI Implementation of a Self-Submitting Parallel Job Queue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and easy to apply methodology for using high-level\nself-submitting parallel job queues in an MPI environment. Using C++, we\nimplemented a library of functions, MPQueue, both for testing our concepts and\nfor use in real applications. In particular, we have applied our ideas toward\nsolving computational combinatorics problems and for finding bifurcation\ndiagrams of solutions of partial differential equations (PDE). Our method is\ngeneral and can be applied in many situations without a lot of programming\neffort. The key idea is that workers themselves can easily submit new jobs to\nthe currently running job queue. Our applications involve complicated data\nstructures, so we employ serialization to allow data to be effortlessly passed\nbetween nodes. Using our library, one can solve large problems in parallel\nwithout being an expert in MPI. We demonstrate our methodology and the features\nof the library with several example programs, and give some results from our\ncurrent PDE research. We show that our techniques are efficient and effective\nvia overhead and scaling experiments.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 21:11:15 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Neuberger", "John M.", ""], ["Sieben", "Nandor", ""], ["Swift", "James W.", ""]]}, {"id": "1204.4565", "submitter": "Swan Dubois", "authors": "Swan Dubois (LIP6), S\\'ebastien Tixeuil (LIP6, IUF), Nini Zhu (LIP6)", "title": "Mariages et Trahisons", "comments": null, "journal-ref": "14\\`emes Rencontres Francophones sur les Aspects Algorithmiques\n  des T\\'el\\'ecommunications (AlgoTel), France (2012)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-stabilizing protocol tolerates by definition transient faults (faults\nof finite duration). Recently, a new class of self-stabilizing protocols that\nare able to tolerate a given number of permanent faults. In this paper, we\nfocus on self-stabilizing protocols able to tolerate Byzantine faults, that is\nfaults that introduce an arbitrary behaviour. We focus on strict-stabilization\nin which the system have to contain the effects of Byzantine faults.\nSpecificaly, we study the possibility to construct in a self-stabilizing way a\nmaximal matching in a network where an arbitrary number of process may become\nByzantine.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 08:59:01 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Dubois", "Swan", "", "LIP6"], ["Tixeuil", "S\u00e9bastien", "", "LIP6, IUF"], ["Zhu", "Nini", "", "LIP6"]]}, {"id": "1204.4779", "submitter": "Takayuki Muranushi", "authors": "Takayuki Muranushi", "title": "Paraiso : An Automated Tuning Framework for Explicit Solvers of Partial\n  Differential Equations", "comments": "52 pages, 14 figures, accepted for publications in Computational\n  Science and Discovery", "journal-ref": null, "doi": "10.1088/1749-4699/5/1/015003", "report-no": null, "categories": "astro-ph.IM cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Paraiso, a domain specific language embedded in functional\nprogramming language Haskell, for automated tuning of explicit solvers of\npartial differential equations (PDEs) on GPUs as well as multicore CPUs. In\nParaiso, one can describe PDE solving algorithms succinctly using tensor\nequations notation. Hydrodynamic properties, interpolation methods and other\nbuilding blocks are described in abstract, modular, re-usable and combinable\nforms, which lets us generate versatile solvers from little set of Paraiso\nsource codes.\n  We demonstrate Paraiso by implementing a compressive hydrodynamics solver. A\nsingle source code less than 500 lines can be used to generate solvers of\narbitrary dimensions, for both multicore CPUs and GPUs. We demonstrate both\nmanual annotation based tuning and evolutionary computing based automated\ntuning of the program.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 05:19:03 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 00:11:04 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Muranushi", "Takayuki", ""]]}, {"id": "1204.5028", "submitter": "Nicolas Le Scouarnec", "authors": "Steve Jiekak, Anne-Marie Kermarrec, Nicolas Le Scouarnec, Gilles\n  Straub and Alexandre Van Kempen", "title": "Regenerating Codes: A System Perspective", "comments": "10 pages, 24 figures; Published in ACM SIGOPS Operating System Review\n  (July 2013). Extended version of a paper published at DISCCO 2012 (IEEE :\n  http://dx.doi.org/10.1109/SRDS.2012.58)", "journal-ref": null, "doi": "10.1145/2506164.2506170", "report-no": null, "categories": "cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of the amount of data stored in cloud systems calls for more\nefficient paradigms for redundancy. While replication is widely used to ensure\ndata availability, erasure correcting codes provide a much better trade-off\nbetween storage and availability. Regenerating codes are good candidates for\nthey also offer low repair costs in term of network bandwidth. While they have\nbeen proven optimal, they are difficult to understand and parameterize. In this\npaper we provide an analysis of regenerating codes for practitioners to grasp\nthe various trade-offs. More specifically we make two contributions: (i) we\nstudy the impact of the parameters by conducting an analysis at the level of\nthe system, rather than at the level of a single device; (ii) we compare the\ncomputational costs of various implementations of codes and highlight the most\nefficient ones. Our goal is to provide system designers with concrete\ninformation to help them choose the best parameters and design for regenerating\ncodes.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 11:17:15 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2012 12:47:06 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2013 14:47:15 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Jiekak", "Steve", ""], ["Kermarrec", "Anne-Marie", ""], ["Scouarnec", "Nicolas Le", ""], ["Straub", "Gilles", ""], ["Van Kempen", "Alexandre", ""]]}, {"id": "1204.5072", "submitter": "Jeffrey Kelling", "authors": "Jeffrey Kelling, G\\'eza \\'Odor, M\\'at\\'e Ferenc Nagy, Henrik Schulz,\n  Karl-Heinz Heinig", "title": "Comparison of Different Parallel Implementations of the 2+1-Dimensional\n  KPZ Model and the 3-Dimensional KMC Model", "comments": "14 pages, 8 figures, to be published in a forthcoming EPJST special\n  issue on \"Computer simulations on GPU\"", "journal-ref": "The European Physical Journal - Special Topics 210, Number 1\n  (2012), 175-187", "doi": "10.1140/epjst/e2012-01645-8", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci cond-mat.stat-mech nlin.CG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that efficient simulations of the Kardar-Parisi-Zhang interface\ngrowth in 2 + 1 dimensions and of the 3-dimensional Kinetic Monte Carlo of\nthermally activated diffusion can be realized both on GPUs and modern CPUs. In\nthis article we present results of different implementations on GPUs using CUDA\nand OpenCL and also on CPUs using OpenCL and MPI. We investigate the runtime\nand scaling behavior on different architectures to find optimal solutions for\nsolving current simulation problems in the field of statistical physics and\nmaterials science.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 14:13:54 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 11:50:58 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2012 14:09:53 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Kelling", "Jeffrey", ""], ["\u00d3dor", "G\u00e9za", ""], ["Nagy", "M\u00e1t\u00e9 Ferenc", ""], ["Schulz", "Henrik", ""], ["Heinig", "Karl-Heinz", ""]]}, {"id": "1204.5402", "submitter": "Marco Aldinucci", "authors": "Marco Aldinucci, Marco Danelutto and Massimo Torquati", "title": "FastFlow tutorial", "comments": "49 pages + cover", "journal-ref": null, "doi": null, "report-no": "TR-12-04", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FastFlow is a structured parallel programming framework targeting shared\nmemory multicores. Its layered design and the optimized implementation of the\ncommunication mechanisms used to implement the FastFlow streaming networks\nprovided to the application programmer as algorithmic skeletons support the\ndevelopment of efficient fine grain parallel applications. FastFlow is\navailable (open source) at SourceForge\n(http://sourceforge.net/projects/mc-fastflow/). This work introduces FastFlow\nprogramming techniques and points out the different ways used to parallelize\nexisting C/C++ code using FastFlow as a software accelerator. In short: this is\na kind of tutorial on FastFlow.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 15:17:53 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Aldinucci", "Marco", ""], ["Danelutto", "Marco", ""], ["Torquati", "Massimo", ""]]}, {"id": "1204.6082", "submitter": "Peter Bailis", "authors": "Peter Bailis, Shivaram Venkataraman, Michael J. Franklin, Joseph M.\n  Hellerstein, Ion Stoica", "title": "Probabilistically Bounded Staleness for Practical Partial Quorums", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  776-787 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data store replication results in a fundamental trade-off between operation\nlatency and data consistency. In this paper, we examine this trade-off in the\ncontext of quorum-replicated data stores. Under partial, or non-strict quorum\nreplication, a data store waits for responses from a subset of replicas before\nanswering a query, without guaranteeing that read and write replica sets\nintersect. As deployed in practice, these configurations provide only basic\neventual consistency guarantees, with no limit to the recency of data returned.\nHowever, anecdotally, partial quorums are often \"good enough\" for practitioners\ngiven their latency benefits. In this work, we explain why partial quorums are\nregularly acceptable in practice, analyzing both the staleness of data they\nreturn and the latency benefits they offer. We introduce Probabilistically\nBounded Staleness (PBS) consistency, which provides expected bounds on\nstaleness with respect to both versions and wall clock time. We derive a\nclosed-form solution for versioned staleness as well as model real-time\nstaleness for representative Dynamo-style systems under internet-scale\nproduction workloads. Using PBS, we measure the latency-consistency trade-off\nfor partial quorum systems. We quantitatively demonstrate how eventually\nconsistent systems frequently return consistent data within tens of\nmilliseconds while offering significant latency benefits.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:26:03 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Bailis", "Peter", ""], ["Venkataraman", "Shivaram", ""], ["Franklin", "Michael J.", ""], ["Hellerstein", "Joseph M.", ""], ["Stoica", "Ion", ""]]}, {"id": "1204.6090", "submitter": "EPTCS", "authors": "Sebastian Bab (TU Berlin), Nadim Sarrouh (TU Berlin)", "title": "Towards a Formal Model of Privacy-Sensitive Dynamic Coalitions", "comments": "In Proceedings FAVO 2011, arXiv:1204.5796", "journal-ref": "EPTCS 83, 2012, pp. 10-21", "doi": "10.4204/EPTCS.83.2", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of dynamic coalitions (also virtual organizations) describes the\ntemporary interconnection of autonomous agents, who share information or\nresources in order to achieve a common goal. Through modern technologies these\ncoalitions may form across company, organization and system borders. Therefor\nquestions of access control and security are of vital significance for the\narchitectures supporting these coalitions.\n  In this paper, we present our first steps to reach a formal framework for\nmodeling and verifying the design of privacy-sensitive dynamic coalition\ninfrastructures and their processes. In order to do so we extend existing\ndynamic coalition modeling approaches with an access-control-concept, which\nmanages access to information through policies. Furthermore we regard the\nprocesses underlying these coalitions and present first works in formalizing\nthese processes. As a result of the present paper we illustrate the usefulness\nof the Abstract State Machine (ASM) method for this task. We demonstrate a\nformal treatment of privacy-sensitive dynamic coalitions by two example ASMs\nwhich model certain access control situations. A logical consideration of these\nASMs can lead to a better understanding and a verification of the ASMs\naccording to the aspired specification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 00:34:18 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Bab", "Sebastian", "", "TU Berlin"], ["Sarrouh", "Nadim", "", "TU Berlin"]]}, {"id": "1204.6170", "submitter": "Wim H. Hesselink", "authors": "Wim H. Hesselink", "title": "A distributed resource allocation algorithm for many processes", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resource allocation is the problem that a process may enter a critical\nsection CS of its code only when its resource requirements are not in conflict\nwith those of other processes in their critical sections. For each execution of\nCS, these requirements are given anew. In the resource requirements, levels can\nbe distinguished, such as e.g. read access or write access. We allow infinitely\nmany processes that communicate by reliable asynchronous messages and have\nfinite memory. A simple starvation-free solution is presented. Processes only\nwait for one another when they have conflicting resource requirements. The\ncorrectness of the solution is argued with invariants and temporal logic. It\nhas been verified with the proof assistant PVS.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 11:14:54 GMT"}, {"version": "v2", "created": "Thu, 31 May 2012 10:49:05 GMT"}], "update_date": "2012-06-01", "authors_parsed": [["Hesselink", "Wim H.", ""]]}, {"id": "1204.6508", "submitter": "Neeraj Sharma", "authors": "Neeraj Sharma, Sandeep Sen", "title": "Efficient cache oblivious algorithms for randomized divide-and-conquer\n  on the multicore model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present randomized algorithms for sorting and convex hull\nthat achieves optimal performance (for speed-up and cache misses) on the\nmulticore model with private cache model. Our algorithms are cache oblivious\nand generalize the randomized divide and conquer strategy given by Reischuk and\nReif and Sen. Although the approach yielded optimal speed-up in the PRAM model,\nwe require additional techniques to optimize cache-misses in an oblivious\nsetting. Under a mild assumption on input and number of processors our\nalgorithm will have optimal time and cache misses with high probability.\nAlthough similar results have been obtained recently for sorting, we feel that\nour approach is simpler and general and we apply it to obtain an optimal\nparallel algorithm for 3D convex hulls with similar bounds. We also present a\nsimple randomized processor allocation technique without the explicit knowledge\nof the number of processors that is likely to find additional applications in\nresource oblivious environments.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 19:31:01 GMT"}, {"version": "v2", "created": "Sun, 27 May 2012 19:41:29 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Sharma", "Neeraj", ""], ["Sen", "Sandeep", ""]]}, {"id": "1204.6628", "submitter": "Federico Calzolari", "authors": "Daniele Licari, Federico Calzolari", "title": "The Anatomy of a Grid portal", "comments": "6 pages", "journal-ref": "Journal of Physics: Conference Series 331 (2011) 072043", "doi": "10.1088/1742-6596/331/7/072043", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new way to deal with Grid portals referring to\nour implementation. L-GRID is a light portal to access the EGEE/EGI Grid\ninfrastructure via Web, allowing users to submit their jobs from a common Web\nbrowser in a few minutes, without any knowledge about the Grid infrastructure.\nIt provides the control over the complete lifecycle of a Grid Job, from its\nsubmission and status monitoring, to the output retrieval. The system,\nimplemented as client-server architecture, is based on the Globus Grid\nmiddleware. The client side application is based on a java applet; the server\nrelies on a Globus User Interface. There is no need of user registration on the\nserver side, and the user needs only his own X.509 personal certificate. The\nsystem is user-friendly, secure (it uses SSL protocol, mechanism for dynamic\ndelegation and identity creation in public key infrastructures), highly\ncustomizable, open source, and easy to install. The X.509 personal certificate\ndoes not get out from the local machine. It allows to reduce the time spent for\nthe job submission, granting at the same time a higher efficiency and a better\nsecurity level in proxy delegation and management.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 13:32:48 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Licari", "Daniele", ""], ["Calzolari", "Federico", ""]]}, {"id": "1204.6629", "submitter": "Federico Calzolari", "authors": "Federico Calzolari, Daniele Licari", "title": "Proxy dynamic delegation in grid gateway", "comments": "6 pages", "journal-ref": "PoS(ISGC 2011 & OGF 31)027", "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays one of the main obstacles the research comes up against is the\ndifficulty in accessing the required computational resources. Grid is able to\noffer the user a wide set of resources, even if they are often too hard to\nexploit for non expert end user. Use simplification has today become a common\npractice in the access and utilization of Cloud, Grid, and data center\nresources. With the launch of L-GRID gateway, we introduced a new way to deal\nwith Grid portals. L-GRID is an extremely light portal developed in order to\naccess the EGI Grid infrastructure via Web, allowing users to submit their jobs\nfrom whatever Web browser in a few minutes, without any knowledge about the\nunderlying Grid infrastructure.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 13:38:03 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Calzolari", "Federico", ""], ["Licari", "Daniele", ""]]}, {"id": "1204.6631", "submitter": "Federico Calzolari", "authors": "Federico Calzolari, Silvia Volpe", "title": "A new job migration algorithm to improve data center efficiency", "comments": "7 pages", "journal-ref": "PoS(ISGC 2011 & OGF 31)008", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The under exploitation of the available resources risks to be one of the main\nproblems for a computing center. The growing demand of computational power\nnecessarily entails more complex approaches in the management of the computing\nresources, with particular attention to the batch queue system scheduler. In a\nheterogeneous batch queue system, available for both serial single core\nprocesses and parallel multi core jobs, it may happen that one or more\ncomputational nodes composing the cluster are not fully occupied, running a\nnumber of jobs lower than their actual capability. A typical case is\nrepresented by more single core jobs running each one over a different multi\ncore server, while more parallel jobs - requiring all the available cores of a\nhost - are queued. A job rearrangement executed at runtime is able to free\nextra resources, in order to host new processes. We present an efficient method\nto improve the computing resources exploitation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 13:44:03 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Calzolari", "Federico", ""], ["Volpe", "Silvia", ""]]}, {"id": "1204.6662", "submitter": "Emna Kallel Laadhar Emna Kallel Laadhar", "authors": "Emna Kallel, Yassine Aoudni, Mouna Baklouti and Mohamed Abid", "title": "Mppsocgen: A framework for automatic generation of mppsoc architecture", "comments": "16 pages; International Journal of Computer Science & Information\n  Technology (IJCSIT) Vol 4, No 2, April 2012", "journal-ref": null, "doi": "10.5121/ijcsit.2012.4201", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Automatic code generation is a standard method in software engineering since\nit improves the code consistency and reduces the overall development time. In\nthis context, this paper presents a design flow for automatic VHDL code\ngeneration of mppSoC (massively parallel processing System-on-Chip)\nconfiguration. Indeed, depending on the application requirements, a framework\nof Netbeans Platform Software Tool named MppSoCGEN was developed in order to\naccelerate the design process of complex mppSoC. Starting from an architecture\nparameters design, VHDL code will be automatically generated using parsing\nmethod. Configuration rules are proposed to have a correct and valid VHDL\nsyntax configuration. Finally, an automatic generation of Processor Elements\nand network topologies models of mppSoC architecture will be done for Stratix\nII device family. Our framework improves its flexibility on Netbeans 5.5\nversion and centrino duo Core 2GHz with 22 Kbytes and 3 seconds average\nruntime. Experimental results for reduction algorithm validate our MppSoCGEN\ndesign flow and demonstrate the efficiency of generated architectures.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 15:04:49 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Kallel", "Emna", ""], ["Aoudni", "Yassine", ""], ["Baklouti", "Mouna", ""], ["Abid", "Mohamed", ""]]}, {"id": "1204.6675", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim", "title": "On the Locality of Some NP-Complete Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the distributed message-passing {LOCAL} model. In this model a\ncommunication network is represented by a graph where vertices host processors,\nand communication is performed over the edges. Computation proceeds in\nsynchronous rounds. The running time of an algorithm is the number of rounds\nfrom the beginning until all vertices terminate. Local computation is free. An\nalgorithm is called {local} if it terminates within a constant number of\nrounds. The question of what problems can be computed locally was raised by\nNaor and Stockmayer \\cite{NS93} in their seminal paper in STOC'93. Since then\nthe quest for problems with local algorithms, and for problems that cannot be\ncomputed locally, has become a central research direction in the field of\ndistributed algorithms \\cite{KMW04,KMW10,LOW08,PR01}.\n  We devise the first local algorithm for an {NP-complete} problem.\nSpecifically, our randomized algorithm computes, with high probability, an\nO(n^{1/2 + epsilon} \\cdot chi)-coloring within O(1) rounds, where epsilon > 0\nis an arbitrarily small constant, and chi is the chromatic number of the input\ngraph. (This problem was shown to be NP-complete in \\cite{Z07}.) On our way to\nthis result we devise a constant-time algorithm for computing (O(1), O(n^{1/2 +\nepsilon}))-network-decompositions. Network-decompositions were introduced by\nAwerbuch et al. \\cite{AGLP89}, and are very useful for solving various\ndistributed problems. The best previously-known algorithm for\nnetwork-decomposition has a polylogarithmic running time (but is applicable for\na wider range of parameters) \\cite{LS93}. We also devise a Delta^{1 +\nepsilon}-coloring algorithm for graphs with sufficiently large maximum degree\nDelta that runs within O(1) rounds. It improves the best previously-known\nresult for this family of graphs, which is O(\\log-star n) \\cite{SW10}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 15:59:47 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Barenboim", "Leonid", ""]]}, {"id": "1204.6691", "submitter": "Drazen Lucanin MSc", "authors": "Drazen Lucanin, Michael Maurer, Toni Mastelic, Ivona Brandic", "title": "Energy Efficient Service Delivery in Clouds in Compliance with the Kyoto\n  Protocol", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-33645-4_9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is revolutionizing the ICT landscape by providing scalable\nand efficient computing resources on demand. The ICT industry - especially data\ncenters, are responsible for considerable amounts of CO2 emissions and will\nvery soon be faced with legislative restrictions, such as the Kyoto protocol,\ndefining caps at different organizational levels (country, industry branch\netc.) A lot has been done around energy efficient data centers, yet there is\nvery little work done in defining flexible models considering CO2. In this\npaper we present a first attempt of modeling data centers in compliance with\nthe Kyoto protocol. We discuss a novel approach for trading credits for\nemission reductions across data centers to comply with their constraints. CO2\ncaps can be integrated with Service Level Agreements and juxtaposed to other\ncomputing commodities (e.g. computational power, storage), setting a foundation\nfor implementing next-generation schedulers and pricing models that support\nKyoto-compliant CO2 trading schemes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 16:28:19 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Lucanin", "Drazen", ""], ["Maurer", "Michael", ""], ["Mastelic", "Toni", ""], ["Brandic", "Ivona", ""]]}]