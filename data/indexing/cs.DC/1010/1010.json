[{"id": "1010.0371", "submitter": "Anolan Milanes", "authors": "Anolan Milan\\'es, Noemi Rodriguez, Roberto Ierusalimschy", "title": "Reflection-based language support for the heterogeneous capture and\n  restoration of running computations", "comments": "26 pages, 3 figures. Submitted to Computer Languages, Systems &\n  Structures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is devoted to the study of the problem of user-level capture and\nrestoration of running computations in heterogeneous environments. Support for\nthose operations has traditionally been offered through ready-made solutions\nfor specific applications, which are difficult to tailor or adapt to different\nneeds. We believe that a more promising approach would be to build specific\nsolutions as needed, over a more general framework for capture and restoration.\nIn this work, in order to explore the basic mechanisms a language should\nprovide to support the implementation of different policies, we extend the Lua\nprogramming language with an API that allows the programmer to reify the\ninternal structures of execution into fine-grained language values.\n", "versions": [{"version": "v1", "created": "Sun, 3 Oct 2010 00:06:41 GMT"}, {"version": "v2", "created": "Tue, 5 Oct 2010 17:44:37 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Milan\u00e9s", "Anolan", ""], ["Rodriguez", "Noemi", ""], ["Ierusalimschy", "Roberto", ""]]}, {"id": "1010.0476", "submitter": "Dimitris S. Papailiopoulos", "authors": "Dimitris S. Papailiopoulos and Alexandros G. Dimakis", "title": "Interference Alignment as a Rank Constrained Rank Minimization", "comments": "27 pages, single column, 7 figures, TSP submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the maximization of the sum degrees-of-freedom for the static\nflat-fading multiple-input multiple-output (MIMO) interference channel is\nequivalent to a rank constrained rank minimization problem (RCRM), when the\nsignal spaces span all available dimensions. The rank minimization corresponds\nto maximizing interference alignment (IA) so that interference spans the lowest\ndimensional subspace possible. The rank constraints account for the useful\nsignal spaces spanning all available spatial dimensions. That way, we\nreformulate all IA requirements to requirements involving ranks. Then, we\npresent a convex relaxation of the RCRM problem inspired by recent results in\ncompressed sensing and low-rank matrix completion theory that rely on\napproximating rank with the nuclear norm. We show that the convex envelope of\nthe sum of ranks of the interference matrices is the normalized sum of their\ncorresponding nuclear norms and introduce tractable constraints that are\nasymptotically equivalent to the rank constraints for the initial problem. We\nalso show that our heuristic relaxation can be tuned for the multi-cell\ninterference channel. Furthermore, we experimentally show that in many cases\nthe proposed algorithm attains perfect interference alignment and in some cases\noutperforms previous approaches for finding precoding and zero-forcing matrices\nfor interference alignment.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 05:52:34 GMT"}, {"version": "v2", "created": "Tue, 7 Dec 2010 17:37:22 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2011 06:55:33 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Papailiopoulos", "Dimitris S.", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1010.0485", "submitter": "Dimitris Papailiopoulos", "authors": "Dimitris S. Papailiopoulos and Alexandros G. Dimakis", "title": "Distributed Storage Codes Meet Multiple-Access Wiretap Channels", "comments": "10 pages, allerton 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider {\\it i)} the overhead minimization of maximum-distance separable\n(MDS) storage codes for the repair of a single failed node and {\\it ii)} the\ntotal secure degrees-of-freedom (S-DoF) maximization in a multiple-access\ncompound wiretap channel. We show that the two problems are connected.\nSpecifically, the overhead minimization for a single node failure of an {\\it\noptimal} MDS code, i.e. one that can achieve the information theoretic overhead\nminimum, is equivalent to maximizing the S-DoF in a multiple-access compound\nwiretap channel. Additionally, we show that maximizing the S-DoF in a\nmultiple-access compound wiretap channel is equivalent to minimizing the\noverhead of an MDS code for the repair of a departed node. An optimal MDS code\nmaps to a full S-DoF channel and a full S-DoF channel maps to an MDS code with\nminimum repair overhead for one failed node. We also state a general framework\nfor code-to-channel and channel-to-code mappings and performance bounds between\nthe two settings. The underlying theme for all connections presented is\ninterference alignment (IA). The connections between the two problems become\napparent when we restate IA as an optimization problem. Specifically, we\nformulate the overhead minimization and the S-DoF maximization as rank\nconstrained, sum-rank and max-rank minimization problems respectively. The\nderived connections allow us to map repair strategies of recently discovered\nrepair codes to beamforming matrices and characterize the maximum S-DoF for the\nsingle antenna multiple-access compound wiretap channel.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 07:15:48 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Papailiopoulos", "Dimitris S.", ""], ["Dimakis", "Alexandros G.", ""]]}, {"id": "1010.0558", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler", "title": "Analyzing Network Coding Gossip Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new technique to analyze the stopping time of gossip protocols that\nare based on random linear network coding (RLNC). Our analysis drastically\nsimplifies, extends and strengthens previous results. We analyze RLNC gossip in\na general framework for network and communication models that encompasses and\nunifies the models used previously in this context. We show, in most settings\nfor the first time, that it converges with high probability in the\ninformation-theoretically optimal time. Most stopping times are of the form O(k\n+ T) where k is the number of messages to be distributed and T is the time it\ntakes to disseminate one message. This means RLNC gossip achieves \"perfect\npipelining\". Our analysis directly extends to highly dynamic networks in which\nthe topology can change completely at any time. This remains true even if the\nnetwork dynamics are controlled by a fully adaptive adversary that knows the\ncomplete network state. Virtually nothing besides simple O(kT) sequential\nflooding protocols was previously known for such a setting. While RLNC gossip\nworks in this wide variety of networks its analysis remains the same and\nextremely simple. This contrasts with more complex proofs that were put forward\nto give less strong results for various special cases.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 12:21:21 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Haeupler", "Bernhard", ""]]}, {"id": "1010.0562", "submitter": "Somayeh Abdi", "authors": "Somayeh Abdi, Hossein Pedram, Somayeh Mohamadi", "title": "The Impact of Data Replicatino on Job Scheduling Performance in\n  Hierarchical data Grid", "comments": "11 pages, 7 figures", "journal-ref": "International journal on applications of graph theory in wireless\n  ad hoc networks and sensor networks (GRAPH-HOC) Vol.2, No.3, September 2010", "doi": "10.5121/jgraphoc.2010.2302", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data-intensive applications data transfer is a primary cause of job\nexecution delay. Data access time depends on bandwidth. The major bottleneck to\nsupporting fast data access in Grids is the high latencies of Wide Area\nNetworks and Internet. Effective scheduling can reduce the amount of data\ntransferred across the internet by dispatching a job to where the needed data\nare present. Another solution is to use a data replication mechanism. Objective\nof dynamic replica strategies is reducing file access time which leads to\nreducing job runtime. In this paper we develop a job scheduling policy and a\ndynamic data replication strategy, called HRS (Hierarchical Replication\nStrategy), to improve the data access efficiencies. We study our approach and\nevaluate it through simulation. The results show that our algorithm has\nimproved 12% over the current strategies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 12:25:04 GMT"}], "update_date": "2010-10-05", "authors_parsed": [["Abdi", "Somayeh", ""], ["Pedram", "Hossein", ""], ["Mohamadi", "Somayeh", ""]]}, {"id": "1010.0958", "submitter": "Partha Sarathi Mandal", "authors": "Punit Sharma and Partha Sarathi Mandal", "title": "Reconstruction of Aggregation Tree in spite of Faulty Nodes in Wireless\n  Sensor Networks", "comments": "this is a 5 page paper. this paper has been submitted to WCSN 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in wireless sensor networks (WSNs) have led to many new\npromissing applications. However data communication between nodes consumes a\nlarge portion of the total energy of WSNs. Consequently efficient data\naggregation technique can help greatly to reduce power consumption. Data\naggregation has emerged as a basic approach in WSNs in order to reduce the\nnumber of transmissions of sensor nodes over {\\it aggregation tree} and hence\nminimizing the overall power consumption in the network. If a sensor node fails\nduring data aggregation then the aggregation tree is disconnected. Hence the\nWSNs rely on in-network aggregation for efficiency but a single faulty node can\nseverely influence the outcome by contributing an arbitrary partial aggregate\nvalue.\n  In this paper we have presented a distributed algorithm that reconstruct the\naggregation tree from the initial aggregation tree excluding the faulty sensor\nnode. This is a synchronous model that is completed in several rounds. Our\nproposed scheme can handle multiple number of faulty nodes as well.\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 17:54:58 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Sharma", "Punit", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1010.1015", "submitter": "Keith Wiley", "authors": "Keith Wiley, Andrew Connolly, Jeff Gardner, Simon Krughof, Magdalena\n  Balazinska, Bill Howe, YongChul Kwon and YingYi Bu", "title": "Astronomy in the Cloud: Using MapReduce for Image Coaddition", "comments": "31 pages, 11 figures, 2 tables", "journal-ref": null, "doi": "10.1086/658877", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the coming decade, astronomical surveys of the sky will generate tens of\nterabytes of images and detect hundreds of millions of sources every night. The\nstudy of these sources will involve computation challenges such as anomaly\ndetection and classification, and moving object tracking. Since such studies\nbenefit from the highest quality data, methods such as image coaddition\n(stacking) will be a critical preprocessing step prior to scientific\ninvestigation. With a requirement that these images be analyzed on a nightly\nbasis to identify moving sources or transient objects, these data streams\npresent many computational challenges. Given the quantity of data involved, the\ncomputational load of these problems can only be addressed by distributing the\nworkload over a large number of nodes. However, the high data throughput\ndemanded by these applications may present scalability challenges for certain\nstorage architectures. One scalable data-processing method that has emerged in\nrecent years is MapReduce, and in this paper we focus on its popular\nopen-source implementation called Hadoop. In the Hadoop framework, the data is\npartitioned among storage attached directly to worker nodes, and the processing\nworkload is scheduled in parallel on the nodes that contain the required input\ndata. A further motivation for using Hadoop is that it allows us to exploit\ncloud computing resources, e.g., Amazon's EC2. We report on our experience\nimplementing a scalable image-processing pipeline for the SDSS imaging database\nusing Hadoop. This multi-terabyte imaging dataset provides a good testbed for\nalgorithm development since its scope and structure approximate future surveys.\nFirst, we describe MapReduce and how we adapted image coaddition to the\nMapReduce framework. Then we describe a number of optimizations to our basic\napproach and report experimental results comparing their performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 20:35:53 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Wiley", "Keith", ""], ["Connolly", "Andrew", ""], ["Gardner", "Jeff", ""], ["Krughof", "Simon", ""], ["Balazinska", "Magdalena", ""], ["Howe", "Bill", ""], ["Kwon", "YongChul", ""], ["Bu", "YingYi", ""]]}, {"id": "1010.1112", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim, Shlomi Dolev, and Rafail Ostrovsky", "title": "Deterministic and Energy-Optimal Wireless Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clock synchronization in a wireless setting where\nprocessors must power-down their radios in order to save energy. Energy\nefficiency is a central goal in wireless networks, especially if energy\nresources are severely limited. In the current setting, the problem is to\nsynchronize clocks of $m$ processors that wake up in arbitrary time points,\nsuch that the maximum difference between wake up times is bounded by a positive\ninteger $n$, where time intervals are appropriately discretized. Currently, the\nbest-known results for synchronization for single-hop networks of $m$\nprocessors is a randomized algorithm due to \\cite{BKO09} of O(\\sqrt {n /m}\n\\cdot poly-log(n)) awake times per processor and a lower bound of\nOmega(\\sqrt{n/m}) of the number of awake times needed per processor\n\\cite{BKO09}. The main open question left in their work is to close the\npoly-log gap between the upper and the lower bound and to de-randomize their\nprobabilistic construction and eliminate error probability. This is exactly\nwhat we do in this paper.\n  That is, we show a {deterministic} algorithm with radio use of Theta(\\sqrt {n\n/m}) that never fails. We stress that our upper bound exactly matches the lower\nbound proven in \\cite{BKO09}, up to a small multiplicative constant. Therefore,\nour algorithm is {optimal} in terms of energy efficiency and completely\nresolves a long sequence of works in this area. In order to achieve these\nresults we devise a novel {adaptive} technique that determines the times when\ndevices power their radios on and off. In addition, we prove several lower\nbounds on the energy efficiency of algorithms for {multi-hop networks}.\nSpecifically, we show that any algorithm for multi-hop networks must have radio\nuse of Omega(\\sqrt n) per processor.\n", "versions": [{"version": "v1", "created": "Wed, 6 Oct 2010 10:18:02 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Barenboim", "Leonid", ""], ["Dolev", "Shlomi", ""], ["Ostrovsky", "Rafail", ""]]}, {"id": "1010.1260", "submitter": "Laura Grigori", "authors": "Ioan O. Hupca, Joel Falcou, Laura Grigori, and Radek Stompor", "title": "Spherical harmonic transform with GPUs", "comments": null, "journal-ref": "Proceedings of Euro-Par 2011, Lecture Notes in Computer Science,\n  2012, Vol. 7155/2012, p. 355", "doi": "10.1007/978-3-642-29737-3_40", "report-no": "INRIA technical report 7409", "categories": "cs.DC astro-ph.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an algorithm for computing an inverse spherical harmonic\ntransform suitable for graphic processing units (GPU). We use CUDA and base our\nimplementation on a Fortran90 routine included in a publicly available parallel\npackage, S2HAT. We focus our attention on the two major sequential steps\ninvolved in the transforms computation, retaining the efficient parallel\nframework of the original code. We detail optimization techniques used to\nenhance the performance of the CUDA-based code and contrast them with those\nimplemented in the Fortran90 version. We also present performance comparisons\nof a single CPU plus GPU unit with the S2HAT code running on either a single or\n4 processors. In particular we find that use of the latest generation of GPUs,\nsuch as NVIDIA GF100 (Fermi), can accelerate the spherical harmonic transforms\nby as much as 18 times with respect to S2HAT executed on one core, and by as\nmuch as 5.5 with respect to S2HAT on 4 cores, with the overall performance\nbeing limited by the Fast Fourier transforms. The work presented here has been\nperformed in the context of the Cosmic Microwave Background simulations and\nanalysis. However, we expect that the developed software will be of more\ngeneral interest and applicability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Oct 2010 20:06:54 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Hupca", "Ioan O.", ""], ["Falcou", "Joel", ""], ["Grigori", "Laura", ""], ["Stompor", "Radek", ""]]}, {"id": "1010.1595", "submitter": "Christian P. Robert", "authors": "Pierre Jacob (Universite Paris-Dauphine and CREST, France), Christian\n  P. Robert (Universite Paris-Dauphine, IuF, and CREST, France), Murray H.\n  Smith (NIWA, Wellington, New Zealand)", "title": "Using parallel computation to improve Independent Metropolis--Hastings\n  based estimation", "comments": "19 pages, 8 figures, to appear in Journal of Computational and\n  Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the implications of the fact that parallel\nraw-power can be exploited by a generic Metropolis--Hastings algorithm if the\nproposed values are independent. In particular, we present improvements to the\nindependent Metropolis--Hastings algorithm that significantly decrease the\nvariance of any estimator derived from the MCMC output, for a null computing\ncost since those improvements are based on a fixed number of target density\nevaluations. Furthermore, the techniques developed in this paper do not\njeopardize the Markovian convergence properties of the algorithm, since they\nare based on the Rao--Blackwell principles of Gelfand and Smith (1990), already\nexploited in Casella and Robert (1996), Atchade and Perron (2005) and Douc and\nRobert (2010). We illustrate those improvements both on a toy normal example\nand on a classical probit regression model, but stress the fact that they are\napplicable in any case where the independent Metropolis-Hastings is applicable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 05:43:27 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 11:37:42 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2011 10:01:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jacob", "Pierre", "", "Universite Paris-Dauphine and CREST, France"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine, IuF, and CREST, France"], ["Smith", "Murray H.", "", "NIWA, Wellington, New Zealand"]]}, {"id": "1010.1812", "submitter": "Muhammad Rahman M.Sc", "authors": "Muhammad Mahbubur Rahman, Afroza Nahar", "title": "Modified Bully Algorithm using Election Commission", "comments": "8 pages,6 figures", "journal-ref": "MASAUM Journal of Computing(MJC),Vol.1 No.3,pp.439-446,October\n  2009, ISSN 2076-0833", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electing leader is a vital issue not only in distributed computing but also\nin communication network [1, 2, 3, 4, 5], centralized mutual exclusion\nalgorithm [6, 7], centralized control IPC, etc. A leader is required to make\nsynchronization between different processes. And different election algorithms\nare used to elect a coordinator among the available processes in the system\nsuch a way that there will be only one coordinator at any time. Bully election\nalgorithm is one of the classical and well-known approaches in coordinator\nelection process. This paper will present a modified version of bully election\nalgorithm using a new concept called election commission. This approach will\nnot only reduce redundant elections but also minimize total number of elections\nand hence it will minimize message passing, network traffic, and complexity of\nthe existing system.\n", "versions": [{"version": "v1", "created": "Sat, 9 Oct 2010 06:00:33 GMT"}], "update_date": "2010-10-13", "authors_parsed": [["Rahman", "Muhammad Mahbubur", ""], ["Nahar", "Afroza", ""]]}, {"id": "1010.2000", "submitter": "Julien Langou", "authors": "Henricus Bouwmeester and Julien Langou", "title": "A Critical Path Approach to Analyzing Parallelism of Algorithmic\n  Variants. Application to Cholesky Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms come with multiple variants which are obtained by changing the\nmathematical approach from which the algorithm is derived. These variants offer\na wide spectrum of performance when implemented on a multicore platform and we\nseek to understand these differences in performances from a theoretical point\nof view. To that aim, we derive and present the critical path lengths of each\nalgorithmic variant for our application problem which enables us to determine a\nlower bound on the time to solution. This metric provides an intuitive grasp of\nthe performance of a variant and we present numerical experiments to validate\nthe tightness of our lower bounds on practical applications. Our case study is\nthe Cholesky inversion and its use in computing the inverse of a symmetric\npositive definite matrix.\n", "versions": [{"version": "v1", "created": "Mon, 11 Oct 2010 05:34:40 GMT"}], "update_date": "2010-10-12", "authors_parsed": [["Bouwmeester", "Henricus", ""], ["Langou", "Julien", ""]]}, {"id": "1010.2438", "submitter": "Marco Aldinucci", "authors": "Marco Aldinucci, Mario Coppo, Ferruccio Damiani, Maurizio Drocco,\n  Massimo Torquati, Angelo Troina", "title": "On Designing Multicore-aware Simulators for Biological Systems", "comments": "19 pages + cover page", "journal-ref": "Proc. of the 19th Euromicro Intl. Conf. on Parallel, Distributed\n  and Network-Based Computing (PDP), Ayia Napa, Cyprus, Feb. 2011. IEEE", "doi": "10.1109/PDP.2011.81", "report-no": "129/2010", "categories": "cs.DC cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic simulation of biological systems is an increasingly popular\ntechnique in bioinformatics. It often is an enlightening technique, which may\nhowever result in being computational expensive. We discuss the main\nopportunities to speed it up on multi-core platforms, which pose new challenges\nfor parallelisation techniques. These opportunities are developed in two\ngeneral families of solutions involving both the single simulation and a bulk\nof independent simulations (either replicas of derived from parameter sweep).\nProposed solutions are tested on the parallelisation of the CWC simulator\n(Calculus of Wrapped Compartments) that is carried out according to proposed\nsolutions by way of the FastFlow programming framework making possible fast\ndevelopment and efficient execution on multi-cores.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 16:46:51 GMT"}, {"version": "v2", "created": "Wed, 13 Oct 2010 12:24:03 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Aldinucci", "Marco", ""], ["Coppo", "Mario", ""], ["Damiani", "Ferruccio", ""], ["Drocco", "Maurizio", ""], ["Torquati", "Massimo", ""], ["Troina", "Angelo", ""]]}, {"id": "1010.2454", "submitter": "Leonid Barenboim", "authors": "Leonid Barenboim and Michael Elkin", "title": "Distributed Deterministic Edge Coloring using Bounded Neighborhood\n  Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the {edge-coloring} problem in the message-passing model of\ndistributed computing. This is one of the most fundamental and well-studied\nproblems in this area. Currently, the best-known deterministic algorithms for\n(2Delta -1)-edge-coloring requires O(Delta) + log-star n time \\cite{PR01},\nwhere Delta is the maximum degree of the input graph. Also, recent results of\n\\cite{BE10} for vertex-coloring imply that one can get an\nO(Delta)-edge-coloring in O(Delta^{epsilon} \\cdot \\log n) time, and an\nO(Delta^{1 + epsilon})-edge-coloring in O(log Delta log n) time, for an\narbitrarily small constant epsilon > 0.\n  In this paper we devise a drastically faster deterministic edge-coloring\nalgorithm. Specifically, our algorithm computes an O(Delta)-edge-coloring in\nO(Delta^{epsilon}) + log-star n time, and an O(Delta^{1 +\nepsilon})-edge-coloring in O(log Delta) + log-star n time. This result improves\nthe previous state-of-the-art {exponentially} in a wide range of Delta,\nspecifically, for 2^{Omega(\\log-star n)} \\leq Delta \\leq polylog(n). In\naddition, for small values of Delta our deterministic algorithm outperforms all\nthe existing {randomized} algorithms for this problem.\n  On our way to these results we study the {vertex-coloring} problem on the\nfamily of graphs with bounded {neighborhood independence}. This is a large\nfamily, which strictly includes line graphs of r-hypergraphs for any r = O(1),\nand graphs of bounded growth. We devise a very fast deterministic algorithm for\nvertex-coloring graphs with bounded neighborhood independence. This algorithm\ndirectly gives rise to our edge-coloring algorithms, which apply to {general}\ngraphs.\n  Our main technical contribution is a subroutine that computes an\nO(Delta/p)-defective p-vertex coloring of graphs with bounded neighborhood\nindependence in O(p^2) + \\log-star n time, for a parameter p, 1 \\leq p \\leq\nDelta.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 17:51:01 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Barenboim", "Leonid", ""], ["Elkin", "Michael", ""]]}, {"id": "1010.2466", "submitter": "Ruo-Wei Hung", "authors": "Ruo-Wei Hung", "title": "Constructing Two Edge-Disjoint Hamiltonian Cycles in Locally Twisted\n  Cubes", "comments": "7 pages, 4 figures", "journal-ref": "Theoretical Computer Science 412/35 (2011) 4747-4753", "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $n$-dimensional hypercube network $Q_n$ is one of the most popular\ninterconnection networks since it has simple structure and is easy to\nimplement. The $n$-dimensional locally twisted cube, denoted by $LTQ_n$, an\nimportant variation of the hypercube, has the same number of nodes and the same\nnumber of connections per node as $Q_n$. One advantage of $LTQ_n$ is that the\ndiameter is only about half of the diameter of $Q_n$. Recently, some\ninteresting properties of $LTQ_n$ were investigated. In this paper, we\nconstruct two edge-disjoint Hamiltonian cycles in the locally twisted cube\n$LTQ_n$, for any integer $n\\geqslant 4$. The presence of two edge-disjoint\nHamiltonian cycles provides an advantage when implementing algorithms that\nrequire a ring structure by allowing message traffic to be spread evenly across\nthe locally twisted cube.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 18:39:31 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Hung", "Ruo-Wei", ""]]}, {"id": "1010.2824", "submitter": "EPTCS", "authors": "Rab\\'ea Ameur-Boulifa, Ludovic Henrio, and Eric Madelaine", "title": "Behavioural Models for Group Communications", "comments": "In Proceedings WCSI 2010, arXiv:1010.2337", "journal-ref": "EPTCS 37, 2010, pp. 42-56", "doi": "10.4204/EPTCS.37.4", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group communication is becoming a more and more popular infrastructure for\nefficient distributed applications. It consists in representing locally a group\nof remote objects as a single object accessed in a single step; communications\nare then broadcasted to all members. This paper provides models for automatic\nverification of group-based applications, typically for detecting deadlocks or\nchecking message ordering. We show how to encode group communication, together\nwith different forms of synchronisation for group results. The proposed models\nare parametric such that, for example, different group sizes or group members\ncould be experimented with the minimum modification of the original model.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 05:16:17 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Ameur-Boulifa", "Rab\u00e9a", ""], ["Henrio", "Ludovic", ""], ["Madelaine", "Eric", ""]]}, {"id": "1010.2828", "submitter": "EPTCS", "authors": "Abdul Malik Khan (Institut T\\'el\\'ecom, T\\'el\\'ecom Sudparis, CNRS UMR\n  SAMOVAR, Evry, France), Sophie Chabridon (Institut T\\'el\\'ecom, T\\'el\\'ecom\n  Sudparis, CNRS UMR SAMOVAR, Evry, France), Antoine Beugnard (Institut\n  T\\'el\\'ecom, T\\'el\\'ecom Bretagne, Brest, France)", "title": "A Reusable Component for Communication and Data Synchronization in\n  Mobile Distributed Interactive Applications", "comments": "In Proceedings WCSI 2010, arXiv:1010.2337", "journal-ref": "EPTCS 37, 2010, pp. 86-100", "doi": "10.4204/EPTCS.37.7", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Distributed Interactive Applications (DIA) such as multiplayer games,\nwhere many participants are involved in a same game session and communicate\nthrough a network, they may have an inconsistent view of the virtual world\nbecause of the communication delays across the network. This issue becomes even\nmore challenging when communicating through a cellular network while executing\nthe DIA client on a mobile terminal. Consistency maintenance algorithms may be\nused to obtain a uniform view of the virtual world. These algorithms are very\ncomplex and hard to program and therefore, the implementation and the future\nevolution of the application logic code become difficult. To solve this\nproblem, we propose an approach where the consistency concerns are handled\nseparately by a distributed component called a Synchronization Medium, which is\nresponsible for the communication management as well as the consistency\nmaintenance. We present the detailed architecture of the Synchronization Medium\nand the generic interfaces it offers to DIAs. We evaluate our approach both\nqualitatively and quantitatively. We first demonstrate that the Synchronization\nMedium is a reusable component through the development of two game\napplications, a car racing game and a space war game. A performance evaluation\nthen shows that the overhead introduced by the Synchronization Medium remains\nacceptable.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 05:16:43 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Khan", "Abdul Malik", "", "Institut T\u00e9l\u00e9com, T\u00e9l\u00e9com Sudparis, CNRS UMR\n  SAMOVAR, Evry, France"], ["Chabridon", "Sophie", "", "Institut T\u00e9l\u00e9com, T\u00e9l\u00e9com\n  Sudparis, CNRS UMR SAMOVAR, Evry, France"], ["Beugnard", "Antoine", "", "Institut\n  T\u00e9l\u00e9com, T\u00e9l\u00e9com Bretagne, Brest, France"]]}, {"id": "1010.2881", "submitter": "Rajkumar Buyya", "authors": "Linlin Wu and Rajkumar Buyya", "title": "Service Level Agreement (SLA) in Utility Computing Systems", "comments": "27 pages, 4 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report CLOUDS-TR-2010-5, Cloud Computing and Distributed\n  Systems Laboratory, The University of Melbourne, Australia, September 3, 2010", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, extensive research has been conducted in the area of Service\nLevel Agreement (SLA) for utility computing systems. An SLA is a formal\ncontract used to guarantee that consumers' service quality expectation can be\nachieved. In utility computing systems, the level of customer satisfaction is\ncrucial, making SLAs significantly important in these environments. Fundamental\nissue is the management of SLAs, including SLA autonomy management or trade off\namong multiple Quality of Service (QoS) parameters. Many SLA languages and\nframeworks have been developed as solutions; however, there is no overall\nclassification for these extensive works. Therefore, the aim of this chapter is\nto present a comprehensive survey of how SLAs are created, managed and used in\nutility computing environment. We discuss existing use cases from Grid and\nCloud computing systems to identify the level of SLA realization in\nstate-of-art systems and emerging challenges for future research.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 11:33:28 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Wu", "Linlin", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1010.3053", "submitter": "Lars Kolb", "authors": "Lars Kolb, Andreas Thor, Erhard Rahm", "title": "Parallel Sorted Neighborhood Blocking with MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud infrastructures enable the efficient parallel execution of\ndata-intensive tasks such as entity resolution on large datasets. We\ninvestigate challenges and possible solutions of using the MapReduce\nprogramming model for parallel entity resolution. In particular, we propose and\nevaluate two MapReduce-based implementations for Sorted Neighborhood blocking\nthat either use multiple MapReduce jobs or apply a tailored data replication.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 00:28:44 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Kolb", "Lars", ""], ["Thor", "Andreas", ""], ["Rahm", "Erhard", ""]]}, {"id": "1010.3233", "submitter": "Joshua White", "authors": "Joshua White, Adam Pilbeam", "title": "A Survey of Virtualization Technologies With Performance Testing", "comments": "6 Pages, 2 Tables, 4 Plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization has rapidly become a go-to technology for increasing\nefficiency in the data center. With virtualization technologies providing\ntremendous flexibility, even disparate architectures may be deployed on a\nsingle machine without interference. Awareness of limitations and requirements\nof physical hosts to be used for virtualization is important. This paper\nreviews the present virtualization methods, virtual computing software, and\nprovides a brief analysis of the performance issues inherent to each. In the\nend we present testing results of KVM-QEMU on two current Multi-Core CPU\nArchitectures and System Configurations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 17:51:50 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["White", "Joshua", ""], ["Pilbeam", "Adam", ""]]}, {"id": "1010.3427", "submitter": "Magnus M. Halldorsson", "authors": "Magnus M. Halldorsson", "title": "Wireless Scheduling with Power Control", "comments": "Revised full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the scheduling of arbitrary wireless links in the physical model\nof interference to minimize the time for satisfying all requests. We study here\nthe combined problem of scheduling and power control, where we seek both an\nassignment of power settings and a partition of the links so that each set\nsatisfies the signal-to-interference-plus-noise (SINR) constraints.\n  We give an algorithm that attains an approximation ratio of $O(\\log n \\cdot\n\\log\\log \\Delta)$, where $n$ is the number of links and $\\Delta$ is the ratio\nbetween the longest and the shortest link length. Under the natural assumption\nthat lengths are represented in binary, this gives the first approximation\nratio that is polylogarithmic in the size of the input. The algorithm has the\ndesirable property of using an oblivious power assignment, where the power\nassigned to a sender depends only on the length of the link. We give evidence\nthat this dependence on $\\Delta$ is unavoidable, showing that any\nreasonably-behaving oblivious power assignment results in a $\\Omega(\\log\\log\n\\Delta)$-approximation.\n  These results hold also for the (weighted) capacity problem of finding a\nmaximum (weighted) subset of links that can be scheduled in a single time slot.\nIn addition, we obtain improved approximation for a bidirectional variant of\nthe scheduling problem, give partial answers to questions about the utility of\ngraphs for modeling physical interference, and generalize the setting from the\nstandard 2-dimensional Euclidean plane to doubling metrics. Finally, we explore\nthe utility of graph models in capturing wireless interference.\n", "versions": [{"version": "v1", "created": "Sun, 17 Oct 2010 16:34:10 GMT"}, {"version": "v2", "created": "Sat, 16 Apr 2011 12:07:22 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Halldorsson", "Magnus M.", ""]]}, {"id": "1010.3619", "submitter": "Souvik Ghosh", "authors": "Souvik Ghosh and Soumyadip Ghosh", "title": "A strong law for the rate of growth of long latency periods in cloud\n  computing service", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-computing shares a common pool of resources across customers at a scale\nthat is orders of magnitude larger than traditional multi-user systems.\nConstituent physical compute servers are allocated multiple \"virtual machines\"\n(VM) to serve simultaneously. Each VM user should ideally be unaffected by\nothers' demand. Naturally, this environment produces new challenges for the\nservice providers in meeting customer expectations while extracting an\nefficient utilization from server resources. We study a new cloud service\nmetric that measures prolonged latency or delay suffered by customers. We model\nthe workload process of a cloud server and analyze the process as the customer\npopulation grows. The capacity required to ensure that average workload does\nnot exceed a threshold over long segments is characterized. This can be used by\ncloud operators to provide service guarantees on avoiding long durations of\nlatency. As part of the analysis, we provide a uniform large-deviation\nprinciple for collections of random variables that is of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 15 Oct 2010 05:06:28 GMT"}], "update_date": "2010-10-19", "authors_parsed": [["Ghosh", "Souvik", ""], ["Ghosh", "Soumyadip", ""]]}, {"id": "1010.3816", "submitter": "Gaurav Khanna", "authors": "Niket K. Choudhary, Rakesh Ginjupalli, Sandeep Navada and Gaurav\n  Khanna", "title": "An Exploration of OpenCL for a Numerical Relativity Application", "comments": "6 pages, 1 table; Accepted for publication in Parallel and\n  Distributed Computing and Systems (PDCS 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "gr-qc cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there is considerable interest in making use of many-core processor\narchitectures, such as Nvidia and AMD graphics processing units (GPUs) for\nscientific computing. In this work we explore the use of the Open Computing\nLanguage (OpenCL) for a typical Numerical Relativity application: a time-domain\nTeukolsky equation solver (a linear, hyperbolic, partial differential equation\nsolver using finite-differencing). OpenCL is the only vendor-agnostic and\nmulti-platform parallel computing framework that has been adopted by all major\nprocessor vendors. Therefore, it allows us to write portable source-code and\nrun it on a wide variety of compute hardware and perform meaningful\ncomparisons. The outcome of our experimentation suggests that it is relatively\nstraightforward to obtain order-of-magnitude gains in overall application\nperformance by making use of many-core GPUs over multi-core CPUs and this fact\nis largely independent of the specific hardware architecture and vendor. We\nalso observe that a single high-end GPU can match the performance of a\nsmall-sized, message-passing based CPU cluster.\n", "versions": [{"version": "v1", "created": "Tue, 19 Oct 2010 07:05:20 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2011 19:15:52 GMT"}], "update_date": "2011-10-04", "authors_parsed": [["Choudhary", "Niket K.", ""], ["Ginjupalli", "Rakesh", ""], ["Navada", "Sandeep", ""], ["Khanna", "Gaurav", ""]]}, {"id": "1010.4018", "submitter": "Chadi Kari", "authors": "Chadi Kari", "title": "A Paradigm for Channel Assignment and Data Migration in Distributed\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we consider the problems of channel assignment in\nwireless networks and data migration in heterogeneous storage systems. We show\nthat a soft edge coloring approach to both problems gives rigorous\napproximation guarantees. In the channel assignment problem arising in wireless\nnetworks a pair of edges incident to a vertex are said to be conflicting if the\nchannels assigned to them are the same. Our goal is to assign channels (color\nedges) so that the number of conflicts is minimized. The problem is NP-hard by\na reduction from Edge coloring and we present two combinatorial algorithms for\nthis case. The first algorithm is based on a distributed greedy method and\ngives a solution with at most $2(1-\\frac{1}{k})|E|$ more conflicts than the\noptimal solution.The approximation ratio if the second algorithm is $1 +\n\\frac{|V|}{|E|}$, which gives a ($1 + o(1)$)-factor for dense graphs and is the\nbest possible unless P = NP. We also consider the data migration problem in\nheterogeneous storage systems. In such systems, data layouts may need to be\nreconfigured over time for load balancing or in the event of system\nfailure/upgrades. It is critical to migrate data to their target locations as\nquickly as possible to obtain the best performance of the system. Most of the\nprevious results on data migration assume that each storage node can perform\nonly one data transfer at a time. However, storage devices tend to have\nheterogeneous capabilities as devices may be added over time due to storage\ndemand increase. We develop algorithms to minimize the data migration time. We\nshow that it is possible to find an optimal migration schedule when all $c_v$'s\nare even. Furthermore, though the problem is NP-hard in general, we give an\nefficient soft edge coloring algorithm that offers a rigorous $(1 +\no(1))$-approximation guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 19 Oct 2010 19:37:09 GMT"}], "update_date": "2010-10-20", "authors_parsed": [["Kari", "Chadi", ""]]}, {"id": "1010.4411", "submitter": "Valmir Barbosa", "authors": "Fabiano de S. Oliveira, Valmir C. Barbosa", "title": "Revisiting deadlock prevention: a probabilistic approach", "comments": null, "journal-ref": "Networks 63 (2014), 203-210", "doi": "10.1002/net.21537", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the deadlock-prevention problem by focusing on priority digraphs\ninstead of the traditional wait-for digraphs. This has allowed us to formulate\ndeadlock prevention in terms of prohibiting the occurrence of directed cycles\neven in the most general of wait models (the so-called AND-OR model, in which\nprohibiting wait-for directed cycles is generally overly restrictive). For a\nparticular case in which the priority digraphs are somewhat simplified, we\nintroduce a Las Vegas probabilistic mechanism for resource granting and analyze\nits key aspects in detail.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 10:12:59 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Oliveira", "Fabiano de S.", ""], ["Barbosa", "Valmir C.", ""]]}, {"id": "1010.4639", "submitter": "Antonio Wendell De Oliveira Rodrigues", "authors": "Antonio Wendell De Oliveira Rodrigues (INRIA Lille - Nord Europe,\n  LIFL), Fr\\'ed\\'eric Guyomarch (INRIA Lille - Nord Europe, LIFL), Yvonnick Le\n  Menach (L2EP), Jean-Luc Dekeyser (INRIA Lille - Nord Europe, LIFL)", "title": "Parallel Sparse Matrix Solver on the GPU Applied to Simulation of\n  Electrical Machines", "comments": null, "journal-ref": "Compumag 2009, Florianopolis : Brazil (2009)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, several industrial applications are being ported to parallel\narchitectures. In fact, these platforms allow acquire more performance for\nsystem modelling and simulation. In the electric machines area, there are many\nproblems which need speed-up on their solution. This paper examines the\nparallelism of sparse matrix solver on the graphics processors. More\nspecifically, we implement the conjugate gradient technique with input matrix\nstored in CSR, and Symmetric CSR and CSC formats. This method is one of the\nmost efficient iterative methods available for solving the finite-element basis\nfunctions of Maxwell's equations. The GPU (Graphics Processing Unit), which is\nused for its implementation, provides mechanisms to parallel the algorithm.\nThus, it increases significantly the computation speed in relation to serial\ncode on CPU based systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 08:46:04 GMT"}], "update_date": "2010-10-25", "authors_parsed": [["Rodrigues", "Antonio Wendell De Oliveira", "", "INRIA Lille - Nord Europe,\n  LIFL"], ["Guyomarch", "Fr\u00e9d\u00e9ric", "", "INRIA Lille - Nord Europe, LIFL"], ["Menach", "Yvonnick Le", "", "L2EP"], ["Dekeyser", "Jean-Luc", "", "INRIA Lille - Nord Europe, LIFL"]]}, {"id": "1010.4813", "submitter": "Bruce Berriman", "authors": "G. Bruce Berriman, Ewa Deelman, Gideon Juve, Moira Regelson, Peter\n  Plavchan", "title": "The Application of Cloud Computing to Astronomy: A Study of Cost and\n  Performance", "comments": "7 pages, accepted for publication in the Proceedings of the e-Science\n  in Astronomy Conference (Brisbane, Australia, December 2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a powerful new technology that is widely used in the\nbusiness world. Recently, we have been investigating the benefits it offers to\nscientific computing. We have used three workflow applications to compare the\nperformance of processing data on the Amazon EC2 cloud with the performance on\nthe Abe high-performance cluster at the National Center for Supercomputing\nApplications (NCSA). We show that the Amazon EC2 cloud offers better\nperformance and value for processor- and memory-limited applications than for\nI/O-bound applications. We provide an example of how the cloud is well suited\nto the generation of a science product: an atlas of periodograms for the\n210,000 light curves released by the NASA Kepler Mission. This atlas will\nsupport the identification of periodic signals, including those due to\ntransiting exoplanets, in the Kepler data sets.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 20:55:26 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Berriman", "G. Bruce", ""], ["Deelman", "Ewa", ""], ["Juve", "Gideon", ""], ["Regelson", "Moira", ""], ["Plavchan", "Peter", ""]]}, {"id": "1010.4822", "submitter": "Bruce Berriman", "authors": "Gideon Juve, Ewa Deelman, Karan Vahi, Gaurang Mehta, Bruce Berriman,\n  Benjamin P. Berman, and Phil Maechling", "title": "Data Sharing Options for Scientific Workflows on Amazon EC2", "comments": "9 pages, 7 figures. Accepted for publication in the Proceedings of\n  Supercomputing 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient data management is a key component in achieving good performance\nfor scientific workflows in distributed environments. Workflow applications\ntypically communicate data between tasks using files. When tasks are\ndistributed, these files are either transferred from one computational node to\nanother, or accessed through a shared storage system. In grids and clusters,\nworkflow data is often stored on network and parallel file systems. In this\npaper we investigate some of the ways in which data can be managed for\nworkflows in the cloud. We ran experiments using three typical workflow\napplications on Amazon's EC2. We discuss the various storage and file systems\nwe used, describe the issues and problems we encountered deploying them on EC2,\nand analyze the resulting performance and cost of the workflows.\n", "versions": [{"version": "v1", "created": "Fri, 22 Oct 2010 22:25:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Juve", "Gideon", ""], ["Deelman", "Ewa", ""], ["Vahi", "Karan", ""], ["Mehta", "Gaurang", ""], ["Berriman", "Bruce", ""], ["Berman", "Benjamin P.", ""], ["Maechling", "Phil", ""]]}, {"id": "1010.4843", "submitter": "Massimo Brescia Dr", "authors": "Massimo Brescia, Giuseppe Longo, George S. Djorgovski, Stefano\n  Cavuoti, Raffaele D'Abrusco, Ciro Donalek, Alessandro Di Guido, Michelangelo\n  Fiore, Mauro Garofalo, Omar Laurino, Ashish Mahabal, Francesco Manna, Alfonso\n  Nocella, Giovanni d'Angelo, Maurizio Paolillo", "title": "DAME: A Web Oriented Infrastructure for Scientific Data Mining &\n  Exploration", "comments": "16 pages, 9 figures, software available at\n  http://voneural.na.infn.it/beta_info.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.DB cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many scientific areas share the same need of being able to deal\nwith massive and distributed datasets and to perform on them complex knowledge\nextraction tasks. This simple consideration is behind the international efforts\nto build virtual organizations such as, for instance, the Virtual Observatory\n(VObs). DAME (DAta Mining & Exploration) is an innovative, general purpose,\nWeb-based, VObs compliant, distributed data mining infrastructure specialized\nin Massive Data Sets exploration with machine learning methods. Initially fine\ntuned to deal with astronomical data only, DAME has evolved in a general\npurpose platform which has found applications also in other domains of human\nendeavor. We present the products and a short outline of a science case,\ntogether with a detailed description of main features available in the beta\nrelease of the web application now released.\n", "versions": [{"version": "v1", "created": "Sat, 23 Oct 2010 04:43:13 GMT"}, {"version": "v2", "created": "Wed, 8 Dec 2010 04:48:34 GMT"}], "update_date": "2010-12-09", "authors_parsed": [["Brescia", "Massimo", ""], ["Longo", "Giuseppe", ""], ["Djorgovski", "George S.", ""], ["Cavuoti", "Stefano", ""], ["D'Abrusco", "Raffaele", ""], ["Donalek", "Ciro", ""], ["Di Guido", "Alessandro", ""], ["Fiore", "Michelangelo", ""], ["Garofalo", "Mauro", ""], ["Laurino", "Omar", ""], ["Mahabal", "Ashish", ""], ["Manna", "Francesco", ""], ["Nocella", "Alfonso", ""], ["d'Angelo", "Giovanni", ""], ["Paolillo", "Maurizio", ""]]}, {"id": "1010.4952", "submitter": "Mikael Fernandus Simalango", "authors": "Mikael Fernandus Simalango, Mun-Young Kang and Sangyoon Oh", "title": "Towards Constraint-based High Performance Cloud System in the Process of\n  Cloud Computing Adoption in an Organization", "comments": "11 pages, 5 figures; Proceedings of 1st KIITA Conference on Smart\n  Enterprise, Seoul, South Korea, October 21st, 2010, pp. 45-55", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Cloud computing is penetrating into various domains and environments, from\ntheoretical computer science to economy, from marketing hype to educational\ncurriculum and from R&D lab to enterprise IT infrastructure. Yet, the currently\ndeveloping state of cloud computing leaves several issues to address and also\naffects cloud computing adoption by organizations. In this paper, we explain\nhow the transition into the cloud can occur in an organization and describe the\nmechanism for transforming legacy infrastructure into a virtual\ninfrastructure-based cloud. We describe the state of the art of infrastructural\ncloud, which is essential in the decision making on cloud adoption, and\nhighlight the challenges that can limit the scale and speed of the adoption. We\nthen suggest a strategic framework for designing a high performance cloud\nsystem. This framework is applicable when transformation cloudbased deployment\nmodel collides with some constraints. We give an example of the implementation\nof the framework in a design of a budget-constrained high availability cloud\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 24 Oct 2010 12:08:12 GMT"}], "update_date": "2010-10-26", "authors_parsed": [["Simalango", "Mikael Fernandus", ""], ["Kang", "Mun-Young", ""], ["Oh", "Sangyoon", ""]]}, {"id": "1010.5308", "submitter": "EPTCS", "authors": "Simon Bliudze, Roberto Bruni, Davide Grohmann, Alexandra Silva", "title": "Proceedings Third Interaction and Concurrency Experience: Guaranteed\n  Interaction", "comments": null, "journal-ref": "EPTCS 38, 2010", "doi": "10.4204/EPTCS.38", "report-no": null, "categories": "cs.LO cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 3rd Interaction and Concurrency\nExperience (ICE 2010) workshop, which was held in Amsterdam, Netherlands on\n10th of June 2010 as a satellite event of DisCoTec'10. Each year, the workshop\nfocuses on a specific topic: the topic of ICE 2010 was Guaranteed Interactions,\nby which we mean, for example, guaranteeing safety, reactivity, quality of\nservice or satisfaction of analysis hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 03:39:19 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Bliudze", "Simon", ""], ["Bruni", "Roberto", ""], ["Grohmann", "Davide", ""], ["Silva", "Alexandra", ""]]}, {"id": "1010.5421", "submitter": "Subhash Kak", "authors": "Subhash Kak", "title": "On the Mesh Array for Matrix Multiplication", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents new properties of the mesh array for matrix\nmultiplication. In contrast to the standard array that requires 3n-2 steps to\ncomplete its computation, the mesh array requires only 2n-1 steps. Symmetries\nof the mesh array computed values are presented which enhance the efficiency of\nthe array for specific applications. In multiplying symmetric matrices, the\nresults are obtained in 3n/2+1 steps. The mesh array is examined for its\napplication as a scrambling system.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 15:10:37 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Kak", "Subhash", ""]]}, {"id": "1010.5497", "submitter": "Guanfeng Liang", "authors": "Guanfeng Liang and Nitin Vaidya", "title": "Multiparty Equality Function Computation in Networks with Point-to-Point\n  Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we study the multiparty communication complexity problem of\nthe multiparty equality function (MEQ): EQ(x_1,...,x_n) = 1 if x_1=...=x_n, and\n0 otherwise. The input vector (x_1,...,x_n) is distributed among n>=2 nodes,\nwith x_i known to node i, where x_i is chosen from the set {1,...,M}, for some\ninteger M>0.\n  Instead of the \"number on the forehand\" model, we consider a point-to-point\ncommunication model (similar to the message passing model), which we believe is\nmore realistic in networking settings. We assume a synchronous fully connected\nnetwork of n nodes, the node IDs (identifiers) are common knowledge. We assume\nthat all point-to-point communication channels/links are private such that when\na node transmits, only the designated recipient can receive the message. The\nidentity of the sender is known to the recipient.\n  We demonstrate that traditional techniques generalized from two-party\ncommunication complexity problem are not sufficient to obtain tight bounds\nunder the point-to-point communication model. We then introduce techniques\nwhich significantly reduce the space of protocols to study. These techniques\nare used to study some instances of the MEQ problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 19:47:12 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Liang", "Guanfeng", ""], ["Vaidya", "Nitin", ""]]}, {"id": "1010.5567", "submitter": "EPTCS", "authors": "Alejandro Mario Hernandez (Technical University of Denmark), Flemming\n  Nielson (Technical University of Denmark)", "title": "History-sensitive versus future-sensitive approaches to security in\n  distributed systems", "comments": "In Proceedings ICE 2010, arXiv:1010.5308", "journal-ref": "EPTCS 38, 2010, pp. 29-43", "doi": "10.4204/EPTCS.38.5", "report-no": null, "categories": "cs.CR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of aspect-oriented techniques as a flexible way to deal\nwith security policies in distributed systems. Recent work suggests to use\naspects for analysing the future behaviour of programs and to make access\ncontrol decisions based on this; this gives the flavour of dealing with\ninformation flow rather than mere access control. We show in this paper that it\nis beneficial to augment this approach with history-based components as is the\ntraditional approach in reference monitor-based approaches to mandatory access\ncontrol. Our developments are performed in an aspect-oriented coordination\nlanguage aiming to describe the Bell-LaPadula policy as elegantly as possible.\nFurthermore, the resulting language has the capability of combining both\nhistory- and future-sensitive policies, providing even more flexibility and\npower.\n", "versions": [{"version": "v1", "created": "Wed, 27 Oct 2010 05:04:17 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Hernandez", "Alejandro Mario", "", "Technical University of Denmark"], ["Nielson", "Flemming", "", "Technical University of Denmark"]]}, {"id": "1010.5573", "submitter": "EPTCS", "authors": "Renaud Sirdey (Commissariat \\`a l'Energie Atomique, France), Pascal\n  Aubry (Commissariat \\`a l'Energie Atomique, France)", "title": "A linear programming approach to general dataflow process network\n  verification and dimensioning", "comments": "In Proceedings ICE 2010, arXiv:1010.5308", "journal-ref": "EPTCS 38, 2010, pp. 115-119", "doi": "10.4204/EPTCS.38.11", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present linear programming-based sufficient conditions,\nsome of them polynomial-time, to establish the liveness and memory boundedness\nof general dataflow process networks. Furthermore, this approach can be used to\nobtain safe upper bounds on the size of the channel buffers of such a network.\n", "versions": [{"version": "v1", "created": "Wed, 27 Oct 2010 05:04:48 GMT"}], "update_date": "2010-10-28", "authors_parsed": [["Sirdey", "Renaud", "", "Commissariat \u00e0 l'Energie Atomique, France"], ["Aubry", "Pascal", "", "Commissariat \u00e0 l'Energie Atomique, France"]]}]