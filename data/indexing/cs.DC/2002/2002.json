[{"id": "2002.00069", "submitter": "Philokypros Ioulianou", "authors": "Ryan Smith, Daniel Palin, Philokypros P. Ioulianou, Vassilios G.\n  Vassilakis, Siamak F. Shahandashti", "title": "Battery draining attacks against edge computing nodes in IoT networks", "comments": "19 pages,", "journal-ref": "Cyber-Physical Systems (2020), pp.1-21", "doi": "10.1080/23335777.2020.1716268", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many IoT devices, especially those deployed at the network edge have limited\npower resources. A number of attacks aim to exhaust these resources and drain\nthe batteries of such edge nodes. In this work, we study the effects of a\nvariety of battery draining attacks against edge nodes. Through simulation, we\nclarify the extent to which such attacks are able to increase the usage and\nhence waste the power resources of edge nodes. Specifically, we implement hello\nflooding, packet flooding, selective forwarding, rank attack, and versioning\nattack in ContikiOS and simulate them in the Cooja simulator, and measure and\nreport a number of time and power resource usage metrics including CPU time,\nlow power mode time, TX/RX time, and battery consumption. Besides, we test the\nstretch attack with three different batteries as an extreme scenario. Our\nextensive measurements enable us to compare the effectiveness of these attacks.\nOur results show that Versioning attack is the most severe attack in terms of\ndraining the power resources of the network, followed by Packet Flooding and\nHello Flood attacks. Furthermore, we confirm that Selective Forwarding and Rank\nattacks are not able to considerably increase the power resource usage in our\nscenarios. By quantifying the effects of these attacks, we demonstrate that\nunder specific scenarios, Versioning attack can be three to four times as\neffective as Packet Flooding and Hello Flood attacks in wasting network\nresources, while Packet Flooding is generally comparable to Hello Flood in CPU\nand TX time usage increase but twice as powerful in draining device batteries.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 21:44:21 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 17:03:48 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Smith", "Ryan", ""], ["Palin", "Daniel", ""], ["Ioulianou", "Philokypros P.", ""], ["Vassilakis", "Vassilios G.", ""], ["Shahandashti", "Siamak F.", ""]]}, {"id": "2002.00141", "submitter": "Dongfang Zhao", "authors": "Abdullah Al-Mamun and Dongfang Zhao", "title": "SciChain: Trustworthy Scientific Data Provenance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art for auditing and reproducing scientific applications on\nhigh-performance computing (HPC) systems is through a data provenance\nsubsystem. While recent advances in data provenance lie in reducing the\nperformance overhead and improving the user's query flexibility, the fidelity\nof data provenance is often overlooked: there is no such a way to ensure that\nthe provenance data itself has not been fabricated or falsified. This paper\nadvocates to leverage blockchains to deliver immutable and autonomous data\nprovenance services such that scientific data are trustworthy. The challenges\nfor adopting blockchains to HPC include designing a new blockchain architecture\ncompatible with the HPC platforms and, more importantly, a set of new consensus\nprotocols for scientific applications atop blockchains. To this end, we have\ndesigned the proof-of-scalable-traceability (POST) protocol and implemented it\nin a blockchain prototype, namely SciChain, the very first blockchain system\nfor HPC. We evaluated SciChain by comparing it with multiple state-of-the-art\nsystems; Experimental results showed that SciChain guaranteed trustworthy data\nwhile incurring orders of magnitude lower overhead.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 04:54:58 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Al-Mamun", "Abdullah", ""], ["Zhao", "Dongfang", ""]]}, {"id": "2002.00160", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Sajjad Rahnama, Jelle Hellings, Mohammad Sadoghi", "title": "ResilientDB: Global Scale Resilient Blockchain Fabric", "comments": null, "journal-ref": "PVLDB 13 (2020) 868-883", "doi": "10.14778/3380750.3380757", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in blockchain technology have inspired innovative new\ndesigns in resilient distributed and database systems. At their core, these\nblockchain applications typically use Byzantine fault-tolerant consensus\nprotocols to maintain a common state across all replicas, even if some replicas\nare faulty or malicious. Unfortunately, existing consensus protocols are not\ndesigned to deal with geo-scale deployments in which many replicas spread\nacross a geographically large area participate in consensus. To address this,\nwe present the Geo-Scale Byzantine FaultTolerant consensus protocol (GeoBFT).\nGeoBFT is designed for excellent scalability by using a topological-aware\ngrouping of replicas in local clusters, by introducing parallelization of\nconsensus at the local level, and by minimizing communication between clusters.\nTo validate our vision of high-performance geo-scale resilient distributed\nsystems, we implement GeoBFT in our efficient ResilientDB permissioned\nblockchain fabric. We show that GeoBFT is not only sound and provides great\nscalability, but also outperforms state-of-the-art consensus protocols by a\nfactor of six in geo-scale deployments.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 07:20:23 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 20:27:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gupta", "Suyash", ""], ["Rahnama", "Sajjad", ""], ["Hellings", "Jelle", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2002.00215", "submitter": "Sergio Esteves", "authors": "Sergio Esteves, Joao Nuno Silva, and Luis Veiga", "title": "Palpatine: Mining Frequent Sequences for Data Prefetching in NoSQL\n  Distributed Key-Value Stores", "comments": "14 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents PALPATINE, the first in-memory application-level cache\nfor Distributed Key-Value (DKV) data stores, capable of prefetching data that\nis likely to be accessed in an immediate future. To predict data accesses,\nPALPATINE continuously captures frequent access patterns to the back store by\nmeans of data mining techniques. With these patterns, PALPATINE builds a\nstochastic graph of accessed items, and makes prefetching decisions based on\nit.\n  Experimental evaluation indicates that PALPATINE can improve the latency of a\nspecific DKV store by more that an order of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 14:19:27 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 20:10:22 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Esteves", "Sergio", ""], ["Silva", "Joao Nuno", ""], ["Veiga", "Luis", ""]]}, {"id": "2002.00306", "submitter": "Aidin Ferdowsi", "authors": "Aidin Ferdowsi and Walid Saad", "title": "Brainstorming Generative Adversarial Networks (BGANs): Towards\n  Multi-Agent Generative Models with Distributed Private Datasets", "comments": "13 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To achieve a high learning accuracy, generative adversarial networks (GANs)\nmust be fed by large datasets that adequately represent the data space.\nHowever, in many scenarios, the available datasets may be limited and\ndistributed across multiple agents, each of which is seeking to learn the\ndistribution of the data on its own. In such scenarios, the local datasets are\ninherently private and agents often do not wish to share them. In this paper,\nto address this multi-agent GAN problem, a novel brainstorming GAN (BGAN)\narchitecture is proposed using which multiple agents can generate real-like\ndata samples while operating in a fully distributed manner and preserving their\ndata privacy. BGAN allows the agents to gain information from other agents\nwithout sharing their real datasets but by \"brainstorming\" via the sharing of\ntheir generated data samples. In contrast to existing distributed GAN\nsolutions, the proposed BGAN architecture is designed to be fully distributed,\nand it does not need any centralized controller. Moreover, BGANs are shown to\nbe scalable and not dependent on the hyperparameters of the agents' deep neural\nnetworks (DNNs) thus enabling the agents to have different DNN architectures.\nTheoretically, the interactions between BGAN agents are analyzed as a game\nwhose unique Nash equilibrium is derived. Experimental results show that BGAN\ncan generate real-like data samples with higher quality and lower\nJensen-Shannon divergence (JSD) and Fr\\'echet Inception distance (FID) compared\nto other distributed GAN architectures.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 02:58:32 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 02:49:06 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Ferdowsi", "Aidin", ""], ["Saad", "Walid", ""]]}, {"id": "2002.00352", "submitter": "Xingwen Zhang", "authors": "Xingwen Zhang, Feng Qi, Zhigang Hua, Shuang Yang", "title": "Solving Billion-Scale Knapsack Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knapsack problems (KPs) are common in industry, but solving KPs is known to\nbe NP-hard and has been tractable only at a relatively small scale. This paper\nexamines KPs in a slightly generalized form and shows that they can be solved\nnearly optimally at scale via distributed algorithms. The proposed approach can\nbe implemented fairly easily with off-the-shelf distributed computing\nframeworks (e.g. MPI, Hadoop, Spark). As an example, our implementation leads\nto one of the most efficient KP solvers known to date -- capable to solve KPs\nat an unprecedented scale (e.g., KPs with 1 billion decision variables and 1\nbillion constraints can be solved within 1 hour). The system has been deployed\nto production and called on a daily basis, yielding significant business\nimpacts at Ant Financial.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 08:51:36 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Zhang", "Xingwen", ""], ["Qi", "Feng", ""], ["Hua", "Zhigang", ""], ["Yang", "Shuang", ""]]}, {"id": "2002.00577", "submitter": "Kangying Lin", "authors": "Huawei Huang, Kangying Lin, Song Guo, Pan Zhou, Zibin Zheng", "title": "Prophet: Proactive Candidate-Selection for Federated Learning by\n  Predicting the Qualities of Training and Reporting Phases", "comments": "We found significant technique errors in our previous version. The\n  proposed DRL-based algorithm cannot solve the large-scale scheduling for\n  federated learning. For the health of relevant research communities, we\n  decide to withdraw our submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the challenge of the device connection is much relieved in 5G\nnetworks, the training latency is still an obstacle preventing Federated\nLearning (FL) from being largely adopted. One of the most fundamental problems\nthat lead to large latency is the bad candidate-selection for FL. In the\ndynamic environment, the mobile devices selected by the existing reactive\ncandidate-selection algorithms very possibly fail to complete the training and\nreporting phases of FL, because the FL parameter server only knows the\ncurrently-observed resources of all candidates. To this end, we study the\nproactive candidate-selection for FL in this paper. We first let each candidate\ndevice predict the qualities of both its training and reporting phases locally\nusing LSTM. Then, the proposed candidateselection algorithm is implemented by\nthe Deep Reinforcement Learning (DRL) framework. Finally, the real-world\ntrace-driven experiments prove that the proposed approach outperforms the\nexisting reactive algorithms\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 06:40:04 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 01:55:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Huang", "Huawei", ""], ["Lin", "Kangying", ""], ["Guo", "Song", ""], ["Zhou", "Pan", ""], ["Zheng", "Zibin", ""]]}, {"id": "2002.00655", "submitter": "Alexander Renz-Wieland", "authors": "Alexander Renz-Wieland, Rainer Gemulla, Steffen Zeuch, Volker Markl", "title": "Dynamic Parameter Allocation in Parameter Servers", "comments": null, "journal-ref": "PVLDB, 13(11): 1877-1890, 2020", "doi": "10.14778/3407790.3407796", "report-no": null, "categories": "cs.LG cs.DB cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To keep up with increasing dataset sizes and model complexity, distributed\ntraining has become a necessity for large machine learning tasks. Parameter\nservers ease the implementation of distributed parameter management---a key\nconcern in distributed training---, but can induce severe communication\noverhead. To reduce communication overhead, distributed machine learning\nalgorithms use techniques to increase parameter access locality (PAL),\nachieving up to linear speed-ups. We found that existing parameter servers\nprovide only limited support for PAL techniques, however, and therefore prevent\nefficient training. In this paper, we explore whether and to what extent PAL\ntechniques can be supported, and whether such support is beneficial. We propose\nto integrate dynamic parameter allocation into parameter servers, describe an\nefficient implementation of such a parameter server called Lapse, and\nexperimentally compare its performance to existing parameter servers across a\nnumber of machine learning tasks. We found that Lapse provides near-linear\nscaling and can be orders of magnitude faster than existing parameter servers.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 11:37:54 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 14:05:58 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 12:52:13 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Renz-Wieland", "Alexander", ""], ["Gemulla", "Rainer", ""], ["Zeuch", "Steffen", ""], ["Markl", "Volker", ""]]}, {"id": "2002.00756", "submitter": "Eike Hermann M\\\"uller", "authors": "Christopher Maynard, Thomas Melvin, Eike Hermann M\\\"uller", "title": "Multigrid preconditioners for the mixed finite element dynamical core of\n  the LFRic atmospheric model", "comments": "22 pages, 6 figures, 5 tables, to appear in Quarterly Journal of the\n  Royal Meteorological Society", "journal-ref": null, "doi": "10.1002/qj.3880", "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide separation of time scales in geophysical fluid dynamics,\nsemi-implicit time integrators are commonly used in operational atmospheric\nforecast models. They guarantee the stable treatment of fast (acoustic and\ngravity) waves, while not suffering from severe restrictions on the timestep\nsize. To propagate the state of the atmosphere forward in time, a non-linear\nequation for the prognostic variables has to be solved at every timestep. Since\nthe nonlinearity is typically weak, this is done with a small number of Newton-\nor Picard- iterations, which in turn require the efficient solution of a large\nsystem on linear equations with $\\mathcal{O}(10^6-10^9)$ unknowns. This linear\nsolve is often the computationally most costly part of the model. In this paper\nan efficient linear solver for the LFRic next-generation model, currently\ndeveloped by the Met Office, is described. The model uses an advanced mimetic\nfinite element discretisation which makes the construction of efficient solvers\nchallenging compared to models using standard finite-difference and\nfinite-volume methods. The linear solver hinges on a bespoke multigrid\npreconditioner of the Schur-complement system for the pressure correction. By\ncomparing to Krylov-subspace methods, the superior performance and robustness\nof the multigrid algorithm is demonstrated for standard test cases and\nrealistic model setups. In production mode, the model will have to run in\nparallel on 100,000s of processing elements. As confirmed by numerical\nexperiments, one particular advantage of the multigrid solver is its excellent\nparallel scalability due to avoiding expensive global reduction operations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:16:55 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 11:09:32 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Maynard", "Christopher", ""], ["Melvin", "Thomas", ""], ["M\u00fcller", "Eike Hermann", ""]]}, {"id": "2002.01081", "submitter": "Naoki Shibata", "authors": "Babatunde Ojetunde, Naoki Shibata, Juntao Gao", "title": "Secure Payment System Utilizing MANET for Disaster Areas", "comments": null, "journal-ref": "in IEEE Transactions on Systems, Man, and Cybernetics: Systems,\n  vol. 49, no. 12, pp. 2651-2663, Dec. 2019", "doi": "10.1109/TSMC.2017.2752203", "report-no": null, "categories": "cs.DC cs.CY cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile payment system in a disaster area have the potential to provide\nelectronic transactions for people purchasing recovery goods like foodstuffs,\nclothes, and medicine. Conversely, to enable transactions in a disaster area,\ncurrent payment systems need communication infrastructures (such as wired\nnetworks and cellular networks) which may be ruined during such disasters as\nlarge-scale earthquakes and flooding and thus cannot be depended on in a\ndisaster area. In this paper, we introduce a new mobile payment system\nutilizing infrastructureless MANETs to enable transactions that permit users to\nshop in disaster areas. Specifically, we introduce an endorsement-based\nmechanism to provide payment guarantees for a customer-to-merchant transaction\nand a multilevel endorsement mechanism with a lightweight scheme based on Bloom\nfilter and Merkle tree to reduce communication overheads. Our mobile payment\nsystem achieves secure transaction by adopting various schemes such as\nlocation-based mutual monitoring scheme and blind signature, while our newly\nintroduce event chain mechanism prevents double spending attacks. As validated\nby simulations, the proposed mobile payment system is useful in a disaster\narea, achieving high transaction completion ratio, 65% - 90% for all scenario\ntested, and is storage-efficient for mobile devices with an overall average of\n7MB merchant message size.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 01:38:14 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ojetunde", "Babatunde", ""], ["Shibata", "Naoki", ""], ["Gao", "Juntao", ""]]}, {"id": "2002.01119", "submitter": "Wei Zhang", "authors": "Wei Zhang, Xiaodong Cui, Abdullah Kayi, Mingrui Liu, Ulrich Finkler,\n  Brian Kingsbury, George Saon, Youssef Mroueh, Alper Buyuktosunoglu, Payel\n  Das, David Kung, Michael Picheny", "title": "Improving Efficiency in Large-Scale Decentralized Distributed Training", "comments": null, "journal-ref": "45th International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP'2020) Oral", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized Parallel SGD (D-PSGD) and its asynchronous variant Asynchronous\nParallel SGD (AD-PSGD) is a family of distributed learning algorithms that have\nbeen demonstrated to perform well for large-scale deep learning tasks. One\ndrawback of (A)D-PSGD is that the spectral gap of the mixing matrix decreases\nwhen the number of learners in the system increases, which hampers convergence.\nIn this paper, we investigate techniques to accelerate (A)D-PSGD based training\nby improving the spectral gap while minimizing the communication cost. We\ndemonstrate the effectiveness of our proposed techniques by running experiments\non the 2000-hour Switchboard speech recognition task and the ImageNet computer\nvision task. On an IBM P9 supercomputer, our system is able to train an LSTM\nacoustic model in 2.28 hours with 7.5% WER on the Hub5-2000 Switchboard (SWB)\ntest set and 13.3% WER on the CallHome (CH) test set using 64 V100 GPUs and in\n1.98 hours with 7.7% WER on SWB and 13.3% WER on CH using 128 V100 GPUs, the\nfastest training time reported to date.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 04:29:09 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Zhang", "Wei", ""], ["Cui", "Xiaodong", ""], ["Kayi", "Abdullah", ""], ["Liu", "Mingrui", ""], ["Finkler", "Ulrich", ""], ["Kingsbury", "Brian", ""], ["Saon", "George", ""], ["Mroueh", "Youssef", ""], ["Buyuktosunoglu", "Alper", ""], ["Das", "Payel", ""], ["Kung", "David", ""], ["Picheny", "Michael", ""]]}, {"id": "2002.01283", "submitter": "Giuliano Taffoni", "authors": "Giuliano Taffoni, Ugo Becciani, Bianca Garilli, Gianmarco Maggio,\n  Fabio Pasian, Grazia Umana, Riccardo Smareglia and Fabio Vitello", "title": "CHIPP: INAF pilot project for HTC, HPC and HPDA", "comments": "4 pages, conference, ADASS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CHIPP (Computing HTC in INAF Pilot Project) is an Italian project funded by\nthe Italian Institute for Astrophysics (INAF) and promoted by the ICT office of\nINAF. The main purpose of the CHIPP project is to coordinate the use of, and\naccess to, already existing high throughput computing and high-performance\ncomputing and data processing resources (for small/medium size programs) for\nthe INAF community. Today, Tier2/Tier3 systems (1,200 CPU/core) are provided at\nthe INAF institutes at Trieste and Catania, but in the future, the project will\nevolve including also other computing infrastructures. During the last two\nyears, more than 30 programs have been approved for a total request of 30\nMillion CPU-h. Most of the programs are HPC, data reduction and analysis,\nmachine learning. In this paper, we describe in details the CHIPP\ninfrastructures and the results of the first two years of activity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 13:51:24 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Taffoni", "Giuliano", ""], ["Becciani", "Ugo", ""], ["Garilli", "Bianca", ""], ["Maggio", "Gianmarco", ""], ["Pasian", "Fabio", ""], ["Umana", "Grazia", ""], ["Smareglia", "Riccardo", ""], ["Vitello", "Fabio", ""]]}, {"id": "2002.01337", "submitter": "Jinhyun Ahn", "authors": "Jin-Hyun Ahn, Osvaldo Simeone, Joonhyuk Kang", "title": "Cooperative Learning via Federated Distillation over Fading Channels", "comments": "to appear in Proc. IEEE International Conference on Acoustics,\n  Speech, and Signal Processing (ICASSP) 2020. arXiv admin note: text overlap\n  with arXiv:1907.02745", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative training methods for distributed machine learning are typically\nbased on the exchange of local gradients or local model parameters. The latter\napproach is known as Federated Learning (FL). An alternative solution with\nreduced communication overhead, referred to as Federated Distillation (FD), was\nrecently proposed that exchanges only averaged model outputs. While prior work\nstudied implementations of FL over wireless fading channels, here we propose\nwireless protocols for FD and for an enhanced version thereof that leverages an\noffline communication phase to communicate ``mixed-up'' covariate vectors. The\nproposed implementations consist of different combinations of digital schemes\nbased on separate source-channel coding and of over-the-air computing\nstrategies based on analog joint source-channel coding. It is shown that the\nenhanced version FD has the potential to significantly outperform FL in the\npresence of limited spectral resources.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 16:38:58 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ahn", "Jin-Hyun", ""], ["Simeone", "Osvaldo", ""], ["Kang", "Joonhyuk", ""]]}, {"id": "2002.01374", "submitter": "Ricardo P\\'erez-Marco", "authors": "Cyril Grunspan, Gabriel Leh\\'ericy, Ricardo P\\'erez-Marco", "title": "Ant Routing scalability for the Lightning Network", "comments": "26 pagew, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ambition of the Lightning Network is to provide a second layer to the\nBitcoin network to enable transactions confirmed instantly, securely and\nanonymously with a world scale capacity using a decentralized protocol. Some of\nthe current propositions and implementations present some difficulties in\nanonymity, scaling and decentalization. The Ant Routing algorithm for the\nLightning Network was proposed in \\cite{GrunspanPerez} for maximal\ndecentralization, anonymity and potential scaling. It solves several problems\nof current implementation, such as channel information update and\ncentralization by beacon nodes. Ant Routing nodes play all the same role and\ndon't require any extra information on the network topology beside for their\nimmediate neighbors. The goal of LN transactions are completed instantaneously\nand anonymously. We study the scaling of the Ant Routing protocol. We propose a\nprecise implementation, with efficient memory management using AVL trees. We\nevaluate the efficiency of the algorithm and we estimate the memory usage of\nnodes by local node workload simulations. We prove that the number of\ntransactions per second that Ant Routing can sustain is of the order of several\nthousands which is enough for a global payment network.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:49:29 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Grunspan", "Cyril", ""], ["Leh\u00e9ricy", "Gabriel", ""], ["P\u00e9rez-Marco", "Ricardo", ""]]}, {"id": "2002.01419", "submitter": "Christina Delimitrou", "authors": "Justin Hu, Ariana Bruno, Brian Ritchken, Brendon Jackson, Mateo\n  Espinosa, Aditya Shah, Christina Delimitrou", "title": "HiveMind: A Scalable and Serverless Coordination Control Platform for\n  UAV Swarms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarms of autonomous devices are increasing in ubiquity and size. There are\ntwo main trains of thought for controlling devices in such swarms; centralized\nand distributed control. Centralized platforms achieve higher output quality\nbut result in high network traffic and limited scalability, while decentralized\nsystems are more scalable, but less sophisticated.\n  In this work we present HiveMind, a centralized coordination control platform\nfor IoT swarms that is both scalable and performant. HiveMind leverages a\ncentralized cluster for all resource-intensive computation, deferring\nlightweight and time-critical operations, such as obstacle avoidance to the\nedge devices to reduce network traffic. HiveMind employs an event-driven\nserverless framework to run tasks on the cluster, guarantees fault tolerance\nboth in the edge devices and serverless functions, and handles straggler tasks\nand underperforming devices. We evaluate HiveMind on a swarm of 16 programmable\ndrones on two scenarios; searching for given items, and counting unique people\nin an area. We show that HiveMind achieves better performance and battery\nefficiency compared to fully centralized and fully decentralized platforms,\nwhile also handling load imbalances and failures gracefully, and allowing edge\ndevices to leverage the cluster to collectively improve their output quality.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 17:38:56 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Hu", "Justin", ""], ["Bruno", "Ariana", ""], ["Ritchken", "Brian", ""], ["Jackson", "Brendon", ""], ["Espinosa", "Mateo", ""], ["Shah", "Aditya", ""], ["Delimitrou", "Christina", ""]]}, {"id": "2002.01447", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "A Prototype of Serverless Lucene", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a working prototype that adapts Lucene, the world's most\npopular and most widely deployed open-source search library, to operate within\na serverless environment in the cloud. Although the serverless search concept\nis not new, this work represents a substantial improvement over a previous\nimplementation in eliminating most custom code and in enabling interactive\nsearch. While there remain limitations to the design, it nevertheless\nchallenges conventional thinking about search architectures for particular\noperating points.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 18:20:06 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "2002.01558", "submitter": "Marco Aldinucci", "authors": "Iacopo Colonnelli, Barbara Cantalupo, Ivan Merelli, and Marco\n  Aldinucci", "title": "StreamFlow: cross-breeding cloud with HPC", "comments": "30 pages - 2020 IEEE Transactions on Emerging Topics in Computing", "journal-ref": null, "doi": "10.1109/TETC.2020.3019202", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflows are among the most commonly used tools in a variety of execution\nenvironments. Many of them target a specific environment; few of them make it\npossible to execute an entire workflow in different environments, e.g.\nKubernetes and batch clusters. We present a novel approach to workflow\nexecution, called StreamFlow, that complements the workflow graph with the\ndeclarative description of potentially complex execution environments, and that\nmakes it possible the execution onto multiple sites not sharing a common data\nspace. StreamFlow is then exemplified on a novel bioinformatics pipeline for\nsingle-cell transcriptomic data analysis workflow.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 22:00:56 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 23:42:19 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 22:20:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Colonnelli", "Iacopo", ""], ["Cantalupo", "Barbara", ""], ["Merelli", "Ivan", ""], ["Aldinucci", "Marco", ""]]}, {"id": "2002.01614", "submitter": "Ji Liu", "authors": "Ji Liu, Abdullah-Al Kafi, Xipeng Shen, Huiyang Zhou", "title": "MKPipe: A Compiler Framework for Optimizing Multi-Kernel Workloads in\n  OpenCL for FPGA", "comments": "12 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenCL for FPGA enables developers to design FPGAs using a programming model\nsimilar for processors. Recent works have shown that code optimization at the\nOpenCL level is important to achieve high computational efficiency. However,\nexisting works either focus primarily on optimizing single kernels or solely\ndepend on channels to design multi-kernel pipelines. In this paper, we propose\na source-to-source compiler framework, MKPipe, for optimizing multi-kernel\nworkloads in OpenCL for FPGA. Besides channels, we propose new schemes to\nenable multi-kernel pipelines. Our optimizing compiler employs a systematic\napproach to explore the tradeoffs of these optimizations methods. To enable\nmore efficient overlapping between kernel execution, we also propose a novel\nworkitem/workgroup-id remapping technique. Furthermore, we propose new\nalgorithms for throughput balancing and resource balancing to tune the\noptimizations upon individual kernels in the multi-kernel workloads. Our\nresults show that our compiler-optimized multi-kernels achieve up to 3.6x (1.4x\non average) speedup over the baseline, in which the kernels have already been\noptimized individually.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 03:05:56 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Liu", "Ji", ""], ["Kafi", "Abdullah-Al", ""], ["Shen", "Xipeng", ""], ["Zhou", "Huiyang", ""]]}, {"id": "2002.01981", "submitter": "Arie Agranonik", "authors": "Arie Agranonik, Maya Herman, Mark Last", "title": "Parallel 3DPIFCM Algorithm for Noisy Brain MRI Images", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we implemented the algorithm we developed in [1] called 3DPIFCM\nin a parallel environment by using CUDA on a GPU. In our previous work we\nintroduced 3DPIFCM which performs segmentation of images in noisy conditions\nand uses particle swarm optimization for finding the optimal algorithm\nparameters to account for noise. This algorithm achieved state of the art\nsegmentation accuracy when compared to FCM (Fuzzy C-Means), IFCMPSO (Improved\nFuzzy C-Means with Particle Swarm Optimization), GAIFCM (Genetic Algorithm\nImproved Fuzzy C-Means) on noisy MRI images of an adult Brain.\n  When using a genetic algorithm or PSO (Particle Swarm Optimization) on a\nsingle machine for optimization we witnessed long execution times for practical\nclinical usage. Therefore, in the current paper our goal was to speed up the\nexecution of 3DPIFCM by taking out parts of the algorithm and executing them as\nkernels on a GPU. The algorithm was implemented using the CUDA [13] framework\nfrom NVIDIA and experiments where performed on a server containing 64GB RAM , 8\ncores and a TITAN X GPU with 3072 SP cores and 12GB of GPU memory.\n  Our results show that the parallel version of the algorithm performs up to\n27x faster than the original sequential version and 68x faster than GAIFCM\nalgorithm. We show that the speedup of the parallel version increases as we\nincrease the size of the image due to better utilization of cores in the GPU.\nAlso, we show a speedup of up to 5x in our Brainweb experiment compared to\nother generic variants such as IFCMPSO and GAIFCM.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:30:29 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Agranonik", "Arie", ""], ["Herman", "Maya", ""], ["Last", "Mark", ""]]}, {"id": "2002.01985", "submitter": "Arie Agranonik", "authors": "Arie Agranonik, Maya Herman, Mark Last", "title": "3DPIFCM Novel Algorithm for Segmentation of Noisy Brain MRI Images", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm named 3DPIFCM, for automatic segmentation of\nnoisy MRI Brain images. The algorithm is an extension of a well-known IFCM\n(Improved Fuzzy C-Means) algorithm. It performs fuzzy segmentation and\nintroduces a fitness function that is affected by proximity of the voxels and\nby the color intensity in 3D images. The 3DPIFCM algorithm uses PSO (Particle\nSwarm Optimization) in order to optimize the fitness function. In addition, the\n3DPIFCM uses 3D features of near voxels to better adjust the noisy artifacts.\nIn our experiments, we evaluate 3DPIFCM on T1 Brainweb dataset with noise\nlevels ranging from 1% to 20% and on a synthetic dataset with ground truth both\nin 3D. The analysis of the segmentation results shows a significant improvement\nin the segmentation quality of up to 28% compared to two generic variants in\nnoisy images and up to 60% when compared to the original FCM (Fuzzy C-Means).\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 20:48:51 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 19:22:59 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Agranonik", "Arie", ""], ["Herman", "Maya", ""], ["Last", "Mark", ""]]}, {"id": "2002.02090", "submitter": "Zhouyuan Huo", "authors": "Zhouyuan Huo, Qian Yang, Bin Gu, Lawrence Carin. Heng Huang", "title": "Faster On-Device Training Using New Federated Momentum Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile crowdsensing has gained significant attention in recent years and has\nbecome a critical paradigm for emerging Internet of Things applications. The\nsensing devices continuously generate a significant quantity of data, which\nprovide tremendous opportunities to develop innovative intelligent\napplications. To utilize these data to train machine learning models while not\ncompromising user privacy, federated learning has become a promising solution.\nHowever, there is little understanding of whether federated learning algorithms\nare guaranteed to converge. We reconsider model averaging in federated learning\nand formulate it as a gradient-based method with biased gradients. This novel\nperspective assists analysis of its convergence rate and provides a new\ndirection for more acceleration. We prove for the first time that the federated\naveraging algorithm is guaranteed to converge for non-convex problems, without\nimposing additional assumptions. We further propose a novel accelerated\nfederated learning algorithm and provide a convergence guarantee. Simulated\nfederated learning experiments are conducted to train deep neural networks on\nbenchmark datasets, and experimental results show that our proposed method\nconverges faster than previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 04:12:43 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Huo", "Zhouyuan", ""], ["Yang", "Qian", ""], ["Gu", "Bin", ""], ["Huang", "Lawrence Carin. Heng", ""]]}, {"id": "2002.02328", "submitter": "Mathieu Chalvidal", "authors": "Mathieu Chalvidal and Emilie Chouzenoux", "title": "Block Distributed Majorize-Minimize Memory Gradient Algorithm and its\n  application to 3D image restoration", "comments": "Accepted at ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern 3D image recovery problems require powerful optimization frameworks to\nhandle high dimensionality while providing reliable numerical solutions in a\nreasonable time. In this perspective, asynchronous parallel optimization\nalgorithms have received an increasing attention by overcoming memory\nlimitation issues and communication bottlenecks. In this work, we propose a\nblock distributed Majorize-Minorize Memory Gradient (BD3MG) optimization\nalgorithm for solving large scale non-convex differentiable optimization\nproblems. Assuming a distributed memory environment, the algorithm casts the\nefficient 3MG scheme into smaller dimension subproblems where blocks of\nvariables are addressed in an asynchronous manner. Convergence of the sequence\nbuilt by the proposed BD3MG method is established under mild assumptions.\nApplication to the restoration of 3D images degraded by a depth-variant blur\nshows that our method yields significant computational time reduction compared\nto several synchronous and asynchronous competitors, while exhibiting great\nscalability potential.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:07:57 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 12:48:18 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Chalvidal", "Mathieu", ""], ["Chouzenoux", "Emilie", ""]]}, {"id": "2002.02440", "submitter": "Michael Rudow", "authors": "Michael Rudow, K.V. Rashmi, Venkatesan Guruswami", "title": "A locality-based approach for coded computation", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed computation infrastructures are often plagued by\nunavailabilities such as failing or slow servers. These unavailabilities\nadversely affect the tail latency of computation in distributed\ninfrastructures. The simple solution of replicating computation entails\nsignificant resource overhead. Coded computation has emerged as a\nresource-efficient alternative, wherein multiple units of data are encoded to\ncreate parity units and the function to be computed is applied to each of these\nunits on distinct servers. A decoder can use the available function outputs to\ndecode the unavailable ones. Existing coded computation approaches are resource\nefficient only for simple variants of linear functions such as multilinear,\nwith even the class of low degree polynomials requiring the same multiplicative\noverhead as replication for practically relevant straggler tolerance.\n  In this paper, we present a new approach to model coded computation via the\nlens of locality of codes. We introduce a generalized notion of locality,\ndenoted computational locality, building upon the locality of an appropriately\ndefined code. We show that computational locality is equivalent to the required\nnumber of workers for coded computation and leverage results from the\nwell-studied locality of codes to design coded computation schemes. We show\nthat recent results on coded computation of multivariate polynomials can be\nderived using local recovering schemes for Reed-Muller codes. We present coded\ncomputation schemes for multivariate polynomials that adaptively exploit\nlocality properties of input data-- an inadmissible technique under existing\nframeworks. These schemes require fewer workers than the lower bound under\nexisting coded computation frameworks, showing that the existing multiplicative\noverhead on the number of servers is not fundamental for coded computation of\nnonlinear functions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:46:28 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Rudow", "Michael", ""], ["Rashmi", "K. V.", ""], ["Guruswami", "Venkatesan", ""]]}, {"id": "2002.02481", "submitter": "Francois Belletti", "authors": "Francois Belletti, Davis King, James Lottes, Yi-Fan Chen, John\n  Anderson", "title": "Sensitivity Analysis in the Dupire Local Volatility Model with\n  Tensorflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In a recent paper, we have demonstrated how the affinity between TPUs and\nmulti-dimensional financial simulation resulted in fast Monte Carlo simulations\nthat could be setup in a few lines of python Tensorflow code. We also presented\na major benefit from writing high performance simulations in an automated\ndifferentiation language such as Tensorflow: a single line of code enabled us\nto estimate sensitivities, i.e. the rate of change in price of financial\ninstrument with respect to another input such as the interest rate, the current\nprice of the underlying, or volatility. Such sensitivities (otherwise known as\nthe famous financial \"Greeks\") are fundamental for risk assessment and risk\nmitigation. In the present follow-up short paper, we extend the developments\nexposed in our previous work about the use of Tensor Processing Units and\nTensorflow for TPUs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:27:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Belletti", "Francois", ""], ["King", "Davis", ""], ["Lottes", "James", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "2002.02509", "submitter": "Rohit Zambre", "authors": "Rohit Zambre, Aparna Chandramowlishwaran, Pavan Balaji", "title": "Scalable Communication Endpoints for MPI+Threads Applications", "comments": "In Proceedings of the 24th IEEE International Conference on Parallel\n  and Distributed Systems (ICPADS), Sentosa, Singapore, December 2018. Best\n  Poster Award", "journal-ref": "In 2018 IEEE 24th International Conference on Parallel and\n  Distributed Systems (ICPADS), pp. 803-812. IEEE, 2018", "doi": "10.1109/PADSW.2018.8645059", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid MPI+threads programming is gaining prominence as an alternative to the\ntraditional \"MPI everywhere'\" model to better handle the disproportionate\nincrease in the number of cores compared with other on-node resources. Current\nimplementations of these two models represent the two extreme cases of\ncommunication resource sharing in modern MPI implementations. In the\nMPI-everywhere model, each MPI process has a dedicated set of communication\nresources (also known as endpoints), which is ideal for performance but is\nresource wasteful. With MPI+threads, current MPI implementations share a single\ncommunication endpoint for all threads, which is ideal for resource usage but\nis hurtful for performance.\n  In this paper, we explore the tradeoff space between performance and\ncommunication resource usage in MPI+threads environments. We first demonstrate\nthe two extreme cases---one where all threads share a single communication\nendpoint and another where each thread gets its own dedicated communication\nendpoint (similar to the MPI-everywhere model) and showcase the inefficiencies\nin both these cases. Next, we perform a thorough analysis of the different\nlevels of resource sharing in the context of Mellanox InfiniBand. Using the\nlessons learned from this analysis, we design an improved resource-sharing\nmodel to produce \\emph{scalable communication endpoints} that can achieve the\nsame performance as with dedicated communication resources per thread but using\njust a third of the resources.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 20:41:37 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zambre", "Rohit", ""], ["Chandramowlishwaran", "Aparna", ""], ["Balaji", "Pavan", ""]]}, {"id": "2002.02516", "submitter": "Ran Cohen", "authors": "Elette Boyle and Ran Cohen and Aarushi Goel", "title": "Breaking the $O(\\sqrt n)$-Bit Barrier: Byzantine Agreement with Polylog\n  Bits Per Party", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine agreement (BA), the task of $n$ parties to agree on one of their\ninput bits in the face of malicious agents, is a powerful primitive that lies\nat the core of a vast range of distributed protocols. Interestingly, in\nprotocols with the best overall communication, the demands of the parties are\nhighly unbalanced: the amortized cost is $\\tilde O(1)$ bits per party, but some\nparties must send $\\Omega(n)$ bits. In best known balanced protocols, the\noverall communication is sub-optimal, with each party communicating $\\tilde\nO(\\sqrt{n})$. In this work, we ask whether asymmetry is inherent for optimizing\ntotal communication. Our contributions in this line are as follows:\n  1) We define a cryptographic primitive, succinctly reconstructed distributed\nsignatures (SRDS), that suffices for constructing $\\tilde O(1)$ balanced BA. We\nprovide two constructions of SRDS from different cryptographic and Public-Key\nInfrastructure (PKI) assumptions.\n  2) The SRDS-based BA follows a paradigm of boosting from \"almost-everywhere\"\nagreement to full agreement, and does so in a single round. We prove that PKI\nsetup and cryptographic assumptions are necessary for such protocols in which\nevery party sends $o(n)$ messages.\n  3) We further explore connections between a natural approach toward attaining\nSRDS and average-case succinct non-interactive argument systems (SNARGs) for a\nparticular type of NP-Complete problems (generalizing Subset-Sum and\nSubset-Product).\n  Our results provide new approaches forward, as well as limitations and\nbarriers, towards minimizing per-party communication of BA. In particular, we\nconstruct the first two BA protocols with $\\tilde O(1)$ balanced communication,\noffering a tradeoff between setup and cryptographic assumptions, and answering\nan open question presented by King and Saia (DISC'09).\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 21:19:32 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 15:28:28 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2021 20:34:36 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 18:51:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Boyle", "Elette", ""], ["Cohen", "Ran", ""], ["Goel", "Aarushi", ""]]}, {"id": "2002.02563", "submitter": "Rohit Zambre", "authors": "Rohit Zambre, Megan Grodowitz, Aparna Chandramowlishwaran, Pavel\n  Shamis", "title": "Breaking Band: A Breakdown of High-performance Communication", "comments": "In Proceedings of the 48th ACM International Conference on Parallel\n  Processing (ICPP), Kyoto, Japan, August 2019", "journal-ref": "In Proceedings of the 48th International Conference on Parallel\n  Processing, pp. 1-10. 2019", "doi": "10.1145/3337821", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The critical path of internode communication on large-scale systems is\ncomposed of multiple components. When a supercomputing application initiates\nthe transfer of a message using a high-level communication routine such as an\nMPI_Send, the payload of the message traverses multiple software stacks, the\nI/O subsystem on both the host and target nodes, and network components such as\nthe switch. In this paper, we analyze where, why, and how much time is spent on\nthe critical path of communication by modeling the overall injection overhead\nand end-to-end latency of a system. We focus our analysis on the performance of\nsmall messages since fine-grained communication is becoming increasingly\nimportant with the growing trend of an increasing number of cores per node. The\nanalytical models present an accurate and detailed breakdown of time spent in\ninternode communication. We validate the models on Arm ThunderX2-based servers\nconnected with Mellanox InfiniBand. This is the first work of this kind on Arm.\nAlongside our breakdown, we describe the methodology to measure the time spent\nin each component so that readers with access to precise CPU timers and a PCIe\nanalyzer can measure breakdowns on systems of their interest. Such a breakdown\nis crucial for software developers, system architects, and researchers to guide\ntheir optimization efforts. As researchers ourselves, we use the breakdown to\nsimulate the impacts and discuss the likelihoods of a set of optimizations that\ntarget the bottlenecks in today's high-performance communication.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 00:15:49 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Zambre", "Rohit", ""], ["Grodowitz", "Megan", ""], ["Chandramowlishwaran", "Aparna", ""], ["Shamis", "Pavel", ""]]}, {"id": "2002.02567", "submitter": "Aditya Gopalan", "authors": "Aditya Gopalan, Abishek Sankararaman, Anwar Walid, Sriram Vishwanath", "title": "Stability and Scalability of Blockchain Systems", "comments": "This is the revised version of the paper", "journal-ref": "Proc. ACM Meas. Anal. Comput. Syst. Vol. 4 No. 2 (2020) Article\n  35, pages 1-35", "doi": "10.1145/3392153", "report-no": null, "categories": "cs.DC cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blockchain paradigm provides a mechanism for content dissemination and\ndistributed consensus on Peer-to-Peer (P2P) networks. While this paradigm has\nbeen widely adopted in industry, it has not been carefully analyzed in terms of\nits network scaling with respect to the number of peers. Applications for\nblockchain systems, such as cryptocurrencies and IoT, require this form of\nnetwork scaling.\n  In this paper, we propose a new stochastic network model for a blockchain\nsystem. We identify a structural property called \\emph{one-endedness}, which we\nshow to be desirable in any blockchain system as it is directly related to\ndistributed consensus among the peers. We show that the stochastic stability of\nthe network is sufficient for the one-endedness of a blockchain. We further\nestablish that our model belongs to a class of network models, called monotone\nseparable models. This allows us to establish upper and lower bounds on the\nstability region. The bounds on stability depend on the connectivity of the P2P\nnetwork through its conductance and allow us to analyze the scalability of\nblockchain systems on large P2P networks. We verify our theoretical insights\nusing both synthetic data and real data from the Bitcoin network.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 00:36:29 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 04:01:26 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 02:38:35 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 04:31:27 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gopalan", "Aditya", ""], ["Sankararaman", "Abishek", ""], ["Walid", "Anwar", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "2002.02596", "submitter": "Xiaowen Gong", "authors": "Xiaowen Gong", "title": "Delay-Optimal Distributed Edge Computing in Wireless Edge Networks", "comments": "This paper has been accepted by IEEE INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By integrating edge computing with parallel computing, distributed edge\ncomputing (DEC) makes use of distributed devices in edge networks to perform\ncomputing in parallel, which can substantially reduce service delays. In this\npaper, we explore DEC that exploits distributed edge devices connected by a\nwireless network to perform a computation task offloaded from an end device. In\nparticular, we study the fundamental problem of minimizing the delay of\nexecuting a distributed algorithm of the computation task. We first establish\nsome structural properties of the optimal communication scheduling policy.\nThen, given these properties, we characterize the optimal computation\nallocation policy, which can be found by an efficient algorithm. Next, based on\nthe optimal computation allocation, we characterize the optimal scheduling\norder of communications for some special cases, and develop an efficient\nalgorithm with a finite approximation ratio to find it for the general case.\nLast, based on the optimal computation allocation and communication scheduling,\nwe further show that the optimal selection of devices can be found efficiently\nfor some special cases. Our results provide some useful insights for the\noptimal computation-communication co-design. We evaluate the performance of the\ntheoretical findings using simulations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:45:54 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Gong", "Xiaowen", ""]]}, {"id": "2002.02641", "submitter": "Avery Miller", "authors": "Avery Miller, Andrzej Pelc, Ram Narayan Yadav", "title": "Deterministic Leader Election in Anonymous Radio Networks", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider leader election in anonymous radio networks modeled as simple\nundirected connected graphs. Nodes communicate in synchronous rounds. Nodes are\nanonymous and execute the same deterministic algorithm, so symmetry can be\nbroken only in one way: by different wake-up times of the nodes. In which\nsituations is it possible to break symmetry and elect a leader using time as\nsymmetry breaker? To answer this question, we consider configurations. A\nconfiguration is the underlying graph with nodes tagged by non-negative\nintegers with the following meaning. A node can either wake up spontaneously in\nthe round shown on its tag, according to some global clock, or can be woken up\nhearing a message sent by one of its already awoken neighbours. The local clock\nof a node starts at its wakeup and nodes do not have access to the global clock\ndetermining their tags. A configuration is feasible if there exists a\ndistributed algorithm that elects a leader for this configuration.\n  Our main result is a complete algorithmic characterization of feasible\nconfigurations: we design a centralized decision algorithm, working in\npolynomial time, whose input is a configuration and which decides if the\nconfiguration is feasible. We also provide a dedicated deterministic\ndistributed leader election algorithm for each feasible configuration that\nelects a leader for this configuration in time $O(n^2\\sigma)$, where $n$ is the\nnumber of nodes and $\\sigma$ is the difference between the largest and smallest\ntag of the configuration. We then prove that there cannot exist a universal\ndeterministic distributed algorithm electing a leader for all feasible\nconfigurations. In fact, we show that such a universal algorithm cannot exist\neven for the class of 4-node feasible configurations. We also prove that a\ndistributed version of our decision algorithm cannot exist.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 06:43:30 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Miller", "Avery", ""], ["Pelc", "Andrzej", ""], ["Yadav", "Ram Narayan", ""]]}, {"id": "2002.02762", "submitter": "Fabrizio Romano Genovese", "authors": "Fabrizio Genovese, David I. Spivak", "title": "A Categorical Semantics for Guarded Petri Nets", "comments": "24 pages (11 appendix), 13 figures", "journal-ref": "In: Gadducci F., Kehrer T. (eds) Graph Transformation. ICGT 2020.\n  Lecture Notes in Computer Science, vol 12150. Springer, Cham", "doi": "10.1007/978-3-030-51372-6_4", "report-no": null, "categories": "math.CT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on the correspondence between Petri nets and free symmetric strict\nmonoidal categories already investigated in the literature, and present a\ncategorical semantics for Petri nets with guards. This comes in two flavors:\nDeterministic and with side-effects. Using the Grothendieck construction, we\nshow how the guard semantics can be internalized in the net itself.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 13:13:52 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 16:27:47 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Genovese", "Fabrizio", ""], ["Spivak", "David I.", ""]]}, {"id": "2002.02780", "submitter": "Joshua Siegel", "authors": "Gregory Falco, Joshua E. Siegel", "title": "A Distributed `Black Box' Audit Trail Design Specification for Connected\n  and Automated Vehicle Data and Software Assurance", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automotive software is increasingly complex and critical to safe vehicle\noperation, and related embedded systems must remain up-to-date to ensure\nlong-term system performance. Update mechanisms and data modification tools\nintroduce opportunities for malicious actors to compromise these cyber-physical\nsystems, and for trusted actors to mistakenly install incompatible software\nversions. A distributed and stratified \"black box\" audit trail for automotive\nsoftware and data provenance is proposed to assure users, service providers,\nand original equipment manufacturers (OEMs) of vehicular software integrity and\nreliability. The proposed black box architecture is both layered and diffuse,\nemploying distributed hash tables (DHT), a parity system and a public\nblockchain to provide high resilience, assurance, scalability, and efficiency\nfor automotive and other high-assurance systems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 13:38:30 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:47:37 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Falco", "Gregory", ""], ["Siegel", "Joshua E.", ""]]}, {"id": "2002.02806", "submitter": "Eddy Truyen", "authors": "Eddy Truyen, Dimitri Van Landuyt, Davy Preuveneers, Bert Lagaisse,\n  Wouter Joosen", "title": "A Comprehensive Feature Comparison Study of Open-Source Container\n  Orchestration Frameworks", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  (1) Background: Container orchestration frameworks provide support for\nmanagement of complex distributed applications. Different frameworks have\nemerged only recently, and they have been in constant evolution as new features\nare being introduced. This reality makes it difficult for practitioners and\nresearchers to maintain a clear view of the technology space. (2) Methods: we\npresent a descriptive feature comparison study of the three most prominent\norchestration frameworks: Docker Swarm, Kubernetes, and Mesos, which can be\ncombined with Marathon, Aurora or DC/OS. This study aims at (i) identifying the\ncommon and unique features of all frameworks, (ii) comparing these frameworks\nqualitatively and quantitatively with respect to genericity in terms of\nsupported features, and (iii) investigating the maturity and stability of the\nframeworks as well as the pioneering nature of each framework by studying the\nhistorical evolution of the frameworks on GitHub. (3) Results: (i) we have\nidentified 124 common features and 54 unique features that we divided into a\ntaxonomy of 9 functional aspects and 27 functional sub-aspects. (ii) Kubernetes\nsupports the highest number of accumulated common and unique features for all 9\nfunctional aspects; however, no evidence has been found for significant\ndifferences in genericity with Docker Swarm and DC/OS. (iii) Very little\nfeature deprecations have been found and 15 out of 27 sub-aspects have been\nidentified as mature and stable. These are pioneered in descending order by\nKubernetes, Mesos, and Marathon. (4) Conclusion: there is a broad and mature\nfoundation that underpins all container orchestration frameworks. Likely areas\nfor further evolution and innovation include system support for improved\ncluster security and container security, performance isolation of GPU, disk and\nnetwork resources, and network plugin architectures.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 14:24:49 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:56:46 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Truyen", "Eddy", ""], ["Van Landuyt", "Dimitri", ""], ["Preuveneers", "Davy", ""], ["Lagaisse", "Bert", ""], ["Joosen", "Wouter", ""]]}, {"id": "2002.02962", "submitter": "Daniel Seemaier", "authors": "Merten Popp, Sebastian Schlag, Christian Schulz, Daniel Seemaier", "title": "Multilevel Acyclic Hypergraph Partitioning", "comments": "arXiv admin note: text overlap with arXiv:1710.01968", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed acyclic hypergraph is a generalized concept of a directed acyclic\ngraph, where each hyperedge can contain an arbitrary number of tails and heads.\nDirected hypergraphs can be used to model data flow and execution dependencies\nin streaming applications. Thus, hypergraph partitioning algorithms can be used\nto obtain efficient parallelizations for multiprocessor architectures. However,\nan acyclicity constraint on the partition is necessary when mapping streaming\napplications to embedded multiprocessors due to resource restrictions on this\ntype of hardware. The acyclic hypergraph partitioning problem is to partition\nthe hypernodes of a directed acyclic hypergraph into a given number of blocks\nof roughly equal size such that the corresponding quotient graph is acyclic\nwhile minimizing an objective function on the partition.\n  Here, we contribute the first n-level algorithm for the acyclic hypergraph\npartitioning problem. Our focus is on acyclic hypergraphs where hyperedges can\nhave one head and arbitrary many tails. Based on this, we engineer a memetic\nalgorithm to further reduce communication cost, as well as to improve\nscheduling makespan on embedded multiprocessor architectures. Experiments\nindicate that our algorithm outperforms previous algorithms that focus on the\ndirected acyclic graph case which have previously been employed in the\napplication domain. Moreover, our experiments indicate that using the directed\nhypergraph model for this type of application yields a significantly smaller\nmakespan.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:03:58 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 11:36:31 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Popp", "Merten", ""], ["Schlag", "Sebastian", ""], ["Schulz", "Christian", ""], ["Seemaier", "Daniel", ""]]}, {"id": "2002.02989", "submitter": "Georg Hager", "authors": "Ayesha Afzal and Georg Hager and Gerhard Wellein", "title": "Desynchronization and Wave Pattern Formation in MPI-Parallel and Hybrid\n  Memory-Bound Programs", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": "10.1007/978-3-030-50743-5_20", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytic, first-principles performance modeling of distributed-memory\nparallel codes is notoriously imprecise. Even for applications with extremely\nregular and homogeneous compute-communicate phases, simply adding communication\ntime to computation time does often not yield a satisfactory prediction of\nparallel runtime due to deviations from the expected simple lockstep pattern\ncaused by system noise, variations in communication time, and inherent load\nimbalance. In this paper, we highlight the specific cases of provoked and\nspontaneous desynchronization of memory-bound, bulk-synchronous pure MPI and\nhybrid MPI+OpenMP programs. Using simple microbenchmarks we observe that\nalthough desynchronization can introduce increased waiting time per process, it\ndoes not necessarily cause lower resource utilization but can lead to an\nincrease in available bandwidth per core. In case of significant communication\noverhead, even natural noise can shove the system into a state of automatic\noverlap of communication and computation, improving the overall time to\nsolution. The saturation point, i.e., the number of processes per memory domain\nrequired to achieve full memory bandwidth, is pivotal in the dynamics of this\nprocess and the emerging stable wave pattern. We also demonstrate how hybrid\nMPI-OpenMP programming can prevent desirable desynchronization by eliminating\nthe bandwidth bottleneck among processes. A Chebyshev filter diagonalization\napplication is used to demonstrate some of the observed effects in a realistic\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 19:25:17 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Afzal", "Ayesha", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2002.03068", "submitter": "Louis Jenkins", "authors": "Garvit Dewan, Louis Jenkins", "title": "Paving the way for Distributed Non-Blocking Algorithms and Data\n  Structures in the Partitioned Global Address Space", "comments": null, "journal-ref": null, "doi": "10.1109/IPDPSW50202.2020.00111", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partitioned global address space has bridged the gap between shared and\ndistributed memory, and with this bridge comes the ability to adapt shared\nmemory concepts, such as non-blocking programming, to distributed systems such\nas supercomputers. To enable non-blocking algorithms, we present ways to\nperform scalable atomic operations on objects in remote memory via remote\ndirect memory address and pointer compression. As a solution to the problem of\nconcurrent-safe reclamation of memory in a distributed system, we adapt\nEpoch-Based Memory Reclamation to distributed memory and implement it in such a\nway to support global-view programming. This construct is designed and\nimplemented for the Chapel programming language, but can be adapted and\ngeneralized to work on other languages and libraries.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 02:21:25 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 06:42:39 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 22:51:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Dewan", "Garvit", ""], ["Jenkins", "Louis", ""]]}, {"id": "2002.03087", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje", "title": "On Probabilistic Byzantine Fault Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine fault tolerance (BFT) has been extensively studied in distributed\ntrustless systems to guarantee system's functioning when up to 1/3 Byzantine\nprocesses exist. Despite a plethora of previous work in BFT systems, they are\nmainly concerned about common knowledge deducible from the states of all\nparticipant processes. In BFT systems, it is crucial to know about which\nknowledge a process knows about the states of other processes and the global\nstate of the system. However, there is a lack of studies about common knowledge\nof Byzantine faults, such as, whether existence of a Byzantine node is known by\nall honest nodes. In a dynamic setting, processes may fail or get compromised\nunexpectedly and unpredictably. It is critical to reason about which processes\nknow about the faulty processes of the network.\n  In this paper, we are interested in studying BFT systems in which Byzantine\nprocesses may misbehave randomly, possibly at some random periods of time. The\nproblem of \\emph{probabilistic Byzantine} (PB) processes studied in this paper\nis more general than the problems previously studied in existing work. We\npropose an approach that allows us to formulate and reason about the concurrent\nknowledge of the PB processes by all processes. We present our study of the\nproposed approach in both synchronous and asynchronous systems.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 04:40:48 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""]]}, {"id": "2002.03109", "submitter": "Pu Yuan", "authors": "Pu Yuan, Kan Zheng, Xiong Xiong, Kuan Zhang and Lei Lei", "title": "Performance Modeling and Analysis of a Hyperledger-based System Using\n  GSPN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highly scalable permissioned blockchain platform, Hyperledger Fabric\nsupports a wide range of industry use cases ranging from governance to finance.\nIn this paper, we propose a model to analyze the performance of a\nHyperledgerbased system by using Generalised Stochastic Petri Nets (GSPN). This\nmodel decomposes a transaction flow into multiple phases and provides a\nsimulation-based approach to obtain the system latency and throughput with a\nspecific arrival rate. Based on this model, we analyze the impact of different\nconfigurations of ordering service on system performance to find out the\nbottleneck. Moreover, a mathematical configuration selection approach is\nproposed to determine the best configuration which can maximize the system\nthroughput. Finally, extensive experiments are performed on a running system to\nvalidate the proposed model and approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 08:05:00 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Yuan", "Pu", ""], ["Zheng", "Kan", ""], ["Xiong", "Xiong", ""], ["Zhang", "Kuan", ""], ["Lei", "Lei", ""]]}, {"id": "2002.03175", "submitter": "Matteo Ceccarello", "authors": "Matteo Ceccarello, Andrea Pietracaprina, Geppino Pucci", "title": "A General Coreset-Based Approach to Diversity Maximization under Matroid\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity maximization is a fundamental problem in web search and data\nmining. For a given dataset $S$ of $n$ elements, the problem requires to\ndetermine a subset of $S$ containing $k\\ll n$ \"representatives\" which minimize\nsome diversity function expressed in terms of pairwise distances, where\ndistance models dissimilarity. An important variant of the problem prescribes\nthat the solution satisfy an additional orthogonal requirement, which can be\nspecified as a matroid constraint (i.e., a feasible solution must be an\nindependent set of size $k$ of a given matroid). While unconstrained diversity\nmaximization admits efficient coreset-based strategies for several diversity\nfunctions, known approaches dealing with the additional matroid constraint\napply only to one diversity function (sum of distances), and are based on an\nexpensive, inherently sequential, local search over the entire input dataset.\nWe devise the first coreset-based algorithms for diversity maximization under\nmatroid constraints for various diversity functions, together with efficient\nsequential, MapReduce and Streaming implementations. Technically, our\nalgorithms rely on the construction of a small coreset, that is, a subset of\n$S$ containing a feasible solution which is no more than a factor $1-\\epsilon$\naway from the optimal solution for $S$. While our algorithms are fully general,\nfor the partition and transversal matroids, if $\\epsilon$ is a constant in\n$(0,1)$ and $S$ has bounded doubling dimension, the coreset size is independent\nof $n$ and it is small enough to afford the execution of a slow sequential\nalgorithm to extract a final, accurate, solution in reasonable time. Extensive\nexperiments show that our algorithms are accurate, fast and scalable, and\ntherefore they are capable of dealing with the large input instances typical of\nthe big data scenario.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 14:46:40 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ceccarello", "Matteo", ""], ["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""]]}, {"id": "2002.03258", "submitter": "Dingwen Tao", "authors": "Cody Rivera, Jieyang Chen, Nan Xiong, Shuaiwen Leon Song, and Dingwen\n  Tao", "title": "TSM2X: High-Performance Tall-and-Skinny Matrix-Matrix Multiplication on\n  GPUs", "comments": "17 pages, 14 figures, published in JPDC", "journal-ref": null, "doi": "10.1016/j.jpdc.2021.02.013", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebra operations have been widely used in big data analytics and\nscientific computations. Many works have been done on optimizing linear algebra\noperations on GPUs with regular-shaped input. However, few works focus on fully\nutilizing GPU resources when the input is not regular-shaped. Current\noptimizations do not consider fully utilizing the memory bandwidth and\ncomputing power; therefore, they can only achieve sub-optimal performance. In\nthis paper, we propose two efficient algorithms -- TSM2R and TSM2L -- for two\nclasses of tall-and-skinny matrix-matrix multiplications on GPUs. Both of them\nfocus on optimizing linear algebra operation with at least one of the input\nmatrices is tall-and-skinny. Specifically, TSM2R is designed for a large\nregular-shaped matrix multiplying a tall-and-skinny matrix, while TSM2L is\ndesigned for a tall-and-skinny matrix multiplying a small regular-shaped\nmatrix. We implement our proposed algorithms and test on several modern NVIDIA\nGPU micro-architectures. Experiments show that, compared to the current\nstate-of-the-art works, (1) TSM2R speeds up the computation by 1.1x~3x and\nimproves the memory bandwidth utilization and computing power utilization by\n8%~47.6% and 7%~37.3%, respectively, when the regular-shaped matrix size is\nrelatively large or medium; and (2) TSM2L speeds up the computation by\n1.1x~3.5x and improve the memory bandwidth utilization by up to 55% when the\nregular-shaped matrix size is relatively small.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 00:53:35 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 05:07:00 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 17:09:09 GMT"}, {"version": "v4", "created": "Tue, 28 Jul 2020 04:07:49 GMT"}, {"version": "v5", "created": "Thu, 18 Feb 2021 07:34:19 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Rivera", "Cody", ""], ["Chen", "Jieyang", ""], ["Xiong", "Nan", ""], ["Song", "Shuaiwen Leon", ""], ["Tao", "Dingwen", ""]]}, {"id": "2002.03260", "submitter": "Tianjian Lu", "authors": "Tianjian Lu, Yi-Fan Chen, Blake Hechtman, Tao Wang, and John Anderson", "title": "Large-Scale Discrete Fourier Transform on TPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present two parallel algorithms for the large-scale discrete\nFourier transform (DFT) on Tensor Processing Unit (TPU) clusters. The two\nparallel algorithms are associated with two formulations of DFT: one is based\non the Kronecker product, to be specific, dense matrix multiplications between\nthe input data and the Vandermonde matrix, denoted as KDFT in this work; the\nother is based on the famous Cooley-Tukey algorithm and phase adjustment,\ndenoted as FFT in this work. Both KDFT and FFT formulations take full advantage\nof TPU's strength in matrix multiplications. The KDFT formulation allows direct\nuse of nonuniform inputs without additional step. In the two parallel\nalgorithms, the same strategy of data decomposition is applied to the input\ndata. Through the data decomposition, the dense matrix multiplications in KDFT\nand FFT are kept local within TPU cores, which can be performed completely in\nparallel. The communication among TPU cores is achieved through the one-shuffle\nscheme in both parallel algorithms, with which sending and receiving data takes\nplace simultaneously between two neighboring cores and along the same direction\non the interconnect network. The one-shuffle scheme is designed for the\ninterconnect topology of TPU clusters, minimizing the time required by the\ncommunication among TPU cores. Both KDFT and FFT are implemented in TensorFlow.\nThe three-dimensional complex DFT is performed on an example of dimension $8192\n\\times 8192 \\times 8192$ with a full TPU Pod: the run time of KDFT is 12.66\nseconds and that of FFT is 8.3 seconds. Scaling analysis is provided to\ndemonstrate the high parallel efficiency of the two DFT implementations on\nTPUs.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 01:15:13 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 08:15:14 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 20:55:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lu", "Tianjian", ""], ["Chen", "Yi-Fan", ""], ["Hechtman", "Blake", ""], ["Wang", "Tao", ""], ["Anderson", "John", ""]]}, {"id": "2002.03274", "submitter": "Shriram Ramesh", "authors": "Shriram Ramesh, Animesh Baranawal and Yogesh Simmhan", "title": "A Distributed Path Query Engine for Temporal Property Graphs", "comments": "An extended version of the paper that appears in IEEE/ACM\n  International Symposium on Cluster, Cloud and Internet Computing (CCGrid),\n  2020", "journal-ref": "IEEE/ACM International Symposium on Cluster, Cloud and Internet\n  Computing (CCGrid), 2020, 499-508", "doi": "10.1109/CCGrid49817.2020.00-43", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property graphs are a common form of linked data, with path queries used to\ntraverse and explore them for enterprise transactions and mining. Temporal\nproperty graphs are a recent variant where time is a first-class entity to be\nqueried over, and their properties and structure vary over time. These are seen\nin social, telecom, transit and epidemic networks. However, current graph\ndatabases and query engines have limited support for temporal relations among\ngraph entities, no support for time-varying entities and/or do not scale on\ndistributed resources. We address this gap by extending a linear path query\nmodel over property graphs to include intuitive temporal predicates and\naggregation operators over temporal graphs. We design a distributed execution\nmodel for these temporal path queries using the interval-centric computing\nmodel, and develop a novel cost model to select an efficient execution plan\nfrom several. We perform detailed experiments of our Granite distributed query\nengine using both static and dynamic temporal property graphs as large as 52M\nvertices, 218M edges and 325M properties, and a 1600-query workload, derived\nfrom the LDBC benchmark. We often offer sub-second query latencies on a\ncommodity cluster, which is 149x-1140x faster compared to industry-leading\nNeo4J shared-memory graph database and the JanusGraph / Spark distributed graph\nquery engine. Granite also completes 100% of the queries for all graphs,\ncompared to only 32-92% workload completion by the baseline systems. Further,\nour cost model selects a query plan that is within 10% of the optimal execution\ntime in 90% of the cases. Despite the irregular nature of graph processing, we\nexhibit a weak-scaling efficiency >= 60% on 8 nodes and >= 40% on 16 nodes, for\nmost query workloads.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 03:41:25 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 14:35:23 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ramesh", "Shriram", ""], ["Baranawal", "Animesh", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "2002.03344", "submitter": "Georg Hager", "authors": "Christie L. Alappat and Johannes Hofmann and Georg Hager and Holger\n  Fehske and Alan R. Bishop and Gerhard Wellein", "title": "Understanding HPC Benchmark Performance on Intel Broadwell and Cascade\n  Lake Processors", "comments": "19 pages, 9 figures, 3 tables. Corrected affiliations and\n  acknowledgments", "journal-ref": null, "doi": "10.1007/978-3-030-50743-5_21", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware platforms in high performance computing are constantly getting more\ncomplex to handle even when considering multicore CPUs alone. Numerous features\nand configuration options in the hardware and the software environment that are\nrelevant for performance are not even known to most application users or\ndevelopers. Microbenchmarks, i.e., simple codes that fathom a particular aspect\nof the hardware, can help to shed light on such issues, but only if they are\nwell understood and if the results can be reconciled with known facts or\nperformance models. The insight gained from microbenchmarks may then be applied\nto real applications for performance analysis or optimization. In this paper we\ninvestigate two modern Intel x86 server CPU architectures in depth: Broadwell\nEP and Cascade Lake SP. We highlight relevant hardware configuration settings\nthat can have a decisive impact on code performance and show how to properly\nmeasure on-chip and off-chip data transfer bandwidths. The new victim L3 cache\nof Cascade Lake and its advanced replacement policy receive due attention.\nFinally we use DGEMM, sparse matrix-vector multiplication, and the HPCG\nbenchmark to make a connection to relevant application scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 11:25:21 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 09:53:06 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Alappat", "Christie L.", ""], ["Hofmann", "Johannes", ""], ["Hager", "Georg", ""], ["Fehske", "Holger", ""], ["Bishop", "Alan R.", ""], ["Wellein", "Gerhard", ""]]}, {"id": "2002.03384", "submitter": "Giuseppe Antonio Di Luna", "authors": "Giuseppe Antonio Di Luna, Paola Flocchini, Nicola Santoro, Giovanni\n  Viglietta, Yukiko Yamauchi", "title": "Mobile RAM and Shape Formation by Programmable Particles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate computational issues in the distributed model Amoebots of\nprogrammable matter. In this model, the computational entities, called\nparticles, are anonymous finite-state machines that operate and move on an\nhexagonal tasselation of the plane. In this paper we show how a constant number\nof such weak particles can simulate a powerful Turing-complete entity that is\nable to move on the plane while computing. We then show an application of our\ntool to the classical Shape-Formation problem, providing a new and much more\ngeneral distributed solution protocol. Indeed, the existing algorithms would\nallow to form only shapes made of arrangements of segments and triangles. Our\nalgorithm allows the particles to form more abstract and general connected\nshapes, including circles and spirals, as well as fractal objects of\nnon-integer dimension, such as the Sierpinski triangle or the Koch snowflake.\nIn lieu of the existing limitation on the formability of a shape depending on\nthe symmetry of the initial configuration of the particles, our result provides\na complete characterization of the connected shapes that can be formed by an\ninitially simply connected set of particles. Furthermore, in the case of\nnon-connected shapes, we give almost-matching necessary and sufficient\nconditions for their formability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 15:41:59 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 08:20:28 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Di Luna", "Giuseppe Antonio", ""], ["Flocchini", "Paola", ""], ["Santoro", "Nicola", ""], ["Viglietta", "Giovanni", ""], ["Yamauchi", "Yukiko", ""]]}, {"id": "2002.03437", "submitter": "Jonathan Katz", "authors": "Erica Blum, Jonathan Katz, Julian Loss", "title": "Network-Agnostic State Machine Replication", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of state machine replication (SMR)---the underlying\nproblem addressed by blockchain protocols---in the presence of a malicious\nadversary who can corrupt some fraction of the parties running the protocol.\nExisting protocols for this task assume either a synchronous network (where all\nmessages are delivered within some known time $\\Delta$) or an asynchronous\nnetwork (where messages can be delayed arbitrarily). Although protocols for the\nlatter case give seemingly stronger guarantees, this is not the case since they\n(inherently) tolerate a lower fraction of corrupted parties.\n  We design an SMR protocol that is network-agnostic in the following sense: if\nit is run in a synchronous network, it tolerates $t_s$ corrupted parties; if\nthe network happens to be asynchronous it is resilient to $t_a \\leq t_s$\nfaults. Our protocol achieves optimal tradeoffs between $t_s$ and $t_a$.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 19:42:57 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:53:05 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 19:18:25 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Blum", "Erica", ""], ["Katz", "Jonathan", ""], ["Loss", "Julian", ""]]}, {"id": "2002.03493", "submitter": "Tianshu Hao", "authors": "Tianshu Hao, Jianfeng Zhan, Kai Hwang, Wanling Gao, Xu Wen", "title": "AI-oriented Medical Workload Allocation for Hierarchical\n  Cloud/Edge/Device Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a hierarchically-structured cloud/edge/device computing environment,\nworkload allocation can greatly affect the overall system performance. This\npaper deals with AI-oriented medical workload generated in emergency rooms (ER)\nor intensive care units (ICU) in metropolitan areas. The goal is to optimize\nAI-workload allocation to cloud clusters, edge servers, and end devices so that\nminimum response time can be achieved in life-saving emergency applications.\n  In particular, we developed a new workload allocation method for the AI\nworkload in distributed cloud/edge/device computing systems. An efficient\nscheduling and allocation strategy is developed in order to reduce the overall\nresponse time to satisfy multi-patient demands. We apply several ICU AI\nworkloads from a comprehensive edge computing benchmark Edge AIBench. The\nhealthcare AI applications involved are short-of-breath alerts, patient\nphenotype classification, and life-death threats. Our experimental results\ndemonstrate the high efficiency and effectiveness in real-life health-care and\nemergency applications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 01:32:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Hao", "Tianshu", ""], ["Zhan", "Jianfeng", ""], ["Hwang", "Kai", ""], ["Gao", "Wanling", ""], ["Wen", "Xu", ""]]}, {"id": "2002.03613", "submitter": "Henrique Moniz", "authors": "Henrique Moniz", "title": "The Istanbul BFT Consensus Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents IBFT, a simple and elegant Byzantine fault-tolerant\nconsensus algorithm that is used to implement state machine replication in the\n\\emph{Quorum} blockchain. IBFT assumes a partially synchronous communication\nmodel, where safety does not depend on any timing assumptions and only liveness\ndepends on periods of synchrony. The algorithm is deterministic, leader-based,\nand optimally resilient - tolerating $f$ faulty processes out of $n$, where $n\n\\geq 3f+1$. During periods of good communication, IBFT achieves termination in\nthree message delays and has $O(n^2)$ total communication complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 09:36:39 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 09:09:29 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Moniz", "Henrique", ""]]}, {"id": "2002.03703", "submitter": "Shihua Zhang", "authors": "Chihao Zhang and Yang Yang and Wei Zhang and Shihua Zhang", "title": "Distributed Bayesian Matrix Decomposition for Big Data Mining and\n  Clustering", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix decomposition is one of the fundamental tools to discover knowledge\nfrom big data generated by modern applications. However, it is still\ninefficient or infeasible to process very big data using such a method in a\nsingle machine. Moreover, big data are often distributedly collected and stored\non different machines. Thus, such data generally bear strong heterogeneous\nnoise. It is essential and useful to develop distributed matrix decomposition\nfor big data analytics. Such a method should scale up well, model the\nheterogeneous noise, and address the communication issue in a distributed\nsystem. To this end, we propose a distributed Bayesian matrix decomposition\nmodel (DBMD) for big data mining and clustering. Specifically, we adopt three\nstrategies to implement the distributed computing including 1) the accelerated\ngradient descent, 2) the alternating direction method of multipliers (ADMM),\nand 3) the statistical inference. We investigate the theoretical convergence\nbehaviors of these algorithms. To address the heterogeneity of the noise, we\npropose an optimal plug-in weighted average that reduces the variance of the\nestimation. Synthetic experiments validate our theoretical results, and\nreal-world experiments show that our algorithms scale up well to big data and\nachieves superior or competing performance compared to other distributed\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:10:53 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zhang", "Chihao", ""], ["Yang", "Yang", ""], ["Zhang", "Wei", ""], ["Zhang", "Shihua", ""]]}, {"id": "2002.03757", "submitter": "Shao-Bo Lin", "authors": "Shao-Bo Lin", "title": "Distributed Learning with Dependent Samples", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on learning rate analysis of distributed kernel ridge\nregression for strong mixing sequences. Using a recently developed integral\noperator approach and a classical covariance inequality for Banach-valued\nstrong mixing sequences, we succeed in deriving optimal learning rate for\ndistributed kernel ridge regression. As a byproduct, we also deduce a\nsufficient condition for the mixing property to guarantee the optimal learning\nrates for kernel ridge regression. Our results extend the applicable range of\ndistributed learning from i.i.d. samples to non-i.i.d. sequences.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:03:45 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Lin", "Shao-Bo", ""]]}, {"id": "2002.03794", "submitter": "Mingzhen Li", "authors": "Mingzhen Li, Yi Liu, Xiaoyan Liu, Qingxiao Sun, Xin You, Hailong Yang,\n  Zhongzhi Luan, Lin Gan, Guangwen Yang, Depei Qian", "title": "The Deep Learning Compiler: A Comprehensive Survey", "comments": null, "journal-ref": "IEEE Transactions on Parallel & Distributed Systems, vol. 32, no.\n  03, pp. 708-727, 2021", "doi": "10.1109/TPDS.2020.3030548", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of deploying various deep learning (DL) models on diverse DL\nhardware has boosted the research and development of DL compilers in the\ncommunity. Several DL compilers have been proposed from both industry and\nacademia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the\nDL models described in different DL frameworks as input, and then generate\noptimized codes for diverse DL hardware as output. However, none of the\nexisting survey has analyzed the unique design architecture of the DL compilers\ncomprehensively. In this paper, we perform a comprehensive survey of existing\nDL compilers by dissecting the commonly adopted design in details, with\nemphasis on the DL oriented multi-level IRs, and frontend/backend\noptimizations. Specifically, we provide a comprehensive comparison among\nexisting DL compilers from various aspects. In addition, we present detailed\nanalysis on the design of multi-level IRs and illustrate the commonly adopted\noptimization techniques. Finally, several insights are highlighted as the\npotential research directions of DL compiler. This is the first survey paper\nfocusing on the design architecture of DL compilers, which we hope can pave the\nroad for future research towards DL compiler.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 07:29:08 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 05:08:44 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 10:34:01 GMT"}, {"version": "v4", "created": "Fri, 28 Aug 2020 09:19:43 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Li", "Mingzhen", ""], ["Liu", "Yi", ""], ["Liu", "Xiaoyan", ""], ["Sun", "Qingxiao", ""], ["You", "Xin", ""], ["Yang", "Hailong", ""], ["Luan", "Zhongzhi", ""], ["Gan", "Lin", ""], ["Yang", "Guangwen", ""], ["Qian", "Depei", ""]]}, {"id": "2002.03805", "submitter": "Francisco Carpio", "authors": "Francisco Carpio, Marta Delgado and Admela Jukan", "title": "Engineering and Experimentally Benchmarking a Container-based Edge\n  Computing System", "comments": null, "journal-ref": null, "doi": "10.1109/ICC40277.2020.9148636", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While edge computing is envisioned to superbly serve latency sensitive\napplications, the implementation-based studies benchmarking its performance are\nfew and far between. To address this gap, we engineer a modular edge cloud\ncomputing system architecture that is built on latest advances in\ncontainerization techniques, including Kafka, for data streaming, Docker, as\napplication platform, and Firebase Cloud, as realtime database system. We\nbenchmark the performance of the system in terms of scalability, resource\nutilization and latency by comparing three scenarios: cloud-only, edge-only and\ncombined edge-cloud. The measurements show that edge-only solution outperforms\nother scenarios only when deployed with data located at one edge only, i.e.,\nwithout edge computing wide data synchronization. In case of applications\nrequiring data synchronization through the cloud, edge-cloud scales around a\nfactor 10 times better than cloud-only, until certain number of concurrent\nusers in the system, and above this point, cloud-only scales better. In terms\nof resource utilization, we observe that whereas the mean utilization increases\nlinearly with the number of user requests, the maximum values for the memory\nand the network I/O heavily increase when with an increasing amount of data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:38:24 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Carpio", "Francisco", ""], ["Delgado", "Marta", ""], ["Jukan", "Admela", ""]]}, {"id": "2002.03850", "submitter": "Rohit Zambre", "authors": "Rohit Zambre, Lars Bergstrom, Laleh Aghababaie Beni, Aparna\n  Chandramowliswharan", "title": "Parallel Performance-Energy Predictive Modeling of Browsers: Case Study\n  of Servo", "comments": "In Proceedings of the 23rd IEEE International Conference on High\n  Performance Computing, Data, and Analytics (HiPC), Hyderabad, India, December\n  2016", "journal-ref": null, "doi": "10.1109/HiPC.2016.013", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mozilla Research is developing Servo, a parallel web browser engine, to\nexploit the benefits of parallelism and concurrency in the web rendering\npipeline. Parallelization results in improved performance for pinterest.com but\nnot for google.com. This is because the workload of a browser is dependent on\nthe web page it is rendering. In many cases, the overhead of creating,\ndeleting, and coordinating parallel work outweighs any of its benefits. In this\npaper, we model the relationship between web page primitives and a web\nbrowser's parallel performance using supervised learning. We discover a feature\nspace that is representative of the parallelism available in a web page and\ncharacterize it using seven key features. Additionally, we consider energy\nusage trade-offs for different levels of performance improvements using\nautomated labeling algorithms. Such a model allows us to predict the degree of\nparallelism available in a web page and decide whether or not to render a web\npage in parallel. This modeling is critical for improving the browser's\nperformance and minimizing its energy usage. We evaluate our model by using\nServo's layout stage as a case study. Experiments on a quad-core Intel Ivy\nBridge (i7-3615QM) laptop show that we can improve performance and energy usage\nby up to 94.52% and 46.32% respectively on the 535 web pages considered in this\nstudy. Looking forward, we identify opportunities to apply this model to other\nstages of a browser's architecture as well as other performance- and\nenergy-critical devices.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 20:16:14 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Zambre", "Rohit", ""], ["Bergstrom", "Lars", ""], ["Beni", "Laleh Aghababaie", ""], ["Chandramowliswharan", "Aparna", ""]]}, {"id": "2002.04013", "submitter": "Max Ryabinin", "authors": "Max Ryabinin, Anton Gusev", "title": "Towards Crowdsourced Training of Large Neural Networks using\n  Decentralized Mixture-of-Experts", "comments": "Advances in Neural Information Processing Systems, 2020. Code URL:\n  https://github.com/mryab/learning-at-home. 16 pages, 6 figures", "journal-ref": "Advances in Neural Information Processing Systems 33 (2020)\n  3659-3672", "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent breakthroughs in deep learning were achieved by training\nincreasingly larger models on massive datasets. However, training such models\ncan be prohibitively expensive. For instance, the cluster used to train GPT-3\ncosts over \\$250 million. As a result, most researchers cannot afford to train\nstate of the art models and contribute to their development. Hypothetically, a\nresearcher could crowdsource the training of large neural networks with\nthousands of regular PCs provided by volunteers. The raw computing power of a\nhundred thousand \\$2500 desktops dwarfs that of a \\$250M server pod, but one\ncannot utilize that power efficiently with conventional distributed training\nmethods. In this work, we propose Learning@home: a novel neural network\ntraining paradigm designed to handle large amounts of poorly connected\nparticipants. We analyze the performance, reliability, and architectural\nconstraints of this paradigm and compare it against existing distributed\ntraining techniques.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 18:39:25 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 15:15:44 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 16:36:55 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ryabinin", "Max", ""], ["Gusev", "Anton", ""]]}, {"id": "2002.04047", "submitter": "Hussain Al-Aqrabi", "authors": "Hussain Al-Aqrabi, Lu Liu, Richard Hill, Nick Antonopoulos", "title": "A Multi-layer hierarchical inter-cloud connectivity model for sequential\n  packet inspection of tenant sessions accessing BI as a service", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business Intelligence (BI) has gained a new lease of life through Cloud\ncomputing as its demand for unlimited hardware and platform resources\nexpandability is fulfilled by the Cloud elasticity features. BI can be\nseamlessly deployed on the Cloud given that its multilayered model coincides\nwith the Cloud multilayer models. It is considered by many Cloud service\nproviders as one of the prominent applications services on public, outsourced\nprivate and outsourced community Clouds. However, in the shared domains of\nCloud computing, BI is exposed to security and privacy threats by virtue of\nexploits, eavesdropping, distributed attacks, malware attacks, and such other\nknown challenges on Cloud computing. Given the multi-layered model of BI and\nCloud computing, its protection on Cloud computing needs to be ensured through\nmultilayered controls. In this paper, a multi-layered security and privacy\nmodel of BI as a service on Cloud computing is proposed through an algorithm\nfor ensuring multi-level session inspections, and ensuring maximum security\ncontrols at all the seven layers, and prevent an attack from occurring. This\nwill not only reduce the risk of security breaches, but allow an organisation\ntime to detect, and respond to an attack. The simulations present the effects\nof distributed attacks on the BI systems by attackers posing as genuine Cloud\ntenants. The results reflect how the attackers are blocked by the multilayered\nsecurity and privacy controls deployed for protecting the BI servers and\ndatabases\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:01:05 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Al-Aqrabi", "Hussain", ""], ["Liu", "Lu", ""], ["Hill", "Richard", ""], ["Antonopoulos", "Nick", ""]]}, {"id": "2002.04077", "submitter": "Joshua Lawrence Benjamin", "authors": "Joshua L. Benjamin, Thomas Gerard, Domani\\c{c} Lavery, Polina Bayvel\n  and Georgios Zervas", "title": "PULSE: Optical circuit switched Data Center architecture operating at\n  nanosecond timescales", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PULSE, a sub-microsecond optical circuit-switched data centre\nnetwork architecture controlled by distributed hardware schedulers. PULSE is a\nflat architecture that uses parallel passive coupler-based broadcast and select\nnetworks. We employ a novel transceiver architecture, for dynamic\nwavelength-timeslot selection, to achieve a reconfiguration time down to\nO(100ps), establishing timeslots of O(10ns). A novel scheduling algorithm that\nhas a clock period of 2.3ns performs multiple iterations to maximize\nthroughput, wavelength usage and reduce latency, enhancing the overall\nperformance. In order to scale, the single-hop PULSE architecture uses\nsub-networks that are disjoint by using multiple transceivers for each node in\n64 node racks. At the reconfiguration circuit duration (epoch = 120 ns), the\nscheduling algorithm is shown to achieve up to 93% throughput and 100%\nwavelength usage of 64 wavelengths, incurring an average latency that ranges\nfrom 0.7-1.2 microseconds with best-case 0.4 microsecond median and 5\nmicrosecond tail latency, limited by the timeslot (20 ns) and epoch size (120\nns). We show how the 4096-node PULSE architecture allows up to 260k optical\nchannels to be re-used across sub-networks achieving a capacity of 25.6 Pbps\nwith an energy consumption of 85 pJ/bit.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:34:20 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 09:32:08 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Benjamin", "Joshua L.", ""], ["Gerard", "Thomas", ""], ["Lavery", "Domani\u00e7", ""], ["Bayvel", "Polina", ""], ["Zervas", "Georgios", ""]]}, {"id": "2002.04156", "submitter": "Jinhyun So", "authors": "Jinhyun So, Basak Guler, and A. Salman Avestimehr", "title": "Turbo-Aggregate: Breaking the Quadratic Aggregation Barrier in Secure\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed framework for training machine learning\nmodels over the data residing at mobile devices, while protecting the privacy\nof individual users. A major bottleneck in scaling federated learning to a\nlarge number of users is the overhead of secure model aggregation across many\nusers. In particular, the overhead of the state-of-the-art protocols for secure\nmodel aggregation grows quadratically with the number of users. In this paper,\nwe propose the first secure aggregation framework, named Turbo-Aggregate, that\nin a network with $N$ users achieves a secure aggregation overhead of\n$O(N\\log{N})$, as opposed to $O(N^2)$, while tolerating up to a user dropout\nrate of $50\\%$. Turbo-Aggregate employs a multi-group circular strategy for\nefficient model aggregation, and leverages additive secret sharing and novel\ncoding techniques for injecting aggregation redundancy in order to handle user\ndropouts while guaranteeing user privacy. We experimentally demonstrate that\nTurbo-Aggregate achieves a total running time that grows almost linear in the\nnumber of users, and provides up to $40\\times$ speedup over the\nstate-of-the-art protocols with up to $N=200$ users. Our experiments also\ndemonstrate the impact of model size and bandwidth on the performance of\nTurbo-Aggregate.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 01:15:41 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 16:52:26 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 20:20:49 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["So", "Jinhyun", ""], ["Guler", "Basak", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2002.04393", "submitter": "Tyler Crain", "authors": "Tyler Crain", "title": "A Simple and Efficient Asynchronous Randomized Binary Byzantine\n  Consensus Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.07867", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simple and efficient asynchronous Binary Byzantine\nfaulty tolerant consensus algorithm. In the algorithm, non-faulty nodes perform\nan initial broadcast followed by a executing a series of rounds each consisting\nof a single message broadcast plus the computation of a global random coin\nusing threshold signatures. Each message is accompanied by a cryptographic\nproof of its validity. Up to one third of the nodes can be faulty and\ntermination is expected in a constant number of rounds. An optimization is\ndescribed allowing the round message plus the coin message to be combined,\nreducing rounds to a single message delay. Geodistributed experiments are run\non replicates in ten data center regions showing average latencies as low as\n400 milliseconds.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 07:21:23 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Crain", "Tyler", ""]]}, {"id": "2002.04436", "submitter": "Ahmad Slo", "authors": "Ahmad Slo, Sukanya Bhowmik, Albert Flaig, Kurt Rothermel", "title": "pSPICE: Partial Match Shedding for Complex Event Processing", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing (CEP) systems continuously process input event\nstreams to detect patterns. Over time, the input event rate might fluctuate and\novershoot the system's capabilities. One way to reduce the overload on the\nsystem is to use load shedding. In this paper, we propose a load shedding\nstrategy for CEP systems which drops a portion of the CEP operator's internal\nstate (a.k.a. partial matches) to maintain a given latency bound. The crucial\nquestion here is how many and which partial matches to drop so that a given\nlatency bound is maintained while minimizing the degradation in the quality of\nresults. In the stream processing domain, different load shedding strategies\nhave been proposed that mainly depend on the importance of individual tuples.\nHowever, as CEP systems perform pattern detection, the importance of events is\nalso influenced by other events in the stream. Our load shedding strategy uses\nMarkov chain and Markov reward process to predict the utility/importance of\npartial matches to determine the ones to be dropped. In addition, we represent\nthe utility in a way that minimizes the overhead of load shedding. Furthermore,\nwe provide algorithms to decide when to start dropping partial matches and how\nmany partial matches to drop. By extensively evaluating our approach on three\nreal-world datasets and several representative queries, we show that the\nadverse impact of our load shedding strategy on the quality of results is\nconsiderably less than the impact of state-of-the-art load shedding strategies.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 14:50:18 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Slo", "Ahmad", ""], ["Bhowmik", "Sukanya", ""], ["Flaig", "Albert", ""], ["Rothermel", "Kurt", ""]]}, {"id": "2002.04533", "submitter": "Haoqian Zhang", "authors": "Haoqian Zhang, Yancheng Zhao, Abhishek Paryani, Ke Yi", "title": "Infnote: A Decentralized Information Sharing Platform Based on\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet censorship has been implemented in several countries to prevent\ncitizens from accessing information and to suppress discussion of specific\ntopics. This paper presents Infnote, a platform that helps eliminate the\nproblem of sharing content in these censorship regimes. Infnote is a\ndecentralized information sharing system based on blockchain and peer-to-peer\nnetwork, aiming to provide an easy-to-use medium for users to share their\nthoughts, insights and views freely without worrying about data tampering and\ndata loss. Infnote provides a solution that is able to work on any level of\nInternet censorship. Infnote uses multi-chains architecture to support various\nindependent applications or different functions in an application.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 16:35:40 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Haoqian", ""], ["Zhao", "Yancheng", ""], ["Paryani", "Abhishek", ""], ["Yi", "Ke", ""]]}, {"id": "2002.04561", "submitter": "Roland Lei{\\ss}a", "authors": "Andr\\'e M\\\"uller (1), Bertil Schmidt (1), Andreas Hildebrandt (1),\n  Richard Membarth (2 and 3), Roland Lei{\\ss}a (3), Matthis Kruse (3),\n  Sebastian Hack (3) ((1) Johannes Gutenberg University, (2) DFKI, (3) Saarland\n  University)", "title": "AnySeq: A High Performance Sequence Alignment Library based on Partial\n  Evaluation", "comments": "To be published in IPDPS 2020. This work is supported by the Federal\n  Ministry of Education and Research (BMBF) as part of the MetaDL, Metacca, and\n  ProThOS projects as well as by the Intel Visual Computing Institute (IVCI)\n  and Cluster of Excellence on Multimodal Computing and Interaction (MMCI) at\n  Saarland University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence alignments are fundamental to bioinformatics which has resulted in a\nvariety of optimized implementations. Unfortunately, the vast majority of them\nare hand-tuned and specific to certain architectures and execution models. This\nnot only makes them challenging to understand and extend, but also difficult to\nport to other platforms. We present AnySeq - a novel library for computing\ndifferent types of pairwise alignments of DNA sequences. Our approach combines\nhigh performance with an intuitively understandable implementation, which is\nachieved through the concept of partial evaluation. Using the AnyDSL compiler\nframework, AnySeq enables the compilation of algorithmic variants that are\nhighly optimized for specific usage scenarios and hardware targets with a\nsingle, uniform codebase. The resulting domain-specific library thus allows the\nvariation of alignment parameters (such as alignment type, scoring scheme, and\ntraceback vs.~plain score) by simple function composition rather than\nmetaprogramming techniques which are often hard to understand. Our\nimplementation supports multithreading and SIMD vectorization on CPUs,\nCUDA-enabled GPUs, and FPGAs. AnySeq is at most 7% slower and in many cases\nfaster (up to 12%) than state-of-the art manually optimized alignment libraries\non CPUs (SeqAn) and on GPUs (NVBio).\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 17:34:12 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["M\u00fcller", "Andr\u00e9", "", "2 and 3"], ["Schmidt", "Bertil", "", "2 and 3"], ["Hildebrandt", "Andreas", "", "2 and 3"], ["Membarth", "Richard", "", "2 and 3"], ["Lei\u00dfa", "Roland", ""], ["Kruse", "Matthis", ""], ["Hack", "Sebastian", ""]]}, {"id": "2002.04568", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, John Graham and Frank Wuerthwein", "title": "Characterizing network paths in and out of the clouds", "comments": "7 pages, 1 figure, 5 tables, to be published in CHEP19 proceedings", "journal-ref": "EPJ Web of Conferences 245, 07059 (2020)", "doi": "10.1051/epjconf/202024507059", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Commercial Cloud computing is becoming mainstream, with funding agencies\nmoving beyond prototyping and starting to fund production campaigns, too. An\nimportant aspect of any scientific computing production campaign is data\nmovement, both incoming and outgoing. And while the performance and cost of VMs\nis relatively well understood, the network performance and cost is not. This\npaper provides a characterization of networking in various regions of Amazon\nWeb Services, Microsoft Azure and Google Cloud Platform, both between Cloud\nresources and major DTNs in the Pacific Research Platform, including OSG data\nfederation caches in the network backbone, and inside the clouds themselves.\nThe paper contains both a qualitative analysis of the results as well as\nlatency and throughput measurements. It also includes an analysis of the costs\ninvolved with Cloud-based networking.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 17:47:06 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Sfiligoi", "Igor", ""], ["Graham", "John", ""], ["Wuerthwein", "Frank", ""]]}, {"id": "2002.04597", "submitter": "Zheng Dong", "authors": "Zheng Dong, Yan Lu, Guangmo Tong, Yuanchao Shu, Shuai Wang, Weisong\n  Shi", "title": "WatchDog: Real-time Vehicle Tracking on Geo-distributed Edge Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle tracking, a core application to smart city video analytics, is\nbecoming more widely deployed than ever before thanks to the increasing number\nof traffic cameras and recent advances of computer vision and machine learning.\nDue to the constraints of bandwidth, latency, and privacy concerns, tracking\ntasks are more preferable to run on edge devices sitting close to the cameras.\nHowever, edge devices are provisioned with a fixed amount of compute budget,\nmaking them incompetent to adapt to time-varying tracking workloads caused by\ntraffic dynamics. In coping with this challenge, we propose WatchDog, a\nreal-time vehicle tracking system fully utilizes edge nodes across the road\nnetwork. WatchDog leverages computer vision tasks with different\nresource-accuracy trade-offs, and decompose and schedule tracking tasks\njudiciously across edge devices based on the current workload to maximize the\nnumber of tasks while ensuring a provable response time bound at each edge\ndevice. Extensive evaluations have been conducted using real-world city-wide\nvehicle trajectory datasets, showing a 100% tracking coverage with real-time\nguarantee.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:41:57 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Dong", "Zheng", ""], ["Lu", "Yan", ""], ["Tong", "Guangmo", ""], ["Shu", "Yuanchao", ""], ["Wang", "Shuai", ""], ["Shi", "Weisong", ""]]}, {"id": "2002.04638", "submitter": "Hung Pham", "authors": "Duc Hung Pham, Krishna V. Palem, M. V. Panduranga Rao", "title": "A polynomial time parallel algorithm for graph isomorphism using a\n  quasipolynomial number of processors", "comments": "ICALP conference submission preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Graph Isomorphism (GI) problem is a theoretically interesting problem\nbecause it has not been proven to be in P nor to be NP-complete. Babai made a\nbreakthrough in 2015 when announcing a quasipolynomial time algorithm for GI\nproblem. Babai's work gives the most theoretically efficient algorithm for GI,\nas well as a strong evidence favoring the idea that class GI $\\ne$ NP and thus\nP $\\ne$ NP. Based on Babai's algorithm, we prove that GI can further be solved\nby a parallel algorithm that runs in polynomial time using a quasipolynomial\nnumber of processors. We achieve that result by identifying the bottlenecks in\nBabai's algorithms and parallelizing them. In particular, we prove that color\nrefinement can be computed in parallel logarithmic time using a polynomial\nnumber of processors, and the $k$-dimensional WL refinement can be computed in\nparallel polynomial time using a quasipolynomial number of processors. Our work\nsuggests that Graph Isomorphism and GI-complete problems can be computed\nefficiently in a parallel computer, and provides insights on speeding up\nparallel GI programs in practice.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:06:29 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Pham", "Duc Hung", ""], ["Palem", "Krishna V.", ""], ["Rao", "M. V. Panduranga", ""]]}, {"id": "2002.04758", "submitter": "Tao Yu", "authors": "Tao Yu, Eugene Bagdasaryan, Vitaly Shmatikov", "title": "Salvaging Federated Learning by Local Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a heavily promoted approach for training ML models\non sensitive data, e.g., text typed by users on their smartphones. FL is\nexpressly designed for training on data that are unbalanced and non-iid across\nthe participants. To ensure privacy and integrity of the federated model,\nlatest FL approaches use differential privacy or robust aggregation to limit\nthe influence of \"outlier\" participants.\n  First, we show that on standard tasks such as next-word prediction, many\nparticipants gain no benefit from FL because the federated model is less\naccurate on their data than the models they can train locally on their own.\nSecond, we show that differential privacy and robust aggregation make this\nproblem worse by further destroying the accuracy of the federated model for\nmany participants.\n  Then, we evaluate three techniques for local adaptation of federated models:\nfine-tuning, multi-task learning, and knowledge distillation. We analyze where\neach technique is applicable and demonstrate that all participants benefit from\nlocal adaptation. Participants whose local models are poor obtain big accuracy\nimprovements over conventional FL. Participants whose local models are better\nthan the federated model and who have no incentive to participate in FL today\nimprove less, but sufficiently to make the adapted federated model better than\ntheir local models.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 01:56:16 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Yu", "Tao", ""], ["Bagdasaryan", "Eugene", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "2002.04860", "submitter": "Minxian Xu", "authors": "Qiheng Zhou, Minxian Xu, Sukhpal Singh Gill, Chengxi Gao, Wenhong\n  Tian, Chengzhong Xu and Rajkumar Buyya", "title": "Energy Efficient Algorithms based on VM Consolidation for Cloud\n  Computing: Comparisons and Evaluations", "comments": "In the Proceedings of the 20th IEEE/ACM International Symposium on\n  Cluster, Cloud and Grid Computing (CCGRID 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing paradigm has revolutionized IT industry and be able to offer\ncomputing as the fifth utility. With the pay-as-you-go model, cloud computing\nenables to offer the resources dynamically for customers anytime. Drawing the\nattention from both academia and industry, cloud computing is viewed as one of\nthe backbones of the modern economy. However, the high energy consumption of\ncloud data centers contributes to high operational costs and carbon emission to\nthe environment. Therefore, Green cloud computing is required to ensure energy\nefficiency and sustainability, which can be achieved via energy efficient\ntechniques. One of the dominant approaches is to apply energy efficient\nalgorithms to optimize resource usage and energy consumption. Currently,\nvarious virtual machine consolidation-based energy efficient algorithms have\nbeen proposed to reduce the energy of cloud computing environment. However,\nmost of them are not compared comprehensively under the same scenario, and\ntheir performance is not evaluated with the same experimental settings. This\nmakes users hard to select the appropriate algorithm for their objectives. To\nprovide insights for existing energy efficient algorithms and help researchers\nto choose the most suitable algorithm, in this paper, we compare several\nstate-of-the-art energy efficient algorithms in depth from multiple\nperspectives, including architecture, modelling and metrics. In addition, we\nalso implement and evaluate these algorithms with the same experimental\nsettings in CloudSim toolkit. The experimental results show the performance\ncomparison of these algorithms with comprehensive results. Finally, detailed\ndiscussions of these algorithms are provided.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:18:01 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Zhou", "Qiheng", ""], ["Xu", "Minxian", ""], ["Gill", "Sukhpal Singh", ""], ["Gao", "Chengxi", ""], ["Tian", "Wenhong", ""], ["Xu", "Chengzhong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2002.04894", "submitter": "Stefan Engblom", "authors": "Jonathan Bull and Stefan Engblom", "title": "Distributed and Adaptive Fast Multipole Method In Three Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general distributed implementation of an adaptive fast multipole\nmethod in three space dimensions. We rely on a balanced type of adaptive space\ndiscretisation which supports a highly transparent and fully distributed\nimplementation. A complexity analysis indicates favorable scaling properties\nand numerical experiments on up to 512 cores and 1 billion source points verify\nthem. The parameters controlling the algorithm are subject to in-depth\nexperiments and the performance response to the input parameters implies that\nthe overall implementation is well-suited to automated tuning.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:06:58 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bull", "Jonathan", ""], ["Engblom", "Stefan", ""]]}, {"id": "2002.04896", "submitter": "Vivek Gavane", "authors": "Vivek Gavane, Supriya Prabhugawankar, Shivam Garg, Archana Achalere,\n  and Rajendra Joshi", "title": "CROFT: A scalable three-dimensional parallel Fast Fourier Transform\n  (FFT) implementation for High Performance Clusters", "comments": "28 Pages, 15 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FFT of three-dimensional (3D) input data is an important computational\nkernel of numerical simulations and is widely used in High Performance\nComputing (HPC) codes running on a large number of processors. Performance of\nmany scientific applications such as Molecular Dynamic simulations depends on\nthe underlying 3D parallel FFT library being used.\n  In this paper, we present C-DACs three-dimensional Fast Fourier Transform\n(CROFT) library which implements three-dimensional parallel FFT using pencil\ndecomposition. To exploit the hyperthreading capabilities of processor cores\nwithout affecting performance, CROFT is designed to use multithreading along\nwith MPI. CROFT implementation has an innovative feature of overlapping compute\nand memory-I/O with MPI communication using multithreading. As opposed to other\n3D FFT implementations, CROFT uses only two threads where one thread is\ndedicated for communication so that it can be effectively overlapped with\ncomputations. Thus, depending on the number of processes used, CROFT achieves\nperformance improvement of about 51% to 42% as compared to FFTW3 library.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:12:15 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 12:57:33 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Gavane", "Vivek", ""], ["Prabhugawankar", "Supriya", ""], ["Garg", "Shivam", ""], ["Achalere", "Archana", ""], ["Joshi", "Rajendra", ""]]}, {"id": "2002.04930", "submitter": "Shuai Wang", "authors": "Shuai Wang and Tsung-Hui Chang", "title": "Federated Matrix Factorization: Algorithm Design and Application to Data\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent demands on data privacy have called for federated learning (FL) as a\nnew distributed learning paradigm in massive and heterogeneous networks.\nAlthough many FL algorithms have been proposed, few of them have considered the\nmatrix factorization (MF) model, which is known to have a vast number of signal\nprocessing and machine learning applications. Different from the existing FL\nalgorithms that are designed for smooth problems with single block of\nvariables, in federated MF (FedMF), one has to deal with challenging non-convex\nand non-smooth problems (due to constraints or regularization) with two blocks\nof variables. In this paper, we address the challenge by proposing two new\nFedMF algorithms, namely, FedMAvg and FedMGS, based on the model averaging and\ngradient sharing principles, respectively. Both FedMAvg and FedMGS adopt\nmultiple steps of local updates per communication round to speed up\nconvergence, and allow only a randomly sampled subset of clients to communicate\nwith the server for reducing the communication cost. Convergence analyses for\nthe two algorithms are respectively presented, which delineate the impacts of\ndata distribution, local update number, and partial client communication on the\nalgorithm performance. By focusing on a data clustering task, extensive\nexperiment results are presented to examine the practical performance of both\nalgorithms, as well as demonstrating their efficacy over the existing\ndistributed clustering algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:48:54 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 09:49:24 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Wang", "Shuai", ""], ["Chang", "Tsung-Hui", ""]]}, {"id": "2002.04989", "submitter": "Shrey Dabhi", "authors": "Shrey Dabhi and Manojkumar Parmar", "title": "Eigenvector Component Calculation Speedup over NumPy for\n  High-Performance Computing", "comments": "Accepted at 8th International Conference on Recent Trends in\n  Computing (ICRTC 2020), to be published in Springer Lecture Notes in Networks\n  and Systems (LNNS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications related to artificial intelligence, machine learning, and system\nidentification simulations essentially use eigenvectors. Calculating\neigenvectors for very large matrices using conventional methods is\ncompute-intensive and renders the applications slow. Recently,\nEigenvector-Eigenvalue Identity formula promising significant speedup was\nidentified. We study the algorithmic implementation of the formula against the\nexisting state-of-the-art algorithms and their implementations to evaluate the\nperformance gains. We provide a first of its kind systematic study of the\nimplementation of the formula. We demonstrate further improvements using\nhigh-performance computing concepts over native NumPy eigenvector\nimplementation which uses LAPACK and BLAS.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 13:44:41 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 16:40:42 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 20:27:06 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2020 07:21:06 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Dabhi", "Shrey", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2002.05024", "submitter": "Mirko Myllykoski", "authors": "Mirko Myllykoski, Carl Christian Kjelgaard Mikkelsen", "title": "Task-based, GPU-accelerated and Robust Library for Solving Dense\n  Nonsymmetric Eigenvalue Problems", "comments": "18 pages, 11 figures (18 when counting sub-figures), 1 tex-files.\n  Invited article submitted to Concurrency and Computation: Practice and\n  Experience. Second author's first name is \"Carl Christian\" and last name\n  \"Kjelgaard Mikkelsen\"", "journal-ref": "Concurrency Computat Pract Exper. (2020) e5915", "doi": "10.1002/cpe.5915", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the StarNEig library for solving dense nonsymmetric\nstandard and generalized eigenvalue problems. The library is built on top of\nthe StarPU runtime system and targets both shared and distributed memory\nmachines. Some components of the library have support for GPU acceleration. The\nlibrary is currently in an early beta state and supports only real matrices.\nSupport for complex matrices is planned for a future release. This paper is\naimed at potential users of the library. We describe the design choices and\ncapabilities of the library, and contrast them to existing software such as\nScaLAPACK. StarNEig implements a ScaLAPACK compatibility layer which should\nassist new users in the transition to StarNEig. We demonstrate the performance\nof the library with a sample of computational experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:28:55 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Myllykoski", "Mirko", ""], ["Mikkelsen", "Carl Christian Kjelgaard", ""]]}, {"id": "2002.05026", "submitter": "Javad Moshfegh", "authors": "Javad Moshfegh, Dimitrios G. Makris, and Marinos N. Vouvakis", "title": "Parallel Direct Domain Decomposition Methods (D3M) for Finite Elements", "comments": "3 pages, 4 figures, 2019 IEEE International Symposium on Antennas and\n  Propagation and USNC-URSI Radio Science Meeting. IEEE, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel direct solution approach based on domain decomposition method\n(DDM) and directed acyclic graph (DAG) scheduling is outlined. Computations are\nrepresented as a sequence of small tasks that operate on domains of DDM or\ndense matrix blocks of a reduced matrix. These tasks can be statically\nscheduled for parallel execution using their DAG dependencies and weights that\ndepend on estimates of computation and communication costs. Performance\ncomparison with MUMPS 5.1.2 on electrically large problems suggest up to 20%\nbetter parallel efficiency, 30% less memory and slightly faster in run-time,\nwhile maintaining the same accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 17:38:43 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Moshfegh", "Javad", ""], ["Makris", "Dimitrios G.", ""], ["Vouvakis", "Marinos N.", ""]]}, {"id": "2002.05034", "submitter": "Yijie Han", "authors": "Yijie Han", "title": "Uniform Linked Lists Contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm (EREW PRAM algorithm) for linked lists\ncontraction. We show that when we contract a linked list from size $n$ to size\n$n/c$ for a suitable constant $c$ we can pack the linked list into an array of\nsize $n/d$ for a constant $1 < d\\leq c$ in the time of 3 coloring the list.\nThus for a set of linked lists with a total of $n$ elements and the longest\nlist has $l$ elements our algorithm contracts them in $O(n\\log\ni/p+(\\log^{(i)}n+\\log i )\\log \\log l+ \\log l)$ time, for an arbitrary\nconstructible integer $i$, with $p$ processors on the EREW PRAM, where\n$\\log^{(1)} n =\\log n$ and $\\log^{(t)}n=\\log \\log^{(t-1)} n$ and $\\log^*n=\\min\n\\{ i|\\log^{(i)} n < 10\\}$. When $i$ is a constant we get time\n$O(n/p+\\log^{(i)}n\\log \\log l+\\log l)$. Thus when $l=\\Omega (\\log^{(c)}n)$ for\nany constant $c$ we achieve $O(n/p+\\log l)$ time. The previous best\ndeterministic EREW PRAM algorithm has time $O(n/p+\\log n)$ and best CRCW PRAM\nalgorithm has time $O(n/p+\\log n/\\log \\log n+\\log l)$.\n  Keywords: Parallel algorithms, linked list, linked list contraction, uniform\nlinked list contraction, EREW PRAM.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 14:50:40 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 14:25:21 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 15:55:56 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Han", "Yijie", ""]]}, {"id": "2002.05038", "submitter": "Jia Qian", "authors": "Jia Qian, Xenofon Fafoutis, Lars Kai Hansen", "title": "Towards Federated Learning: Robustness Analytics to Data Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning allows remote centralized server training models without\nto access the data stored in distributed (edge) devices. Most work assume the\ndata generated from edge devices is identically and independently sampled from\na common population distribution. However, such ideal sampling may not be\nrealistic in many contexts where edge devices correspond to units in variable\ncontext. Also, models based on intrinsic agency, such as active sampling\nschemes, may lead to highly biased sampling. So an imminent question is how\nrobust Federated Learning is to biased sampling? In this work, we investigate\ntwo such scenarios. First, we study Federated Learning of a classifier from\ndata with edge device class distribution heterogeneity. Second, we study\nFederated Learning of a classifier with active sampling at the edge. We present\nevidence in both scenarios, that federated learning is robust to data\nheterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:11:17 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Qian", "Jia", ""], ["Fafoutis", "Xenofon", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "2002.05129", "submitter": "Daniel Anderson", "authors": "Umut A. Acar, Daniel Anderson, Guy E. Blelloch, Laxman Dhulipala, Sam\n  Westrick", "title": "Parallel Batch-dynamic Trees via Change Propagation", "comments": null, "journal-ref": "Proceedings of The 28th Annual European Symposium on Algorithms\n  (ESA '20) (2020) 2:1-2:23", "doi": "10.4230/LIPIcs.ESA.2020.2", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic trees problem is to maintain a forest subject to edge insertions\nand deletions while facilitating queries such as connectivity, path weights,\nand subtree weights. Dynamic trees are a fundamental building block of a large\nnumber of graph algorithms. Although traditionally studied in the single-update\nsetting, dynamic algorithms capable of supporting batches of updates are\nincreasingly relevant today due to the emergence of rapidly evolving dynamic\ndatasets. Since processing updates on a single processor is often unrealistic\nfor large batches of updates, designing parallel batch-dynamic algorithms that\nachieve provably low span is important for many applications. In this work, we\ndesign the first work-efficient parallel batch-dynamic algorithm for dynamic\ntrees that is capable of supporting both path queries and subtree queries, as\nwell as a variety of non-local queries. To achieve this, we propose a framework\nfor algorithmically dynamizing static round-synchronous algorithms that allows\nus to obtain parallel batch-dynamic algorithms with good bounds on their work\nand span. In our framework, the algorithm designer can apply the technique to\nany suitably defined static algorithm. We then obtain theoretical guarantees\nfor algorithms in our framework by defining the notion of a computation\ndistance between two executions of the underlying algorithm.\n  Our dynamic trees algorithm is obtained by applying our dynamization\nframework to the parallel tree contraction algorithm of Miller and Reif, and\nthen performing a novel analysis of the computation distance of this algorithm\nunder batch updates. We show that $k$ updates can be performed in $O(k\n\\log(1+n/k))$ work in expectation, which matches an existing algorithm of Tseng\net al. while providing support for a substantially larger number of queries and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:20:20 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 22:56:00 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Acar", "Umut A.", ""], ["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Dhulipala", "Laxman", ""], ["Westrick", "Sam", ""]]}, {"id": "2002.05200", "submitter": "Giulia Guidi", "authors": "Alberto Zeni, Giulia Guidi, Marquita Ellis, Nan Ding, Marco D.\n  Santambrogio, Steven Hofmeyr, Ayd{\\i}n Bulu\\c{c}, Leonid Oliker, Katherine\n  Yelick", "title": "LOGAN: High-Performance GPU-Based X-Drop Long-Read Alignment", "comments": null, "journal-ref": "34th IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS), 2020", "doi": null, "report-no": null, "categories": "q-bio.GN cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise sequence alignment is one of the most computationally intensive\nkernels in genomic data analysis, accounting for more than 90% of the runtime\nfor key bioinformatics applications. This method is particularly expensive for\nthird-generation sequences due to the high computational cost of analyzing\nsequences of length between 1Kb and 1Mb. Given the quadratic overhead of exact\npairwise algorithms for long alignments, the community primarily relies on\napproximate algorithms that search only for high-quality alignments and stop\nearly when one is not found. In this work, we present the first GPU\noptimization of the popular X-drop alignment algorithm, that we named LOGAN.\nResults show that our high-performance multi-GPU implementation achieves up to\n181.6 GCUPS and speed-ups up to 6.6x and 30.7x using 1 and 6 NVIDIA Tesla V100,\nrespectively, over the state-of-the-art software running on two IBM Power9\nprocessors using 168 CPU threads, with equivalent accuracy. We also demonstrate\na 2.3x LOGAN speed-up versus ksw2, a state-of-art vectorized algorithm for\nsequence alignment implemented in minimap2, a long-read mapping software. To\nhighlight the impact of our work on a real-world application, we couple LOGAN\nwith a many-to-many long-read alignment software called BELLA, and demonstrate\nthat our implementation improves the overall BELLA runtime by up to 10.6x.\nFinally, we adapt the Roofline model for LOGAN and demonstrate that our\nimplementation is near-optimal on the NVIDIA Tesla V100s.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 19:53:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zeni", "Alberto", ""], ["Guidi", "Giulia", ""], ["Ellis", "Marquita", ""], ["Ding", "Nan", ""], ["Santambrogio", "Marco D.", ""], ["Hofmeyr", "Steven", ""], ["Bulu\u00e7", "Ayd\u0131n", ""], ["Oliker", "Leonid", ""], ["Yelick", "Katherine", ""]]}, {"id": "2002.05373", "submitter": "Usman Khan", "authors": "Ran Xin, Soummya Kar, Usman A. Khan", "title": "Gradient tracking and variance reduction for decentralized optimization\n  and machine learning", "comments": "accepted for publication, IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized methods to solve finite-sum minimization problems are important\nin many signal processing and machine learning tasks where the data is\ndistributed over a network of nodes and raw data sharing is not permitted due\nto privacy and/or resource constraints. In this article, we review\ndecentralized stochastic first-order methods and provide a unified algorithmic\nframework that combines variance-reduction with gradient tracking to achieve\nboth robust performance and fast convergence. We provide explicit theoretical\nguarantees of the corresponding methods when the objective functions are smooth\nand strongly-convex, and show their applicability to non-convex problems via\nnumerical experiments. Throughout the article, we provide intuitive\nillustrations of the main technical ideas by casting appropriate tradeoffs and\ncomparisons among the methods of interest and by highlighting applications to\ndecentralized training of machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 07:17:07 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Xin", "Ran", ""], ["Kar", "Soummya", ""], ["Khan", "Usman A.", ""]]}, {"id": "2002.05376", "submitter": "Sumathi Sivasubramaniam", "authors": "John Augustine and Keerti Choudhary and Avi Cohen and David Peleg and\n  Sumathi Sivasubramaniam and Suman Sourav", "title": "Distributed Graph Realizations", "comments": "22 pages, 4 figures. Short version to appear in IPDPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph realization problems from a distributed perspective and we\nstudy it in the node capacitated clique (NCC) model of distributed computing,\nrecently introduced for representing peer-to-peer networks. We focus on two\ncentral variants, degree-sequence realization and minimum\nthreshold-connectivity realization both of which result in overlay network\nrealizations. Overlay network realizations can be either explicit or implicit.\nExplicit realizations require both endpoints of any edge in the realized graph\nto be aware of the edge. In implicit realizations, on the other hand, at least\none endpoint of each edge of the realized graph needs to be aware of the edge.\nThe main realization algorithms we present are the following.\n  1. An $\\tilde{O}(\\min\\{\\sqrt{m},\\Delta\\})$ time algorithm for implicit\nrealization of a degree sequence. Here, $\\Delta = \\max_v d(v)$ is the maximum\ndegree and $m = (1/2) \\sum_v d(v)$ is the number of edges in the final\nrealization. An $\\tilde{O}(\\Delta)$ time algorithm for an explicit realization\nof a degree sequence. We first compute an implicit realization and then\ntransform it into an explicit one in $\\tilde{O}(\\Delta)$ additional rounds.\n  2. An $\\tilde{O}(\\Delta)$ time algorithm for the threshold connectivity\nproblem that obtains an explicit solution and an improved $\\tilde{O}(1)$\nalgorithm for implicit realization when all nodes know each other's IDs. These\nalgorithms are 2-approximations w.r.t. the number of edges.\n  We complement our upper bounds with lower bounds to show that the above\nalgorithms are tight up to factors of $\\log n$. Additionally, we provide\nalgorithms for realizing trees and an $\\tilde{O}(1)$ round algorithm for\napproximate degree sequence realization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 07:36:34 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 17:49:31 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 15:27:25 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Augustine", "John", ""], ["Choudhary", "Keerti", ""], ["Cohen", "Avi", ""], ["Peleg", "David", ""], ["Sivasubramaniam", "Sumathi", ""], ["Sourav", "Suman", ""]]}, {"id": "2002.05382", "submitter": "Ana\\\"is Durand", "authors": "L\\'elia Blin, Ana\\\"is Durand, S\\'ebastien Tixeuil", "title": "Ressource Efficient Stabilization for Local Tasks despite Unknown\n  Capacity Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilizing protocols enable distributed systems to recover correct\nbehavior starting from any arbitrary configuration. In particular, when\nprocessors communicate by message passing, fake messages may be placed in\ncommunication links by an adversary. When the number of such fake messages is\nunknown, self-stabilization may require huge resources: (a) generic solutions\n(a.k.a. data link protocols) require unbounded resources, which makes them\nunrealistic to deploy and (b) specific solutions (e.g., census or tree\nconstruction) require $O(n\\log n)$ or $O(\\Delta\\log n)$ bits of memory per\nnode, where $n$ denotes the network size and $\\Delta$ its maximum degree, which\nmay prevent scalability.\n  We investigate the possibility of resource efficient self-stabilizing\nprotocols in this context. Specifically, we present a self-stabilizing protocol\nfor $(\\Delta+1)$-coloring in any n-node graph, under the asynchronous\nmessage-passing model. It is deterministic, it converges in $O(k\\Delta n^2\\log\nn)$ message exchanges, where $k$ is the bound of the link capacity in terms of\nnumber of messages, and it uses messages on $O(\\log\\log n+\\log\\Delta)$ bits\nwith a memory of $O(\\Delta\\log\\Delta+\\log\\log n)$ bits at each node. The\nresource consumption of our protocol is thus almost oblivious to the number of\nnodes, enabling scalability. Moreover, a striking property of our protocol is\nthat the nodes do not need to know the number, or any bound on the number of\nmessages initially present in each communication link of the initial\n(potentially corrupted) network configuration. This permits our protocol to\nhandle any future network with unknown message capacity communication links. A\nkey building block of our coloring scheme is a spanning directed acyclic graph\nconstruction, that is of independent interest, and can serve as a useful tool\nfor solving other tasks in this challenging setting.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 08:03:13 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Blin", "L\u00e9lia", ""], ["Durand", "Ana\u00efs", ""], ["Tixeuil", "S\u00e9bastien", ""]]}, {"id": "2002.05516", "submitter": "Filip Hanzely", "authors": "Filip Hanzely and Peter Richt\\'arik", "title": "Federated Learning of a Mixture of Global and Local Models", "comments": "40 pages, 8 algorithms, 6 figures, 1 table (minor changes compared to\n  the previous versions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new optimization formulation for training federated learning\nmodels. The standard formulation has the form of an empirical risk minimization\nproblem constructed to find a single global model trained from the private data\nstored across all participating devices. In contrast, our formulation seeks an\nexplicit trade-off between this traditional global model and the local models,\nwhich can be learned by each device from its own private data without any\ncommunication. Further, we develop several efficient variants of SGD (with and\nwithout partial participation and with and without variance reduction) for\nsolving the new formulation and prove communication complexity guarantees.\nNotably, our methods are similar but not identical to federated averaging /\nlocal SGD, thus shedding some light on the role of local steps in federated\nlearning. In particular, we are the first to i) show that local steps can\nimprove communication for problems with heterogeneous data, and ii) point out\nthat personalization yields reduced communication complexity.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 09:17:08 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 15:05:50 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 06:30:47 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Hanzely", "Filip", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2002.05531", "submitter": "Ayesha Abdul Majeed Ms", "authors": "Ayesha Abdul Majeed, Peter Kilpatrick, Ivor Spence, and Blesson\n  Varghese", "title": "Modelling Fog Offloading Performance", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.04945", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing has emerged as a computing paradigm aimed at addressing the\nissues of latency, bandwidth and privacy when mobile devices are communicating\nwith remote cloud services. The concept is to offload compute services closer\nto the data. However many challenges exist in the realisation of this approach.\nDuring offloading, (part of) the application underpinned by the services may be\nunavailable, which the user will experience as down time. This paper describes\nwork aimed at building models to allow prediction of such down time based on\nmetrics (operational data) of the underlying and surrounding infrastructure.\nSuch prediction would be invaluable in the context of automated Fog offloading\nand adaptive decision making in Fog orchestration. Models that cater for four\ncontainer-based stateless and stateful offload techniques, namely Save and\nLoad, Export and Import, Push and Pull and Live Migration, are built using four\n(linear and non-linear) regression techniques. Experimental results comprising\nover 42 million data points from multiple lab-based Fog infrastructure are\npresented. The results highlight that reasonably accurate predictions (measured\nby the coefficient of determination for regression models, mean absolute\npercentage error, and mean absolute error) may be obtained when considering 25\nmetrics relevant to the infrastructure.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 09:27:00 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Majeed", "Ayesha Abdul", ""], ["Kilpatrick", "Peter", ""], ["Spence", "Ivor", ""], ["Varghese", "Blesson", ""]]}, {"id": "2002.05547", "submitter": "Arnab Chatterjee", "authors": "Arnab Chatterjee and Yash Pitroda and Manojkumar Parmar", "title": "Dynamic Role-Based Access Control for Decentralized Applications", "comments": "6 pages, 3 figures, 1 table", "journal-ref": "Blockchain -- ICBC 2020. Lecture Notes in Computer Science, vol\n  12404. Springer, Cham", "doi": "10.1007/978-3-030-59638-5_13", "report-no": null, "categories": "cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access control management is an integral part of maintaining the security of\nan application. Although there has been significant work in the field of cloud\naccess control mechanisms, however, with the advent of Distributed Ledger\nTechnology (DLT), on-chain access control management frameworks hardly exist.\nExisting access control management mechanisms are tightly coupled with the\nbusiness logic, resulting in governance issues, non-coherent with existing\nIdentity Management Solutions, low security, and compromised usability. We\npropose a novel framework to implement dynamic role-based access control for\ndecentralized applications (dApps). The framework allows for managing access\ncontrol on a dApp, which is completely decoupled from the business application\nand integrates seamlessly with any dApps. The smart contract architecture\nallows for the independent management of business logic and execution of access\ncontrol policies. It also facilitates secure, low cost, and a high degree of\nflexibility of access control management. The proposed framework promotes\ndecentralized governance of access control policies and efficient smart\ncontract upgrades. We also provide quantitative and qualitative metrics for the\nefficacy and efficiency of the framework. Any Turing complete smart contract\nprogramming language is an excellent fit to implement the framework. We expect\nthis framework to benefit enterprise and non-enterprise dApps and provide\ngreater access control flexibility and effective integration with traditional\nand state of the art identity management solutions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:57:43 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 06:03:02 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chatterjee", "Arnab", ""], ["Pitroda", "Yash", ""], ["Parmar", "Manojkumar", ""]]}, {"id": "2002.05645", "submitter": "Maral Mesmakhosroshahi", "authors": "Bharadwaj Pudipeddi, Maral Mesmakhosroshahi, Jinwen Xi, and Sujeeth\n  Bharadwaj", "title": "Training Large Neural Networks with Constant Memory using a New\n  Execution Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely popular transformer-based NLP models such as BERT and Turing-NLG have\nenormous capacity trending to billions of parameters. Current execution methods\ndemand brute-force resources such as HBM devices and high speed\ninterconnectivity for data parallelism. In this paper, we introduce a new\nrelay-style execution technique called L2L (layer-to-layer) where at any given\nmoment, the device memory is primarily populated only with the executing\nlayer(s)'s footprint. The model resides in the DRAM memory attached to either a\nCPU or an FPGA as an entity we call eager param-server (EPS). To overcome the\nbandwidth issues of shuttling parameters to and from EPS, the model is executed\na layer at a time across many micro-batches instead of the conventional method\nof minibatches over whole model. L2L is implemented using 16GB V100 devices for\nBERT-Large running it with a device batch size of up to 256. Our results show\n45% reduction in memory and 40% increase in the throughput compared to the\nstate-of-the-art baseline. L2L is also able to fit models up to 50 Billion\nparameters on a machine with a single 16GB V100 and 512GB CPU memory and\nwithout requiring any model partitioning. L2L scales to arbitrary depth\nallowing researchers to develop on affordable devices which is a big step\ntoward democratizing AI. By running the optimizer in the host EPS, we show a\nnew form of mixed precision for faster throughput and convergence. In addition,\nthe EPS enables dynamic neural architecture approaches by varying layers across\niterations. Finally, we also propose and demonstrate a constant memory\nvariation of L2L and we propose future enhancements. This work has been\nperformed on GPUs first, but also targeted towards all high TFLOPS/Watt\naccelerators.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:29:47 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 18:21:52 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 22:51:58 GMT"}, {"version": "v4", "created": "Tue, 2 Jun 2020 23:54:24 GMT"}, {"version": "v5", "created": "Fri, 5 Jun 2020 03:00:26 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Pudipeddi", "Bharadwaj", ""], ["Mesmakhosroshahi", "Maral", ""], ["Xi", "Jinwen", ""], ["Bharadwaj", "Sujeeth", ""]]}, {"id": "2002.05659", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz and Marie Haikel-Elsabeh", "title": "Analysis of Users' Behaviour and Adoption Trends of Social Media Payment\n  Platforms", "comments": null, "journal-ref": "2019 International Conference on Computing, Electronics &\n  Communications Engineering (iCCECE), London, United Kingdom, 2019, pp.\n  197-202", "doi": "10.1109/iCCECE46942.2019.8941991", "report-no": null, "categories": "cs.CY cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of Electronic Commerce (E-commerce) has been further\nescalated by multifaceted emerging payment solutions such as cryptocurrencies,\nmobile, peer-to-peer (P2P) and social media payment platforms. While these\ntechnological advancements are gaining tremendous popularity, mostly for their\nease of use, various impediments such as security and privacy concerns,\nsocietal and cultural norms etc. forbear the users' adoption trends to some\nextents. This article examines the current status of the social media payment\nplatforms as well as the projection of future adoption trends. Our research\nunderlines the motivations and obstacles to the adoption of social media\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 12:08:43 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Haikel-Elsabeh", "Marie", ""]]}, {"id": "2002.05710", "submitter": "Daniel Anderson", "authors": "Daniel Anderson, Guy E. Blelloch, Kanat Tangwongsan", "title": "Work-efficient Batch-incremental Minimum Spanning Trees with\n  Applications to the Sliding Window Model", "comments": null, "journal-ref": "Proceedings of the 32nd ACM Symposium on Parallelism in Algorithms\n  and Architectures (SPAA '20) (2020) 51-61", "doi": "10.1145/3350755.3400241", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for dynamically maintaining minimum spanning trees (MSTs) have\nreceived much attention in both the parallel and sequential settings. While\nprevious work has given optimal algorithms for dense graphs, all existing\nparallel batch-dynamic algorithms perform polynomial work per update in the\nworst case for sparse graphs. In this paper, we present the first\nwork-efficient parallel batch-dynamic algorithm for incremental MST, which can\ninsert $\\ell$ edges in $O(\\ell \\log(1+n/\\ell))$ work in expectation and\n$O(\\text{polylog}(n))$ span w.h.p. The key ingredient of our algorithm is an\nalgorithm for constructing a compressed path tree of an edge-weighted tree,\nwhich is a smaller tree that contains all pairwise heaviest edges between a\ngiven set of marked vertices. Using our batch-incremental MST algorithm, we\ndemonstrate a range of applications that become efficiently solvable in\nparallel in the sliding-window model, such as graph connectivity, approximate\nMSTs, testing bipartiteness, $k$-certificates, cycle-freeness, and maintaining\nsparsifiers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:51:07 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Anderson", "Daniel", ""], ["Blelloch", "Guy E.", ""], ["Tangwongsan", "Kanat", ""]]}, {"id": "2002.05814", "submitter": "Zhuohan Li", "authors": "Siyuan Zhuang, Zhuohan Li, Danyang Zhuo, Stephanie Wang, Eric Liang,\n  Robert Nishihara, Philipp Moritz, Ion Stoica", "title": "Hoplite: Efficient Collective Communication for Task-Based Distributed\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective communication systems such as MPI offer high performance group\ncommunication primitives at the cost of application flexibility. Today, an\nincreasing number of distributed applications (e.g, reinforcement learning)\nrequire flexibility in expressing dynamic and asynchronous communication\npatterns. To accommodate these applications, task-based distributed computing\nframeworks (e.g., Ray, Dask, Hydro) have become popular as they allow\napplications to dynamically specify communication by invoking tasks, or\nfunctions, at runtime. This design makes efficient collective communication\nchallenging because (1) the group of communicating processes is chosen at\nruntime, and (2) processes may not all be ready at the same time.\n  We design and implement Hoplite, a communication layer for task-based\ndistributed systems that achieves high performance collective communication\nwithout compromising application flexibility. The key idea of Hoplite is to use\ndistributed protocols to compute a data transfer schedule on the fly. This\nenables the same optimizations used in traditional collective communication,\nbut for applications that specify the communication incrementally. We show that\nHoplite can achieve similar performance compared with a traditional collective\ncommunication library, MPICH. We port a popular distributed computing\nframework, Ray, on atop of Hoplite. We show that Hoplite can speed up\nasynchronous parameter server and distributed reinforcement learning workloads\nthat are difficult to execute efficiently with traditional collective\ncommunication by up to 8.1x and 3.9x, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 23:48:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zhuang", "Siyuan", ""], ["Li", "Zhuohan", ""], ["Zhuo", "Danyang", ""], ["Wang", "Stephanie", ""], ["Liang", "Eric", ""], ["Nishihara", "Robert", ""], ["Moritz", "Philipp", ""], ["Stoica", "Ion", ""]]}, {"id": "2002.05869", "submitter": "Vitor Almeida", "authors": "Vitor Pinheiro de Almeida and Sukanya Bhowmik and Markus Endler and\n  Kurt Rothermel", "title": "DSCEP: An Infrastructure for Distributed Semantic Complex Event\n  Processing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Today most applications continuously produce information under the form of\nstreams, due to the advent of the means of collecting data. Sensors and social\nnetworks collect an immense variety and volume of data, from different\nreal-life situations and at a considerable velocity. Increasingly, applications\nrequire processing of heterogeneous data streams from different sources\ntogether with large background knowledge. To use only the information on the\ndata stream is not enough for many use cases. Semantic Complex Event Processing\n(CEP) systems have evolved from the classical rule-based CEP systems, by\nintegrating high-level knowledge representation and RDF stream processing using\nboth the data stream and background static knowledge. Additionally, CEP\napproaches lack the capability to semantically interpret and analyze data,\nwhich Semantic CEP (SCEP) attempts to address. SCEP has several limitations;\none of them is related to their high processing time. This paper provides a\nconceptual model and an implementation of an infrastructure for distributed\nSCEP, where each SCEP operator can process part of the data and send it to\nother SCEP operators in order to achieves some answer. We show that by\nsplitting the RDF stream processing and the background knowledge using the\nconcept of SCEP operators, it's possible to considerably reduce processing\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 04:36:51 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["de Almeida", "Vitor Pinheiro", ""], ["Bhowmik", "Sukanya", ""], ["Endler", "Markus", ""], ["Rothermel", "Kurt", ""]]}, {"id": "2002.05896", "submitter": "Ahmad Slo", "authors": "Ahmad Slo, Sukanya Bhowmik, Kurt Rothermel", "title": "eSPICE: Probabilistic Load Shedding from Input Event Streams in Complex\n  Event Processing", "comments": "13 pages", "journal-ref": null, "doi": "10.1145/3361525.3361548", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex event processing systems process the input event streams on-the-fly.\nSince input event rate could overshoot the system's capabilities and results in\nviolating a defined latency bound, load shedding is used to drop a portion of\nthe input event streams. The crucial question here is how many and which events\nto drop so the defined latency bound is maintained and the degradation in the\nquality of results is minimized. In stream processing domain, different load\nshedding strategies have been proposed but they mainly depend on the importance\nof individual tuples (events). However, as complex event processing systems\nperform pattern detection, the importance of events is also influenced by other\nevents in the same pattern. In this paper, we propose a load shedding framework\ncalled eSPICE for complex event processing systems. eSPICE depends on building\na probabilistic model that learns about the importance of events in a window.\nThe position of an event in a window and its type are used as features to build\nthe model. Further, we provide algorithms to decide when to start dropping\nevents and how many events to drop. Moreover, we extensively evaluate the\nperformance of eSPICE on two real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 07:35:26 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Slo", "Ahmad", ""], ["Bhowmik", "Sukanya", ""], ["Rothermel", "Kurt", ""]]}, {"id": "2002.05973", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao", "title": "Algebraic Structure of Blockchains: A Group-Theoretical Primer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent advances of blockchain systems, notably in the form of\ncryptocurrency, have drawn tremendous interests from both researchers and\npractitioners, limited studies existed toward the theoretical foundation of\nblockchains. This paper presents the first study on the algebraic structure of\nblockchains with an emphasis on the internal properties under algebraic groups.\nWe axiomatically construct a blockchain group and derive some interesting\nproperties that can be potentially taken into the design space and parametric\nanalysis of real-world blockchain systems.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:28:59 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zhao", "Dongfang", ""]]}, {"id": "2002.05983", "submitter": "Hamid Reza Zohouri", "authors": "Hamid Reza Zohouri, Artur Podobas, Satoshi Matsuoka", "title": "High-Performance High-Order Stencil Computation on FPGAs Using OpenCL", "comments": "Published at RAW'18: 25th Anniversary of Reconfigurable Architectures\n  Workshop held in conjunction with IPDPS'18", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00027", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate the performance of FPGAs for high-order stencil\ncomputation using High-Level Synthesis. We show that despite the higher\ncomputation intensity and on-chip memory requirement of such stencils compared\nto first-order ones, our design technique with combined spatial and temporal\nblocking remains effective. This allows us to reach similar, or even higher,\ncompute performance compared to first-order stencils. We use an OpenCL-based\ndesign that, apart from parameterizing performance knobs, also parameterizes\nthe stencil radius. Furthermore, we show that our performance model exhibits\nthe same accuracy as first-order stencils in predicting the performance of\nhigh-order ones. On an Intel Arria 10 GX 1150 device, for 2D and 3D star-shaped\nstencils, we achieve over 700 and 270 GFLOP/s of compute performance,\nrespectively, up to a stencil radius of four. These results outperform the\nstate-of-the-art YASK framework on a modern Xeon for 2D and 3D stencils, and\noutperform a modern Xeon Phi for 2D stencils, while achieving competitive\nperformance in 3D. Furthermore, our FPGA design achieves better power\nefficiency in almost all cases.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 11:54:34 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zohouri", "Hamid Reza", ""], ["Podobas", "Artur", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "2002.06005", "submitter": "Corinna Coupette", "authors": "Corinna Coupette and Christoph Lenzen", "title": "A Breezing Proof of the KMW Bound", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their seminal paper from 2004, Kuhn, Moscibroda, and Wattenhofer (KMW)\nproved a hardness result for several fundamental graph problems in the LOCAL\nmodel: For any (randomized) algorithm, there are input graphs with $n$ nodes\nand maximum degree $\\Delta$ on which $\\Omega(\\min\\{\\sqrt{\\log n/\\log \\log\nn},\\log \\Delta/\\log \\log \\Delta\\})$ (expected) communication rounds are\nrequired to obtain polylogarithmic approximations to a minimum vertex cover,\nminimum dominating set, or maximum matching. Via reduction, this hardness\nextends to symmetry breaking tasks like finding maximal independent sets or\nmaximal matchings. Today, more than $15$ years later, there is still no proof\nof this result that is easy on the reader. Setting out to change this, in this\nwork, we provide a fully self-contained and $\\mathit{simple}$ proof of the KMW\nlower bound. The key argument is algorithmic, and it relies on an invariant\nthat can be readily verified from the generation rules of the lower bound\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 12:49:15 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 06:34:38 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:55:10 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 20:19:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Coupette", "Corinna", ""], ["Lenzen", "Christoph", ""]]}, {"id": "2002.06098", "submitter": "Ramy E. Ali", "authors": "Ramy E. Ali", "title": "Consistency Analysis of Replication-Based Probabilistic Key-Value Stores", "comments": null, "journal-ref": "IEEE International Conference on Communications IEEE International\n  Conference on Communications (ICC), 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.IR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial quorum systems are widely used in distributed key-value stores due to\ntheir latency benefits at the expense of providing weaker consistency\nguarantees. The probabilistically bounded staleness framework (PBS) studied the\nlatency-consistency trade-off of Dynamo-style partial quorum systems through\nMonte Carlo event-based simulations. In this paper, we study the\nlatency-consistency trade-off for such systems analytically and derive a\nclosed-form expression for the inconsistency probability. Our approach allows\nfine-tuning of latency and consistency guarantees in key-value stores, which is\nintractable using Monte Carlo event-based simulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:09:47 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 00:48:09 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 19:05:23 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 19:42:03 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ali", "Ramy E.", ""]]}, {"id": "2002.06129", "submitter": "Gregory Kiar", "authors": "Pierre Rioux, Gregory Kiar, Alexandre Hutton, Alan C. Evans, Shawn T.\n  Brown", "title": "Deploying large fixed file datasets with SquashFS and Singularity", "comments": "5 pages, 2 figures, 2 tables. Submitted to PEARC 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared high-performance computing (HPC) platforms, such as those provided by\nXSEDE and Compute Canada, enable researchers to carry out large-scale\ncomputational experiments at a fraction of the cost of the cloud. Most systems\nrequire the use of distributed filesystems (e.g. Lustre) for providing a highly\nmulti-user, large capacity storage environment. These suffer performance\npenalties as the number of files increases due to network contention and\nmetadata performance. We demonstrate how a combination of two technologies,\nSingularity and SquashFS, can help developers, integrators, architects, and\nscientists deploy large datasets (O(10M) files) on these shared systems with\nminimal performance limitations. The proposed integration enables more\nefficient access and indexing than normal file-based dataset installations,\nwhile providing transparent file access to users and processes. Furthermore,\nthe approach does not require administrative privileges on the target system.\nWhile the examples studied here have been taken from the field of neuroimaging,\nthe technologies adopted are not specific to that field. Currently, this\nsolution is limited to read-only datasets. We propose the adoption of this\ntechnology for the consumption and dissemination of community datasets across\nshared computing resources.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:00:32 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Rioux", "Pierre", ""], ["Kiar", "Gregory", ""], ["Hutton", "Alexandre", ""], ["Evans", "Alan C.", ""], ["Brown", "Shawn T.", ""]]}, {"id": "2002.06258", "submitter": "Justin Wozniak", "authors": "Justin M. Wozniak and Hemant Sharma and Timothy G. Armstrong and\n  Michael Wilde and Jonathan D. Almer and Ian Foster", "title": "Big Data Staging with MPI-IO for Interactive X-ray Science", "comments": "10 pages, submitted to International Symposium on Big Data Computing\n  (BDC 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New techniques in X-ray scattering science experiments produce large data\nsets that can require millions of high-performance processing hours per week of\ncomputation for analysis. In such applications, data is typically moved from\nX-ray detectors to a large parallel file system shared by all nodes of a\npetascale supercomputer and then is read repeatedly as different science\napplication tasks proceed. However, this straightforward implementation causes\nsignificant contention in the file system. We propose an alternative approach\nin which data is instead staged into and cached in compute node memory for\nextended periods, during which time various processing tasks may efficiently\naccess it. We describe here such a big data staging framework, based on MPI-IO\nand the Swift parallel scripting language. We discuss a range of large-scale\ndata management issues involved in X-ray scattering science and measure the\nperformance benefits of the new staging framework for high-energy diffraction\nmicroscopy, an important emerging application in data-intensive X-ray\nscattering. We show that our framework accelerates scientific processing\nturnaround from three months to under 10 minutes, and that our I/O technique\nreduces input overheads by a factor of 5 on 8K Blue Gene/Q nodes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 21:40:51 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Wozniak", "Justin M.", ""], ["Sharma", "Hemant", ""], ["Armstrong", "Timothy G.", ""], ["Wilde", "Michael", ""], ["Almer", "Jonathan D.", ""], ["Foster", "Ian", ""]]}, {"id": "2002.06352", "submitter": "Mengwei Xu", "authors": "Mengwei Xu, Yuxin Zhao, Kaigui Bian, Gang Huang, Qiaozhu Mei, Xuanzhe\n  Liu", "title": "Federated Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To preserve user privacy while enabling mobile intelligence, techniques have\nbeen proposed to train deep neural networks on decentralized data. However,\ntraining over decentralized data makes the design of neural architecture quite\ndifficult as it already was. Such difficulty is further amplified when\ndesigning and deploying different neural architectures for heterogeneous mobile\nplatforms. In this work, we propose an automatic neural architecture search\ninto the decentralized training, as a new DNN training paradigm called\nFederated Neural Architecture Search, namely federated NAS. To deal with the\nprimary challenge of limited on-client computational and communication\nresources, we present FedNAS, a highly optimized framework for efficient\nfederated NAS. FedNAS fully exploits the key opportunity of insufficient model\ncandidate re-training during the architecture search process, and incorporates\nthree key optimizations: parallel candidates training on partial clients, early\ndropping candidates with inferior performance, and dynamic round numbers.\nTested on large-scale datasets and typical CNN architectures, FedNAS achieves\ncomparable model accuracy as state-of-the-art NAS algorithm that trains models\nwith centralized data, and also reduces the client cost by up to two orders of\nmagnitude compared to a straightforward design of federated NAS.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 10:01:05 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 02:55:53 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 11:31:43 GMT"}, {"version": "v4", "created": "Sun, 14 Jun 2020 03:21:52 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Xu", "Mengwei", ""], ["Zhao", "Yuxin", ""], ["Bian", "Kaigui", ""], ["Huang", "Gang", ""], ["Mei", "Qiaozhu", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "2002.06403", "submitter": "Ashutosh Bhatia Dr.", "authors": "Aman Sharma, Ashutosh Bhatia", "title": "Bitcoin's Blockchain Data Analytics: A Graph Theoretic Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is the most popular cryptocurrency used worldwide. It provides\npseudonymity to its users by establishing identity using public keys as\ntransaction end-points. These transactions are recorded on an immutable public\nledger called Blockchain which is an append-only data structure. The popularity\nof Bitcoin has increased unreasonably. The general trend shows a positive\nresponse from the common masses indicating an increase in trust and privacy\nconcerns which makes an interesting use case from the analysis point of view.\nMoreover, since the blockchain is publicly available and up-to-date, any\nanalysis would provide a live insight into the usage patterns which ultimately\nwould be useful for making a number of inferences by law-enforcement agencies,\neconomists, tech-enthusiasts, etc. In this paper, we study various applications\nand techniques of performing data analytics over Bitcoin blockchain from a\ngraph theoretic perspective. We also propose a framework for performing such\ndata analytics and explored a couple of use cases using the proposed framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 16:07:34 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sharma", "Aman", ""], ["Bhatia", "Ashutosh", ""]]}, {"id": "2002.06499", "submitter": "Ivy Peng", "authors": "Ivy Peng and Kai Wu and Jie Ren and Dong Li and Maya Gokhale", "title": "Demystifying the Performance of HPC Scientific Applications on NVM-based\n  Memory Systems", "comments": "34th IEEE International Parallel and Distributed Processing Symposium\n  (IPDPS2020)", "journal-ref": null, "doi": "10.1109/IPDPS47924.2020.00098", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of high-density byte-addressable non-volatile memory (NVM) is\npromising to accelerate data- and compute-intensive applications. Current NVM\ntechnologies have lower performance than DRAM and, thus, are often paired with\nDRAM in a heterogeneous main memory. Recently, byte-addressable NVM hardware\nbecomes available. This work provides a timely evaluation of representative HPC\napplications from the \"Seven Dwarfs\" on NVM-based main memory. Our results\nquantify the effectiveness of DRAM-cached-NVM for accelerating HPC applications\nand enabling large problems beyond the DRAM capacity. On uncached-NVM, HPC\napplications exhibit three tiers of performance sensitivity, i.e., insensitive,\nscaled, and bottlenecked. We identify write throttling and concurrency control\nas the priorities in optimizing applications. We highlight that concurrency\nchange may have a diverging effect on read and write accesses in applications.\nBased on these findings, we explore two optimization approaches. First, we\nprovide a prediction model that uses datasets from a small set of\nconfigurations to estimate performance at various concurrency and data sizes to\navoid exhaustive search in the configuration space. Second, we demonstrate that\nwrite-aware data placement on uncached-NVM could achieve $2$x performance\nimprovement with a 60% reduction in DRAM usage.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 03:25:51 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Peng", "Ivy", ""], ["Wu", "Kai", ""], ["Ren", "Jie", ""], ["Li", "Dong", ""], ["Gokhale", "Maya", ""]]}, {"id": "2002.06538", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Distributed Sketching Methods for Privacy Preserving Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study distributed sketching methods for large scale\nregression problems. We leverage multiple randomized sketches for reducing the\nproblem dimensions as well as preserving privacy and improving straggler\nresilience in asynchronous distributed systems. We derive novel approximation\nguarantees for classical sketching methods and analyze the accuracy of\nparameter averaging for distributed sketches. We consider random matrices\nincluding Gaussian, randomized Hadamard, uniform sampling and leverage score\nsampling in the distributed setting. Moreover, we propose a hybrid approach\ncombining sampling and fast random projections for better computational\nefficiency. We illustrate the performance of distributed sketches in a\nserverless computing platform with large scale experiments.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 08:35:48 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 00:36:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "2002.06540", "submitter": "Burak Bartan", "authors": "Burak Bartan, Mert Pilanci", "title": "Distributed Averaging Methods for Randomized Second Order Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider distributed optimization problems where forming the Hessian is\ncomputationally challenging and communication is a significant bottleneck. We\ndevelop unbiased parameter averaging methods for randomized second order\noptimization that employ sampling and sketching of the Hessian. Existing works\ndo not take the bias of the estimators into consideration, which limits their\napplication to massively parallel computation. We provide closed-form formulas\nfor regularization parameters and step sizes that provably minimize the bias\nfor sketched Newton directions. We also extend the framework of second order\naveraging methods to introduce an unbiased distributed optimization framework\nfor heterogeneous computing systems with varying worker resources.\nAdditionally, we demonstrate the implications of our theoretical findings via\nlarge scale experiments performed on a serverless computing platform.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 09:01:18 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bartan", "Burak", ""], ["Pilanci", "Mert", ""]]}, {"id": "2002.06545", "submitter": "Shir Cohen", "authors": "Shir Cohen, Idit Keidar, Alexander Spiegelman", "title": "Not a COINcidence: Sub-Quadratic Asynchronous Byzantine Agreement WHP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  King and Saia were the first to break the quadratic word complexity bound for\nByzantine Agreement in synchronous systems against an adaptive adversary, and\nAlgorand broke this bound with near-optimal resilience (first in the\nsynchronous model and then with eventual-synchrony). Yet the question of\nasynchronous sub-quadratic Byzantine Agreement remained open. To the best of\nour knowledge, we are the first to answer this question in the affirmative. A\nkey component of our solution is a shared coin algorithm based on a VRF. A\nsecond essential ingredient is VRF-based committee sampling, which we formalize\nand utilize in the asynchronous model for the first time. Our algorithms work\nagainst a delayed-adaptive adversary, which cannot perform after-the-fact\nremovals but has full control of Byzantine processes and full information about\ncommunication in earlier rounds. Using committee sampling and our shared coin,\nwe solve Byzantine Agreement with high probability, with a word complexity of\n$\\widetilde{O}(n)$ and $O(1)$ expected time, breaking the $O(n^2)$ bit barrier\nfor asynchronous Byzantine Agreement.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 09:23:03 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 11:07:08 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 15:02:50 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cohen", "Shir", ""], ["Keidar", "Idit", ""], ["Spiegelman", "Alexander", ""]]}, {"id": "2002.06667", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Frank Wuerthwein, Benedikt Riedel, and David Schultz", "title": "Running a Pre-Exascale, Geographically Distributed, Multi-Cloud\n  Scientific Simulation", "comments": "18 pages, 5 figures, 4 tables, to be published in Proceedings of ISC\n  High Performance 2020", "journal-ref": "Lecture Notes in Computer Science, vol 12151, year 2020. Springer", "doi": "10.1007/978-3-030-50743-5_2", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As we approach the Exascale era, it is important to verify that the existing\nframeworks and tools will still work at that scale. Moreover, public Cloud\ncomputing has been emerging as a viable solution for both prototyping and\nurgent computing. Using the elasticity of the Cloud, we have thus put in place\na pre-exascale HTCondor setup for running a scientific simulation in the Cloud,\nwith the chosen application being IceCube's photon propagation simulation. I.e.\nthis was not a purely demonstration run, but it was also used to produce\nvaluable and much needed scientific results for the IceCube collaboration. In\norder to reach the desired scale, we aggregated GPU resources across 8 GPU\nmodels from many geographic regions across Amazon Web Services, Microsoft\nAzure, and the Google Cloud Platform. Using this setup, we reached a peak of\nover 51k GPUs corresponding to almost 380 PFLOP32s, for a total integrated\ncompute of about 100k GPU hours. In this paper we provide the description of\nthe setup, the problems that were discovered and overcome, as well as a short\ndescription of the actual science output of the exercise.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 20:13:39 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sfiligoi", "Igor", ""], ["Wuerthwein", "Frank", ""], ["Riedel", "Benedikt", ""], ["Schultz", "David", ""]]}, {"id": "2002.06762", "submitter": "Lawrence Li", "authors": "Seth Gilbert, Lawrence Li", "title": "How fast can you update your MST? (Dynamic algorithms for cluster\n  computing)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagine a large graph that is being processed by a cluster of computers,\ne.g., described by the $k$-machine model or the Massively Parallel Computation\nModel. The graph, however, is not static; instead it is receiving a constant\nstream of updates. How fast can the cluster process the stream of updates? The\nfundamental question we want to ask in this paper is whether we can update the\ngraph fast enough to keep up with the stream. We focus specifically on the\nproblem of maintaining a minimum spanning tree (MST), and we give an algorithm\nfor the $k$-machine model that can process $O(k)$ graph updates per $O(1)$\nrounds with high probability. (And these results carry over to the Massively\nParallel Computation (MPC) model.) We also show a lower bound, i.e., it is\nimpossible to process $k^{1+\\epsilon}$ updates in $O(1)$ rounds. Thus we\nprovide a nearly tight answer to the question of how fast a cluster can respond\nto a stream of graph modifications while maintaining an MST.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:08:14 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gilbert", "Seth", ""], ["Li", "Lawrence", ""]]}, {"id": "2002.06779", "submitter": "Xiong Zheng", "authors": "Xiong Zheng and Vijay Garg", "title": "Byzantine Lattice Agreement in Asynchronous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Byzantine lattice agreement (BLA) problem in asynchronous\ndistributed message passing systems. In the BLA problem, each process proposes\na value from a join semi-lattice and needs to output a value also in the\nlattice such that all output values of correct processes lie on a chain despite\nthe presence of Byzantine processes. We present an algorithm for this problem\nwith round complexity of $O(\\log f)$ which tolerates $f < \\frac{n}{5}$\nByzantine failures in the asynchronous setting without digital signatures,\nwhere $n$ is the number of processes. We also show how this algorithm can be\nmodified to work in the authenticated setting (i.e., with digital signatures)\nto tolerate $f < \\frac{n}{3}$ Byzantine failures.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 05:44:03 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zheng", "Xiong", ""], ["Garg", "Vijay", ""]]}, {"id": "2002.06790", "submitter": "Hongming Huang", "authors": "Hongming Huang, Peng Cheng, Hong Xu, Yongqiang Xiong", "title": "Simulating Performance of ML Systems with Offline Profiling", "comments": "Accepted to The MLOps 2020 workshop, colocated with MLSys 2020. 2\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advocate that simulation based on offline profiling is a promising\napproach to better understand and improve the complex ML systems. Our approach\nuses operation-level profiling and dataflow based simulation to ensure it\noffers a unified and automated solution for all frameworks and ML models, and\nis also accurate by considering the various parallelization strategies in a\nreal system.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 06:13:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Huang", "Hongming", ""], ["Cheng", "Peng", ""], ["Xu", "Hong", ""], ["Xiong", "Yongqiang", ""]]}, {"id": "2002.06960", "submitter": "Gregorio Quintana-Ort\\'i", "authors": "Nathan Heavner, Per-Gunnar Martinsson, Gregorio Quintana-Ort\\'i", "title": "Computing rank-revealing factorizations of matrices stored out-of-core", "comments": "23 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CL cs.DC cs.DS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes efficient algorithms for computing rank-revealing\nfactorizations of matrices that are too large to fit in RAM, and must instead\nbe stored on slow external memory devices such as solid-state or spinning disk\nhard drives (out-of-core or out-of-memory). Traditional algorithms for\ncomputing rank revealing factorizations, such as the column pivoted QR\nfactorization, or techniques for computing a full singular value decomposition\nof a matrix, are very communication intensive. They are naturally expressed as\na sequence of matrix-vector operations, which become prohibitively expensive\nwhen data is not available in main memory. Randomization allows these methods\nto be reformulated so that large contiguous blocks of the matrix can be\nprocessed in bulk. The paper describes two distinct methods. The first is a\nblocked version of column pivoted Householder QR, organized as a \"left-looking\"\nmethod to minimize the number of write operations (which are more expensive\nthan read operations on a spinning disk drive). The second method results in a\nso called UTV factorization which expresses a matrix $A$ as $A = U T V^*$ where\n$U$ and $V$ are unitary, and $T$ is triangular. This method is organized as an\nalgorithm-by-blocks, in which floating point operations overlap read and write\noperations. The second method incorporates power iterations, and is\nexceptionally good at revealing the numerical rank; it can often be used as a\nsubstitute for a full singular value decomposition. Numerical experiments\ndemonstrate that the new algorithms are almost as fast when processing data\nstored on a hard drive as traditional algorithms are for data stored in main\nmemory. To be precise, the computational time for fully factorizing an $n\\times\nn$ matrix scales as $cn^{3}$, with a scaling constant $c$ that is only\nmarginally larger when the matrix is stored out of core.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:58:08 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 12:18:40 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Heavner", "Nathan", ""], ["Martinsson", "Per-Gunnar", ""], ["Quintana-Ort\u00ed", "Gregorio", ""]]}, {"id": "2002.06993", "submitter": "Alexander Spiegelman", "authors": "Alexander Spiegelman", "title": "In Search for a Linear Byzantine Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-standing byzantine agreement problem gets more attention in recent\nyears due to the increasing demand for scalable geo-replicated Byzantine state\nmachine replication (SMR) systems (e.g., Blockchains). To date, the key\nbottleneck of such systems is the communication cost of the byzantine agreement\nthey employ as a building block, which motivates many researchers to search for\nlow-communication byzantine agreement protocols. The conventional approach is\nto design deterministic protocols in the eventually synchronous communication\nmodel that are optimized to reduce the communication cost after the global\nstabilization time (GST).\n  In this paper, we challenge the conventional approach and argue it is not the\nbest fit for scalable SMR systems since it might induce an unbounded\ncommunication cost during asynchronous periods before GST, which we prove to be\ninherent. Instead, we forgo eventual synchrony and propose a different approach\nthat hopes for the best (synchrony) but prepares for the worst (asynchrony).\nAccordingly, we design an optimistic protocol that first tries to reach an\nagreement via an efficient deterministic algorithm that relies on synchrony for\ntermination, and then, only if an agreement was not reached due to asynchrony,\nthe protocol uses a randomized asynchronous algorithm for fallback that\nguarantees termination with probability $1$. Although randomized asynchronous\nalgorithms are considered to be costly, we design our solution to pay this cost\nonly when an equivalent cost has already been paid while unsuccessfully trying\nthe synchronous protocol. Moreover, we formally prove that our protocol\nachieves optimal communication complexity under all network conditions and\nfailure scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:07:44 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Spiegelman", "Alexander", ""]]}, {"id": "2002.07053", "submitter": "Yuanhao Wei", "authors": "Guy E. Blelloch, Yuanhao Wei", "title": "Concurrent Reference Counting and Resource Management in Wait-free\n  Constant Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem when implementing concurrent programs is efficiently\nprotecting against unsafe races between processes reading and then using a\nresource (e.g., memory blocks, file descriptors, or network connections) and\nother processes that are concurrently overwriting and then destructing the same\nresource. Such read-destruct races can be protected with locks, or with\nlock-free solutions such as hazard-pointers or read-copy-update (RCU).\n  In this paper we describe a method for protecting read-destruct races with\nexpected constant time overhead, $O(P^2)$ space and $O(P^2)$ delayed destructs,\nand with just single word atomic memory operations (reads, writes, and CAS). It\nis based on an interface with four primitives, an acquire-release pair to\nprotect accesses, and a retire-eject pair to delay the destruct until it is\nsafe. We refer to this as the acquire-retire interface. Using the\nacquire-retire interface, we develop simple implementations for three common\nuse cases: (1) memory reclamation with applications to stacks and queues, (2)\nreference counted objects, and (3) objects manage by ownership with moves,\ncopies, and destructs. The first two results significantly improve on previous\nresults, and the third application is original. Importantly, all operations\nhave expected constant time overhead.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 16:47:50 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 14:45:45 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Blelloch", "Guy E.", ""], ["Wei", "Yuanhao", ""]]}, {"id": "2002.07175", "submitter": "Umut Can Cabuk", "authors": "Umut Can Cabuk, Eylul Adiguzel, Enis Karaarslan", "title": "A Survey on Feasibility and Suitability of Blockchain Techniques for the\n  E-Voting Systems", "comments": null, "journal-ref": "International Journal of Advanced Research in Computer and\n  Communication Engineering (IJARCCE), Vol. 7, Issue 3, March 2018", "doi": "10.17148/IJARCCE.2018.7324", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the second decade of the 21st century, blockchain definitely became one of\nthe most trending computational technologies. This research aims to question\nthe feasibility and suitability of using blockchain technology within e-voting\nsystems, regarding both technical and non-technical aspects. In today's world,\nalthough the course of this spreading is considerably slow, several countries\nalready use means of e-voting due to many social and economic reasons, which we\nfurther investigated. Nevertheless, the number of countries offering various\ne-government solutions, apart from e-voting, is significantly high. E-voting\nsystems, naturally, require much more attention and assurance regarding\npotential security and anonymity issues, since voting is one of the few\nextremely critical governmental processes. Nevertheless, e-voting is not purely\na governmental service, but many companies and nonprofit organizations would\nbenefit the cost-efficiency, scalability, remote accessibility, and ease of use\nthat it provides. Blockchain technology is claimed to be able to address some,\nobviously not all, important security concerns, including anonymity,\nconfidentiality, integrity, and non-repudiation. The analysis results presented\nin this article mostly confirm these claims.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:19:54 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Cabuk", "Umut Can", ""], ["Adiguzel", "Eylul", ""], ["Karaarslan", "Enis", ""]]}, {"id": "2002.07215", "submitter": "Ali HeydariGorji", "authors": "Ali HeydariGorji, Mahdi Torabzadehkashi, Siavash Rezaei, Hossein\n  Bobarshad, Vladimir Alves, Pai H. Chou", "title": "STANNIS: Low-Power Acceleration of Deep Neural Network Training Using\n  Computational Storage", "comments": null, "journal-ref": null, "doi": "10.1109/DAC18072.2020.9218687", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a framework for distributed, in-storage training of\nneural networks on clusters of computational storage devices. Such devices not\nonly contain hardware accelerators but also eliminate data movement between the\nhost and storage, resulting in both improved performance and power savings.\nMore importantly, this in-storage processing style of training ensures that\nprivate data never leaves the storage while fully controlling the sharing of\npublic data. Experimental results show up to 2.7x speedup and 69% reduction in\nenergy consumption and no significant loss in accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 19:22:35 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 18:56:52 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["HeydariGorji", "Ali", ""], ["Torabzadehkashi", "Mahdi", ""], ["Rezaei", "Siavash", ""], ["Bobarshad", "Hossein", ""], ["Alves", "Vladimir", ""], ["Chou", "Pai H.", ""]]}, {"id": "2002.07303", "submitter": "Mikhail (Michael) Raskin", "authors": "Mikhail Raskin", "title": "Constructive expressive power of population protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are a model of distributed computation intended for the\nstudy of networks of independent computing agents with dynamic communication\nstructure. Each agent has a finite number of states, and communication\nopportunities occur nondeterministically, allowing the agents involved to\nchange their states based on each other's states. Population protocols are\noften studied in terms of reaching a consensus on whether the input\nconfiguration satisfied some predicate.\n  In the present paper we propose an alternative point of view. Instead of\nstudying the properties of inputs that a protocol can recognise, we study the\nproperties of outputs that a protocol eventually ensures. We define\nconstructive expressive power. We show that for general population protocols\nand immediate observation population protocols the constructive expressive\npower coincides with the normal expressive power.\n  Immediate observation protocols also preserve their relatively low\nverification complexity in the constructive expressive power setting.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 23:53:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Raskin", "Mikhail", ""]]}, {"id": "2002.07378", "submitter": "Jiaqi Zhang", "authors": "Jiaqi Zhang, Keyou You, Tamer Ba\\c{s}ar", "title": "Distributed Adaptive Newton Methods with Globally Superlinear\n  Convergence", "comments": "Submitted to IEEE Transactions on Automatic Control. 14 pages, 4\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the distributed optimization problem over a network\nwhere the global objective is to optimize a sum of local functions using only\nlocal computation and communication. Since the existing algorithms either adopt\na linear consensus mechanism, which converges at best linearly, or assume that\neach node starts sufficiently close to an optimal solution, they cannot achieve\nglobally superlinear convergence. To break through the linear consensus rate,\nwe propose a finite-time set-consensus method, and then incorporate it into\nPolyak's adaptive Newton method, leading to our distributed adaptive Newton\nalgorithm (DAN). To avoid transmitting local Hessians, we adopt a low-rank\napproximation idea to compress the Hessian and design a communication-efficient\nDAN-LA. Then, the size of transmitted messages in DAN-LA is reduced to $O(p)$\nper iteration, where $p$ is the dimension of decision vectors and is the same\nas the first-order methods. We show that DAN and DAN-LA can globally achieve\nquadratic and superlinear convergence rates, respectively. Numerical\nexperiments on logistic regression problems are finally conducted to show the\nadvantages over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 05:29:30 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 13:43:48 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhang", "Jiaqi", ""], ["You", "Keyou", ""], ["Ba\u015far", "Tamer", ""]]}, {"id": "2002.07399", "submitter": "Yikai Yan", "authors": "Yikai Yan, Chaoyue Niu, Yucheng Ding, Zhenzhe Zheng, Fan Wu, Guihai\n  Chen, Shaojie Tang, Zhihua Wu", "title": "Distributed Non-Convex Optimization with Sublinear Speedup under\n  Intermittent Client Availability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a new distributed machine learning framework, where a\nbunch of heterogeneous clients collaboratively train a model without sharing\ntraining data. In this work, we consider a practical and ubiquitous issue when\ndeploying federated learning in mobile environments: intermittent client\navailability, where the set of eligible clients may change during the training\nprocess. Such intermittent client availability would seriously deteriorate the\nperformance of the classical Federated Averaging algorithm (FedAvg for short).\nThus, we propose a simple distributed non-convex optimization algorithm, called\nFederated Latest Averaging (FedLaAvg for short), which leverages the latest\ngradients of all clients, even when the clients are not available, to jointly\nupdate the global model in each iteration. Our theoretical analysis shows that\nFedLaAvg attains the convergence rate of $O(E^{1/2}/(N^{1/4} T^{1/2}))$,\nachieving a sublinear speedup with respect to the total number of clients. We\nimplement FedLaAvg along with several baselines and evaluate them over the\nbenchmarking MNIST and Sentiment140 datasets. The evaluation results\ndemonstrate that FedLaAvg achieves more stable training than FedAvg in both\nconvex and non-convex settings and indeed reaches a sublinear speedup.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:32:18 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 15:13:20 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 02:47:23 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yan", "Yikai", ""], ["Niu", "Chaoyue", ""], ["Ding", "Yucheng", ""], ["Zheng", "Zhenzhe", ""], ["Wu", "Fan", ""], ["Chen", "Guihai", ""], ["Tang", "Shaojie", ""], ["Wu", "Zhihua", ""]]}, {"id": "2002.07403", "submitter": "Alexander Hentschel", "authors": "Alexander Hentschel, Yahya Hassanzadeh-Nazarabadi, Ramtin Seraj,\n  Dieter Shirley, Layne Lafrance", "title": "Flow: Separating Consensus and Compute -- Block Formation and Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current blockchains require all full nodes to execute all tasks limits\nthe throughput of existing blockchains, which are well documented and among the\nmost significant hurdles for the widespread adoption of decentralized\ntechnology.\n  This paper extends out presentation of Flow, a pipelined blockchain\narchitecture, which separates the process of consensus on the transaction order\nfrom transaction computation. As we experimentally showed in our previous white\npaper, our architecture provides a significant throughput improvement while\npreserving the security of the system. Flow exploits the heterogeneity offered\nby the nodes, in terms of bandwidth, storage, and computational capacity, and\ndefines the roles for the nodes based on their tasks in the pipeline, i.e.,\nCollector, Consensus, Execution, and Verification. While transaction collection\nfrom the user agents is completed through the bandwidth-optimized Collector\nNodes, the execution of them is done by the compute-optimized Execution Nodes.\nChecking the execution result is then distributed among a more extensive set of\nVerification Nodes, which confirm the result is correct in a distributed and\nparallel manner. In contrast to more traditional blockchain architectures,\nFlow's Consensus Nodes do not execute the transaction. Instead, Verification\nNodes report observed faulty executions to the Consensus Nodes, which\nadjudicate the received challenges and slash malicious actors.\n  In this paper, we detail the lifecycle of the transactions from the\nsubmission to the system until they are getting executed. The paper covers the\nCollector, Consensus, and Execution role. We provide a protocol specification\nof collecting the transactions, forming a block, and executing the resulting\nblock. Moreover, we elaborate on the safety and liveness of the system\nconcerning these processes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 06:48:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Hentschel", "Alexander", ""], ["Hassanzadeh-Nazarabadi", "Yahya", ""], ["Seraj", "Ramtin", ""], ["Shirley", "Dieter", ""], ["Lafrance", "Layne", ""]]}, {"id": "2002.07454", "submitter": "Yucheng Ding", "authors": "Yucheng Ding, Chaoyue Niu, Yikai Yan, Zhenzhe Zheng, Fan Wu, Guihai\n  Chen, Shaojie Tang, Rongfei Jia", "title": "Distributed Optimization over Block-Cyclic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider practical data characteristics underlying federated learning,\nwhere unbalanced and non-i.i.d. data from clients have a block-cyclic\nstructure: each cycle contains several blocks, and each client's training data\nfollow block-specific and non-i.i.d. distributions. Such a data structure would\nintroduce client and block biases during the collaborative training: the single\nglobal model would be biased towards the client or block specific data. To\novercome the biases, we propose two new distributed optimization algorithms\ncalled multi-model parallel SGD (MM-PSGD) and multi-chain parallel SGD\n(MC-PSGD) with a convergence rate of $O(1/\\sqrt{NT})$, achieving a linear\nspeedup with respect to the total number of clients. In particular, MM-PSGD\nadopts the block-mixed training strategy, while MC-PSGD further adds the\nblock-separate training strategy. Both algorithms create a specific predictor\nfor each block by averaging and comparing the historical global models\ngenerated in this block from different cycles. We extensively evaluate our\nalgorithms over the CIFAR-10 dataset. Evaluation results demonstrate that our\nalgorithms significantly outperform the conventional federated averaging\nalgorithm in terms of test accuracy, and also preserve robustness for the\nvariance of critical parameters.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 09:47:15 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ding", "Yucheng", ""], ["Niu", "Chaoyue", ""], ["Yan", "Yikai", ""], ["Zheng", "Zhenzhe", ""], ["Wu", "Fan", ""], ["Chen", "Guihai", ""], ["Tang", "Shaojie", ""], ["Jia", "Rongfei", ""]]}, {"id": "2002.07463", "submitter": "Geppino Pucci", "authors": "Andrea Pietracaprina, Geppino Pucci, Federico Sold\\`a", "title": "Coreset-based Strategies for Robust Center-type Problems", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset $V$ of points from some metric space, the popular $k$-center\nproblem requires to identify a subset of $k$ points (centers) in $V$ minimizing\nthe maximum distance of any point of $V$ from its closest center. The\n\\emph{robust} formulation of the problem features a further parameter $z$ and\nallows up to $z$ points of $V$ (outliers) to be disregarded when computing the\nmaximum distance from the centers. In this paper, we focus on two important\nconstrained variants of the robust $k$-center problem, namely, the Robust\nMatroid Center (RMC) problem, where the set of returned centers are constrained\nto be an independent set of a matroid of rank $k$ built on $V$, and the Robust\nKnapsack Center (RKC) problem, where each element $i\\in V$ is given a positive\nweight $w_i<1$ and the aggregate weight of the returned centers must be at most\n1. We devise coreset-based strategies for the two problems which yield\nefficient sequential, MapReduce, and Streaming algorithms. More specifically,\nfor any fixed $\\epsilon>0$, the algorithms return solutions featuring a\n$(3+\\epsilon)$-approximation ratio, which is a mere additive term $\\epsilon$\naway from the 3-approximations achievable by the best known polynomial-time\nsequential algorithms for the two problems. Moreover, the algorithms\nobliviously adapt to the intrinsic complexity of the dataset, captured by its\ndoubling dimension $D$. For wide ranges of the parameters $k,z,\\epsilon, D$, we\nobtain a sequential algorithm with running time linear in $|V|$, and\nMapReduce/Streaming algorithms with few rounds/passes and substantially\nsublinear local/working memory.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:04:08 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Pietracaprina", "Andrea", ""], ["Pucci", "Geppino", ""], ["Sold\u00e0", "Federico", ""]]}, {"id": "2002.07505", "submitter": "Hatem Khalloof", "authors": "Hatem Khalloof, Wilfried Jakob, Shadi Shahoud, Clemens Duepmeier and\n  Veit Hagenmeyer", "title": "A Scalable Method for Scheduling Distributed Energy Resources using\n  Parallelized Population-based Metaheuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an increasing integration of distributed renewable\nenergy resources into existing electric power grids. Due to the uncertain\nnature of renewable energy resources, network operators are faced with new\nchallenges in balancing load and generation. In order to meet the new\nrequirements, intelligent distributed energy resource plants can be used which\nprovide as virtual power plants e.g. demand side management or flexible\ngeneration. However, the calculation of an adequate schedule for the unit\ncommitment of such distributed energy resources is a complex optimization\nproblem which is typically too complex for standard optimization algorithms if\nlarge numbers of distributed energy resources are considered. For solving such\ncomplex optimization tasks, population-based metaheuristics -- as e.g.\nevolutionary algorithms -- represent powerful alternatives. Admittedly,\nevolutionary algorithms do require lots of computational power for solving such\nproblems in a timely manner. One promising solution for this performance\nproblem is the parallelization of the usually time-consuming evaluation of\nalternative solutions. In the present paper, a new generic and highly scalable\nparallel method for unit commitment of distributed energy resources using\nmetaheuristic algorithms is presented. It is based on microservices, container\nvirtualization and the publish/subscribe messaging paradigm for scheduling\ndistributed energy resources. Scalability and applicability of the proposed\nsolution are evaluated by performing parallelized optimizations in a big data\nenvironment for three distinct distributed energy resource scheduling\nscenarios. The new method provides cluster or cloud parallelizability and is\nable to deal with a comparably large number of distributed energy resources.\nThe application of the new proposed method results in very good performance for\nscaling up optimization speed.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 11:51:28 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 13:02:27 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Khalloof", "Hatem", ""], ["Jakob", "Wilfried", ""], ["Shahoud", "Shadi", ""], ["Duepmeier", "Clemens", ""], ["Hagenmeyer", "Veit", ""]]}, {"id": "2002.07509", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Rodrigo R. Barbieri, Enrique S. dos Santos, Gustavo M. D. Vieira", "title": "Decentralized Validation for Non-malicious Arbitrary Fault Tolerance in\n  Paxos", "comments": "14 pages", "journal-ref": "WTF '19: Proceedings of the XX Fault Tolerance Workshop, SBC,\n  2019, 34-47", "doi": "10.5753/wtf.2019.7713", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault-tolerant distributed systems offer high reliability because even if\nfaults in their components occur, they do not exhibit erroneous behavior.\nDepending on the fault model adopted, hardware and software errors that do not\nresult in a process crashing are usually not tolerated. To tolerate these\nrather common failures the usual solution is to adopt a stronger fault model,\nsuch as the arbitrary or Byzantine fault model. Algorithms created for this\nfault model, however, are considerably more complex and require more system\nresources than the ones developed for less strict fault models. One approach to\nreach a middle ground is the non-malicious arbitrary fault model. This model\nassumes it is possible to detect and filter faults with a given probability, if\nthese faults are not created with malicious intent, allowing the isolation and\nmapping of these faults to benign faults. In this paper we describe how we\nincremented an implementation of active replication in the non-malicious fault\nmodel with a basic type of distributed validation, where a deviation from the\nexpected algorithm behavior will make a process crash. We experimentally\nevaluate this implementation using a fault injection framework showing that it\nis feasible to extend the concept of non-malicious failures beyond hardware\nfailures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 11:58:09 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Barbieri", "Rodrigo R.", ""], ["Santos", "Enrique S. dos", ""], ["Vieira", "Gustavo M. D.", ""]]}, {"id": "2002.07512", "submitter": "Guntur Dharma Putra", "authors": "Guntur Dharma Putra, Volkan Dedeoglu, Salil S Kanhere, Raja Jurdak", "title": "Poster Abstract: Towards Scalable and Trustworthy Decentralized\n  Collaborative Intrusion Detection System for IoT", "comments": "Accepted to ACM/IEEE IoTDI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Intrusion Detection System (IDS) aims to alert users of incoming attacks\nby deploying a detector that monitors network traffic continuously. As an\neffort to increase detection capabilities, a set of independent IDS detectors\ntypically work collaboratively to build intelligence of holistic network\nrepresentation, which is referred to as Collaborative Intrusion Detection\nSystem (CIDS). However, developing an effective CIDS, particularly for the IoT\necosystem raises several challenges. Recent trends and advances in blockchain\ntechnology, which provides assurance in distributed trust and secure immutable\nstorage, may contribute towards the design of effective CIDS. In this poster\nabstract, we present our ongoing work on a decentralized CIDS for IoT, which is\nbased on blockchain technology. We propose an architecture that provides\naccountable trust establishment, which promotes incentives and penalties, and\nscalable intrusion information storage by exchanging bloom filters. We are\ncurrently implementing a proof-of-concept of our modular architecture in a\nlocal test-bed and evaluate its effectiveness in detecting common attacks in\nIoT networks and the associated overhead.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 12:15:26 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Putra", "Guntur Dharma", ""], ["Dedeoglu", "Volkan", ""], ["Kanhere", "Salil S", ""], ["Jurdak", "Raja", ""]]}, {"id": "2002.07539", "submitter": "Oded Naor", "authors": "Oded Naor, Idit Keidar", "title": "Expected Linear Round Synchronization: The Missing Link for Linear\n  Byzantine SMR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State Machine Replication (SMR) solutions often divide time into rounds, with\na designated leader driving decisions in each round. Progress is guaranteed\nonce all correct processes synchronize to the same round, and the leader of\nthat round is correct. Recently suggested Byzantine SMR solutions such as\nHotStuff, Tendermint, and LibraBFT achieve progress with a linear message\ncomplexity and a constant time complexity once such round synchronization\noccurs. But round synchronization itself incurs an additional cost. By Dolev\nand Reischuk's lower bound, any deterministic solution must have $\\Omega(n^2)$\ncommunication complexity. Yet the question of randomized round synchronization\nwith an expected linear message complexity remained open.\n  We present an algorithm that, for the first time, achieves round\nsynchronization with expected linear message complexity and expected constant\nlatency. Existing protocols can use our round synchronization algorithm to\nsolve Byzantine SMR with the same asymptotic performance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:11:51 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 07:40:26 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 08:24:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Naor", "Oded", ""], ["Keidar", "Idit", ""]]}, {"id": "2002.07545", "submitter": "EPTCS", "authors": "B\\'eatrice B\\'erard (Sorbonne Universit\\'e, CNRS, LIP6, Paris,\n  France), Benedikt Bollig (CNRS & LSV, ENS Paris-Saclay, Universit\\'e\n  Paris-Saclay, France), Patricia Bouyer (CNRS & LSV, ENS Paris-Saclay,\n  Universit\\'e Paris-Saclay, France), Matthias F\\\"ugger (CNRS & LSV, ENS\n  Paris-Saclay, Universit\\'e Paris-Saclay, Inria, France), Nathalie Sznajder\n  (Sorbonne Universit\\'e, CNRS, LIP6, Paris, France)", "title": "Synthesis in Presence of Dynamic Links", "comments": "In Proceedings GandALF 2020, arXiv:2009.09360", "journal-ref": "EPTCS 326, 2020, pp. 33-49", "doi": "10.4204/EPTCS.326.3", "report-no": null, "categories": "cs.FL cs.DC cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distributed synthesis is to automatically generate a\ndistributed algorithm, given a target communication network and a specification\nof the algorithm's correct behavior.\n  Previous work has focused on static networks with an a priori fixed message\nsize. This approach has two shortcomings: Recent work in distributed computing\nis shifting towards dynamically changing communication networks rather than\nstatic ones, and an important class of distributed algorithms are so-called\nfull-information protocols, where nodes piggy-pack previously received messages\nonto current messages.\n  In this work, we consider the synthesis problem for a system of two nodes\ncommunicating in rounds over a dynamic link whose message size is not bounded.\nGiven a network model, i.e., a set of link directions, in each round of the\nexecution, the adversary choses a link from the network model, restricted only\nby the specification, and delivers messages according to the current link's\ndirections. Motivated by communication buses with direct acknowledge\nmechanisms, we further assume that nodes are aware of which messages have been\ndelivered.\n  We show that the synthesis problem is decidable for a network model if and\nonly if it does not contain the empty link that dismisses both nodes' messages.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:21:24 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 01:24:22 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["B\u00e9rard", "B\u00e9atrice", "", "Sorbonne Universit\u00e9, CNRS, LIP6, Paris,\n  France"], ["Bollig", "Benedikt", "", "CNRS & LSV, ENS Paris-Saclay, Universit\u00e9\n  Paris-Saclay, France"], ["Bouyer", "Patricia", "", "CNRS & LSV, ENS Paris-Saclay,\n  Universit\u00e9 Paris-Saclay, France"], ["F\u00fcgger", "Matthias", "", "CNRS & LSV, ENS\n  Paris-Saclay, Universit\u00e9 Paris-Saclay, Inria, France"], ["Sznajder", "Nathalie", "", "Sorbonne Universit\u00e9, CNRS, LIP6, Paris, France"]]}, {"id": "2002.07565", "submitter": "Zhijie Ren", "authors": "Zhijie Ren, Ziheng Zhou", "title": "SURFACE: A Practical Blockchain Consensus Algorithm for Real-World\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SURFACE, standing for Secure, Use-case adaptive, and Relatively Fork-free\nApproach of Chain Extension, is a consensus algorithm that is designed for\nreal-world networks and enjoys the benefits from both the Nakamoto consensus\nand Byzantine Fault Tolerance (BFT) consensus. In SURFACE, a committee is\nrandomly selected every round to validate and endorse the proposed new block.\nThe size of the committee can be adjusted according to the underlying network\nto make the blockchain mostly fork-free with a reasonable overhead in\ncommunication. Consequently, the blockchain can normally achieve fast\nprobabilistic confirmation with high throughput and low latency. SURFACE also\nprovides a BFT mechanism to guarantee ledger consistency in case of an extreme\nnetwork situation such as large network partition or being under massive DDoS\nattacks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 13:58:11 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 05:47:37 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 05:55:26 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ren", "Zhijie", ""], ["Zhou", "Ziheng", ""]]}, {"id": "2002.07582", "submitter": "Indika Weerasingha Dewage", "authors": "Indika Kumara, Jun Han, Alan Colman, Willem-Jan van den Heuvel, Damian\n  A. Tamburri, Malinda Kapuruge", "title": "SDSN@RT: a middleware environment for single-instance multi-tenant cloud\n  applications", "comments": null, "journal-ref": "Software: Practice and Experience 49.5 (2019): 813-839", "doi": "10.1002/spe.2686", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the Single-Instance Multi-Tenancy (SIMT) model for composite\nSoftware-as-a-Service (SaaS) applications, a single composite application\ninstance can host multiple tenants, yielding the benefits of better service and\nresource utilization, and reduced operational cost for the SaaS provider. An\nSIMT application needs to share services and their aggregation (the\napplication) among its tenants while supporting variations in the functional\nand performance requirements of the tenants. The SaaS provider requires a\nmiddleware environment that can deploy, enact and manage a designed SIMT\napplication, to achieve the varied requirements of the different tenants in a\ncontrolled manner. This paper presents the SDSN@RT (Software-Defined Service\nNetworks @ RunTime) middleware environment that can meet the aforementioned\nrequirements. SDSN@RT represents an SIMT composite cloud application as a\nmulti-tenant service network, where the same service network simultaneously\nhosts a set of virtual service networks (VSNs), one for each tenant. A service\nnetwork connects a set of services, and coordinates the interactions between\nthem. A VSN realizes the requirements for a specific tenant and can be\ndeployed, configured, and logically isolated in the service network at runtime.\nSDSN@RT also supports the monitoring and runtime changes of the deployed\nmulti-tenant service networks. We show the feasibility of SDSN@RT with a\nprototype implementation, and demonstrate its capabilities to host SIMT\napplications and support their changes with a case study. The performance study\nof the prototype implementation shows that the runtime capabilities of our\nmiddleware incur little overhead.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 19:37:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kumara", "Indika", ""], ["Han", "Jun", ""], ["Colman", "Alan", ""], ["Heuvel", "Willem-Jan van den", ""], ["Tamburri", "Damian A.", ""], ["Kapuruge", "Malinda", ""]]}, {"id": "2002.07584", "submitter": "Alon Rashelbach", "authors": "Alon Rashelbach, Ori Rottenstreich, Mark Silberstein", "title": "A Computational Approach to Packet Classification", "comments": "To appear in SIGCOMM 2020", "journal-ref": null, "doi": "10.1145/3387514.3405886", "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-field packet classification is a crucial component in modern\nsoftware-defined data center networks. To achieve high throughput and low\nlatency, state-of-the-art algorithms strive to fit the rule lookup data\nstructures into on-die caches; however, they do not scale well with the number\nof rules. We present a novel approach, NuevoMatch, which improves the memory\nscaling of existing methods. A new data structure, Range Query Recursive Model\nIndex (RQ-RMI), is the key component that enables NuevoMatch to replace most of\nthe accesses to main memory with model inference computations. We describe an\nefficient training algorithm that guarantees the correctness of the\nRQ-RMI-based classification. The use of RQ-RMI allows the rules to be\ncompressed into model weights that fit into the hardware cache. Further, it\ntakes advantage of the growing support for fast neural network processing in\nmodern CPUs, such as wide vector instructions, achieving a rate of tens of\nnanoseconds per lookup. Our evaluation using 500K multi-field rules from the\nstandard ClassBench benchmark shows a geometric mean compression factor of\n4.9x, 8x, and 82x, and average performance improvement of 2.4x, 2.6x, and 1.6x\nin throughput compared to CutSplit, NeuroCuts, and TupleMerge, all\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:47:02 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 06:18:41 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Rashelbach", "Alon", ""], ["Rottenstreich", "Ori", ""], ["Silberstein", "Mark", ""]]}, {"id": "2002.07606", "submitter": "Ma\\\"el Guiraud", "authors": "Ma\\\"el Guiraud and Yann Strozecki", "title": "Scheduling periodic messages on a shared link", "comments": "22 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-RAN is a recent architecture for mobile networks where the processing\nunits are located in distant data-centers while, until now, they were attached\nto antennas. The main challenge, to fulfill protocol time constraints, is to\nguarantee a low latency for the periodic messages sent from each antenna to its\nprocessing unit and back. The problem we address is to find a sending scheme of\nthese periodic messages without contention nor buffering. We focus on a simple\nbut common star shaped topology, where all contentions are on a single link\nshared by all antennas. For messages of arbitrary size, we show that there is\nalways a solution as soon as the load of the network is less than $40\\%$.\nMoreover, we explain how we can restrict our study to message of size $1$\nwithout increasing too much the global latency. For message of size $1$, we\nprove that it is always possible to schedule them, when the load is less than\n$61\\%$ using a polynomial time algorithm. Moreover, using a simple random\ngreedy algorithm, we show that almost all instances of a given load admit a\nsolution, explaining why most greedy algorithms work so well in practice.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:58:13 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:00:17 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Guiraud", "Ma\u00ebl", ""], ["Strozecki", "Yann", ""]]}, {"id": "2002.07649", "submitter": "Mohamad Ahmadi", "authors": "Mohamad Ahmadi and Fabian Kuhn", "title": "Distributed Maximum Matching Verification in CONGEST", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum cardinality matching problem in a standard distributed\nsetting, where the nodes $V$ of a given $n$-node network graph $G=(V,E)$\ncommunicate over the edges $E$ in synchronous rounds. More specifically, we\nconsider the distributed CONGEST model, where in each round, each node of $G$\ncan send an $O(\\log n)$-bit message to each of its neighbors. We show that for\nevery graph $G$ and a matching $M$ of $G$, there is a randomized CONGEST\nalgorithm to verify $M$ being a maximum matching of $G$ in time $O(|M|)$ and\ndisprove it in time $O(D + \\ell)$, where $D$ is the diameter of $G$ and $\\ell$\nis the length of a shortest augmenting path. We hope that our algorithm\nconstitutes a significant step towards developing a CONGEST algorithm to\ncompute a maximum matching in time $\\tilde{O}(s^*)$, where $s^*$ is the size of\na maximum matching.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:39:54 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Ahmadi", "Mohamad", ""], ["Kuhn", "Fabian", ""]]}, {"id": "2002.07659", "submitter": "Jan Studen\\'y", "authors": "Yi-Jun Chang, Jan Studen\\'y, Jukka Suomela", "title": "Distributed graph problems through an automata-theoretic lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The locality of a graph problem is the smallest distance $T$ such that each\nnode can choose its own part of the solution based on its radius-$T$\nneighborhood. In many settings, a graph problem can be solved efficiently with\na distributed or parallel algorithm if and only if it has a small locality.\n  In this work we seek to automate the study of solvability and locality: given\nthe description of a graph problem $\\Pi$, we would like to determine if $\\Pi$\nis solvable and what is the asymptotic locality of $\\Pi$ as a function of the\nsize of the graph. Put otherwise, we seek to automatically synthesize efficient\ndistributed and parallel algorithms for solving $\\Pi$.\n  We focus on locally checkable graph problems; these are problems in which a\nsolution is globally feasible if it looks feasible in all constant-radius\nneighborhoods. Prior work on such problems has brought primarily bad news:\nquestions related to locality are undecidable in general, and even if we focus\non the case of labeled paths and cycles, determining locality is\n$\\mathsf{PSPACE}$-hard (Balliu et al., PODC 2019).\n  We complement prior negative results with efficient algorithms for the cases\nof unlabeled paths and cycles and, as an extension, for rooted trees. We\nintroduce a new automata-theoretic perspective for studying locally checkable\ngraph problems. We represent a locally checkable problem $\\Pi$ as a\nnondeterministic finite automaton $\\mathcal{M}$ over a unary alphabet. We\nidentify polynomial-time-computable properties of the automaton $\\mathcal{M}$\nthat near-completely capture the solvability and locality of $\\Pi$ in cycles\nand paths, with the exception of one specific case that is\n$\\mbox{co-$\\mathsf{NP}$}$-complete.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:47:08 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 19:06:38 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Studen\u00fd", "Jan", ""], ["Suomela", "Jukka", ""]]}, {"id": "2002.07672", "submitter": "Christoph Welzel", "authors": "Marius Bozga, Javier Esparza, Radu Iosif, Joseph Sifakis, Christoph\n  Welzel", "title": "Structural Invariants for the Verification of Systems with Parameterized\n  Architectures", "comments": "preprint; to be published in the proceedings of TACAS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider parameterized concurrent systems consisting of a finite but\nunknown number of components, obtained by replicating a given set of finite\nstate automata. Components communicate by executing atomic interactions whose\nparticipants update their states simultaneously. We introduce an interaction\nlogic to specify both the type of interactions (e.g.\\ rendez-vous, broadcast)\nand the topology of the system (e.g.\\ pipeline, ring). The logic can be easily\nembedded in monadic second order logic of finitely many successors, and is\ntherefore decidable.\n  Proving safety properties of such a parameterized system, like deadlock\nfreedom or mutual exclusion, requires to infer an inductive invariant that\ncontains all reachable states of all system instances, and no unsafe state. We\npresent a method to automatically synthesize inductive invariants directly from\nthe formula describing the interactions, without costly fixed point iterations.\nWe experimentally prove that this invariant is strong enough to verify safety\nproperties of a large number of systems including textbook examples (dining\nphilosophers, synchronization schemes), classical mutual exclusion algorithms,\ncache-coherence protocols and self-stabilization algorithms, for an arbitrary\nnumber of components.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:00:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bozga", "Marius", ""], ["Esparza", "Javier", ""], ["Iosif", "Radu", ""], ["Sifakis", "Joseph", ""], ["Welzel", "Christoph", ""]]}, {"id": "2002.07750", "submitter": "Zhen Chen", "authors": "Zhen Chen, Zhuqing Jia, Zhiying Wang and Syed A. Jafar", "title": "GCSA Codes with Noise Alignment for Secure Coded Multi-Party Batch\n  Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A secure multi-party batch matrix multiplication problem (SMBMM) is\nconsidered, where the goal is to allow a master to efficiently compute the\npairwise products of two batches of massive matrices, by distributing the\ncomputation across S servers. Any X colluding servers gain no information about\nthe input, and the master gains no additional information about the input\nbeyond the product. A solution called Generalized Cross Subspace Alignment\ncodes with Noise Alignment (GCSA-NA) is proposed in this work, based on\ncross-subspace alignment codes. The state of art solution to SMBMM is a coding\nscheme called polynomial sharing (PS) that was proposed by Nodehi and\nMaddah-Ali. GCSA-NA outperforms PS codes in several key aspects - more\nefficient and secure inter-server communication, lower latency, flexible\ninter-server network topology, efficient batch processing, and tolerance to\nstragglers. The idea of noise alignment can also be combined with N-source\nCross Subspace Alignment (N-CSA) codes and fast matrix multiplication\nalgorithms like Strassen's construction. Moreover, noise alignment can be\napplied to symmetric secure private information retrieval to achieve the\nasymptotic capacity.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:36:56 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 05:46:43 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Chen", "Zhen", ""], ["Jia", "Zhuqing", ""], ["Wang", "Zhiying", ""], ["Jafar", "Syed A.", ""]]}, {"id": "2002.07752", "submitter": "Prasanth Chatarasi", "authors": "Prasanth Chatarasi, Hyoukjun Kwon, Natesh Raina, Saurabh Malik,\n  Vaisakh Haridas, Angshuman Parashar, Michael Pellauer, Tushar Krishna, Vivek\n  Sarkar", "title": "Marvel: A Data-centric Compiler for DNN Operators on Spatial\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of a spatial DNN accelerator depends heavily on the compiler\nand its cost model ability to generate optimized mappings for various operators\nof DNN models on to the accelerator's compute and memory resources. But,\nexisting cost models lack a formal boundary over the operators for precise and\ntractable analysis, which poses adaptability challenges for new DNN operators.\nTo address this challenge, we leverage the recently introduced Maestro\nData-Centric (MDC) notation. We develop a formal understanding of DNN operators\nwhose mappings can be described in the MDC notation, because any mapping\nadhering to the notation is always analyzable by the MDC's cost model.\nFurthermore, we introduce a transformation for translating mappings into the\nMDC notation for exploring the mapping space.\n  Searching for the optimal mappings is challenging because of the large space\nof mappings, and this challenge gets exacerbated with new operators and diverse\naccelerator configurations.To address this challenge, we propose a decoupled\noff-chip/on-chip approach that decomposes the mapping space into off-chip and\non-chip subspaces, and first optimizes the off-chip subspace followed by the\non-chip subspace. The motivation for this decomposition is to reduce the size\nof the search space dramatically and also to prioritize the optimization of\noff-chip data movement, which is 2-3 orders of magnitude more compared to the\non-chip data movement. We implemented our approach in a tool called {\\em\nMarvel}, and another major benefit of our approach is that it is applicable to\nany DNN operator conformable with the MDC notation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:39:21 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 19:08:00 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Chatarasi", "Prasanth", ""], ["Kwon", "Hyoukjun", ""], ["Raina", "Natesh", ""], ["Malik", "Saurabh", ""], ["Haridas", "Vaisakh", ""], ["Parashar", "Angshuman", ""], ["Pellauer", "Michael", ""], ["Krishna", "Tushar", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2002.07795", "submitter": "Yehia Arafa", "authors": "Yehia Arafa, Ammar ElWazir, Abdelrahman ElKanishy, Youssef Aly,\n  Ayatelrahman Elsayed, Abdel-Hameed Badawy, Gopinath Chennupati, Stephan\n  Eidenbenz, and Nandakishore Santhi", "title": "Verified Instruction-Level Energy Consumption Measurement for NVIDIA\n  GPUs", "comments": null, "journal-ref": null, "doi": "10.1145/3387902.3392613", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are prevalent in modern computing systems at all scales. They consume a\nsignificant fraction of the energy in these systems. However, vendors do not\npublish the actual cost of the power/energy overhead of their internal\nmicroarchitecture. In this paper, we accurately measure the energy consumption\nof various PTX instructions found in modern NVIDIA GPUs. We provide an\nexhaustive comparison of more than 40 instructions for four high-end NVIDIA\nGPUs from four different generations (Maxwell, Pascal, Volta, and Turing).\nFurthermore, we show the effect of the CUDA compiler optimizations on the\nenergy consumption of each instruction. We use three different software\ntechniques to read the GPU on-chip power sensors, which use NVIDIA's NVML API\nand provide an in-depth comparison between these techniques. Additionally, we\nverified the software measurement techniques against a custom-designed hardware\npower measurement. The results show that Volta GPUs have the best energy\nefficiency of all the other generations for the different categories of the\ninstructions. This work should aid in understanding NVIDIA GPUs'\nmicroarchitecture. It should also make energy measurements of any GPU kernel\nboth efficient and accurate.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:49:24 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:58:14 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Arafa", "Yehia", ""], ["ElWazir", "Ammar", ""], ["ElKanishy", "Abdelrahman", ""], ["Aly", "Youssef", ""], ["Elsayed", "Ayatelrahman", ""], ["Badawy", "Abdel-Hameed", ""], ["Chennupati", "Gopinath", ""], ["Eidenbenz", "Stephan", ""], ["Santhi", "Nandakishore", ""]]}, {"id": "2002.07800", "submitter": "Krzysztof Nowicki", "authors": "Krzysztof Nowicki, Krzysztof Onak", "title": "Dynamic Graph Algorithms with Batch Updates in the Massively Parallel\n  Computation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic graph algorithms in the Massively Parallel Computation\nmodel, which was inspired by practical data processing systems. Our goal is to\nprovide algorithms that can efficiently handle large batches of edge insertions\nand deletions.\n  We show algorithms that require fewer rounds to update a solution to problems\nsuch as Minimum Spanning Forest, 2-Edge Connected Components, and Maximal\nMatching than would be required by their static counterparts to compute it from\nscratch. They work in the most restrictive memory regime, in which local memory\nper machine is strongly sublinear in the number of graph vertices. Improving on\nthe size of the batch they can handle efficiently would improve on the round\ncomplexity of known static algorithms on sparse graphs.\n  Our algorithms can process batches of updates of size $\\Theta(S)$, for\nMinimum Spanning Forest and 2-Edge Connected Components, and\n$\\Theta(S^{1-\\varepsilon})$, for Maximal Matching, in $O(1)$ rounds, where $S$\nis the local memory of a single machine.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:54:32 GMT"}, {"version": "v2", "created": "Mon, 11 Jan 2021 17:52:10 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Nowicki", "Krzysztof", ""], ["Onak", "Krzysztof", ""]]}, {"id": "2002.07904", "submitter": "Michael Luby", "authors": "Michael Luby", "title": "Repair rate lower bounds for distributed storage", "comments": "18 pages, 3 figures, submitted to IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary objectives of a distributed storage system is to reliably\nstore a large amount $dsize$ of source data for a long duration using a large\nnumber $N$ of unreliable storage nodes, each with capacity $nsize$. The storage\noverhead $\\beta$ is the fraction of system capacity available beyond $dsize$,\ni.e., $\\beta = 1- \\frac{dsize}{N \\cdot nsize}$. Storage nodes fail randomly\nover time and are replaced with initially empty nodes, and thus data is erased\nfrom the system at an average rate $erate = \\lambda \\cdot N \\cdot nsize$, where\n$1/\\lambda$ is the average lifetime of a node before failure. To maintain\nrecoverability of the source data, a repairer continually reads data over a\nnetwork from nodes at some average rate $rrate$, and generates and writes data\nto nodes based on the read data. The main result is that, for any repairer, if\nthe source data is recoverable at each point in time then it must be the case\nthat $rrate \\ge \\frac{erate}{2 \\cdot \\beta}$ asymptotically as $N$ goes to\ninfinity and beta goes to zero. This inequality provides a fundamental lower\nbound on the average rate that any repairer needs to read data from the system\nin order to maintain recoverability of the source data.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:17:02 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Luby", "Michael", ""]]}, {"id": "2002.07970", "submitter": "Patrick Diehl", "authors": "Tianyi Zhang, Shahrzad Shirzad, Bibek Wagle, Adrian S. Lemoine,\n  Patrick Diehl, and Hartmut Kaiser", "title": "Supporting OpenMP 5.0 Tasks in hpxMP -- A study of an OpenMP\n  implementation within Task Based Runtime Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OpenMP has been the de facto standard for single node parallelism for more\nthan a decade. Recently, asynchronous many-task runtime (AMT) systems have\nincreased in popularity as a new programming paradigm for high performance\ncomputing applications. One of the major challenges of this new paradigm is the\nincompatibility of the OpenMP thread model and other AMTs. Highly optimized\nOpenMP-based libraries do not perform well when coupled with AMTs because the\nthreading of both libraries will compete for resources. This paper is a\nfollow-up paper on the fundamental implementation of hpxMP, an implementation\nof the OpenMP standard which utilizes the C++ standard library for Parallelism\nand Concurrency (HPX) to schedule and manage tasks. In this paper, we present\nthe implementation of task features, e.g. taskgroup, task depend, and\ntask_reduction, of the OpenMP 5.0 standard and optimization of the #pragma omp\nparallel for pragma. We use the daxpy benchmark, the Barcelona OpenMP Tasks\nSuite, Parallel research kernels, and OpenBLAS benchmarks to compare the\ndifferent OpenMp implementations: hpxMP, llvm-OpenMP, and GOMP.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:50:04 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhang", "Tianyi", ""], ["Shirzad", "Shahrzad", ""], ["Wagle", "Bibek", ""], ["Lemoine", "Adrian S.", ""], ["Diehl", "Patrick", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2002.08027", "submitter": "Minghong Fang", "authors": "Minghong Fang, Jia Liu", "title": "Toward Low-Cost and Stable Blockchain Networks", "comments": "Accepted by IEEE ICC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envisioned to be the future of secured distributed systems, blockchain\nnetworks have received increasing attention from both the industry and academia\nin recent years. However, blockchain mining processes demand high hardware\ncosts and consume a vast amount of energy (studies have shown that the amount\nof energy consumed in Bitcoin mining is almost the same as the electricity used\nin Ireland). To address the high mining cost problem of blockchain networks, in\nthis paper, we propose a blockchain mining resources allocation algorithm to\nreduce the mining cost in PoW-based (proof-of-work-based) blockchain networks.\nWe first propose an analytical queueing model for general blockchain networks.\nIn our queueing model, transactions arrive randomly to the queue and are served\nin a batch manner with unknown service rate probability distribution and\nagnostic to any priority mechanism. Then, we leverage the Lyapunov optimization\ntechniques to propose a dynamic mining resources allocation algorithm (DMRA),\nwhich is parameterized by a tuning parameter $K>0$. We show that our algorithm\nachieves an $[O(1/K), O(K)]$ cost-optimality-gap-vs-delay tradeoff. Our\nsimulation results also demonstrate the effectiveness of DMRA in reducing\nmining costs.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 06:42:33 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 20:39:50 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Fang", "Minghong", ""], ["Liu", "Jia", ""]]}, {"id": "2002.08088", "submitter": "Marco D'Amico", "authors": "Marco D'Amico and Ana Jokanovic and Julita Corbalan", "title": "Holistic Slowdown Driven Scheduling and Resource Management for\n  Malleable Jobs", "comments": null, "journal-ref": "In Proceedings of the 48th International Conference on Parallel\n  Processing (ICPP 2019). Association for Computing Machinery, New York, NY,\n  USA, Article 31", "doi": "10.1145/3337821.3337909", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In job scheduling, the concept of malleability has been explored since many\nyears ago. Research shows that malleability improves system performance, but\nits utilization in HPC never became widespread. The causes are the difficulty\nin developing malleable applications, and the lack of support and integration\nof the different layers of the HPC software stack. However, in the last years,\nmalleability in job scheduling is becoming more critical because of the\nincreasing complexity of hardware and workloads. In this context, using nodes\nin an exclusive mode is not always the most efficient solution as in\ntraditional HPC jobs, where applications were highly tuned for static\nallocations, but offering zero flexibility to dynamic executions. This paper\nproposes a new holistic, dynamic job scheduling policy, Slowdown Driven\n(SD-Policy), which exploits the malleability of applications as the key\ntechnology to reduce the average slowdown and response time of jobs. SD-Policy\nis based on backfill and node sharing. It applies malleability to running jobs\nto make room for jobs that will run with a reduced set of resources, only when\nthe estimated slowdown improves over the static approach. We implemented\nSD-Policy in SLURM and evaluated it in a real production environment, and with\na simulator using workloads of up to 198K jobs. Results show better resource\nutilization with the reduction of makespan, response time, slowdown, and energy\nconsumption, up to respectively 7%, 50%, 70%, and 6%, for the evaluated\nworkloads.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 09:53:14 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["D'Amico", "Marco", ""], ["Jokanovic", "Ana", ""], ["Corbalan", "Julita", ""]]}, {"id": "2002.08101", "submitter": "Martin Florian", "authors": "Martin Florian, Sebastian Henningsen, Charmaine Ndolo, Bj\\\"orn\n  Scheuermann", "title": "The Sum of Its Parts: Analysis of Federated Byzantine Agreement Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Byzantine Agreement Systems (FBASs) are a fascinating new paradigm\nin the context of consensus protocols. Originally proposed for powering the\nStellar payment network, FBASs can instantiate Byzantine quorum systems without\nrequiring out-of-band agreement on a common set of validators; every node is\nfree to decide for itself with whom it requires agreement. Sybil-resistant and\nyet energy-efficient consensus protocols can therefore be built upon FBASs, and\nthe \"decentrality\" possible with the FBAS paradigm might be sufficient to\nreduce the use of environmentally unsustainable proof-of-work protocols. In\nthis paper, we first demonstrate how the robustness of individual FBASs can be\ndetermined, by precisely determining their safety and liveness buffers and\ntherefore enabling a comparison with threshold-based quorum systems. Using\nsimulations and example node configuration strategies, we then empirically\ninvestigate the hypothesis that while FBASs can be bootstrapped in a bottom-up\nfashion from individual preferences, strategic considerations should\nadditionally be applied by node operators in order to arrive at FBASs that are\nrobust and amenable to monitoring. Finally, we investigate the reported\n\"open-membership\" property of FBASs. We observe that an often small group of\nnodes is exclusively relevant for determining safety and liveness buffers, and\nprove that membership in this top tier is conditional on the approval by\ncurrent top tier nodes if maintaining safety is a core requirement.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:57:21 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:53:09 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Florian", "Martin", ""], ["Henningsen", "Sebastian", ""], ["Ndolo", "Charmaine", ""], ["Scheuermann", "Bj\u00f6rn", ""]]}, {"id": "2002.08161", "submitter": "Salvatore Cielo", "authors": "Salvatore Cielo, Luigi Iapichino, Fabio Baruffa, Matteo Bugli and\n  Christoph Federrath", "title": "Honing and proofing Astrophysical codes on the road to Exascale.\n  Experiences from code modernization on many-core systems", "comments": "16 pages, 10 figures, 4 tables. To be published in Future Generation\n  of Computer Systems (FGCS), Special Issue on \"On The Road to Exascale II:\n  Advances in High Performance Computing and Simulations\"", "journal-ref": "Future Generation of Computer Systems,Volume 112, November 2020,\n  Pages 93-107", "doi": "10.1016/j.future.2020.05.003", "report-no": null, "categories": "cs.DC astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The complexity of modern and upcoming computing architectures poses severe\nchallenges for code developers and application specialists, and forces them to\nexpose the highest possible degree of parallelism, in order to make the best\nuse of the available hardware. The Intel$^{(R)}$ Xeon Phi$^{(TM)}$ of second\ngeneration (code-named Knights Landing, henceforth KNL) is the latest many-core\nsystem, which implements several interesting hardware features like for example\na large number of cores per node (up to 72), the 512 bits-wide vector registers\nand the high-bandwidth memory. The unique features of KNL make this platform a\npowerful testbed for modern HPC applications. The performance of codes on KNL\nis therefore a useful proxy of their readiness for future architectures. In\nthis work we describe the lessons learnt during the optimisation of the widely\nused codes for computational astrophysics P-Gadget-3, Flash and Echo. Moreover,\nwe present results for the visualisation and analysis tools VisIt and yt. These\nexamples show that modern architectures benefit from code optimisation at\ndifferent levels, even more than traditional multi-core systems. However, the\nlevel of modernisation of typical community codes still needs improvements, for\nthem to fully utilise resources of novel architectures.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 13:18:01 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Cielo", "Salvatore", ""], ["Iapichino", "Luigi", ""], ["Baruffa", "Fabio", ""], ["Bugli", "Matteo", ""], ["Federrath", "Christoph", ""]]}, {"id": "2002.08216", "submitter": "Dennis Olivetti", "authors": "Sebastian Brandt, Dennis Olivetti", "title": "Truly Tight-in-$\\Delta$ Bounds for Bipartite Maximal Matching and\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent breakthrough result, Balliu et al. [FOCS'19] proved a\ndeterministic $\\Omega(\\min(\\Delta,\\log n /\\log \\log n))$-round and a randomized\n$\\Omega(\\min(\\Delta,\\log \\log n/\\log \\log \\log n))$-round lower bound for the\ncomplexity of the bipartite maximal matching problem on $n$-node graphs in the\nLOCAL model of distributed computing.\n  Both lower bounds are asymptotically tight as a function of the maximum\ndegree $\\Delta$.\n  We provide truly tight bounds in $\\Delta$ for the complexity of bipartite\nmaximal matching and many natural variants, up to and including the additive\nconstant.\n  As a by-product, our results yield a considerably simplified version of the\nproof by Balliu et al.\n  We show that our results can be obtained via bounded automatic round\nelimination, a version of the recent automatic round elimination technique by\nBrandt [PODC'19] that is particularly suited for automatization from a\npractical perspective.\n  In this context, our work can be seen as another step towards the\nautomatization of lower bounds in the LOCAL model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 14:42:39 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Brandt", "Sebastian", ""], ["Olivetti", "Dennis", ""]]}, {"id": "2002.08233", "submitter": "Md. Khaledur Rahman", "authors": "Md. Khaledur Rahman, Majedul Haque Sujon, Ariful Azad", "title": "BatchLayout: A Batch-Parallel Force-Directed Graph Layout Algorithm in\n  Shared Memory", "comments": "Accepted for publication in PacificVis 2020", "journal-ref": "IEEE Pacific Visualization Symposium (PacificVis 2020)", "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Force-directed algorithms are widely used to generate aesthetically pleasing\nlayouts of graphs or networks arisen in many scientific disciplines. To\nvisualize large-scale graphs, several parallel algorithms have been discussed\nin the literature. However, existing parallel algorithms do not utilize memory\nhierarchy efficiently and often offer limited parallelism. This paper addresses\nthese limitations with BatchLayout, an algorithm that groups vertices into\nminibatches and processes them in parallel. BatchLayout also employs cache\nblocking techniques to utilize memory hierarchy efficiently. More parallelism\nand improved memory accesses coupled with force approximating techniques,\nbetter initialization, and optimized learning rate make BatchLayout\nsignificantly faster than other state-of-the-art algorithms such as ForceAtlas2\nand OpenOrd. The visualization quality of layouts from BatchLayout is\ncomparable or better than similar visualization tools. All of our source code,\nlinks to datasets, results and log files are available at\nhttps://github.com/khaled-rahman/BatchLayout.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 20:06:32 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Rahman", "Md. Khaledur", ""], ["Sujon", "Majedul Haque", ""], ["Azad", "Ariful", ""]]}, {"id": "2002.08295", "submitter": "Abdul Dakkak", "authors": "Abdul Dakkak, Cheng Li, Jinjun Xiong, Wen-mei Hwu", "title": "MLModelScope: A Distributed Platform for Model Evaluation and\n  Benchmarking at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Learning (ML) and Deep Learning (DL) innovations are being introduced\nat such a rapid pace that researchers are hard-pressed to analyze and study\nthem. The complicated procedures for evaluating innovations, along with the\nlack of standard and efficient ways of specifying and provisioning ML/DL\nevaluation, is a major \"pain point\" for the community. This paper proposes\nMLModelScope, an open-source, framework/hardware agnostic, extensible and\ncustomizable design that enables repeatable, fair, and scalable model\nevaluation and benchmarking. We implement the distributed design with support\nfor all major frameworks and hardware, and equip it with web, command-line, and\nlibrary interfaces. To demonstrate MLModelScope's capabilities we perform\nparallel evaluation and show how subtle changes to model evaluation pipeline\naffects the accuracy and HW/SW stack choices affect performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:13:01 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Dakkak", "Abdul", ""], ["Li", "Cheng", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2002.08299", "submitter": "Quanquan C. Liu", "authors": "Amartya Shankha Biswas, Talya Eden, Quanquan C. Liu, Slobodan\n  Mitrovi\\'c, Ronitt Rubinfeld", "title": "Parallel Algorithms for Small Subgraph Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counting is a fundamental problem in analyzing massive graphs, often\nstudied in the context of social and complex networks. There is a rich\nliterature on designing efficient, accurate, and scalable algorithms for this\nproblem. In this work, we tackle this challenge and design several new\nalgorithms for subgraph counting in the Massively Parallel Computation (MPC)\nmodel:\n  Given a graph $G$ over $n$ vertices, $m$ edges and $T$ triangles, our first\nmain result is an algorithm that, with high probability, outputs a\n$(1+\\varepsilon)$-approximation to $T$, with optimal round and space complexity\nprovided any $S \\geq \\max{(\\sqrt m, n^2/m)}$ space per machine, assuming\n$T=\\Omega(\\sqrt{m/n})$.\n  Our second main result is an $\\tilde{O}_{\\delta}(\\log \\log n)$-rounds\nalgorithm for exactly counting the number of triangles, parametrized by the\narboricity $\\alpha$ of the input graph. The space per machine is\n$O(n^{\\delta})$ for any constant $\\delta$, and the total space is $O(m\\alpha)$,\nwhich matches the time complexity of (combinatorial) triangle counting in the\nsequential model. We also prove that this result can be extended to exactly\ncounting $k$-cliques for any constant $k$, with the same round complexity and\ntotal space $O(m\\alpha^{k-2})$. Alternatively, allowing $O(\\alpha^2)$ space per\nmachine, the total space requirement reduces to $O(n\\alpha^2)$.\n  Finally, we prove that a recent result of Bera, Pashanasangi and Seshadhri\n(ITCS 2020) for exactly counting all subgraphs of size at most $5$, can be\nimplemented in the MPC model in $\\tilde{O}_{\\delta}(\\sqrt{\\log n})$ rounds,\n$O(n^{\\delta})$ space per machine and $O(m\\alpha^3)$ total space. Therefore,\nthis result also exhibits the phenomenon that a time bound in the sequential\nmodel translates to a space bound in the MPC model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 17:16:41 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 17:51:56 GMT"}, {"version": "v3", "created": "Sat, 30 May 2020 03:32:48 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Biswas", "Amartya Shankha", ""], ["Eden", "Talya", ""], ["Liu", "Quanquan C.", ""], ["Mitrovi\u0107", "Slobodan", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "2002.08326", "submitter": "Cong Guo", "authors": "Cong Guo, Yangjie Zhou, Jingwen Leng, Yuhao Zhu, Zidong Du, Quan Chen,\n  Chao Li, Bin Yao and Minyi Guo", "title": "Balancing Efficiency and Flexibility for DNN Acceleration via Temporal\n  GPU-Systolic Array Integration", "comments": "6 pages, 9 figures, DAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research interest in specialized hardware accelerators for deep neural\nnetworks (DNN) spikes recently owing to their superior performance and\nefficiency. However, today's DNN accelerators primarily focus on accelerating\nspecific \"kernels\" such as convolution and matrix multiplication, which are\nvital but only part of an end-to-end DNN-enabled application. Meaningful\nspeedups over the entire application often require supporting computations that\nare, while massively parallel, ill-suited to DNN accelerators. Integrating a\ngeneral-purpose processor such as a CPU or a GPU incurs significant data\nmovement overhead and leads to resource under-utilization on the DNN\naccelerators.\n  We propose Simultaneous Multi-mode Architecture (SMA), a novel architecture\ndesign and execution model that offers general-purpose programmability on DNN\naccelerators in order to accelerate end-to-end applications. The key to SMA is\nthe temporal integration of the systolic execution model with the GPU-like SIMD\nexecution model. The SMA exploits the common components shared between the\nsystolic-array accelerator and the GPU, and provides lightweight\nreconfiguration capability to switch between the two modes in-situ. The SMA\nachieves up to 63% performance improvement while consuming 23% less energy than\nthe baseline Volta architecture with TensorCore.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:44:20 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 10:27:55 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Guo", "Cong", ""], ["Zhou", "Yangjie", ""], ["Leng", "Jingwen", ""], ["Zhu", "Yuhao", ""], ["Du", "Zidong", ""], ["Chen", "Quan", ""], ["Li", "Chao", ""], ["Yao", "Bin", ""], ["Guo", "Minyi", ""]]}, {"id": "2002.08423", "submitter": "Vaikkunth Mugunthan", "authors": "Vaikkunth Mugunthan, Anton Peraire-Bueno and Lalana Kagal", "title": "PrivacyFL: A simulator for privacy-preserving and secure federated\n  learning", "comments": null, "journal-ref": null, "doi": "10.1145/3340531.3412771", "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a technique that enables distributed clients to\ncollaboratively learn a shared machine learning model while keeping their\ntraining data localized. This reduces data privacy risks, however, privacy\nconcerns still exist since it is possible to leak information about the\ntraining dataset from the trained model's weights or parameters. Setting up a\nfederated learning environment, especially with security and privacy\nguarantees, is a time-consuming process with numerous configurations and\nparameters that can be manipulated. In order to help clients ensure that\ncollaboration is feasible and to check that it improves their model accuracy, a\nreal-world simulator for privacy-preserving and secure federated learning is\nrequired. In this paper, we introduce PrivacyFL, which is an extensible, easily\nconfigurable and scalable simulator for federated learning environments. Its\nkey features include latency simulation, robustness to client departure,\nsupport for both centralized and decentralized learning, and configurable\nprivacy and security mechanisms based on differential privacy and secure\nmultiparty computation. In this paper, we motivate our research, describe the\narchitecture of the simulator and associated protocols, and discuss its\nevaluation in numerous scenarios that highlight its wide range of functionality\nand its advantages. Our paper addresses a significant real-world problem:\nchecking the feasibility of participating in a federated learning environment\nunder a variety of circumstances. It also has a strong practical impact because\norganizations such as hospitals, banks, and research institutes, which have\nlarge amounts of sensitive data and would like to collaborate, would greatly\nbenefit from having a system that enables them to do so in a privacy-preserving\nand secure manner.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:16:13 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 03:30:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Mugunthan", "Vaikkunth", ""], ["Peraire-Bueno", "Anton", ""], ["Kagal", "Lalana", ""]]}, {"id": "2002.08765", "submitter": "Tyler Crain", "authors": "Tyler Crain", "title": "Two More Algorithms for Randomized Signature-Free Asynchronous Binary\n  Byzantine Consensus with $t < n/3$ and $O(n^2)$ Messages and $O(1)$ Round\n  Expected Termination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes two randomized, asynchronous, round based, Binary\nByzantine faulty tolerant consensus algorithms based on the algorithms of [25]\nand [26]. Like the algorithms of [25] and [26] they do not use signatures, use\n$O(n^2)$ messages per round (where each message is composed of a round number\nand a constant number of bits), tolerate up to one third failures, and have\nexpected termination in constant number of rounds.\n  The first, like [26], uses a weak common coin (i.e. one that can return\ndifferent values at different processes with a constant probability) to ensure\ntermination. The algorithm consists of $5$ to $7$ message broadcasts per round.\nAn optimization is described that reduces this to $4$ to $5$ broadcasts per\nround for rounds following the first round. Comparatively, [26] consists of $8$\nto $12$ message broadcasts per round.\n  The second algorithm, like [25], uses a perfect common coin (i.e. one that\nreturns the same value at all non-faulty processes) for both termination and\ncorrectness. Unlike [25], it does not require a fair scheduler to ensure\ntermination. Furthermore, the algorithm consists of $2$ to $3$ message\nbroadcasts for the first round and $1$ to $2$ broadcasts for the following\nrounds, while [29] consists of $2$ to $3$ broadcasts per round.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:49:59 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Crain", "Tyler", ""]]}, {"id": "2002.08863", "submitter": "Hans van Ditmarsch", "authors": "Hans van Ditmarsch, Eric Goubault, Jeremy Ledent, Sergio Rajsbaum", "title": "Knowledge and simplicial complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes are a versatile and convenient paradigm on which to\nbuild all the tools and techniques of the logic of knowledge, on the assumption\nthat initial epistemic models can be described in a distributed fashion. Thus,\nwe can define: knowledge, belief, bisimulation, the group notions of mutual,\ndistributed and common knowledge, and also dynamics in the shape of simplicial\naction models. We give a survey on how to interpret all such notions on\nsimplicial complexes, building upon the foundations laid in prior work by\nGoubault and others.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:03:19 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["van Ditmarsch", "Hans", ""], ["Goubault", "Eric", ""], ["Ledent", "Jeremy", ""], ["Rajsbaum", "Sergio", ""]]}, {"id": "2002.08886", "submitter": "Navid Hashemi Tonekaboni", "authors": "Navid Hashemi Tonekaboni, Lakshmish Ramaswamy, Deepak Mishra, Sorush\n  Omidvar", "title": "Spatio-Temporal Coverage Enhancement in Drive-By Sensing Through\n  Utility-Aware Mobile Agent Selection", "comments": "10 pages, 19 figures, IEEE International Conference on Mobile Data\n  Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the drive-by sensing paradigm has become increasingly\npopular for cost-effective monitoring of urban areas. Drive-by sensing is a\nform of crowdsensing wherein sensor-equipped vehicles (aka, mobile agents) are\nthe primary data gathering agents. Enhancing the efficacy of drive-by sensing\nposes many challenges, an important one of which is to select non-dedicated\nmobile agents on which a limited number of sensors are to be mounted. This\nproblem, which we refer to as the mobile-agent selection problem, has a\nsignificant impact on the spatio-temporal coverage of the drive-by sensing\nplatforms and the resultant datasets. The challenge here is to achieve maximum\nspatiotemporal coverage while taking the relative importance levels of\ngeographical areas into account. In this paper, we address this problem in the\ncontext of the SCOUTS project, the goal of which is to map and analyze the\nurban heat island phenomenon accurately.\n  Our work makes several major technical contributions. First, we delineate a\nmodel for representing the mobile agents selection problem. This model takes\ninto account the trajectories of the vehicles (public transportation buses in\nour case) and the relative importance of the urban regions, and formulates it\nas an optimization problem. Second, we provide two algorithms that are based\nupon the utility (coverage) values of mobile agents, namely, a hotspot-based\nalgorithm that limits the search space to important sub-regions and a\nutility-aware genetic algorithm that enables the latter algorithm to make\nunbiased selections. Third, we design a highly efficient coverage redundancy\nminimization algorithm that, at each step, chooses the mobile agent, which\nprovides maximal improvement to the spatio-temporal coverage. This paper\nreports a series of experiments on a real-world dataset from Athens, GA, USA,\nto demonstrate the effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 04:23:12 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Tonekaboni", "Navid Hashemi", ""], ["Ramaswamy", "Lakshmish", ""], ["Mishra", "Deepak", ""], ["Omidvar", "Sorush", ""]]}, {"id": "2002.08892", "submitter": "Ankit Singh Rawat", "authors": "Venkata Gandikota, Arya Mazumdar, Ankit Singh Rawat", "title": "Reliable Distributed Clustering with Redundant Data Assignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present distributed generalized clustering algorithms that\ncan handle large scale data across multiple machines in spite of straggling or\nunreliable machines. We propose a novel data assignment scheme that enables us\nto obtain global information about the entire data even when some machines fail\nto respond with the results of the assigned local computations. The assignment\nscheme leads to distributed algorithms with good approximation guarantees for a\nvariety of clustering and dimensionality reduction problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:44:37 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Gandikota", "Venkata", ""], ["Mazumdar", "Arya", ""], ["Rawat", "Ankit Singh", ""]]}, {"id": "2002.08908", "submitter": "Xingyu Zhou", "authors": "Xingyu Zhou, Ness Shroff and Adam Wierman", "title": "Asymptotically Optimal Load Balancing in Large-scale Heterogeneous\n  Systems with Multiple Dispatchers", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the load balancing problem in large-scale heterogeneous systems\nwith multiple dispatchers. We introduce a general framework called\nLocal-Estimation-Driven (LED). Under this framework, each dispatcher keeps\nlocal (possibly outdated) estimates of queue lengths for all the servers, and\nthe dispatching decision is made purely based on these local estimates. The\nlocal estimates are updated via infrequent communications between dispatchers\nand servers. We derive sufficient conditions for LED policies to achieve\nthroughput optimality and delay optimality in heavy-traffic, respectively.\nThese conditions directly imply delay optimality for many previous local-memory\nbased policies in heavy traffic. Moreover, the results enable us to design new\ndelay optimal policies for heterogeneous systems with multiple dispatchers.\nFinally, the heavy-traffic delay optimality of the LED framework directly\nresolves a recent open problem on how to design optimal load balancing schemes\nusing delayed information.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:52:53 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zhou", "Xingyu", ""], ["Shroff", "Ness", ""], ["Wierman", "Adam", ""]]}, {"id": "2002.08947", "submitter": "Hanrui Wang", "authors": "Zhekai Zhang, Hanrui Wang, Song Han, William J. Dally", "title": "SpArch: Efficient Architecture for Sparse Matrix Multiplication", "comments": "The first two authors have equal contributions; 15 pages, 18 figures;\n  Published as a conference paper in HPCA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Sparse Matrix-Matrix Multiplication (SpGEMM) is a ubiquitous task\nin various engineering and scientific applications. However, inner product\nbased SpGENN introduces redundant input fetches for mismatched nonzero\noperands, while outer product based approach suffers from poor output locality\ndue to numerous partial product matrices. Inefficiency in the reuse of either\ninputs or outputs data leads to extensive and expensive DRAM access.\n  To address this problem, this paper proposes an efficient sparse matrix\nmultiplication accelerator architecture, SpArch, which jointly optimizes the\ndata locality for both input and output matrices. We first design a highly\nparallelized streaming-based merger to pipeline the multiply and merge stage of\npartial matrices so that partial matrices are merged on chip immediately after\nproduced. We then propose a condensed matrix representation that reduces the\nnumber of partial matrices by three orders of magnitude and thus reduces DRAM\naccess by 5.4x. We further develop a Huffman tree scheduler to improve the\nscalability of the merger for larger sparse matrices, which reduces the DRAM\naccess by another 1.8x. We also resolve the increased input matrix read induced\nby the new representation using a row prefetcher with near-optimal buffer\nreplacement policy, further reducing the DRAM access by 1.5x. Evaluated on 20\nbenchmarks, SpArch reduces the total DRAM access by 2.8x over previous\nstate-of-the-art. On average, SpArch achieves 4x, 19x, 18x, 17x, 1285x speedup\nand 6x, 164x, 435x, 307x, 62x energy savings over OuterSPACE, MKL, cuSPARSE,\nCUSP, and ARM Armadillo, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 18:54:40 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Zhang", "Zhekai", ""], ["Wang", "Hanrui", ""], ["Han", "Song", ""], ["Dally", "William J.", ""]]}, {"id": "2002.08958", "submitter": "Mher Safaryan", "authors": "Mher Safaryan and Egor Shulgin and Peter Richt\\'arik", "title": "Uncertainty Principle for Communication Compression in Distributed and\n  Federated Learning and the Search for an Optimal Compressor", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": "Information and Inference: A Journal of the IMA, 2021", "doi": "10.1093/imaiai/iaab006", "report-no": null, "categories": "cs.LG cs.DC cs.IT math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to mitigate the high communication cost in distributed and federated\nlearning, various vector compression schemes, such as quantization,\nsparsification and dithering, have become very popular. In designing a\ncompression method, one aims to communicate as few bits as possible, which\nminimizes the cost per communication round, while at the same time attempting\nto impart as little distortion (variance) to the communicated messages as\npossible, which minimizes the adverse effect of the compression on the overall\nnumber of communication rounds. However, intuitively, these two goals are\nfundamentally in conflict: the more compression we allow, the more distorted\nthe messages become. We formalize this intuition and prove an {\\em uncertainty\nprinciple} for randomized compression operators, thus quantifying this\nlimitation mathematically, and {\\em effectively providing asymptotically tight\nlower bounds on what might be achievable with communication compression}.\nMotivated by these developments, we call for the search for the optimal\ncompression operator. In an attempt to take a first step in this direction, we\nconsider an unbiased compression method inspired by the Kashin representation\nof vectors, which we call {\\em Kashin compression (KC)}. In contrast to all\npreviously proposed compression mechanisms, KC enjoys a {\\em dimension\nindependent} variance bound for which we derive an explicit formula even in the\nregime when only a few bits need to be communicate per each vector entry.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:20:51 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 09:40:43 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 11:13:35 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Safaryan", "Mher", ""], ["Shulgin", "Egor", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2002.09009", "submitter": "Andre Luckow", "authors": "Andre Luckow and Shantenu Jha", "title": "Methods and Experiences for Developing Abstractions for Data-intensive,\n  Scientific Applications", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing software for scientific applications that require the integration\nof diverse types of computing, instruments, and data present challenges that\nare distinct from commercial software. These applications require scale, and\nthe need to integrate various programming and computational models with\nevolving and heterogeneous infrastructure. Pervasive and effective abstractions\nfor distributed infrastructures are thus critical; however, the process of\ndeveloping abstractions for scientific applications and infrastructures is not\nwell understood. While theory-based approaches for system development are\nsuited for well-defined, closed environments, they have severe limitations for\ndesigning abstractions for scientific systems and applications. The design\nscience research (DSR) method provides the basis for designing practical\nsystems that can handle real-world complexities at all levels. In contrast to\ntheory-centric approaches, DSR emphasizes both practical relevance and\nknowledge creation by building and rigorously evaluating all artifacts. We show\nhow DSR provides a well-defined framework for developing abstractions and\nmiddleware systems for distributed systems. Specifically, we address the\ncritical problem of distributed resource management on heterogeneous\ninfrastructure over a dynamic range of scales, a challenge that currently\nlimits many scientific applications. We use the pilot-abstraction, a widely\nused resource management abstraction for high-performance, high throughput, big\ndata, and streaming applications, as a case study for evaluating the DSR\nactivities. For this purpose, we analyze the research process and artifacts\nproduced during the design and evaluation of the pilot-abstraction. We find DSR\nprovides a concise framework for iteratively designing and evaluating systems.\nFinally, we capture our experiences and formulate different lessons learned.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 20:40:34 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 19:43:24 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Luckow", "Andre", ""], ["Jha", "Shantenu", ""]]}, {"id": "2002.09095", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Yibo Xu, Yonggui Yan, Colin Sutcher-Shepard, Leopold\n  Grinberg, Jie Chen", "title": "Parallel and distributed asynchronous adaptive stochastic gradient\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient methods (SGMs) are the predominant approaches to train\ndeep learning models. The adaptive versions (e.g., Adam and AMSGrad) have been\nextensively used in practice, partly because they achieve faster convergence\nthan the non-adaptive versions while incurring little overhead. On the other\nhand, asynchronous (async) parallel computing has exhibited significantly\nhigher speed-up over its synchronous (sync) counterpart. Async-parallel\nnon-adaptive SGMs have been well studied in the literature from the\nperspectives of both theory and practical performance. Adaptive SGMs can also\nbe implemented without much difficulty in an async-parallel way. However, to\nthe best of our knowledge, no theoretical result of async-parallel adaptive\nSGMs has been established. The difficulty for analyzing adaptive SGMs with\nasync updates originates from the second moment term. In this paper, we propose\nan async-parallel adaptive SGM based on AMSGrad. We show that the proposed\nmethod inherits the convergence guarantee of AMSGrad for both convex and\nnon-convex problems, if the staleness (also called delay) caused by asynchrony\nis bounded. Our convergence rate results indicate a nearly linear\nparallelization speed-up if $\\tau=o(K^{\\frac{1}{4}})$, where $\\tau$ is the\nstaleness and $K$ is the number of iterations. The proposed method is tested on\nboth convex and non-convex machine learning problems, and the numerical results\ndemonstrate its clear advantages over the sync counterpart and the\nasync-parallel nonadaptive SGM.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 02:26:26 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 03:40:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Xu", "Yangyang", ""], ["Xu", "Yibo", ""], ["Yan", "Yonggui", ""], ["Sutcher-Shepard", "Colin", ""], ["Grinberg", "Leopold", ""], ["Chen", "Jie", ""]]}, {"id": "2002.09130", "submitter": "Paul Liu", "authors": "Wenzheng Li, Paul Liu, Jan Vondrak", "title": "A polynomial lower bound on adaptive complexity of submodular\n  maximization", "comments": "To appear in STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-data applications, it is desirable to design algorithms with a high\ndegree of parallelization. In the context of submodular optimization, adaptive\ncomplexity has become a widely-used measure of an algorithm's \"sequentiality\".\nAlgorithms in the adaptive model proceed in rounds, and can issue polynomially\nmany queries to a function $f$ in each round. The queries in each round must be\nindependent, produced by a computation that depends only on query results\nobtained in previous rounds.\n  In this work, we examine two fundamental variants of submodular maximization\nin the adaptive complexity model: cardinality-constrained monotone\nmaximization, and unconstrained non-mono-tone maximization. Our main result is\nthat an $r$-round algorithm for cardinality-constrained monotone maximization\ncannot achieve an approximation factor better than $1 - 1/e - \\Omega(\\min \\{\n\\frac{1}{r}, \\frac{\\log^2 n}{r^3} \\})$, for any $r < n^c$ (where $c>0$ is some\nconstant). This is the first result showing that the number of rounds must blow\nup polynomially large as we approach the optimal factor of $1-1/e$.\n  For the unconstrained non-monotone maximization problem, we show a positive\nresult: For every instance, and every $\\delta>0$, either we obtain a\n$(1/2-\\delta)$-approximation in $1$ round, or a\n$(1/2+\\Omega(\\delta^2))$-approximation in $O(1/\\delta^2)$ rounds. In particular\n(and in contrast to the cardinality-constrained case), there cannot be an\ninstance where (i) it is impossible to achieve an approximation factor better\nthan $1/2$ regardless of the number of rounds, and (ii) it takes $r$ rounds to\nachieve a factor of $1/2-O(1/r)$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 04:54:45 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 23:27:14 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Li", "Wenzheng", ""], ["Liu", "Paul", ""], ["Vondrak", "Jan", ""]]}, {"id": "2002.09268", "submitter": "Peter Davies", "authors": "Peter Davies, Vijaykrishna Gurunathan, Niusha Moshrefi, Saleh\n  Ashkboos, Dan Alistarh", "title": "New Bounds For Distributed Mean Estimation and Variance Reduction", "comments": "42 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed mean estimation (DME), in which $n$\nmachines are each given a local $d$-dimensional vector $x_v \\in \\mathbb{R}^d$,\nand must cooperate to estimate the mean of their inputs $\\mu = \\frac 1n\\sum_{v\n= 1}^n x_v$, while minimizing total communication cost.\n  DME is a fundamental construct in distributed machine learning, and there has\nbeen considerable work on variants of this problem, especially in the context\nof distributed variance reduction for stochastic gradients in parallel SGD.\nPrevious work typically assumes an upper bound on the norm of the input\nvectors, and achieves an error bound in terms of this norm. However, in many\nreal applications, the input vectors are concentrated around the correct output\n$\\mu$, but $\\mu$ itself has large norm. In such cases, previous output error\nbounds perform poorly.\n  In this paper, we show that output error bounds need not depend on input\nnorm. We provide a method of quantization which allows distributed mean\nestimation to be performed with solution quality dependent only on the distance\nbetween inputs, not on input norm, and show an analogous result for distributed\nvariance reduction. The technique is based on a new connection with lattice\ntheory. We also provide lower bounds showing that the communication to error\ntrade-off of our algorithms is asymptotically optimal.\n  As the lattices achieving optimal bounds under $\\ell_2$-norm can be\ncomputationally impractical, we also present an extension which leverages\neasy-to-use cubic lattices, and is loose only up to a logarithmic factor in\n$d$. We show experimentally that our method yields practical improvements for\ncommon applications, relative to prior approaches.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:27:13 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 14:33:28 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 14:03:21 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 15:50:18 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Davies", "Peter", ""], ["Gurunathan", "Vijaykrishna", ""], ["Moshrefi", "Niusha", ""], ["Ashkboos", "Saleh", ""], ["Alistarh", "Dan", ""]]}, {"id": "2002.09344", "submitter": "Simon Shillaker", "authors": "Simon Shillaker, Peter Pietzuch", "title": "Faasm: Lightweight Isolation for Efficient Stateful Serverless Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is an excellent fit for big data processing because it\ncan scale quickly and cheaply to thousands of parallel functions. Existing\nserverless platforms isolate functions in ephemeral, stateless containers,\npreventing them from directly sharing memory. This forces users to duplicate\nand serialise data repeatedly, adding unnecessary performance and resource\ncosts. We believe that a new lightweight isolation approach is needed, which\nsupports sharing memory directly between functions and reduces resource\noverheads.\n  We introduce Faaslets, a new isolation abstraction for high-performance\nserverless computing. Faaslets isolate the memory of executed functions using\nsoftware-fault isolation (SFI), as provided by WebAssembly, while allowing\nmemory regions to be shared between functions in the same address space.\nFaaslets can thus avoid expensive data movement when functions are co-located\non the same machine. Our runtime for Faaslets, Faasm, isolates other resources,\ne.g. CPU and network, using standard Linux cgroups, and provides a low-level\nPOSIX host interface for networking, file system access and dynamic loading. To\nreduce initialisation times, Faasm restores Faaslets from already-initialised\nsnapshots. We compare Faasm to a standard container-based platform and show\nthat, when training a machine learning model, it achieves a 2x speed-up with\n10x less memory; for serving machine learning inference, Faasm doubles the\nthroughput and reduces tail latency by 90%.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 15:07:00 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:58:14 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Shillaker", "Simon", ""], ["Pietzuch", "Peter", ""]]}, {"id": "2002.09474", "submitter": "Elena Limonova", "authors": "Elena Limonova and Arseny Terekhin and Dmitry Nikolaev and Vladimir\n  Arlazarov", "title": "Fast Implementation of Morphological Filtering Using ARM NEON Extension", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Applied Engineering Research (ISSN\n  0973-4562), Volume 11, Number 24 (2016), pp. 11675-11680", "doi": null, "report-no": null, "categories": "cs.DC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider speedup potential of morphological image filtering\non ARM processors. Morphological operations are widely used in image analysis\nand recognition and their speedup in some cases can significantly reduce\noverall execution time of recognition. More specifically, we propose fast\nimplementation of erosion and dilation using ARM SIMD extension NEON. These\noperations with the rectangular structuring element are separable. They were\nimplemented using the advantages of separability as sequential horizontal and\nvertical passes. Each pass was implemented using van Herk/Gil-Werman algorithm\nfor large windows and low-constant linear complexity algorithm for small\nwindows. Final implementation was improved with SIMD and used a combination of\nthese methods. We also considered fast transpose implementation of 8x8 and\n16x16 matrices using ARM NEON to get additional computational gain for\nmorphological operations. Experiments showed 3 times efficiency increase for\nfinal implementation of erosion and dilation compared to van Herk/Gil-Werman\nalgorithm without SIMD, 5.7 times speedup for 8x8 matrix transpose and 12 times\nspeedup for 16x16 matrix transpose compared to transpose without SIMD.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:55:34 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Limonova", "Elena", ""], ["Terekhin", "Arseny", ""], ["Nikolaev", "Dmitry", ""], ["Arlazarov", "Vladimir", ""]]}, {"id": "2002.09477", "submitter": "Chen Yuan", "authors": "Yi Lu, Chen Yuan, Xiang Zhang, Hua Huang, Guangyi Liu, Renchang Dai,\n  Zhiwei Wang", "title": "Graph Computing based Distributed State Estimation with PMUs", "comments": "5 pages, 3 figures, 3 tables, 2020 IEEE Power and Energy Society\n  General Meeting. arXiv admin note: substantial text overlap with\n  arXiv:1902.06893", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA cs.PF eess.SP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power system state estimation plays a fundamental and critical role in the\nenergy management system (EMS). To achieve a high performance and accurate\nsystem states estimation, a graph computing based distributed state estimation\napproach is proposed in this paper. Firstly, a power system network is divided\ninto multiple areas. Reference buses are selected with PMUs being installed at\nthese buses for each area. Then, the system network is converted into multiple\nindependent areas. In this way, the power system state estimation could be\nconducted in parallel for each area and the estimated system states are\nobtained without compromise of accuracy. IEEE 118-bus system and MP 10790-bus\nsystem are employed to verify the results accuracy and present the promising\ncomputation performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 21:58:32 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lu", "Yi", ""], ["Yuan", "Chen", ""], ["Zhang", "Xiang", ""], ["Huang", "Hua", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""], ["Wang", "Zhiwei", ""]]}, {"id": "2002.09481", "submitter": "Vojtech Mrazek", "authors": "Filip Vaverka, Vojtech Mrazek, Zdenek Vasicek, Lukas Sekanina", "title": "TFApprox: Towards a Fast Emulation of DNN Approximate Hardware\n  Accelerators on GPU", "comments": "To appear at the 23rd Design, Automation and Test in Europe (DATE\n  2020). Grenoble, France", "journal-ref": null, "doi": "10.23919/DATE48585.2020.9116299", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficiency of hardware accelerators of deep neural networks (DNN) can\nbe improved by introducing approximate arithmetic circuits. In order to\nquantify the error introduced by using these circuits and avoid the expensive\nhardware prototyping, a software emulator of the DNN accelerator is usually\nexecuted on CPU or GPU. However, this emulation is typically two or three\norders of magnitude slower than a software DNN implementation running on CPU or\nGPU and operating with standard floating point arithmetic instructions and\ncommon DNN libraries. The reason is that there is no hardware support for\napproximate arithmetic operations on common CPUs and GPUs and these operations\nhave to be expensively emulated. In order to address this issue, we propose an\nefficient emulation method for approximate circuits utilized in a given DNN\naccelerator which is emulated on GPU. All relevant approximate circuits are\nimplemented as look-up tables and accessed through a texture memory mechanism\nof CUDA capable GPUs. We exploit the fact that the texture memory is optimized\nfor irregular read-only access and in some GPU architectures is even\nimplemented as a dedicated cache. This technique allowed us to reduce the\ninference time of the emulated DNN accelerator approximately 200 times with\nrespect to an optimized CPU version on complex DNNs such as ResNet. The\nproposed approach extends the TensorFlow library and is available online at\nhttps://github.com/ehw-fit/tf-approximate.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 08:22:56 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vaverka", "Filip", ""], ["Mrazek", "Vojtech", ""], ["Vasicek", "Zdenek", ""], ["Sekanina", "Lukas", ""]]}, {"id": "2002.09504", "submitter": "Jan Verschelde", "authors": "Simon Telen, Marc Van Barel, and Jan Verschelde", "title": "Robust Numerical Tracking of One Path of a Polynomial Homotopy on\n  Parallel Shared Memory Computers", "comments": "Accepted for publication in the proceedings of CASC 2020 (Computer\n  Algebra in Scientific Computing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tracking one solution path defined by a polynomial\nhomotopy on a parallel shared memory computer. Our robust path tracker applies\nNewton's method on power series to locate the closest singular parameter value.\nOn top of that, it computes singular values of the Hessians of the polynomials\nin the homotopy to estimate the distance to the nearest different path.\nTogether, these estimates are used to compute an appropriate adaptive stepsize.\nFor n-dimensional problems, the cost overhead of our robust path tracker is\nO(n), compared to the commonly used predictor-corrector methods. This cost\noverhead can be reduced by a multithreaded program on a parallel shared memory\ncomputer.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:04:12 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 23:41:54 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 21:36:17 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Telen", "Simon", ""], ["Van Barel", "Marc", ""], ["Verschelde", "Jan", ""]]}, {"id": "2002.09539", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Hao Liang, Gauri Joshi", "title": "Overlap Local-SGD: An Algorithmic Approach to Hide Communication Delays\n  in Distributed SGD", "comments": "Accepted to ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed stochastic gradient descent (SGD) is essential for scaling the\nmachine learning algorithms to a large number of computing nodes. However, the\ninfrastructures variability such as high communication delay or random node\nslowdown greatly impedes the performance of distributed SGD algorithm,\nespecially in a wireless system or sensor networks. In this paper, we propose\nan algorithmic approach named Overlap-Local-SGD (and its momentum variant) to\noverlap the communication and computation so as to speedup the distributed\ntraining procedure. The approach can help to mitigate the straggler effects as\nwell. We achieve this by adding an anchor model on each node. After multiple\nlocal updates, locally trained models will be pulled back towards the\nsynchronized anchor model rather than communicating with others. Experimental\nresults of training a deep neural network on CIFAR-10 dataset demonstrate the\neffectiveness of Overlap-Local-SGD. We also provide a convergence guarantee for\nthe proposed algorithm under non-convex objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:33:49 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Jianyu", ""], ["Liang", "Hao", ""], ["Joshi", "Gauri", ""]]}, {"id": "2002.09541", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Evaluation of Automatic FPGA Offloading for Loop Statements of\n  Applications", "comments": "7 pages, 4 figure, in Japanese, IEICE Technical Report, SWIM2019-25", "journal-ref": "IEICE Technical Report, SWIM2019-25, Feb. 2020. (c) 2020 IEICE", "doi": null, "report-no": "IEICE Technical Report, SWIM2019-25, Feb. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the prediction of Moore's law slowing down, utilization\nof hardware other than CPU such as FPGA which is energy effective is\nincreasing. However, when using heterogeneous hardware other than CPUs,\nbarriers of technical skills such as OpenCL are high. Based on that, I have\nproposed environment adaptive software that enables automatic conversion,\nconfiguration, and high-performance operation of once written code, according\nto the hardware to be placed. Partly of the offloading to the GPU was automated\npreviously. In this paper, I propose and evaluate an automatic extraction\nmethod of appropriate offload target loop statements of source code as the\nfirst step of offloading to FPGA. I evaluate the effectiveness of the proposed\nmethod using an existing application.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 20:35:36 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2002.09560", "submitter": "Eunjung Yoon", "authors": "Eunjung Yoon, Peng Liu", "title": "Practical Verification of MapReduce Computation Integrity via Partial\n  Re-execution", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data processing is often outsourced to powerful, but untrusted cloud\nservice providers that provide agile and scalable computing resources to weaker\nclients. However, untrusted cloud services do not ensure the integrity of data\nand computations while clients have no control over the outsourced computation\nor no means to check the correctness of the execution. Despite a growing\ninterest and recent progress in verifiable computation, the existing techniques\nare still not practical enough for big data processing due to high verification\noverhead. In this paper, we present a solution called V-MR (Verifiable\nMapReduce), which is a framework that verifies the integrity of MapReduce\ncomputation outsourced in the untrusted cloud via partial re-execution. V-MR is\npractically effective and efficient in that (1) it can detect the violation of\nMapReduce computation integrity and identify the malicious workers involved in\nthe that produced the incorrect computation. (2) it can reduce the overhead of\nverification via partial re-execution with carefully selected input data and\nprogram code using program analysis. The experiment results of a prototype of\nV-MR show that V-MR can verify the integrity of MapReduce computation\neffectively with small overhead for partial re-execution.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 21:49:21 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Yoon", "Eunjung", ""], ["Liu", "Peng", ""]]}, {"id": "2002.09561", "submitter": "Adel Dabah", "authors": "A. Dabah, H. Ltaief, Z. Rezki, M.-A. Arfaoui, M.-S. Alouini, D. Keyes", "title": "Performance / Complexity Trade-offs of the Sphere Decoder Algorithm for\n  Massive MIMO Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive MIMO systems are seen by many researchers as a paramount technology\ntoward next generation networks. This technology consists of hundreds of\nantennas that are capable of sending and receiving simultaneously a huge amount\nof data. One of the main challenges when using this technology is the necessity\nof an efficient decoding framework. The latter must guarantee both a low\ncomplexity and a good signal detection accuracy. The Sphere Decoder (SD)\nalgorithm represents one of the promising decoding algorithms in terms of\ndetection accuracy. However, it is inefficient for dealing with large MIMO\nsystems due to its prohibitive complexity. To overcome this drawback, we\npropose to revisit the sequential SD algorithm and implement several variants\nthat aim at finding appropriate trade-offs between complexity and performance.\nThen, we propose an efficient high-level parallel SD scheme based on the\nmaster/worker paradigm, which permits multiple SD instances to simultaneously\nexplore the search space, while mitigating the overheads from load imbalance.\nThe results of our parallel SD implementation outperform the state-of-the-art\nby more than 5x using similar MIMO configuration systems, and show a\nsuper-linear speedup on multicore platforms. Moreover, this paper presents a\nnew hybrid implementation that combines the strengths of SD and K-best\nalgorithms, i.e., maintaining the detection accuracy of SD, while reducing the\ncomplexity using the K-best way of pruning search space. The hybrid approach\nextends our parallel SD implementation: the master contains the SD search tree,\nand the workers use the K-best algorithm to accelerate its exploration. The\nresulting hybrid approach enhances the diversification gain, and therefore,\nlowers the overall complexity. Our synergistic hybrid approach permits to deal\nwith large MIMO configurations up to 100x100, without sacrificing the accuracy\nand complexity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 21:55:38 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Dabah", "A.", ""], ["Ltaief", "H.", ""], ["Rezki", "Z.", ""], ["Arfaoui", "M. -A.", ""], ["Alouini", "M. -S.", ""], ["Keyes", "D.", ""]]}, {"id": "2002.09610", "submitter": "Ce Jin", "authors": "Mohsen Ghaffari, Christoph Grunau, Ce Jin", "title": "Improved MPC Algorithms for MIS, Matching, and Coloring on Trees and\n  Beyond", "comments": "To appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $O(\\log\\log n)$ round scalable Massively Parallel Computation\nalgorithms for maximal independent set and maximal matching, in trees and more\ngenerally graphs of bounded arboricity, as well as for constant coloring trees.\nFollowing the standards, by a scalable MPC algorithm, we mean that these\nalgorithms can work on machines that have capacity/memory as small as\n$n^{\\delta}$ for any positive constant $\\delta<1$. Our results improve over the\n$O(\\log^2\\log n)$ round algorithms of Behnezhad et al. [PODC'19]. Moreover, our\nmatching algorithm is presumably optimal as its bound matches an\n$\\Omega(\\log\\log n)$ conditional lower bound of Ghaffari, Kuhn, and Uitto\n[FOCS'19].\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 03:07:25 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 12:54:22 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Grunau", "Christoph", ""], ["Jin", "Ce", ""]]}, {"id": "2002.09692", "submitter": "Zhenheng Tang", "authors": "Zhenheng Tang, Shaohuai Shi, Xiaowen Chu", "title": "Communication-Efficient Decentralized Learning with Sparsification and\n  Adaptive Peer Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning techniques such as federated learning have enabled\nmultiple workers to train machine learning models together to reduce the\noverall training time. However, current distributed training algorithms\n(centralized or decentralized) suffer from the communication bottleneck on\nmultiple low-bandwidth workers (also on the server under the centralized\narchitecture). Although decentralized algorithms generally have lower\ncommunication complexity than the centralized counterpart, they still suffer\nfrom the communication bottleneck for workers with low network bandwidth. To\ndeal with the communication problem while being able to preserve the\nconvergence performance, we introduce a novel decentralized training algorithm\nwith the following key features: 1) It does not require a parameter server to\nmaintain the model during training, which avoids the communication pressure on\nany single peer. 2) Each worker only needs to communicate with a single peer at\neach communication round with a highly compressed model, which can\nsignificantly reduce the communication traffic on the worker. We theoretically\nprove that our sparsification algorithm still preserves convergence properties.\n3) Each worker dynamically selects its peer at different communication rounds\nto better utilize the bandwidth resources. We conduct experiments with\nconvolutional neural networks on 32 workers to verify the effectiveness of our\nproposed algorithm compared to seven existing methods. Experimental results\nshow that our algorithm significantly reduces the communication traffic and\ngenerally select relatively high bandwidth peers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 12:31:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Tang", "Zhenheng", ""], ["Shi", "Shaohuai", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2002.09755", "submitter": "Xikui Wang", "authors": "Steven Jacobs, Xikui Wang, Michael J. Carey, Vassilis J. Tsotras, Md\n  Yusuf Sarwar Uddin", "title": "BAD to the Bone: Big Active Data at its Core", "comments": "30 pages. Accepted by VLDBJ", "journal-ref": null, "doi": "10.1007/s00778-020-00616-7", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually all of today's Big Data systems are passive in nature, responding\nto queries posted by their users. Instead, we are working to shift Big Data\nplatforms from passive to active. In our view, a Big Active Data (BAD) system\nshould continuously and reliably capture Big Data while enabling timely and\nautomatic delivery of relevant information to a large pool of interested users,\nas well as supporting retrospective analyses of historical information. While\nvarious scalable streaming query engines have been created, their active\nbehavior is limited to a (relatively) small window of the incoming data. To\nthis end we have created a BAD platform that combines ideas and capabilities\nfrom both Big Data and Active Data (e.g., Publish/Subscribe, Streaming\nEngines). It supports complex subscriptions that consider not only newly\narrived items but also their relationships to past, stored data. Further, it\ncan provide actionable notifications by enriching the subscription results with\nother useful data. Our platform extends an existing open-source Big Data\nManagement System, Apache AsterixDB, with an active toolkit. The toolkit\ncontains features to rapidly ingest semistructured data, share execution\npipelines among users, manage scaled user data subscriptions, and actively\nmonitor the state of the data to produce individualized information for each\nuser. This paper describes the features and design of our current BAD data\nplatform and demonstrates its ability to scale without sacrificing query\ncapabilities or result individualization.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 19:32:51 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 00:09:29 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jacobs", "Steven", ""], ["Wang", "Xikui", ""], ["Carey", "Michael J.", ""], ["Tsotras", "Vassilis J.", ""], ["Uddin", "Md Yusuf Sarwar", ""]]}, {"id": "2002.09964", "submitter": "Hossein Taheri", "authors": "Hossein Taheri, Aryan Mokhtari, Hamed Hassani, Ramtin Pedarsani", "title": "Quantized Decentralized Stochastic Learning over Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MA cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a decentralized stochastic learning problem where data points are\ndistributed among computing nodes communicating over a directed graph. As the\nmodel size gets large, decentralized learning faces a major bottleneck that is\nthe heavy communication load due to each node transmitting large messages\n(model updates) to its neighbors. To tackle this bottleneck, we propose the\nquantized decentralized stochastic learning algorithm over directed graphs that\nis based on the push-sum algorithm in decentralized consensus optimization.\nMore importantly, we prove that our algorithm achieves the same convergence\nrates of the decentralized stochastic learning algorithm with\nexact-communication for both convex and non-convex losses. Numerical\nevaluations corroborate our main theoretical results and illustrate significant\nspeed-up compared to the exact-communication methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 18:25:39 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 09:12:25 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 07:41:26 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 17:06:54 GMT"}, {"version": "v5", "created": "Mon, 28 Dec 2020 10:02:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Taheri", "Hossein", ""], ["Mokhtari", "Aryan", ""], ["Hassani", "Hamed", ""], ["Pedarsani", "Ramtin", ""]]}, {"id": "2002.10018", "submitter": "Ami Paz", "authors": "Pierre Fraigniaud, Fran\\c{c}ois Le Gall, Harumichi Nishimura, Ami Paz", "title": "Distributed Quantum Proofs for Replicated Data", "comments": "To be presented in ITCS 2021", "journal-ref": "Proceedings of the 12th Innovations in Theoretical Computer\n  Science Conference (ITCS2021), pp. 28:1-28:20, 2021", "doi": "10.4230/LIPIcs.ITCS.2021.28", "report-no": null, "categories": "cs.DC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper tackles the issue of $\\textit{checking}$ that all copies of a large\ndata set replicated at several nodes of a network are identical. The fact that\nthe replicas may be located at distant nodes prevents the system from verifying\ntheir equality locally, i.e., by having each node consult only nodes in its\nvicinity. On the other hand, it remains possible to assign\n$\\textit{certificates}$ to the nodes, so that verifying the consistency of the\nreplicas can be achieved locally. However, we show that, as the data set is\nlarge, classical certification mechanisms, including distributed Merlin-Arthur\nprotocols, cannot guarantee good completeness and soundness simultaneously,\nunless they use very large certificates. The main result of this paper is a\ndistributed $\\textit{quantum}$ Merlin-Arthur protocol enabling the nodes to\ncollectively check the consistency of the replicas, based on small\ncertificates, and in a single round of message exchange between neighbors, with\nshort messages. In particular, the certificate-size is logarithmic in the size\nof the data set, which gives an exponential advantage over classical\ncertification mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 00:09:55 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 09:16:19 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Nishimura", "Harumichi", ""], ["Paz", "Ami", ""]]}, {"id": "2002.10047", "submitter": "Jessica Shi", "authors": "Jessica Shi and Laxman Dhulipala and Julian Shun", "title": "Parallel Clique Counting and Peeling Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new parallel algorithm for $k$-clique counting/listing that has\npolylogarithmic span (parallel time) and is work-efficient (matches the work of\nthe best sequential algorithm) for sparse graphs. Our algorithm is based on\ncomputing low out-degree orientations, which we present new linear-work and\npolylogarithmic-span algorithms for computing in parallel. We also present new\nparallel algorithms for producing unbiased estimations of clique counts using\ngraph sparsification. Finally, we design two new parallel work-efficient\nalgorithms for approximating the $k$-clique densest subgraph, the first of\nwhich is a $1/k$-approximation and the second of which is a\n$1/(k(1+\\epsilon))$-approximation and has polylogarithmic span. Our first\nalgorithm does not have polylogarithmic span, but we prove that it solves a\nP-complete problem.\n  In addition to the theoretical results, we also implement the algorithms and\npropose various optimizations to improve their practical performance. On a\n30-core machine with two-way hyper-threading, our algorithms achieve\n13.23--38.99x and 1.19--13.76x self-relative parallel speedup for $k$-clique\ncounting and $k$-clique densest subgraph, respectively. Compared to the\nstate-of-the-art parallel $k$-clique counting algorithms, we achieve up to\n9.88x speedup, and compared to existing implementations of $k$-clique densest\nsubgraph, we achieve up to 11.83x speedup. We are able to compute the\n$4$-clique counts on the largest publicly-available graph with over two hundred\nbillion edges for the first time.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:48:47 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 02:28:06 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 02:13:51 GMT"}, {"version": "v4", "created": "Fri, 16 Jul 2021 17:30:15 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Shi", "Jessica", ""], ["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""]]}, {"id": "2002.10083", "submitter": "Oguz Selvitopi", "authors": "Oguz Selvitopi, Md Taufique Hussain, Ariful Azad, Ayd{\\i}n Bulu\\c{c}", "title": "Optimizing High Performance Markov Clustering for Pre-Exascale\n  Architectures", "comments": null, "journal-ref": "34th IEEE International Parallel and Distributed Processing\n  Symposium (IPDPS), 2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HipMCL is a high-performance distributed memory implementation of the popular\nMarkov Cluster Algorithm (MCL) and can cluster large-scale networks within\nhours using a few thousand CPU-equipped nodes. It relies on sparse matrix\ncomputations and heavily makes use of the sparse matrix-sparse matrix\nmultiplication kernel (SpGEMM). The existing parallel algorithms in HipMCL are\nnot scalable to Exascale architectures, both due to their communication costs\ndominating the runtime at large concurrencies and also due to their inability\nto take advantage of accelerators that are increasingly popular.\n  In this work, we systematically remove scalability and performance\nbottlenecks of HipMCL. We enable GPUs by performing the expensive expansion\nphase of the MCL algorithm on GPU. We propose a CPU-GPU joint distributed\nSpGEMM algorithm called pipelined Sparse SUMMA and integrate a probabilistic\nmemory requirement estimator that is fast and accurate. We develop a new\nmerging algorithm for the incremental processing of partial results produced by\nthe GPUs, which improves the overlap efficiency and the peak memory usage. We\nalso integrate a recent and faster algorithm for performing SpGEMM on CPUs. We\nvalidate our new algorithms and optimizations with extensive evaluations. With\nthe enabling of the GPUs and integration of new algorithms, HipMCL is up to\n12.4x faster, being able to cluster a network with 70 million proteins and 68\nbillion connections just under 15 minutes using 1024 nodes of ORNL's Summit\nsupercomputer.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 05:39:51 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Selvitopi", "Oguz", ""], ["Hussain", "Md Taufique", ""], ["Azad", "Ariful", ""], ["Bulu\u00e7", "Ayd\u0131n", ""]]}, {"id": "2002.10105", "submitter": "Qiang Wang", "authors": "Qiang Wang, Shaohuai Shi, Canhui Wang, Xiaowen Chu", "title": "Communication Contention Aware Scheduling of Multiple Deep Learning\n  Training Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Deep Learning (DDL) has rapidly grown its popularity since it\nhelps boost the training performance on high-performance GPU clusters.\nEfficient job scheduling is indispensable to maximize the overall performance\nof the cluster when training multiple jobs simultaneously. However, existing\nschedulers do not consider the communication contention of multiple\ncommunication tasks from different distributed training jobs, which could\ndeteriorate the system performance and prolong the job completion time. In this\npaper, we first establish a new DDL job scheduling framework which organizes\nDDL jobs as Directed Acyclic Graphs (DAGs) and considers communication\ncontention between nodes. We then propose an efficient algorithm, LWF-$\\kappa$,\nto balance the GPU utilization and consolidate the allocated GPUs for each job.\nWhen scheduling those communication tasks, we observe that neither avoiding all\nthe contention nor blindly accepting them is optimal to minimize the job\ncompletion time. We thus propose a provable algorithm, AdaDUAL, to efficiently\nschedule those communication tasks. Based on AdaDUAL, we finally propose\nAda-SRSF for the DDL job scheduling problem. Simulations on a 64-GPU cluster\nconnected with 10 Gbps Ethernet show that LWF-$\\kappa$ achieves up to\n$1.59\\times$ improvement over the classical first-fit algorithms. More\nimportantly, Ada-SRSF reduces the average job completion time by $20.1\\%$ and\n$36.7\\%$, as compared to the SRSF(1) scheme (avoiding all the contention) and\nthe SRSF(2) scheme (blindly accepting all of two-way communication contention)\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 07:50:56 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Wang", "Qiang", ""], ["Shi", "Shaohuai", ""], ["Wang", "Canhui", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2002.10245", "submitter": "Wesley Darvin", "authors": "Giordano Salvador, Wesley H. Darvin, Muhammad Huzaifa, Johnathan\n  Alsop, Matthew D. Sinclair, Sarita V. Adve", "title": "Specializing Coherence, Consistency, and Push/Pull for GPU Graph\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides the first study to explore the interaction of update\npropagation with and without fine-grained synchronization (push vs. pull),\nemerging coherence protocols (GPU vs. DeNovo coherence), and software-centric\nconsistency models (DRF0, DRF1, and DRFrlx) for graph workloads on emerging\nintegrated GPU-CPU systems with native unified shared memory. We study 6 graph\napplications with 6 graph inputs for a total of 36 workloads running on 12\nsystem (hardware+software) configurations reflecting the above design space of\nupdate propagation, coherence, and memory consistency. We make three key\ncontributions. First, we show that there is no single best system configuration\nfor all workloads, motivating systems with flexible coherence and consistency\nsupport. Second, we develop a model to accurately predict the best system\nconfiguration -- this model can be used by software designers to decide on push\nvs. pull and the consistency model and by flexible hardware to invoke the\nappropriate coherence and consistency configuration for the given workload.\nThird, we show that the design dimensions explored here are inter-dependent,\nreinforcing the need for software-hardware co-design in the above design\ndimensions. For example, software designers deciding on push vs. pull must\nconsider the consistency model supported by hardware -- in some cases, push\nmaybe better if hardware supports DRFrlx while pull may be better if hardware\ndoes not support DRFrlx.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:34:30 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 16:59:37 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Salvador", "Giordano", ""], ["Darvin", "Wesley H.", ""], ["Huzaifa", "Muhammad", ""], ["Alsop", "Johnathan", ""], ["Sinclair", "Matthew D.", ""], ["Adve", "Sarita V.", ""]]}, {"id": "2002.10502", "submitter": "Xiaodong Cui", "authors": "Xiaodong Cui, Wei Zhang, Ulrich Finkler, George Saon, Michael Picheny,\n  David Kung", "title": "Distributed Training of Deep Neural Network Acoustic Models for\n  Automatic Speech Recognition", "comments": "Accepted to IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has witnessed great progress in Automatic Speech Recognition\n(ASR) due to advances in deep learning. The improvements in performance can be\nattributed to both improved models and large-scale training data. Key to\ntraining such models is the employment of efficient distributed learning\ntechniques. In this article, we provide an overview of distributed training\ntechniques for deep neural network acoustic models for ASR. Starting with the\nfundamentals of data parallel stochastic gradient descent (SGD) and ASR\nacoustic modeling, we will investigate various distributed training strategies\nand their realizations in high performance computing (HPC) environments with an\nemphasis on striking the balance between communication and computation.\nExperiments are carried out on a popular public benchmark to study the\nconvergence, speedup and recognition performance of the investigated\nstrategies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 19:31:50 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Cui", "Xiaodong", ""], ["Zhang", "Wei", ""], ["Finkler", "Ulrich", ""], ["Saon", "George", ""], ["Picheny", "Michael", ""], ["Kung", "David", ""]]}, {"id": "2002.10610", "submitter": "M. Hadi Amini", "authors": "Ahmed Imteaj, Urmish Thakker, Shiqiang Wang, Jian Li, M. Hadi Amini", "title": "Federated Learning for Resource-Constrained IoT Devices: Panoramas and\n  State-of-the-art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, devices are equipped with advanced sensors with higher\nprocessing/computing capabilities. Further, widespread Internet availability\nenables communication among sensing devices. As a result, vast amounts of data\nare generated on edge devices to drive Internet-of-Things (IoT), crowdsourcing,\nand other emerging technologies. The collected extensive data can be\npre-processed, scaled, classified, and finally, used for predicting future\nevents using machine learning (ML) methods. In traditional ML approaches, data\nis sent to and processed in a central server, which encounters communication\noverhead, processing delay, privacy leakage, and security issues. To overcome\nthese challenges, each client can be trained locally based on its available\ndata and by learning from the global model. This decentralized learning\nstructure is referred to as Federated Learning (FL). However, in large-scale\nnetworks, there may be clients with varying computational resource\ncapabilities. This may lead to implementation and scalability challenges for FL\ntechniques. In this paper, we first introduce some recently implemented\nreal-life applications of FL. We then emphasize on the core challenges of\nimplementing the FL algorithms from the perspective of resource limitations\n(e.g., memory, bandwidth, and energy budget) of client clients. We finally\ndiscuss open issues associated with FL and highlight future directions in the\nFL area concerning resource-constrained devices.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 01:03:29 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Imteaj", "Ahmed", ""], ["Thakker", "Urmish", ""], ["Wang", "Shiqiang", ""], ["Li", "Jian", ""], ["Amini", "M. Hadi", ""]]}, {"id": "2002.10641", "submitter": "Chen Dingding", "authors": "Ziyu Chen, Wenxin Zhang, Yanchen Deng, Dingding Chen, Qing Li", "title": "RMB-DPOP: Refining MB-DPOP by Reducing Redundant Inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MB-DPOP is an important complete algorithm for solving Distributed Constraint\nOptimization Problems (DCOPs) by exploiting a cycle-cut idea to implement\nmemory-bounded inference. However, each cluster root in the algorithm is\nresponsible for enumerating all the instantiations of its cycle-cut nodes,\nwhich would cause redundant inferences when its branches do not have the same\ncycle-cut nodes. Additionally, a large number of cycle-cut nodes and the\niterative nature of MB-DPOP further exacerbate the pathology. As a result,\nMB-DPOP could suffer from huge coordination overheads and cannot scale up well.\nTherefore, we present RMB-DPOP which incorporates several novel mechanisms to\nreduce redundant inferences and improve the scalability of MB-DPOP. First,\nusing the independence among the cycle-cut nodes in different branches, we\ndistribute the enumeration of instantiations into different branches whereby\nthe number of nonconcurrent instantiations reduces significantly and each\nbranch can perform memory bounded inference asynchronously. Then, taking the\ntopology into the consideration, we propose an iterative allocation mechanism\nto choose the cycle-cut nodes that cover a maximum of active nodes in a cluster\nand break ties according to their relative positions in a pseudo-tree. Finally,\na caching mechanism is proposed to further reduce unnecessary inferences when\nthe historical results are compatible with the current instantiations. We\ntheoretically show that with the same number of cycle-cut nodes RMB-DPOP\nrequires as many messages as MB-DPOP in the worst case and the experimental\nresults show our superiorities over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 03:19:29 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chen", "Ziyu", ""], ["Zhang", "Wenxin", ""], ["Deng", "Yanchen", ""], ["Chen", "Dingding", ""], ["Li", "Qing", ""]]}, {"id": "2002.10671", "submitter": "Xu Chen", "authors": "Qiong Wu and Kaiwen He and Xu Chen", "title": "Personalized Federated Learning for Intelligent IoT Applications: A\n  Cloud-Edge based Framework", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) have widely penetrated in different aspects of\nmodern life and many intelligent IoT services and applications are emerging.\nRecently, federated learning is proposed to train a globally shared model by\nexploiting a massive amount of user-generated data samples on IoT devices while\npreventing data leakage. However, the device, statistical and model\nheterogeneities inherent in the complex IoT environments pose great challenges\nto traditional federated learning, making it unsuitable to be directly\ndeployed. In this article we advocate a personalized federated learning\nframework in a cloud-edge architecture for intelligent IoT applications. To\ncope with the heterogeneity issues in IoT environments, we investigate emerging\npersonalized federated learning methods which are able to mitigate the negative\neffects caused by heterogeneity in different aspects. With the power of edge\ncomputing, the requirements for fast-processing capacity and low latency in\nintelligent IoT applications can also be achieved. We finally provide a case\nstudy of IoT based human activity recognition to demonstrate the effectiveness\nof personalized federated learning for intelligent IoT applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 05:11:06 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 13:46:54 GMT"}, {"version": "v3", "created": "Sat, 2 May 2020 08:44:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wu", "Qiong", ""], ["He", "Kaiwen", ""], ["Chen", "Xu", ""]]}, {"id": "2002.10726", "submitter": "Hadrien Hendrikx", "authors": "Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, Laurent\n  Massoulie", "title": "Statistically Preconditioned Accelerated Gradient Method for Distributed\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of distributed empirical risk minimization where\nmultiple machines compute the gradients in parallel and a centralized server\nupdates the model parameters. In order to reduce the number of communications\nrequired to reach a given accuracy, we propose a \\emph{preconditioned}\naccelerated gradient method where the preconditioning is done by solving a\nlocal optimization problem over a subsampled dataset at the server. The\nconvergence rate of the method depends on the square root of the relative\ncondition number between the global and local loss functions. We estimate the\nrelative condition number for linear prediction models by studying\n\\emph{uniform} concentration of the Hessians over a bounded domain, which\nallows us to derive improved convergence rates for existing preconditioned\ngradient methods and our accelerated method. Experiments on real-world datasets\nillustrate the benefits of acceleration in the ill-conditioned regime.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 08:23:43 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Hendrikx", "Hadrien", ""], ["Xiao", "Lin", ""], ["Bubeck", "Sebastien", ""], ["Bach", "Francis", ""], ["Massoulie", "Laurent", ""]]}, {"id": "2002.10752", "submitter": "Volker Turau", "authors": "Volker Turau", "title": "Analysis of Amnesiac Flooding", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The broadcast operation in distributed systems is used to spread information\nlocated at some nodes to all other nodes. This operation is often realized by\nflooding, where the source nodes send a message containing the information to\nall neighbors. Each node receiving the message for the first time forwards it\nto all other neighbors. A stateless variant of flooding for synchronous systems\nis called amnesiac flooding. In this case, every time a node receives a\nmessage, it forwards it to those neighbors, from which it did not receive the\nmessage in the current round. The algorithm is oblivious and therefore scales\nvery well. Stateless protocols are advantageous in high volume applications,\nincreasing performance by removing the load caused by retention of session\ninformation and by providing crash tolerance. In this paper we analyze the\ntermination time of amnesiac flooding. We define the (k,c)-flooding problem,\nwhich aims at finding a set $S$ of size $k$, such that amnesiac flooding when\nstarted concurrently by all nodes of $S$ terminates in a minimal number of\nrounds. We prove that this problem is NP-complete. We provide sharp upper and\nlower bounds for the time complexity of amnesiac flooding and reveal a\ndiscrepancy between bipartite and non-bipartite graphs. All results are based\non the insight, that for every non-bipartite graph there exists a bipartite\ngraph such that the execution of amnesiac flooding on both graphs is strongly\ncorrelated. This construction considerably simplifies existing proofs for\namnesiac flooding and allows to analyze the (k,c)-flooding problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 09:11:01 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 08:04:37 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 09:47:34 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 15:03:45 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Turau", "Volker", ""]]}, {"id": "2002.10780", "submitter": "Dennis Olivetti", "authors": "Alkida Balliu, Fabian Kuhn, Dennis Olivetti", "title": "Distributed Edge Coloring in Time Quasi-Polylogarithmic in Delta", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of coloring the edges of an $n$-node graph of maximum degree\n$\\Delta$ with $2\\Delta - 1$ colors is one of the key symmetry breaking problems\nin the area of distributed graph algorithms. While there has been a lot of\nprogress towards the understanding of this problem, the dependency of the\nrunning time on $\\Delta$ has been a long-standing open question. Very recently,\nKuhn [SODA '20] showed that the problem can be solved in time\n$2^{O(\\sqrt{\\log\\Delta})}+O(\\log^* n)$.\n  In this paper, we study the edge coloring problem in the distributed LOCAL\nmodel. We show that the $(\\mathit{degree}+1)$-list edge coloring problem, and\nthus also the $(2\\Delta-1)$-edge coloring problem, can be solved\ndeterministically in time $\\log^{O(\\log\\log\\Delta)}\\Delta + O(\\log^* n)$. This\nis a significant improvement over the result of Kuhn [SODA '20].\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:23:23 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Balliu", "Alkida", ""], ["Kuhn", "Fabian", ""], ["Olivetti", "Dennis", ""]]}, {"id": "2002.10783", "submitter": "Marco Muniz", "authors": "Anne Ejsing, Martin Jensen, Marco Mu\\~niz, Jacob N{\\o}rhave, and Lars\n  Rechter", "title": "Near Optimal Task Graph Scheduling with Priced Timed Automata and Priced\n  Timed Markov Decision Processes", "comments": "Technical report for near optimal task graph scheduling using Uppaal\n  Cora and Uppaal Stratego", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task graph scheduling is a relevant problem in computer science with\napplication to diverse real world domains. Task graph scheduling suffers from a\ncombinatorial explosion and thus finding optimal schedulers is a difficult\ntask.\n  In this paper we present a methodology for computing near-optimal preemptive\nand non-preemptive schedulers for task graphs. The task graph scheduling\nproblem is reduced to location reachability via the fastest path in Priced\nTimed Automata (PTA) and Priced Timed Markov Decision Processes (PTMDP).\nAdditionally, we explore the effect of using chains to reduce the computation\ntime for finding schedules.\n  We have implemented our models in UPPAAL CORA and UPPAAL STRATEGO. We conduct\nan exhaustive experimental evaluation where we compare our resulting schedules\nwith the best-known schedules of a state of the art tool. A significant number\nof our resulting schedules are shown to be shorter than or equal to the\nbest-known schedules.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:37:22 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ejsing", "Anne", ""], ["Jensen", "Martin", ""], ["Mu\u00f1iz", "Marco", ""], ["N\u00f8rhave", "Jacob", ""], ["Rechter", "Lars", ""]]}, {"id": "2002.10889", "submitter": "Michael Dinitz", "authors": "Michael Dinitz and Caleb Robelle", "title": "Efficient and Simple Algorithms for Fault Tolerant Spanners", "comments": "15 pages. Appeared at PODC 2020. This revision improves the running\n  time slightly and incorporates reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that a version of the greedy algorithm gives a\nconstruction of fault-tolerant spanners that is size-optimal, at least for\nvertex faults. However, the algorithm to construct this spanner is not\npolynomial-time, and the best-known polynomial time algorithm is significantly\nsuboptimal. Designing a polynomial-time algorithm to construct (near-)optimal\nfault-tolerant spanners was given as an explicit open problem in the two most\nrecent papers on fault-tolerant spanners ([Bodwin, Dinitz, Parter, Vassilevka\nWilliams SODA '18] and [Bodwin, Patel PODC '19]). We give a surprisingly simple\nalgorithm which runs in polynomial time and constructs fault-tolerant spanners\nthat are extremely close to optimal (off by only a linear factor in the\nstretch) by modifying the greedy algorithm to run in polynomial time. To\ncomplement this result, we also give simple distributed constructions in both\nthe LOCAL and CONGEST models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:31:02 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 14:32:23 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dinitz", "Michael", ""], ["Robelle", "Caleb", ""]]}, {"id": "2002.10890", "submitter": "Andrea Borghesi", "authors": "Andrea Borghesi, Giuseppe Tagliavini, Michele Lombardi, Luca Benini,\n  Michela Milano", "title": "Combining Learning and Optimization for Transprecision Computing", "comments": null, "journal-ref": "Proceedings of the 17th ACM International Conference on Computing\n  Frontiers, May 2020, Pages 10-18", "doi": "10.1145/3387902.3392615", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing demands of the worldwide IT infrastructure stress the need for\nreduced power consumption, which is addressed in so-called transprecision\ncomputing by improving energy efficiency at the expense of precision. For\nexample, reducing the number of bits for some floating-point operations leads\nto higher efficiency, but also to a non-linear decrease of the computation\naccuracy. Depending on the application, small errors can be tolerated, thus\nallowing to fine-tune the precision of the computation. Finding the optimal\nprecision for all variables in respect of an error bound is a complex task,\nwhich is tackled in the literature via heuristics. In this paper, we report on\na first attempt to address the problem by combining a Mathematical Programming\n(MP) model and a Machine Learning (ML) model, following the Empirical Model\nLearning methodology. The ML model learns the relation between variables\nprecision and the output error; this information is then embedded in the MP\nfocused on minimizing the number of bits. An additional refinement phase is\nthen added to improve the quality of the solution. The experimental results\ndemonstrate an average speedup of 6.5\\% and a 3\\% increase in solution quality\ncompared to the state-of-the-art. In addition, experiments on a hardware\nplatform capable of mixed-precision arithmetic (PULPissimo) show the benefits\nof the proposed approach, with energy savings of around 40\\% compared to\nfixed-precision.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:52:15 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Borghesi", "Andrea", ""], ["Tagliavini", "Giuseppe", ""], ["Lombardi", "Michele", ""], ["Benini", "Luca", ""], ["Milano", "Michela", ""]]}, {"id": "2002.10941", "submitter": "Tae Jun Ham", "authors": "Tae Jun Ham, Sung Jun Jung, Seonghak Kim, Young H. Oh, Yeonhong Park,\n  Yoonho Song, Jung-Hun Park, Sanghee Lee, Kyoung Park, Jae W. Lee, Deog-Kyoon\n  Jeong", "title": "A$^3$: Accelerating Attention Mechanisms in Neural Networks with\n  Approximation", "comments": "To be published in 2020 IEEE International Symposium on High\n  Performance Computer Architecture (HPCA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computational demands of neural networks, many hardware\naccelerators for the neural networks have been proposed. Such existing neural\nnetwork accelerators often focus on popular neural network types such as\nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs);\nhowever, not much attention has been paid to attention mechanisms, an emerging\nneural network primitive that enables neural networks to retrieve most relevant\ninformation from a knowledge-base, external memory, or past states. The\nattention mechanism is widely adopted by many state-of-the-art neural networks\nfor computer vision, natural language processing, and machine translation, and\naccounts for a large portion of total execution time. We observe today's\npractice of implementing this mechanism using matrix-vector multiplication is\nsuboptimal as the attention mechanism is semantically a content-based search\nwhere a large portion of computations ends up not being used. Based on this\nobservation, we design and architect A3, which accelerates attention mechanisms\nin neural networks with algorithmic approximation and hardware specialization.\nOur proposed accelerator achieves multiple orders of magnitude improvement in\nenergy efficiency (performance/watt) as well as substantial speedup over the\nstate-of-the-art conventional hardware.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 02:09:21 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Ham", "Tae Jun", ""], ["Jung", "Sung Jun", ""], ["Kim", "Seonghak", ""], ["Oh", "Young H.", ""], ["Park", "Yeonhong", ""], ["Song", "Yoonho", ""], ["Park", "Jung-Hun", ""], ["Lee", "Sanghee", ""], ["Park", "Kyoung", ""], ["Lee", "Jae W.", ""], ["Jeong", "Deog-Kyoon", ""]]}, {"id": "2002.11082", "submitter": "An Xu", "authors": "An Xu, Zhouyuan Huo, Heng Huang", "title": "Optimal Gradient Quantization Condition for Communication-Efficient\n  Distributed Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The communication of gradients is costly for training deep neural networks\nwith multiple devices in computer vision applications. In particular, the\ngrowing size of deep learning models leads to higher communication overheads\nthat defy the ideal linear training speedup regarding the number of devices.\nGradient quantization is one of the common methods to reduce communication\ncosts. However, it can lead to quantization error in the training and result in\nmodel performance degradation. In this work, we deduce the optimal condition of\nboth the binary and multi-level gradient quantization for \\textbf{ANY} gradient\ndistribution. Based on the optimal condition, we develop two novel quantization\nschemes: biased BinGrad and unbiased ORQ for binary and multi-level gradient\nquantization respectively, which dynamically determine the optimal quantization\nlevels. Extensive experimental results on CIFAR and ImageNet datasets with\nseveral popular convolutional neural networks show the superiority of our\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:28:39 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Xu", "An", ""], ["Huo", "Zhouyuan", ""], ["Huang", "Heng", ""]]}, {"id": "2002.11223", "submitter": "Krishna Pillutla", "authors": "Yassine Laguel, Krishna Pillutla, J\\'er\\^ome Malick, Zaid Harchaoui", "title": "Device Heterogeneity in Federated Learning: A Superquantile Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a federated learning framework to handle heterogeneous client\ndevices which do not conform to the population data distribution. The approach\nhinges upon a parameterized superquantile-based objective, where the parameter\nranges over levels of conformity. We present an optimization algorithm and\nestablish its convergence to a stationary point. We show how to practically\nimplement it using secure aggregation by interleaving iterations of the usual\nfederated averaging method with device filtering. We conclude with numerical\nexperiments on neural networks as well as linear models on tasks from computer\nvision and natural language processing.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 23:37:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Laguel", "Yassine", ""], ["Pillutla", "Krishna", ""], ["Malick", "J\u00e9r\u00f4me", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "2002.11270", "submitter": "Yang Zhao", "authors": "Yang Zhao, Chaojian Li, Yue Wang, Pengfei Xu, Yongan Zhang, and\n  Yingyan Lin", "title": "DNN-Chip Predictor: An Analytical Performance Predictor for DNN\n  Accelerators with Various Dataflows and Hardware Architectures", "comments": "Accepted by 45th International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP'2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent breakthroughs in deep neural networks (DNNs) have spurred a\ntremendously increased demand for DNN accelerators. However, designing DNN\naccelerators is non-trivial as it often takes months/years and requires\ncross-disciplinary knowledge. To enable fast and effective DNN accelerator\ndevelopment, we propose DNN-Chip Predictor, an analytical performance predictor\nwhich can accurately predict DNN accelerators' energy, throughput, and latency\nprior to their actual implementation. Our Predictor features two highlights:\n(1) its analytical performance formulation of DNN ASIC/FPGA accelerators\nfacilitates fast design space exploration and optimization; and (2) it supports\nDNN accelerators with different algorithm-to-hardware mapping methods (i.e.,\ndataflows) and hardware architectures. Experiment results based on 2 DNN models\nand 3 different ASIC/FPGA implementations show that our DNN-Chip Predictor's\npredicted performance differs from those of chip measurements of FPGA/ASIC\nimplementation by no more than 17.66% when using different DNN models, hardware\narchitectures, and dataflows. We will release code upon acceptance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 02:59:18 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 02:52:32 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhao", "Yang", ""], ["Li", "Chaojian", ""], ["Wang", "Yue", ""], ["Xu", "Pengfei", ""], ["Zhang", "Yongan", ""], ["Lin", "Yingyan", ""]]}, {"id": "2002.11273", "submitter": "Weixing Ji", "authors": "Jianhua Gao, Weixing Ji, Zhaonian Tan, Yueyan Zhao", "title": "A Systematic Survey of General Sparse Matrix-Matrix Multiplication", "comments": "19 pages, 11 figures, 2 tables, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SpGEMM (General Sparse Matrix-Matrix Multiplication) has attracted much\nattention from researchers in fields of multigrid methods and graph analysis.\nMany optimization techniques have been developed for certain application fields\nand computing architecture over the decades. The objective of this paper is to\nprovide a structured and comprehensive overview of the research on SpGEMM.\nExisting optimization techniques have been grouped into different categories\nbased on their target problems and architectures. Covered topics include SpGEMM\napplications, size prediction of result matrix, matrix partitioning and load\nbalancing, result accumulating, and target architecture-oriented optimization.\nThe rationales of different algorithms in each category are analyzed, and a\nwide range of SpGEMM algorithms are summarized. This survey sufficiently\nreveals the latest progress and research status of SpGEMM optimization from\n1977 to 2019. More specifically, an experimentally comparative study of\nexisting implementations on CPU and GPU is presented. Based on our findings, we\nhighlight future research directions and how future studies can leverage our\nfindings to encourage better design and implementation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:15:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Gao", "Jianhua", ""], ["Ji", "Weixing", ""], ["Tan", "Zhaonian", ""], ["Zhao", "Yueyan", ""]]}, {"id": "2002.11302", "submitter": "Ariful Azad", "authors": "Zhixiang Gu, Jose Moreira, David Edelsohn, Ariful Azad", "title": "Bandwidth-Optimized Parallel Algorithms for Sparse Matrix-Matrix\n  Multiplication using Propagation Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix-matrix multiplication (SpGEMM) is a widely used kernel in\nvarious graph, scientific computing and machine learning algorithms. It is well\nknown that SpGEMM is a memory-bound operation, and its peak performance is\nexpected to be bound by the memory bandwidth. Yet, existing algorithms fail to\nsaturate the memory bandwidth, resulting in suboptimal performance under the\nRoofline model. In this paper we characterize existing SpGEMM algorithms based\non their memory access patterns and develop practical lower and upper bounds\nfor SpGEMM performance. We then develop an SpGEMM algorithm based on outer\nproduct matrix multiplication. The newly developed algorithm called PB-SpGEMM\nsaturates memory bandwidth by using the propagation blocking technique and by\nperforming in-cache sorting and merging. For many practical matrices, PB-SpGEMM\nruns 20%-50% faster than the state-of-the-art heap and hash SpGEMM algorithms\non modern multicore processors. Most importantly, PB-SpGEMM attains performance\npredicted by the Roofline model, and its performance remains stable with\nrespect to matrix size and sparsity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 04:47:54 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Gu", "Zhixiang", ""], ["Moreira", "Jose", ""], ["Edelsohn", "David", ""], ["Azad", "Ariful", ""]]}, {"id": "2002.11321", "submitter": "Zhuolun Xiang", "authors": "Kartik Nayak, Ling Ren, Elaine Shi, Nitin H. Vaidya, Zhuolun Xiang", "title": "Improved Extension Protocols for Byzantine Broadcast and Agreement", "comments": "Will appear in DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine broadcast (BB) and Byzantine agreement (BA) are two most\nfundamental problems and essential building blocks in distributed computing,\nand improving their efficiency is of interest to both theoreticians and\npractitioners. In this paper, we study extension protocols of BB and BA, i.e.,\nprotocols that solve BB/BA with long inputs of $l$ bits using lower costs than\n$l$ single-bit instances. We present new protocols with improved communication\ncomplexity in almost all settings: authenticated BA/BB with $t<n/2$,\nauthenticated BB with $t<(1-\\epsilon)n$, unauthenticated BA/BB with $t<n/3$,\nand asynchronous reliable broadcast and BA with $t<n/3$. The new protocols are\nadvantageous and significant in several aspects. First, they achieve the\nbest-possible communication complexity of $\\Theta(nl)$ for wider ranges of\ninput sizes compared to prior results. Second, the authenticated extension\nprotocols achieve optimal communication complexity given the current best\navailable BB/BA protocols for short messages. Third, to the best of our\nknowledge, our asynchronous and authenticated protocols in the setting are the\nfirst extension protocols in that setting.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 06:31:32 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 22:57:06 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 22:47:52 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Nayak", "Kartik", ""], ["Ren", "Ling", ""], ["Shi", "Elaine", ""], ["Vaidya", "Nitin H.", ""], ["Xiang", "Zhuolun", ""]]}, {"id": "2002.11331", "submitter": "Mark Overton", "authors": "Mark A. Overton", "title": "Romu: Fast Nonlinear Pseudo-Random Number Generators Providing High\n  Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Romu family of pseudo-random number generators (PRNGs) which\ncombines the nonlinear operation of rotation with the linear operations of\nmultiplication and (optionally) addition. Compared to conventional linear-only\nPRNGs, this mixture of linear and nonlinear operations achieves a greater\ndegree of randomness using the same number of arithmetic operations. Or\nequivalently, it achieves the same randomness with fewer operations, resulting\nin higher speed. The statistical properties of these generators are strong, as\nthey pass BigCrush and PractRand -- the most stringent test suites available.\nIn addition, Romu generators take maximum advantage of instruction-level\nparallelism in modern superscalar processors, giving them an output latency of\nzero clock-cycles when inlined, thus adding no delay to an application.\nScaled-down versions of these generators can be created and tested, enabling\none to estimate the maximum number of values the full-size generators can\nsupply before their randomness declines, ensuring the success of large jobs.\nSuch capacity-estimates are rare for conventional PRNGs. A linear PRNG has a\nsingle cycle of states of known length comprising almost all possible states.\nHowever, a Romu generator computes pseudo-random permutations of those states,\ncreating multiple cycles with pseudo-random lengths which cannot be determined\nby theory. But the ease of creating state-sizes of 128 or more bits allows (1)\nshort cycles to be constrained to vanishingly low probabilities, and (2)\nthousands of parallel streams to be created having infinitesimal probabilities\nof overlap.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 07:27:05 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Overton", "Mark A.", ""]]}, {"id": "2002.11343", "submitter": "Zhi Zhou", "authors": "Siqi Luo, Xu Chen, Qiong Wu, Zhi Zhou and Shuai Yu", "title": "HFEL: Joint Edge Association and Resource Allocation for Cost-Efficient\n  Hierarchical Federated Edge Learning", "comments": "submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) has been proposed as an appealing approach to handle\ndata privacy issue of mobile devices compared to conventional machine learning\nat the remote cloud with raw user data uploading. By leveraging edge servers as\nintermediaries to perform partial model aggregation in proximity and relieve\ncore network transmission overhead, it enables great potentials in low-latency\nand energy-efficient FL. Hence we introduce a novel Hierarchical Federated Edge\nLearning (HFEL) framework in which model aggregation is partially migrated to\nedge servers from the cloud. We further formulate a joint computation and\ncommunication resource allocation and edge association problem for device users\nunder HFEL framework to achieve global cost minimization. To solve the problem,\nwe propose an efficient resource scheduling algorithm in the HFEL framework. It\ncan be decomposed into two subproblems: \\emph{resource allocation} given a\nscheduled set of devices for each edge server and \\emph{edge association} of\ndevice users across all the edge servers. With the optimal policy of the convex\nresource allocation subproblem for a set of devices under a single edge server,\nan efficient edge association strategy can be achieved through iterative global\ncost reduction adjustment process, which is shown to converge to a stable\nsystem point. Extensive performance evaluations demonstrate that our HFEL\nframework outperforms the proposed benchmarks in global cost saving and\nachieves better training performance compared to conventional federated\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 08:10:33 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 05:16:10 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Luo", "Siqi", ""], ["Chen", "Xu", ""], ["Wu", "Qiong", ""], ["Zhou", "Zhi", ""], ["Yu", "Shuai", ""]]}, {"id": "2002.11364", "submitter": "Zhize Li", "authors": "Zhize Li and Dmitry Kovalev and Xun Qian and Peter Richt\\'arik", "title": "Acceleration for Compressed Gradient Descent in Distributed and\n  Federated Optimization", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high communication cost in distributed and federated learning\nproblems, methods relying on compression of communicated messages are becoming\nincreasingly popular. While in other contexts the best performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of iterations, there are no methods which combine the benefits of both\ngradient compression and acceleration. In this paper, we remedy this situation\nand propose the first accelerated compressed gradient descent (ACGD) methods.\nIn the single machine regime, we prove that ACGD enjoys the rate\n$O\\Big((1+\\omega)\\sqrt{\\frac{L}{\\mu}}\\log \\frac{1}{\\epsilon}\\Big)$ for\n$\\mu$-strongly convex problems and\n$O\\Big((1+\\omega)\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ for convex problems,\nrespectively, where $\\omega$ is the compression parameter. Our results improve\nupon the existing non-accelerated rates $O\\Big((1+\\omega)\\frac{L}{\\mu}\\log\n\\frac{1}{\\epsilon}\\Big)$ and $O\\Big((1+\\omega)\\frac{L}{\\epsilon}\\Big)$,\nrespectively, and recover the optimal rates of accelerated gradient descent as\na special case when no compression ($\\omega=0$) is applied. We further propose\na distributed variant of ACGD (called ADIANA) and prove the convergence rate\n$\\widetilde{O}\\Big(\\omega+\\sqrt{\\frac{L}{\\mu}}+\\sqrt{\\big(\\frac{\\omega}{n}+\\sqrt{\\frac{\\omega}{n}}\\big)\\frac{\\omega\nL}{\\mu}}\\Big)$, where $n$ is the number of devices/workers and $\\widetilde{O}$\nhides the logarithmic factor $\\log \\frac{1}{\\epsilon}$. This improves upon the\nprevious best result $\\widetilde{O}\\Big(\\omega + \\frac{L}{\\mu}+\\frac{\\omega\nL}{n\\mu} \\Big)$ achieved by the DIANA method of Mishchenko et al. (2019).\nFinally, we conduct several experiments on real-world datasets which\ncorroborate our theoretical results and confirm the practical superiority of\nour accelerated methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:03:23 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:36:17 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Li", "Zhize", ""], ["Kovalev", "Dmitry", ""], ["Qian", "Xun", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2002.11378", "submitter": "Ohad Ben-Baruch", "authors": "Ohad Ben-Baruch, Danny Hendler and Matan Rusanovsky", "title": "Upper and Lower Bounds on the Space Complexity of Detectable Object", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of systems with non-volatile main memory (NVM) increases the\ninterest in the design of \\emph{recoverable concurrent objects} that are robust\nto crash-failures, since their operations are able to recover from such\nfailures by using state retained in NVM. Of particular interest are recoverable\nalgorithms that, in addition to ensuring object consistency, also provide\n\\emph{detectability}, a correctness condition requiring that the recovery code\ncan infer if the failed operation was linearized or not and, in the former\ncase, obtain its response.\n  In this work, we investigate the space complexity of detectable algorithms\nand the external support they require. We make the following three\ncontributions. First, we present the first wait-free bounded-space detectable\nread/write and CAS object implementations. Second, we prove that the bit\ncomplexity of every $N$-process obstruction-free detectable CAS implementation,\nassuming values from a domain of size at least $N$, is $\\Omega(N)$. Finally, we\nprove that the following holds for obstruction-free detectable implementations\nof a large class of objects: their recoverable operations must be provided with\n\\emph{auxiliary state} -- state that is not required by the non-recoverable\ncounterpart implementation -- whose value must be provided from outside the\noperation, either by the system or by the caller of the operation. In contrast,\nthis external support is, in general, not required if the recoverable algorithm\nis not detectable.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 09:42:02 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ben-Baruch", "Ohad", ""], ["Hendler", "Danny", ""], ["Rusanovsky", "Matan", ""]]}, {"id": "2002.11505", "submitter": "Janne H. Korhonen", "authors": "Vitaly Aksenov and Dan Alistarh and Janne H. Korhonen", "title": "Relaxed Scheduling for Scalable Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to leverage large-scale hardware parallelism has been one of the\nkey enablers of the accelerated recent progress in machine learning.\nConsequently, there has been considerable effort invested into developing\nefficient parallel variants of classic machine learning algorithms. However,\ndespite the wealth of knowledge on parallelization, some classic machine\nlearning algorithms often prove hard to parallelize efficiently while\nmaintaining convergence.\n  In this paper, we focus on efficient parallel algorithms for the key machine\nlearning task of inference on graphical models, in particular on the\nfundamental belief propagation algorithm. We address the challenge of\nefficiently parallelizing this classic paradigm by showing how to leverage\nscalable relaxed schedulers in this context. We present an extensive empirical\nstudy, showing that our approach outperforms previous parallel belief\npropagation implementations both in terms of scalability and in terms of\nwall-clock convergence time, on a range of practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 10:28:04 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 15:54:24 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Alistarh", "Dan", ""], ["Korhonen", "Janne H.", ""]]}, {"id": "2002.11511", "submitter": "Bulbul Ahmmed", "authors": "B. Ahmmed, M. K. Mudunuru, S. Karra, S. C. James, and V. V. Vesselinov", "title": "A Comparative Study of Machine Learning Models for Predicting the State\n  of Reactive Mixing", "comments": "31 pages", "journal-ref": null, "doi": "10.1016/j.jcp.2021.110147", "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate predictions of reactive mixing are critical for many Earth and\nenvironmental science problems. To investigate mixing dynamics over time under\ndifferent scenarios, a high-fidelity, finite-element-based numerical model is\nbuilt to solve the fast, irreversible bimolecular reaction-diffusion equations\nto simulate a range of reactive-mixing scenarios. A total of 2,315 simulations\nare performed using different sets of model input parameters comprising various\nspatial scales of vortex structures in the velocity field, time-scales\nassociated with velocity oscillations, the perturbation parameter for the\nvortex-based velocity, anisotropic dispersion contrast, and molecular\ndiffusion. Outputs comprise concentration profiles of the reactants and\nproducts. The inputs and outputs of these simulations are concatenated into\nfeature and label matrices, respectively, to train 20 different machine\nlearning (ML) emulators to approximate system behavior. The 20 ML emulators\nbased on linear methods, Bayesian methods, ensemble learning methods, and\nmultilayer perceptron (MLP), are compared to assess these models. The ML\nemulators are specifically trained to classify the state of mixing and predict\nthree quantities of interest (QoIs) characterizing species production, decay,\nand degree of mixing. Linear classifiers and regressors fail to reproduce the\nQoIs; however, ensemble methods (classifiers and regressors) and the MLP\naccurately classify the state of reactive mixing and the QoIs. Among ensemble\nmethods, random forest and decision-tree-based AdaBoost faithfully predict the\nQoIs. At run time, trained ML emulators are $\\approx10^5$ times faster than the\nhigh-fidelity numerical simulations. Speed and accuracy of the ensemble and MLP\nmodels facilitate uncertainty quantification, which usually requires 1,000s of\nmodel run, to estimate the uncertainty bounds on the QoIs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 22:50:19 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ahmmed", "B.", ""], ["Mudunuru", "M. K.", ""], ["Karra", "S.", ""], ["James", "S. C.", ""], ["Vesselinov", "V. V.", ""]]}, {"id": "2002.11528", "submitter": "Nikolas Ioannou", "authors": "Kornilios Kourtis, Animesh Trivedi, Nikolas Ioannou", "title": "Safe and Efficient Remote Application Code Execution on Disaggregated\n  NVM Storage with eBPF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid improvements in NVM storage devices, the performance bottleneck is\ngradually shifting to the network, thus giving rise to the notion of \"data\nmovement wall\". To reduce the amount of data movement over the network,\nresearchers have proposed near-data computing by shipping operations and\ncompute-extensions closer to storage devices. However, running arbitrary,\nuser-provided extensions in a shared, disaggregated storage environment\npresents multiple challenges regarding safety, isolation, and performance.\nInstead of approaching this problem from scratch, in this work we make a case\nfor leveraging the Linux kernel eBPF framework to program disaggregated NVM\nstorage devices. eBPF offers a safe, verifiable, and high-performance way of\nexecuting untrusted, user-defined code in a shared runtime. In this paper, we\ndescribe our experiences building a first prototype that supports remote\noperations on storage using eBPF, discuss the limitations of our approach, and\ndirections for addressing them.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:43:53 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Kourtis", "Kornilios", ""], ["Trivedi", "Animesh", ""], ["Ioannou", "Nikolas", ""]]}, {"id": "2002.11534", "submitter": "Jinming Xu", "authors": "Jinming Xu, Ye Tian, Ying Sun, Gesualdo Scutari", "title": "Distributed Algorithms for Composite Optimization: Unified Framework and\n  Convergence Analysis", "comments": "arXiv admin note: text overlap with arXiv:1910.09817", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed composite optimization over networks: agents minimize a\nsum of smooth (strongly) convex functions, the agents' sum-utility, plus a\nnonsmooth (extended-valued) convex one. We propose a general unified\nalgorithmic framework for such a class of problems and provide a unified\nconvergence analysis leveraging the theory of operator splitting.\nDistinguishing features of our scheme are: (i) When the agents' functions are\nstrongly convex, the algorithm converges at a linear rate, whose dependence on\nthe agents' functions and network topology is decoupled, matching the typical\nrates of centralized optimization; the rate expression improves on existing\nresults; (ii) When the objective function is convex (but not strongly convex),\nsimilar separation as in (i) is established for the coefficient of the proved\nsublinear rate; (iii) The algorithm can adjust the ratio between the number of\ncommunications and computations to achieve a rate (in terms of computations)\nindependent on the network connectivity; and (iv) A by-product of our analysis\nis a tuning recommendation for several existing (non accelerated) distributed\nalgorithms yielding the fastest provably (worst-case) convergence rate. This is\nthe first time that a general distributed algorithmic framework applicable to\ncomposite optimization enjoys all such properties.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 07:34:40 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 00:22:39 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Xu", "Jinming", ""], ["Tian", "Ye", ""], ["Sun", "Ying", ""], ["Scutari", "Gesualdo", ""]]}, {"id": "2002.11593", "submitter": "Antonio Fern\\'andez Anta", "authors": "Vicent Cholvi and Antonio Fernandez Anta and Chryssis Georgiou and\n  Nicolas Nicolaou and Michel Raynal", "title": "Appending Atomically in Byzantine Distributed Ledgers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Distributed Ledger Object (DLO) is a concurrent object that maintains a\ntotally ordered sequence of records, and supports two basic operations: append,\nwhich appends a record at the end of the sequence, and get, which returns the\nsequence of records. In this work we provide a proper formalization of a\nByzantine-tolerant Distributed Ledger Object (BDLO), which is a DLO in a\ndistributed system in which processes may deviate arbitrarily from their\nindented behavior, i.e. they may be Byzantine. Our formal definition is\naccompanied by algorithms to implement BDLOs by utilizing an underlying\nByzantine Atomic Broadcast service.\n  We then utilize the BDLO implementations to solve the Atomic Appends problem\nagainst Byzantine processes. The Atomic Appends problem emerges when several\nclients have records to append, the record of each client has to be appended to\na different BDLO, and it must be guaranteed that either all records are\nappended or none. We present distributed algorithms implementing solutions for\nthe Atomic Appends problem when the clients (which are involved in the appends)\nand the servers (which maintain the BDLOs) may be Byzantine.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 16:22:47 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Cholvi", "Vicent", ""], ["Anta", "Antonio Fernandez", ""], ["Georgiou", "Chryssis", ""], ["Nicolaou", "Nicolas", ""], ["Raynal", "Michel", ""]]}, {"id": "2002.11771", "submitter": "Dongfang Zhao", "authors": "Dongfang Zhao and Tonglin Li", "title": "Distributed Cross-Blockchain Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interoperability across multiple or many blockchains would play a\ncritical role in the forthcoming blockchain-based data management paradigm. In\nparticular, how to ensure the ACID properties of those transactions across an\narbitrary number of blockchains remains an open problem in both academic and\nindustry: Existing solutions either work for only two blockchains or requires a\ncentralized component, neither of which would meet the scalability requirement\nin practice. This short paper shares our vision and some early results toward\nscalable cross-blockchain transactions. Specifically, we design two distributed\ncommit protocols and, both analytically and experimentally, demonstrate their\neffectiveness.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:57:59 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Zhao", "Dongfang", ""], ["Li", "Tonglin", ""]]}, {"id": "2002.11780", "submitter": "Mayuresh Kunjir", "authors": "Mayuresh Kunjir and Shivnath Babu", "title": "Black or White? How to Develop an AutoTuner for Memory-based Analytics\n  [Extended Version]", "comments": "Main version in ACM SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3380591", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lot of interest today in building autonomous (or, self-driving)\ndata processing systems. An emerging school of thought is to leverage AI-driven\n\"black box\" algorithms for this purpose. In this paper, we present a contrarian\nview. We study the problem of autotuning the memory allocation for applications\nrunning on modern distributed data processing systems. For this problem, we\nshow that an empirically-driven \"white-box\" algorithm, called RelM, that we\nhave developed provides a close-to-optimal tuning at a fraction of the\noverheads compared to state-of-the-art AI-driven \"black box\" algorithms,\nnamely, Bayesian Optimization (BO) and Deep Distributed Policy Gradient (DDPG).\nThe main reason for RelM's superior performance is that the memory management\nin modern memory-based data analytics systems is an interplay of algorithms at\nmultiple levels: (i) at the resource-management level across various containers\nallocated by resource managers like Kubernetes and YARN, (ii) at the container\nlevel among the OS, pods, and processes such as the Java Virtual Machine (JVM),\n(iii) at the application level for caching, aggregation, data shuffles, and\napplication data structures, and (iv) at the JVM level across various pools\nsuch as the Young and Old Generation. RelM understands these interactions and\nuses them in building an analytical solution to autotune the memory management\nknobs. In another contribution, called GBO, we use the RelM's analytical models\nto speed up Bayesian Optimization. Through an evaluation based on Apache Spark,\nwe showcase that RelM's recommendations are significantly better than what\ncommonly-used Spark deployments provide, and are close to the ones obtained by\nbrute-force exploration; while GBO provides optimality guarantees for a higher,\nbut still significantly lower compared to the state-of-the-art AI-driven\npolicies, cost overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 20:34:28 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Kunjir", "Mayuresh", ""], ["Babu", "Shivnath", ""]]}, {"id": "2002.11795", "submitter": "Ashwin Nayak", "authors": "Frederic Magniez and Ashwin Nayak", "title": "Quantum Distributed Complexity of Set Disjointness on a Line", "comments": "21 pages, 2 figures. In v3, background on Theorem 3.5 (from prior\n  work) was included in an appendix. In v4, a detail in the statement of\n  Theorem 3.5 has been corrected, and corresponding changes have been made in\n  the rest of the paper. The results remain unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $x,y\\in\\{0,1\\}^n$, Set Disjointness consists in deciding whether\n$x_i=y_i=1$ for some index $i \\in [n]$. We study the problem of computing this\nfunction in a distributed computing scenario in which the inputs $x$ and $y$\nare given to the processors at the two extremities of a path of length $d$. Set\nDisjointness on a Line was introduced by Le Gall and Magniez (PODC 2018) for\nproving lower bounds on the quantum distributed complexity of computing the\ndiameter of an arbitrary network in the CONGEST model.\n  In this work, we prove an unconditional lower bound of\n$\\widetilde{\\Omega}(\\sqrt[3]{n d^2}+\\sqrt{n} )$ rounds for Set Disjointness on\na Line. This is the first non-trivial lower bound when there is no restriction\non the memory used by the processors. The result gives us a new lower bound of\n$\\widetilde{\\Omega} (\\sqrt[3]{n\\delta^2}+\\sqrt{n} )$ on the number of rounds\nrequired for computing the diameter $\\delta$ of any $n$-node network with\nquantum messages of size $O(\\log n)$ in the CONGEST model.\n  We draw a connection between the distributed computing scenario above and a\nnew model of query complexity. In this model, an algorithm computing a\nbi-variate function $f$ has access to the inputs $x$ and $y$ through two\nseparate oracles $O_x$ and $O_y$, respectively. The restriction is that the\nalgorithm is required to alternately make $d$ queries to $O_x$ and $d$ queries\nto $O_y$. The technique we use for deriving the round lower bound for Set\nDisjointness on a Line also applies to the number of rounds in this query\nmodel. We provide an algorithm for Set Disjointness in this query model with\nround complexity that matches the round lower bound stated above, up to a\npolylogarithmic factor. In this sense, the round lower bound we show for Set\nDisjointness on a Line is optimal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:17:53 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 08:27:13 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 19:06:40 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 03:10:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Magniez", "Frederic", ""], ["Nayak", "Ashwin", ""]]}, {"id": "2002.11880", "submitter": "Soheil Behnezhad", "authors": "Soheil Behnezhad, Mahsa Derakhshan, MohammadTaghi Hajiaghayi", "title": "Stochastic Matching with Few Queries: $(1-\\varepsilon)$ Approximation", "comments": "A version of this paper is to appear at STOC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we are given an arbitrary graph $G=(V, E)$ and know that each\nedge in $E$ is going to be realized independently with some probability $p$.\nThe goal in the stochastic matching problem is to pick a sparse subgraph $Q$ of\n$G$ such that the realized edges in $Q$, in expectation, include a matching\nthat is approximately as large as the maximum matching among the realized edges\nof $G$. The maximum degree of $Q$ can depend on $p$, but not on the size of\n$G$.\n  This problem has been subject to extensive studies over the years and the\napproximation factor has been improved from $0.5$ to $0.5001$ to $0.6568$ and\neventually to $2/3$. In this work, we analyze a natural sampling-based\nalgorithm and show that it can obtain all the way up to $(1-\\epsilon)$\napproximation, for any constant $\\epsilon > 0$.\n  A key and of possible independent interest component of our analysis is an\nalgorithm that constructs a matching on a stochastic graph, which among some\nother important properties, guarantees that each vertex is matched\nindependently from the vertices that are sufficiently far. This allows us to\nbypass a previously known barrier towards achieving $(1-\\epsilon)$\napproximation based on existence of dense Ruzsa-Szemer\\'edi graphs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 02:33:03 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Behnezhad", "Soheil", ""], ["Derakhshan", "Mahsa", ""], ["Hajiaghayi", "MohammadTaghi", ""]]}, {"id": "2002.12115", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Improvement of Automatic GPU Offloading Technology for Application Loop\n  Statements", "comments": "7 pages, 5 figure, in Japanese, IEICE Technical Report, NS2019-219", "journal-ref": "IEICE Technical Report, NS2019-219, Mar. 2020. (c) 2020 IEICE", "doi": null, "report-no": "IEICE Technical Report, NS2019-219, Mar. 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the slowing down of Moore's law, utilization of\nhardware other than CPU such as GPU or FPGA is increasing. However, when using\nheterogeneous hardware other than CPUs, barriers of technical skills such as\nCUDA and HDL are high. Based on that, I have proposed environment adaptive\nsoftware that enables automatic conversion, configuration, and high-performance\noperation of once written code, according to the hardware to be placed. Partly\nof the offloading to the GPU and FPGA was automated previously. In this paper,\nI improve and propose a previous automatic GPU offloading method to expand\napplicapable software and enhance performances more. I evaluate the\neffectiveness of the proposed method in multiple applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 14:31:29 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "2002.12151", "submitter": "Blaise Pascal Tine", "authors": "Fares Elsabbagh, Blaise Tine, Priyadarshini Roshan, Ethan Lyons, Euna\n  Kim, Da Eun Shim, Lingjun Zhu, Sung Kyu Lim and Hyesoon kim", "title": "Vortex: OpenCL Compatible RISC-V GPGPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The current challenges in technology scaling are pushing the semiconductor\nindustry towards hardware specialization, creating a proliferation of\nheterogeneous systems-on-chip, delivering orders of magnitude performance and\npower benefits compared to traditional general-purpose architectures. This\ntransition is getting a significant boost with the advent of RISC-V with its\nunique modular and extensible ISA, allowing a wide range of low-cost processor\ndesigns for various target applications. In addition, OpenCL is currently the\nmost widely adopted programming framework for heterogeneous platforms available\non mainstream CPUs, GPUs, as well as FPGAs and custom DSP. In this work, we\npresent Vortex, a RISC-V General-Purpose GPU that supports OpenCL. Vortex\nimplements a SIMT architecture with a minimal ISA extension to RISC-V that\nenables the execution of OpenCL programs. We also extended OpenCL runtime\nframework to use the new ISA. We evaluate this design using 15nm technology. We\nalso show the performance and energy numbers of running them with a subset of\nbenchmarks from the Rodinia Benchmark suite.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:01:11 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Elsabbagh", "Fares", ""], ["Tine", "Blaise", ""], ["Roshan", "Priyadarshini", ""], ["Lyons", "Ethan", ""], ["Kim", "Euna", ""], ["Shim", "Da Eun", ""], ["Zhu", "Lingjun", ""], ["Lim", "Sung Kyu", ""], ["kim", "Hyesoon", ""]]}, {"id": "2002.12301", "submitter": "Hiroki Matsutani", "authors": "Rei Ito, Mineto Tsukada, Hiroki Matsutani", "title": "An On-Device Federated Learning Approach for Cooperative Model Update\n  between Edge Devices", "comments": null, "journal-ref": "IEEE Access (2021)", "doi": "10.1109/ACCESS.2021.3093382", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most edge AI focuses on prediction tasks on resource-limited edge devices\nwhile the training is done at server machines. However, retraining or\ncustomizing a model is required at edge devices as the model is becoming\noutdated due to environmental changes over time. To follow such a concept\ndrift, a neural-network based on-device learning approach is recently proposed,\nso that edge devices train incoming data at runtime to update their model. In\nthis case, since a training is done at distributed edge devices, the issue is\nthat only a limited amount of training data can be used for each edge device.\nTo address this issue, one approach is a cooperative learning or federated\nlearning, where edge devices exchange their trained results and update their\nmodel by using those collected from the other devices. In this paper, as an\non-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme\nLearning Machine) to sequentially train a model based on recent samples and\ncombine it with autoencoder for anomaly detection. We extend it for an\non-device federated learning so that edge devices can exchange their trained\nresults and update their model by using those collected from the other edge\ndevices. This cooperative model update is one-shot while it can be repeatedly\napplied to synchronize their model. Our approach is evaluated with anomaly\ndetection tasks generated from a driving dataset of cars, a human activity\ndataset, and MNIST dataset. The results demonstrate that the proposed on-device\nfederated learning can produce a merged model by integrating trained results\nfrom multiple edge devices as accurately as traditional backpropagation based\nneural networks and a traditional federated learning approach with lower\ncomputation or communication cost.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:15:38 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 16:07:35 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 12:20:24 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 12:28:56 GMT"}, {"version": "v5", "created": "Sun, 27 Jun 2021 14:59:30 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ito", "Rei", ""], ["Tsukada", "Mineto", ""], ["Matsutani", "Hiroki", ""]]}, {"id": "2002.12410", "submitter": "Peter Richt\\'arik", "authors": "Aleksandr Beznosikov and Samuel Horv\\'ath and Peter Richt\\'arik and\n  Mher Safaryan", "title": "On Biased Compression for Distributed Learning", "comments": "39 pages, 10 Figures, 25 Theorems and Lemmas, 8 New Compression\n  Operators, 2 Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, various communication compression techniques have\nemerged as an indispensable tool helping to alleviate the communication\nbottleneck in distributed learning. However, despite the fact {\\em biased}\ncompressors often show superior performance in practice when compared to the\nmuch more studied and understood {\\em unbiased} compressors, very little is\nknown about them. In this work we study three classes of biased compression\noperators, two of which are new, and their performance when applied to\n(stochastic) gradient descent and distributed (stochastic) gradient descent. We\nshow for the first time that biased compressors can lead to linear convergence\nrates both in the single node and distributed settings. Our {\\em distributed}\nSGD method enjoys the ergodic rate $\\mathcal{O}\\left(\\frac{\\delta L \\exp(-K)\n}{\\mu} + \\frac{(C + D)}{K\\mu}\\right)$, where $\\delta$ is a compression\nparameter which grows when more compression is applied, $L$ and $\\mu$ are the\nsmoothness and strong convexity constants, $C$ captures stochastic gradient\nnoise ($C=0$ if full gradients are computed on each node) and $D$ captures the\nvariance of the gradients at the optimum ($D=0$ for over-parameterized models).\nFurther, via a theoretical study of several synthetic and empirical\ndistributions of communicated gradients, we shed light on why and by how much\nbiased compressors outperform their unbiased variants. Finally, we propose a\nnew highly performing biased compressor---combination of Top-$k$ and natural\ndithering---which in our experiments outperforms all other compression\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 19:52:24 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Beznosikov", "Aleksandr", ""], ["Horv\u00e1th", "Samuel", ""], ["Richt\u00e1rik", "Peter", ""], ["Safaryan", "Mher", ""]]}, {"id": "2002.12418", "submitter": "Huan Wang", "authors": "Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou,\n  Yafeng Yang, Zongyang Cui, Yu Cai, Tianhang Yu, Chengfei Lv, Zhihua Wu", "title": "MNN: A Universal and Efficient Inference Engine", "comments": "Accepted by MLSys 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep learning models on mobile devices draws more and more\nattention recently. However, designing an efficient inference engine on devices\nis under the great challenges of model compatibility, device diversity, and\nresource limitation. To deal with these challenges, we propose Mobile Neural\nNetwork (MNN), a universal and efficient inference engine tailored to mobile\napplications. In this paper, the contributions of MNN include: (1) presenting a\nmechanism called pre-inference that manages to conduct runtime optimization;\n(2)deliveringthorough kernel optimization on operators to achieve optimal\ncomputation performance; (3) introducing backend abstraction module which\nenables hybrid scheduling and keeps the engine lightweight. Extensive benchmark\nexperiments demonstrate that MNN performs favorably against other popular\nlightweight deep learning frameworks. MNN is available to public at:\nhttps://github.com/alibaba/MNN.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:03:16 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jiang", "Xiaotang", ""], ["Wang", "Huan", ""], ["Chen", "Yiliu", ""], ["Wu", "Ziqi", ""], ["Wang", "Lichuan", ""], ["Zou", "Bin", ""], ["Yang", "Yafeng", ""], ["Cui", "Zongyang", ""], ["Cai", "Yu", ""], ["Yu", "Tianhang", ""], ["Lv", "Chengfei", ""], ["Wu", "Zhihua", ""]]}, {"id": "2002.12516", "submitter": "Corey Tessler", "authors": "Corey Tessler, Venkata P. Modekurthy, Nathan Fisher, Abusayeed\n  Saifullah", "title": "Bringing Inter-Thread Cache Benefits to Federated Scheduling -- Extended\n  Results & Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiprocessor scheduling of hard real-time tasks modeled by directed acyclic\ngraphs (DAGs) exploits the inherent parallelism presented by the model. For DAG\ntasks, a node represents a request to execute an object on one of the available\nprocessors. In one DAG task, there may be multiple execution requests for one\nobject, each represented by a distinct node. These distinct execution requests\noffer an opportunity to reduce their combined cache overhead through\ncoordinated scheduling of objects as threads within a parallel task. The goal\nof this work is to realize this opportunity by incorporating the cache-aware\nBUNDLE-scheduling algorithm into federated scheduling of sporadic DAG task\nsets.\n  This is the first work to incorporate instruction cache sharing into\nfederated scheduling. The result is a modification of the DAG model named the\nDAG with objects and threads (DAG-OT). Under the DAG-OT model, descriptions of\nnodes explicitly include their underlying executable object and number of\nthreads. When possible, nodes assigned the same executable object are collapsed\ninto a single node; joining their threads when BUNDLE-scheduled. Compared to\nthe DAG model, the DAG-OT model with cache-aware scheduling reduces the number\nof cores allocated to individual tasks by approximately 20 percent in the\nsynthetic evaluation and up to 50 percent on a novel parallel computing\nplatform implementation. By reducing the number of allocated cores, the DAG-OT\nmodel is able to schedule a subset of previously infeasible task sets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 02:37:26 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tessler", "Corey", ""], ["Modekurthy", "Venkata P.", ""], ["Fisher", "Nathan", ""], ["Saifullah", "Abusayeed", ""]]}, {"id": "2002.12654", "submitter": "Misha Abraham", "authors": "Misha Abraham and Himajit Aithal and Krishnan Mohan", "title": "Real time Smart Contracts for IoT using Blockchain and Collaborative\n  Intelligence based Dynamic Pricing for the next generation Smart Toll\n  Application", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The confluence of Internet of Things(IoT) , Blockchain(BC) and Artificial\nIntelligence(AI) acts as a key accelerator for enabling Machine Economy. To be\nready for future businesses these technologies needs to be adapted by extending\nthe IoT capabilities to Economy of Things (EoT) capabilities. In this paper we\nfocus on one such implementation experience for Smart Toll Transaction\napplication in the domain of mobility. Our paper showcases a possible solution\nby leveraging negotiations, decision making, distributed learning capabilities\nat the devices level using AI-enabled Multi-Agent Systems and the real-time\nsmart contracts between the Cars and Tolls using Blockchain. This solution also\nshowcases the monetization of real time data coming from various IoT devices\nwhich are part of vehicles and infrastructure. While blockchain secures the\nprivacy of the participants it also acts as an economic transactional layer and\ngovernance layer between the devices in the networ\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:16:26 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Abraham", "Misha", ""], ["Aithal", "Himajit", ""], ["Mohan", "Krishnan", ""]]}, {"id": "2002.12664", "submitter": "Chao Wang", "authors": "Chao Wang, Kezhao Huang, Xuehai Qian", "title": "Comprehensive Framework of RDMA-enabled Concurrency Control Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop RCC, the first unified and comprehensive\nRDMA-enabled distributed transaction processing framework supporting six\nserializable concurrency control protocols: not only the classical protocols\nNOWAIT, WAITDIE, and OCC, but also more advanced MVCC and SUNDIAL, and even\nCALVIN, the deterministic concurrency control protocol. Our goal is to\nunbiasedly compare the protocols in a common execution environment with the\nconcurrency control protocol being the only changeable component. We focus on\nthe correct and efficient implementation using key techniques, such as\nco-routines, outstanding requests, and doorbell batching, with two-sided and\none-sided communication primitives. Based on RCC, we get the deep insights that\ncannot be obtained by any existing systems. Most importantly, we obtain the\nexecution stage latency breakdowns with one-sided and two-sided primitive for\neach protocol, which are analyzed to develop more efficient hybrid\nimplementations. Our results show that three hybrid designs are indeed better\nthan both one-sided and two-sided implementations by up to 17.8%. We believe\nthat RCC is a significant advance over the state-of-the-art; it can both\nprovide performance insights and be used as the common infrastructure for fast\nprototyping new implementations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:47:13 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 00:49:35 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 06:19:26 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 07:55:26 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Wang", "Chao", ""], ["Huang", "Kezhao", ""], ["Qian", "Xuehai", ""]]}, {"id": "2002.12688", "submitter": "Chuan Xu", "authors": "Giovanni Neglia and Chuan Xu and Don Towsley and Gianmarco Calbi", "title": "Decentralized gradient methods: does topology matter?", "comments": "A version of this paper is to appear at AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus-based distributed optimization methods have recently been advocated\nas alternatives to parameter server and ring all-reduce paradigms for large\nscale training of machine learning models. In this case, each worker maintains\na local estimate of the optimal parameter vector and iteratively updates it by\naveraging the estimates obtained from its neighbors, and applying a correction\non the basis of its local dataset. While theoretical results suggest that\nworker communication topology should have strong impact on the number of epochs\nneeded to converge, previous experiments have shown the opposite conclusion.\nThis paper sheds lights on this apparent contradiction and show how sparse\ntopologies can lead to faster convergence even in the absence of communication\ndelays.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:59:25 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Neglia", "Giovanni", ""], ["Xu", "Chuan", ""], ["Towsley", "Don", ""], ["Calbi", "Gianmarco", ""]]}, {"id": "2002.12729", "submitter": "Mansaf Alam Dr", "authors": "Syed Arshad Ali, Manzoor Ansari and Mansaf Alam", "title": "Resource Management Techniques for Cloud-Based IoT Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is an Internet-based environment of connected\ndevices and applications. IoT creates an environment where physical devices and\nsensors are flawlessly combined into information nodes to deliver innovative\nand smart services for human-being to make their life easier and more\nefficient. The main objective of the IoT devices-network is to generate data,\nwhich are converted into useful information by the data analysis process, it\nalso provides useful resources to the end users. IoT resource management is a\nkey challenge to ensure the quality of end user experience. Many IoT smart\ndevices and technologies like sensors, actuators, RFID, UMTS, 3G, and GSM etc.\nare used to develop IoT networks. Cloud Computing plays an important role in\nthese networks deployment by providing physical resources as virtualized\nresources consist of memory, computation power, network bandwidth, virtualized\nsystem and device drivers in secure and pay as per use basis. One of the major\nconcerns of Cloud-based IoT environment is resource management, which ensures\nefficient resource utilization, load balancing, reduce SLA violation, and\nimprove the system performance by reducing operational cost and energy\nconsumption. Many researchers have been proposed IoT based resource management\ntechniques. The focus of this paper is to investigate these proposed resource\nallocation techniques and finds which parameters must be considered for\nimprovement in resource allocation for IoT networks. Further, this paper also\nuncovered challenges and issues of Cloud-based resource allocation for IoT\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 09:50:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Ali", "Syed Arshad", ""], ["Ansari", "Manzoor", ""], ["Alam", "Mansaf", ""]]}, {"id": "2002.12769", "submitter": "Mengshuo Jia", "authors": "Mengshuo Jia, Yi Wang, Chen Shen, Gabriela Hug", "title": "Privacy-Preserving Distributed Clustering for Electrical Load Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical load profiling supports retailers and distribution network\noperators in having a better understanding of the consumption behavior of\nconsumers. However, traditional clustering methods for load profiling are\ncentralized and require access to all the smart meter data, thus causing\nprivacy issues for consumers and retailers. To tackle this issue, we propose a\nprivacy-preserving distributed clustering framework for load profiling by\ndeveloping a privacy-preserving accelerated average consensus (PP-AAC)\nalgorithm with proven convergence. Using the proposed framework, we modify\nseveral commonly used clustering methods, including k-means, fuzzy C-means, and\nGaussian mixture model, to provide privacy-preserving distributed clustering\nmethods. In this way, load profiling can be performed only by local\ncalculations and information sharing between neighboring data owners without\nsacrificing privacy. Meanwhile, compared to traditional centralized clustering\nmethods, the computational time consumed by each data owner is significantly\nreduced. The privacy and complexity of the proposed privacy-preserving\ndistributed clustering framework are analyzed. The correctness, efficiency,\neffectiveness, and privacy-preserving feature of the proposed framework and the\nproposed PP-AAC algorithm are verified using a real-world Irish residential\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:50:02 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Jia", "Mengshuo", ""], ["Wang", "Yi", ""], ["Shen", "Chen", ""], ["Hug", "Gabriela", ""]]}]