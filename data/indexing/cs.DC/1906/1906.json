[{"id": "1906.00039", "submitter": "Alexander Brandt", "authors": "Mohammadali Asadi, Alexander Brandt, Robert H. C. Moir, Marc Moreno\n  Maza, Yuzhen Xie", "title": "On the Parallelization of Triangular Decomposition of Polynomial Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the parallelization of algorithms for solving polynomial systems\nsymbolically by way of triangular decomposition. Algorithms for solving\npolynomial systems combine low-level routines for performing arithmetic\noperations on polynomials and high-level procedures which produce the different\ncomponents (points, curves, surfaces) of the solution set. The latter\n\"component-level\" parallelization of triangular decompositions, our focus here,\nbelongs to the class of dynamic irregular parallel applications. Possible\nspeedup factors depend on geometrical properties of the solution set (number of\ncomponents, their dimensions and degrees); these algorithms do not scale with\nthe number of processors. In this paper we combine two different concurrency\nschemes, the fork-join model and producer-consumer patterns, to better capture\nopportunities for component-level parallelization. We report on our\nimplementation with the publicly available BPAS library. Our experimentation\nwith 340 systems yields promising results.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 19:16:47 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Asadi", "Mohammadali", ""], ["Brandt", "Alexander", ""], ["Moir", "Robert H. C.", ""], ["Maza", "Marc Moreno", ""], ["Xie", "Yuzhen", ""]]}, {"id": "1906.00142", "submitter": "Davood Mohajerani", "authors": "Alexander Brandt, Davood Mohajerani, Marc Moreno Maza, Jeeva Paudel,\n  Lin-Xiao Wang", "title": "A Technique for Finding Optimal Program Launch Parameters Targeting\n  Manycore Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new technique to dynamically determine the values\nof program parameters in order to optimize the performance of a multithreaded\nprogram P. To be precise, we describe a novel technique to statically build\nanother program, say, R, that can dynamically determine the optimal values of\nprogram parameters to yield the best program performance for P given values for\nits data and hardware parameters. While this technique can be applied to\nparallel programs in general, we are particularly interested in programs\ntargeting manycore accelerators. Our technique has successfully been employed\nfor GPU kernels using the MWP-CWP performance model for CUDA.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 03:32:57 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Brandt", "Alexander", ""], ["Mohajerani", "Davood", ""], ["Maza", "Marc Moreno", ""], ["Paudel", "Jeeva", ""], ["Wang", "Lin-Xiao", ""]]}, {"id": "1906.00197", "submitter": "Stefano Forti", "authors": "Stefano Forti, Federica Paganelli, Antonio Brogi", "title": "Probabilistic QoS-aware Placement of VNF chains at the Edge", "comments": null, "journal-ref": "Theory and Practice of Logic Programming (2021)", "doi": "10.1017/S1471068421000016", "report-no": null, "categories": "cs.NI cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying IoT-enabled Virtual Network Function (VNF) chains to Cloud-Edge\ninfrastructures requires determining a placement for each VNF that satisfies\nall set deployment requirements as well as a software-defined routing of\ntraffic flows between consecutive functions that meets all set communication\nrequirements. In this article, we present a declarative solution, EdgeUsher, to\nthe problem of how to best place VNF chains to Cloud-Edge infrastructures.\nEdgeUsher can determine all eligible placements for a set of VNF chains to a\nCloud-Edge infrastructure so to satisfy all of their hardware, IoT, security,\nbandwidth, and latency requirements. It exploits probability distributions to\nmodel the dynamic variations in the available Cloud-Edge infrastructure, and to\nassess output eligible placements against those variations.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:08:01 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 17:10:48 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 13:15:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Forti", "Stefano", ""], ["Paganelli", "Federica", ""], ["Brogi", "Antonio", ""]]}, {"id": "1906.00219", "submitter": "Chuan-Chi Lai", "authors": "Chuan-Chi Lai, Tien-Chun Wang, Chuan-Ming Liu, Li-Chun Wang", "title": "Probabilistic Top-k Dominating Query Monitoring over Multiple Uncertain\n  IoT Data Streams in Edge Computing Environments", "comments": "14 pages, 9 figures, Accepted by IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2019.2920908", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the valuable features and information in Big Data has become one\nof the important research issues in Data Science. In most Internet of Things\n(IoT) applications, the collected data are uncertain and imprecise due to\nsensor device variations or transmission errors. In addition, the sensing data\nmay change as time evolves. We refer an uncertain data stream as a dataset that\nhas velocity, veracity, and volume properties simultaneously. This paper\nemploys the parallelism in edge computing environments to facilitate the top-k\ndominating query process over multiple uncertain IoT data streams. The\nchallenges of this problem include how to quickly update the result for\nprocessing uncertainty and reduce the computation cost as well as provide\nhighly accurate results. By referring to the related existing papers for\ncertain data, we provide an effective probabilistic top-k dominating query\nprocess on uncertain data streams, which can be parallelized easily. After\ndiscussing the properties of the proposed approach, we validate our methods\nthrough the complexity analysis and extensive simulated experiments. In\ncomparison with the existing works, the experimental results indicate that our\nmethod can improve almost 60% computation time, reduce nearly 20% communication\ncost between servers, and provide highly accurate results in most scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 12:54:52 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Lai", "Chuan-Chi", ""], ["Wang", "Tien-Chun", ""], ["Liu", "Chuan-Ming", ""], ["Wang", "Li-Chun", ""]]}, {"id": "1906.00239", "submitter": "HMN Dilum Bandara", "authors": "HMN Dilum Bandara and Xiwei Xu and Ingo Weber", "title": "Patterns for Blockchain Data Migration", "comments": "40 pages, 13 figures, 1 table", "journal-ref": null, "doi": "10.1145/3424771.3424796", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid evolution of technological, economic, and regulatory\nlandscapes, contemporary blockchain platforms are all but certain to undergo\nmajor changes. Therefore, the applications that rely on them will eventually\nneed to migrate from one blockchain instance to another to remain competitive\nand secure, as well as to enhance the business process, performance, cost\nefficiency, privacy, and regulatory compliance. However, the differences in\ndata and smart contract representations, modes of hosting, transaction fees, as\nwell as the need to preserve consistency, immutability, and data provenance\nintroduce unique challenges over database migration. We first present a set of\nblockchain migration scenarios and data fidelity levels using an illustrative\nexample. We then present a set of migration patterns to address those scenarios\nand the above data management challenges. Finally, we demonstrate how the\neffort, cost, and risk of migration could be minimized by choosing a suitable\nset of data migration patterns, data fidelity level, and proactive system\ndesign. Practical considerations and research challenges are also highlighted.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 15:10:21 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 13:05:33 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 00:42:20 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Bandara", "HMN Dilum", ""], ["Xu", "Xiwei", ""], ["Weber", "Ingo", ""]]}, {"id": "1906.00245", "submitter": "Hong-Ning Dai Prof.", "authors": "Hong-Ning Dai and Zibin Zheng and Yan Zhang", "title": "Blockchain for Internet of Things: A Survey", "comments": "19 pages, 9 figures", "journal-ref": "IEEE Internet of Things Journal, Vol. 6, No. 5, Oct. 2019", "doi": "10.1109/JIOT.2019.2920987", "report-no": null, "categories": "cs.NI cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Internet of Things (IoT) is reshaping the incumbent industry to smart\nindustry featured with data-driven decision-making. However, intrinsic features\nof IoT result in a number of challenges such as decentralization, poor\ninteroperability, privacy and security vulnerabilities. Blockchain technology\nbrings the opportunities in addressing the challenges of IoT. In this paper, we\ninvestigate the integration of blockchain technology with IoT. We name such\nsynthesis of blockchain and IoT as Blockchain of Things (BCoT). This paper\npresents an in-depth survey of BCoT and discusses the insights of this new\nparadigm. In particular, we first briefly introduce IoT and discuss the\nchallenges of IoT. Then we give an overview of blockchain technology. We next\nconcentrate on introducing the convergence of blockchain and IoT and presenting\nthe proposal of BCoT architecture. We further discuss the issues about using\nblockchain for 5G beyond in IoT as well as industrial applications of BCoT.\nFinally, we outline the open research directions in this promising area.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 15:33:47 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 14:43:45 GMT"}, {"version": "v3", "created": "Sun, 1 Sep 2019 11:38:39 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 03:54:19 GMT"}, {"version": "v5", "created": "Fri, 7 Feb 2020 02:24:50 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Dai", "Hong-Ning", ""], ["Zheng", "Zibin", ""], ["Zhang", "Yan", ""]]}, {"id": "1906.00298", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos and Xing Hu and Sam Toueg", "title": "On Atomic Registers and Randomized Consensus in M&M Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent distributed systems technology, Aguilera et al.\nintroduced a hybrid model of distributed computing, called message-and-memory\nmodel or m&m model for short [1]. In this model, processes can communicate by\nmessage passing and also by accessing some shared memory (e.g., through some\nRDMA connections). We first consider the basic problem of implementing an\natomic single-writer multi-reader (SWMR) register shared by all the processes\nin m&m systems. Specifically, we give an algorithm that implements such a\nregister in m&m systems and show that it is optimal in the number of process\ncrashes that it can tolerate. This generalizes the well-known implementation of\nan atomic SWMR register in a pure message-passing system [5]. We then combine\nour register implementation for m&m systems with the well-known randomized\nconsensus algorithm of Aspnes and Herlihy [4], and obtain a randomized\nconsensus algorithm for m&m systems that is also optimal in the number of\nprocess crashes that it can tolerate. Finally, we determine the minimum number\nof RDMA connections that is sufficient to implement a SWMR register, or solve\nrandomized consensus, in an m&m system with t process crashes, for any given t.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 21:49:56 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 23:15:32 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 18:26:17 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 22:50:59 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "1906.00482", "submitter": "Fabian Kuhn", "authors": "Mohsen Ghaffari and Fabian Kuhn", "title": "On the Use of Randomness in Local Distributed Graph Algorithms", "comments": "21 pages, conference version in ACM Symp. on Principles of\n  Distributed Computing (PODC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to better understand randomization in local distributed graph\nalgorithms by exploring how randomness is used and what we can gain from it: -\nWe first ask the question of how much randomness is needed to obtain efficient\nrandomized algorithms. We show that for all locally checkable problems for\nwhich polylog $n$-time randomized algorithms exist, there are such algorithms\neven if either (I) there is a only a single (private) independent random bit in\neach polylog $n$-neighborhood of the graph, (II) the (private) bits of\nrandomness of different nodes are only polylog $n$-wise independent, or (III)\nthere are only polylog $n$ bits of global shared randomness (and no private\nrandomness). - Second, we study how much we can improve the error probability\nof randomized algorithms. For all locally checkable problems for which polylog\n$n$-time randomized algorithms exist, we show that there are such algorithms\nthat succeed with probability $1-n^{-2^{\\varepsilon(\\log\\log n)^2}}$ and more\ngenerally $T$-round algorithms, for $T\\geq$ polylog $n$, that succeed with\nprobability $1-n^{-2^{\\varepsilon\\log^2T}}$. We also show that polylog $n$-time\nrandomized algorithms with success probability $1-2^{-2^{\\log^\\varepsilon n}}$\nfor some $\\varepsilon>0$ can be derandomized to polylog $n$-time deterministic\nalgorithms. Both of the directions mentioned above, reducing the amount of\nrandomness and improving the success probability, can be seen as partial\nderandomization of existing randomized algorithms. In all the above cases, we\nalso show that any significant improvement of our results would lead to a major\nbreakthrough, as it would imply significantly more efficient deterministic\ndistributed algorithms for a wide class of problems.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 20:51:15 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Kuhn", "Fabian", ""]]}, {"id": "1906.00490", "submitter": "Romolo Marotta", "authors": "Romolo Marotta, Davide Tiriticco, Pierangelo Di Sanzo, Alessandro\n  Pellegrini, Bruno Ciciani and Francesco Quaglia", "title": "Mutable Locks: Combining the Best of Spin and Sleep Locks", "comments": "Added a missing author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present Mutable Locks, a synchronization construct with\nthe same execution semantic of traditional locks (such as spin locks or sleep\nlocks), but with a self-tuned optimized trade off between responsiveness---in\nthe access to a just released critical section---and CPU-time usage during\nthreads' wait phases. It tackles the need for modern synchronization supports,\nin the era of multi-core machines, whose runtime behavior should be optimized\nalong multiple dimensions (performance vs resource consumption) with no\nintervention by the application programmer. Our proposal is intended for\nexploitation in generic concurrent applications where scarce or none knowledge\nis available about the underlying software/hardware stack and the actual\nworkload, an adverse scenario for static choices between spinning and sleeping\nfaced by mutable locks just thanks to their hybrid waiting phases and\nself-tuning capabilities.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 21:35:47 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 14:49:06 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Marotta", "Romolo", ""], ["Tiriticco", "Davide", ""], ["Di Sanzo", "Pierangelo", ""], ["Pellegrini", "Alessandro", ""], ["Ciciani", "Bruno", ""], ["Quaglia", "Francesco", ""]]}, {"id": "1906.00719", "submitter": "Kazuyuki Shudo", "authors": "Yusuke Aoki, Kazuyuki Shudo", "title": "Proximity Neighbor Selection in Blockchain Networks", "comments": "Proc. 2nd IEEE Int'l Conf. on Blockchain (IEEE Blockchain 2019), July\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains have attracted a great deal of attention as a technology for the\ndistributed management of register information at multiple nodes without a\ncentralized system. However, they possess the drawbacks of low transaction\nthroughput and long approval time. These problems can be addressed by\nshortening the block generation interval; however, shortening this interval\nalone has the effect of increasing the frequency of forks. In this study, we\naim to shorten the block generation interval without increasing the fork\ngeneration rate by improving the network topology of the nodes and shortening\nthe propagation time. We propose a neighbor node selection method forming a\nnetwork topology with a short block propagation time. A blockchain simulator is\nused to demonstrate the effect of the proposed neighbor node selection method\non the propagation delay of the network. This result indicates that the\nproposed method improves block propagation time.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 11:38:31 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Aoki", "Yusuke", ""], ["Shudo", "Kazuyuki", ""]]}, {"id": "1906.00834", "submitter": "Maruthi Rohit Ayyagari", "authors": "Maruthi Rohit Ayyagari", "title": "Cache Contention on Multicore Systems: An Ontology-based Approach", "comments": "6 pages, 3 figures. International Journal of Engineering Trends and\n  Technology 2019", "journal-ref": null, "doi": "10.14445/22312803/IJCTT-V67I5P110", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicore processors have proved to be the right choice for both desktop and\nserver systems because it can support high performance with an acceptable\nbudget expenditure. In this work, we have compared several works in cache\ncontention and found that such works have identified several techniques for\ncache contention other than cache size including FSB, Memory Controller and\nprefetching hardware. We found that Distributed Intensity Online (DIO) is a\nvery promising cache contention algorithm since it can achieve up to 2% from\nthe optimal technique. Moreover, we propose a new framework for cache\ncontention based on resource ontologies. In which ontologies instances will be\nused for communication between diverse processes instead of grasping schedules\nbased on hardware.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 14:32:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ayyagari", "Maruthi Rohit", ""]]}, {"id": "1906.00850", "submitter": "Ihab Mohammed", "authors": "Ihab Mohammed, Shadha Tabatabai, Ala Al-Fuqaha, Junaid Qadir", "title": "Opportunistic Data Ferrying in Areas with Limited Information and\n  Communications Infrastructure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in smart cities is rapidly rising due to the global rise in\nurbanization and the wide-scale instrumentation of modern cities. Due to the\nconsiderable infrastructural cost of setting up smart cities and smart\ncommunities, researchers are exploring the use of existing vehicles on the\nroads as \"message ferries\" for the transport data for smart community\napplications to avoid the cost of installing new communication infrastructure.\nIn this paper, we propose an opportunistic data ferry selection algorithm that\nstrives to select vehicles that can minimize the overall delay for data\ndelivery from a source to a given destination. Our proposed opportunistic\nalgorithm utilizes an ensemble of online hiring algorithms, which are run\ntogether in passive mode, to select the online hiring algorithm that has\nperformed the best in recent history. The proposed ensemble based algorithm is\nevaluated empirically using real-world traces from taxies plying routes in\nShanghai, China, and its performance is compared against a baseline of four\nstate-of-the-art online hiring algorithms. A number of experiments are\nconducted and our results indicate that the proposed algorithm can reduce the\noverall delay compared to the baseline by an impressive 13% to 258%.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 05:37:27 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Mohammed", "Ihab", ""], ["Tabatabai", "Shadha", ""], ["Al-Fuqaha", "Ala", ""], ["Qadir", "Junaid", ""]]}, {"id": "1906.00854", "submitter": "Philip Brown", "authors": "Philip N. Brown", "title": "Designing for Emergent Security in Heterogeneous Human-Machine Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work seeks to design decisionmaking rules for autonomous agents to\njointly influence and optimize the behavior of teamed human decisionmakers in\nthe presence of an adversary. We study a situation in which computational jobs\nare scheduled on servers by a collection of autonomous machines in concert with\nself-interested human decisionmakers, and the human and machine schedulers must\nreact to an adversary's attack on one of the servers. We show a simple machine\nscheduling policy such that if all schedulers have permission to schedule jobs\non all servers, increasing the penetration of machine schedulers always\nincreases the level of security in the system, even when the machine schedulers\nhave no explicit coordination or communication amongst themselves. However, we\nshow a companion result in which simple constraints on server availability can\nnullify the machine schedulers' ability to effectively influence human\nschedulers; here, even if machine schedulers control an overwhelming majority\nof jobs, are socially-aware, and fully coordinated amongst themselves, they are\nincapable of influencing human decisionmakers to mitigate the harm of an\nattack.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 13:06:55 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Brown", "Philip N.", ""]]}, {"id": "1906.00874", "submitter": "Steffen B\\\"orm", "authors": "Roc\\'io Carratal\\'a-S\\'aez, Sven Christophersen, Jos\\'e I. Aliaga,\n  Vicen\\c{c} Beltran, Steffen B\\\"orm and Enrique S. Quintana-Ort\\'i", "title": "Exploiting nested task-parallelism in the $\\mathcal{H}-LU$ factorization", "comments": null, "journal-ref": "Journal of Computational Science, volume 33, pages 20-33 (2019)", "doi": "10.1016/j.jocs.2019.02.004", "report-no": null, "categories": "cs.MS cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the parallelization of the LU factorization of hierarchical\nmatrices ($\\mathcal{H}$-matrices) arising from boundary element methods. Our\napproach exploits task-parallelism via the OmpSs programming model and runtime,\nwhich discovers the data-flow parallelism intrinsic to the operation at\nexecution time, via the analysis of data dependencies based on the memory\naddresses of the tasks' operands. This is especially challenging for\n$\\mathcal{H}$-matrices, as the structures containing the data vary in dimension\nduring the execution. We tackle this issue by decoupling the data structure\nfrom that used to detect dependencies. Furthermore, we leverage the support for\nweak operands and early release of dependencies, recently introduced in\nOmpSs-2, to accelerate the execution of parallel codes with nested\ntask-parallelism and fine-grain tasks.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 15:29:17 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Carratal\u00e1-S\u00e1ez", "Roc\u00edo", ""], ["Christophersen", "Sven", ""], ["Aliaga", "Jos\u00e9 I.", ""], ["Beltran", "Vicen\u00e7", ""], ["B\u00f6rm", "Steffen", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""]]}, {"id": "1906.01128", "submitter": "Millad Ghane", "authors": "Millad Ghane, Sunita Chandrasekaran, Margaret S. Cheung", "title": "Assessing Performance Implications of Deep Copy Operations via\n  Microbenchmarking", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As scientific frameworks become sophisticated, so do their data structures.\nCurrent data structures are no longer simple in design and they have been\nprogressively complicated. The typical trend in designing data structures in\nscientific applications are basically nested data structures: pointing to a\ndata structure within another one. Managing nested data structures on a modern\nheterogeneous system requires tremendous effort due to the separate memory\nspace design.\n  In this paper, we will discuss the implications of deep copy on data\ntransfers on current heterogeneous. Then, we will discuss the two options that\nare currently available to perform the memory copy operations on complex\nstructures and will introduce pointerchain directive that we proposed.\nAfterwards, we will introduce a set of extensive benchmarks to compare the\navailable approaches. Our goal is to make our proposed benchmarks a basis to\nexamine the efficiency of upcoming approaches that address the challenge of\ndeep copy operations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:44:02 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 21:52:30 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Ghane", "Millad", ""], ["Chandrasekaran", "Sunita", ""], ["Cheung", "Margaret S.", ""]]}, {"id": "1906.01211", "submitter": "Jean-Philip Piquemal", "authors": "Luc-Henri Jolly, Alejandro Duran, Louis Lagard\\`ere, Jay W. Ponder,\n  Pengyu Ren, Jean-Philip Piquemal", "title": "Raising the Performance of the Tinker-HP Molecular Modeling Package\n  [Article v1.0]", "comments": null, "journal-ref": "LiveCoMS, 2019, 1 (2), 10409", "doi": "10.33011/livecoms.1.2.10409", "report-no": null, "categories": "cs.MS cs.DC physics.chem-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This living paper reviews the present High Performance Computing (HPC)\ncapabilities of the Tinker-HP molecular modeling package. We focus here on the\nreference, double precision, massively parallel molecular dynamics engine\npresent in Tinker-HP and dedicated to perform large scale simulations. We show\nhow it can be adapted to recent Intel Central Processing Unit (CPU) petascale\narchitectures. First, we discuss the new set of Intel Advanced Vector\nExtensions 512 (Intel AVX-512) instructions present in recent Intel processors\n(e.g., the Intel Xeon Scalable and Intel Xeon Phi 2nd generation processors)\nallowing for larger vectorization enhancements. These instructions constitute\nthe central source of potential computational gains when using the latest\nprocessors, justifying important vectorization efforts for developers. We then\nbriefly review the organization of the Tinker-HP code and identify the\ncomputational hotspots which require Intel AVX-512 optimization and we propose\na general and optimal strategy to vectorize those particular parts of the code.\nWe intended to present our optimization strategy in a pedagogical way so it\ncould benefit to other researchers and students interested in gaining\nperformances in their own software. Finally we present the performance\nenhancements obtained compared to the unoptimized code both sequentially and at\nthe scaling limit in parallel for classical non-polarizable (CHARMM) and\npolarizable force fields (AMOEBA). This paper never ceases to be updated as we\naccumulate new data on the associated Github repository between new versions of\nthis living paper.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 06:01:37 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:18:58 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 07:52:09 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Jolly", "Luc-Henri", ""], ["Duran", "Alejandro", ""], ["Lagard\u00e8re", "Louis", ""], ["Ponder", "Jay W.", ""], ["Ren", "Pengyu", ""], ["Piquemal", "Jean-Philip", ""]]}, {"id": "1906.01260", "submitter": "Jinoh Kim", "authors": "Taejoon Kim, Yu Gu, Jinoh Kim", "title": "A Hybrid Cache Architecture for Meeting Per-Tenant Performance Goals in\n  a Private Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in-memory cache system is an important component in a cloud for the data\naccess performance. As the tenants may have different performance goals for\ndata access depending on the nature of their tasks, effectively managing the\nmemory cache is a crucial concern in such a shared computing environment. Two\nextreme methods for managing the memory cache are unlimited sharing and\ncomplete isolation, both of which would be inefficient with the expensive\nstorage complexity to meet the per-tenant performance requirement. In this\npaper, we present a new cache model that incorporates global caching (based on\nunlimited sharing) and static caching (offering complete isolation) for a\nprivate cloud, in which it is critical to offer the guaranteed performance\nwhile minimizing the operating cost. This paper also presents a cache insertion\nalgorithm tailored to the proposed cache model. From an extensive set of\nexperiments conducted on the simulation and emulation settings, the results\nconfirm the validity of the presented cache architecture and insertion\nalgorithm showing the optimized use of the cache space for meeting the\nper-tenant performance requirement.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 08:16:34 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kim", "Taejoon", ""], ["Gu", "Yu", ""], ["Kim", "Jinoh", ""]]}, {"id": "1906.01365", "submitter": "Manuel Bravo", "authors": "Manuel Bravo and Alexey Gotsman", "title": "Reconfigurable Atomic Transaction Commit (Extended Version)", "comments": "Extended version of the PODC' 19 paper: Reconfigurable Atomic\n  Transaction Commit", "journal-ref": null, "doi": "10.1145/3293611.3331590", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern data stores achieve scalability by partitioning data into shards and\nfault-tolerance by replicating each shard across several servers. A key\ncomponent of such systems is a Transaction Certification Service (TCS), which\natomically commits a transaction spanning multiple shards. Existing TCS\nprotocols require 2f+1 crash-stop replicas per shard to tolerate f failures. In\nthis paper we present atomic commit protocols that require only f+1 replicas\nand reconfigure the system upon failures using an external reconfiguration\nservice. We furthermore rigorously prove that these protocols correctly\nimplement a recently proposed TCS specification. We present protocols in two\ndifferent models--the standard asynchronous message-passing model and a model\nwith Remote Direct Memory Access (RDMA), which allows a machine to access the\nmemory of another machine over the network without involving the latter's CPU.\nOur protocols are inspired by a recent FARM system for RDMA-based transaction\nprocessing. Our work codifies the core ideas of FARM as distributed TCS\nprotocols, rigorously proves them correct and highlights the trade-offs\nrequired by the use of RDMA.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 12:07:26 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Bravo", "Manuel", ""], ["Gotsman", "Alexey", ""]]}, {"id": "1906.01600", "submitter": "Georgios Leontidis", "authors": "George Onoufriou, Ronald Bickerton, Simon Pearson, Georgios Leontidis", "title": "Nemesyst: A Hybrid Parallelism Deep Learning-Based Framework Applied for\n  Internet of Things Enabled Food Retailing Refrigeration Systems", "comments": "25 pages, 13 figures, 4 tables, 2 appendices", "journal-ref": "Computers in Industry, 2019", "doi": "10.1016/j.compind.2019.103133", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has attracted considerable attention across multiple\napplication domains, including computer vision, signal processing and natural\nlanguage processing. Although quite a few single node deep learning frameworks\nexist, such as tensorflow, pytorch and keras, we still lack a complete\nprocessing structure that can accommodate large scale data processing, version\ncontrol, and deployment, all while staying agnostic of any specific single node\nframework. To bridge this gap, this paper proposes a new, higher level\nframework, i.e. Nemesyst, which uses databases along with model\nsequentialisation to allow processes to be fed unique and transformed data at\nthe point of need. This facilitates near real-time application and makes models\navailable for further training or use at any node that has access to the\ndatabase simultaneously. Nemesyst is well suited as an application framework\nfor internet of things aggregated control systems, deploying deep learning\ntechniques to optimise individual machines in massive networks. To demonstrate\nthis framework, we adopted a case study in a novel domain; deploying deep\nlearning to optimise the high speed control of electrical power consumed by a\nmassive internet of things network of retail refrigeration systems in\nproportion to load available on the UK National Grid (a demand side response).\nThe case study demonstrated for the first time in such a setting how deep\nlearning models, such as Recurrent Neural Networks (vanilla and Long-Short-Term\nMemory) and Generative Adversarial Networks paired with Nemesyst, achieve\ncompelling performance, whilst still being malleable to future adjustments as\nboth the data and requirements inevitably change over time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 17:23:09 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 22:55:00 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Onoufriou", "George", ""], ["Bickerton", "Ronald", ""], ["Pearson", "Simon", ""], ["Leontidis", "Georgios", ""]]}, {"id": "1906.01736", "submitter": "Xiangyi Chen", "authors": "Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei Steven Wu, Mingyi Hong", "title": "Distributed Training with Heterogeneous Data: Bridging Median- and\n  Mean-Based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is a growing interest in the study of median-based algorithms\nfor distributed non-convex optimization. Two prominent such algorithms include\nsignSGD with majority vote, an effective approach for communication reduction\nvia 1-bit compression on the local gradients, and medianSGD, an algorithm\nrecently proposed to ensure robustness against Byzantine workers. The\nconvergence analyses for these algorithms critically rely on the assumption\nthat all the distributed data are drawn iid from the same distribution.\nHowever, in applications such as Federated Learning, the data across different\nnodes or machines can be inherently heterogeneous, which violates such an iid\nassumption. This work analyzes signSGD and medianSGD in distributed settings\nwith heterogeneous data. We show that these algorithms are non-convergent\nwhenever there is some disparity between the expected median and mean over the\nlocal gradients. To overcome this gap, we provide a novel gradient correction\nmechanism that perturbs the local gradients with noise, together with a series\nresults that provable close the gap between mean and median of the gradients.\nThe proposed methods largely preserve nice properties of these methods, such as\nthe low per-iteration communication complexity of signSGD, and further enjoy\nglobal convergence to stationary solutions. Our perturbation technique can be\nof independent interest when one wishes to estimate mean through a median\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 21:48:50 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 08:04:54 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Chen", "Xiangyi", ""], ["Chen", "Tiancong", ""], ["Sun", "Haoran", ""], ["Wu", "Zhiwei Steven", ""], ["Hong", "Mingyi", ""]]}, {"id": "1906.01864", "submitter": "Xingzhou Zhang", "authors": "Xingzhou Zhang, Yifan Wang, Sidi Lu, Liangkai Liu, Lanyu Xu, Weisong\n  Shi", "title": "OpenEI: An Open Framework for Edge Intelligence", "comments": "12 pages, 6 figures, ICDCS 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last five years, edge computing has attracted tremendous attention\nfrom industry and academia due to its promise to reduce latency, save\nbandwidth, improve availability, and protect data privacy to keep data secure.\nAt the same time, we have witnessed the proliferation of AI algorithms and\nmodels which accelerate the successful deployment of intelligence mainly in\ncloud services. These two trends, combined together, have created a new\nhorizon: Edge Intelligence (EI). The development of EI requires much attention\nfrom both the computer systems research community and the AI community to meet\nthese demands. However, existing computing techniques used in the cloud are not\napplicable to edge computing directly due to the diversity of computing sources\nand the distribution of data sources. We envision that there missing a\nframework that can be rapidly deployed on edge and enable edge AI capabilities.\nTo address this challenge, in this paper we first present the definition and a\nsystematic review of EI. Then, we introduce an Open Framework for Edge\nIntelligence (OpenEI), which is a lightweight software platform to equip edges\nwith intelligent processing and data sharing capability. We analyze four\nfundamental EI techniques which are used to build OpenEI and identify several\nopen problems based on potential research directions. Finally, four typical\napplication scenarios enabled by OpenEI are presented.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 07:41:39 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Zhang", "Xingzhou", ""], ["Wang", "Yifan", ""], ["Lu", "Sidi", ""], ["Liu", "Liangkai", ""], ["Xu", "Lanyu", ""], ["Shi", "Weisong", ""]]}, {"id": "1906.01878", "submitter": "Xingzhou Zhang", "authors": "Xingzhou Zhang, Yifan Wang, Weisong Shi", "title": "pCAMP: Performance Comparison of Machine Learning Packages on the Edges", "comments": "6 pages, 3 figures, USENIX HotEdge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has changed the computing paradigm. Products today are built\nwith machine intelligence as a central attribute, and consumers are beginning\nto expect near-human interaction with the appliances they use. However, much of\nthe deep learning revolution has been limited to the cloud. Recently, several\nmachine learning packages based on edge devices have been announced which aim\nto offload the computing to the edges. However, little research has been done\nto evaluate these packages on the edges, making it difficult for end users to\nselect an appropriate pair of software and hardware. In this paper, we make a\nperformance comparison of several state-of-the-art machine learning packages on\nthe edges, including TensorFlow, Caffe2, MXNet, PyTorch, and TensorFlow Lite.\nWe focus on evaluating the latency, memory footprint, and energy of these tools\nwith two popular types of neural networks on different edge devices. This\nevaluation not only provides a reference to select appropriate combinations of\nhardware and software packages for end users but also points out possible\nfuture directions to optimize packages for developers.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 08:24:49 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 05:03:03 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhang", "Xingzhou", ""], ["Wang", "Yifan", ""], ["Shi", "Weisong", ""]]}, {"id": "1906.01992", "submitter": "Sabri Pllana", "authors": "Andre Viebke, Sabri Pllana, Suejb Memeti, Joanna Kolodziej", "title": "Performance Modelling of Deep Learning on Intel Many Integrated Core\n  Architectures", "comments": "Preprint, HPCS. arXiv admin note: substantial text overlap with\n  arXiv:1702.07908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex problems, such as natural language processing or visual object\ndetection, are solved using deep learning. However, efficient training of\ncomplex deep convolutional neural networks for large data sets is\ncomputationally demanding and requires parallel computing resources. In this\npaper, we present two parameterized performance models for estimation of\nexecution time of training convolutional neural networks on the Intel many\nintegrated core architecture. While for the first performance model we\nminimally use measurement techniques for parameter value estimation, in the\nsecond model we estimate more parameters based on measurements. We evaluate the\nprediction accuracy of performance models in the context of training three\ndifferent convolutional neural network architectures on the Intel Xeon Phi. The\nachieved average performance prediction accuracy is about 15% for the first\nmodel and 11% for second model.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 10:14:50 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Viebke", "Andre", ""], ["Pllana", "Sabri", ""], ["Memeti", "Suejb", ""], ["Kolodziej", "Joanna", ""]]}, {"id": "1906.01993", "submitter": "Sepehr Assadi", "authors": "Sepehr Assadi, MohammadHossein Bateni, Vahab Mirrokni", "title": "Distributed Weighted Matching via Randomized Composable Coresets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum weight matching is one of the most fundamental combinatorial\noptimization problems with a wide range of applications in data mining and\nbioinformatics. Developing distributed weighted matching algorithms is\nchallenging due to the sequential nature of efficient algorithms for this\nproblem. In this paper, we develop a simple distributed algorithm for the\nproblem on general graphs with approximation guarantee of $2+\\varepsilon$ that\n(nearly) matches that of the sequential greedy algorithm. A key advantage of\nthis algorithm is that it can be easily implemented in only two rounds of\ncomputation in modern parallel computation frameworks such as MapReduce. We\nalso demonstrate the efficiency of our algorithm in practice on various graphs\n(some with half a trillion edges) by achieving objective values always close to\nwhat is achievable in the centralized setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 12:43:16 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Assadi", "Sepehr", ""], ["Bateni", "MohammadHossein", ""], ["Mirrokni", "Vahab", ""]]}, {"id": "1906.02367", "submitter": "Deepesh Data", "authors": "Debraj Basu, Deepesh Data, Can Karakus, Suhas Diggavi", "title": "Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification,\n  and Local Computations", "comments": "50 pages; 8 figures; full version of a paper in NeurIPS 2019 with the\n  same title", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication bottleneck has been identified as a significant issue in\ndistributed optimization of large-scale learning models. Recently, several\napproaches to mitigate this problem have been proposed, including different\nforms of gradient compression or computing local models and mixing them\niteratively. In this paper, we propose \\emph{Qsparse-local-SGD} algorithm,\nwhich combines aggressive sparsification with quantization and local\ncomputation along with error compensation, by keeping track of the difference\nbetween the true and compressed gradients. We propose both synchronous and\nasynchronous implementations of \\emph{Qsparse-local-SGD}. We analyze\nconvergence for \\emph{Qsparse-local-SGD} in the \\emph{distributed} setting for\nsmooth non-convex and convex objective functions. We demonstrate that\n\\emph{Qsparse-local-SGD} converges at the same rate as vanilla distributed SGD\nfor many important classes of sparsifiers and quantizers. We use\n\\emph{Qsparse-local-SGD} to train ResNet-50 on ImageNet and show that it\nresults in significant savings over the state-of-the-art, in the number of bits\ntransmitted to reach target accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 00:46:30 GMT"}, {"version": "v2", "created": "Sat, 2 Nov 2019 20:28:26 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Basu", "Debraj", ""], ["Data", "Deepesh", ""], ["Karakus", "Can", ""], ["Diggavi", "Suhas", ""]]}, {"id": "1906.02456", "submitter": "Francois Le Gall", "authors": "Taisuke Izumi and Fran\\c{c}ois Le Gall", "title": "Quantum Distributed Algorithm for the All-Pairs Shortest Path Problem in\n  the CONGEST-CLIQUE Model", "comments": "23 pages; accepted to PODC'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The All-Pairs Shortest Path problem (APSP) is one of the most central\nproblems in distributed computation. In the CONGEST-CLIQUE model, in which $n$\nnodes communicate with each other over a fully connected network by exchanging\nmessages of $O(\\log n)$ bits in synchronous rounds, the best known general\nalgorithm for APSP uses $\\tilde O(n^{1/3})$ rounds. Breaking this barrier is a\nfundamental challenge in distributed graph algorithms. In this paper we\ninvestigate for the first time quantum distributed algorithms in the\nCONGEST-CLIQUE model, where nodes can exchange messages of $O(\\log n)$ quantum\nbits, and show that this barrier can be broken: we construct a $\\tilde\nO(n^{1/4})$-round quantum distributed algorithm for the APSP over directed\ngraphs with polynomial weights in the CONGEST-CLIQUE model. This speedup in the\nquantum setting contrasts with the case of the standard CONGEST model, for\nwhich Elkin et al. (PODC 2014) showed that quantum communication does not offer\nsignificant advantages over classical communication.\n  Our quantum algorithm is based on a relationship discovered by Vassilevska\nWilliams and Williams (JACM 2018) between the APSP and the detection of\nnegative triangles in a graph. The quantum part of our algorithm exploits the\nframework for quantum distributed search recently developed by Le Gall and\nMagniez (PODC 2018). Our main technical contribution is a method showing how to\nimplement multiple quantum searches (one for each edge in the graph) in\nparallel without introducing congestions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 07:34:03 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Izumi", "Taisuke", ""], ["Gall", "Fran\u00e7ois Le", ""]]}, {"id": "1906.02702", "submitter": "Shi Pu", "authors": "Shi Pu, Alex Olshevsky, Ioannis Ch. Paschalidis", "title": "A Sharp Estimate on the Transient Time of Distributed Stochastic\n  Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with minimizing the average of $n$ cost functions\nover a network in which agents may communicate and exchange information with\neach other. We consider the setting where only noisy gradient information is\navailable. To solve the problem, we study the distributed stochastic gradient\ndescent (DSGD) method and perform a non-asymptotic convergence analysis. For\nstrongly convex and smooth objective functions, DSGD asymptotically achieves\nthe optimal network independent convergence rate compared to centralized\nstochastic gradient descent (SGD). Our main contribution is to characterize the\ntransient time needed for DSGD to approach the asymptotic convergence rate,\nwhich we show behaves as $K_T=\\mathcal{O}\\left(\\frac{n}{(1-\\rho_w)^2}\\right)$,\nwhere $1-\\rho_w$ denotes the spectral gap of the mixing matrix. Moreover, we\nconstruct a \"hard\" optimization problem for which we show the transient time\nneeded for DSGD to approach the asymptotic convergence rate is lower bounded by\n$\\Omega \\left(\\frac{n}{(1-\\rho_w)^2} \\right)$, implying the sharpness of the\nobtained result. Numerical experiments demonstrate the tightness of the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 16:57:42 GMT"}, {"version": "v10", "created": "Tue, 3 Mar 2020 02:27:07 GMT"}, {"version": "v11", "created": "Sat, 30 Jan 2021 01:31:32 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 02:29:22 GMT"}, {"version": "v3", "created": "Fri, 28 Jun 2019 15:36:12 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 16:51:40 GMT"}, {"version": "v5", "created": "Tue, 23 Jul 2019 15:04:31 GMT"}, {"version": "v6", "created": "Fri, 26 Jul 2019 19:17:53 GMT"}, {"version": "v7", "created": "Fri, 11 Oct 2019 14:24:59 GMT"}, {"version": "v8", "created": "Thu, 6 Feb 2020 05:43:06 GMT"}, {"version": "v9", "created": "Wed, 12 Feb 2020 12:08:54 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Pu", "Shi", ""], ["Olshevsky", "Alex", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1906.02770", "submitter": "Tosin Adewumi", "authors": "Tosin P. Adewumi and Marcus Liwicki", "title": "Inner For-Loop for Speeding Up Blockchain Mining", "comments": "6 pages, 1 table and 2 figures", "journal-ref": "Open Computer Science, 10(1), pp. 42-47 (2020)", "doi": "10.1515/comp-2020-0004", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the authors propose to increase the efficiency of blockchain\nmining by using a population-based approach. Blockchain relies on solving\ndifficult mathematical problems as proof-of-work within a network before blocks\nare added to the chain. Brute force approach, advocated by some as the fastest\nalgorithm for solving partial hash collisions and implemented in Bitcoin\nblockchain, implies exhaustive, sequential search. It involves incrementing the\nnonce (number) of the header by one, then taking a double SHA-256 hash at each\ninstance and comparing it with a target value to ascertain if lower than that\ntarget. It excessively consumes both time and power. In this paper, the\nauthors, therefore, suggest using an inner for-loop for the population-based\napproach. Comparison shows that it's a slightly faster approach than brute\nforce, with an average speed advantage of about 1.67% or 3,420 iterations per\nsecond and 73% of the time performing better. Also, we observed that the more\nthe total particles deployed, the better the performance until a pivotal point.\nFurthermore, a recommendation on taming the excessive use of power by networks,\nlike Bitcoin's, by using penalty by consensus is suggested.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 18:43:11 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 13:50:45 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Adewumi", "Tosin P.", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1906.02818", "submitter": "Francois Belletti", "authors": "Francois Belletti, Davis King, Kun Yang, Roland Nelet, Yusef Shafi,\n  Yi-Fan Chen, John Anderson", "title": "Tensor Processing Units for Financial Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Monte Carlo methods are critical to many routines in quantitative finance\nsuch as derivatives pricing, hedging and risk metrics. Unfortunately, Monte\nCarlo methods are very computationally expensive when it comes to running\nsimulations in high-dimensional state spaces where they are still a method of\nchoice in the financial industry. Recently, Tensor Processing Units (TPUs) have\nprovided considerable speedups and decreased the cost of running Stochastic\nGradient Descent (SGD) in Deep Learning. After highlighting computational\nsimilarities between training neural networks with SGD and simulating\nstochastic processes, we ask in the present paper whether TPUs are accurate,\nfast and simple enough to use for financial Monte Carlo. Through a theoretical\nreminder of the key properties of such methods and thorough empirical\nexperiments we examine the fitness of TPUs for option pricing, hedging and risk\nmetrics computation. In particular we demonstrate that, in spite of the use of\nmixed precision, TPUs still provide accurate estimators which are fast to\ncompute when compared to GPUs. We also show that the Tensorflow programming\nmodel for TPUs is elegant, expressive and simplifies automated differentiation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:11:05 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 20:40:12 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 17:34:20 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 19:14:34 GMT"}, {"version": "v5", "created": "Mon, 27 Jan 2020 22:20:16 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Belletti", "Francois", ""], ["King", "Davis", ""], ["Yang", "Kun", ""], ["Nelet", "Roland", ""], ["Shafi", "Yusef", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "1906.02888", "submitter": "Vatche Ishakian", "authors": "Paul Castro, Vatche Ishakian, Vinod Muthusamy, Aleksander Slominski", "title": "The server is dead, long live the server: Rise of Serverless Computing,\n  Overview of Current State and Future Trends in Research and Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing -- an emerging cloud-native paradigm for the deployment\nof applications and services -- represents an evolution in cloud application\ndevelopment, programming models, abstractions, and platforms. It promises a\nreal pay-as-you-go billing (with millisecond granularity) with no waste of\nresources, and lowers the bar for developers by asking them to delegate all\ntheir operational complexity and scalability to the cloud provider. Delivering\non these promises comes at the expense of restricting functionality. In this\narticle we provide an overview of serverless computing, its evolution, general\narchitecture, key characteristics and uses cases that made it an attractive\noption for application development. Based on discussions with academics and\nindustry experts during a series of organized serverless computing workshops\n(WoSC), we also identify the technical challenges and open problems.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 03:40:06 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Castro", "Paul", ""], ["Ishakian", "Vatche", ""], ["Muthusamy", "Vinod", ""], ["Slominski", "Aleksander", ""]]}, {"id": "1906.03024", "submitter": "Debasish Pattanayak", "authors": "Debasish Pattanayak, H. Ramesh, and Partha Sarathi Mandal", "title": "Chauffeuring a Crashed Robot from a Disk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evacuation of robots from a disk has attained a lot of attention recently. We\nvisit the problem from the perspective of fault-tolerance. We consider two\nrobots trying to evacuate from a disk via a single hidden exit on the perimeter\nof the disk. The robots communicate wirelessly. The robots are susceptible to\ncrash faults after which they stop moving and communicating. We design the\nalgorithms for tolerating one fault. The objective is to minimize the\nworst-case time required to evacuate both the robots from the disk. When the\nnon-faulty robot chauffeurs the crashed robot, it takes $\\alpha \\geq 1$ amount\nof time to travel unit distance. With this, we also provide a lower bound for\nthe evacuation time. Further, we evaluate the worst-case of the algorithms for\ndifferent values of $\\alpha$ and the crash time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 11:37:51 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Pattanayak", "Debasish", ""], ["Ramesh", "H.", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "1906.03109", "submitter": "Udit Gupta", "authors": "Udit Gupta, Carole-Jean Wu, Xiaodong Wang, Maxim Naumov, Brandon\n  Reagen, David Brooks, Bradford Cottel, Kim Hazelwood, Bill Jia, Hsien-Hsin S.\n  Lee, Andrey Malevich, Dheevatsa Mudigere, Mikhail Smelyanskiy, Liang Xiong,\n  Xuan Zhang", "title": "The Architectural Implications of Facebook's DNN-based Personalized\n  Recommendation", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread application of deep learning has changed the landscape of\ncomputation in the data center. In particular, personalized recommendation for\ncontent ranking is now largely accomplished leveraging deep neural networks.\nHowever, despite the importance of these models and the amount of compute\ncycles they consume, relatively little research attention has been devoted to\nsystems for recommendation. To facilitate research and to advance the\nunderstanding of these workloads, this paper presents a set of real-world,\nproduction-scale DNNs for personalized recommendation coupled with relevant\nperformance metrics for evaluation. In addition to releasing a set of\nopen-source workloads, we conduct in-depth analysis that underpins future\nsystem design and optimization for at-scale recommendation: Inference latency\nvaries by 60% across three Intel server generations, batching and co-location\nof inferences can drastically improve latency-bounded throughput, and the\ndiverse composition of recommendation models leads to different optimization\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 03:04:30 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 14:50:09 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 18:12:11 GMT"}, {"version": "v4", "created": "Sat, 15 Feb 2020 17:58:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gupta", "Udit", ""], ["Wu", "Carole-Jean", ""], ["Wang", "Xiaodong", ""], ["Naumov", "Maxim", ""], ["Reagen", "Brandon", ""], ["Brooks", "David", ""], ["Cottel", "Bradford", ""], ["Hazelwood", "Kim", ""], ["Jia", "Bill", ""], ["Lee", "Hsien-Hsin S.", ""], ["Malevich", "Andrey", ""], ["Mudigere", "Dheevatsa", ""], ["Smelyanskiy", "Mikhail", ""], ["Xiong", "Liang", ""], ["Zhang", "Xuan", ""]]}, {"id": "1906.03113", "submitter": "Paul Burkhardt", "authors": "Paul Burkhardt", "title": "Optimal algebraic Breadth-First Search for sparse graphs", "comments": "Will appear in ACM Transactions on Knowledge Discovery from Data,\n  Vol. 15, No. 5, 2021", "journal-ref": "ACM Transactions on Knowledge Discovery from Data, 15(5):1-19,\n  2021", "doi": "10.1145/3446216", "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a rise in the popularity of algebraic methods for graph\nalgorithms given the development of the GraphBLAS library and other sparse\nmatrix methods. An exemplar for these approaches is Breadth-First Search (BFS).\nThe algebraic BFS algorithm is simply a recurrence of matrix-vector\nmultiplications with the $n \\times n$ adjacency matrix, but the many redundant\noperations over nonzeros ultimately lead to suboptimal performance. Therefore\nan optimal algebraic BFS should be of keen interest especially if it is easily\nintegrated with existing matrix methods.\n  Current methods, notably in the GraphBLAS, use a Sparse Matrix masked-Sparse\nVector (SpMmSpV) multiplication in which the input vector is kept in a sparse\nrepresentation in each step of the BFS, and nonzeros in the vector are masked\nin subsequent steps. This has been an area of recent research in GraphBLAS and\nother libraries. While in theory these masking methods are asymptotically\noptimal on sparse graphs, many add work that leads to suboptimal runtime. We\ngive a new optimal, algebraic BFS for sparse graphs, thus closing a gap in the\nliterature.\n  Our method multiplies progressively smaller submatrices of the adjacency\nmatrix at each step. Let $n$ and $m$ refer to the number of vertices and edges,\nrespectively. On a sparse graph, our method takes $O(n)$ algebraic operations\nas opposed to $O(m)$ operations needed by theoretically optimal sparse matrix\napproaches. Thus for sparse graphs it matches the bounds of the best-known\nsequential algorithm and on a Parallel Random Access Machine (PRAM) it is\nwork-optimal. Our result holds for both directed and undirected graphs.\nCompared to a leading GraphBLAS library our method achieves up to 24x faster\nsequential time and for parallel computation it can be 17x faster on large\ngraphs and 12x faster on large-diameter graphs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 14:06:23 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 14:43:59 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 19:33:26 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 21:52:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Burkhardt", "Paul", ""]]}, {"id": "1906.03155", "submitter": "Jian Li", "authors": "Jian Li, Yong Liu, Weiping Wang", "title": "Distributed Learning with Random Features", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed learning and random projections are the most common techniques in\nlarge scale nonparametric statistical learning. In this paper, we study the\ngeneralization properties of kernel ridge regression using both distributed\nmethods and random features. Theoretical analysis shows the combination\nremarkably reduces computational cost while preserving the optimal\ngeneralization accuracy under standard assumptions. In a benign case,\n$\\mathcal{O}(\\sqrt{N})$ partitions and $\\mathcal{O}(\\sqrt{N})$ random features\nare sufficient to achieve $\\mathcal{O}(1/N)$ learning rate, where $N$ is the\nlabeled sample size. Further, we derive more refined results by using\nadditional unlabeled data to enlarge the number of partitions and by generating\nfeatures in a data-dependent way to reduce the number of random features.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:19:58 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 01:59:54 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Jian", ""], ["Liu", "Yong", ""], ["Wang", "Weiping", ""]]}, {"id": "1906.03196", "submitter": "Albert-Jan Yzelman", "authors": "Wijnand Suijlen and A. N. Yzelman", "title": "Lightweight Parallel Foundations: a model-compliant communication layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Lightweight Parallel Foundations (LPF), an interoperable and\nmodel-compliant communication layer adhering to a strict performance model of\nparallel computations. LPF consists of twelve primitives, each with strict\nperformance guarantees, two of which enable interoperability.\n  We argue that the principles of interoperability and model compliance suffice\nfor the practical use of immortal algorithms: algorithms that are proven\noptimal once, and valid forever. These are ideally also implemented once, and\nusable from a wide range of sequential and parallel environments. This paradigm\nis evaluated by implementing an immortal fast Fourier transform (FFT) using\nLPF, and compared to state-of-the-art FFT implementations. We find it performs\non par to Intel MKL FFT while consistently outperforming FFTW, thus showing\nmodel compliance can be achieved without sacrificing performance.\n  Interoperability encourages the propagation of immortal algorithms as widely\nas possible. We evaluate this by integrating an LPF PageRank into Spark,\nwithout changing any PageRank nor Spark source codes, and while requiring only\na minimal interface layer.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:05:21 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Suijlen", "Wijnand", ""], ["Yzelman", "A. N.", ""]]}, {"id": "1906.03567", "submitter": "Thai T. Vu", "authors": "Thai T. Vu, Diep N. Nguyen, Dinh Thai Hoang, Eryk Dutkiewicz, Thuy V.\n  Nguyen", "title": "Optimal Energy Efficiency with Delay Constraints for Multi-layer\n  Cooperative Fog Computing Networks", "comments": "Final revision submitted to IEEE Transactions on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a joint offloading and resource allocation framework for a\nmulti-layer cooperative fog computing network, aiming to minimize the total\nenergy consumption of multiple mobile devices subject to their service delay\nrequirements. The resulting optimization involves both binary (offloading\ndecisions) and real variables (resource allocations), making it an NP-hard and\ncomputationally intractable problem. To tackle it, we first propose an improved\nbranch-and-bound algorithm (IBBA) that is implemented in a centralized manner.\nHowever, due to the large size of the cooperative fog computing network, the\ncomputational complexity of the proposed IBBA is relatively high. To speed up\nthe optimal solution searching as well as to enable its distributed\nimplementation, we then leverage the unique structure of the underlying problem\nand the parallel processing at fog nodes. To that end, we propose a distributed\nframework, namely feasibility finding Benders decomposition (FFBD), that\ndecomposes the original problem into a master problem for the offloading\ndecision and subproblems for resource allocation. The master problem (MP) is\nthen equipped with powerful cutting-planes to exploit the fact of resource\nlimitation at fog nodes. The subproblems (SP) for resource allocation can find\ntheir closed-form solutions using our fast solution detection method. These\n(simpler) subproblems can then be solved in parallel at fog nodes. The\nnumerical results show that the FFBD always returns the optimal solution of the\nproblem with significantly less computation time (e.g., compared with the\ncentralized IBBA approach). The FFBD with the fast solution detection method,\nnamely FFBD-F, can reduce up to $60\\%$ and $90\\%$ of computation time,\nrespectively, compared with those of the conventional FFBD, namely FFBD-S, and\nIBBA.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 05:06:26 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 03:32:25 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 14:24:51 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Vu", "Thai T.", ""], ["Nguyen", "Diep N.", ""], ["Hoang", "Dinh Thai", ""], ["Dutkiewicz", "Eryk", ""], ["Nguyen", "Thuy V.", ""]]}, {"id": "1906.03595", "submitter": "Rajagopal A", "authors": "Rajagopal. A, Nirmala. V", "title": "Federated AI lets a team imagine together: Federated Learning of GANs", "comments": "Keywords. Artificial Intelligence, Distributed Machine Learning,\n  Generative Deep Learning, Generative Adversarial Networks, Federated\n  learning, Creative AI, AI based Collaboration, AI planning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envisioning a new imaginative idea together is a popular human need.\nImagining together as a team can often lead to breakthrough ideas, but the\ncollaboration effort can also be challenging, especially when the team members\nare separated by time and space. What if there is a AI that can assist the team\nto collaboratively envision new ideas?. Is it possible to develop a working\nmodel of such an AI? This paper aims to design such an intelligence. This paper\nproposes a approach to design a creative and collaborative intelligence by\nemploying a form of distributed machine learning approach called Federated\nLearning along with fusion on Generative Adversarial Networks, GAN. This\ncollaborative creative AI presents a new paradigm in AI, one that lets a team\nof two or more to come together to imagine and envision ideas that synergies\nwell with interests of all members of the team. In short, this paper explores\nthe design of a novel type of AI paradigm, called Federated AI Imagination, one\nthat lets geographically distributed teams to collaboratively imagine.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 08:44:23 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["A", "Rajagopal.", ""], ["V", "Nirmala.", ""]]}, {"id": "1906.03623", "submitter": "Seyed Amir Alavi", "authors": "Seyed Amir Alavi, Kamyar Mehran, Yang Hao, Ardavan Rahimian, Hamed\n  Mirsaeedi, Vahid Vahidinasab", "title": "A Distributed Event-Triggered Control Strategy for DC Microgrids Based\n  on Publish-Subscribe Model Over Industrial Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TSG.2018.2856893", "report-no": null, "categories": "eess.SP cs.DC cs.MA cs.NI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a complete design, analysis, and performance evaluation\nof a novel distributed event-triggered control and estimation strategy for DC\nmicrogrids. The primary objective of this work is to efficiently stabilize the\ngrid voltage, and to further balance the energy level of the energy storage\n(ES) systems. The locally-installed distributed controllers are utilised to\nreduce the number of transmitted packets and battery usage of the installed\nsensors, based on a proposed event-triggered communication scheme. Also, to\nreduce the network traffic, an optimal observer is employed which utilizes a\nmodified Kalman consensus filter (KCF) to estimate the state of the DC\nmicrogrid via the distributed sensors. Furthermore, in order to effectively\nprovide an intelligent data exchange mechanism for the proposed event-triggered\ncontroller, the publish-subscribe communication model is employed to setup a\ndistributed control infrastructure in industrial wireless sensor networks\n(WSNs). The performance of the proposed control and estimation strategy is\nvalidated via the simulations of a DC microgrid composed of renewable energy\nsources (RESs). The results confirm the appropriateness of the implemented\nstrategy for the optimal utilization of the advanced industrial network\narchitectures in the smart grids.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 11:45:40 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Alavi", "Seyed Amir", ""], ["Mehran", "Kamyar", ""], ["Hao", "Yang", ""], ["Rahimian", "Ardavan", ""], ["Mirsaeedi", "Hamed", ""], ["Vahidinasab", "Vahid", ""]]}, {"id": "1906.03819", "submitter": "Alexander Spiegelman", "authors": "Kfir Lev-Ari, Alexander Spiegelman, Idit Keidar, and Dahlia Malkhi", "title": "FairLedger: A Fair Blockchain Protocol for Financial Institutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial institutions are currently looking into technologies for\npermissioned blockchains. A major effort in this direction is Hyperledger, an\nopen source project hosted by the Linux Foundation and backed by a consortium\nof over a hundred companies. A key component in permissioned blockchain\nprotocols is a byzantine fault tolerant (BFT) consensus engine that orders\ntransactions. However, currently available BFT solutions in Hyperledger (as\nwell as in the literature at large) are inadequate for financial settings; they\nare not designed to ensure fairness or to tolerate selfish behavior that arises\nwhen financial institutions strive to maximize their own profit.\n  We present FairLedger, a permissioned blockchain BFT protocol, which is fair,\ndesigned to deal with rational behavior, and, no less important, easy to\nunderstand and implement. The secret sauce of our protocol is a new\ncommunication abstraction, called detectable all-to-all (DA2A), which allows us\nto detect participants (byzantine or rational) that deviate from the protocol,\nand punish them. We implement FairLedger in the Hyperledger open source\nproject, using Iroha framework, one of the biggest projects therein. To\nevaluate FairLegder's performance, we also implement it in the PBFT framework\nand compare the two protocols. Our results show that in failure-free scenarios\nFairLedger achieves better throughput than both Iroha's implementation and PBFT\nin wide-area settings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 07:18:48 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Lev-Ari", "Kfir", ""], ["Spiegelman", "Alexander", ""], ["Keidar", "Idit", ""], ["Malkhi", "Dahlia", ""]]}, {"id": "1906.03884", "submitter": "Karthee Sivalingam", "authors": "Karthee Sivalingam, Harvey Richardson, Adrian Tate, Martin Lafferty", "title": "LASSi: Metric based I/O analytics for HPC", "comments": "12 pages, 6 figures, 3 tables, SpringSim-HPC, 2019 April 29-May 2,\n  Tucson, AZ, 2019 Society for Modeling and Simulation International (SCS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LASSi is a tool aimed at analyzing application usage and contention caused by\nuse of shared resources (filesystem or network) in a HPC system. LASSi was\ninitially developed to support the ARCHER system where there are large\nvariations in application requirements and occasional user complaints regarding\nfilesystem performance manifested by variation in job runtimes or poor\ninteractive response. LASSi takes an approach of defining derivative risk and\nops metrics that relate to unusually high application I/O behaviour. The\nmetrics are shown to correlate to applications that can experience variable\nperformance or that may impact the performance of other applications. LASSi\nuses I/O statistics over time to provide application I/O profiles and has been\nautomated to generate daily reports for ARCHER. We demonstrate how LASSi\nprovides holistic I/O analysis by monitoring filesystem I/O, generating coarse\nprofiles of filesystems and application runs and automating analysis of\napplication slowdown using metrics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 10:36:46 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Sivalingam", "Karthee", ""], ["Richardson", "Harvey", ""], ["Tate", "Adrian", ""], ["Lafferty", "Martin", ""]]}, {"id": "1906.03891", "submitter": "Karthee Sivalingam", "authors": "Andrew Turner, Dominic Sloan-Murphy, Karthee Sivalingam, Harvey\n  Richardson, Julian Kunkel", "title": "Analysis of parallel I/O use on the UK national supercomputing service,\n  ARCHER using Cray LASSi and EPCC SAFE", "comments": "15 pages, 19 figures, 5 tables, 2019 Cray User Group Meeting (CUG) ,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how we have used a combination of the LASSi tool\n(developed by Cray) and the SAFE software (developed by EPCC) to collect and\nanalyse Lustre I/O performance data for all jobs running on the UK national\nsupercomputing service, ARCHER; and to provide reports on I/O usage for users\nin our standard reporting framework. We also present results from analysis of\nparallel I/O use on ARCHER and analysis on the potential impact of different\napplications on file system performance using metrics we have derived from the\nLASSi data. We show that the performance data from LASSi reveals how the same\napplication can stress different components of the file system depending on how\nit is run, and how the LASSi risk metrics allow us to identify use cases that\ncould potentially cause issues for global I/O performance and work with users\nto improve their I/O use. We use the IO-500 benchmark to help us understand how\nLASSi risk metrics correspond to observed performance on the ARCHER file\nsystems. We also use LASSi data imported into SAFE to identify I/O use patterns\nassociated with different research areas, understand how the research workflow\ngives rise to the observed patterns and project how this will affect I/O\nrequirements in the future. Finally, we provide an overview of likely future\ndirections for the continuation of this work.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 10:45:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Turner", "Andrew", ""], ["Sloan-Murphy", "Dominic", ""], ["Sivalingam", "Karthee", ""], ["Richardson", "Harvey", ""], ["Kunkel", "Julian", ""]]}, {"id": "1906.03999", "submitter": "Hema Venkata Krishna Giri Narra", "authors": "Krishna Narra, Zhifeng Lin, Ganesh Ananthanarayanan, Salman\n  Avestimehr, Murali Annavaram", "title": "Collage Inference: Achieving low tail latency during distributed image\n  classification using coded redundancy models", "comments": "4 pages, CodML workshop at International Conference on Machine\n  Learning (ICML 2019). arXiv admin note: text overlap with arXiv:1904.12222", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing the latency variance in machine learning inference is a key\nrequirement in many applications. Variance is harder to control in a cloud\ndeployment in the presence of stragglers. In spite of this challenge, inference\nis increasingly being done in the cloud, due to the advent of affordable\nmachine learning as a service (MLaaS) platforms. Existing approaches to reduce\nvariance rely on replication which is expensive and partially negates the\naffordability of MLaaS. In this work, we argue that MLaaS platforms also\nprovide unique opportunities to cut the cost of redundancy. In MLaaS platforms,\nmultiple inference requests are concurrently received by a load balancer which\ncan then create a more cost-efficient redundancy coding across a larger\ncollection of images. We propose a novel convolutional neural network model,\nCollage-CNN, to provide a low-cost redundancy framework. A Collage-CNN model\ntakes a collage formed by combining multiple images and performs multi-image\nclassification in one shot, albeit at slightly lower accuracy. We then augment\na collection of traditional single image classifiers with a single Collage-CNN\nclassifier which acts as a low-cost redundant backup. Collage-CNN then provides\nbackup classification results if a single image classification straggles.\nDeploying the Collage-CNN models in the cloud, we demonstrate that the 99th\npercentile tail latency of inference can be reduced by 1.47X compared to\nreplication based approaches while providing high accuracy. Also, variation in\ninference latency can be reduced by 9X with a slight increase in average\ninference latency.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 20:18:58 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Narra", "Krishna", ""], ["Lin", "Zhifeng", ""], ["Ananthanarayanan", "Ganesh", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "1906.04051", "submitter": "Nikolaos Cheimarios", "authors": "E.I. Ioannidis, N. Cheimarios, A.N. Spyropoulos, A.G. Boudouvis", "title": "On the performance of various parallel GMRES implementations on CPU and\n  GPU clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the need for computational power and efficiency rises, parallel systems\nbecome increasingly popular among various scientific fields. While multiple\ncore-based architectures have been the center of attention for many years, the\nrapid development of general purposes GPU-based architectures takes high\nperformance computing to the next level. In this work, different\nimplementations of a parallel version of the preconditioned GMRES - an\nestablished iterative solver for large and sparse linear equation sets - are\npresented, each of them on different computing architectures: From distributed\nand shared memory core-based to GPU-based architectures. The computational\nexperiments emanate from the dicretization of a benchmark boundary value\nproblem with the finite element method. Major advantages and drawbacks of the\nvarious implementations are addressed in terms of parallel speedup, execution\ntime and memory issues. Among others, comparison of the results in the\ndifferent architectures, show the high potentials of GPU-based architectures.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:57:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ioannidis", "E. I.", ""], ["Cheimarios", "N.", ""], ["Spyropoulos", "A. N.", ""], ["Boudouvis", "A. G.", ""]]}, {"id": "1906.04381", "submitter": "Mohammad Jalalzai", "authors": "Mohammad M. Jalalzai, Costas Busch", "title": "Window Based BFT Blockchain Consensus", "comments": "2018 IEEE International Conference on Internet of Things (iThings)\n  and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber,\n  Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)", "journal-ref": null, "doi": "10.1109/Cybermatics_2018.2018.00184", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is surge of interest to the blockchain technology not only in the\nscientific community but in the business community as well. Proof of Work (PoW)\nand Byzantine Fault Tolerant (BFT) are the two main classes of consensus\nprotocols that are used in the blockchain consensus layer. PoW is highly\nscalable but very slow with about 7 (transactions/second) performance. BFT\nbased protocols are highly efficient but their scalability are limited to only\ntens of nodes. One of the main reasons for the BFT limitation is the quadratic\n$O(n^2)$ communication complexity of BFT based protocols for $n$ nodes that\nrequires $n \\times n$ broadcasting. In this paper, we present the {\\em Musch}\nprotocol which is BFT based and provides communication complexity $O(f n + n)$\nfor $f$ failures and $n$ nodes, where $f < n/3$, without compromising the\nlatency. Hence, the performance adjusts to $f$ such that for constant $f$ the\ncommunication complexity is linear. Musch achieves this by introducing the\nnotion of exponentially increasing windows of nodes to which complains are\nreported, instead of broadcasting to all the nodes. To our knowledge, this is\nthe first BFT-based blockchain protocol which efficiently addresses\nsimultaneously the issues of communication complexity and latency under the\npresence of failures.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 04:06:34 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Jalalzai", "Mohammad M.", ""], ["Busch", "Costas", ""]]}, {"id": "1906.04410", "submitter": "Yutaka Shikano", "authors": "Kentaro Tamura, Yutaka Shikano", "title": "Quantum Random Numbers generated by the Cloud Superconducting Quantum\n  Computer", "comments": "21 pages, 5 figures, submitted to the paper in Book \"Mathematics,\n  Quantum Theory, and Cryptography\" Mathematics for Industry, Springer. In the\n  revised manuscript, the results of the simulator with the noise parameters\n  were added", "journal-ref": "International Symposium on Mathematics, Quantum Theory, and\n  Cryptography, Mathematics for Industry, vol 33 (Springer, Singapore, 2021) 17\n  -- 37", "doi": "10.1007/978-981-15-5191-8_6", "report-no": null, "categories": "quant-ph cs.CR cs.DC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cloud quantum computer is similar to a random number generator in that its\nphysical mechanism is inaccessible to its users. In this respect, a cloud\nquantum computer is a black box. In both devices, its users decide the device\ncondition from the output. A framework to achieve this exists in the field of\nrandom number generation in the form of statistical tests for random number\ngenerators. In the present study, we generated random numbers on a 20-qubit\ncloud quantum computer and evaluated the condition and stability of its qubits\nusing statistical tests for random number generators. As a result, we observed\nthat some qubits were more biased than others. Statistical tests for random\nnumber generators may provide a simple indicator of qubit condition and\nstability, enabling users to decide for themselves which qubits inside a cloud\nquantum computer to use.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 06:36:30 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 22:02:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tamura", "Kentaro", ""], ["Shikano", "Yutaka", ""]]}, {"id": "1906.04488", "submitter": "Nicolas Skatchkovsky", "authors": "Nicolas Skatchkovsky and Osvaldo Simeone", "title": "Optimizing Pipelined Computation and Communication for\n  Latency-Constrained Edge Learning", "comments": "to be published in IEEE Communication Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a device that is connected to an edge processor via a communication\nchannel. The device holds local data that is to be offloaded to the edge\nprocessor so as to train a machine learning model, e.g., for regression or\nclassification. Transmission of the data to the learning processor, as well as\ntraining based on Stochastic Gradient Descent (SGD), must be both completed\nwithin a time limit. Assuming that communication and computation can be\npipelined, this letter investigates the optimal choice for the packet payload\nsize, given the overhead of each data packet transmission and the ratio between\nthe computation and the communication rates. This amounts to a tradeoff between\nbias and variance, since communicating the entire data set first reduces the\nbias of the training process but it may not leave sufficient time for learning.\nAnalytical bounds on the expected optimality gap are derived so as to enable an\neffective optimization, which is validated in numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:38:09 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 09:52:46 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Skatchkovsky", "Nicolas", ""], ["Simeone", "Osvaldo", ""]]}, {"id": "1906.04550", "submitter": "Siavash Ghiasvand", "authors": "Siavash Ghiasvand and Florina M. Ciorba", "title": "Anomaly Detection in High Performance Computers: A Vicinity Perspective", "comments": "9 pages, Submitted to the 18th IEEE International Symposium on\n  Parallel and Distributed Computing", "journal-ref": null, "doi": "10.1109/ispdc.2019.00024", "report-no": null, "categories": "cs.DC cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In response to the demand for higher computational power, the number of\ncomputing nodes in high performance computers (HPC) increases rapidly. Exascale\nHPC systems are expected to arrive by 2020. With drastic increase in the number\nof HPC system components, it is expected to observe a sudden increase in the\nnumber of failures which, consequently, poses a threat to the continuous\noperation of the HPC systems. Detecting failures as early as possible and,\nideally, predicting them, is a necessary step to avoid interruptions in HPC\nsystems operation. Anomaly detection is a well-known general purpose approach\nfor failure detection, in computing systems. The majority of existing methods\nare designed for specific architectures, require adjustments on the computing\nsystems hardware and software, need excessive information, or pose a threat to\nusers' and systems' privacy. This work proposes a node failure detection\nmechanism based on a vicinity-based statistical anomaly detection approach\nusing passively collected and anonymized system log entries. Application of the\nproposed approach on system logs collected over 8 months indicates an anomaly\ndetection precision between 62% to 81%.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 13:06:02 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ghiasvand", "Siavash", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1906.04624", "submitter": "Oksana Shadura", "authors": "Oksana Shadura (1), Brian Paul Bockelman (1) ((1) University of\n  Nebraska-Lincoln, (2) Morgridge Institute for Research)", "title": "ROOT I/O compression algorithms and their performance impact within Run\n  3", "comments": "Submitted to proceedings of ACAT 2019", "journal-ref": null, "doi": "10.1088/1742-6596/1525/1/012049", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LHCs Run3 will push the envelope on data-intensive workflows and, since\nat the lowest level this data is managed using the ROOT software framework,\npreparations for managing this data are starting already. At the beginning of\nLHC Run 1, all ROOT data was compressed with the ZLIB algorithm; since then,\nROOT has added support for additional algorithms such as LZMA and LZ4, each\nwith unique strengths. This work must continue as industry introduces new\ntechniques - ROOT can benefit saving disk space or reducing the I/O and\nbandwidth for online and offline needs of experiments by introducing better\ncompression algorithms. In addition to alternate algorithms, we have been\nexploring alternate techniques to improve parallelism and apply\npre-conditioners to the serialized data.\n  We have performed a survey of the performance of the new compression\ntechniques. Our survey includes various use cases of data compression of ROOT\nfiles provided by different LHC experiments. We also provide insight into\nsolutions applied to resolve bottlenecks in compression algorithms, resulting\nin improved ROOT performance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:36:03 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 07:51:44 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Shadura", "Oksana", ""], ["Bockelman", "Brian Paul", ""]]}, {"id": "1906.04703", "submitter": "Elad Michael Schiller (PhD)", "authors": "Antonio Casimiro, Emelie Ekenstedt and Elad Michael Schiller", "title": "Membership-based Manoeuvre Negotiation in Autonomous and Safety-critical\n  Vehicular Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fault-tolerant negotiation-based intersection crossing protocol is\npresented. Rigorous analytic proofs are used for demonstrating the correctness\nand fault-tolerance properties. Experimental results validate the correctness\nproof via detailed computer simulations and provide a preliminary evaluation of\nthe system performances. The results are compared to the ones that can be\nachieved via a risk estimator with and without combining the proposed protocol.\nOur fault model considers packet-loss, noisy sensory information and malicious\ndriving. Our preliminary results show a reduction in the number of dangerous\nsituations and vehicle collisions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:10:50 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Casimiro", "Antonio", ""], ["Ekenstedt", "Emelie", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1906.05020", "submitter": "Leonardo Bautista Gomez", "authors": "Julien Adam, Maxime Kermarquer, Jean-Baptiste Besnard, Leonardo\n  Bautista-Gomez, Marc Perache, Patrick Carribault, Julien Jaeger, Allen D.\n  Malony, Sameer Shende", "title": "Checkpoint/restart approaches for a thread-based MPI runtime", "comments": "This research has been partially sponsored by the European Union s\n  Horizon 2020 Programme under the LEGaTO Project (www.legato-project.eu),\n  grant agreement 780681 and the Mont-Blanc 2020 project, grant agreement no.\n  779877", "journal-ref": null, "doi": "10.1016/j.parco.2019.02.006", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault-tolerance has always been an important topic when it comes to running\nmassively parallel programs at scale. Statistically, hardware and software\nfailures are expected to occur more often on systems gathering millions of\ncomputing units. Moreover, the larger jobs are, the more computing hours would\nbe wasted by a crash. In this paper, we describe the work done in our MPI\nruntime to enable both transparent and application-level checkpointing\nmechanisms. Unlike the MPI 4.0 User-Level Failure Mitigation (ULFM) interface,\nour work targets solely Checkpoint/Restart and ignores other features such as\nresiliency. We show how existing checkpointing methods can be practically\napplied to a thread-based MPI implementation given sufficient runtime\ncollaboration. The two main contributions are the preservation of high-speed\nnetwork performance during transparent C/R and the over-subscription of\ncheckpoint data replication thanks to a dedicated user-level scheduler support.\nThese techniques are measured on MPI benchmarks such as IMB, Lulesh and\nHeatdis, and associated overhead and trade-offs are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 09:18:43 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Adam", "Julien", ""], ["Kermarquer", "Maxime", ""], ["Besnard", "Jean-Baptiste", ""], ["Bautista-Gomez", "Leonardo", ""], ["Perache", "Marc", ""], ["Carribault", "Patrick", ""], ["Jaeger", "Julien", ""], ["Malony", "Allen D.", ""], ["Shende", "Sameer", ""]]}, {"id": "1906.05038", "submitter": "Leonardo Bautista Gomez", "authors": "Kai Keller and Leonardo Bautista Gomez", "title": "Application-Level Differential Checkpointing for HPC Applications with\n  Dynamic Datasets", "comments": "This project has received funding from the European Unions Seventh\n  Framework Programme (FP7/2007-2013) and the Horizon 2020 (H2020) funding\n  framework under grant agreement no. H2020-FETHPC-754304 (DEEP-EST); and the\n  LEGaTO Project (legato- project.eu), grant agreement No 780681", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing (HPC) requires resilience techniques such as\ncheckpointing in order to tolerate failures in supercomputers. As the number of\nnodes and memory in supercomputers keeps on increasing, the size of checkpoint\ndata also increases dramatically, sometimes causing an I/O bottleneck.\nDifferential checkpointing (dCP) aims to minimize the checkpointing overhead by\nonly writing data differences. This is typically implemented at the memory page\nlevel, sometimes complemented with hashing algorithms. However, such a\ntechnique is unable to cope with dynamic-size datasets. In this work, we\npresent a novel dCP implementation with a new file format that allows\nfragmentation of protected datasets in order to support dynamic sizes. We\nidentify dirty data blocks using hash algorithms. In order to evaluate the dCP\nperformance, we ported the HPC applications xPic, LULESH 2.0 and Heat2D and\nanalyze them regarding their potential of reducing I/O with dCP and how this\ndata reduction influences the checkpoint performance. In our experiments, we\nachieve reductions of up to 62% of the checkpoint time.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 09:55:01 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Keller", "Kai", ""], ["Gomez", "Leonardo Bautista", ""]]}, {"id": "1906.05132", "submitter": "Nicolas Liochon", "authors": "Olivier B\\'egassat, Blazej Kolad, Nicolas Gailly, Nicolas Liochon", "title": "Handel: Practical Multi-Signature Aggregation for Large Byzantine\n  Committees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Handel, a Byzantine-tolerant aggregation protocol that allows for\nthe quick aggregation of cryptographic signatures over a WAN. Handel has\npolylogarithmic time, communication and processing complexity. We implemented\nHandel as an open source Go library with a flexible design to support any\nassociative and commutative aggregation function. We tested Handel on 2000 AWS\ninstances running two nodes per instance and located in 10 AWS regions. The\n4000 signatures are aggregated in less than 900 milliseconds with an average\nper-node communication cost of 56KB.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 13:38:25 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 18:18:13 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["B\u00e9gassat", "Olivier", ""], ["Kolad", "Blazej", ""], ["Gailly", "Nicolas", ""], ["Liochon", "Nicolas", ""]]}, {"id": "1906.05153", "submitter": "Christian Schindelhauer", "authors": "Christian Schindelhauer, Aditya Oak and Thomas Janson", "title": "Collaborative Broadcast in O(log log n) Rounds", "comments": "extended abstract accepted for ALGOSENSORS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the multihop broadcasting problem for $n$ nodes placed uniformly\nat random in a disk and investigate the number of hops required to transmit a\nsignal from the central node to all other nodes under three communication\nmodels: Unit-Disk-Graph (UDG), Signal-to-Noise-Ratio (SNR), and the wave\nsuperposition model of multiple input/multiple output (MIMO). In the MIMO\nmodel, informed nodes cooperate to produce a stronger superposed signal. We do\nnot consider the problem of transmitting a full message nor do we consider\ninterference. In each round, the informed senders try to deliver to other nodes\nthe required signal strength such that the received signal can be distinguished\nfrom the noise. We assume sufficiently high node density $\\rho= \\Omega(\\log\nn)$. In the unit-disk graph model, broadcasting needs $O(\\sqrt{n/\\rho})$\nrounds. In the other models, we use an Expanding Disk Broadcasting Algorithm,\nwhere in a round only triggered nodes within a certain distance from the\ninitiator node contribute to the broadcasting operation. This algorithm\nachieves a broadcast in only $O(\\frac{\\log n}{\\log \\rho})$ rounds in the\nSNR-model. Adapted to the MIMO model, it broadcasts within $O(\\log \\log n -\n\\log \\log \\rho)$ rounds. All bounds are asymptotically tight and hold with high\nprobability, i.e. $1- n^{-O(1)}$.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:09:29 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 17:52:28 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 09:16:27 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Schindelhauer", "Christian", ""], ["Oak", "Aditya", ""], ["Janson", "Thomas", ""]]}, {"id": "1906.05276", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev, Dmitry Ilin, Pavel Kolyasnikov, Ilya Zakharov,\n  Sergey Malykh", "title": "Programming Technologies for the Development of Web-Based Platform for\n  Digital Psychological Tools", "comments": "12 pages", "journal-ref": null, "doi": "10.14569/IJACSA.2018.090806", "report-no": null, "categories": "cs.HC cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The choice of the tools and programming technologies for information systems\ncreation is relevant. For every projected system, it is necessary to define a\nnumber of criteria for development environment, used libraries and\ntechnologies. The paper describes the choice of technological solutions using\nthe example of the developed web-based platform of the Russian Academy of\nEducation. This platform is used to provide information support for the\nactivities of psychologists in their research (including population and\nlongitudinal researches). There are following system features: large scale and\nsignificant amount of developing time that needs implementation and ensuring\nthe guaranteed computing reliability of a wide range of digital tools used in\npsychological research; ensuring functioning in different environments when\nconducting mass research in schools that have different characteristics of\ncomputing resources and communication channels; possibility of services\nscaling; security and privacy of data; use of technologies and programming\ntools that would ensure the compatibility and conversion of data with other\ntools of psychological research processing. Some criteria were introduced for\nthe developed system. These criteria take into account the feature of the\nfunctioning and life cycle of the software. A specific example shows the\nselection of appropriate technological solutions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 13:14:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Nikulchev", "Evgeny", ""], ["Ilin", "Dmitry", ""], ["Kolyasnikov", "Pavel", ""], ["Zakharov", "Ilya", ""], ["Malykh", "Sergey", ""]]}, {"id": "1906.05345", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas, Emina Soljanin", "title": "Optimizing Redundancy Levels in Master-Worker Compute Clusters for\n  Straggler Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime variability in computing systems causes some tasks to straggle and\ntake much longer than expected to complete. These straggler tasks are known to\nsignificantly slowdown distributed computation. Job execution with speculative\nexecution of redundant tasks has been the most widely deployed technique for\nmitigating the impact of stragglers, and many recent theoretical papers have\nstudied the advantages and disadvantages of using redundancy under various\nsystem and service models. However, no clear guidelines could yet be found on\nwhen, for which jobs, and how much redundancy should be employed in\nMaster-Worker compute clusters, which is the most widely adopted architecture\nin modern compute systems. We are concerned with finding a strategy for\nscheduling jobs with redundancy that works well in practice. This is a complex\noptimization problem, which we address in stages. We first use Reinforcement\nLearning (RL) techniques to learn good scheduling principles from realistic\nexperience. Building on these principles, we derive a simple scheduling policy\nand present an approximate analysis of its performance. Specifically, we derive\nexpressions to decide when and which jobs should be scheduled with how much\nredundancy. We show that policy that we devise in this way performs as good as\nthe more complex policies that are derived by RL. Finally, we extend our\napproximate analysis to the case when system employs the other widely deployed\nremedy for stragglers, which is relaunching straggler tasks after waiting some\ntime. We show that scheduling with redundancy significantly outperforms\nstraggler relaunch policy when the offered load on the system is low or\nmoderate, and performs slightly worse when the offered load is very high.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:37:01 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Soljanin", "Emina", ""]]}, {"id": "1906.05552", "submitter": "Chrysoula Stathakopoulou", "authors": "Chrysoula Stathakopoulou, Tudor David, Matej Pavlovic, Marko Vukoli\\'c", "title": "Mir-BFT: High-Throughput Robust BFT for Decentralized Networks", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Mir-BFT, a robust Byzantine fault-tolerant (BFT) total\norder broadcast protocol aimed at maximizing throughput on wide-area networks\n(WANs), targeting deployments in decentralized networks, such as permissioned\nand Proof-of-Stake permissionless blockchain systems.\n  Mir-BFT is the first BFT protocol that allows multiple leaders to propose\nrequest batches independently (i.e., parallel leaders), in a way that precludes\nrequest duplication attacks by malicious (Byzantine) clients, by rotating the\nassignment of a partitioned request hash space to leaders.\n  As this mechanism removes a single-leader bandwidth bottleneck and exposes a\ncomputation bottleneck related to authenticating clients even on a WAN, our\nprotocol further boosts throughput using a client signature verification\nsharding optimization.\n  Our evaluation shows that Mir-BFT outperforms state-of-the-art and orders\nmore than 60000 signed Bitcoin-sized (500-byte) transactions per second on a\nwidely distributed 100 nodes, 1 Gbps WAN setup, with typical latencies of few\nseconds.\n  We also evaluate Mir-BFT under different crash and Byzantine faults,\ndemonstrating its performance robustness.\n  Mir-BFT relies on classical BFT protocol constructs, which simplifies\nreasoning about its correctness.\n  Specifically, Mir-BFT is a generalization of the celebrated and scrutinized\nPBFT protocol. In a nutshell, Mir-BFT follows PBFT \"safety-wise\", with changes\nneeded to accommodate novel features restricted to PBFT liveness.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 08:58:33 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 11:25:03 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 17:03:01 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Stathakopoulou", "Chrysoula", ""], ["David", "Tudor", ""], ["Pavlovic", "Matej", ""], ["Vukoli\u0107", "Marko", ""]]}, {"id": "1906.05558", "submitter": "Wei Cai", "authors": "Tian Min, Hanyi Wang, Yaoze Guo and Wei Cai", "title": "Blockchain Games: A Survey", "comments": null, "journal-ref": "IEEE Conference on Games (CoG 2019), London, United Kingdom, Aug\n  20-23, 2019", "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the support of the blockchain systems, the cryptocurrency has changed\nthe world of virtual assets. Digital games, especially those with massive\nmulti-player scenarios, will be significantly impacted by this novel\ntechnology. However, there are insufficient academic studies on this topic. In\nthis work, we filled the blank by surveying the state-of-the-art blockchain\ngames. We discuss the blockchain integration for games and then categorize\nexisting blockchain games from the aspects of their genres and technical\nplatforms. Moreover, by analyzing the industrial trend with a statistical\napproach, we envision the future of blockchain games from technological and\ncommercial perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:16:19 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Min", "Tian", ""], ["Wang", "Hanyi", ""], ["Guo", "Yaoze", ""], ["Cai", "Wei", ""]]}, {"id": "1906.05574", "submitter": "Dragos-Adrian Seredinschi M.Sc.", "authors": "Rachid Guerraoui and Petr Kuznetsov and Matteo Monti and Matej\n  Pavlovic and Dragos-Adrian Seredinschi", "title": "The Consensus Number of a Cryptocurrency (Extended Version)", "comments": "11 pages. This is an extended version of a conference article\n  appearing in PODC'19. In the proceedings of the 2019 ACM Symposium on\n  Principles of Distributed Computing, July 29-August 2, 2019, Toronto, ON,\n  Canada. ACM, New York, NY, USA. arXiv admin note: substantial text overlap\n  with arXiv:1812.10844", "journal-ref": null, "doi": "10.1145/3293611.3331589", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many blockchain-based algorithms, such as Bitcoin, implement a decentralized\nasset transfer system, often referred to as a cryptocurrency. As stated in the\noriginal paper by Nakamoto, at the heart of these systems lies the problem of\npreventing double-spending; this is usually solved by achieving consensus on\nthe order of transfers among the participants. In this paper, we treat the\nasset transfer problem as a concurrent object and determine its consensus\nnumber, showing that consensus is, in fact, not necessary to prevent\ndouble-spending. We first consider the problem as defined by Nakamoto, where\nonly a single process---the account owner---can withdraw from each account.\nSafety and liveness need to be ensured for correct account owners, whereas\nmisbehaving account owners might be unable to perform transfers. We show that\nthe consensus number of an asset transfer object is $1$. We then consider a\nmore general $k$-shared asset transfer object where up to $k$ processes can\natomically withdraw from the same account, and show that this object has\nconsensus number $k$. We establish our results in the context of shared memory\nwith benign faults, allowing us to properly understand the level of difficulty\nof the asset transfer problem. We also translate these results in the message\npassing setting with Byzantine players, a model that is more relevant in\npractice. In this model, we describe an asynchronous Byzantine fault-tolerant\nasset transfer implementation that is both simpler and more efficient than\nstate-of-the-art consensus-based solutions. Our results are applicable to both\nthe permissioned (private) and permissionless (public) setting, as normally\ntheir differentiation is hidden by the abstractions on top of which our\nalgorithms are based.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:45:24 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Kuznetsov", "Petr", ""], ["Monti", "Matteo", ""], ["Pavlovic", "Matej", ""], ["Seredinschi", "Dragos-Adrian", ""]]}, {"id": "1906.05677", "submitter": "Yuanyuan Tian", "authors": "Brian Hentschel, Peter J. Haas, Yuanyuan Tian", "title": "Temporally-Biased Sampling Schemes for Online Model Management", "comments": "49 pages, 18 figures. arXiv admin note: substantial text overlap with\n  arXiv:1801.09709", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To maintain the accuracy of supervised learning models in the presence of\nevolving data streams, we provide temporally-biased sampling schemes that\nweight recent data most heavily, with inclusion probabilities for a given data\nitem decaying over time according to a specified \"decay function\". We then\nperiodically retrain the models on the current sample. This approach speeds up\nthe training process relative to training on all of the data. Moreover,\ntime-biasing lets the models adapt to recent changes in the data while---unlike\nin a sliding-window approach---still keeping some old data to ensure robustness\nin the face of temporary fluctuations and periodicities in the data values. In\naddition, the sampling-based approach allows existing analytic algorithms for\nstatic data to be applied to dynamic streaming data essentially without change.\nWe provide and analyze both a simple sampling scheme (T-TBS) that\nprobabilistically maintains a target sample size and a novel reservoir-based\nscheme (R-TBS) that is the first to provide both control over the decay rate\nand a guaranteed upper bound on the sample size. If the decay function is\nexponential, then control over the decay rate is complete, and R-TBS maximizes\nboth expected sample size and sample-size stability. For general decay\nfunctions, the actual item inclusion probabilities can be made arbitrarily\nclose to the nominal probabilities, and we provide a scheme that allows a\ntrade-off between sample footprint and sample-size stability. The R-TBS and\nT-TBS schemes are of independent interest, extending the known set of\nunequal-probability sampling schemes. We discuss distributed implementation\nstrategies; experiments in Spark illuminate the performance and scalability of\nthe algorithms, and show that our approach can increase machine learning\nrobustness in the face of evolving data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:02:38 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hentschel", "Brian", ""], ["Haas", "Peter J.", ""], ["Tian", "Yuanyuan", ""]]}, {"id": "1906.05832", "submitter": "Ruosong Wang", "authors": "Santosh S. Vempala and Ruosong Wang and David P. Woodruff", "title": "The Communication Complexity of Optimization", "comments": "To appear in SODA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the communication complexity of a number of distributed\noptimization problems. We start with the problem of solving a linear system.\nSuppose there is a coordinator together with $s$ servers $P_1, \\ldots, P_s$,\nthe $i$-th of which holds a subset $A^{(i)} x = b^{(i)}$ of $n_i$ constraints\nof a linear system in $d$ variables, and the coordinator would like to output\n$x \\in \\mathbb{R}^d$ for which $A^{(i)} x = b^{(i)}$ for $i = 1, \\ldots, s$. We\nassume each coefficient of each constraint is specified using $L$ bits. We\nfirst resolve the randomized and deterministic communication complexity in the\npoint-to-point model of communication, showing it is $\\tilde{\\Theta}(d^2L +\nsd)$ and $\\tilde{\\Theta}(sd^2L)$, respectively. We obtain similar results for\nthe blackboard model.\n  When there is no solution to the linear system, a natural alternative is to\nfind the solution minimizing the $\\ell_p$ loss. While this problem has been\nstudied, we give improved upper or lower bounds for every value of $p \\ge 1$.\nOne takeaway message is that sampling and sketching techniques, which are\ncommonly used in earlier work on distributed optimization, are neither optimal\nin the dependence on $d$ nor on the dependence on the approximation $\\epsilon$,\nthus motivating new techniques from optimization to solve these problems.\n  Towards this end, we consider the communication complexity of optimization\ntasks which generalize linear systems. For linear programming, we first resolve\nthe communication complexity when $d$ is constant, showing it is\n$\\tilde{\\Theta}(sL)$ in the point-to-point model. For general $d$ and in the\npoint-to-point model, we show an $\\tilde{O}(sd^3 L)$ upper bound and an\n$\\tilde{\\Omega}(d^2 L + sd)$ lower bound. We also show if one perturbs the\ncoefficients randomly by numbers as small as $2^{-\\Theta(L)}$, then the upper\nbound is $\\tilde{O}(sd^2 L) + \\textrm{poly}(dL)$.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 17:29:56 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 03:46:29 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Vempala", "Santosh S.", ""], ["Wang", "Ruosong", ""], ["Woodruff", "David P.", ""]]}, {"id": "1906.05922", "submitter": "Bing Li Dr.", "authors": "Bing Li, Mengjie Mao, Xiaoxiao Liu, Tao Liu, Zihao Liu, Wujie Wen,\n  Yiran Chen, Hai (Helen) Li", "title": "Thread Batching for High-performance Energy-efficient GPU Memory Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multi-threading in GPU imposes tremendous pressure on memory\nsubsystems. Due to rapid growth in thread-level parallelism of GPU and slowly\nimproved peak memory bandwidth, the memory becomes a bottleneck of GPU's\nperformance and energy efficiency. In this work, we propose an integrated\narchitectural scheme to optimize the memory accesses and therefore boost the\nperformance and energy efficiency of GPU. Firstly, we propose a thread batch\nenabled memory partitioning (TEMP) to improve GPU memory access parallelism. In\nparticular, TEMP groups multiple thread blocks that share the same set of pages\ninto a thread batch and applies a page coloring mechanism to bound each stream\nmultiprocessor (SM) to the dedicated memory banks. After that, TEMP dispatches\nthe thread batch to an SM to ensure high-parallel memory-access streaming from\nthe different thread blocks. Secondly, a thread batch-aware scheduling (TBAS)\nscheme is introduced to improve the GPU memory access locality and to reduce\nthe contention on memory controllers and interconnection networks. Experimental\nresults show that the integration of TEMP and TBAS can achieve up to 10.3%\nperformance improvement and 11.3% DRAM energy reduction across diverse GPU\napplications. We also evaluate the performance interference of the mixed\nCPU+GPU workloads when they are run on a heterogeneous system that employs our\nproposed schemes. Our results show that a simple solution can effectively\nensure the efficient execution of both GPU and CPU applications.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:44:51 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Li", "Bing", "", "Helen"], ["Mao", "Mengjie", "", "Helen"], ["Liu", "Xiaoxiao", "", "Helen"], ["Liu", "Tao", "", "Helen"], ["Liu", "Zihao", "", "Helen"], ["Wen", "Wujie", "", "Helen"], ["Chen", "Yiran", "", "Helen"], ["Hai", "", "", "Helen"], ["Li", "", ""]]}, {"id": "1906.05936", "submitter": "Kwangmin Yu", "authors": "Kwangmin Yu and Thomas Flynn and Shinjae Yoo and Nicholas D'Imperio", "title": "Layered SGD: A Decentralized and Synchronous SGD Algorithm for Scalable\n  Deep Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) is the most popular algorithm for training\ndeep neural networks (DNNs). As larger networks and datasets cause longer\ntraining times, training on distributed systems is common and distributed SGD\nvariants, mainly asynchronous and synchronous SGD, are widely used.\nAsynchronous SGD is communication efficient but suffers from accuracy\ndegradation due to delayed parameter updating. Synchronous SGD becomes\ncommunication intensive when the number of nodes increases regardless of its\nadvantage. To address these issues, we introduce Layered SGD (LSGD), a new\ndecentralized synchronous SGD algorithm. LSGD partitions computing resources\ninto subgroups that each contain a communication layer (communicator) and a\ncomputation layer (worker). Each subgroup has centralized communication for\nparameter updates while communication between subgroups is handled by\ncommunicators. As a result, communication time is overlapped with I/O latency\nof workers. The efficiency of the algorithm is tested by training a deep\nnetwork on the ImageNet classification task.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:31:55 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Yu", "Kwangmin", ""], ["Flynn", "Thomas", ""], ["Yoo", "Shinjae", ""], ["D'Imperio", "Nicholas", ""]]}, {"id": "1906.06205", "submitter": "Chi Zhang", "authors": "Chi Zhang, Qianxiao Li", "title": "Distributed Optimization for Over-Parameterized Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization often consists of two updating phases: local\noptimization and inter-node communication. Conventional approaches require\nworking nodes to communicate with the server every one or few iterations to\nguarantee convergence. In this paper, we establish a completely different\nconclusion that each node can perform an arbitrary number of local optimization\nsteps before communication. Moreover, we show that the more local updating can\nreduce the overall communication, even for an infinity number of steps where\neach node is free to update its local model to near-optimality before\nexchanging information. The extra assumption we make is that the optimal sets\nof local loss functions have a non-empty intersection, which is inspired by the\nover-paramterization phenomenon in large-scale optimization and deep learning.\nOur theoretical findings are confirmed by both distributed convex optimization\nand deep learning experiments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 13:52:26 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Zhang", "Chi", ""], ["Li", "Qianxiao", ""]]}, {"id": "1906.06239", "submitter": "Alexandre Maurer", "authors": "Rachid Guerraoui and Alexandre Maurer", "title": "Gathering with extremely restricted visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of making mobile processes gather or\nconverge at a same position (as performed by swarms of animals in Nature).\nExisting works assume that each process can see all other processes, or all\nprocesses within a certain radius. In this paper, we introduce a new model with\nan extremely restricted visibility: each process can only see one other process\n(its closest neighbor). Our goal is to see if (and to what extent) the\ngathering and convergence problems can be solved in this setting. We first show\nthat, surprisingly, the problem can be solved for a small number of processes\n(at most 5), but not beyond. This is due to indeterminacy in the case where\nthere are several closest neighbors for a same process. By removing this\nindeterminacy with an additional hypothesis (choosing the closest neighbor\naccording to an order on the positions of processes), we then show that the\nproblem can be solved for any number of processes. We also show that up to one\ncrash failure can be tolerated for the convergence problem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:11:45 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Guerraoui", "Rachid", ""], ["Maurer", "Alexandre", ""]]}, {"id": "1906.06240", "submitter": "Mario Almeida", "authors": "Mario Almeida, Liang Wang, Jeremy Blackburn, Konstantina Papagiannaki,\n  Jon Crowcroft", "title": "Diffusing Your Mobile Apps: Extending In-Network Function Virtualization\n  to Mobile Function Offloading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the huge disparity between the limited battery capacity of user\ndevices and the ever-growing energy demands of modern mobile apps, we propose\nINFv. It is the first offloading system able to cache, migrate and dynamically\nexecute on demand functionality from mobile devices in ISP networks. It aims to\nbridge this gap by extending the promising NFV paradigm to mobile applications\nin order to exploit in-network resources. In this paper, we present the overall\ndesign, state-of-the-art technologies adopted, and various engineering details\nin the INFv system. We also carefully study the deployment configurations by\ninvestigating over 20K Google Play apps, as well as thorough evaluations with\nrealistic settings. In addition to a significant improvement in battery life\n(up to 6.9x energy reduction) and execution time (up to 4x faster), INFv has\ntwo distinct advantages over previous systems: 1) a non-intrusive offloading\nmechanism transparent to existing apps; 2) an inherent framework support to\neffectively balance computation load and exploit the proximity of in-network\nresources. Both advantages together enable a scalable and incremental\ndeployment of computation offloading framework in practical ISPs' networks.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:12:41 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Almeida", "Mario", ""], ["Wang", "Liang", ""], ["Blackburn", "Jeremy", ""], ["Papagiannaki", "Konstantina", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1906.06242", "submitter": "Joosep Pata", "authors": "Joosep Pata, Maria Spiropulu", "title": "Processing Columnar Collider Data with GPU-Accelerated Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At high energy physics experiments, processing billions of records of\nstructured numerical data from collider events to a few statistical summaries\nis a common task. The data processing is typically more complex than standard\nquery languages allow, such that custom numerical codes are used. At present,\nthese codes mostly operate on individual event records and are parallelized in\nmulti-step data reduction workflows using batch jobs across CPU farms. Based on\na simplified top quark pair analysis with CMS Open Data, we demonstrate that it\nis possible to carry out significant parts of a collider analysis at a rate of\naround a million events per second on a single multicore server with optional\nGPU acceleration. This is achieved by representing HEP event data as\nmemory-mappable sparse arrays of columns, and by expressing common analysis\noperations as kernels that can be used to process the event data in parallel.\nWe find that only a small number of relatively simple functional kernels are\nneeded for a generic HEP analysis. The approach based on columnar processing of\ndata could speed up and simplify the cycle for delivering physics results at\nHEP experiments. We release the \\texttt{hepaccelerate} prototype library as a\ndemonstrator of such methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:14:03 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 05:33:34 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Pata", "Joosep", ""], ["Spiropulu", "Maria", ""]]}, {"id": "1906.06297", "submitter": "Joshua Romero", "authors": "Joshua Romero, Mauro Bisson, Massimiliano Fatica, Massimo Bernaschi", "title": "A Performance Study of the 2D Ising Model on GPUs", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2020.107473", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of the two-dimensional Ising model is used as a benchmark to\nshow the computational capabilities of Graphic Processing Units (GPUs). The\nrich programming environment now available on GPUs and flexible hardware\ncapabilities allowed us to quickly experiment with several implementation\nideas: a simple stencil-based algorithm, recasting the stencil operations into\nmatrix multiplies to take advantage of Tensor Cores available on NVIDIA GPUs,\nand a highly optimized multi-spin coding approach. Using the managed memory API\navailable in CUDA allows for simple and efficient distribution of these\nimplementations across a multi-GPU NVIDIA DGX-2 server. We show that even a\nbasic GPU implementation can outperform current results published on TPUs and\nthat the optimized multi-GPU implementation can simulate very large lattices\nfaster than custom FPGA solutions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 17:09:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Romero", "Joshua", ""], ["Bisson", "Mauro", ""], ["Fatica", "Massimiliano", ""], ["Bernaschi", "Massimo", ""]]}, {"id": "1906.06420", "submitter": "Elad Michael Schiller (PhD)", "authors": "Chryssis Georgiou, Oskar Lundstr\\\"om and Elad Michael Schiller", "title": "Self-Stabilizing Snapshot Objects for Asynchronous Fail-Prone Network\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A snapshot object simulates the behavior of an array of\nsingle-writer/multi-reader shared registers that can be read atomically.\nDelporte-Gallet et al. proposed two fault-tolerant algorithms for snapshot\nobjects in asynchronous crash-prone message-passing systems. Their first\nalgorithm is \\emph{non-blocking}; it allows snapshot operations to terminate\nonce all write operations have ceased. It uses $O(n)$ messages of $O(n \\nu)$\nbits, where $n$ is the number of nodes and $\\nu$ is the number of bits it takes\nto represent the object. Their second algorithm allows snapshot operations to\nalways terminate independently of write operations. It incurs $O(n^2)$\nmessages.\n  The fault model of Delporte-Gallet et al. considers node crashes. We aim at\nthe design of even more robust snapshot objects via the lenses of\nself-stabilization---a very strong notion of fault-tolerance. In addition to\nDelporte-Gallet et al.'s fault model, our self-stabilizing algorithm can\nrecover after the occurrence of transient faults; these faults represent\narbitrary violations of the assumptions according to which the system was\ndesigned to operate.\n  We propose self-stabilizing variations of Delporte-Gallet et al.'s\nnon-blocking algorithm and always-terminating algorithm. Our algorithms have\nsimilar communication costs to the ones by Delporte-Gallet et al. and $O(1)$\nrecovery time from transient faults. The main differences are that our proposal\nconsiders repeated gossiping of $O(\\nu)$ bit messages and deals with bounded\nspace. We also consider an input parameter, $\\delta$, for which we claim an\nability to balance the costs of snapshot operations. We validate our\ncorrectness proof, evaluate the performance of Delporte-Gallet et al.'s\nalgorithms and our proposed variations and investigate the properties of\n$\\delta$ via PlanetLab experiments, where significant latency and communication\ncosts reduction are observed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 22:04:43 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 21:48:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Georgiou", "Chryssis", ""], ["Lundstr\u00f6m", "Oskar", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1906.06432", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, and Sungchul Kim", "title": "Linear-time Hierarchical Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in graphs has many important and fundamental applications\nincluding in distributed systems, compression, image segmentation,\ndivide-and-conquer graph algorithms such as nested dissection, document and\nword clustering, circuit design, among many others. Finding these densely\nconnected regions of graphs remains an important and challenging problem. Most\nwork has focused on scaling up existing methods to handle large graphs. These\nmethods often partition the graph into two or more communities. In this work,\nwe focus on the problem of hierarchical community detection (i.e., finding a\nhierarchy of dense community structures going from the lowest granularity to\nthe largest) and describe an approach that runs in linear time with respect to\nthe number of edges and thus fast and efficient for large-scale networks. The\nexperiments demonstrate the effectiveness of the approach quantitatively.\nFinally, we show an application of it for visualizing large networks with\nhundreds of thousands of nodes/links.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:29:37 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""]]}, {"id": "1906.06440", "submitter": "Evangelos Georganas", "authors": "Evangelos Georganas, Kunal Banerjee, Dhiraj Kalamkar, Sasikanth\n  Avancha, Anand Venkat, Michael Anderson, Greg Henry, Hans Pabst, Alexander\n  Heinecke", "title": "High-Performance Deep Learning via a Single Building Block", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) is one of the most prominent branches of machine learning.\nDue to the immense computational cost of DL workloads, industry and academia\nhave developed DL libraries with highly-specialized kernels for each\nworkload/architecture, leading to numerous, complex code-bases that strive for\nperformance, yet they are hard to maintain and do not generalize. In this work,\nwe introduce the batch-reduce GEMM kernel and show how the most popular DL\nalgorithms can be formulated with this kernel as the basic building-block.\nConsequently, the DL library-development degenerates to mere (potentially\nautomatic) tuning of loops around this sole optimized kernel. By exploiting our\nnew kernel we implement Recurrent Neural Networks, Convolution Neural Networks\nand Multilayer Perceptron training and inference primitives in just 3K lines of\nhigh-level code. Our primitives outperform vendor-optimized libraries on\nmulti-node CPU clusters, and we also provide proof-of-concept CNN kernels\ntargeting GPUs. Finally, we demonstrate that the batch-reduce GEMM kernel\nwithin a tensor compiler yields high-performance CNN primitives, further\namplifying the viability of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 00:02:36 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 03:56:02 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Georganas", "Evangelos", ""], ["Banerjee", "Kunal", ""], ["Kalamkar", "Dhiraj", ""], ["Avancha", "Sasikanth", ""], ["Venkat", "Anand", ""], ["Anderson", "Michael", ""], ["Henry", "Greg", ""], ["Pabst", "Hans", ""], ["Heinecke", "Alexander", ""]]}, {"id": "1906.06490", "submitter": "Fangyu Gai", "authors": "Fangyu Gai, Cesar Grajales, Jianyu Niu, Mohammad Mussadiq Jalalzai,\n  and Chen Feng", "title": "A Secure Consensus Protocol for Sidechains", "comments": "17 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sidechain technology has been envisioned as a promising solution to\naccelerate today's public blockchains in terms of scalability and\ninteroperability. By relying on the mainchain for security, different\nsidechains can formulate their own rules to reach consensus. Although the\nliterature has considered the possibility of using consensus protocols in the\nsidechain, so far a tailor-made consensus protocol for sidechains with high\nperformance and formal security proof has not been attempted. To fill this gap,\nwe introduce Cumulus, a low overhead, highly efficient, security provable\nsidechain protocol. Cumulus makes use of smart contracts to ensure that only\none block proposed in the sidechain will be enforced on the mainchain in each\nround, thereby achieving consensus in an efficient manner. We give a formal\nspecification of Cumulus which ensures safety and liveness without any online\nrequirements of clients. For security analysis, we provide formal security\ndefinitions and proofs under Universally Composable Security (UCS) model. As a\nproof of concept, we implement Cumulus and evaluate it in an Ethereum testnet.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 07:48:31 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 00:42:08 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 01:31:29 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Gai", "Fangyu", ""], ["Grajales", "Cesar", ""], ["Niu", "Jianyu", ""], ["Jalalzai", "Mohammad Mussadiq", ""], ["Feng", "Chen", ""]]}, {"id": "1906.06496", "submitter": "Shiye Lei", "authors": "Tian Wang, Shiye Lei, Youyou Jiang, Choi Chang, Hichem Snoussi,\n  Guangcun Shan", "title": "Accelerating temporal action proposal generation via high performance\n  computing", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action recognition always depends on temporal action proposal\ngeneration to hypothesize actions and algorithms usually need to process very\nlong video sequences and output the starting and ending times of each potential\naction in each video suffering from high computation cost. To address this,\nbased on boundary sensitive network we propose a new temporal convolution\nnetwork called Multipath Temporal ConvNet (MTN), which consists of two parts\ni.e. Multipath DenseNet and SE-ConvNet. In this work, one novel high\nperformance ring parallel architecture based on Message Passing Interface (MPI)\nis further introduced into temporal action proposal generation, which is a\nreliable communication protocol, in order to respond to the requirements of\nlarge memory occupation and a large number of videos. Remarkably, the total\ndata transmission is reduced by adding a connection between multiple computing\nload in the newly developed architecture. It is found that, compared to the\ntraditional Parameter Server architecture, our parallel architecture has higher\nefficiency on temporal action detection task with multiple GPUs, which is\nsuitable for dealing with the tasks of temporal action proposal generation,\nespecially for large datasets of millions of videos. We conduct experiments on\nActivityNet-1.3 and THUMOS14, where our method outperforms other state-of-art\ntemporal action detection methods with high recall and high temporal precision.\nIn addition, a time metric is further proposed here to evaluate the speed\nperformance in the distributed training process.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 08:35:34 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 05:12:50 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 10:08:11 GMT"}, {"version": "v4", "created": "Fri, 24 Apr 2020 06:35:10 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Wang", "Tian", ""], ["Lei", "Shiye", ""], ["Jiang", "Youyou", ""], ["Chang", "Choi", ""], ["Snoussi", "Hichem", ""], ["Shan", "Guangcun", ""]]}, {"id": "1906.06504", "submitter": "Yanhao Chen", "authors": "Yanhao Chen (1), Fei Hua (1), Chaozhang Huang (1), Jeremy Bierema (1),\n  Chi Zhang (2), Eddy Z. Zhang (1) ((1) Rutgers University, New Brunswick, NJ,\n  USA, (2) University Of Pittsburgh, Pittsburgh, PA, USA)", "title": "Accelerating Concurrent Heap on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priority queue, often implemented as a heap, is an abstract data type that\nhas been used in many well-known applications like Dijkstra's shortest path\nalgorithm, Prim's minimum spanning tree, Huffman encoding, and the\nbranch-and-bound algorithm. However, it is challenging to exploit the\nparallelism of the heap on GPUs since the control divergence and memory\nirregularity must be taken into account. In this paper, we present a parallel\ngeneralized heap model that works effectively on GPUs. We also prove the\nlinearizability of our generalized heap model which enables us to reason about\nthe expected results. We evaluate our concurrent heap thoroughly and show a\nmaximum 19.49X speedup compared to the sequential CPU implementation and 2.11X\nspeedup compared with the existing GPU implementation. We also apply our heap\nto single source shortest path with up to 1.23X speedup and 0/1 knapsack\nproblem with up to 12.19X speedup.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 09:14:11 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chen", "Yanhao", ""], ["Hua", "Fei", ""], ["Huang", "Chaozhang", ""], ["Bierema", "Jeremy", ""], ["Zhang", "Chi", ""], ["Zhang", "Eddy Z.", ""]]}, {"id": "1906.06851", "submitter": "Sheng Yang", "authors": "Mosharaf Chowdhury, Samir Khuller, Manish Purohit, Sheng Yang, Jie You", "title": "Near Optimal Coflow Scheduling in Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3323165.3323179", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coflow scheduling problem has emerged as a popular abstraction in the\nlast few years to study data communication problems within a data center. In\nthis basic framework, each coflow has a set of communication demands and the\ngoal is to schedule many coflows in a manner that minimizes the total weighted\ncompletion time. A coflow is said to complete when all its communication needs\nare met. This problem has been extremely well studied for the case of complete\nbipartite graphs that model a data center with full bisection bandwidth and\nseveral approximation algorithms and effective heuristics have been proposed\nrecently.\n  In this work, we study a slightly different model of coflow scheduling in\ngeneral graphs (to capture traffic between data centers) and develop practical\nand efficient approximation algorithms for it. Our main result is a randomized\n2 approximation algorithm for the single path and free path model,\nsignificantly improving prior work. In addition, we demonstrate via extensive\nexperiments that the algorithm is practical, easy to implement and performs\nwell in practice.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 05:51:33 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Chowdhury", "Mosharaf", ""], ["Khuller", "Samir", ""], ["Purohit", "Manish", ""], ["Yang", "Sheng", ""], ["You", "Jie", ""]]}, {"id": "1906.06956", "submitter": "Panagiotis Tampakis", "authors": "Panagiotis Tampakis, Nikos Pelekis, Christos Doulkeridis and Yannis\n  Theodoridis", "title": "Scalable Distributed Subtrajectory Clustering", "comments": null, "journal-ref": "2019 IEEE International Conference on Big Data (Big Data)", "doi": "10.1109/BigData47090.2019.9005563", "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory clustering is an important operation of knowledge discovery from\nmobility data. Especially nowadays, the need for performing advanced analytic\noperations over massively produced data, such as mobility traces, in efficient\nand scalable ways is imperative. However, discovering clusters of complete\ntrajectories can overlook significant patterns that exist only for a small\nportion of their lifespan. In this paper, we address the problem of Distributed\nSubtrajectory Clustering in an efficient and highly scalable way. The problem\nis challenging because the subtrajectories to be clustered are not known in\nadvance, but they need to be discovered dynamically based on adjacent\nsubtrajectories in space and time. Towards this objective, we split the\noriginal problem to three sub-problems, namely Subtrajectory Join, Trajectory\nSegmentation and Clustering and Outlier Detection, and deal with each one in a\ndistributed fashion by utilizing the MapReduce programming model. The\nefficiency and the effectiveness of our solution is demonstrated experimentally\nover a synthetic and two large real datasets from the maritime and urban\ndomains and through comparison with two state of the art subtrajectory\nclustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 11:15:21 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 07:32:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tampakis", "Panagiotis", ""], ["Pelekis", "Nikos", ""], ["Doulkeridis", "Christos", ""], ["Theodoridis", "Yannis", ""]]}, {"id": "1906.07055", "submitter": "Shiqiang Wang", "authors": "Stephen Pasteris, Shiqiang Wang, Mark Herbster, Ting He", "title": "Service Placement with Provable Guarantees in Heterogeneous Edge\n  Computing Systems", "comments": "Extended version of paper presented at IEEE INFOCOM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) is a promising technique for providing\nlow-latency access to services at the network edge. The services are hosted at\nvarious types of edge nodes with both computation and communication\ncapabilities. Due to the heterogeneity of edge node characteristics and user\nlocations, the performance of MEC varies depending on where the service is\nhosted. In this paper, we consider such a heterogeneous MEC system, and focus\non the problem of placing multiple services in the system to maximize the total\nreward. We show that the problem is NP-hard via reduction from the set cover\nproblem, and propose a deterministic approximation algorithm to solve the\nproblem, which has an approximation ratio that is not worse than\n$\\left(1-e^{-1}\\right)/4$. The proposed algorithm is based on two sub-routines\nthat are suitable for small and arbitrarily sized services, respectively. The\nalgorithm is designed using a novel way of partitioning each edge node into\nmultiple slots, where each slot contains one service. The approximation\nguarantee is obtained via a specialization of the method of conditional\nexpectations, which uses a randomized procedure as an intermediate step. In\naddition to theoretical guarantees, simulation results also show that the\nproposed algorithm outperforms other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 14:29:35 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Pasteris", "Stephen", ""], ["Wang", "Shiqiang", ""], ["Herbster", "Mark", ""], ["He", "Ting", ""]]}, {"id": "1906.07105", "submitter": "Adones Rukundo Mr", "authors": "Adones Rukundo, Aras Atalar and Philippas Tsigas", "title": "Monotonically relaxing concurrent data-structure semantics for\n  performance: An efficient 2D design framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a significant amount of work in the literature proposing\nsemantic relaxation of concurrent data structures for improving scalability and\nperformance. By relaxing the semantics of a data structure, a bigger design\nspace, that allows weaker synchronization and more useful parallelism, is\nunveiled. Investigating new data structure designs, capable of trading\nsemantics for achieving better performance in a monotonic way, is a major\nchallenge in the area. We algorithmically address this challenge in this paper.\nWe present an efficient, lock-free, concurrent data structure design framework\nfor out-of-order semantic relaxation. Our framework introduces a new two\ndimensional algorithmic design, that uses multiple instances of a given data\nstructure. The first dimension of our design is the number of data structure\ninstances operations are spread to, in order to benefit from parallelism\nthrough disjoint memory access. The second dimension is the number of\nconsecutive operations that try to use the same data structure instance in\norder to benefit from data locality. Our design can flexibly explore this\ntwo-dimensional space to achieve the property of monotonically relaxing\nconcurrent data structure semantics for achieving better throughput performance\nwithin a tight deterministic relaxation bound, as we prove in the paper. We\nshow how our framework can instantiate lock-free out-of-order queues, stacks,\ncounters and dequeues. We provide implementations of these relaxed data\nstructures and evaluate their performance and behaviour on two parallel\narchitectures. Experimental evaluation shows that our two-dimensional data\nstructures significantly outperform the respected previous proposed ones with\nrespect to scalability and throughput performance. Moreover, their throughput\nincreases monotonically as relaxation increases.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 16:13:15 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Rukundo", "Adones", ""], ["Atalar", "Aras", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1906.07225", "submitter": "Ming Yan", "authors": "Yao Li and Ming Yan", "title": "On linear convergence of two decentralized algorithms", "comments": "accepted to JOTA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized algorithms solve multi-agent problems over a connected network,\nwhere the information can only be exchanged with the accessible neighbors.\nThough there exist several decentralized optimization algorithms, there are\nstill gaps in convergence conditions and rates between decentralized and\ncentralized algorithms. In this paper, we fill some gaps by considering two\ndecentralized algorithms: EXTRA and NIDS. They both converge linearly with\nstrongly convex objective functions. We will answer two questions regarding\nthem. What are the optimal upper bounds for their stepsizes? Do decentralized\nalgorithms require more properties on the functions for linear convergence than\ncentralized ones? More specifically, we relax the required conditions for\nlinear convergence of both algorithms. For EXTRA, we show that the stepsize is\ncomparable to that of centralized algorithms. For NIDS, the upper bound of the\nstepsize is shown to be exactly the same as the centralized ones. In addition,\nwe relax the requirement for the objective functions and the mixing matrices.\nWe provide the linear convergence results for both algorithms under the weakest\nconditions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 19:11:29 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 09:51:52 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Yao", ""], ["Yan", "Ming", ""]]}, {"id": "1906.07312", "submitter": "Pankaj Saha", "authors": "Pankaj Saha, Madhusudhan Govindaraju, Suresh Marru, Marlon Pierce", "title": "MultiCloud Resource Management using Apache Mesos for Planned\n  Integration with Apache Airavata", "comments": "Gateways 2016", "journal-ref": null, "doi": "10.6084/m9.figshare.4491629.v2", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss initial results and our planned approach for incorporating Apache\nMesos based resource management that will enable design and development of\nscheduling strategies for Apache Airavata jobs so that they can be launched on\nmultiple clouds, wherein several VMs do not have Public IP addresses. We\npresent initial work and next steps on the design of a meta-scheduler using\nApache Mesos. Apache Mesos presents a unified view of resources available\nacross several clouds and clusters. Our meta-scheduler can potentially examine\nand identify the cases where multiple small jobs have been submitted by the\nsame scientists and then redirect job from the same community account or user\nto different clusters. Our approach uses a NAT firewall to make nodes/VMs,\nwithout a Public IP, visible to Mesos for the unified view.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:06:46 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 22:23:12 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Saha", "Pankaj", ""], ["Govindaraju", "Madhusudhan", ""], ["Marru", "Suresh", ""], ["Pierce", "Marlon", ""]]}, {"id": "1906.07471", "submitter": "Laurens Versluis", "authors": "Laurens Versluis, Roland Math\\'a, Sacheendra Talluri, Tim Hegeman,\n  Radu Prodan, Ewa Deelman, Alexandru Iosup", "title": "The Workflow Trace Archive: Open-Access Data from Public and Private\n  Computing Infrastructures -- Technical Report", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic, relevant, and reproducible experiments often need input traces\ncollected from real-world environments. We focus in this work on traces of\nworkflows---common in datacenters, clouds, and HPC infrastructures. We show\nthat the state-of-the-art in using workflow-traces raises important issues: (1)\nthe use of realistic traces is infrequent, and (2) the use of realistic, {\\it\nopen-access} traces even more so. Alleviating these issues, we introduce the\nWorkflow Trace Archive (WTA), an open-access archive of workflow traces from\ndiverse computing infrastructures and tooling to parse, validate, and analyze\ntraces. The WTA includes ${>}48$ million workflows captured from ${>}10$\ncomputing infrastructures, representing a broad diversity of trace domains and\ncharacteristics. To emphasize the importance of trace diversity, we\ncharacterize the WTA contents and analyze in simulation the impact of trace\ndiversity on experiment results. Our results indicate significant differences\nin characteristics, properties, and workflow structures between workload\nsources, domains, and fields.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 09:54:56 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 14:34:31 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Versluis", "Laurens", ""], ["Math\u00e1", "Roland", ""], ["Talluri", "Sacheendra", ""], ["Hegeman", "Tim", ""], ["Prodan", "Radu", ""], ["Deelman", "Ewa", ""], ["Iosup", "Alexandru", ""]]}, {"id": "1906.07509", "submitter": "Alessio Netti", "authors": "Alessio Netti, Micha Mueller, Axel Auweter, Carla Guillen, Michael\n  Ott, Daniele Tafani and Martin Schulz", "title": "From Facility to Application Sensor Data: Modular, Continuous and\n  Holistic Monitoring with DCDB", "comments": "Accepted at the The International Conference for High Performance\n  Computing, Networking, Storage, and Analysis (SC) 2019", "journal-ref": null, "doi": "10.1145/3295500.3356191", "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's HPC installations are highly-complex systems, and their complexity\nwill only increase as we move to exascale and beyond. At each layer, from\nfacilities to systems, from runtimes to applications, a wide range of tuning\ndecisions must be made in order to achieve efficient operation. This, however,\nrequires systematic and continuous monitoring of system and user data. While\nmany insular solutions exist, a system for holistic and facility-wide\nmonitoring is still lacking in the current HPC ecosystem. In this paper we\nintroduce DCDB, a comprehensive monitoring system capable of integrating data\nfrom all system levels. It is designed as a modular and highly-scalable\nframework based on a plugin infrastructure. All monitored data is aggregated at\na distributed noSQL data store for analysis and cross-system correlation. We\ndemonstrate the performance and scalability of DCDB, and describe two use cases\nin the area of energy management and characterization.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:55:16 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 06:49:39 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 07:11:42 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Netti", "Alessio", ""], ["Mueller", "Micha", ""], ["Auweter", "Axel", ""], ["Guillen", "Carla", ""], ["Ott", "Michael", ""], ["Tafani", "Daniele", ""], ["Schulz", "Martin", ""]]}, {"id": "1906.07629", "submitter": "Fabrizio Romano Genovese PhD", "authors": "Statebox Team: Fabrizio Genovese, Jelle Herold", "title": "The Mathematical Specification of the Statebox Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document defines the mathematical backbone of the Statebox programming\nlanguage. In the simplest way possible, Statebox can be seen as a clever way to\ntie together different theoretical structures to maximize their benefits and\nlimit their downsides. Since consistency and correctness are central requisites\nfor our language, it became clear from the beginning that such tying could not\nbe achieved by just hacking together different pieces of code representing\nimplementations of the structures we wanted to leverage: Rigorous mathematics\nis employed to ensure both conceptual consistency of the language and\nreliability of the code itself. The mathematics presented here is what guided\nthe implementation process, and we deemed very useful to release it to the\npublic to help people wanting to audit our work to better understand the code\nitself.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:04:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 14:26:53 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Statebox Team", "", ""], ["Genovese", "Fabrizio", ""], ["Herold", "Jelle", ""]]}, {"id": "1906.07743", "submitter": "Fande Kong", "authors": "Fande Kong, Yaqi Wang, Derek R. Gaston, Alexander D. Lindsay, Cody J.\n  Permann, Richard C. Martineau", "title": "A scalable multilevel domain decomposition preconditioner with a\n  subspace-based coarsening algorithm for the neutron transport calculations", "comments": "Submitted to Numerical Linear Algebra with Applications (Copper\n  Mountain Conference special issue). 23 pages and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multigroup neutron transport equations has been widely used to study the\ninteractions of neutrons with their background materials in nuclear reactors.\nHigh-resolution simulations of the multigroup neutron transport equations using\nmodern supercomputers require the development of scalable parallel solving\ntechniques. In this paper, we study a scalable transport method for solving the\nalgebraic system arising from the discretization of the multigroup neutron\ntransport equations. The proposed transport method consists of a fully coupled\nNewton solver for the generalized eigenvalue problems and GMRES together with a\nnovel multilevel domain decomposition preconditioner for the Jacobian system.\nThe multilevel preconditioner has been successfully used for many problems, but\nthe construction of coarse spaces for certain problems, especially for\nunstructured mesh problems, is expensive and often unscalable. We introduce a\nnew subspace-based coarsening algorithm to address this issue by exploring the\nstructure of the matrix in the discretized version of the neutron transport\nproblems. We numerically demonstrate that the proposed transport method is\nhighly scalable with more than 10,000 processor cores for the 3D C5G7 benchmark\nproblem on unstructured meshes with billions of unknowns. Compared with the\ntraditional multilevel domain decomposition method, the new approach equipped\nwith the subspace-based coarsening algorithm is much faster on the construction\nof coarse spaces.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:04:34 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Kong", "Fande", ""], ["Wang", "Yaqi", ""], ["Gaston", "Derek R.", ""], ["Lindsay", "Alexander D.", ""], ["Permann", "Cody J.", ""], ["Martineau", "Richard C.", ""]]}, {"id": "1906.07749", "submitter": "Volker Haarslev", "authors": "Zixi Quan and Volker Haarslev", "title": "A Framework for Parallelizing OWL Classification in Description Logic\n  Reasoners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report on a black-box approach to parallelize existing\ndescription logic (DL) reasoners for the Web Ontology Language (OWL). We focus\non OWL ontology classification, which is an important inference service and\nsupported by every major OWL/DL reasoner. We propose a flexible parallel\nframework which can be applied to existing OWL reasoners in order to speed up\ntheir classification process. In order to test its performance, we evaluated\nour framework by parallelizing major OWL reasoners for concept classification.\nIn comparison to the selected black-box reasoner our results demonstrate that\nthe wall clock time of ontology classification can be improved by one order of\nmagnitude for most real-world ontologies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 18:16:17 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Quan", "Zixi", ""], ["Haarslev", "Volker", ""]]}, {"id": "1906.07840", "submitter": "Urmish Thakker", "authors": "Newsha Ardalani, Urmish Thakker, Aws Albarghouthi, Karu Sankaralingam", "title": "A Static Analysis-based Cross-Architecture Performance Prediction Using\n  Machine Learning", "comments": "Published at 2nd International Workshop on AI-assisted Design for\n  Architecture Phoenix, AZ, June 22, 2019, colocated with ISCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Porting code from CPU to GPU is costly and time-consuming; Unless much time\nis invested in development and optimization, it is not obvious, a priori, how\nmuch speed-up is achievable or how much room is left for improvement. Knowing\nthe potential speed-up a priori can be very useful: It can save hundreds of\nengineering hours, help programmers with prioritization and algorithm\nselection. We aim to address this problem using machine learning in a\nsupervised setting, using solely the single-threaded source code of the\nprogram, without having to run or profile the code. We propose a static\nanalysis-based cross-architecture performance prediction framework (Static\nXAPP) which relies solely on program properties collected using static analysis\nof the CPU source code and predicts whether the potential speed-up is above or\nbelow a given threshold. We offer preliminary results that show we can achieve\n94% accuracy in binary classification, in average, across different thresholds\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 23:06:32 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Ardalani", "Newsha", ""], ["Thakker", "Urmish", ""], ["Albarghouthi", "Aws", ""], ["Sankaralingam", "Karu", ""]]}, {"id": "1906.07850", "submitter": "Mohammad Javad Amiri", "authors": "Mohammad Javad Amiri, Sujaya Maiyya, Divyakant Agrawal, Amr El Abbadi", "title": "SeeMoRe: A Fault-Tolerant Protocol for Hybrid Cloud Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale data management systems utilize State Machine Replication to\nprovide fault tolerance and to enhance performance. Fault-tolerant protocols\nare extensively used in the distributed database infrastructure of large\nenterprises such as Google, Amazon, and Facebook, as well as permissioned\nblockchain systems like IBM's Hyperledger Fabric. However, and in spite of\nyears of intensive research, existing fault-tolerant protocols do not\nadequately address all the characteristics of distributed system applications.\nIn particular, hybrid cloud environments consisting of private and public\nclouds are widely used by enterprises. However, fault-tolerant protocols have\nnot been adapted for such environments. In this paper, we introduce SeeMoRe, a\nhybrid State Machine Replication protocol to handle both crash and malicious\nfailures in a public/private cloud environment. SeeMoRe considers a private\ncloud consisting of nonmalicious nodes (either correct or crash) and a public\ncloud with both Byzantine faulty and correct nodes. SeeMoRe has three different\nmodes which can be used depending on the private cloud load and the\ncommunication latency between the public and the private cloud. We also\nintroduce a dynamic mode switching technique to transition from one mode to\nanother. Furthermore, we evaluate SeeMoRe using a series of benchmarks. The\nexperiments reveal that SeeMoRe's performance is close to the state of the art\ncrash fault-tolerant protocols while tolerating malicious failures.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 23:45:35 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Amiri", "Mohammad Javad", ""], ["Maiyya", "Sujaya", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "1906.07852", "submitter": "Cuneyt Gurcan Akcora", "authors": "Cuneyt Gurcan Akcora and Yitao Li and Yulia R. Gel and Murat\n  Kantarcioglu", "title": "BitcoinHeist: Topological Data Analysis for Ransomware Detection on the\n  Bitcoin Blockchain", "comments": "15 pages, 11 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proliferation of cryptocurrencies (e.g., Bitcoin) that allow pseudo-anonymous\ntransactions, has made it easier for ransomware developers to demand ransom by\nencrypting sensitive user data. The recently revealed strikes of ransomware\nattacks have already resulted in significant economic losses and societal harm\nacross different sectors, ranging from local governments to health care.\n  Most modern ransomware use Bitcoin for payments. However, although Bitcoin\ntransactions are permanently recorded and publicly available, current\napproaches for detecting ransomware depend only on a couple of heuristics\nand/or tedious information gathering steps (e.g., running ransomware to collect\nransomware related Bitcoin addresses). To our knowledge, none of the previous\napproaches have employed advanced data analytics techniques to automatically\ndetect ransomware related transactions and malicious Bitcoin addresses.\n  By capitalizing on the recent advances in topological data analysis, we\npropose an efficient and tractable data analytics framework to automatically\ndetect new malicious addresses in a ransomware family, given only a limited\nrecords of previous transactions. Furthermore, our proposed techniques exhibit\nhigh utility to detect the emergence of new ransomware families, that is,\nransomware with no previous records of transactions. Using the existing known\nransomware data sets, we show that our proposed methodology provides\nsignificant improvements in precision and recall for ransomware transaction\ndetection, compared to existing heuristic based approaches, and can be utilized\nto automate ransomware detection.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 00:01:40 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Akcora", "Cuneyt Gurcan", ""], ["Li", "Yitao", ""], ["Gel", "Yulia R.", ""], ["Kantarcioglu", "Murat", ""]]}, {"id": "1906.08148", "submitter": "Anton G. Artemov", "authors": "Anton G. Artemov", "title": "Approximate multiplication of nearly sparse matrices with decay in a\n  fully recursive distributed task-based parallel framework", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider parallel implementations of approximate\nmultiplication of large matrices with exponential decay of elements. Such\nmatrices arise in computations related to electronic structure calculations and\nsome other fields of computational science. Commonly, sparsity is introduced by\ndropping out small entries (truncation) of input matrices. Another approach,\nthe sparse approximate multiplication algorithm [M. Challacombe and N. Bock,\narXiv preprint 1011.3534, 2010] performs truncation of sub-matrix products. We\nconsider these two methods and their combination, i.e. truncation of both input\nmatrices and sub-matrix products. Implementations done using the Chunks and\nTasks programming model and library [E. H. Rubensson and E. Rudberg, Parallel\nComput., 40:328-343, 2014] are presented and discussed. We show that the\nabsolute error in the Frobenius norm behaves as $O\\left(n^{1/2} \\right), n\n\\longrightarrow \\infty $ and $O\\left(\\tau^{p/2} \\right), \\tau \\longrightarrow\n0,\\,\\, \\forall p < 2$ for all three methods, where $n$ is the matrix size and\n$\\tau$ is the truncation threshold. We compare the methods on a model problem\nand show that the combined method outperforms the original two. The methods are\nalso applied to matrices coming from large chemical systems with $\\sim 10^6$\natoms. We show that the combination of the two methods achieves better weak\nscaling by reducing the amount of communication by a factor of $\\approx 2$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:21:44 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 15:33:14 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 09:38:51 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2019 07:30:43 GMT"}, {"version": "v5", "created": "Wed, 2 Sep 2020 11:34:07 GMT"}, {"version": "v6", "created": "Fri, 4 Sep 2020 12:34:51 GMT"}, {"version": "v7", "created": "Sat, 20 Feb 2021 11:26:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Artemov", "Anton G.", ""]]}, {"id": "1906.08168", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Srinivas Sridharan, Bharat Kaul", "title": "Automatic Model Parallelism for Deep Neural Networks with Compiler and\n  Hardware Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep neural networks (DNNs) have been enormously successful in tasks that\nwere hitherto in the human-only realm such as image recognition, and language\ntranslation. Owing to their success the DNNs are being explored for use in ever\nmore sophisticated tasks. One of the ways that the DNNs are made to scale for\nthe complex undertakings is by increasing their size -- deeper and wider\nnetworks can model well the additional complexity. Such large models are\ntrained using model parallelism on multiple compute devices such as multi-GPUs\nand multi-node systems.\n  In this paper, we develop a compiler-driven approach to achieve model\nparallelism. We model the computation and communication costs of a dataflow\ngraph that embodies the neural network training process and then, partition the\ngraph using heuristics in such a manner that the communication between compute\ndevices is minimal and we have a good load balance. The hardware scheduling\nassistants are proposed to assist the compiler in fine tuning the distribution\nof work at runtime.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 10:18:15 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Sridharan", "Srinivas", ""], ["Kaul", "Bharat", ""]]}, {"id": "1906.08169", "submitter": "Oksana Shadura", "authors": "Brian Bockelman (1), Zhe Zhang (2), Oksana Shadura (2) ((1) Morgridge\n  Institute for Research, (2) University Nebraska-Lincoln)", "title": "Speeding HEP Analysis with ROOT Bulk I/O", "comments": "Submitted to proceedings of ACAT 2019", "journal-ref": null, "doi": "10.1088/1742-6596/1525/1/012048", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distinct HEP workflows have distinct I/O needs; while ROOT I/O excels at\nserializing complex C++ objects common to reconstruction, analysis workflows\ntypically have simpler objects and can sustain higher event rates. To meet\nthese workflows, we have developed a \"bulk I/O\" interface, allowing multiple\nevents data to be returned per library call. This reduces ROOT-related\noverheads and increases event rates - orders-of-magnitude improvements are\nshown in microbenchmarks. Unfortunately, this bulk interface is difficult to\nuse as it requires users to identify when it is applicable and they still\n\"think\" in terms of events, not arrays of data. We have integrated the bulk I/O\ninterface into the new RDataFrame analysis framework inside ROOT. As\nRDataFrame's interface can provide improved type information, the framework\nitself can determine what data is readable via the bulk IO and automatically\nswitch between interfaces. We demonstrate how this can improve event rates when\nreading analysis data formats, such as CMS's NanoAOD.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:03:30 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bockelman", "Brian", ""], ["Zhang", "Zhe", ""], ["Shadura", "Oksana", ""]]}, {"id": "1906.08170", "submitter": "Chit-Kwan Lin", "authors": "Chit-Kwan Lin and Stephen J. Tarsa", "title": "Branch Prediction Is Not a Solved Problem: Measurements, Opportunities,\n  and Future Directions", "comments": "11 pages, 10 figures. Submitted to IISWC 2019", "journal-ref": null, "doi": "10.1109/IISWC47752.2019.9042108", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern branch predictors predict the vast majority of conditional branch\ninstructions with near-perfect accuracy, allowing superscalar, out-of-order\nprocessors to maximize speculative efficiency and thus performance. However,\nthis impressive overall effectiveness belies a substantial missed opportunity\nin single-threaded instructions per cycle (IPC). For example, we show that\ncorrecting the mispredictions made by the state-of-the-art TAGE-SC-L branch\npredictor on SPECint 2017 would improve IPC by margins similar to an advance in\nprocess technology node.\n  In this work, we measure and characterize these mispredictions. We find that\nthey categorically arise from either (1) a small number of systematically\nhard-to-predict (H2P) branches; or (2) rare branches with low dynamic execution\ncounts. Using data from SPECint 2017 and additional large code footprint\napplications, we quantify the occurrence and IPC impact of these two\ncategories. We then demonstrate that increasing the resources afforded to\nexisting branch predictors does not alone address the root causes of most\nmispredictions. This leads us to reexamine basic assumptions in branch\nprediction and to propose new research directions that, for example, deploy\nmachine learning to improve pattern matching for H2Ps, and use on-chip phase\nlearning to track long-term statistics for rare branches.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:50:31 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lin", "Chit-Kwan", ""], ["Tarsa", "Stephen J.", ""]]}, {"id": "1906.08172", "submitter": "Chuo-Ling Chang", "authors": "Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha\n  Uboweja, Michael Hays, Fan Zhang, Chuo-Ling Chang, Ming Guang Yong, Juhyun\n  Lee, Wan-Teh Chang, Wei Hua, Manfred Georg and Matthias Grundmann", "title": "MediaPipe: A Framework for Building Perception Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building applications that perceive the world around them is challenging. A\ndeveloper needs to (a) select and develop corresponding machine learning\nalgorithms and models, (b) build a series of prototypes and demos, (c) balance\nresource consumption against the quality of the solutions, and finally (d)\nidentify and mitigate problematic cases. The MediaPipe framework addresses all\nof these challenges. A developer can use MediaPipe to build prototypes by\ncombining existing perception components, to advance them to polished\ncross-platform applications and measure system performance and resource\nconsumption on target platforms. We show that these features enable a developer\nto focus on the algorithm or model development and use MediaPipe as an\nenvironment for iteratively improving their application with results\nreproducible across different devices and platforms. MediaPipe will be\nopen-sourced at https://github.com/google/mediapipe.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 05:49:22 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Lugaresi", "Camillo", ""], ["Tang", "Jiuqiang", ""], ["Nash", "Hadon", ""], ["McClanahan", "Chris", ""], ["Uboweja", "Esha", ""], ["Hays", "Michael", ""], ["Zhang", "Fan", ""], ["Chang", "Chuo-Ling", ""], ["Yong", "Ming Guang", ""], ["Lee", "Juhyun", ""], ["Chang", "Wan-Teh", ""], ["Hua", "Wei", ""], ["Georg", "Manfred", ""], ["Grundmann", "Matthias", ""]]}, {"id": "1906.08173", "submitter": "Xinxin Liu", "authors": "Xinxin Liu, Yu Hua, Xuan Li, Qifan Liu", "title": "Write-Optimized and Consistent RDMA-based NVM Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to deliver high performance in cloud computing, we generally exploit\nand leverage RDMA (Remote Direct Memory Access) in networking and NVM\n(Non-Volatile Memory) in end systems. Due to no involvement of CPU, one-sided\nRDMA becomes efficient to access the remote memory, and NVM technologies have\nthe strengths of non-volatility, byte-addressability and DRAM-like latency. In\norder to achieve end-to-end high performance, many efforts aim to synergize\none-sided RDMA and NVM. Due to the need to guarantee Remote Data Atomicity\n(RDA), we have to consume extra network round-trips, remote CPU participation\nand double NVM writes. In order to address these problems, we propose a\nzero-copy log-structured memory design for Efficient Remote Data Atomicity,\ncalled Erda. In Erda, clients directly transfer data to the destination address\nat servers via one-sided RDMA writes without redundant copy and remote CPU\nconsumption. To detect the incompleteness of fetched data, we verify a checksum\nwithout client-server coordination. We further ensure metadata consistency by\nleveraging an 8-byte atomic update in the hash table, which also contains the\naddress information for the stale data. When a failure occurs, the server\nproperly restores to a consistent version. Experimental results show that\ncompared with Redo Logging (a CPU involvement scheme) and Read After Write (a\nnetwork dominant scheme), Erda reduces NVM writes approximately by 50%, as well\nas significantly improves throughput and decreases latency.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 09:39:55 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Liu", "Xinxin", ""], ["Hua", "Yu", ""], ["Li", "Xuan", ""], ["Liu", "Qifan", ""]]}, {"id": "1906.08176", "submitter": "Tommy Mckinnon", "authors": "Tommy Mckinnon", "title": "MaGPoS -- A novel decentralized consensus mechanism combining magnetism\n  and proof of stake", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe MaGPoS, a novel consensus mechanism which is well suited to\ndecentralized blockchain based protocols. MaGPoS is based on a combination of\nthe well known physics of nano-scale magnetism, and previous implementations of\nproof of stake. This system has been studied by hundreds of thousands of\nscientists worldwide for over a hundred years, giving it an extreme level of\nreliability that is needed for a consensus mechanism. We start by explaining\nthe physics, and study the properties that make it particularly beneficial for\nuse in a consensus mechanism. We then show how to apply the physical model to a\ndecentralized network of nodes, each with their own copy of a blockchain. After\nthis, we describe some example calculations that a node in the decentralized\nnetwork would make, and provide pseudo code for implementation. Finally, we\ndiscuss the how the model achieves all of the important properties that one\nexpects of a consensus mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 05:55:18 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Mckinnon", "Tommy", ""]]}, {"id": "1906.08177", "submitter": "Mehrdad Salimitari", "authors": "Mehrdad Salimitari, Mohsen Joneidi, and Mainak Chatterjee", "title": "AI-enabled Blockchain: An Outlier-aware Consensus Protocol for\n  Blockchain-based IoT Networks", "comments": "This paper is accepted in IEEE GLOBECOM 2019 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new framework for a secure and robust consensus in blockchain-based IoT\nnetworks is proposed using machine learning. Hyperledger fabric, which is a\nblockchain platform developed as part of the Hyperledger project, though looks\nvery apt for IoT applications, has comparatively low tolerance for malicious\nactivities in an untrustworthy environment. To that end, we propose AI-enabled\nblockchain (AIBC) with a 2-step consensus protocol that uses an outlier\ndetection algorithm for consensus in an IoT network implemented on hyperledger\nfabric platform. The outlier-aware consensus protocol exploits a supervised\nmachine learning algorithm which detects anomaly activities via a learned\ndetector in the first step. Then, the data goes through the inherent Practical\nByzantine Fault Tolerance (PBFT) consensus protocol in the hyperledger fabric\nfor ledger update. We measure and report the performance of our framework with\nrespect to the various delay components. Results reveal that our implemented\nAIBC network (2-step consensus protocol) improves hyperledger fabric\nperformance in terms of fault tolerance by marginally compromising the delay\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 21:29:38 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 19:36:51 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Salimitari", "Mehrdad", ""], ["Joneidi", "Mohsen", ""], ["Chatterjee", "Mainak", ""]]}, {"id": "1906.08239", "submitter": "Kyle Singer", "authors": "Kyle Singer, Kunal Agrawal, I-Ting Angelina Lee", "title": "Reduced I/O Latency with Futures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task parallelism research has traditionally focused on optimizing\ncomputation-intensive applications. Due to the proliferation of commodity\nparallel processors, there has been recent interest in supporting interactive\napplications. Such interactive applications frequently rely on I/O operations\nthat may incur significant latency. In order to increase performance, when a\nparticular thread of control is blocked on an I/O operation, ideally we would\nlike to hide this latency by using the processing resources to do other ready\nwork instead of blocking or spin waiting on this I/O. There has been limited\nprior work on hiding this latency and only one result that provides a\ntheoretical bound for interactive applications that use I/Os. In this work, we\npropose a method for hiding the latency of I/O operations by using the futures\nabstraction. We provide a theoretical analysis of our algorithm that shows our\nalgorithm provides better execution time guarantees than prior work. We also\nimplemented the algorithm in a practically efficient prototype library that\nruns on top of the Cilk-F runtime, a runtime system that supports futures\nwithin the context of the Cilk Plus language, and performed experiments that\ndemonstrate the efficiency of our implementation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 17:37:23 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Singer", "Kyle", ""], ["Agrawal", "Kunal", ""], ["Lee", "I-Ting Angelina", ""]]}, {"id": "1906.08320", "submitter": "Badih Ghazi", "authors": "Badih Ghazi, Rasmus Pagh, Ameya Velingker", "title": "Scalable and Differentially Private Distributed Aggregation in the\n  Shuffled Model", "comments": "17 pages, 1 figure, 1 table, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning promises to make machine learning feasible on distributed,\nprivate datasets by implementing gradient descent using secure aggregation\nmethods. The idea is to compute a global weight update without revealing the\ncontributions of individual users. Current practical protocols for secure\naggregation work in an \"honest but curious\" setting where a curious adversary\nobserving all communication to and from the server cannot learn any private\ninformation assuming the server is honest and follows the protocol. A more\nscalable and robust primitive for privacy-preserving protocols is shuffling of\nuser data, so as to hide the origin of each data item. Highly scalable and\nsecure protocols for shuffling, so-called mixnets, have been proposed as a\nprimitive for privacy-preserving analytics in the Encode-Shuffle-Analyze\nframework by Bittau et al., which was later analytically studied by Erlingsson\net al. and Cheu et al.. The recent papers by Cheu et al., and Balle et al. have\ngiven protocols for secure aggregation that achieve differential privacy\nguarantees in this \"shuffled model\". Their protocols come at a cost, though:\nEither the expected aggregation error or the amount of communication per user\nscales as a polynomial $n^{\\Omega(1)}$ in the number of users $n$. In this\npaper we propose simple and more efficient protocol for aggregation in the\nshuffled model, where communication as well as error increases only\npolylogarithmically in $n$. Our new technique is a conceptual \"invisibility\ncloak\" that makes users' data almost indistinguishable from random noise while\nintroducing zero distortion on the sum.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 19:30:05 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 17:21:17 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 17:53:21 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ghazi", "Badih", ""], ["Pagh", "Rasmus", ""], ["Velingker", "Ameya", ""]]}, {"id": "1906.08452", "submitter": "Quoc-Viet Pham", "authors": "Quoc-Viet Pham, Fang Fang, Vu Nguyen Ha, Md. Jalil Piran, Mai Le, Long\n  Bao Le, Won-Joo Hwang, Zhiguo Ding", "title": "A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals,\n  Technology Integration, and State-of-the-Art", "comments": "43 pages, revised manuscript submitted to IEEE Communications Surveys\n  & Tutorials for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the emergence of new compute-intensive applications and the vision\nof the Internet of Things (IoT), it is foreseen that the emerging 5G network\nwill face an unprecedented increase in traffic volume and computation demands.\nHowever, end users mostly have limited storage capacities and finite processing\ncapabilities, thus how to run compute-intensive applications on\nresource-constrained users has recently become a natural concern. Mobile edge\ncomputing (MEC), a key technology in the emerging fifth generation (5G)\nnetwork, can optimize mobile resources by hosting compute-intensive\napplications, process large data before sending to the cloud, provide the cloud\ncomputing capabilities within the radio access network (RAN) in close proximity\nto mobile users, and offer context-aware services with the help of RAN\ninformation. Therefore, MEC enables a wide variety of applications, where the\nreal-time response is strictly required, e.g., driverless vehicles, augmented\nreality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G\ncould become a reality with the advent of new technological concepts. The\nsuccessful realization of MEC in the 5G network is still in its infancy and\ndemands for constant efforts from both academic and industry communities. In\nthis survey, we first provide a holistic overview of MEC technology and its\npotential use cases and applications. Then, we outline up-to-date researches on\nthe integration of MEC with the new technologies that will be deployed in 5G\nand beyond. We also summarize testbeds and experimental evaluations, and open\nsource activities, for edge computing. We further summarize lessons learned\nfrom state-of-the-art research works as well as discuss challenges and\npotential future directions for MEC research.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 05:44:40 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 03:39:56 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Pham", "Quoc-Viet", ""], ["Fang", "Fang", ""], ["Ha", "Vu Nguyen", ""], ["Piran", "Md. Jalil", ""], ["Le", "Mai", ""], ["Le", "Long Bao", ""], ["Hwang", "Won-Joo", ""], ["Ding", "Zhiguo", ""]]}, {"id": "1906.08602", "submitter": "Myoungsoo Jung", "authors": "Sungjoon Koh, Jie Zhang, Miryeong Kwon, Jungyeon Yoon, David Donofrio,\n  Nam Sung Kim, Myoungsoo Jung", "title": "Exploring Fault-Tolerant Erasure Codes for Scalable All-Flash Array\n  Clusters", "comments": "19 pages, 46 figures. arXiv admin note: substantial text overlap with\n  arXiv:1709.05365", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems ( Volume: 30\n  , Issue: 6 , June 1 2019 )", "doi": "10.1109/TPDS.2018.2884722", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale systems with all-flash arrays have become increasingly common in\nmany computing segments. To make such systems resilient, we can adopt erasure\ncoding such as Reed-Solomon (RS) code as an alternative to replication because\nerasure coding incurs a significantly lower storage overhead than replication.\nTo understand the impact of using erasure coding on the system performance and\nother system aspects such as CPU utilization and network traffic, we build a\nstorage cluster that consists of approximately 100 processor cores with more\nthan 50 high-performance solid-state drives (SSDs), and evaluate the cluster\nwith a popular open-source distributed parallel file system, called Ceph.\nSpecifically, we analyze the behaviors of a system adopting erasure coding from\nthe following five viewpoints, and compare with those of another system using\nreplication: (1) storage system I/O performance; (2) computing and software\noverheads; (3) I/O amplification; (4) network traffic among storage nodes, and\n(5) impact of physical data layout on performance of RS-coded SSD arrays. For\nall these analyses, we examine two representative RS configurations, used by\nGoogle file systems, and compare them with triple replication employed by a\ntypical parallel file system as a default fault tolerance mechanism. Lastly, we\ncollect 96 block-level traces from the cluster and release them to the public\ndomain for the use of other researchers.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 05:30:19 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Koh", "Sungjoon", ""], ["Zhang", "Jie", ""], ["Kwon", "Miryeong", ""], ["Yoon", "Jungyeon", ""], ["Donofrio", "David", ""], ["Kim", "Nam Sung", ""], ["Jung", "Myoungsoo", ""]]}, {"id": "1906.08689", "submitter": "Zheng Wang", "authors": "Lu Yuan, Jie Ren, Ling Gao, Zhanyong Tang, Zheng Wang", "title": "Using Machine Learning to Optimize Web Interactions on Heterogeneous\n  Mobile Multi-cores", "comments": "Accepted to be published in IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The web has become a ubiquitous application development platform for mobile\nsystems. Yet, web access on mobile devices remains an energy-hungry activity.\nPrior work in the field mainly focuses on the initial page loading stage, but\nfails to exploit the opportunities for energy-efficiency optimization while the\nuser is interacting with a loaded page. This paper presents a novel approach\nfor performing energy optimization for interactive mobile web browsing. At the\nheart of our approach is a set of machine learning models, which estimate\n\\emph{at runtime} the frames per second for a given user interaction input by\nrunning the computation-intensive web render engine on a specific processor\ncore under a given clock speed. We use the learned predictive models as a\nutility function to quickly search for the optimal processor setting to\ncarefully trade responsive time for reduced energy consumption. We integrate\nour techniques to the open-source Chromium browser and apply it to two\nrepresentative mobile user events: scrolling and pinching (i.e., zoom in and\nout). We evaluate the developed system on the landing pages of the top-100\nhottest websites and two big.LITTLE heterogeneous mobile platforms. Our\nextensive experiments show that the proposed approach reduces the system-wide\nenergy consumption by over 36\\% on average and up to 70\\%. This translates to\nan over 17\\% improvement on energy-efficiency over a state-of-the-art\nevent-based web browser scheduler, but with significantly fewer violations on\nthe quality of service.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:21:09 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 16:07:13 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 13:11:49 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yuan", "Lu", ""], ["Ren", "Jie", ""], ["Gao", "Ling", ""], ["Tang", "Zhanyong", ""], ["Wang", "Zheng", ""]]}, {"id": "1906.08819", "submitter": "Batuhan Hangun", "authors": "Batuhan Hang\\\"un and \\\"Onder Eyecio\\u{g}lu", "title": "Performance Comparison Between OpenCV Built in CPU and GPU Functions on\n  Image Processing Operations", "comments": "Image Processing, CUDA, Parallel Processing, OpenCV, GPU", "journal-ref": "International Journal of Engineering Science and Application 1, 2\n  (2017), 34-41", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image Processing is a specialized area of Digital Signal Processing which\ncontains various mathematical and algebraic operations such as matrix\ninversion, transpose of matrix, derivative, convolution, Fourier Transform etc.\nOperations like those require higher computational capabilities than daily\nusage purposes of computers. At that point, with increased image sizes and more\ncomplex operations, CPUs may be unsatisfactory since they use Serial Processing\nby default. GPUs are the solution that come up with greater speed compared to\nCPUs because of their Parallel Processing/Computation nature. A parallel\ncomputing platform and programming model named CUDA was created by NVIDIA and\nimplemented by the graphics processing units (GPUs) which were produced by\nthem. In this paper, computing performance of some commonly used Image\nProcessing operations will be compared on OpenCV's built in CPU and GPU\nfunctions that use CUDA.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 19:38:13 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Hang\u00fcn", "Batuhan", ""], ["Eyecio\u011flu", "\u00d6nder", ""]]}, {"id": "1906.08879", "submitter": "Ravichandra Addanki", "authors": "Ravichandra Addanki, Shaileshh Bojja Venkatakrishnan, Shreyan Gupta,\n  Hongzi Mao and Mohammad Alizadeh", "title": "Placeto: Learning Generalizable Device Placement Algorithms for\n  Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Placeto, a reinforcement learning (RL) approach to efficiently\nfind device placements for distributed neural network training. Unlike prior\napproaches that only find a device placement for a specific computation graph,\nPlaceto can learn generalizable device placement policies that can be applied\nto any graph. We propose two key ideas in our approach: (1) we represent the\npolicy as performing iterative placement improvements, rather than outputting a\nplacement in one shot; (2) we use graph embeddings to capture relevant\ninformation about the structure of the computation graph, without relying on\nnode labels for indexing. These ideas allow Placeto to train efficiently and\ngeneralize to unseen graphs. Our experiments show that Placeto requires up to\n6.1x fewer training steps to find placements that are on par with or better\nthan the best placements found by prior approaches. Moreover, Placeto is able\nto learn a generalizable placement policy for any given family of graphs, which\ncan then be used without any retraining to predict optimized placements for\nunseen graphs from the same family. This eliminates the large overhead incurred\nby prior RL approaches whose lack of generalizability necessitates re-training\nfrom scratch every time a new graph is to be placed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:08:51 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Addanki", "Ravichandra", ""], ["Venkatakrishnan", "Shaileshh Bojja", ""], ["Gupta", "Shreyan", ""], ["Mao", "Hongzi", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "1906.08911", "submitter": "Vivek Kale PhD", "authors": "Vivek Kale, Christian Iwainsky, Michael Klemm, Jonas H. Muller\n  Korndorfer and Florina M. Ciorba", "title": "Toward a Standard Interface for User-Defined Scheduling in OpenMP", "comments": "16 pages with references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CL cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel loops are an important part of OpenMP programs. Efficient scheduling\nof parallel loops can improve performance of the programs. The current OpenMP\nspecification only offers three options for loop scheduling, which are\ninsufficient in certain instances. Given the large number of other possible\nscheduling strategies, it is infeasible to standardize each one. A more viable\napproach is to extend the OpenMP standard to allow for users to define loop\nscheduling strategies. The approach will enable standard-compliant\napplication-specific scheduling. This work analyzes the principal components\nrequired by user-defined scheduling and proposes two competing interfaces as\ncandidates for the OpenMP standard. We conceptually compare the two proposed\ninterfaces with respect to the three host languages of OpenMP, i.e., C, C++,\nand Fortran. These interfaces serve the OpenMP community as a basis for\ndiscussion and prototype implementation for user-defined scheduling.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:47:32 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 20:29:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Kale", "Vivek", ""], ["Iwainsky", "Christian", ""], ["Klemm", "Michael", ""], ["Korndorfer", "Jonas H. Muller", ""], ["Ciorba", "Florina M.", ""]]}, {"id": "1906.08926", "submitter": "Taehyong Ri", "authors": "U Yongnam, Ri Taehyong", "title": "Scheduling for Flexible Manufacturing System with Objective Function to\n  be Minimization of Total Processing Time and Unbalance of Machine Load", "comments": "11Pages,3Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For scheduling in flexible manufacturing system (FMS), many factors should be\nconsidered, it is difficult to solve the scheduling problem by satisfying\ndifferent criteria (production cost, utilization of system, number of movements\nof part, make-span, and tardiness in due date and so on) and constrains. The\npaper proposes mathematical model of a job shop scheduling problem (JSSP) to\nbalance the load of all machines and utilize effectively all machines in FMS.\nThis paper defines the evaluation function of the unbalance of the machine load\nand formulates the optimization problem with two objectives minimizing\nunbalance of the machine load and the total processing time, scheduling problem\nhaving been solved by integer linear programming, thus scheduling problem\nhaving been solved. The results of calculation show that the total processing\ntime on all machines is reduced and machine loading is balanced better than\nprevious works, and job shop scheduling also could be scheduled more easily in\nFMS.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 02:49:15 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Yongnam", "U", ""], ["Taehyong", "Ri", ""]]}, {"id": "1906.08936", "submitter": "Maofan Yin", "authors": "Team Rocket, Maofan Yin, Kevin Sekniqi, Robbert van Renesse, and Emin\n  G\\\"un Sirer", "title": "Scalable and Probabilistic Leaderless BFT Consensus through\n  Metastability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a family of leaderless Byzantine fault tolerance\nprotocols, built around a metastable mechanism via network subsampling. These\nprotocols provide a strong probabilistic safety guarantee in the presence of\nByzantine adversaries while their concurrent and leaderless nature enables them\nto achieve high throughput and scalability. Unlike blockchains that rely on\nproof-of-work, they are quiescent and green. Unlike traditional consensus\nprotocols where one or more nodes typically process linear bits in the number\nof total nodes per decision, no node processes more than logarithmic bits. It\ndoes not require accurate knowledge of all participants and exposes new\npossible tradeoffs and improvements in safety and liveness for building\nconsensus protocols.\n  The paper describes the Snow protocol family, analyzes its guarantees, and\ndescribes how it can be used to construct the core of an internet-scale\nelectronic payment system called Avalanche, which is evaluated in a large scale\ndeployment. Experiments demonstrate that the system can achieve high throughput\n(3400 tps), provide low confirmation latency (1.35 sec), and scale well\ncompared to existing systems that deliver similar functionality. For our\nimplementation and setup, the bottleneck of the system is in transaction\nverification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 03:55:19 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:54:44 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Rocket", "Team", ""], ["Yin", "Maofan", ""], ["Sekniqi", "Kevin", ""], ["van Renesse", "Robbert", ""], ["Sirer", "Emin G\u00fcn", ""]]}, {"id": "1906.08986", "submitter": "Wei Wang", "authors": "Wei Wang and Meihui Zhang and Gang Chen and H. V. Jagadish and Beng\n  Chin Ooi and Kian-Lee Tan", "title": "Database Meets Deep Learning: Challenges and Opportunities", "comments": "The first version of this paper has appeared in SIGMOD Record. In\n  this (third) version, we extend it to include the recent developments in this\n  field and references to recent work (especially for section 3.2 and section\n  4.2)", "journal-ref": "ACM SIGMOD Record, Volume 45 Issue 2, June 2016, Pages 17-22", "doi": "10.1145/3003665.3003669", "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently become very popular on account of its incredible\nsuccess in many complex data-driven applications, such as image classification\nand speech recognition. The database community has worked on data-driven\napplications for many years, and therefore should be playing a lead role in\nsupporting this new wave. However, databases and deep learning are different in\nterms of both techniques and applications. In this paper, we discuss research\nproblems at the intersection of the two fields. In particular, we discuss\npossible improvements for deep learning systems from a database perspective,\nand analyze database applications that may benefit from deep learning\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 07:26:31 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 03:34:25 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Jagadish", "H. V.", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""]]}, {"id": "1906.09073", "submitter": "Bernadette Charron-Bost", "authors": "Bernadette Charron-Bost and Shlomo Moran", "title": "MinMax Algorithms for Stabilizing Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stabilizing consensus problem, each agent of a networked system has an\ninput value and is repeatedly writing an output value; it is required that\neventually all the output values stabilize to the same value which, moreover,\nmust be one of the input values. We study this problem for a synchronous model\nwith identical and anonymous agents that are connected by a time-varying\ntopology. Our main result is a generic MinMax algorithm that solves the\nstabilizing consensus problem in this model when, in each sufficiently long but\nbounded period of time, there is an agent, called a root, that can send\nmessages, possibly indirectly, to all the agents. Such topologies are highly\ndynamic (in particular, roots may change arbitrarily over time) and enforce no\nstrong connectivity property (an agent may be never a root). Our distributed\nMinMax algorithms require neither central control (e.g., synchronous starts)\nnor any global information (eg.,on the size of the network), and are quite\nefficient in terms of message size and storage requirements.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 11:24:43 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Charron-Bost", "Bernadette", ""], ["Moran", "Shlomo", ""]]}, {"id": "1906.09086", "submitter": "Fatima Haouari", "authors": "Fatima Haouari, Emna Baccour, Aiman Erbad, Amr Mohamed, and Mohsen\n  Guizani", "title": "QoE-Aware Resource Allocation for Crowdsourced Live Streaming: A Machine\n  Learning Approach", "comments": "This paper was accepted in the Proceedings of ICC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.MM cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the tremendous technological advancement of personal devices and\nthe prevalence of wireless mobile network accesses, the world has witnessed an\nexplosion in crowdsourced live streaming. Ensuring a better viewers quality of\nexperience (QoE) is the key to maximize the audiences number and increase\nstreaming providers' profits. This can be achieved by advocating a\ngeo-distributed cloud infrastructure to allocate the multimedia resources as\nclose as possible to viewers, in order to minimize the access delay and video\nstalls. Moreover, allocating the exact needed resources beforehand avoids\nover-provisioning, which may lead to significant costs by the service\nproviders. In the contrary, under-provisioning might cause significant delays\nto the viewers. In this paper, we introduce a prediction driven resource\nallocation framework, to maximize the QoE of viewers and minimize the resource\nallocation cost. First, by exploiting the viewers locations available in our\nunique dataset, we implement a machine learning model to predict the viewers\nnumber near each geo-distributed cloud site. Second, based on the predicted\nresults that showed to be close to the actual values, we formulate an\noptimization problem to proactively allocate resources at the viewers\nproximity. Additionally, we will present a trade-off between the video access\ndelay and the cost of resource allocation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:57:06 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Haouari", "Fatima", ""], ["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1906.09122", "submitter": "Nishant Saurabh", "authors": "Nishant Saurabh, Julian Remmers, Dragi Kimovski, Radu Prodan, Jorge G.\n  Barbosa", "title": "Semantics-aware Virtual Machine Image Management in IaaS Clouds", "comments": null, "journal-ref": "33rd IEEE International Parallel & Distributed Processing\n  Symposium , IPDPS 2019, Rio de Janerio, Brazil (May 20-24 2019)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrastructure-as-a-service (IaaS) Clouds concurrently accommodate diverse\nsets of user requests, requiring an efficient strategy for storing and\nretrieving virtual machine images (VMIs) at a large scale. The VMI storage\nmanagement require dealing with multiple VMIs, typically in the magnitude of\ngigabytes, which entails VMI sprawl issues hindering the elastic resource\nmanagement and provisioning. Nevertheless, existing techniques to facilitate\nVMI management overlook VMI semantics (i.e at the level of base image and\nsoftware packages) with either restricted possibility to identify and extract\nreusable functionalities or with higher VMI publish and retrieval overheads. In\nthis paper, we design, implement and evaluate Expelliarmus, a novel VMI\nmanagement system that helps to minimize storage, publish and retrieval\noverheads. To achieve this goal, Expelliarmus incorporates three complementary\nfeatures. First, it makes use of VMIs modelled as semantic graphs to expedite\nthe similarity computation between multiple VMIs. Second, Expelliarmus provides\na semantic aware VMI decomposition and base image selection to extract and\nstore non-redundant base image and software packages. Third, Expelliarmus can\nalso assemble VMIs based on the required software packages upon user request.\nWe evaluate Expelliarmus through a representative set of synthetic Cloud VMIs\non the real test-bed. Experimental results show that our semantic-centric\napproach is able to optimize repository size by 2.2-16 times compared to\nstate-of-the-art systems (e.g. IBM's Mirage and Hemera) with significant VMI\npublish and slight retrieval performance improvement.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 13:08:14 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 17:19:13 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Saurabh", "Nishant", ""], ["Remmers", "Julian", ""], ["Kimovski", "Dragi", ""], ["Prodan", "Radu", ""], ["Barbosa", "Jorge G.", ""]]}, {"id": "1906.09182", "submitter": "Nishant Saurabh", "authors": "Nishant Saurabh, Dragi Kimovski, Simon Ostermann, Radu Prodan", "title": "VM Image Repository and Distribution Models for Federated Clouds: State\n  of the Art, Possible Directions and Open Issues", "comments": "MI storage repository, VMI distribution, Federated cloud", "journal-ref": "Euro-Par 2016: Euro-Par 2016: Parallel Processing Workshops", "doi": "10.1007/978-3-319-58943-5_21", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging trend of Federated Cloud models enlist virtualization as a\nsignificant concept to offer a large scale distributed Infrastructure as a\nService collaborative paradigm to end users. Virtualization leverage Virtual\nMachines (VM) instantiated from user specific templates labelled as VM Images\n(VMI). To this extent, the rapid provisioning of VMs with varying user requests\nensuring Quality of Service (QoS) across multiple cloud providers largely\ndepends upon the image repository architecture and distribution policies. We\ndiscuss the possible state-of-art in VMI storage repository and distribution\nmechanisms for efficient VM provisioning in federated clouds. In addition, we\npresent and compare various representative systems in this realm. Furthermore,\nwe define a design space, identify current limitations, challenges and open\ntrends for VMI repositories and distribution techniques within federated\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 15:04:49 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Saurabh", "Nishant", ""], ["Kimovski", "Dragi", ""], ["Ostermann", "Simon", ""], ["Prodan", "Radu", ""]]}, {"id": "1906.09314", "submitter": "Christian Cachin", "authors": "Christian Cachin, Bj\\\"orn Tackmann", "title": "Asymmetric Distributed Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quorum systems are a key abstraction in distributed fault-tolerant computing\nfor capturing trust assumptions. They can be found at the core of many\nalgorithms for implementing reliable broadcasts, shared memory, consensus and\nother problems. This paper introduces asymmetric Byzantine quorum systems that\nmodel subjective trust. Every process is free to choose which combinations of\nother processes it trusts and which ones it considers faulty. Asymmetric quorum\nsystems strictly generalize standard Byzantine quorum systems, which have only\none global trust assumption for all processes. This work also presents\nprotocols that implement abstractions of shared memory and broadcast primitives\nwith processes prone to Byzantine faults and asymmetric trust. The model and\nprotocols pave the way for realizing more elaborate algorithms with asymmetric\ntrust.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 20:47:40 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Cachin", "Christian", ""], ["Tackmann", "Bj\u00f6rn", ""]]}, {"id": "1906.09390", "submitter": "Vasileios Porpodas", "authors": "Vasileios Porpodas", "title": "ZOFI: Zero-Overhead Fault Injection Tool for Fast Transient Fault\n  Coverage Analysis", "comments": "Updated bibstyle", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental evaluation of fault-tolerance studies relies on tools that\ninject errors while programs are running, and then monitor the execution and\nthe output for faulty execution. In particular, the established methodology in\nsoftware-based transient-fault reliability studies, involves running each\nworkload hundreds or thousands of times, injecting a random bit-flip in the\nprocess. The majority of such studies rely on custom-built fault-injection\ntools that are based on either a modified processor simulator, or a code\ninstrumentation framework. Such tools are non-trivial to develop, and are\nusually orders of magnitude slower than native execution.\n  In this paper we present ZOFI, a novel timing-based fault-injection tool that\nis aimed at being used in fault-coverage studies for transient faults. ZOFI is\na zero-overhead tool, meaning that the analyzed workload runs at native speed.\nThis is orders-of-magnitude faster compared to common approaches that are\ndesigned around simulators or code instrumentation tools. ZOFI is free software\nand is available at https://github.com/vporpo/zofi.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 05:33:13 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 17:47:59 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 16:57:46 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 00:47:41 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Porpodas", "Vasileios", ""]]}, {"id": "1906.09438", "submitter": "Bingqing Shen", "authors": "Bingqing Shen, Jingzhi Guo", "title": "Efficient Peer-to-Peer Content Sharing for Learning in Virtual Worlds", "comments": null, "journal-ref": "Journal of Universal Computer Science, vol. 25, no. 5 (2019),\n  465-48", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual world technologies provide new and immersive space for learning,\ntraining, and education. They are enabled by the content creation and content\nsharing function for allowing users to create and interoperate various learning\nobjects. Unfortunately, virtual world content sharing based on persistent\nvirtual world content storage, to the best of our knowledge, does not exist. In\nthis paper, we address this problem by proposing a content sharing scheme based\non Virtual Net, a virtual world persistency framework. For efficient content\nretrieval, three strategies have been proposed to reduce communication overhead\nand content load delay.By integrating these strategies, a virtual world content\nsearch and retrieval algorithm has been devised. The experiment results verify\nthe effectiveness of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 12:06:58 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Shen", "Bingqing", ""], ["Guo", "Jingzhi", ""]]}, {"id": "1906.09548", "submitter": "Phuong-Duy Nguyen", "authors": "Phuong-Duy Nguyen and Vu Nguyen Ha and Long Bao Le", "title": "Computation Offloading and Resource Allocation for Backhaul Limited\n  Cooperative MEC Systems", "comments": null, "journal-ref": null, "doi": "10.1109/VTCFall.2019.8891244", "report-no": null, "categories": "cs.DC cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we jointly optimize computation offloading and resource\nallocation to minimize the weighted sum of energy consumption of all mobile\nusers in a backhaul limited cooperative MEC system with multiple fog servers.\nConsidering the partial offloading strategy and TDMA transmission at each base\nstation, the underlying optimization problem with constraints on maximum task\nlatency and limited computation resource at mobile users and fog servers is\nnon-convex. We propose to convexify the problem exploiting the relationship\namong some optimization variables from which an optimal algorithm is proposed\nto solve the resulting problem. We then present numerical results to\ndemonstrate the significant gains of our proposed design compared to\nconventional designs without exploiting cooperation among fog servers and a\ngreedy algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 03:40:17 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Nguyen", "Phuong-Duy", ""], ["Ha", "Vu Nguyen", ""], ["Le", "Long Bao", ""]]}, {"id": "1906.09702", "submitter": "Matthias Noack", "authors": "Matthias Noack", "title": "Heterogeneous Active Messages (HAM) -- Implementing Lightweight Remote\n  Procedure Calls in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HAM (Heterogeneous Active Messages), a C++-only active messaging\nsolution for heterogeneous distributed systems.Combined with a communication\nprotocol, HAM can be used as a generic Remote Procedure Call (RPC) mechanism.\nIt has been used in HAM-Offload to implement a low-overhead offloading\nframework for inter- and intra-node offloading between different architectures\nincluding accelerators like the Intel Xeon Phi x100 series and the NEC\nSX-Aurora TSUBASA Vector Engine. HAM uses template meta-programming to\nimplicitly generate active message types and their corresponding handler\nfunctions. Heterogeneity is enabled by providing an efficient address\ntranslation mechanism between the individual handler code addresses of\nprocesses running different binaries on different architectures, as well a\nhooks to inject serialisation and deserialisation code on a per-type basis.\nImplementing such a solution in modern C++ sheds some light on the shortcomings\nand grey areas of the C++ standard when it comes to distributed and\nheterogeneous environments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:23:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Noack", "Matthias", ""]]}, {"id": "1906.09779", "submitter": "Niklas Carlsson", "authors": "Niklas Carlsson and Derek Eager", "title": "Had You Looked Where I'm Looking: Cross-user Similarities in Viewing\n  Behavior for 360$^{\\circ}$ Video and Caching Implications", "comments": "13+ pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand and usage of 360$^{\\circ}$ video services are expected to\nincrease. However, despite these services being highly bandwidth intensive, not\nmuch is known about the potential value that basic bandwidth saving techniques\nsuch as server or edge-network on-demand caching (e.g., in a CDN) could have\nwhen used for delivery of such services. This problem is both important and\ncomplicated as client-side solutions have been developed that split the full\n360$^{\\circ}$ view into multiple tiles, and adapt the quality of the downloaded\ntiles based on the user's expected viewing direction and bandwidth conditions.\nTo better understand the potential bandwidth savings that caching-based\ntechniques may offer for this context, this paper presents the first\ncharacterization of the similarities in the viewing directions of users\nwatching the same 360$^{\\circ}$ video, the overlap in viewports of these users\n(the area of the full 360$^{\\circ}$ view they actually see), and the potential\ncache hit rates for different video categories, network conditions, and\naccuracy levels in the prediction of future viewing direction when prefetching.\nThe results provide substantial insight into the conditions under which overlap\ncan be considerable and caching effective, and can inform the design of new\ncaching system policies tailored for 360$^{\\circ}$ video.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:31:03 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Carlsson", "Niklas", ""], ["Eager", "Derek", ""]]}, {"id": "1906.09791", "submitter": "Mehmet Aydar", "authors": "Mehmet Aydar and Serkan Ayvaz and Salih Cemil Cetin", "title": "Towards a Blockchain based digital identity verification, record\n  attestation and record sharing system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Covid-19 pandemic has made individuals and organizations to rethink the\nway of handling identity verification and credentials sharing particularly in\nquarantined situations. In this study, we investigate the inefficiencies of\ntraditional identity systems, and discuss how a proper implementation of\nBlockchain technology would result in safer, more secure, privacy respecting\nand remote friendly identity systems. As a result, we propose a Blockchain\nbased framework for digital identity verification, record attestation and\nrecord sharing, and we explain the framework in details with certain use cases.\nOur proposed framework promotes individuals to fully control their identity\ndata and govern the level of the identity data sharing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 08:52:32 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 13:19:00 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Aydar", "Mehmet", ""], ["Ayvaz", "Serkan", ""], ["Cetin", "Salih Cemil", ""]]}, {"id": "1906.09889", "submitter": "Stephen Tarsa", "authors": "Stephen J Tarsa, Chit-Kwan Lin, Gokce Keskin, Gautham Chinya, Hong\n  Wang", "title": "Improving Branch Prediction By Modeling Global History with\n  Convolutional Neural Networks", "comments": "2nd ISCA International Workshop on AI Assisted Design for\n  Architecture (AIDArc), June 2019, Phoenix, AZ, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPU branch prediction has hit a wall--existing techniques achieve\nnear-perfect accuracy on 99% of static branches, and yet the mispredictions\nthat remain hide major performance gains. In a companion report, we show that a\nprimary source of mispredictions is a handful of systematically hard-to-predict\nbranches (H2Ps), e.g. just 10 static instructions per SimPoint phase in SPECint\n2017. The lost opportunity posed by these mispredictions is significant to the\nCPU: 14.0% in instructions-per-cycle (IPC) on Intel SkyLake and 37.4% IPC when\nthe pipeline is scaled four-fold, on par with gains from process technology.\nHowever, up to 80% of this upside is unreachable by the best known branch\npredictors, even when afforded exponentially more resources.\n  New approaches are needed, and machine learning (ML) provides a palette of\npowerful predictors. A growing body of work has shown that ML models are\ndeployable within the microarchitecture to optimize hardware at runtime, and\nare one way to customize CPUs post-silicon by training to customer\napplications. We develop this scenario for branch prediction using\nconvolutional neural networks (CNNs) to boost accuracy for H2Ps. Step-by-step,\nwe (1) map CNNs to the global history data used by existing branch predictors;\n(2) show how CNNs improve H2P prediction in SPEC 2017; (3) adapt 2-bit CNN\ninference to the constraints of current branch prediction units; and (4)\nestablish that CNN helper predictors are reusable across application executions\non different inputs, enabling us to amortize offline training and deploy ML\npattern matching to improve IPC.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 23:19:08 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Tarsa", "Stephen J", ""], ["Lin", "Chit-Kwan", ""], ["Keskin", "Gokce", ""], ["Chinya", "Gautham", ""], ["Wang", "Hong", ""]]}, {"id": "1906.09962", "submitter": "Richard Olaniyan", "authors": "Muthucumaru Maheswaran, Robert Wenger, Richard Olaniyan, Salman Memon,\n  Olamilekan Fadahunsi and Richboy Echomgbe", "title": "A Language for Programming Edge Clouds for Next Generation IoT\n  Applications", "comments": "22 pages, 9 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For effective use of edge computing in an IoT application, we need to\npartition the application into tasks and map them into the cloud, fog (edge\nserver), device levels such that the resources at the different levels are\noptimally used to meet the overall quality of service requirements. In this\npaper, we consider four concerns about application-to-fog mapping: task\nplacement at different levels, data filtering to limit network loading, fog\nfail-over, and data consistency, and reacting to hotspots at the edge. We\ndescribe a programming language and middleware we created for edge computing\nthat addresses the above four concerns. The language has a distributed-node\nprogramming model that allows programs to be written for a collection of nodes\norganized into a cloud, fog, device hierarchy. The paper describes the major\ndesign elements of the language and explains the prototype implementation. The\nunique distributed-node programming model embodied in the language enables new\nedge-oriented programming patterns that are highly suitable for cognitive or\ndata-intensive edge computing workloads. The paper presents result from an\ninitial evaluation of the language prototype and also a distributed shell and a\nsmart parking app that were developed using the programming language.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 01:33:55 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Maheswaran", "Muthucumaru", ""], ["Wenger", "Robert", ""], ["Olaniyan", "Richard", ""], ["Memon", "Salman", ""], ["Fadahunsi", "Olamilekan", ""], ["Echomgbe", "Richboy", ""]]}, {"id": "1906.09963", "submitter": "Richard Olaniyan", "authors": "Richard Olaniyan and Muthucumaru Maheswaran", "title": "Multi-Point Synchronization for Fog-Controlled Internet of Things", "comments": "12 pages, 14 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fog-resident controller architecture for synchronizing\nthe operations of large collections of Internet of Things (IoT) such as drones,\nInternet of Vehicles, etc. Synchronization in IoT is grouped into different\nclasses, use cases identified and multi-point synchronous scheduling algorithms\nare developed to schedule tasks with varying timing requirements; strict\n(synchronous) and relaxed (asynchronous and local) onto a bunch of worker nodes\nthat are coordinated by a fog resident controller in the presence of\ndisconnections and worker failures. The algorithms use time-based or\ncomponent-based redundancy to cope with failures and embed a publish-subscribe\nmessage update scheme to reduce the message overhead at the controller as the\nnumber of workers increase. The performance of the algorithms are evaluated\nusing trace-driven experiments and practicability is shown by implementing the\ntime-based redundancy synchronous scheduling algorithm in JAMScript -- a\npolyglot programming platform for Cloud of Things and report initial findings.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:50:11 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Olaniyan", "Richard", ""], ["Maheswaran", "Muthucumaru", ""]]}, {"id": "1906.09995", "submitter": "Nguyen Ho Ms.", "authors": "Nguyen Ho, Huy Vo, Mai Vu, Torben Bach Pedersen", "title": "AMIC: An Adaptive Information Theoretic Method to Identify Multi-Scale\n  Temporal Correlations in Big Time Series Data -- Accepted Version", "comments": null, "journal-ref": null, "doi": "10.1109/TBDATA.2019.2907987", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent development in computing, sensing and crowd-sourced data have resulted\nin an explosion in the availability of quantitative information. The\npossibilities of analyzing this so-called Big Data to inform research and the\ndecision-making process are virtually endless. In general, analyses have to be\ndone across multiple data sets in order to bring out the most value of Big\nData. A first important step is to identify temporal correlations between data\nsets. Given the characteristics of Big Data in terms of volume and velocity,\ntechniques that identify correlations not only need to be fast and scalable,\nbut also need to help users in ordering the correlations across temporal scales\nso that they can focus on important relationships. In this paper, we present\nAMIC (Adaptive Mutual Information-based Correlation), a method based on mutual\ninformation to identify correlations at multiple temporal scales in large time\nseries. Discovered correlations are suggested to users in an order based on the\nstrength of the relationships. Our method supports an adaptive streaming\ntechnique that minimizes duplicated computation and is implemented on top of\nApache Spark for scalability. We also provide a comprehensive evaluation on the\neffectiveness and the scalability of AMIC using both synthetic and real-world\ndata sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 14:31:46 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 21:28:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Ho", "Nguyen", ""], ["Vo", "Huy", ""], ["Vu", "Mai", ""], ["Pedersen", "Torben Bach", ""]]}, {"id": "1906.10105", "submitter": "Mahdi Soleymani", "authors": "Mohammad Vahid Jamali, Mahdi Soleymani, and Hessam Mahdavifar", "title": "Coded Distributed Computing: Performance Limits and Code Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of coded distributed computing where a large linear\ncomputational job, such as a matrix multiplication, is divided into $k$ smaller\ntasks, encoded using an $(n,k)$ linear code, and performed over $n$ distributed\nnodes. The goal is to reduce the average execution time of the computational\njob. We provide a connection between the problem of characterizing the average\nexecution time of a coded distributed computing system and the problem of\nanalyzing the error probability of codes of length $n$ used over erasure\nchannels. Accordingly, we present closed-form expressions for the execution\ntime using binary random linear codes and the best execution time any\nlinear-coded distributed computing system can achieve. It is also shown that\nthere exist good binary linear codes that attain, asymptotically, the best\nperformance any linear code, not necessarily binary, can achieve. We also\ninvestigate the performance of coded distributed computing systems using polar\nand Reed-Muller (RM) codes that can benefit from low-complexity decoding, and\nsuperior performance, respectively, as well as explicit constructions. The\nproposed framework in this paper can enable efficient designs of distributed\ncomputing systems given the rich literature in the channel coding theory.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:45:14 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Jamali", "Mohammad Vahid", ""], ["Soleymani", "Mahdi", ""], ["Mahdavifar", "Hessam", ""]]}, {"id": "1906.10239", "submitter": "Jan S. Rellermeyer", "authors": "Jan S. Rellermeyer, Maher Amer, Richard Smutzer, Karthick Rajamani", "title": "Container Density Improvements with Dynamic Memory Extension using NAND\n  Flash", "comments": "APSys 2018", "journal-ref": null, "doi": "10.1145/3265723.3265740", "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While containers efficiently implement the idea of operating-system-level\napplication virtualization, they are often insufficient to increase the server\nutilization to a desirable level. The reason is that in practice many\ncontainerized applications experience a limited amount of load while there are\nfew containers with a high load. In such a scenario, the virtual memory\nmanagement system can become the limiting factor to container density even\nthough the working set of active containers would fit into main memory. In this\npaper, we describe and evaluate a system for transparently moving memory pages\nin and out of DRAM and to a NAND Flash medium which is attached through the\nmemory bus. This technique, called Diablo Memory Expansion (DMX), operates on a\nprediction model and is able to relieve the pressure on the memory system. We\npresent a benchmark for container density and show that even under an overall\nconstant workload, adding additional containers adversely affects\nperformance-critical applications in Docker. When using the DMX technology of\nthe Memory1 system, however, the performance of the critical workload remains\nstable.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 21:23:18 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rellermeyer", "Jan S.", ""], ["Amer", "Maher", ""], ["Smutzer", "Richard", ""], ["Rajamani", "Karthick", ""]]}, {"id": "1906.10261", "submitter": "Temitope Ajileye", "authors": "Temitope Ajileye, Boris Motik, Ian Horrocks", "title": "Datalog Materialisation in Distributed RDF Stores with Dynamic Data\n  Exchange", "comments": "16 pages, ISWC conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several centralised RDF systems support datalog reasoning by precomputing and\nstoring all logically implied triples using the wellknown seminaive algorithm.\nLarge RDF datasets often exceed the capacity of centralised RDF systems, and a\ncommon solution is to distribute the datasets in a cluster of shared-nothing\nservers. While numerous distributed query answering techniques are known,\ndistributed seminaive evaluation of arbitrary datalog rules is less understood.\nIn fact, most distributed RDF stores either support no reasoning or can handle\nonly limited datalog fragments. In this paper we extend the dynamic data\nexchange approach for distributed query answering by Potter et al. [12] to a\nreasoning algorithm that can handle arbitrary rules while preserving important\nproperties such as nonrepetition of inferences. We also show empirically that\nour algorithm scales well to very large RDF datasets\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:52:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ajileye", "Temitope", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1906.10275", "submitter": "Abusayeed Saifullah", "authors": "Abusayeed Saifullah", "title": "2-Edge-Connectivity and 2-Vertex-Connectivity of an Asynchronous\n  Distributed Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization for non-masking fault-tolerant distributed system has\nreceived considerable research interest over the last decade. In this paper, we\npropose a self-stabilizing algorithm for 2-edge-connectivity and\n2-vertex-connectivity of an asynchronous distributed computer network. It is\nbased on a self-stabilizing depth-first search, and is not a composite\nalgorithm in the sense that it is not composed of a number of self-stabilizing\nalgorithms that run concurrently. The time and space complexities of the\nalgorithm are the same as those of the underlying self-stabilizing depth-first\nsearch algorithm which are O(dn\\Delta) rounds and O(n\\log \\Delta) bits per\nprocessor, respectively, where \\Delta (<= n) is an upper bound on the degree of\na node, d (<= n) is the diameter of the graph, and n is the number of nodes in\nthe network.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 20:55:18 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Saifullah", "Abusayeed", ""]]}, {"id": "1906.10347", "submitter": "Edward Hu", "authors": "Bodun Hu, Christopher J. Rossbach", "title": "ALTIS: Modernizing GPGPU Benchmarking", "comments": "ISPASS 2020. Project: https://github.com/utcs-scea/altis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Altis, a benchmark suite for modern GPGPU computing.\nPrevious benchmark suites such as Rodinia and SHOC have served the research\ncommunity well, but were developed years ago when hardware was more limited,\nsoftware supported fewer features, and production hardware-accelerated\nworkloads were scarce. Since that time, GPU compute density and memory capacity\nhas grown exponentially, programmability features such as unified memory,\ndemand paging, and HyperQ have matured, and new workloads such as deep neural\nnetworks (DNNs), graph analytics, and crypto-currencies have emerged in\nproduction environments, stressing the hardware and software in ways that\nprevious benchmarks did not anticipate. Drawing inspiration from Rodinia and\nSHOC, Altis is a benchmark suite designed for modern GPU architectures and\nmodern GPU runtimes, representing a diverse set of application domains. By\nadopting and extending applications from Rodinia and SHOC, adding new\napplications, and focusing on CUDA platforms, Altis better represents modern\nGPGPU workloads to enable support GPGPU research in both architecture and\nsystem software.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 06:54:40 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 07:10:39 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Hu", "Bodun", ""], ["Rossbach", "Christopher J.", ""]]}, {"id": "1906.10368", "submitter": "Roland Schmid", "authors": "Roland Schmid, Roger Wattenhofer", "title": "PermitBFT: Exploring the Byzantine Fast-Path", "comments": "20 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PermitBFT establishes a permissioned byzantine ledger in the partially\nsynchronous networking model. For n replicas, PermitBFT tolerates up to f < n/3\nbyzantine replicas. It is the first BFT protocol to achieve a latency of just 2\nmessage delays despite tolerating byzantine replicas throughout the \"fast\ntrack\", as long as they are not the leader. The design of PermitBFT relies on\ntwo fundamental concepts. First, in PermitBFT the participating nodes do not\nwait for a distinguished leader to act and subsequently confirm its actions,\nbut send permits to the next leader proactively. Second, PermitBFT achieves a\nseparation of the decision powers that are usually concentrated on a single\nleader node. A leader in PermitBFT controls which transactions to include in a\nnew block, but not where to append the block in the block graph.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 08:10:20 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 19:09:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Schmid", "Roland", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1906.10468", "submitter": "Adam Lev-Libfeld", "authors": "Adam Lev-Libfeld, Alexander Margolin", "title": "Fast Data: Moving beyond from Big Data's map-reduce", "comments": null, "journal-ref": "journal of geopython no. 1, jun 20 2016", "doi": null, "report-no": null, "categories": "cs.DC cs.GL cs.NI cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data may not be the solution many are looking for. The latest rise of Big\nData methods and systems is partly due to the new abilities these techniques\nprovide, partly to the simplicity of the software design and partly because the\nbuzzword itself has value to investors and clients. That said, popularity is\nnot a measure for suitability and the Big Data approach might not be the best\nsolution, or even an applicable one, to many common problems. Namely, time\ndependent problems whose solution may be bound or cached in any manner can\nbenefit greatly from moving to partly stateless, flow oriented functions and\ndata models. This paper presents such a model to substitute the traditional\nmap-shuffle-reduce models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 12:13:27 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Lev-Libfeld", "Adam", ""], ["Margolin", "Alexander", ""]]}, {"id": "1906.10487", "submitter": "Armin Mehrabian", "authors": "Armin Mehrabian, Mario Miscuglio, Yousra Alkabani, Volker J. Sorger,\n  Tarek El-Ghazawi", "title": "A Winograd-based Integrated Photonics Accelerator for Convolutional\n  Neural Networks", "comments": "12 pages, photonics, artificial intelligence, convolutional neural\n  networks, Winograd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks (NNs) have become the mainstream technology in the artificial\nintelligence (AI) renaissance over the past decade. Among different types of\nneural networks, convolutional neural networks (CNNs) have been widely adopted\nas they have achieved leading results in many fields such as computer vision\nand speech recognition. This success in part is due to the widespread\navailability of capable underlying hardware platforms. Applications have always\nbeen a driving factor for design of such hardware architectures. Hardware\nspecialization can expose us to novel architectural solutions, which can\noutperform general purpose computers for tasks at hand. Although different\napplications demand for different performance measures, they all share speed\nand energy efficiency as high priorities. Meanwhile, photonics processing has\nseen a resurgence due to its inherited high speed and low power nature. Here,\nwe investigate the potential of using photonics in CNNs by proposing a CNN\naccelerator design based on Winograd filtering algorithm. Our evaluation\nresults show that while a photonic accelerator can compete with\ncurrent-state-of-the-art electronic platforms in terms of both speed and power,\nit has the potential to improve the energy efficiency by up to three orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 12:57:07 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 18:50:06 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Mehrabian", "Armin", ""], ["Miscuglio", "Mario", ""], ["Alkabani", "Yousra", ""], ["Sorger", "Volker J.", ""], ["El-Ghazawi", "Tarek", ""]]}, {"id": "1906.10496", "submitter": "Jan S. Rellermeyer", "authors": "Jan S. Rellermeyer, Sobhan Omranian Khorasani, Dan Graur, Apourva\n  Parthasarathy", "title": "The Coming Age of Pervasive Data Processing", "comments": "ISPDC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging Big Data analytics and machine learning applications require a\nsignificant amount of computational power. While there exists a plethora of\nlarge-scale data processing frameworks which thrive in handling the various\ncomplexities of data-intensive workloads, the ever-increasing demand of\napplications have made us reconsider the traditional ways of scaling (e.g.,\nscale-out) and seek new opportunities for improving the performance. In order\nto prepare for an era where data collection and processing occur on a wide\nrange of devices, from powerful HPC machines to small embedded devices, it is\ncrucial to investigate and eliminate the potential sources of inefficiency in\nthe current state of the art platforms. In this paper, we address the current\nand upcoming challenges of pervasive data processing and present directions for\ndesigning the next generation of large-scale data processing systems.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 20:19:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Rellermeyer", "Jan S.", ""], ["Khorasani", "Sobhan Omranian", ""], ["Graur", "Dan", ""], ["Parthasarathy", "Apourva", ""]]}, {"id": "1906.10575", "submitter": "Wayne Mitchell", "authors": "Wayne B. Mitchell, Robert Strzodka, Robert D. Falgout", "title": "Parallel Performance of Algebraic Multigrid Domain Decomposition\n  (AMG-DD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic multigrid (AMG) is a widely used scalable solver and preconditioner\nfor large-scale linear systems resulting from the discretization of a wide\nclass of elliptic PDEs. While AMG has optimal computational complexity, the\ncost of communication has become a significant bottleneck that limits its\nscalability as processor counts continue to grow on modern machines. This paper\nexamines the design, implementation, and parallel performance of a novel\nalgorithm, Algebraic Multigrid Domain Decomposition (AMG-DD), designed\nspecifically to limit communication. The goal of AMG-DD is to provide a\nlow-communication alternative to standard AMG V-cycles by trading some\nadditional computational overhead for a significant reduction in communication\ncost. Numerical results show that AMG-DD achieves superior accuracy per\ncommunication cost compared to AMG, and speedup over AMG is demonstrated on a\nlarge GPU cluster.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:47:23 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 12:18:59 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mitchell", "Wayne B.", ""], ["Strzodka", "Robert", ""], ["Falgout", "Robert D.", ""]]}, {"id": "1906.10602", "submitter": "Shiyuan Deng", "authors": "Shiyuan Deng, Xiao Yan, Kelvin K.W. Ng, Chenyu Jiang, James Cheng", "title": "Pyramid: A General Framework for Distributed Similarity Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity search is a core component in various applications such as image\nmatching, product recommendation and low-shot classification. However, single\nmachine solutions are usually insufficient due to the large cardinality of\nmodern datasets and stringent latency requirement of on-line query processing.\nWe present Pyramid, a general and efficient framework for distributed\nsimilarity search. Pyramid supports search with popular similarity functions\nincluding Euclidean distance, angular distance and inner product. Different\nfrom existing distributed solutions that are based on KD-tree or locality\nsensitive hashing (LSH), Pyramid is based on Hierarchical Navigable Small World\ngraph (HNSW), which is the state of the art similarity search algorithm on a\nsingle machine. To achieve high query processing throughput, Pyramid partitions\na dataset into sub-datasets containing similar items for index building and\nassigns a query to only some of the sub-datasets for query processing. To\nprovide the robustness required by production deployment, Pyramid also supports\nfailure recovery and straggler mitigation. Pyramid offers a set of concise API\nsuch that users can easily use Pyramid without knowing the details of\ndistributed execution. Experiments on large-scale datasets show that Pyramid\nproduces quality results for similarity search, achieves high query processing\nthroughput and is robust under node failure and straggler.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:23:52 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Deng", "Shiyuan", ""], ["Yan", "Xiao", ""], ["Ng", "Kelvin K. W.", ""], ["Jiang", "Chenyu", ""], ["Cheng", "James", ""]]}, {"id": "1906.10613", "submitter": "Wayne Mitchell", "authors": "Wayne Mitchell, Tom Manteuffel", "title": "Advances in Implementation, Theoretical Motivation, and Numerical\n  Results for the Nested Iteration with Range Decomposition Algorithm", "comments": null, "journal-ref": "Numer Linear Algebra Appl. 2018; 25:e2149", "doi": "10.1002/nla.2149", "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a low-communication algorithm for solving elliptic partial\ndifferential equations (PDE's) on high-performance machines, the nested\niteration with range decomposition algorithm (NIRD). Previous work has shown\nthat NIRD converges to a high level of accuracy within a small, fixed number of\niterations (usually one or two) when applied to simple elliptic problems. This\npaper makes some improvements to the NIRD algorithm (including the addition of\nadaptivity during preprocessing, wider choice of partitioning functions, and\nmodified error measurement) that enhance the method's accuracy and scalability,\nespecially on more difficult problems. In addition, an updated convergence\nproof is presented based on heuristic assumptions that are supported by\nnumerical evidence. Furthermore, a new performance model is developed that\nshows increased performance benefits for NIRD when problems are more expensive\nto solve using traditional methods. Finally, extensive testing on a variety of\nelliptic problems provides additional insight into the behavior of NIRD and\nadditional evidence that NIRD achieves excellent convergence on a wide class of\nelliptic PDE's and, as such, should be a very competitive method for solving\nPDE's on large parallel computers.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:41:21 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Mitchell", "Wayne", ""], ["Manteuffel", "Tom", ""]]}, {"id": "1906.10664", "submitter": "Mehmet Aktas", "authors": "Mehmet Fatih Aktas and Emina Soljanin", "title": "Straggler Mitigation at Scale", "comments": "To appear in Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime performance variability at the servers has been a major issue,\nhindering the predictable and scalable performance in modern distributed\nsystems. Executing requests or jobs redundantly over multiple servers has been\nshown to be effective for mitigating variability, both in theory and practice.\nSystems that employ redundancy has drawn significant attention, and numerous\npapers have analyzed the pain and gain of redundancy under various service\nmodels and assumptions on the runtime variability. This paper presents a cost\n(pain) vs. latency (gain) analysis of executing jobs of many tasks by employing\nreplicated or erasure coded redundancy. Tail heaviness of service time\nvariability is decisive on the pain and gain of redundancy and we quantify its\neffect by deriving expressions for the cost and latency. Specifically, we try\nto answer four questions: 1) How do replicated and coded redundancy compare in\nthe cost vs. latency tradeoff? 2) Can we introduce redundancy after waiting\nsome time and expect to reduce the cost? 3) Can relaunching the tasks that\nappear to be straggling after some time help to reduce cost and/or latency? 4)\nIs it effective to use redundancy and relaunching together? We validate the\nanswers we found for each of the questions via simulations that use empirical\ndistributions extracted from a Google cluster data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 16:58:02 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 14:07:57 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Aktas", "Mehmet Fatih", ""], ["Soljanin", "Emina", ""]]}, {"id": "1906.10684", "submitter": "Wei-Ting Chang", "authors": "Wei-Ting Chang, Ravi Tandon", "title": "On the Upload versus Download Cost for Secure and Private Matrix\n  Multiplication", "comments": "To appear at IEEE Information Theory Workshop (ITW) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of secure and private distributed matrix\nmultiplication. Specifically, we focus on a scenario where a user wants to\ncompute the product of a confidential matrix $A$, with a matrix $B_\\theta$,\nwhere $\\theta\\in\\{1,\\dots,M\\}$. The set of candidate matrices\n$\\{B_1,\\dots,B_M\\}$ are public, and available at all the $N$ servers. The goal\nof the user is to distributedly compute $AB_{\\theta}$, such that $(a)$ no\ninformation is leaked about the matrix $A$ to any server; and $(b)$ the index\n$\\theta$ is kept private from each server. Our goal is to understand the\nfundamental tradeoff between the upload vs download cost for this problem. Our\nmain contribution is to show that the lower convex hull of following (upload,\ndownload) pairs: $(U,D)=(N/(K-1),(K/(K-1))(1+(K/N)+\\dots+(K/N)^{M-1}))$ for\n$K=2,\\dots,N$ is achievable. The scheme improves upon state-of-the-art existing\nschemes for this problem, and leverages ideas from secret sharing and coded\nprivate information retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 17:59:10 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Chang", "Wei-Ting", ""], ["Tandon", "Ravi", ""]]}, {"id": "1906.10718", "submitter": "Jia Qian", "authors": "Jia Qian, Sayantan Sengupta, Lars Kai Hansen", "title": "Active Learning Solution on Distributed Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 becomes possible through the convergence between Operational and\nInformation Technologies. All the requirements to realize the convergence is\nintegrated on the Fog Platform. Fog Platform is introduced between the cloud\nserver and edge devices when the unprecedented generation of data causes the\nburden of the cloud server, leading the ineligible latency. In this new\nparadigm, we divide the computation tasks and push it down to edge devices.\nFurthermore, local computing (at edge side) may improve privacy and trust. To\naddress these problems, we present a new method, in which we decompose the data\naggregation and processing, by dividing them between edge devices and fog nodes\nintelligently. We apply active learning on edge devices; and federated learning\non the fog node which significantly reduces the data samples to train the model\nas well as the communication cost. To show the effectiveness of the proposed\nmethod, we implemented and evaluated its performance for an image\nclassification task. In addition, we consider two settings: massively\ndistributed and non-massively distributed and offer the corresponding\nsolutions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 18:27:07 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Qian", "Jia", ""], ["Sengupta", "Sayantan", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1906.10817", "submitter": "Songze Li", "authors": "Songze Li, Saeid Sahraei, Mingchao Yu, Salman Avestimehr, Sreeram\n  Kannan, Pramod Viswanath", "title": "Coded State Machine -- Scaling State Machine Execution under Byzantine\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an information-theoretic framework, named Coded State Machine\n(CSM), to securely and efficiently execute multiple state machines on untrusted\nnetwork nodes, some of which are Byzantine. The standard method of solving this\nproblem is using State Machine Replication, which achieves high security at the\ncost of low efficiency. We propose CSM, which achieves the optimal linear\nscaling in storage efficiency, throughput, and security simultaneously with the\nsize of the network. The storage efficiency is scaled via the design of\nLagrange coded states and coded input commands that require the same storage\nsize as their origins. The computational efficiency is scaled using a novel\ndelegation algorithm, called INTERMIX, which is an information-theoretically\nverifiable matrix-vector multiplication algorithm of independent interest.\nUsing INTERMIX, the network nodes securely delegate their coding operations to\na single worker node, and a small group of randomly selected auditor nodes\nverify its correctness, so that computational efficiency can scale almost\nlinearly with the network size, without compromising on security.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 02:39:18 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Li", "Songze", ""], ["Sahraei", "Saeid", ""], ["Yu", "Mingchao", ""], ["Avestimehr", "Salman", ""], ["Kannan", "Sreeram", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1906.10860", "submitter": "Adam Lev-Libfeld", "authors": "Adam Lev-Libfeld", "title": "Lawn: an Unbound Low Latency Timer Data Structure for Large Scale, High\n  Throughput Systems", "comments": "6 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As demand for Real-Time applications rises among the general public, the\nimportance of enabling large-scale, unbound algorithms to solve conventional\nproblems with low to no latency is critical for product viability. Timer\nalgorithms are prevalent in the core mechanisms behind operating systems,\nnetwork protocol implementation, stream processing, and several database\ncapabilities. This paper presents a field-tested algorithm for low latency,\nunbound range timer structure, based upon the well excepted Timing Wheel\nalgorithm. Using a set of queues hashed by TTL, the algorithm allows for a\nsimpler implementation, minimal overhead no overflow and no performance\ndegradation in comparison to the current state of the algorithms under typical\nuse cases.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 06:16:13 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 09:50:24 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Lev-Libfeld", "Adam", ""]]}, {"id": "1906.10917", "submitter": "Pierre Sutra", "authors": "Pierre Sutra", "title": "On the correctness of Egalitarian Paxos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies a problem in both the TLA+ specification and the\nimplementation of the Egalitarian Paxos protocol. It is related to how replicas\nswitch from one ballot to another when computing the dependencies of a command.\nThe problem may lead replicas to diverge and break the linearizability of the\nreplicated service.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 08:55:57 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:38:01 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sutra", "Pierre", ""]]}, {"id": "1906.10944", "submitter": "Linus Seelinger", "authors": "Linus Seelinger and Anne Reinarz and Robert Scheichl", "title": "A High-Performance Implementation of a Robust Preconditioner for\n  Heterogeneous Problems", "comments": "Fixed error in definition of partition of unity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient implementation of the highly robust and scalable\nGenEO preconditioner in the high-performance PDE framework DUNE. The GenEO\ncoarse space is constructed by combining low energy solutions of a local\ngeneralised eigenproblem using a partition of unity. In this paper we\ndemonstrate both weak and strong scaling for the GenEO solver on over 15,000\ncores by solving an industrially motivated problem with over 200 million\ndegrees of freedom. Further, we show that for highly complex parameter\ndistributions arising in certain real-world applications, established methods\nbecome intractable while GenEO remains fully effective. The purpose of this\npaper is two-fold: to demonstrate the robustness and high parallel efficiency\nof the solver and to document the technical details that are crucial to the\nefficiency of the code.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 09:59:13 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 10:02:14 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Seelinger", "Linus", ""], ["Reinarz", "Anne", ""], ["Scheichl", "Robert", ""]]}, {"id": "1906.10970", "submitter": "Andreas Gocht", "authors": "Andreas Gocht, Robert Sch\\\"one, Mario Bielert", "title": "Q-Learning Inspired Self-Tuning for Energy Efficiency in HPC", "comments": "4 pages short paper, HPCS 2019, AHPC 2019, READEX, HAEC, Horizon2020,\n  H2020 grant agreement number 671657, DFG, CRC 912", "journal-ref": null, "doi": "10.1109/HPCS48598.2019.9188112", "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System self-tuning is a crucial task to lower the energy consumption of\ncomputers. Traditional approaches decrease the processor frequency in idle or\nsynchronisation periods. However, in High-Performance Computing (HPC) this is\nnot sufficient: if the executed code is load balanced, there are neither idle\nnor synchronisation phases that can be exploited. Therefore, alternative\nself-tuning approaches are needed, which allow exploiting different compute\ncharacteristics of HPC programs.\n  The novel notion of application regions based on function call stacks,\nintroduced in the Horizon 2020 Project READEX, allows us to define such a\nself-tuning approach. In this paper, we combine these regions with the\nQ-Learning typical state-action maps, which save information about available\nstates, possible actions to take, and the expected rewards. By exploiting the\nexisting processor power interface, we are able to provide direct feedback to\nthe learning process. This approach allows us to save up to 15% energy, while\nonly adding a minor runtime overhead.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 11:00:16 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 14:45:28 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Gocht", "Andreas", ""], ["Sch\u00f6ne", "Robert", ""], ["Bielert", "Mario", ""]]}, {"id": "1906.11056", "submitter": "Rajkumar Buyya", "authors": "Shreshth Tuli, Nipam Basumatary and Rajkumar Buyya", "title": "EdgeLens: Deep Learning based Object Detection in Integrated IoT, Fog\n  and Cloud Computing Environments", "comments": "7 pages, 11 figures, conference paper", "journal-ref": "Proceedings of the 4th IEEE International Conference on\n  Information Systems and Computer Networks (ISCON 2019, IEEE Press, USA),\n  Mathura, India, November 21-22, 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive applications are growing at an increasing rate and there is a\ngrowing need to solve scalability and high-performance issues in them. By the\nadvent of Cloud computing paradigm, it became possible to harness remote\nresources to build and deploy these applications. In recent years, new set of\napplications and services based on Internet of Things (IoT) paradigm, require\nto process large amount of data in very less time. Among them surveillance and\nobject detection have gained prime importance, but cloud is unable to bring\ndown the network latencies to meet the response time requirements. This problem\nis solved by Fog computing which harnesses resources in the edge of the network\nalong with remote cloud resources as required. However, there is still a lack\nof frameworks that are successfully able to integrate sophisticated software\nand applications, especially deep learning, with fog and cloud computing\nenvironments. In this work, we propose a framework to deploy deep\nlearning-based applications in fog-cloud environments to harness edge and cloud\nresources to provide better service quality for such applications. Our proposed\nframework, called EdgeLens, adapts to the application or user requirements to\nprovide high accuracy or low latency modes of services. We also tested the\nperformance of the software in terms of accuracy, response time, jitter,\nnetwork bandwidth and power consumption and show how EdgeLens adapts to\ndifferent service requirements.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:53:47 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Tuli", "Shreshth", ""], ["Basumatary", "Nipam", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1906.11121", "submitter": "Yuichi Sudo", "authors": "Yuichi Sudo, Toshimitsu Masuzawa", "title": "Leader Election Requires Logarithmic Time in Population Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that every leader election protocol requires logarithmic\nstabilization time both in expectation and with high probability in the\npopulation protocol model. This lower bound holds even if each agent has\nknowledge of the exact size of a population and is allowed to use an\narbitrarily large number of agent states. This lower bound concludes that the\nprotocol given in [Sudo et al., SSS 2019] is time-optimal in expectation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:50:19 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 11:01:23 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 05:05:17 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2019 11:06:43 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sudo", "Yuichi", ""], ["Masuzawa", "Toshimitsu", ""]]}, {"id": "1906.11204", "submitter": "S\\'ebastien Vaucher", "authors": "S\\'ebastien Vaucher and Valerio Schiavoni and Pascal Felber", "title": "Stress-SGX: Load and Stress your Enclaves for Fun and Profit", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": "in Networked Systems, Springer International Publishing, 2019, pp.\n  358-363", "doi": "10.1007/978-3-030-05529-5_24", "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest generation of Intel processors supports Software Guard Extensions\n(SGX), a set of instructions that implements a Trusted Execution Environment\n(TEE) right inside the CPU, by means of so-called enclaves. This paper presents\nStress-SGX, an easy-to-use stress-test tool to evaluate the performance of\nSGX-enabled nodes. We build on top of the popular Stress-NG tool, while only\nkeeping the workload injectors (stressors) that are meaningful in the SGX\ncontext. We report on several insights and lessons learned about porting legacy\ncode to run inside an SGX enclave, as well as the limitations introduced by\nthis process. Finally, we use Stress-SGX to conduct a study comparing the\nperformance of different SGX-enabled machines.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 16:47:44 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Vaucher", "S\u00e9bastien", ""], ["Schiavoni", "Valerio", ""], ["Felber", "Pascal", ""]]}, {"id": "1906.11229", "submitter": "Christian Gorenflo", "authors": "Christian Gorenflo, Lukasz Golab, Srinivasan Keshav", "title": "XOX Fabric: A hybrid approach to blockchain transaction execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance and scalability are major concerns for blockchains:\npermissionless systems are typically limited by slow proof of X consensus\nalgorithms and sequential post-order transaction execution on every node of the\nnetwork. By introducing a small amount of trust in their participants,\npermissioned blockchain systems such as Hyperledger Fabric can benefit from\nmore efficient consensus algorithms and make use of parallel pre-order\nexecution on a subset of network nodes. Fabric, in particular, has been shown\nto handle tens of thousands of transactions per second. However, this\nperformance is only achievable for contention-free transaction workloads. If\nmany transactions compete for a small set of hot keys in the world state, the\neffective throughput drops drastically. We therefore propose XOX: a novel\ntwo-pronged transaction execution approach that both minimizes invalid\ntransactions in the Fabric blockchain and maximizes concurrent execution. Our\napproach additionally prevents unintentional denial of service attacks by\nclients re-submitting conflicting transactions. Even under fully contentious\nworkloads, XOX can handle more than 3000 transactions per second, all of which\nwould be discarded by regular Fabric.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 17:43:11 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 17:10:23 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 23:25:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gorenflo", "Christian", ""], ["Golab", "Lukasz", ""], ["Keshav", "Srinivasan", ""]]}, {"id": "1906.11321", "submitter": "Isabelly Rocha", "authors": "Isabelly Rocha, Christian G\\\"ottel, Pascal Felber, Marcelo Pasin,\n  Romain Rouvoy, Valerio Schiavoni", "title": "HEATS: Heterogeneity- and Energy-Aware Task-based Scheduling", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cloud providers usually offer diverse types of hardware for their users.\nCustomers exploit this option to deploy cloud instances featuring GPUs, FPGAs,\narchitectures other than x86 (e.g., ARM, IBM Power8), or featuring certain\nspecific extensions (e.g, Intel SGX). We consider in this work the instances\nused by customers to deploy containers, nowadays the de facto standard for\nmicro-services, or to execute computing tasks. In doing so, the underlying\ncontainer orchestrator (e.g., Kubernetes) should be designed so as to take into\naccount and exploit this hardware diversity. In addition, besides the feature\nrange provided by different machines, there is an often overlooked diversity in\nthe energy requirements introduced by hardware heterogeneity, which is simply\nignored by default container orchestrator's placement strategies. We introduce\nHEATS, a new task-oriented and energy-aware orchestrator for containerized\napplications targeting heterogeneous clusters. HEATS allows customers to trade\nperformance vs. energy requirements. Our system first learns the performance\nand energy features of the physical hosts. Then, it monitors the execution of\ntasks on the hosts and opportunistically migrates them onto different cluster\nnodes to match the customer-required deployment trade-offs. Our HEATS prototype\nis implemented within Google's Kubernetes. The evaluation with synthetic traces\nin our cluster indicate that our approach can yield considerable energy savings\n(up to 8.5%) and only marginally affect the overall runtime of deployed tasks\n(by at most 7%). HEATS is released as open-source.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 19:59:57 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Rocha", "Isabelly", ""], ["G\u00f6ttel", "Christian", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Rouvoy", "Romain", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "1906.11327", "submitter": "Omri Ben-Eliezer", "authors": "Omri Ben-Eliezer and Eylon Yogev", "title": "The Adversarial Robustness of Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.CR cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling is a fundamental primitive in modern algorithms, statistics,\nand machine learning, used as a generic method to obtain a small yet\n\"representative\" subset of the data. In this work, we investigate the\nrobustness of sampling against adaptive adversarial attacks in a streaming\nsetting: An adversary sends a stream of elements from a universe $U$ to a\nsampling algorithm (e.g., Bernoulli sampling or reservoir sampling), with the\ngoal of making the sample \"very unrepresentative\" of the underlying data\nstream. The adversary is fully adaptive in the sense that it knows the exact\ncontent of the sample at any given point along the stream, and can choose which\nelement to send next accordingly, in an online manner.\n  Well-known results in the static setting indicate that if the full stream is\nchosen in advance (non-adaptively), then a random sample of size $\\Omega(d /\n\\varepsilon^2)$ is an $\\varepsilon$-approximation of the full data with good\nprobability, where $d$ is the VC-dimension of the underlying set system\n$(U,R)$. Does this sample size suffice for robustness against an adaptive\nadversary? The simplistic answer is \\emph{negative}: We demonstrate a set\nsystem where a constant sample size (corresponding to VC-dimension $1$)\nsuffices in the static setting, yet an adaptive adversary can make the sample\nvery unrepresentative, as long as the sample size is (strongly) sublinear in\nthe stream length, using a simple and easy-to-implement attack.\n  However, this attack is \"theoretical only\", requiring the set system size to\n(essentially) be exponential in the stream length. This is not a coincidence:\nWe show that to make Bernoulli or reservoir sampling robust against adaptive\nadversaries, the modification required is solely to replace the VC-dimension\nterm $d$ in the sample size with the cardinality term $\\log |R|$. This nearly\nmatches the bound imposed by the attack.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 20:15:54 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Ben-Eliezer", "Omri", ""], ["Yogev", "Eylon", ""]]}, {"id": "1906.11461", "submitter": "Volkan Dedeoglu", "authors": "Volkan Dedeoglu, Raja Jurdak, Guntur D. Putra, Ali Dorri, Salil S.\n  Kanhere", "title": "A Trust Architecture for Blockchain in IoT", "comments": "Submitted to MobiQuitous2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain is a promising technology for establishing trust in IoT networks,\nwhere network nodes do not necessarily trust each other. Cryptographic hash\nlinks and distributed consensus mechanisms ensure that the data stored on an\nimmutable blockchain can not be altered or deleted. However, blockchain\nmechanisms do not guarantee the trustworthiness of data at the origin. We\npropose a layered architecture for improving the end-to-end trust that can be\napplied to a diverse range of blockchain-based IoT applications. Our\narchitecture evaluates the trustworthiness of sensor observations at the data\nlayer and adapts block verification at the blockchain layer through the\nproposed data trust and gateway reputation modules. We present the performance\nevaluation of the data trust module using a simulated indoor target\nlocalization and the gateway reputation module using an end-to-end blockchain\nimplementation, together with a qualitative security analysis for the\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 07:11:43 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Dedeoglu", "Volkan", ""], ["Jurdak", "Raja", ""], ["Putra", "Guntur D.", ""], ["Dorri", "Ali", ""], ["Kanhere", "Salil S.", ""]]}, {"id": "1906.11524", "submitter": "Seri Khoury", "authors": "Ken-ichi Kawarabayashi, Seri Khoury, Aaron Schild, and Gregory\n  Schwartzman", "title": "Improved Distributed Approximations for Maximum Independent Set", "comments": "This version contains improved results compared to the previous one", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present improved results for approximating maximum-weight independent set\n($\\MaxIS$) in the CONGEST and LOCAL models of distributed computing. Given an\ninput graph, let $n$ and $\\Delta$ be the number of nodes and maximum degree,\nrespectively, and let $\\MIS(n,\\Delta)$ be the the running time of finding a\n\\emph{maximal} independent set ($\\MIS$) in the CONGEST model. Bar-Yehuda et al.\n[PODC 2017] showed that there is an algorithm in the CONGEST model that finds a\n$\\Delta$-approximation for $\\MaxIS$ in $O(\\MIS(n,\\Delta)\\log W)$ rounds, where\n$W$ is the maximum weight of a node in the graph, which can be as high as\n$\\poly (n)$. Whether their algorithm is deterministic or randomized depends on\nthe $\\MIS$ algorithm that is used as a black-box.\n  Our main result in this work is a randomized $(\\poly(\\log\\log\nn)/\\epsilon)$-round algorithm that finds, with high probability, a\n$(1+\\epsilon)\\Delta$-approximation for $\\MaxIS$ in the CONGEST model. That is,\nby sacrificing only a tiny fraction of the approximation guarantee, we achieve\nan \\emph{exponential} speed-up in the running time over the previous best known\nresult. Due to a lower bound of $\\Omega(\\sqrt{\\log n/\\log \\log n})$ that was\ngiven by Kuhn, Moscibroda and Wattenhofer [JACM, 2016] on the number of rounds\nfor any (possibly randomized) algorithm that finds a maximal independent set\n(even in the LOCAL model) this result implies that finding a\n$(1+\\epsilon)\\Delta$-approximation for $\\MaxIS$ is exponentially easier than\n$\\MIS$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 09:52:02 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 05:06:30 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Kawarabayashi", "Ken-ichi", ""], ["Khoury", "Seri", ""], ["Schild", "Aaron", ""], ["Schwartzman", "Gregory", ""]]}, {"id": "1906.11721", "submitter": "Parwat Singh Anjana", "authors": "Shrey Baheti, Parwat Singh Anjana, Sathya Peri, Yogesh Simmhan", "title": "DiPETrans: A Framework for Distributed Parallel Execution of\n  Transactions of Blocks in Blockchain", "comments": "38 Pages, 25 Figures, and 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contemporary blockchain such as Bitcoin and Ethereum execute transactions\nserially by miners and validators and determine the Proof-of-Work (PoW). Such\nserial execution is unable to exploit modern multi-core resources efficiently,\nhence limiting the system throughput and increasing the transaction acceptance\nlatency. The objective of this work is to increase the transaction throughput\nby introducing parallel transaction execution using a static analysis\ntechnique. We propose a framework DiPETrans for the distributed execution of\nthe transactions in a block. Here, peers in the blockchain network form a\ncommunity to execute the transactions and find the PoW parallelly, using a\nleader-follower approach. During mining, the leader statically analyzes the\ntransactions, creates different groups (shards) of independent transactions,\nand distributes them to followers to execute them in parallel. After the\ntransaction executes, the community's compute power is utilized to solve the\nPoW concurrently. When a block is successfully created, the leader broadcasts\nthe proposed block to other peers in the network for validation. On receiving a\nblock, validators re-execute the block transactions and accept the block if\nthey reach the same state as shared by the miner. Validation can also be done\nas a community, in parallel, following the same leader-follower approach as\nmining. We report experiments using over 5 Million real transactions from the\nEthereum blockchain and execute them using our DiPETrans framework to\nempirically validate the benefits of our techniques over traditional sequential\nexecution. We achieve a maximum speedup of 2.2x for the miner and 2.0x for the\nvalidator, with 100 to 500 transactions per block. Further, we achieve a peak\nof 5x end-to-end block creation speedup using a parallel miner over a serial\nminer when using 6 machines in the community.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:09:11 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 14:30:45 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 04:56:48 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 16:21:43 GMT"}, {"version": "v5", "created": "Sat, 13 Mar 2021 06:00:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baheti", "Shrey", ""], ["Anjana", "Parwat Singh", ""], ["Peri", "Sathya", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1906.12018", "submitter": "Ruoming Jin", "authors": "Ruoming Jin, Zhen Peng, Wendell Wu, Feodor Dragan, Gagan Agrawal, Bin\n  Ren", "title": "Pruned Landmark Labeling Meets Vertex Centric Computation: A\n  Surprisingly Happy Marriage!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study how the Pruned Landmark Labeling (PPL) algorithm can\nbe parallelized in a scalable fashion, producing the same results as the\nsequential algorithm. More specifically, we parallelize using a Vertex-Centric\n(VC) computational model on a modern SIMD powered multicore architecture. We\ndesign a new VC-PLL algorithm that resolves the apparent mismatch between the\ninherent sequential dependence of the PLL algorithm and the Vertex- Centric\n(VC) computing model. Furthermore, we introduce a novel batch execution model\nfor VC computation and the BVC-PLL algorithm to reduce the computational\ninefficiency in VC-PLL. Quite surprisingly, the theoretical analysis reveals\nthat under a reasonable assumption, BVC-PLL has lower computational and memory\naccess costs than PLL and indicates it may run faster than PLL as a sequential\nalgorithm. We also demonstrate how BVC-PLL algorithm can be extended to handle\ndirected graphs and weighted graphs and how it can utilize the hierarchical\nparallelism on a modern parallel computing architecture. Extensive experiments\non real-world graphs not only show the sequential BVC-PLL can run more than two\ntimes faster than the original PLL, but also demonstrates its parallel\nefficiency and scalability.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 02:19:19 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Jin", "Ruoming", ""], ["Peng", "Zhen", ""], ["Wu", "Wendell", ""], ["Dragan", "Feodor", ""], ["Agrawal", "Gagan", ""], ["Ren", "Bin", ""]]}, {"id": "1906.12043", "submitter": "Shuheng Shen", "authors": "Shuheng Shen and Linli Xu and Jingchang Liu and Xianfeng Liang and\n  Yifei Cheng", "title": "Faster Distributed Deep Net Training: Computation and Communication\n  Decoupled Stochastic Gradient Descent", "comments": "IJCAI2019, 20 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increase in the amount of data and the expansion of model scale,\ndistributed parallel training becomes an important and successful technique to\naddress the optimization challenges. Nevertheless, although distributed\nstochastic gradient descent (SGD) algorithms can achieve a linear iteration\nspeedup, they are limited significantly in practice by the communication cost,\nmaking it difficult to achieve a linear time speedup. In this paper, we propose\na computation and communication decoupled stochastic gradient descent\n(CoCoD-SGD) algorithm to run computation and communication in parallel to\nreduce the communication cost. We prove that CoCoD-SGD has a linear iteration\nspeedup with respect to the total computation capability of the hardware\nresources. In addition, it has a lower communication complexity and better time\nspeedup comparing with traditional distributed SGD algorithms. Experiments on\ndeep neural network training demonstrate the significant improvements of\nCoCoD-SGD: when training ResNet18 and VGG16 with 16 Geforce GTX 1080Ti GPUs,\nCoCoD-SGD is up to 2-3$\\times$ faster than traditional synchronous SGD.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 05:20:05 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 05:35:35 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Shen", "Shuheng", ""], ["Xu", "Linli", ""], ["Liu", "Jingchang", ""], ["Liang", "Xianfeng", ""], ["Cheng", "Yifei", ""]]}, {"id": "1906.12098", "submitter": "Andr\\'es Goens", "authors": "Sebastian Ertel and Justus Adam and Norman A. Rink and Andr\\'es Goens\n  and Jeronimo Castrillon", "title": "Category-Theoretic Foundations of \"STCLang: State Thread Composition as\n  a Foundation for Monadic Dataflow Parallelism\"", "comments": "11 Pages, Supplementary to article published in the Haskell'19\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript gives a category-theoretic foundation to the composition of\nState Threads as a Foundation for Monadic Dataflow Parallelism. It serves as a\nsupplementary formalization of the concepts introduced in the Article \"STCLang:\nState Thread Composition as a Foundation for Monadic Dataflow Parallelism\", as\npublished in Proceedings of the 12th ACM SIGPLAN International Symposium on\nHaskell (Haskell'19).\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:54:55 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 12:55:26 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Ertel", "Sebastian", ""], ["Adam", "Justus", ""], ["Rink", "Norman A.", ""], ["Goens", "Andr\u00e9s", ""], ["Castrillon", "Jeronimo", ""]]}, {"id": "1906.12140", "submitter": "Swanand Kadhe", "authors": "Swanand Kadhe, Jichan Chung, and Kannan Ramchandran", "title": "SeF: A Secure Fountain Architecture for Slashing Storage Costs in\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Full nodes, which synchronize the entire blockchain history and independently\nvalidate all the blocks, form the backbone of any blockchain network by playing\na vital role in ensuring security properties. On the other hand, a user running\na full node needs to pay a heavy price in terms of storage costs. E.g., the\nBitcoin blockchain size has grown over 215GB, in spite of its low throughput.\nThe ledger size for a high throughput blockchain Ripple has already reached\n9TB, and it is growing at an astonishing rate of 12GB per day!\n  In this paper, we propose an architecture based on 'fountain codes', a class\nof erasure codes, that enables any full node to 'encode' validated blocks into\na small number of 'coded blocks', thereby reducing its storage costs by orders\nof magnitude. In particular, our proposed \"Secure Fountain (SeF)\" architecture\ncan achieve a near-optimal trade-off between the storage savings per node and\nthe 'bootstrap cost' in terms of the number of (honest) storage-constrained\nnodes a new node needs to contact to recover the blockchain. A key technical\ninnovation in SeF codes is to make fountain codes secure against adversarial\nnodes that can provide maliciously formed coded blocks. Our idea is to use the\nheader-chain as a 'side-information' to check whether a coded block is\nmaliciously formed while it is getting decoded. Further, the 'rateless\nproperty' of fountain codes helps in achieving high decentralization and\nscalability. Our experiments demonstrate that SeF codes tuned to achieve 1000x\nstorage savings enable full nodes to encode the 191GB Bitcoin blockchain into\n195MB on average. A new node can recover the blockchain from an arbitrary set\nof storage-constrained nodes as long as the set contains ~1100 honest nodes on\naverage. Note that for a 1000x storage savings, the fundamental bound on the\nnumber of honest nodes to contact is 1000: we need about 10% more in practice.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 11:32:33 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Kadhe", "Swanand", ""], ["Chung", "Jichan", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1906.12345", "submitter": "Shi Pu", "authors": "Shi Pu, Alex Olshevsky, Ioannis Ch. Paschalidis", "title": "Asymptotic Network Independence in Distributed Stochastic Optimization\n  for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a discussion of several recent results which, in certain\nscenarios, are able to overcome a barrier in distributed stochastic\noptimization for machine learning. Our focus is the so-called asymptotic\nnetwork independence property, which is achieved whenever a distributed method\nexecuted over a network of n nodes asymptotically converges to the optimal\nsolution at a comparable rate to a centralized method with the same\ncomputational power as the entire network. We explain this property through an\nexample involving the training of ML models and sketch a short mathematical\nanalysis for comparing the performance of distributed stochastic gradient\ndescent (DSGD) with centralized stochastic gradient decent (SGD).\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 17:55:19 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 15:15:16 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 19:00:47 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2019 07:42:02 GMT"}, {"version": "v5", "created": "Tue, 18 Feb 2020 06:58:00 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Pu", "Shi", ""], ["Olshevsky", "Alex", ""], ["Paschalidis", "Ioannis Ch.", ""]]}]