[{"id": "2006.00064", "submitter": "Scott Schneider", "authors": "Scott Schneider, Xavier Guerin, Shaohan Hu and Kun-Lung Wu", "title": "A Cloud Native Platform for Stateful Streaming", "comments": "18 pages, 11 figures, submitted to OSDI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the architecture of a cloud native version of IBM Streams, with\nKubernetes as our target platform. Streams is a general purpose streaming\nsystem with its own platform for managing applications and the compute clusters\nthat execute those applications. Cloud native Streams replaces that platform\nwith Kubernetes. By using Kubernetes as its platform, Streams is able to\noffload job management, life cycle tracking, address translation, fault\ntolerance and scheduling. This offloading is possible because we define custom\nresources that natively integrate into Kubernetes, allowing Streams to use\nKubernetes' eventing system as its own. We use four design patterns to\nimplement our system: controllers, conductors, coordinators and causal chains.\nComposing controllers, conductors and coordinators allows us to build\ndeterministic state machines out of an asynchronous distributed system. The\nresulting implementation eliminates 75% of the original platform code. Our\nexperimental results show that the performance of Kubernetes is an adequate\nreplacement in most cases, but it has problems with oversubscription,\nnetworking latency, garbage collection and pod recovery.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:18:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Schneider", "Scott", ""], ["Guerin", "Xavier", ""], ["Hu", "Shaohan", ""], ["Wu", "Kun-Lung", ""]]}, {"id": "2006.00342", "submitter": "Hemant Mehta", "authors": "Hemant Mehta, Paul Harvey, Omer Rana, Rajkumar Buyya, Blesson Varghese", "title": "WattsApp: Power-Aware Container Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Containers are becoming a popular workload deployment mechanism in modern\ndistributed systems. However, there are limited software-based methods\n(hardware-based methods are expensive requiring hardware level changes) for\nobtaining the power consumed by containers for facilitating power-aware\ncontainer scheduling, an essential activity for efficient management of\ndistributed systems. This paper presents WattsApp, a tool underpinned by a six\nstep software-based method for power-aware container scheduling to minimize\npower cap violations on a server. The proposed method relies on a neural\nnetwork-based power estimation model and a power capped container scheduling\ntechnique. Experimental studies are pursued in a lab-based environment on 10\nbenchmarks deployed on Intel and ARM processors. The results highlight that the\npower estimation model has negligible overheads for data collection - nearly\n90% of all data samples can be estimated with less than a 10% error, and the\nMean Absolute Percentage Error (MAPE) is less than 6%. The power-aware\nscheduling of WattsApp is more effective than Intel's Running Power Average\nLimit (RAPL) based power capping for both single and multiple containers as it\ndoes not degrade the performance of all containers running on the server. The\nresults confirm the feasibility of WattsApp.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 19:44:56 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Mehta", "Hemant", ""], ["Harvey", "Paul", ""], ["Rana", "Omer", ""], ["Buyya", "Rajkumar", ""], ["Varghese", "Blesson", ""]]}, {"id": "2006.00441", "submitter": "Yawen Zhang", "authors": "Qinggang Zhou, Yawen Zhang, Pengcheng Li, Xiaoyong Liu, Jun Yang,\n  Runsheng Wang and Ru Huang", "title": "DaSGD: Squeezing SGD Parallelization Performance in Distributed Training\n  Using Delayed Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art deep learning algorithms rely on distributed training\nsystems to tackle the increasing sizes of models and training data sets.\nMinibatch stochastic gradient descent (SGD) algorithm requires workers to halt\nforward/back propagations, to wait for gradients aggregated from all workers,\nand to receive weight updates before the next batch of tasks. This synchronous\nexecution model exposes the overheads of gradient/weight communication among a\nlarge number of workers in a distributed training system. We propose a new SGD\nalgorithm, DaSGD (Local SGD with Delayed Averaging), which parallelizes SGD and\nforward/back propagations to hide 100% of the communication overhead. By\nadjusting the gradient update scheme, this algorithm uses hardware resources\nmore efficiently and reduces the reliance on the low-latency and\nhigh-throughput inter-connects. The theoretical analysis and the experimental\nresults show its convergence rate O(1/sqrt(K)), the same as SGD. The\nperformance evaluation demonstrates it enables a linear performance scale-up\nwith the cluster size.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 05:43:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhou", "Qinggang", ""], ["Zhang", "Yawen", ""], ["Li", "Pengcheng", ""], ["Liu", "Xiaoyong", ""], ["Yang", "Jun", ""], ["Wang", "Runsheng", ""], ["Huang", "Ru", ""]]}, {"id": "2006.00508", "submitter": "Prateek Sharma", "authors": "Alexander Fuerst, Ahmed Ali-Eldin, Prashant Shenoy, Prateek Sharma", "title": "Cloud-scale VM Deflation for Running Interactive Applications On\n  Transient Servers", "comments": "To appear at ACM HPDC 2020", "journal-ref": null, "doi": "10.1145/3369583.3392675", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient computing has become popular in public cloud environments for\nrunning delay-insensitive batch and data processing applications at low cost.\nSince transient cloud servers can be revoked at any time by the cloud provider,\nthey are considered unsuitable for running interactive application such as web\nservices. In this paper, we present VM deflation as an alternative mechanism to\nserver preemption for reclaiming resources from transient cloud servers under\nresource pressure. Using real traces from top-tier cloud providers, we show the\nfeasibility of using VM deflation as a resource reclamation mechanism for\ninteractive applications in public clouds. We show how current hypervisor\nmechanisms can be used to implement VM deflation and present cluster deflation\npolicies for resource management of transient and on-demand cloud VMs.\nExperimental evaluation of our deflation system on a Linux cluster shows that\nmicroservice-based applications can be deflated by up to 50\\% with negligible\nperformance overhead. Our cluster-level deflation policies allow overcommitment\nlevels as high as 50\\%, with less than a 1\\% decrease in application\nthroughput, and can enable cloud platforms to increase revenue by 30\\%.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 12:40:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Fuerst", "Alexander", ""], ["Ali-Eldin", "Ahmed", ""], ["Shenoy", "Prashant", ""], ["Sharma", "Prateek", ""]]}, {"id": "2006.00812", "submitter": "Linna Ruan", "authors": "Linna Ruan, Shaoyong Guo, Xuesong Qiu, Rajkumar Buyya", "title": "Fog Computing for Smart Grids: Challenges and Solutions", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart grids (SGs) enable integration of diverse power sources including\nrenewable energy resources. They can contribute to the reduction of harmful gas\nemission, and support two-way information flow to enhance energy efficiency,\nalong with small-scale Microgrids, acting as a promising solution to cope with\nenvironmental problems. However, with the emerging of mission-critical and\ndelay-sensitive applications, traditional Cloud-based data processing mode\nbecomes less satisfying. The use of Fog computing to empower the edge-side\nprocessing capability of Smart grid systems is considered as a potential\nsolution to address the problem. In this chapter, we aim to offer a\ncomprehensive analysis of application of Fog computing in Smart grids. We begin\nwith an overview of Smart grids and Fog computing. Then, by surveying the\nexisting research, we summarize the main Fog computing enabled Smart grid\napplications, key problems and the possible methods. We conclude the chapter by\ndiscussing the research challenges and future directions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:33:24 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 06:37:22 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 23:45:10 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ruan", "Linna", ""], ["Guo", "Shaoyong", ""], ["Qiu", "Xuesong", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2006.00819", "submitter": "Junchang Wang", "authors": "Junchang Wang, Xiong Fu, Fu Xiao, Chen Tian", "title": "DHash: Enabling Dynamic and Efficient Hash Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a specified average load factor, hash tables offer the appeal of\nconstant time lookup operations. However, hash tables could face severe hash\ncollisions because of malicious attacks, buggy applications, or even bursts of\nincoming data, compromising this practical advantage. In this paper, we present\nDHash, a hash table that overcomes this challenge by allowing programmers to\ndynamically change its hash function on the fly, without affecting other\nconcurrent operations such as lookup, insert, and delete. DHash is modular and\nallows programmers to select a variety of lock-free/wait-free set algorithms as\nthe implementation of hash table buckets. With this flexibility, they can make\ntrade-offs between the algorithm's progress guarantee, performance, and\nengineering efforts, and create DHash implementations that meet their\nrequirements best. Evaluations on three types of architectures show that DHash\nnoticeably outperforms other practical alternatives under heavy workloads. With\na load factor of 20, DHash outperforms the other three most widely used hash\ntables by factors of 1.4-2.0, and when the load factor increases to 200, DHash\nis 2.3-6.2 times faster.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:54:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Junchang", ""], ["Fu", "Xiong", ""], ["Xiao", "Fu", ""], ["Tian", "Chen", ""]]}, {"id": "2006.00917", "submitter": "Peter Hillmann", "authors": "Tobias Uhlig and Peter Hillmann and Oliver Rose", "title": "Evaluation of the general applicability of Dragoon for the k-center\n  problem", "comments": null, "journal-ref": "Winter Simulation Conference 2016", "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CC cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The k-center problem is a fundamental problem we often face when considering\ncomplex service systems. Typical challenges include the placement of warehouses\nin logistics or positioning of servers for content delivery networks. We\npreviously have proposed Dragoon as an effective algorithm to approach the\nk-center problem. This paper evaluates Dragoon with a focus on potential worst\ncase behavior in comparison to other techniques. We use an evolutionary\nalgorithm to generate instances of the k-center problem that are especially\nchallenging for Dragoon. Ultimately, our experiments confirm the previous good\nresults of Dragoon, however, we also can reliably find scenarios where it is\nclearly outperformed by other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:54:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Uhlig", "Tobias", ""], ["Hillmann", "Peter", ""], ["Rose", "Oliver", ""]]}, {"id": "2006.00928", "submitter": "Sebastian M\\\"uller", "authors": "Sebastian M\\\"uller, Andreas Penzkofer, Bartosz Ku\\'smierz, Darcy\n  Camargo, William J. Buchanan", "title": "Fast Probabilistic Consensus with Weighted Votes", "comments": null, "journal-ref": null, "doi": null, "report-no": "Conference: FTC 2020 - Future Technologies Conference 2020", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast probabilistic consensus (FPC) is a voting consensus protocol that is\nrobust and efficient in Byzantine infrastructure. We propose an adaption of the\nFPC to a setting where the voting power is proportional to the nodes\nreputations. We model the reputation using a Zipf law and show using\nsimulations that the performance of the protocol in Byzantine infrastructure\nincreases with the Zipf exponent. Moreover, we propose several improvements of\nthe FPC that decrease the failure rates significantly and allow the protocol to\nwithstand adversaries with higher weight. We distinguish between cautious and\nberserk strategies of the adversaries and propose an efficient method to detect\nthe more harmful berserk strategies. Our study refers at several points to a\nspecific implementation of the IOTA protocol, but the principal results hold\nfor general implementations of reputation models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:23:37 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["M\u00fcller", "Sebastian", ""], ["Penzkofer", "Andreas", ""], ["Ku\u015bmierz", "Bartosz", ""], ["Camargo", "Darcy", ""], ["Buchanan", "William J.", ""]]}, {"id": "2006.01029", "submitter": "Ehud Shapiro", "authors": "Ouri Poupko, Ehud Shapiro and Nimrod Talmon", "title": "Fault-Tolerant Distributed-Ledger Implementation of Digital Social\n  Contracts", "comments": "arXiv admin note: text overlap with arXiv:2005.06261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A companion paper defined the notion of digital social contracts, presented a\ndesign for a social-contracts programming language, and demonstrated its\npotential utility via example social contracts. The envisioned setup consists\nof people with genuine identifiers, which are unique and singular cryptographic\nkey pairs, that operate software agents thus identified on their mobile device.\nThe abstract model of digital social contracts consists of a transition system\nspecifying concurrent, non-deterministic asynchronous agents that operate on a\nshared ledger by performing digital speech acts, which are\ncryptographically-signed sequentially-indexed digital actions. Here, we address\nthe distributed-ledger implementation of digital social contracts in the\npresence of faulty agents: we present a design of a fault-tolerant\ndistributed-ledger transition system and show that it implements the abstract\nshared-ledger model of digital social contracts, and discuss its resilience to\nfaulty agents. The result is a novel ledger architecture that is distributed\nwith a blockchain-per-person (as opposed to centralized with one blockchain for\nall), partially-ordered (as opposed to totally-ordered), locally-replicated (as\nopposed to globally-replicated), asynchronous (as opposed to\nglobally-synchronized), peer-to-peer with each agent being both an actor and a\nvalidator (as opposed to having dedicated miners, validators, and clients),\nenvironmentally-friendly (as opposed to the environmentally-harmful\nProof-of-Work), self-sufficient (as opposed to the energy-hogging Proof-of-Work\nor capital-hogging Proof-of-Stake) and egalitarian (as opposed to the\nplutocratic Proof-of-Work and Proof-of-Stake).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:53:25 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 16:02:26 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 07:56:36 GMT"}, {"version": "v4", "created": "Tue, 21 Jul 2020 13:57:57 GMT"}, {"version": "v5", "created": "Sat, 19 Sep 2020 07:24:08 GMT"}, {"version": "v6", "created": "Thu, 19 Nov 2020 22:40:42 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Poupko", "Ouri", ""], ["Shapiro", "Ehud", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2006.01048", "submitter": "Raz Saremi", "authors": "Jordan Urbaczek, Razieh Saremi, Mostaan Lotfalian Saremi, and Julian\n  Togelius", "title": "Greedy Scheduling: A Neural Network Method to Reduce Task Failure in\n  Software Crowdsourcing", "comments": "7 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Highly dynamic and competitive crowdsourcing software development\n(CSD) marketplaces may experience task failure due to unforeseen reasons, such\nas increased competition over shared supplier resources, or uncertainty\nassociated with a dynamic worker supply. Existing analysis reveals an average\ntask failure ratio of 15.7\\% in software crowdsourcing markets.\n  Goal: The objective of this study is to provide a task scheduling\nrecommendation model for software crowdsourcing platforms in order to improve\nthe success and efficiency of software crowdsourcing.\n  Method: We propose a task scheduling method based on neural networks, and\ndevelop a system that can predict and analyze task failure probability upon\narrival. More specifically, the model uses a range of input variables,\nincluding the number of open tasks in the platform, the average task similarity\nbetween arriving tasks and open tasks, the winner's monetary prize, and task\nduration, to predict the probability of task failure on the planned arrival\ndate and two surplus days. This prediction will offer the recommended day\nassociated with the lowest task failure probability to post the task. The\nproposed model is based on the workflow and data of Topcoder, one of the\nprimary software crowdsourcing platforms.\n  Results: We present a model that suggests the best recommended arrival dates\nfor any task in the project with surplus of two days per task in the project.\nThe model on average provided 4\\% lower failure ratio per project.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 01:42:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 22:12:06 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 02:24:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Urbaczek", "Jordan", ""], ["Saremi", "Razieh", ""], ["Saremi", "Mostaan Lotfalian", ""], ["Togelius", "Julian", ""]]}, {"id": "2006.01072", "submitter": "Chenxing Li", "authors": "Chenxing Li, Fan Long, Guang Yang", "title": "GHAST: Breaking Confirmation Delay Barrier in Nakamoto Consensus via\n  Adaptive Weighted Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Initiated from Nakamoto's Bitcoin system, blockchain technology has\ndemonstrated great capability of building secure consensus among decentralized\nparties at Internet-scale, i.e., without relying on any centralized trusted\nparty. Nowadays, blockchain systems find applications in various fields. But\nthe performance is increasingly becoming a bottleneck, especially when\npermissionless participation is retained for full decentralization.\n  In this work, we present a new consensus protocol named GHAST (Greedy\nHeaviest Adaptive Sub-Tree) which organizes blocks in a Tree-Graph structure\n(i.e., a directed acyclic graph (DAG) with a tree embedded) that allows fast\nand concurrent block generation. GHAST protocol simultaneously achieves a\nlogarithmically bounded liveness guarantee and low confirmation latency. More\nspecifically, for maximum latency $d$ and adversarial computing power bounded\naway from 50\\%, GHAST guarantees confirmation with confidence $\\ge\n1-\\varepsilon$ after a time period of $O(d\\cdot \\log(1/\\varepsilon))$. When\nthere is no observable attack, GHAST only needs $3d$ time to achieve\nconfirmation at the same confidence level as six-block-confirmation in Bitcoin,\nwhile it takes roughly $360d$ in Bitcoin.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:53:29 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Li", "Chenxing", ""], ["Long", "Fan", ""], ["Yang", "Guang", ""]]}, {"id": "2006.01128", "submitter": "J\\'anos V\\'egh", "authors": "J\\'anos V\\'egh", "title": "Introducing temporal behavior to computing science", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The abstraction introduced by von Neumann correctly reflected the state of\nthe art 70 years ago.\n  Although it omitted data transmission time between components of the\ncomputer, it served as an excellent base for classic computing for decades.\n  Modern computer components and architectures, however, require to consider\ntheir temporal behavior: data transmission time in contemporary systems may be\nhigher than their processing time.\n  Using the classic paradigm leaves some issues unexplained, from enormously\nhigh power consumption to days-long training of artificial neural networks to\nfailures of some cutting-edge supercomputer projects.\n  The paper introduces the up to now missing timely behavior (a temporal logic)\ninto computing, while keeps the solid computing science base.\n  The careful analysis discovers that with considering the timely behavior of\ncomponents and architectural principles, the mystic issues have a trivial\nexplanation.\n  Some classic design principles must be revised, and the temporal logic\nenables us to design a more powerful and efficient computing.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:21:29 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 12:26:49 GMT"}, {"version": "v3", "created": "Sat, 11 Jul 2020 15:00:53 GMT"}, {"version": "v4", "created": "Sat, 26 Sep 2020 18:58:45 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["V\u00e9gh", "J\u00e1nos", ""]]}, {"id": "2006.01251", "submitter": "Pedro Garc\\'ia-L\\'opez", "authors": "Pedro Garc\\'ia-L\\'opez, Aleksander Slominski, Simon Shillaker, Michael\n  Behrendt, Barnard Metzler", "title": "Serverless End Game: Disaggregation enabling Transparency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, the distributed systems community has struggled to smooth the\ntransition from local to remote computing. Transparency means concealing the\ncomplexities of distributed programming like remote locations, failures or\nscaling. For us, full transparency implies that we can compile, debug and run\nunmodified single-machine code over effectively unlimited compute, storage, and\nmemory resources. We elaborate in this article why resource disaggregation in\nserverless computing is the definitive catalyst to enable full transparency in\nthe Cloud. We demonstrate with two experiments that we can achieve transparency\ntoday over disaggregated serverless resources and obtain comparable performance\nto local executions. We also show that locality cannot be neglected for many\nproblems and we present five open research challenges: granular middleware and\nlocality, memory disaggregation, virtualization, elastic programming models,\nand optimized deployment. If full transparency is possible, who needs explicit\nuse of middleware if you can treat remote entities as local ones? Can we close\nthe curtains of distributed systems complexity for the majority of users?\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 20:41:12 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Garc\u00eda-L\u00f3pez", "Pedro", ""], ["Slominski", "Aleksander", ""], ["Shillaker", "Simon", ""], ["Behrendt", "Michael", ""], ["Metzler", "Barnard", ""]]}, {"id": "2006.01279", "submitter": "Fubao Wu", "authors": "Fubao Wu, Lixin Gao", "title": "Scalable Top-k Query on Information Networks with Hierarchical\n  Inheritance Relations", "comments": "18 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph query, pattern mining and knowledge discovery become challenging on\nlarge-scale heterogeneous information networks (HINs). State-of-the-art\ntechniques involving path propagation mainly focus on the inference on nodes\nlabels and neighborhood structures. However, entity links in the real world\nalso contain rich hierarchical inheritance relations. For example, the\nvulnerability of a product version is likely to be inherited from its older\nversions. Taking advantage of the hierarchical inheritances can potentially\nimprove the quality of query results. Motivated by this, we explore\nhierarchical inheritance relations between entities and formulate the problem\nof graph query on HINs with hierarchical inheritance relations. We propose a\ngraph query search algorithm by decomposing the original query graph into\nmultiple star queries and apply a star query algorithm to each star query.\nFurther candidates from each star query result are then constructed for final\ntop-k query answers to the original query. To efficiently obtain the graph\nquery result from a large-scale HIN, we design a bound-based pruning technique\nby using uniform cost search to prune search spaces. We implement our algorithm\nin GraphX to test the effectiveness and efficiency on synthetic and real-world\ndatasets. Compared with two common graph query algorithms, our algorithm can\neffectively obtain more accurate results and competitive performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 21:35:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wu", "Fubao", ""], ["Gao", "Lixin", ""]]}, {"id": "2006.01311", "submitter": "Hamouma Moumen", "authors": "Badreddine Benreguia and Hamouma Moumen", "title": "Self-stabilizing Algorithm for Minimal $\\alpha$-Dominating Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-stabilizing algorithm for the minimal $\\alpha$-dominating set is\nproposed in this paper. The $\\alpha$-domination parameter has not used before\nin self-stabilization paradigm. Using an arbitrary graph with $n$ nodes and $m$\nedges, the proposed algorithm converges in $O(nm)$ moves under distributed\ndaemon. Simulation tests and mathematical proofs show the efficiency of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:15:21 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Benreguia", "Badreddine", ""], ["Moumen", "Hamouma", ""]]}, {"id": "2006.01318", "submitter": "Tarek Elgamal", "authors": "Tarek Elgamal, Shu Shi, Varun Gupta, Rittwik Jana, Klara Nahrstedt", "title": "SiEVE: Semantically Encoded Video Analytics on Edge and Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision and neural networks have made it possible\nfor more surveillance videos to be automatically searched and analyzed by\nalgorithms rather than humans. This happened in parallel with advances in edge\ncomputing where videos are analyzed over hierarchical clusters that contain\nedge devices, close to the video source. However, the current video analysis\npipeline has several disadvantages when dealing with such advances. For\nexample, video encoders have been designed for a long time to please human\nviewers and be agnostic of the downstream analysis task (e.g., object\ndetection). Moreover, most of the video analytics systems leverage 2-tier\narchitecture where the encoded video is sent to either a remote cloud or a\nprivate edge server but does not efficiently leverage both of them. In response\nto these advances, we present SIEVE, a 3-tier video analytics system to reduce\nthe latency and increase the throughput of analytics over video streams. In\nSIEVE, we present a novel technique to detect objects in compressed video\nstreams. We refer to this technique as semantic video encoding because it\nallows video encoders to be aware of the semantics of the downstream task\n(e.g., object detection). Our results show that by leveraging semantic video\nencoding, we achieve close to 100% object detection accuracy with decompressing\nonly 3.5% of the video frames which results in more than 100x speedup compared\nto classical approaches that decompress every video frame.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:44:34 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Elgamal", "Tarek", ""], ["Shi", "Shu", ""], ["Gupta", "Varun", ""], ["Jana", "Rittwik", ""], ["Nahrstedt", "Klara", ""]]}, {"id": "2006.01331", "submitter": "Prasanth Chatarasi", "authors": "Prasanth Chatarasi, Stephen Neuendorffer, Samuel Bayliss, Kees\n  Vissers, Vivek Sarkar", "title": "Vyasa: A High-Performance Vectorizing Compiler for Tensor Convolutions\n  on the Xilinx AI Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Xilinx's AI Engine is a recent industry example of energy-efficient vector\nprocessing that includes novel support for 2D SIMD datapaths and shuffle\ninterconnection network. The current approach to programming the AI Engine\nrelies on a C/C++ API for vector intrinsics. While an advance over\nassembly-level programming, it requires the programmer to specify a number of\nlow-level operations based on detailed knowledge of the hardware. To address\nthese challenges, we introduce Vyasa, a new programming system that extends the\nHalide DSL compiler to automatically generate code for the AI Engine. We\nevaluated Vyasa on 36 CONV2D and 6 CONV3D workloads, and achieved geometric\nmeans of 7.6 and 23.3 MACs/cycle for 32-bit and 16-bit operands (which\nrepresent 95.9% and 72.8% of the peak performance respectively). For 4 of these\nworkloads for which expert-written codes were available to us, Vyasa\ndemonstrated a geometric mean performance improvement of 1.10x with 50x smaller\ncode relative to the expert-written codes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 01:25:49 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Chatarasi", "Prasanth", ""], ["Neuendorffer", "Stephen", ""], ["Bayliss", "Samuel", ""], ["Vissers", "Kees", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2006.01354", "submitter": "Tan N. Le", "authors": "Tan N. Le, Zhenhua Liu", "title": "Flex: Closing the Gaps between Usage and Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centers are giant factories of Internet data and services. Worldwide\ndata centers consume energy and emit emissions more than airline industry.\nUnfortunately, most of data centers are significantly underutilized. One of the\nmajor reasons is the big gaps between the real usage and the provisioned\nresources because users tend to over-estimate their demand and data center\noperators often rely on users' requests for resource allocation. In this paper,\nwe first conduct an in-depth analysis of a Google cluster trace to unveil the\nroot causes for low utilization and highlight the great potential to improve\nit. We then developed an online resource manager Flex to maximize the cluster\nutilization while satisfying the Quality of Service (QoS). Large-scale\nevaluations based on real-world traces show that Flex admits up to 1.74x more\nrequests and 1.6x higher utilization compared to tradition schedulers while\nmaintaining the QoS.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:41:39 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Le", "Tan N.", ""], ["Liu", "Zhenhua", ""]]}, {"id": "2006.01402", "submitter": "Maher Kachmar", "authors": "Maher Kachmar, David Kaeli", "title": "A Smart Background Scheduler for Storage Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's enterprise storage systems, supported data services such as\nsnapshot delete or drive rebuild can cause tremendous performance interference\nif executed inline along with heavy foreground IO, often leading to missing\nSLOs (Service Level Objectives). Typical storage system applications such as\nweb or VDI (Virtual Desktop Infrastructure) follow a repetitive high/low\nworkload pattern that can be learned and forecasted. We propose a\npriority-based background scheduler that learns this repetitive pattern and\nallows storage systems to maintain peak performance and in turn meet service\nlevel objectives (SLOs) while supporting a number of data services. When\nforeground IO demand intensifies, system resources are dedicated to service\nforeground IO requests and any background processing that can be deferred are\nrecorded to be processed in future idle cycles as long as forecast shows that\nstorage pool has remaining capacity. The smart background scheduler adopts a\nresource partitioning model that allows both foreground and background IO to\nexecute together as long as foreground IOs are not impacted where the scheduler\nharness any free cycle to clear background debt. Using traces from VDI\napplication, we show how our technique surpasses a method that statically limit\nthe deferred background debt and improve SLO violations from 54.6% when using a\nfixed background debt watermark to merely a 6.2% if dynamically set by our\nsmart background scheduler.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 05:39:56 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kachmar", "Maher", ""], ["Kaeli", "David", ""]]}, {"id": "2006.01514", "submitter": "Panagiotis Gkikopoulos", "authors": "Panagiotis Gkikopoulos, Josef Spillner and Valerio Schiavoni", "title": "Monitoring Data Distribution and Exploitation in a Global-Scale\n  Microservice Artefact Observatory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusable microservice artefacts are often deployed as black or grey boxes,\nwith little concern for their properties and quality, beyond a syntactical\ninterface description. This leads application developers to chaotic and\nopportunistic assumptions about how a composite application will behave in the\nreal world. Systematically analyzing and tracking these publicly available\nartefacts will grant much needed predictability to microservice-based\ndeployments. By establishing a distributed observatory and knowledge base, it\nis possible to track microservice repositories and analyze the artefacts\nreliably, and provide insights on their properties and quality to developers\nand researchers alike. This position paper argues for a federated research\ninfrastructure with consensus voting among participants to establish and\npreserve ground truth about the insights.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 10:46:58 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Gkikopoulos", "Panagiotis", ""], ["Spillner", "Josef", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2006.01598", "submitter": "Peter Hillmann", "authors": "Peter Hillmann and Tobias Uhlig and Gabi Dreo Rodosek and Oliver Rose", "title": "A Novel Approach to Solve K-Center Problems with Geographical Placement", "comments": "International Conference on Service Operations and Logistics, and\n  Informatics 2015. arXiv admin note: text overlap with arXiv:2005.13905", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The facility location problem is a well-known challenge in logistics that is\nproven to be NP-hard. In this paper we specifically simulate the geographical\nplacement of facilities to provide adequate service to customers. Determining\nreasonable center locations is an important challenge for a management since it\ndirectly effects future service costs. Generally, the objective is to place the\ncentral nodes such that all customers have convenient access to them. We\nanalyze the problem and compare different placement strategies and evaluate the\nnumber of required centers. We use several existing approaches and propose a\nnew heuristic for the problem. For our experiments we consider various\nscenarios and employ simulation to evaluate the performance of the optimization\nalgorithms. Our new optimization approach shows a significant improvement. The\npresented results are generally applicable to many domains, e.g., the placement\nof military bases, the planning of content delivery networks, or the placement\nof warehouses.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:01:05 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Hillmann", "Peter", ""], ["Uhlig", "Tobias", ""], ["Rodosek", "Gabi Dreo", ""], ["Rose", "Oliver", ""]]}, {"id": "2006.01758", "submitter": "Peter Vitols", "authors": "Henri Aare, Peter Vitols", "title": "The Ritva Blockchain: Enabling Confidential Transactions at Scale", "comments": "The paper has been updated to address the editorial comments. arXiv\n  admin note: substantial text overlap with arXiv:1905.06460, arXiv:1811.12628\n  by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed ledger technology has been widely hailed as the break-through\ntechnology. It has realised a great number of application scenarios, and\nimproved workflow of many domains. Nonetheless, there remain a few major\nconcerns in adopting and deploying the distributed ledger technology at scale.\nIn this white paper, we tackle two of them, namely the throughput scalability\nand confidentiality protection for transactions. We learn from the existing\nbody of research, and build a scale-out blockchain platform that champions\nprivacy called RVChain. RVChain takes advantage of trusted execution\nenvironment to offer confidentiality protection for transactions, and scales\nthe throughput of the network in proportion with the number of network\nparticipants by supporting parallel shadow chains.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 01:48:11 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Aare", "Henri", ""], ["Vitols", "Peter", ""]]}, {"id": "2006.01816", "submitter": "Baturalp Buyukates", "authors": "Emre Ozfatura and Baturalp Buyukates and Deniz Gunduz and Sennur\n  Ulukus", "title": "Age-Based Coded Computation for Bias Reduction in Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded computation can be used to speed up distributed learning in the\npresence of straggling workers. Partial recovery of the gradient vector can\nfurther reduce the computation time at each iteration; however, this can result\nin biased estimators, which may slow down convergence, or even cause\ndivergence. Estimator bias will be particularly prevalent when the straggling\nbehavior is correlated over time, which results in the gradient estimators\nbeing dominated by a few fast servers. To mitigate biased estimators, we design\na $timely$ dynamic encoding framework for partial recovery that includes an\nordering operator that changes the codewords and computation orders at workers\nover time. To regulate the recovery frequencies, we adopt an $age$ metric in\nthe design of the dynamic encoding scheme. We show through numerical results\nthat the proposed dynamic encoding strategy increases the timeliness of the\nrecovered computations, which as a result, reduces the bias in model updates,\nand accelerates the convergence compared to the conventional static partial\nrecovery schemes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:51:11 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ozfatura", "Emre", ""], ["Buyukates", "Baturalp", ""], ["Gunduz", "Deniz", ""], ["Ulukus", "Sennur", ""]]}, {"id": "2006.01866", "submitter": "Alexander Engelmann", "authors": "Alexander Engelmann, Yuning Jiang, Henrieke Benner, Ruchuan Ou, Boris\n  Houska, Timm Faulwasser", "title": "ALADIN-$\\alpha$ -- An open-source MATLAB toolbox for distributed\n  non-convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an open-source software for distributed and\ndecentralized non-convex optimization named ALADIN-$\\alpha$. ALADIN-$\\alpha$ is\na MATLAB implementation of the Augmented Lagrangian Alternating Direction\nInexact Newton (ALADIN) algorithm, which is tailored towards rapid prototyping\nfor non-convex distributed optimization. An improved version of the recently\nproposed bi-level variant of ALADIN is included enabling decentralized\nnon-convex optimization. A collection of application examples from different\napplications fields including chemical engineering, robotics, and power systems\nunderpins the application potential of ALADIN-$\\alpha$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:31:07 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Engelmann", "Alexander", ""], ["Jiang", "Yuning", ""], ["Benner", "Henrieke", ""], ["Ou", "Ruchuan", ""], ["Houska", "Boris", ""], ["Faulwasser", "Timm", ""]]}, {"id": "2006.01885", "submitter": "Eli Goldweber", "authors": "Eli Goldweber, Nuda Zhang, and Manos Kapritsos", "title": "On the Significance of Consecutive Ballots in Paxos", "comments": "19 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper we examine the Paxos protocol and demonstrate how the discrete\nnumbering of ballots can be leveraged to weaken the conditions for learning.\nSpecifically, we define the notion of consecutive ballots and use this to\ndefine Consecutive Quorums. Consecutive Quorums weakens the learning criterion\nsuch that a learner does not need matching $accept$ messages sent in the $same\n\\; ballot$ from a majority of acceptors to learn a value. We prove that this\nmodification preserves the original safety and liveness guarantees of Paxos. We\ndefine $Consecutive \\; Paxos$ which encapsulates the properties of discrete\nconsecutive ballots. To establish the correctness of these results, we, in\naddition to a paper proof, formally verify the correctness of a State Machine\nReplication Library built on top of an optimized version of Multi-Paxos\nmodified to reflect $Consecutive \\; Paxos$.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:08:46 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Goldweber", "Eli", ""], ["Zhang", "Nuda", ""], ["Kapritsos", "Manos", ""]]}, {"id": "2006.01957", "submitter": "Muhammad Hilman", "authors": "Muhammad H. Hilman, Maria A. Rodriguez, Rajkumar Buyya", "title": "Workflow-as-a-Service Cloud Platform and Deployment of Bioinformatics\n  Workflow Applications", "comments": "submitted as book chapter to Knowledge Management in Development of\n  Data-Intensive Software Systems (KMDDIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow management systems (WMS) support the composition and deployment of\nworkflow-oriented applications in distributed computing environments. They hide\nthe complexity of managing large-scale applications, which includes the\ncontrolling data pipelining between tasks, ensuring the application's\nexecution, and orchestrating the distributed computational resources to get a\nreasonable processing time. With the increasing trends of scientific workflow\nadoption, the demand to deploy them using a third-party service begins to\nincrease. Workflow-as-a-service (WaaS) is a term representing the platform that\nserves the users who require to deploy their workflow applications on\nthird-party cloud-managed services. This concept drives the existing WMS\ntechnology to evolve towards the development of the WaaS cloud platform. Based\non this requirement, we extend CloudBus WMS functionality to handle the\nworkload of multiple workflows and develop the WaaS cloud platform prototype.\nWe implemented the Elastic Budget-constrained resource Provisioning and\nScheduling algorithm for Multiple workflows (EBPSM) algorithm that is capable\nof scheduling multiple workflows and evaluated the platform using two\nbioinformatics workflows. Our experimental results show that the platform is\ncapable of efficiently handling multiple workflows execution and gaining its\npurpose to minimize the makespan while meeting the budget.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:39:55 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Hilman", "Muhammad H.", ""], ["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2006.02055", "submitter": "Mohsen Amini Salehi", "authors": "Davood Ghatreh Samani, Chavit Denninnart, Josef Bacik, Mohsen Amini\n  Salehi", "title": "The Art of CPU-Pinning: Evaluating and Improving the Performance of\n  Virtualization and Containerization Platforms", "comments": null, "journal-ref": "The 49th International Conference on Parallel Processing (ICPP\n  2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers offer a variety of execution platforms in form of bare-metal,\nVM, and containers. However, due to the pros and cons of each execution\nplatform, choosing the appropriate platform for a specific cloud-based\napplication has become a challenge for solution architects. The possibility to\ncombine these platforms (e.g. deploying containers within VMs) offers new\ncapacities that makes the challenge even further complicated. However, there is\na little study in the literature on the pros and cons of deploying different\napplication types on various execution platforms. In particular, evaluation of\ndiverse hardware configurations and different CPU provisioning methods, such as\nCPU pinning, have not been sufficiently studied in the literature. In this\nwork, the performance overhead of container, VM, and bare-metal execution\nplatforms are measured and analyzed for four categories of real-world\napplications, namely video processing, parallel processing (MPI), web\nprocessing, and No-SQL, respectively representing CPU intensive, parallel\nprocessing, and two IO intensive processes. Our analyses reveal a set of\ninteresting and sometimes counterintuitive findings that can be used as best\npractices by the solution architects to efficiently deploy cloud-based\napplications. Here are some notable mentions: (A) Under specific circumstances,\ncontainers can impose a higher overhead than VMs; (B) Containers on top of VMs\ncan mitigate the overhead of VMs for certain applications; (C) Containers with\na large number of cores impose a lower overhead than those with a few cores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 05:47:14 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Samani", "Davood Ghatreh", ""], ["Denninnart", "Chavit", ""], ["Bacik", "Josef", ""], ["Salehi", "Mohsen Amini", ""]]}, {"id": "2006.02085", "submitter": "Amit Saha", "authors": "Johnu George, Ce Gao, Richard Liu, Hou Gang Liu, Yuan Tang, Ramdoot\n  Pydipaty, Amit Kumar Saha", "title": "A Scalable and Cloud-Native Hyperparameter Tuning System", "comments": "Fixed some typos, no content change at all from previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Katib: a scalable, cloud-native, and\nproduction-ready hyperparameter tuning system that is agnostic of the\nunderlying machine learning framework. Though there are multiple hyperparameter\ntuning systems available, this is the first one that caters to the needs of\nboth users and administrators of the system. We present the motivation and\ndesign of the system and contrast it with existing hyperparameter tuning\nsystems, especially in terms of multi-tenancy, scalability, fault-tolerance,\nand extensibility. It can be deployed on local machines, or hosted as a service\nin on-premise data centers, or in private/public clouds. We demonstrate the\nadvantage of our system using experimental results as well as real-world,\nproduction use cases. Katib has active contributors from multiple companies and\nis open-sourced at \\emph{https://github.com/kubeflow/katib} under the Apache\n2.0 license.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:44:25 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:26:45 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["George", "Johnu", ""], ["Gao", "Ce", ""], ["Liu", "Richard", ""], ["Liu", "Hou Gang", ""], ["Tang", "Yuan", ""], ["Pydipaty", "Ramdoot", ""], ["Saha", "Amit Kumar", ""]]}, {"id": "2006.02155", "submitter": "Brian Kroth", "authors": "Carlo Curino, Neha Godwal, Brian Kroth, Sergiy Kuryata, Greg Lapinski,\n  Siqi Liu, Slava Oks, Olga Poppe, Adam Smiechowski, Ed Thayer, Markus Weimer,\n  Yiwen Zhu", "title": "MLOS: An Infrastructure for Automated Software Performance Engineering", "comments": "4 pages, DEEM 2020", "journal-ref": null, "doi": "10.1145/3399579.3399927", "report-no": null, "categories": "cs.DC cs.DB cs.LG cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing modern systems software is a complex task that combines business\nlogic programming and Software Performance Engineering (SPE). The later is an\nexperimental and labor-intensive activity focused on optimizing the system for\na given hardware, software, and workload (hw/sw/wl) context.\n  Today's SPE is performed during build/release phases by specialized teams,\nand cursed by: 1) lack of standardized and automated tools, 2) significant\nrepeated work as hw/sw/wl context changes, 3) fragility induced by a\n\"one-size-fit-all\" tuning (where improvements on one workload or component may\nimpact others). The net result: despite costly investments, system software is\noften outside its optimal operating point - anecdotally leaving 30% to 40% of\nperformance on the table.\n  The recent developments in Data Science (DS) hints at an opportunity:\ncombining DS tooling and methodologies with a new developer experience to\ntransform the practice of SPE. In this paper we present: MLOS, an ML-powered\ninfrastructure and methodology to democratize and automate Software Performance\nEngineering. MLOS enables continuous, instance-level, robust, and trackable\nsystems optimization. MLOS is being developed and employed within Microsoft to\noptimize SQL Server performance. Early results indicated that component-level\noptimizations can lead to 20%-90% improvements when custom-tuning for a\nspecific hw/sw/wl, hinting at a significant opportunity. However, several\nresearch challenges remain that will require community involvement. To this\nend, we are in the process of open-sourcing the MLOS core infrastructure, and\nwe are engaging with academic institutions to create an educational program\naround Software 2.0 and MLOS ideas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 22:38:30 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 11:10:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Curino", "Carlo", ""], ["Godwal", "Neha", ""], ["Kroth", "Brian", ""], ["Kuryata", "Sergiy", ""], ["Lapinski", "Greg", ""], ["Liu", "Siqi", ""], ["Oks", "Slava", ""], ["Poppe", "Olga", ""], ["Smiechowski", "Adam", ""], ["Thayer", "Ed", ""], ["Weimer", "Markus", ""], ["Zhu", "Yiwen", ""]]}, {"id": "2006.02219", "submitter": "Jonas Ellert", "authors": "Jonas Ellert, Johannes Fischer, Nodari Sitchinava", "title": "LCP-Aware Parallel String Sorting", "comments": "Accepted at Euro-Par 2020 and to be published by Springer as part of\n  the conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When lexicographically sorting strings, it is not always necessary to inspect\nall symbols. For example, the lexicographical rank of \"europar\" amongst the\nstrings \"eureka\", \"eurasia\", and \"excells\" only depends on its so called\nrelevant prefix \"euro\". The distinguishing prefix size $D$ of a set of strings\nis the number of symbols that actually need to be inspected to establish the\nlexicographical ordering of all strings. Efficient string sorters should be\n$D$-aware, i.e. their complexity should depend on $D$ rather than on the total\nnumber $N$ of all symbols in all strings. While there are many $D$-aware\nsorters in the sequential setting, there appear to be no such results in the\nPRAM model. We propose a framework yielding a $D$-aware modification of any\nexisting PRAM string sorter. The derived algorithms are work-optimal with\nrespect to their original counterpart: If the original algorithm requires\n$O(w(N))$ work, the derived one requires $O(w(D))$ work. The execution time\nincreases only by a small factor that is logarithmic in the length of the\nlongest relevant prefix. Our framework universally works for deterministic and\nrandomized algorithms in all variations of the PRAM model, such that future\nimprovements in ($D$-unaware) parallel string sorting will directly result in\nimprovements in $D$-aware parallel string sorting.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 12:30:53 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Ellert", "Jonas", ""], ["Fischer", "Johannes", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "2006.02230", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Gagandeep\n  Goyal, Ramakrishna Upadrasta, Bharat Kaul", "title": "PolyDL: Polyhedral Optimizations for Creation of High Performance DL\n  primitives", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.02145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have revolutionized many aspects of our lives.\nThe use of DNNs is becoming ubiquitous including in softwares for image\nrecognition, speech recognition, speech synthesis, language translation, to\nname a few. he training of DNN architectures however is computationally\nexpensive. Once the model is created, its use in the intended application - the\ninference task, is computationally heavy too and the inference needs to be fast\nfor real time use. For obtaining high performance today, the code of Deep\nLearning (DL) primitives optimized for specific architectures by expert\nprogrammers exposed via libraries is the norm. However, given the constant\nemergence of new DNN architectures, creating hand optimized code is expensive,\nslow and is not scalable.\n  To address this performance-productivity challenge, in this paper we present\ncompiler algorithms to automatically generate high performance implementations\nof DL primitives that closely match the performance of hand optimized\nlibraries. We develop novel data reuse analysis algorithms using the polyhedral\nmodel to derive efficient execution schedules automatically. In addition,\nbecause most DL primitives use some variant of matrix multiplication at their\ncore, we develop a flexible framework where it is possible to plug in library\nimplementations of the same in lieu of a subset of the loops. We show that such\na hybrid compiler plus a minimal library-use approach results in\nstate-of-the-art performance. We develop compiler algorithms to also perform\noperator fusions that reduce data movement through the memory hierarchy of the\ncomputer system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:44:09 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 15:43:42 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Heinecke", "Alexander", ""], ["Avancha", "Sasikanth", ""], ["Goyal", "Gagandeep", ""], ["Upadrasta", "Ramakrishna", ""], ["Kaul", "Bharat", ""]]}, {"id": "2006.02318", "submitter": "Amir Behrouzi-Far", "authors": "Amir Behrouzi-Far and Emina Soljanin", "title": "Efficient Replication for Straggler Mitigation in Distributed Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Master-worker distributed computing systems use task replication in order to\nmitigate the effect of slow workers, known as stragglers. Tasks are grouped\ninto batches and assigned to one or more workers for execution. We first\nconsider the case when the batches do not overlap and, using the results from\nmajorization theory, show that, for a general class of workers' service time\ndistributions, a balanced assignment of batches to workers minimizes the\naverage job compute time. We next show that this balanced assignment of\nnon-overlapping batches achieves lower average job compute time compared to the\noverlapping schemes proposed in the literature. Furthermore, we derive the\noptimum redundancy level as a function of the service time distribution at\nworkers. We show that the redundancy level that minimizes average job compute\ntime is not necessarily the same as the redundancy level that maximizes the\npredictability of job compute time, and thus there exists a trade-off between\noptimizing the two metrics. Finally, by running experiments on Google cluster\ntraces, we observe that redundancy can reduce the compute time of the jobs in\nGoogle clusters by an order of magnitude, and that the optimum level of\nredundancy depends on the distribution of tasks' service time.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:05:04 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 15:42:30 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Behrouzi-Far", "Amir", ""], ["Soljanin", "Emina", ""]]}, {"id": "2006.02368", "submitter": "Hayk Saribekyan", "authors": "George Giakkoupis, Frederik Mallmann-Trenn, Hayk Saribekyan", "title": "How to Spread a Rumor: Call Your Neighbors or Take a Walk?", "comments": "Full version of an article that appeared at the 2019 ACM Symposium on\n  Principles of Distributed Computing; 33 pages; 1 figure", "journal-ref": null, "doi": "10.1145/3293611.3331622", "report-no": null, "categories": "cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of randomized information dissemination in networks. We\ncompare the now standard PUSH-PULL protocol, with agent-based alternatives\nwhere information is disseminated by a collection of agents performing\nindependent random walks. In the VISIT-EXCHANGE protocol, both nodes and agents\nstore information, and each time an agent visits a node, the two exchange all\nthe information they have. In the MEET-EXCHANGE protocol, only the agents store\ninformation, and exchange their information with each agent they meet.\n  We consider the broadcast time of a single piece of information in an\n$n$-node graph for the above three protocols, assuming a linear number of\nagents that start from the stationary distribution. We observe that there are\ngraphs on which the agent-based protocols are significantly faster than\nPUSH-PULL, and graphs where the converse is true. We attribute the good\nperformance of agent-based algorithms to their inherently fair bandwidth\nutilization, and conclude that, in certain settings, agent-based information\ndissemination, separately or in combination with PUSH-PULL, can significantly\nimprove the broadcast time.\n  The graphs considered above are highly non-regular. Our main technical result\nis that on any regular graph of at least logarithmic degree, PUSH-PULL and\nVISIT-EXCHANGE have the same asymptotic broadcast time. The proof uses a novel\ncoupling argument which relates the random choices of vertices in PUSH-PULL\nwith the random walks in VISIT-EXCHANGE. Further, we show that the broadcast\ntime of MEET-EXCHANGE is asymptotically at least as large as the other two's on\nall regular graphs, and strictly larger on some regular graphs.\n  As far as we know, this is the first systematic and thorough comparison of\nthe running times of these very natural information dissemination protocols.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 16:32:17 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Giakkoupis", "George", ""], ["Mallmann-Trenn", "Frederik", ""], ["Saribekyan", "Hayk", ""]]}, {"id": "2006.02456", "submitter": "Pavlos Papadopoulos", "authors": "Will Abramson, Adam James Hall, Pavlos Papadopoulos, Nikolaos\n  Pitropakis, William J Buchanan", "title": "A Distributed Trust Framework for Privacy-Preserving Machine Learning", "comments": "To be published in the proceedings of the 17th International\n  Conference on Trust, Privacy and Security in Digital Business - TrustBus2020", "journal-ref": "17th International Conference TrustBus 2020", "doi": "10.1007/978-3-030-58986-8_14", "report-no": "TrustBus 2020, LNCS 12395, pp. 205--220, 2020", "categories": "cs.CR cs.CY cs.DC cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When training a machine learning model, it is standard procedure for the\nresearcher to have full knowledge of both the data and model. However, this\nengenders a lack of trust between data owners and data scientists. Data owners\nare justifiably reluctant to relinquish control of private information to third\nparties. Privacy-preserving techniques distribute computation in order to\nensure that data remains in the control of the owner while learning takes\nplace. However, architectures distributed amongst multiple agents introduce an\nentirely new set of security and trust complications. These include data\npoisoning and model theft. This paper outlines a distributed infrastructure\nwhich is used to facilitate peer-to-peer trust between distributed agents;\ncollaboratively performing a privacy-preserving workflow. Our outlined\nprototype sets industry gatekeepers and governance bodies as credential\nissuers. Before participating in the distributed learning workflow, malicious\nactors must first negotiate valid credentials. We detail a proof of concept\nusing Hyperledger Aries, Decentralised Identifiers (DIDs) and Verifiable\nCredentials (VCs) to establish a distributed trust architecture during a\nprivacy-preserving machine learning experiment. Specifically, we utilise secure\nand authenticated DID communication channels in order to facilitate a federated\nlearning workflow related to mental health care data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:06:13 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Abramson", "Will", ""], ["Hall", "Adam James", ""], ["Papadopoulos", "Pavlos", ""], ["Pitropakis", "Nikolaos", ""], ["Buchanan", "William J", ""]]}, {"id": "2006.02464", "submitter": "Jonathan Mace", "authors": "Arpan Gujarati, Reza Karimi, Safya Alzayat, Wei Hao, Antoine Kaufmann,\n  Ymir Vigfusson, Jonathan Mace", "title": "Serving DNNs like Clockwork: Performance Predictability from the Bottom\n  Up", "comments": "In Proceedings of the 14th USENIX Symposium on Operating Systems\n  Design and Implementation (OSDI '20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning inference is becoming a core building block for interactive\nweb applications. As a result, the underlying model serving systems on which\nthese applications depend must consistently meet low latency targets. Existing\nmodel serving architectures use well-known reactive techniques to alleviate\ncommon-case sources of latency, but cannot effectively curtail tail latency\ncaused by unpredictable execution times. Yet the underlying execution times are\nnot fundamentally unpredictable - on the contrary we observe that inference\nusing Deep Neural Network (DNN) models has deterministic performance. Here,\nstarting with the predictable execution times of individual DNN inferences, we\nadopt a principled design methodology to successively build a fully distributed\nmodel serving system that achieves predictable end-to-end performance. We\nevaluate our implementation, Clockwork, using production trace workloads, and\nshow that Clockwork can support thousands of models while simultaneously\nmeeting 100ms latency targets for 99.9999% of requests. We further demonstrate\nthat Clockwork exploits predictable execution times to achieve tight\nrequest-level service-level objectives (SLOs) as well as a high degree of\nrequest-level performance isolation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:18:45 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 15:52:20 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gujarati", "Arpan", ""], ["Karimi", "Reza", ""], ["Alzayat", "Safya", ""], ["Hao", "Wei", ""], ["Kaufmann", "Antoine", ""], ["Vigfusson", "Ymir", ""], ["Mace", "Jonathan", ""]]}, {"id": "2006.02515", "submitter": "Paul Hofmann", "authors": "Mar{\\i}a Arenas-Mart{\\i}nez and Sergio Herrero-Lopez and Abel Sanchez\n  and John R. Williams and Paul Roth and Paul Hofmann and Alexander Zeier", "title": "A Comparative Study of Data Storage and Processing Architectures for the\n  Smart Grid", "comments": null, "journal-ref": null, "doi": "10.1109/SMARTGRID.2010.5622058", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of governments and organizations around the world agree that the\nfirst step to address national and international problems such as energy\nindependence, global warming or emergency resilience, is the redesign of\nelectricity networks, known as Smart Grids. Typically, power grids have\nbroadcast power from generation plants to large population of consumers on a\nsub-optimal way. Nevertheless, the fusion of energy delivery networks and\ndigital information networks, along with the introduction of intelligent\nmonitoring systems (Smart Meters) and renewable energies, would enable two-way\nelectricity trading relationships between electricity suppliers and electricity\nconsumers. The availability of real-time information on electricity demand and\npricing, would enable suppliers optimizing their delivery systems, while\nconsumers would have the means to minimize their bill by turning on appliances\nat off-peak hours. The construction of the Smart Grid entails the design and\ndeployment of information networks and systems of unprecedented requirements on\nstorage, real-time event processing and availability. In this paper, a series\nof system architectures to store and process Smart Meter reading data are\nexplored and compared aiming to establish a solid foundation in which future\nintelligent systems could be supported.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 20:36:05 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Arenas-Mart\u0131nez", "Mar\u0131a", ""], ["Herrero-Lopez", "Sergio", ""], ["Sanchez", "Abel", ""], ["Williams", "John R.", ""], ["Roth", "Paul", ""], ["Hofmann", "Paul", ""], ["Zeier", "Alexander", ""]]}, {"id": "2006.02602", "submitter": "Weicheng Xue", "authors": "Weicheng Xue and Christopher J. Roy", "title": "Multi-GPU Performance Optimization of a CFD Code using OpenACC on\n  Different Platforms", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.6036", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the multi-GPU performance of a 3D buoyancy driven\ncavity solver using MPI and OpenACC directives on different platforms. The\npaper shows that decomposing the total problem in different dimensions affects\nthe strong scaling performance significantly for the GPU. Without proper\nperformance optimizations, it is shown that 1D domain decomposition scales\npoorly on multiple GPUs due to the noncontiguous memory access. The performance\nusing whatever decompositions can be benefited from a series of performance\noptimizations in the paper. Since the buoyancy driven cavity code is\nlatency-bounded on the clusters examined, a series of optimizations both\nagnostic and tailored to the platforms are designed to reduce the latency cost\nand improve memory throughput between hosts and devices efficiently. First, the\nparallel message packing/unpacking strategy developed for noncontiguous data\nmovement between hosts and devices improves the overall performance by about a\nfactor of 2. Second, transferring different data based on the stencil sizes for\ndifferent variables further reduces the communication overhead. These two\noptimizations are general enough to be beneficial to stencil computations\nhaving ghost changes on all of the clusters tested. Third, GPUDirect is used to\nimprove the communication on clusters which have the hardware and software\nsupport for direct communication between GPUs without staging CPU's memory.\nFinally, overlapping the communication and computations is shown to be not\nefficient on multi-GPUs if only using MPI or MPI+OpenACC. Although we believe\nour implementation has revealed enough overlap, the actual running does not\nutilize the overlap well due to a lack of asynchronous progression.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 00:57:48 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Xue", "Weicheng", ""], ["Roy", "Christopher J.", ""]]}, {"id": "2006.02735", "submitter": "Sungho Lee", "authors": "Sungho Lee, Minsu Kim, Jemin Lee, Ruei-Hau Hsu, Tony Q. S. Quek", "title": "Is Blockchain Suitable for Data Freshness? -- Age-of-Information\n  Perspective", "comments": "7 pages, 6 figures; This paper is under review in IEEE Network\n  Magazine", "journal-ref": null, "doi": "10.1109/MNET.011.2000044", "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in blockchain have led to a significant interest in\ndeveloping blockchain-based applications. While data can be retained in\nblockchains, the stored values can be deleted or updated. From a user viewpoint\nthat searches for the data, it is unclear whether the discovered data from the\nblockchain storage is relevant for real-time decision-making process for\nblockchain-based application. The data freshness issue serves as a critical\nfactor especially in dynamic networks handling real-time information. In\ngeneral, transactions to renew the data require additional processing time\ninside the blockchain network, which is called ledger-commitment latency. Due\nto this problem, some users may receive outdated data. As a result, it is\nimportant to investigate if blockchain is suitable for providing real-time data\nservices. In this article, we first describe blockchain-enabled (BCE) networks\nwith Hyperledger Fabric (HLF). Then, we define age of information (AoI) of BCE\nnetworks and investigate the influential factors in this AoI. Analysis and\nexperiments are conducted to support our proposed framework. Lastly, we\nconclude by discussing some future challenges.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 09:48:40 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lee", "Sungho", ""], ["Kim", "Minsu", ""], ["Lee", "Jemin", ""], ["Hsu", "Ruei-Hau", ""], ["Quek", "Tony Q. S.", ""]]}, {"id": "2006.02759", "submitter": "Damien Chapon", "authors": "Loic Strafella, Damien Chapon", "title": "Boosting I/O and visualization for exascale era using Hercule: test case\n  on RAMSES", "comments": "9 pages, 8 figures. Proceedings of ASTRONUM 2019, July 2019, Paris,\n  France. Submitted to Journal of Physics Conference Series", "journal-ref": "Journal of Physics: Conference Series, Volume 1623, 14th Int.\n  Conf. on Numerical Modeling of Space Plasma Flows: ASTRONUM-2019", "doi": "10.1088/1742-6596/1623/1/012019", "report-no": null, "categories": "cs.DC astro-ph.CO astro-ph.GA cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been clearly identified that I/O is one of the bottleneck to extend\napplication for the exascale era. New concepts such as 'in transit' and 'in\nsitu' visualization and analysis have been identified as key technologies to\ncircumvent this particular issue. A new parallel I/O and data management\nlibrary called Hercule, developed at CEA-DAM, has been integrated to Ramses, an\nAMR simulation code for self-gravitating fluids. Splitting the original Ramses\noutput format in Hercule database formats dedicated to either\ncheckpoints/restarts (HProt format) or post-processing (HDep format) not only\nimproved I/O performance and scalability of the Ramses code but also introduced\nmuch more flexibility in the simulation outputs to help astrophysicists prepare\ntheir DMP (Data Management Plan). Furthermore, the very lightweight and\npurpose-specific post-processing format (HDep) will significantly improve the\noverall performance of analysis and visualization tools such as PyMSES 5. An\nintroduction to the Hercule parallel I/O library as well as I/O benchmark\nresults will be discussed.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:36:04 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Strafella", "Loic", ""], ["Chapon", "Damien", ""]]}, {"id": "2006.02833", "submitter": "Yaser Mansouri", "authors": "Yaser Mansouri, Victor Prokhorenko, and M. Ali Babar", "title": "An Automated Implementation of Hybrid Cloud for Performance Evaluation\n  of Distributed Databases", "comments": null, "journal-ref": "Journal of Network and Computer Applications (JNCA), 2020", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Hybrid cloud is an integration of resources between private and public\nclouds. It enables users to horizontally scale their on-premises infrastructure\nup to public clouds in order to improve performance and cut up-front investment\ncost. This model of applications deployment is called cloud bursting that\nallows data-intensive applications especially distributed database systems to\nhave the benefit of both private and public clouds. In this work, we present an\nautomated implementation of a hybrid cloud using (i) a robust and zero-cost\nLinux-based VPN to make a secure connection between private and public clouds,\nand (ii) Terraform as a software tool to deploy infrastructure resources based\non the requirements of hybrid cloud. We also explore performance evaluation of\ncloud bursting for six modern and distributed database systems on the hybrid\ncloud spanning over local OpenStack and Microsoft Azure. Our results reveal\nthat MongoDB and MySQL Cluster work efficient in terms of throughput and\noperations latency if they burst into a public cloud to supply their resources.\nIn contrast, the performance of Cassandra, Riak, Redis, and Couchdb reduces if\nthey significantly leverage their required resources via cloud bursting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 13:08:27 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Mansouri", "Yaser", ""], ["Prokhorenko", "Victor", ""], ["Babar", "M. Ali", ""]]}, {"id": "2006.02893", "submitter": "Diksha Gupta", "authors": "Diksha Gupta, Jared Saia, and Maxwell Young", "title": "ToGCom: An Asymmetric Sybil Defense", "comments": "30 pages. arXiv admin note: substantial text overlap with\n  arXiv:1911.06462", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof-of-work (PoW) is one of the most common techniques to defend against\nSybil attacks. Unfortunately, current PoW defenses have two main drawbacks.\nFirst, they require work to be done even in the absence of an attack. Second,\nduring an attack, they require good identities (IDs) to spend as much as the\nattacker.\n  Recent theoretical work by Gupta, Saia, and Young suggests the possibility of\novercoming these two drawbacks. In particular, they describe a new algorithm,\nGMCom, that always ensures that a minority of IDs are Sybil. They show that\nrate at which all good IDs perform computation is $O(J_G + \\sqrt{T(J_G+1)})$,\nwhere $J_G$ is the join rate of good IDs, and $T$ is the rate at which the\nadversary performs computation.\n  Unfortunately, this cost bound only holds in the case where (1) GMCom always\nknows the join rate of good IDs; and (2) there is a fixed constant amount of\ntime that separates join events by good IDs. Here, we present ToGCom, which\nremoves these two shortcomings. To do so, we design and analyze a mechanism for\nestimating the join rate of good IDs; and also devise a new method for setting\nthe computational cost to join the system. Additionally, we evaluate the\nperformance of ToGCom alongside prior PoW-based defenses. Based on our\nexperiments, we design heuristics that further improve the performance of\nToGCom by up to $3$ orders of magnitude over these previous Sybil defenses.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 06:52:20 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "2006.02924", "submitter": "Todd Mytkowicz", "authors": "Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Olli Saarikivi, Tianju\n  Xu, Vadim Eksarevskiy, Jaliya Ekanayake, Emad Barsoum", "title": "Scaling Distributed Training with Adaptive Summation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is an inherently sequential training\nalgorithm--computing the gradient at batch $i$ depends on the model parameters\nlearned from batch $i-1$. Prior approaches that break this dependence do not\nhonor them (e.g., sum the gradients for each batch, which is not what\nsequential SGD would do) and thus potentially suffer from poor convergence.\nThis paper introduces a novel method to combine gradients called Adasum (for\nadaptive sum) that converges faster than prior work. Adasum is easy to\nimplement, almost as efficient as simply summing gradients, and is integrated\ninto the open-source toolkit Horovod.\n  This paper first provides a formal justification for Adasum and then\nempirically demonstrates Adasum is more accurate than prior gradient\naccumulation methods. It then introduces a series of case-studies to show\nAdasum works with multiple frameworks, (TensorFlow and PyTorch), scales\nmultiple optimizers (Momentum-SGD, Adam, and LAMB) to larger batch-sizes while\nstill giving good downstream accuracy. Finally, it proves that Adasum\nconverges.\n  To summarize, Adasum scales Momentum-SGD on the MLPerf Resnet50 benchmark to\n64K examples before communication (no MLPerf v0.5 entry converged with more\nthan 16K), the Adam optimizer to 64K examples before communication on\nBERT-LARGE (prior work showed Adam stopped scaling at 16K), and the LAMB\noptimizer to 128K before communication on BERT-LARGE (prior work used 64K), all\nwhile maintaining downstream accuracy metrics. Finally, if a user does not need\nto scale, we show LAMB with Adasum on BERT-LARGE converges in 30% fewer steps\nthan the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:08:20 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Maleki", "Saeed", ""], ["Musuvathi", "Madan", ""], ["Mytkowicz", "Todd", ""], ["Saarikivi", "Olli", ""], ["Xu", "Tianju", ""], ["Eksarevskiy", "Vadim", ""], ["Ekanayake", "Jaliya", ""], ["Barsoum", "Emad", ""]]}, {"id": "2006.03042", "submitter": "Francisco Maturana", "authors": "Francisco Maturana, V. S. Chaitanya Mukka, K. V. Rashmi", "title": "Access-optimal Linear MDS Convertible Codes for All Parameters", "comments": "This is an extended version of an IEEE ISIT 2020 paper with the same\n  title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale distributed storage systems, erasure codes are used to achieve\nfault tolerance in the face of node failures. Tuning code parameters to\nobserved failure rates has been shown to significantly reduce storage cost.\nSuch tuning of redundancy requires \"code conversion\", i.e., a change in code\ndimension and length on already encoded data. Convertible codes are a new class\nof codes designed to perform such conversions efficiently. The access cost of\nconversion is the number of nodes accessed during conversion.\n  Existing literature has characterized the access cost of conversion of linear\nMDS convertible codes only for a specific and small subset of parameters. In\nthis paper, we present lower bounds on the access cost of conversion of linear\nMDS codes for all valid parameters. Furthermore, we show that these lower\nbounds are tight by presenting an explicit construction for access-optimal\nlinear MDS convertible codes for all valid parameters. En route, we show that,\none of the degrees-of-freedom in the design of convertible codes that was\ninconsequential in the previously studied parameter regimes, turns out to be\ncrucial when going beyond these regimes and adds to the challenge in the\nanalysis and code construction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:51:43 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Maturana", "Francisco", ""], ["Mukka", "V. S. Chaitanya", ""], ["Rashmi", "K. V.", ""]]}, {"id": "2006.03104", "submitter": "Ulf Leser", "authors": "Christopher Schiefer, Marc Bux, Joergen Brandt, Clemens Messerschmidt,\n  Knut Reinert, Dieter Beule, Ulf Leser", "title": "Portability of Scientific Workflows in NGS Data Analysis: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of next-generation sequencing (NGS) data requires complex\ncomputational workflows consisting of dozens of autonomously developed yet\ninterdependent processing steps. Whenever large amounts of data need to be\nprocessed, these workflows must be executed on a parallel and/or distributed\nsystems to ensure reasonable runtime. Porting a workflow developed for a\nparticular system on a particular hardware infrastructure to another system or\nto another infrastructure is non-trivial, which poses a major impediment to the\nscientific necessities of workflow reproducibility and workflow reusability. In\nthis work, we describe our efforts to port a state-of-the-art workflow for the\ndetection of specific variants in whole-exome sequencing of mice. The workflow\noriginally was developed in the scientific workflow system snakemake for\nexecution on a high-performance cluster controlled by Sun Grid Engine. In the\nproject, we ported it to the scientific workflow system SaasFee that can\nexecute workflows on (multi-core) stand-alone servers or on clusters of\narbitrary sizes using the Hadoop. The purpose of this port was that also owners\nof low-cost hardware infrastructures, for which Hadoop was made for, become\nable to use the workflow. Although both the source and the target system are\ncalled scientific workflow systems, they differ in numerous aspects, ranging\nfrom the workflow languages to the scheduling mechanisms and the file access\ninterfaces. These differences resulted in various problems, some expected and\nmore unexpected, that had to be resolved before the workflow could be run with\nequal semantics. As a side-effect, we also report cost/runtime ratios for a\nstate-of-the-art NGS workflow on very different hardware platforms: A\ncomparably cheap stand-alone server (80 threads), a mid-cost, mid-sized cluster\n(552 threads), and a high-end HPC system (3784 threads).\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:10:48 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Schiefer", "Christopher", ""], ["Bux", "Marc", ""], ["Brandt", "Joergen", ""], ["Messerschmidt", "Clemens", ""], ["Reinert", "Knut", ""], ["Beule", "Dieter", ""], ["Leser", "Ulf", ""]]}, {"id": "2006.03108", "submitter": "Russell Hewett", "authors": "Russell J. Hewett and Thomas J. Grady II", "title": "A Linear Algebraic Approach to Model Parallelism in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks (DNNs) in large-cluster computing environments\nis increasingly necessary, as networks grow in size and complexity. Local\nmemory and processing limitations require robust data and model parallelism for\ncrossing compute node boundaries. We propose a linear-algebraic approach to\nmodel parallelism in deep learning, which allows parallel distribution of any\ntensor in the DNN. Rather than rely on automatic differentiation tools, which\ndo not universally support distributed memory parallelism models, we show that\nparallel data movement operations, e.g., broadcast, sum-reduce, and halo\nexchange, are linear operators, and by defining the relevant spaces and inner\nproducts, we manually develop the adjoint, or backward, operators required for\ngradient-based training of DNNs. We build distributed DNN layers using these\nparallel primitives, composed with sequential layer implementations, and\ndemonstrate their application by building and training a distributed DNN using\nDistDL, a PyTorch and MPI-based distributed deep learning toolkit.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:38:05 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hewett", "Russell J.", ""], ["Grady", "Thomas J.", "II"]]}, {"id": "2006.03178", "submitter": "Daniel Zhang", "authors": "Daniel Zhang, Yue Ma, X. Sharon Hu, Dong Wang", "title": "Towards Privacy-aware Task Allocation in Social Sensing based Edge\n  Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance in mobile computing, Internet of Things, and ubiquitous\nwireless connectivity, social sensing based edge computing (SSEC) has emerged\nas a new computation paradigm where people and their personally owned devices\ncollect sensor measurements from the physical world and process them at the\nedge of the network. This paper focuses on a privacy-aware task allocation\nproblem where the goal is to optimize the computation task allocation in SSEC\nsystems while respecting the users' customized privacy settings. It introduces\na novel Game-theoretic Privacy-aware Task Allocation (G-PATA) framework to\nachieve the goal. G-PATA includes (i) a bottom-up game-theoretic model to\ngenerate the maximum payoffs at end devices while satisfying the end user's\nprivacy settings; (ii) a top-down incentive scheme to adjust the rewards for\nthe tasks to ensure that the task allocation decisions made by end devices meet\nthe Quality of Service (QoS) requirements of the applications. Furthermore, the\nframework incorporates an efficient load balancing and iteration reduction\ncomponent to adapt to the dynamic changes in status and privacy configurations\nof end devices. The G-PATA framework was implemented on a real-world edge\ncomputing platform that consists of heterogeneous end devices (Jetson TX1 and\nTK1 boards, and Raspberry Pi3). We compare G-PATA with state-of-the-art task\nallocation schemes through two real-world social sensing applications. The\nresults show that G-PATA significantly outperforms existing approaches under\nvarious privacy settings (our scheme achieved as much as 47% improvements in\ndelay reduction for the application and 15% more payoffs for end devices\ncompared to the baselines.).\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 00:21:27 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhang", "Daniel", ""], ["Ma", "Yue", ""], ["Hu", "X. Sharon", ""], ["Wang", "Dong", ""]]}, {"id": "2006.03206", "submitter": "Chinmay Kulkarni", "authors": "Chinmay Kulkarni, Badrish Chandramouli, Ryan Stutsman", "title": "Achieving High Throughput and Elasticity in a Larger-than-Memory Store", "comments": null, "journal-ref": "PVLDB, 14(8): 1427 - 1440, 2021", "doi": "10.14778/3457390.3457406", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of sensors, mobile applications and machines now generate billions\nof events. Specialized many-core key-value stores (KVSs) can ingest and index\nthese events at high rates (over 100 Mops/s on one machine) if events are\ngenerated on the same machine; however, to be practical and cost-effective they\nmust ingest events over the network and scale across cloud resources\nelastically.\n  We present Shadowfax, a new distributed KVS based on FASTER, that\ntransparently spans DRAM, SSDs, and cloud blob storage while serving 130\nMops/s/VM over commodity Azure VMs using conventional Linux TCP. Beyond high\nsingle-VM performance, Shadowfax uses a unique approach to distributed\nreconfiguration that avoids any server-side key ownership checks or cross-core\ncoordination both during normal operation and migration. Hence, Shadowfax can\nshift load in 17 s to improve system throughput by 10 Mops/s with little\ndisruption. Compared to the state-of-the-art, it has 8x better throughput (than\nSeastar+memcached) and avoids costly I/O to move cold data during migration. On\n12 machines, Shadowfax retains its high throughput to perform 930 Mops/s,\nwhich, to the best of our knowledge, is the highest reported throughput for a\ndistributed KVS used for large-scale data ingestion and indexing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:32:06 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 22:21:59 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kulkarni", "Chinmay", ""], ["Chandramouli", "Badrish", ""], ["Stutsman", "Ryan", ""]]}, {"id": "2006.03208", "submitter": "Sharif Abuadbba Dr", "authors": "Sharif Abuadbba, Ayman Ibaida, Ibrahim Khalil, Naveen Chilamkurti,\n  Surya Nepal and Xinghuo Yu", "title": "Can the Multi-Incoming Smart Meter Compressed Streams be Re-Compressed?", "comments": "8 pages. Submitted to IEEE Transaction on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart meters have currently attracted attention because of their high\nefficiency and throughput performance. They transmit a massive volume of\ncontinuously collected waveform readings (e.g. monitoring). Although many\ncompression models are proposed, the unexpected size of these compressed\nstreams required endless storage and management space which poses a unique\nchallenge. Therefore, this paper explores the question of can the compressed\nsmart meter readings be re-compressed? We first investigate the applicability\nof re-applying general compression algorithms directly on compressed streams.\nThe results were poor due to the lack of redundancy. We further propose a novel\ntechnique to enhance the theoretical entropy and exploit that to re-compress.\nThis is successfully achieved by using unsupervised learning as a similarity\nmeasurement to cluster the compressed streams into subgroups. The streams in\nevery subgroup have been interleaved, followed by the first derivative to\nminimize the values and increase the redundancy. After that, two rotation steps\nhave been applied to rearrange the readings in a more consecutive format before\napplying a developed dynamic run length. Finally, entropy coding is performed.\nBoth mathematical and empirical experiments proved the significant improvement\nof the compressed streams entropy (i.e. almost reduced by half) and the\nresultant compression ratio (i.e. up to 50%).\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 02:36:42 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Abuadbba", "Sharif", ""], ["Ibaida", "Ayman", ""], ["Khalil", "Ibrahim", ""], ["Chilamkurti", "Naveen", ""], ["Nepal", "Surya", ""], ["Yu", "Xinghuo", ""]]}, {"id": "2006.03231", "submitter": "Yulai Zhang", "authors": "Yulai Zhang, Jiachen Wang, Gang Cen, and Guiming Luo", "title": "Parallel ensemble methods for causal direction inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the causal direction between two variables from their observation\ndata is one of the most fundamental and challenging topics in data science. A\ncausal direction inference algorithm maps the observation data into a binary\nvalue which represents either x causes y or y causes x. The nature of these\nalgorithms makes the results unstable with the change of data points. Therefore\nthe accuracy of the causal direction inference can be improved significantly by\nusing parallel ensemble frameworks. In this paper, new causal direction\ninference algorithms based on several ways of parallel ensemble are proposed.\nTheoretical analyses on accuracy rates are given. Experiments are done on both\nof the artificial data sets and the real world data sets. The accuracy\nperformances of the methods and their computational efficiencies in parallel\ncomputing environment are demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 05:07:52 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhang", "Yulai", ""], ["Wang", "Jiachen", ""], ["Cen", "Gang", ""], ["Luo", "Guiming", ""]]}, {"id": "2006.03249", "submitter": "Yukiko Yamauchi", "authors": "Paola Flocchini and Nicola Santoro and Masafumi Yamashita and Yukiko\n  Yamauchi", "title": "A Characterization of Semi-Synchrony for Asynchronous Robots with\n  Limited Visibility, and its Application to Luminous Synchronizer Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile robot system consists of anonymous mobile robots, each of which\nautonomously performs sensing, computation, and movement according to a common\nalgorithm, so that the robots collectively achieve a given task. There are two\nmain models of time and activation of the robots. In the semi-synchronous model\n(SSYNC), the robots share a common notion of time; at each time unit, a subset\nof the robots is activated, and each performs all three actions (sensing,\ncomputation, and movement) in that time unit. In the asynchronous model\n(ASYNC), there is no common notion of time, the robots are activated at\narbitrary times, and the duration of each action is arbitrary but finite.\n  In this paper, we investigate the problem of synchronizing ASNYC robots with\nlimited sensing range, i.e., limited visibility. We first present a sufficient\ncondition for an ASYNC execution of a common algorithm ${\\cal A}$ to have a\ncorresponding SSYNC execution of ${\\cal A}$; our condition imposes timing\nconstraints on the activation schedule of the robots and visibility constraints\nduring movement. Then, we prove that this condition is necessary (with\nprobability $1$) under a randomized ASYNC adversary. Finally, we present a\nsynchronization algorithm for luminous ASYNC robots with limited visibility,\neach equipped with a light that can take a constant number of colors. Our\nalgorithm enables luminous ASYNC robots to simulate any algorithm ${\\cal A}$,\ndesigned for the (non-luminous) SSYNC robots and satisfying visibility\nconstraints.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 06:10:53 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 06:43:15 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Flocchini", "Paola", ""], ["Santoro", "Nicola", ""], ["Yamashita", "Masafumi", ""], ["Yamauchi", "Yukiko", ""]]}, {"id": "2006.03271", "submitter": "Valerio Schiavoni Dr", "authors": "Pascal Maissen and Pascal Felber and Peter Kropf and Valerio Schiavoni", "title": "FaaSdom: A Benchmark Suite for Serverless Computing", "comments": "ACM DEBS'20", "journal-ref": null, "doi": "10.1145/3401025.3401738", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless computing has become a major trend among cloud providers. With\nserverless computing, developers fully delegate the task of managing the\nservers, dynamically allocating the required resources, as well as handling\navailability and fault-tolerance matters to the cloud provider. In doing so,\ndevelopers can solely focus on the application logic of their software, which\nis then deployed and completely managed in the cloud. Despite its increasing\npopularity, not much is known regarding the actual system performance\nachievable on the currently available serverless platforms. Specifically, it is\ncumbersome to benchmark such systems in a language- or runtime-independent\nmanner. Instead, one must resort to a full application deployment, to later\ntake informed decisions on the most convenient solution along several\ndimensions, including performance and economic costs. FaaSdom is a modular\narchitecture and proof-of-concept implementation of a benchmark suite for\nserverless computing platforms. It currently supports the current mainstream\nserverless cloud providers (i.e., AWS, Azure, Google, IBM), a large set of\nbenchmark tests and a variety of implementation languages. The suite fully\nautomatizes the deployment, execution and clean-up of such tests, providing\ninsights (including historical) on the performance observed by serverless\napplications. FaaSdom also integrates a model to estimate budget costs for\ndeployments across the supported providers. FaaSdom is open-source and\navailable at https://github.com/bschitter/benchmark-suite-serverless-computing.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 07:43:40 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Maissen", "Pascal", ""], ["Felber", "Pascal", ""], ["Kropf", "Peter", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2006.03318", "submitter": "Hongyu Zhu", "authors": "Hongyu Zhu, Amar Phanishayee, Gennady Pekhimenko", "title": "Daydream: Accurately Estimating the Efficacy of Optimizations for DNN\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network (DNN) training jobs use complex and heterogeneous\nsoftware/hardware stacks. The efficacy of software-level optimizations can vary\nsignificantly when used in different deployment configurations. It is onerous\nand error-prone for ML practitioners and system developers to implement each\noptimization separately, and determine which ones will improve performance in\ntheir own configurations. Unfortunately, existing profiling tools do not aim to\nanswer predictive questions such as \"How will optimization X affect the\nperformance of my model?\". We address this critical limitation, and proposes a\nnew profiling tool, Daydream, to help programmers efficiently explore the\nefficacy of DNN optimizations. Daydream models DNN execution with a\nfine-grained dependency graph based on low-level traces collected by CUPTI, and\npredicts runtime by simulating execution based on the dependency graph.\nDaydream maps the low-level traces using DNN domain-specific knowledge, and\nintroduces a set of graph-transformation primitives that can easily model a\nwide variety of optimizations. We show that Daydream is able to model most\nmainstream DNN optimization techniques, and accurately predict the efficacy of\noptimizations that will result in significant performance improvements.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:08:16 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Zhu", "Hongyu", ""], ["Phanishayee", "Amar", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2006.03594", "submitter": "Seyyedali Hosseinalipour", "authors": "Seyyedali Hosseinalipour and Christopher G. Brinton and Vaneet\n  Aggarwal and Huaiyu Dai and Mung Chiang", "title": "From Federated to Fog Learning: Distributed Machine Learning over\n  Heterogeneous Wireless Networks", "comments": "This paper is accepted for publication in IEEE Communications\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) tasks are becoming ubiquitous in today's network\napplications. Federated learning has emerged recently as a technique for\ntraining ML models at the network edge by leveraging processing capabilities\nacross the nodes that collect the data. There are several challenges with\nemploying conventional federated learning in contemporary networks, due to the\nsignificant heterogeneity in compute and communication capabilities that exist\nacross devices. To address this, we advocate a new learning paradigm called fog\nlearning which will intelligently distribute ML model training across the\ncontinuum of nodes from edge devices to cloud servers. Fog learning enhances\nfederated learning along three major dimensions: network, heterogeneity, and\nproximity. It considers a multi-layer hybrid learning framework consisting of\nheterogeneous devices with various proximities. It accounts for the topology\nstructures of the local networks among the heterogeneous nodes at each network\nlayer, orchestrating them for collaborative/cooperative learning through\ndevice-to-device (D2D) communications. This migrates from star network\ntopologies used for parameter transfers in federated learning to more\ndistributed topologies at scale. We discuss several open research directions to\nrealizing fog learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:11:18 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 21:16:51 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 14:42:05 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Hosseinalipour", "Seyyedali", ""], ["Brinton", "Christopher G.", ""], ["Aggarwal", "Vaneet", ""], ["Dai", "Huaiyu", ""], ["Chiang", "Mung", ""]]}, {"id": "2006.03720", "submitter": "Anirban Das", "authors": "Anirban Das, Andrew Leaf, Carlos A. Varela, Stacy Patterson", "title": "Skedulix: Hybrid Cloud Scheduling for Cost-Efficient Execution of\n  Serverless Applications", "comments": "10 pages, 5 figures, 2020 IEEE 13th International Conference on Cloud\n  Computing (CLOUD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for scheduling multifunction serverless applications\nover a hybrid public-private cloud. A set of serverless jobs is input as a\nbatch, and the objective is to schedule function executions over the hybrid\nplatform to minimize the cost of public cloud use, while completing all jobs by\na specified deadline. As this scheduling problem is NP-Hard, we propose a\ngreedy algorithm that dynamically determines both the order and placement of\neach function execution using predictive models of function execution time and\nnetwork latencies. We present a prototype implementation of our framework that\nuses AWS Lambda and OpenFaaS, for the public and private cloud, respectively.\nWe evaluate our prototype in live experiments using a mixture of compute and\nI/O heavy serverless applications. Our results show that our framework can\nachieve a speedup in batch processing of up to 1.92 times that of an approach\nthat uses only the private cloud, at 40.5% the cost of an approach that uses\nonly the public cloud.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 22:27:42 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Das", "Anirban", ""], ["Leaf", "Andrew", ""], ["Varela", "Carlos A.", ""], ["Patterson", "Stacy", ""]]}, {"id": "2006.03746", "submitter": "Shreyas Pai", "authors": "Reuven Bar-Yehuda, Keren Censor-Hillel, Yannic Maus, Shreyas Pai,\n  Sriram V. Pemmaraju", "title": "Distributed Approximation on Power Graphs", "comments": "Appears in PODC 2020. 40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate graph problems in the following setting: we are given a graph\n$G$ and we are required to solve a problem on $G^2$. While we focus mostly on\nexploring this theme in the distributed CONGEST model, we show new results and\nsurprising connections to the centralized model of computation. In the CONGEST\nmodel, it is natural to expect that problems on $G^2$ would be quite difficult\nto solve efficiently on $G$, due to congestion. However, we show that the\npicture is both more complicated and more interesting.\n  Specifically, we encounter two phenomena acting in opposing directions: (i)\nslowdown due to congestion and (ii) speedup due to structural properties of\n$G^2$.\n  We demonstrate these two phenomena via two fundamental graph problems,\nnamely, Minimum Vertex Cover (MVC) and Minimum Dominating Set (MDS). Among our\nmany contributions, the highlights are the following.\n  - In the CONGEST model, we show an $O(n/\\epsilon)$-round\n$(1+\\epsilon)$-approximation algorithm for MVC on $G^2$, while no\n$o(n^2)$-round algorithm is known for any better-than-2 approximation for MVC\non $G$.\n  - We show a centralized polynomial time $5/3$-approximation algorithm for MVC\non $G^2$, whereas a better-than-2 approximation is UGC-hard for $G$.\n  - In contrast, for MDS, in the CONGEST model, we show an\n$\\tilde{\\Omega}(n^2)$ lower bound for a constant approximation factor for MDS\non $G^2$, whereas an $\\Omega(n^2)$ lower bound for MDS on $G$ is known only for\nexact computation.\n  In addition to these highlighted results, we prove a number of other results\nin the distributed CONGEST model including an $\\tilde{\\Omega}(n^2)$ lower bound\nfor computing an exact solution to MVC on $G^2$, a conditional hardness result\nfor obtaining a $(1+\\epsilon)$-approximation to MVC on $G^2$, and an $O(\\log\n\\Delta)$-approximation to the MDS problem on $G^2$ in $\\mbox{poly}\\log n$\nrounds.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 01:14:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bar-Yehuda", "Reuven", ""], ["Censor-Hillel", "Keren", ""], ["Maus", "Yannic", ""], ["Pai", "Shreyas", ""], ["Pemmaraju", "Sriram V.", ""]]}, {"id": "2006.03994", "submitter": "Shirin Tahmasebi", "authors": "Shirin Tahmasebi, Jafar Habibi and Abolhassan Shamsaie", "title": "A Scalable Architecture for Monitoring IoT Devices Using Ethereum and\n  Fog Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent considerable developments in the Internet of Things (IoT),\nbillions of resource-constrained devices are interconnected through the\ninternet. Monitoring this huge number of IoT devices that are heterogeneous in\nterms of underlying communication protocols and data format is challenging. The\nmajority of existing IoT device monitoring solutions heavily rely on\ncentralized architectures. Since using centralized architectures comes at the\nexpense of trusting an authority, it has several inherent drawbacks, including\nvulnerability to security attacks, lack of data privacy, and unauthorized data\nmanipulation. Hence, a new decentralized approach is crucial to remedy these\ndrawbacks. One of the most promising technologies which is widely used to\nprovide decentralization is blockchain. Additionally, to ease the burden of\ncommunication overhead and computational power on resource-constrained IoT\ndevices, fog computing can be exploited to decrease communication latency and\nprovide better network scalability.\n  In this paper, we propose a scalable blockchain-based architecture for\nmonitoring IoT devices using fog computing. To demonstrate the feasibility and\nusability of the proposed solution, we have implemented a proof-of-concept\nprototype, leveraging Ethereum smart contracts. Finally, a comprehensive\nevaluation is conducted. The evaluation results indicate that the proposed\nsolution is significantly scalable and compatible with resource-constrained IoT\ndevices.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 23:13:56 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 05:46:48 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Tahmasebi", "Shirin", ""], ["Habibi", "Jafar", ""], ["Shamsaie", "Abolhassan", ""]]}, {"id": "2006.04105", "submitter": "Cristian Mart\\'in Dr", "authors": "Cristian Mart\\'in, Peter Langendoerfer, Pouya Soltani Zarrin, Manuel\n  D\\'iaz and Bartolom\\'e Rubio", "title": "Kafka-ML: connecting the data stream with ML/AI frameworks", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) and Artificial Intelligence (AI) have a dependency on\ndata sources to train, improve and make predictions through their algorithms.\nWith the digital revolution and current paradigms like the Internet of Things,\nthis information is turning from static data into continuous data streams.\nHowever, most of the ML/AI frameworks used nowadays are not fully prepared for\nthis revolution. In this paper, we proposed Kafka-ML, an open-source framework\nthat enables the management of TensorFlow ML/AI pipelines through data streams\n(Apache Kafka). Kafka-ML provides an accessible and user-friendly Web User\nInterface where users can easily define ML models, to then train, evaluate and\ndeploy them for inference. Kafka-ML itself and its deployed components are\nfully managed through containerization technologies, which ensure its\nportability and easy distribution and other features such as fault-tolerance\nand high availability. Finally, a novel approach has been introduced to manage\nand reuse data streams, which may lead to the (no) utilization of data storage\nand file systems.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 10:08:02 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 14:27:06 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mart\u00edn", "Cristian", ""], ["Langendoerfer", "Peter", ""], ["Zarrin", "Pouya Soltani", ""], ["D\u00edaz", "Manuel", ""], ["Rubio", "Bartolom\u00e9", ""]]}, {"id": "2006.04122", "submitter": "Prabath Abeysekara", "authors": "Prabath Abeysekara, Hai Dong and A.K. Qin", "title": "Distributed Machine Learning for Predictive Analytics in Mobile Edge\n  Computing Based IoT Environments", "comments": "Accepted to be published at the International Joint Conference on\n  Neural Networks(IJCNN) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive analytics in Mobile Edge Computing (MEC) based Internet of Things\n(IoT) is becoming a high demand in many real-world applications. A prediction\nproblem in an MEC-based IoT environment typically corresponds to a collection\nof tasks with each task solved in a specific MEC environment based on the data\naccumulated locally, which can be regarded as a Multi-task Learning (MTL)\nproblem. However, the heterogeneity of the data (non-IIDness) accumulated\nacross different MEC environments challenges the application of general MTL\ntechniques in such a setting. Federated MTL (FMTL) has recently emerged as an\nattempt to address this issue. Besides FMTL, there exists another powerful but\nunder-exploited distributed machine learning technique, called Network Lasso\n(NL), which is inherently related to FMTL but has its own unique features. In\nthis paper, we made an in-depth evaluation and comparison of these two\ntechniques on three distinct IoT datasets representing real-world application\nscenarios. Experimental results revealed that NL outperformed FMTL in MEC-based\nIoT environments in terms of both accuracy and computational efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 11:27:41 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 16:49:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Abeysekara", "Prabath", ""], ["Dong", "Hai", ""], ["Qin", "A. K.", ""]]}, {"id": "2006.04200", "submitter": "Abhishek Dubey", "authors": "Ayan Mukhopadhyay and Geoffrey Pettet and Sayyed Vazirizade and Di Lu\n  and Said El Said and Alex Jaimes and Hiba Baroud and Yevgeniy Vorobeychik and\n  Mykel Kochenderfer and Abhishek Dubey", "title": "A Review of Incident Prediction, Resource Allocation, and Dispatch\n  Models for Emergency Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last fifty years, researchers have developed statistical, data-driven,\nanalytical, and algorithmic approaches for designing and improving emergency\nresponse management (ERM) systems. The problem is inherently difficult and\nconstitutes spatio-temporal decision making under uncertainty, which has been\naddressed in the literature with varying assumptions and approaches. This\nsurvey provides a detailed review of these approaches, focusing on the key\nchallenges and issues regarding three subprocesses that are part of this\nproblem (a) incident prediction, (b) resource allocation, and (c)\ncomputer-aided dispatch to handle the emergency conditions. We highlight the\nstrengths and weaknesses of prior work in this domain and explore the\nsimilarities and differences between different modeling paradigms. We conclude\nby illustrating remain challenges and opportunities for future research in this\ncomplex domain.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 16:50:59 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 22:04:06 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 19:42:58 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 12:06:58 GMT"}, {"version": "v5", "created": "Tue, 1 Sep 2020 06:30:26 GMT"}, {"version": "v6", "created": "Mon, 1 Feb 2021 16:42:36 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Mukhopadhyay", "Ayan", ""], ["Pettet", "Geoffrey", ""], ["Vazirizade", "Sayyed", ""], ["Lu", "Di", ""], ["Said", "Said El", ""], ["Jaimes", "Alex", ""], ["Baroud", "Hiba", ""], ["Vorobeychik", "Yevgeniy", ""], ["Kochenderfer", "Mykel", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2006.04236", "submitter": "Sara Riazi", "authors": "Sara Riazi and Boyana Norris", "title": "Distributed-Memory Vertex-Centric Network Embedding for Large-Scale\n  Graphs", "comments": "2019 IEEE 5th International Conference on Big Data Intelligence and\n  Computing (DATACOM)", "journal-ref": null, "doi": "10.1109/DataCom.2019.00029", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding is an important step in many different computations based\non graph data. However, existing approaches are limited to small or middle size\ngraphs with fewer than a million edges. In practice, web or social network\ngraphs are orders of magnitude larger, thus making most current methods\nimpractical for very large graphs. To address this problem, we introduce a new\ndistributed-memory parallel network embedding method based on Apache Spark and\nGraphX. We demonstrate the scalability of our method as well as its ability to\ngenerate meaningful embeddings for vertex classification and link prediction on\nboth real-world and synthetic graphs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 19:05:47 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Riazi", "Sara", ""], ["Norris", "Boyana", ""]]}, {"id": "2006.04549", "submitter": "Juan Jos\\'e Garc\\'ia-Castro Crespo", "authors": "Juan-Jos\\'e Crespo, German Maglione-Mathey, Jos\\'e L. S\\'anchez,\n  Francisco J. Alfaro-Cort\\'es, Jos\\'e Flich", "title": "TDSR: Transparent Distributed Segment-Based Routing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Component reliability and performance pose a great challenge for\ninterconnection networks. Future technology scaling such as transistor\nintegration capacity in VLSI design will result in higher device degradation\nand manufacture variability. As a consequence, changes in the network arise,\noften rendering irregular topologies. This paper proposes a topology-agnostic\ndistributed segment-based algorithm able to handle switch discovery in any\ntopology while guaranteeing connectivity among switches. The proposal, known as\nTransparent Distributed Segment-Based Routing (TDSR), has been applied to\nmeshes with defective link configurations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:06:50 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Crespo", "Juan-Jos\u00e9", ""], ["Maglione-Mathey", "German", ""], ["S\u00e1nchez", "Jos\u00e9 L.", ""], ["Alfaro-Cort\u00e9s", "Francisco J.", ""], ["Flich", "Jos\u00e9", ""]]}, {"id": "2006.04577", "submitter": "J\\\"urgen Maier", "authors": "J\\\"urgen Maier and Andreas Steininger", "title": "Online Test Vector Insertion: A Concurrent Built-In Self-Testing (CBIST)\n  Approach for Asynchronous Logic", "comments": "7 pages, 9 figures", "journal-ref": "17th International Symposium on Design and Diagnostics of\n  Electronic Circuits & Systems, 2014, pp. 33-38", "doi": "10.1109/DDECS.2014.6868759", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complementing concurrent checking with online testing is crucial for\npreventing fault accumulation in fault-tolerant systems with long mission\ntimes. While implementing a non-intrusive online test is cumbersome in a\nsynchronous environment, this task becomes even more challenging in\nasynchronous designs. The latter receive increasing attention, mainly due to\ntheir elastic timing behaviour; however the issues related with their testing\nremain a key obstacle for their wide adoption.\n  In this paper we present a novel approach for testing of asynchronous\ncircuits that leverages the redundancy present in the conventional 4-phase\nprotocol for implementing a fully transparent and fully concurrent test\nprocedure. The key idea is to use the protocol's unproductive NULL phase for\nprocessing test vectors, thus effectively interleaving the incoming 4-phase\ndata stream with a test data stream in a 2-phase fashion. We present\nimplementation templates for the fundamental building blocks required and give\na proof-of-concept by an example application that also serves as a platform for\nevaluating the overheads of our solution which turn out to be moderate.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:18:49 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Maier", "J\u00fcrgen", ""], ["Steininger", "Andreas", ""]]}, {"id": "2006.04616", "submitter": "Orestis Alpos", "authors": "Orestis Alpos, Christian Cachin", "title": "Consensus Beyond Thresholds: Generalized Byzantine Quorums Made Live", "comments": "To appear in the proceedings of SRDS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Byzantine fault-tolerant (BFT) consensus protocols address only\nthreshold failures, where the participating nodes fail independently of each\nother, each one fails equally likely, and the protocol's guarantees follow from\na simple bound on the number of faulty nodes. With the widespread deployment of\nByzantine consensus in blockchains and distributed ledgers today, however, more\nsophisticated trust assumptions are needed.\n  This paper presents the first implementation of BFT consensus with\ngeneralized quorums. It starts from a number of generalized trust structures\nmotivated by practice and explores methods to specify and implement them\nefficiently. In particular, it expresses the trust assumption by a monotone\nBoolean formula (MBF) with threshold operators and by a monotone span program\n(MSP), a linear-algebraic model for computation.\n  An implementation of HotStuff BFT consensus using these quorum systems is\ndescribed as well and compared to the existing threshold model. Benchmarks with\nHotStuff running on up to 40 replicas demonstrate that the MBF specification\nincurs no significant slowdown, whereas the MSP expression affects latency and\nthroughput noticeably due to the involved computations.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:12:35 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 15:16:08 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Alpos", "Orestis", ""], ["Cachin", "Christian", ""]]}, {"id": "2006.04625", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "Sebastian Brandt, Christoph Grunau, V\\'aclav Rozho\\v{n}", "title": "Generalizing the Sharp Threshold Phenomenon for the Distributed\n  Complexity of the Lov\\'asz Local Lemma", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Brandt, Maus and Uitto [PODC'19] showed that, in a restricted\nsetting, the dependency of the complexity of the distributed Lov\\'asz Local\nLemma (LLL) on the chosen LLL criterion exhibits a sharp threshold phenomenon:\nThey proved that, under the LLL criterion $p2^d < 1$, if each random variable\naffects at most $3$ events, the deterministic complexity of the LLL in the\nLOCAL model is $O(d^2 + \\log^* n)$. In stark contrast, under the criterion\n$p2^d \\leq 1$, there is a randomized lower bound of $\\Omega(\\log \\log n)$ by\nBrandt et al. [STOC'16] and a deterministic lower bound of $\\Omega(\\log n)$ by\nChang, Kopelowitz and Pettie [FOCS'16]. Brandt, Maus and Uitto conjectured that\nthe same behavior holds for the unrestricted setting where each random variable\naffects arbitrarily many events.\n  We prove their conjecture, by providing an algorithm that solves the LLL in\ntime $O(d^2 + \\log^* n)$ under the LLL criterion $p2^d < 1$, which is tight in\nbounded-degree graphs due to an $\\Omega(\\log^* n)$ lower bound by Chung, Pettie\nand Su [PODC'14]. By the work of Brandt, Maus and Uitto, obtaining such an\nalgorithm can be reduced to proving that all members in a certain family of\nfunctions in arbitrarily high dimensions are convex on some specific domain.\nUnfortunately, an analytical description of these functions is known only for\ndimension at most $3$, which led to the aforementioned restriction of their\nresult. While obtaining those descriptions for functions of (substantially)\nhigher dimension seems out of the reach of current techniques, we show that\ntheir convexity can be inferred by combinatorial means.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:24:36 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Brandt", "Sebastian", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2006.04693", "submitter": "Yang Zhao", "authors": "Leong Mei Han, Yang Zhao, Jun Zhao", "title": "Blockchain-Based Differential Privacy Cost Management System", "comments": "This paper appears in ACM ASIA Conference on Computer and\n  Communications Security (ACMASIACCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy preservation is a big concern for various sectors. To protect\nindividual user data, one emerging technology is differential privacy. However,\nit still has limitations for datasets with frequent queries, such as the fast\naccumulation of privacy cost. To tackle this limitation, this paper explores\nthe integration of a secured decentralised ledger, blockchain. Blockchain will\nbe able to keep track of all noisy responses generated with differential\nprivacy algorithm and allow for certain queries to reuse old responses. In this\npaper, a demo of a proposed blockchain-based privacy management system is\ndesigned as an interactive decentralised web application (DApp). The demo\ncreated illustrates that leveraging on blockchain will allow the total privacy\ncost accumulated to decrease significantly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:47:14 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Han", "Leong Mei", ""], ["Zhao", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "2006.04695", "submitter": "Yang Zhao", "authors": "Hans Albert Lianto, Yang Zhao, Jun Zhao", "title": "Attacks to Federated Learning: Responsive Web User Interface to Recover\n  Training Data from User Gradients", "comments": "This paper appears in ACM ASIA Conference on Computer and\n  Communications Security (ACMASIACCS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) is an emerging privacy standard to protect\nindividual user data. One scenario where LDP can be applied is federated\nlearning, where each user sends in his/her user gradients to an aggregator who\nuses these gradients to perform stochastic gradient descent. In a case where\nthe aggregator is untrusted and LDP is not applied to each user gradient, the\naggregator can recover sensitive user data from these gradients. In this paper,\nwe present a new interactive web demo showcasing the power of local\ndifferential privacy by visualizing federated learning with local differential\nprivacy. Moreover, the live demo shows how LDP can prevent untrusted\naggregators from recovering sensitive training data. A measure called the\nexp-hamming recovery is also created to show the extent of how much data the\naggregator can recover.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:50:26 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:30:42 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Lianto", "Hans Albert", ""], ["Zhao", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "2006.04754", "submitter": "Felix Beierle", "authors": "Zolt\\'an Andr\\'as Lux and Dirk Thatmann and Sebastian Zickau and Felix\n  Beierle", "title": "Distributed-Ledger-based Authentication with Decentralized Identifiers\n  and Verifiable Credentials", "comments": "Accepted for publication at the 2nd Conference on Blockchain Research\n  & Applications for Innovative Networks and Services (BRAINS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Authentication with username and password is becoming an inconvenient process\nfor the user. End users typically have little control over their personal\nprivacy, and data breaches effecting millions of users have already happened\nseveral times. We have implemented a proof of concept decentralized OpenID\nConnect Provider by marrying it with Self-Sovereign Identity, which gives users\nthe freedom to choose from a very large pool of identity providers instead of\njust a select few corporations, thus enabling the democratization of the highly\ncentralized digital identity landscape. Furthermore, we propose a verifiable\ncredential powered decentralized Public Key Infrastructure using distributed\nledger technologies, which creates a straightforward and verifiable way for\nretrieving digital certificates.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:09:34 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lux", "Zolt\u00e1n Andr\u00e1s", ""], ["Thatmann", "Dirk", ""], ["Zickau", "Sebastian", ""], ["Beierle", "Felix", ""]]}, {"id": "2006.04798", "submitter": "Mehdi Sadi", "authors": "Mehdi Sadi and Ujjwal Guin", "title": "Yield Loss Reduction and Test of AI and Deep Learning Accelerators", "comments": "Pre-print of IEEE Transactions on CAD Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With data-driven analytics becoming mainstream, the global demand for\ndedicated AI and Deep Learning accelerator chips is soaring. These\naccelerators, designed with densely packed Processing Elements (PE), are\nespecially vulnerable to the manufacturing defects and functional faults common\nin the advanced semiconductor process nodes resulting in significant yield\nloss. In this work, we demonstrate an application-driven methodology of binning\nthe AI accelerator chips, and yield loss reduction by correlating the circuit\nfaults in the PEs of the accelerator with the desired accuracy of the target AI\nworkload. We exploit the inherent fault tolerance features of trained deep\nlearning models and a strategy of selective deactivation of faulty PEs to\ndevelop the presented yield loss reduction and test methodology. An analytical\nrelationship is derived between fault location, fault rate, and the AI task's\naccuracy for deciding if the accelerator chip can pass the final yield test. A\nyield-loss reduction aware fault isolation, ATPG, and test flow are presented\nfor the multiply and accumulate units of the PEs. Results obtained with widely\nused AI/deep learning benchmarks demonstrate that the accelerators can sustain\n5% fault-rate in PE arrays while suffering from less than 1% accuracy loss,\nthus enabling product-binning and yield loss reduction of these chips.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 04:26:12 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 18:58:53 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 20:23:57 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sadi", "Mehdi", ""], ["Guin", "Ujjwal", ""]]}, {"id": "2006.04865", "submitter": "Diksha Gupta", "authors": "Diksha Gupta, Jared Saia, Maxwell Young", "title": "Resource Burning for Permissionless Systems", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof-of-work puzzles and CAPTCHAS consume enormous amounts of energy and\ntime. These techniques are examples of resource burning: verifiable consumption\nof resources solely to convey information.\n  Can these costs be eliminated? It seems unlikely since resource burning\nshares similarities with \"money burning\" and \"costly signaling\", which are\nfoundational to game theory, biology, and economics. Can these costs be\nreduced? Yes, research shows we can significantly lower the asymptotic costs of\nresource burning in many different settings.\n  In this paper, we survey the literature on resource burning; take positions\nbased on predictions of how the tool is likely to evolve; and propose several\nopen problems targeted at the theoretical distributed-computing research\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:33:39 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Gupta", "Diksha", ""], ["Saia", "Jared", ""], ["Young", "Maxwell", ""]]}, {"id": "2006.04940", "submitter": "Andreas Vitalis", "authors": "Andreas Vitalis", "title": "An Improved and Parallel Version of a Scalable Algorithm for Analyzing\n  Time Series Data", "comments": "14 pages, 5 figures, 28 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, very large amounts of data are produced and stored in all branches of\nsociety including science. Mining these data meaningfully has become a\nconsiderable challenge and is of the broadest possible interest. The size, both\nin numbers of observations and dimensionality thereof, requires data mining\nalgorithms to possess time complexities with both variables that are linear or\nnearly linear. One such algorithm, see Comput. Phys. Commun. 184, 2446-2453\n(2013), arranges observations into a sequence called the progress index. The\nprogress index steps through distinct regions of high sampling density\nsequentially. By means of suitable annotations, it allows a compact\nrepresentation of the behavior of complex systems, which is encoded in the\noriginal data set. The only essential parameter is a notion of distance between\nobservations. Here, we present the shared memory parallelization of the key\nstep in constructing the progress index, which is the calculation of an\napproximation of the minimum spanning tree of the complete graph of\nobservations. We demonstrate that excellent parallel efficiencies are obtained\nfor up to 72 logical (CPU) cores. In addition, we introduce three conceptual\nadvances to the algorithm that improve its controllability and the\ninterpretability of the progress index itself.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 10:01:48 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Vitalis", "Andreas", ""]]}, {"id": "2006.04969", "submitter": "Heiko Hamann", "authors": "Heiko Hamann and Andreagiovanni Reina", "title": "Scalability in Computing and Robotics", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TC.2021.3089044", "report-no": null, "categories": "cs.DC cs.MA cs.PF cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient engineered systems require scalability. A scalable system has\nincreasing performance with increasing system size. In an ideal case, the\nincrease in performance (e.g., speedup) corresponds to the number of units that\nare added to the system. However, if multiple units work on the same task, then\ncoordination among these units is required. This coordination can introduce\noverheads with an impact on system performance. The coordination costs can lead\nto sublinear improvement or even diminishing performance with increasing system\nsize. However, there are also systems that implement efficient coordination and\nexploit collaboration of units to attain superlinear improvement. Modeling the\nscalability dynamics is key to understanding efficient systems. Known laws of\nscalability, such as Amdahl's law, Gustafson's law, and Gunther's Universal\nScalability Law, are minimalistic phenomenological models that explain a rich\nvariety of system behaviors through concise equations. While useful to gain\ngeneral insights, the phenomenological nature of these models may limit the\nunderstanding of the underlying dynamics, as they are detached from first\nprinciples that could explain coordination overheads among units. Through a\ndecentralized system approach, we propose a general model based on generic\ninteractions between units that is able to describe, as specific cases, any\ngeneral pattern of scalability included by previously reported laws. The\nproposed general model of scalability is built on first principles, or at least\non a microscopic description of interaction between units, and therefore has\nthe potential to contribute to a better understanding of system behavior and\nscalability. We show that this model can be applied to a diverse set of\nsystems, such as parallel supercomputers, robot swarms, or wireless sensor\nnetworks, creating a unified view on interdisciplinary design for scalability.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 22:28:59 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:25:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hamann", "Heiko", ""], ["Reina", "Andreagiovanni", ""]]}, {"id": "2006.04984", "submitter": "Siva Kumar Sastry Hari", "authors": "Siva Kumar Sastry Hari, Michael B. Sullivan, Timothy Tsai, and Stephen\n  W. Keckler", "title": "Making Convolutions Resilient via Algorithm-Based Error Detection\n  Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of Convolutional Neural Networks (CNNs) to accurately process\nreal-time telemetry has boosted their use in safety-critical and\nhigh-performance computing systems. As such systems require high levels of\nresilience to errors, CNNs must execute correctly in the presence of hardware\nfaults. Full duplication provides the needed assurance but incurs a prohibitive\n100% overhead. Algorithmic techniques are known to offer low-cost solutions,\nbut the practical feasibility and performance of such techniques have never\nbeen studied for CNN deployment platforms (e.g., TensorFlow or TensorRT on\nGPUs). In this paper, we focus on algorithmically verifying Convolutions, which\nare the most resource-demanding operations in CNNs. We use checksums to verify\nconvolutions, adding a small amount of redundancy, far less than\nfull-duplication. We first identify the challenges that arise in employing\nAlgorithm-Based Error Detection (ABED) for Convolutions in optimized inference\nplatforms that fuse multiple network layers and use reduced-precision\noperations, and demonstrate how to overcome them. We propose and evaluate\nvariations of ABED techniques that offer implementation complexity, runtime\noverhead, and coverage trade-offs. Results show that ABED can detect all\ntransient hardware errors that might otherwise corrupt output and does so while\nincurring low runtime overheads (6-23%), offering at least 1.6X throughput to\nworkloads compared to full duplication.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 23:17:57 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Hari", "Siva Kumar Sastry", ""], ["Sullivan", "Michael B.", ""], ["Tsai", "Timothy", ""], ["Keckler", "Stephen W.", ""]]}, {"id": "2006.05016", "submitter": "Peter Vaillancourt", "authors": "Peter Vaillancourt, Bennett Wineholt, Brandon Barker, Plato\n  Deliyannis, Jackie Zheng, Akshay Suresh, Adam Brazier, Rich Knepper, Rich\n  Wolski", "title": "Reproducible and Portable Workflows for Scientific Computing and HPC in\n  the Cloud", "comments": "Accepted for publication in the ACM conference proceedings for\n  Practice and Experience in Advanced Research Computing (PEARC '20)", "journal-ref": null, "doi": "10.1145/3311790.3396659", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of cloud computing services for science has\nchanged the way scientific code can be developed, deployed, and run. Many\nmodern scientific workflows are capable of running on cloud computing\nresources. Consequently, there is an increasing interest in the scientific\ncomputing community in methods, tools, and implementations that enable moving\nan application to the cloud and simplifying the process, and decreasing the\ntime to meaningful scientific results. In this paper, we have applied the\nconcepts of containerization for portability and multi-cloud automated\ndeployment with industry-standard tools to three scientific workflows. We show\nhow our implementations provide reduced complexity to portability of both the\napplications themselves, and their deployment across private and public clouds.\nEach application has been packaged in a Docker container with its dependencies\nand necessary environment setup for production runs. Terraform and Ansible have\nbeen used to automate the provisioning of compute resources and the deployment\nof each scientific application in a Multi-VM cluster. Each application has been\ndeployed on the AWS and Aristotle Cloud Federation platforms. Variation in data\nmanagement constraints, Multi-VM MPI communication, and embarrassingly parallel\ninstance deployments were all explored and reported on. We thus present a\nsample of scientific workflows that can be simplified using the tools and our\nproposed implementation to deploy and run in a variety of cloud environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:29:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Vaillancourt", "Peter", ""], ["Wineholt", "Bennett", ""], ["Barker", "Brandon", ""], ["Deliyannis", "Plato", ""], ["Zheng", "Jackie", ""], ["Suresh", "Akshay", ""], ["Brazier", "Adam", ""], ["Knepper", "Rich", ""], ["Wolski", "Rich", ""]]}, {"id": "2006.05075", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager, Rajeev Muralidhar and Rajkumar Buyya", "title": "Artificial Intelligence (AI)-Centric Management of Resources in Modern\n  Distributed Computing Systems", "comments": "Presented at IEEE cloud summit, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary Distributed Computing Systems (DCS) such as Cloud Data Centres\nare large scale, complex, heterogeneous, and distributed across multiple\nnetworks and geographical boundaries. On the other hand, the Internet of Things\n(IoT)-driven applications are producing a huge amount of data that requires\nreal-time processing and fast response. Managing these resources efficiently to\nprovide reliable services to end-users or applications is a challenging task.\nThe existing Resource Management Systems (RMS) rely on either static or\nheuristic solutions inadequate for such composite and dynamic systems. The\nadvent of Artificial Intelligence (AI) due to data availability and processing\ncapabilities manifested into possibilities of exploring data-driven solutions\nin RMS tasks that are adaptive, accurate, and efficient. In this regard, this\npaper aims to draw the motivations and necessities for data-driven solutions in\nresource management. It identifies the challenges associated with it and\noutlines the potential future research directions detailing where and how to\napply the data-driven techniques in the different RMS tasks. Finally, it\nprovides a conceptual data-driven RMS model for DCS and presents the two\nreal-time use cases (GPU frequency scaling and data centre resource management\nfrom Google Cloud and Microsoft Azure) demonstrating AI-centric approaches'\nfeasibility.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 06:54:07 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 01:47:18 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ilager", "Shashikant", ""], ["Muralidhar", "Rajeev", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2006.05096", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Yuanming Li, Yizheng Huang, Yonggang Wen, Jianxiong\n  Yin and Kyle Guan", "title": "MLModelCI: An Automatic Cloud Platform for Efficient MLaaS", "comments": "4 pages, 4 figures", "journal-ref": "In Proceedings of the 28th ACM International Conference on\n  Multimedia (2020) 4453-4456", "doi": "10.1145/3394171.3414535", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLModelCI provides multimedia researchers and developers with a one-stop\nplatform for efficient machine learning (ML) services. The system leverages\nDevOps techniques to optimize, test, and manage models. It also containerizes\nand deploys these optimized and validated models as cloud services (MLaaS). In\nits essence, MLModelCI serves as a housekeeper to help users publish models.\nThe models are first automatically converted to optimized formats for\nproduction purpose and then profiled under different settings (e.g., batch size\nand hardware). The profiling information can be used as guidelines for\nbalancing the trade-off between performance and cost of MLaaS. Finally, the\nsystem dockerizes the models for ease of deployment to cloud environments. A\nkey feature of MLModelCI is the implementation of a controller, which allows\nelastic evaluation which only utilizes idle workers while maintaining online\nservice quality. Our system bridges the gap between current ML training and\nserving systems and thus free developers from manual and tedious work often\nassociated with service deployment. We release the platform as an open-source\nproject on GitHub under Apache 2.0 license, with the aim that it will\nfacilitate and streamline more large-scale ML applications and research\nprojects.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 07:48:20 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Li", "Yuanming", ""], ["Huang", "Yizheng", ""], ["Wen", "Yonggang", ""], ["Yin", "Jianxiong", ""], ["Guan", "Kyle", ""]]}, {"id": "2006.05117", "submitter": "Huaizheng Zhang", "authors": "Huaizheng Zhang, Yuanming Li, Qiming Ai, Yong Luo, Yonggang Wen,\n  Yichao Jin and Nguyen Binh Duong Ta", "title": "Hysia: Serving DNN-Based Video-to-Retail Applications in Cloud", "comments": "4 pages, 4 figures", "journal-ref": "In Proceedings of the 28th ACM International Conference on\n  Multimedia (2020) 4457-4460", "doi": "10.1145/3394171.3414536", "report-no": null, "categories": "cs.MM cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining \\underline{v}ideo streaming and online \\underline{r}etailing (V2R)\nhas been a growing trend recently. In this paper, we provide practitioners and\nresearchers in multimedia with a cloud-based platform named Hysia for easy\ndevelopment and deployment of V2R applications. The system consists of: 1) a\nback-end infrastructure providing optimized V2R related services including data\nengine, model repository, model serving and content matching; and 2) an\napplication layer which enables rapid V2R application prototyping. Hysia\naddresses industry and academic needs in large-scale multimedia by: 1)\nseamlessly integrating state-of-the-art libraries including NVIDIA video SDK,\nFacebook faiss, and gRPC; 2) efficiently utilizing GPU computation; and 3)\nallowing developers to bind new models easily to meet the rapidly changing deep\nlearning (DL) techniques. On top of that, we implement an orchestrator for\nfurther optimizing DL model serving performance. Hysia has been released as an\nopen source project on GitHub, and attracted considerable attention. We have\npublished Hysia to DockerHub as an official image for seamless integration and\ndeployment in current cloud environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 08:45:53 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Zhang", "Huaizheng", ""], ["Li", "Yuanming", ""], ["Ai", "Qiming", ""], ["Luo", "Yong", ""], ["Wen", "Yonggang", ""], ["Jin", "Yichao", ""], ["Ta", "Nguyen Binh Duong", ""]]}, {"id": "2006.05238", "submitter": "Saehyun Ahn", "authors": "Saehyun Ahn, Jung-Woo Chang, and Suk-Ju Kang", "title": "An Efficient Accelerator Design Methodology for Deformable Convolutional\n  Networks", "comments": "IEEE International Conference on Image Processing (ICIP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable convolutional networks have demonstrated outstanding performance\nin object recognition tasks with an effective feature extraction. Unlike\nstandard convolution, the deformable convolution decides the receptive field\nsize using dynamically generated offsets, which leads to an irregular memory\naccess. Especially, the memory access pattern varies both spatially and\ntemporally, making static optimization ineffective. Thus, a naive\nimplementation would lead to an excessive memory footprint. In this paper, we\npresent a novel approach to accelerate deformable convolution on FPGA. First,\nwe propose a novel training method to reduce the size of the receptive field in\nthe deformable convolutional layer without compromising accuracy. By optimizing\nthe receptive field, we can compress the maximum size of the receptive field by\n12.6 times. Second, we propose an efficient systolic architecture to maximize\nits efficiency. We then implement our design on FPGA to support the optimized\ndataflow. Experimental results show that our accelerator achieves up to 17.25\ntimes speedup over the state-of-the-art accelerator.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:16:44 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 10:40:25 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ahn", "Saehyun", ""], ["Chang", "Jung-Woo", ""], ["Kang", "Suk-Ju", ""]]}, {"id": "2006.05390", "submitter": "David Galindo", "authors": "Marcin Abram, David Galindo, Daniel Honerkamp, Jonathan Ward, Jin-Mann\n  Wong", "title": "Democratising blockchain: A minimal agency consensus model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel consensus protocol based on a hybrid approach, that\ncombines a directed acyclic graph (DAG) and a classical chain of blocks. This\narchitecture allows us to enforce collective block construction, minimising the\nmonopolistic power of the round-leader. In this way, we decrease the\npossibility for collusion among senders and miners, as well as miners\nthemselves, allowing the use of more incentive compatible and fair pricing\nstrategies. We investigate these possibilities alongside the ability to use the\nDAG structure to minimise the risk of transaction censoring. We conclude by\nproviding preliminary benchmarks of our protocol and by exploring further\nresearch directions.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:39:10 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Abram", "Marcin", ""], ["Galindo", "David", ""], ["Honerkamp", "Daniel", ""], ["Ward", "Jonathan", ""], ["Wong", "Jin-Mann", ""]]}, {"id": "2006.05752", "submitter": "Haider Al-Lawati", "authors": "Nuwan Ferdinand, Haider Al-Lawati, Stark C. Draper and Matthew Nokleby", "title": "Anytime MiniBatch: Exploiting Stragglers in Online Distributed\n  Optimization", "comments": "International Conference on Learning Representations (ICLR), May\n  2019, New Orleans, LA, USA", "journal-ref": "Proc. of the 7th Int. Conf. on Learning Representations (ICLR),\n  May 2019, New Orleans, LA, USA", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization is vital in solving large-scale machine learning\nproblems. A widely-shared feature of distributed optimization techniques is the\nrequirement that all nodes complete their assigned tasks in each computational\nepoch before the system can proceed to the next epoch. In such settings, slow\nnodes, called stragglers, can greatly slow progress. To mitigate the impact of\nstragglers, we propose an online distributed optimization method called Anytime\nMinibatch. In this approach, all nodes are given a fixed time to compute the\ngradients of as many data samples as possible. The result is a variable\nper-node minibatch size. Workers then get a fixed communication time to average\ntheir minibatch gradients via several rounds of consensus, which are then used\nto update primal variables via dual averaging. Anytime Minibatch prevents\nstragglers from holding up the system without wasting the work that stragglers\ncan complete. We present a convergence analysis and analyze the wall time\nperformance. Our numerical results show that our approach is up to 1.5 times\nfaster in Amazon EC2 and it is up to five times faster when there is greater\nvariability in compute node performance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 09:53:02 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ferdinand", "Nuwan", ""], ["Al-Lawati", "Haider", ""], ["Draper", "Stark C.", ""], ["Nokleby", "Matthew", ""]]}, {"id": "2006.05808", "submitter": "Joerg Evermann", "authors": "Joerg Evermann", "title": "Adapting Workflow Management Systems to BFT Blockchains -- The YAWL\n  Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology provides an auditable and tamper-proof distributed\nstorage infrastructure for information records. This can be leveraged to\nsupport distributed workflow management. Compared to proof-of-work consensus,\npopularized by Bitcoin and Ethereum, blockchains based on BFT (byzantine fault\ntolerance) ordering consensus trade scalability for immediacy and finality of\nconsensus. This makes them easier to use as distribution infrastructure, as\napplications need not be adapted to deal with eventual consistency and delayed\nconsensus of proof-of-work blockchains. Hence, applications such as workflow\nengines can be easily ported to such a blockchain infrastructure to take\nadvantage of their decentralized integrity assurance and information\ndistribution model. In this paper we describe how the YAWL workflow engine can\nbe used on a BFT based blockchain infrastructure to enable collaborative\nworkflows across different organizations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:55:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Evermann", "Joerg", ""]]}, {"id": "2006.05817", "submitter": "Jie Tang Dr.", "authors": "Hang Zhao and Jie Tang", "title": "A Data Streaming Process Framework for Autonomous Driving By Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the rapid development of sensing technology and the\nInternet of Things (IoT), sensors play increasingly important roles in traffic\ncontrol, medical monitoring, industrial production and etc. They generated high\nvolume of data in a streaming way that often need to be processed in real time.\nTherefore, streaming data computing technology plays an indispensable role in\nthe real-time processing of sensor data in high throughput but low latency. In\nview of the above problems, the proposed framework is implemented on top of\nSpark Streaming, which builds up a gray model based traffic flow monitor, a\ntraffic prediction orientated prediction layer and a fuzzy control based Batch\nInterval dynamic adjustment layer for Spark Streaming. It could forecast the\nvariation of sensors data arrive rate, make streaming Batch Interval adjustment\nin advance and implement real-time streaming process by edge. Therefore, it can\nrealize the monitor and prediction of the data flow changes of the autonomous\ndriving vehicle sensor data in geographical coverage of edge computing node\narea, meanwhile minimize the end-to-end latency but satisfy the application\nthroughput requirements. The experiments show that it can predict short-term\ntraffic with no more than 4% relative error in a whole day. By making batch\nconsuming rate close to data generating rate, it can maintain system stability\nwell even when arrival data rate changes rapidly. The Batch Interval can be\nconverged to a suitable value in two minutes when data arrival rate is doubled.\nCompared with vanilla version Spark Streaming, where there has serious task\naccumulation and introduces large delay, it can reduce 35% latency by squeezing\nBatch Interval when data arrival rate is low; it also can significantly improve\nsystem throughput by only at most 25% Batch Interval increase when data arrival\nrate is high.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 13:04:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zhao", "Hang", ""], ["Tang", "Jie", ""]]}, {"id": "2006.05862", "submitter": "Mathias Bourgoin", "authors": "Mathias Bourgoin, Benjamin Canou, Emmanuel Chailloux, Adrien Jonquet\n  and Philippe Wang", "title": "Objective Caml for Multicore Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Caml is a famous dialect of the ML family languages. It is\nwell-known for its performance as a compiled programming language, notably\nthanks to its incremental generational automatic memory collection. However,\nfor historical reasons, the latter was built for monocore processors. One\nconsequence is the runtime library assumes there is effectively no more than\none thread running at a time, which allows many optimisations for monocore\narchitectures: very few thread mutexes are sufficient to prevent more than a\nsingle thread to run at a time. This makes memory allocation and collection\nquite easier. The way it was built makes it not possible to take advantage of\nnow widespread multicore CPU architectures.\n  This paper presents our feedback on removing Objective Caml's garbage\ncollector and designing a \"Stop-The-World Stop&Copy\" garbage collector to\npermit threads to take advantage of multicore architectures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:38:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bourgoin", "Mathias", ""], ["Canou", "Benjamin", ""], ["Chailloux", "Emmanuel", ""], ["Jonquet", "Adrien", ""], ["Wang", "Philippe", ""]]}, {"id": "2006.05901", "submitter": "Elad Michael Schiller (PhD)", "authors": "Shlomi Dolev, Ariel Hanemann, Elad Micahel Schiller and Shantanu\n  Sharma", "title": "Self-Stabilizing Automatic Repeat Request Algorithms for (Bounded\n  Capacity, Omitting, Duplicating and non-FIFO) Computer Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end communication over the network layer (or data link in overlay\nnetworks) is one of the most important communication tasks in every\ncommunication network, including legacy communication networks as well as\nmobile ad hoc networks, peer-to-peer networks, and mesh networks. Reliable\nend-to-end communications are based on Automatic Repeat reQuest (ARQ)\nalgorithms for dealing with packet failures, such as packet drops. We study ARQ\nalgorithms that exchange packets to deliver (high level) messages in\nfirst-in-first-out (FIFO) order without omissions or duplications. We present a\nself-stabilizing ARQ algorithm that can be applied to networks of bounded\ncapacity that are prone to packet loss, duplication, and reordering. Our\nanalysis considers Lamport's happened-before relation when demonstrating\nstabilization without assuming the presence of a fair scheduler. It shows that\nthe length of the longest chain of Lamport's happened-before relation is 8 for\nany system run.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:41:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Dolev", "Shlomi", ""], ["Hanemann", "Ariel", ""], ["Schiller", "Elad Micahel", ""], ["Sharma", "Shantanu", ""]]}, {"id": "2006.06048", "submitter": "Nikolaos Kallimanis", "authors": "Nikolaos D. Kallimanis, Eleni Kanellou, Charidimos Kiosterakis", "title": "Efficient Partial Snapshot Implementations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the $\\lambda$-scanner snapshot, a variation of the\nsnapshot object, which supports any fixed amount of $0 < \\lambda \\leq n$\ndifferent $SCAN$ operations being active at any given time. Whenever $\\lambda$\nis equal to the number of processes $n$ in the system, the $\\lambda$-scanner\nobject implements a multi-scanner object, while in case that $\\lambda$ is equal\nto $1$, the $\\lambda$-scanner object implements a single-scanner object. We\npresent the $\\lambda-Snap$ snapshot object, a wait-free $\\lambda$-scanner\nsnapshot implementation that has a step complexity of $O(\\lambda)$ for $UPDATE$\noperations and $O(\\lambda m)$ for $SCAN$ operations. The space complexity of\n$\\lambda-Snap$ is $O(\\lambda m)$. $\\lambda-Snap$ provides a trade-off between\nthe step/space complexity and the maximum number of $SCAN$ operations that the\nsystem can afford to be active on any given point in time. The low space\ncomplexity that our implementations provide makes them more appealing in real\nsystem applications. Moreover, we provide a slightly modified version of the\n$\\lambda-Snap$ implementation, which is called partial $\\lambda-Snap$, that is\nable to support dynamic partial scan operations. In such an object, processes\ncan execute modified $SCAN$ operations called $PARTIAL\\_SCAN$ that could obtain\na part of the snapshot object avoiding to read the whole set of components.\n  In this work, we first provide a simple single-scanner version of\n$\\lambda-Snap$, which is called $1-Snap$. We provide $1-Snap$ just for\npresentation purposes, since it is simpler than $\\lambda-Snap$. The $UPDATE$ in\n$1-Snap$ has a step complexity of $O(1)$, while the $SCAN$ has a step\ncomplexity of $O(m)$. This implementation uses $O(m)$ $CAS$ registers.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:10:00 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Kallimanis", "Nikolaos D.", ""], ["Kanellou", "Eleni", ""], ["Kiosterakis", "Charidimos", ""]]}, {"id": "2006.06052", "submitter": "Denis Demidov", "authors": "Denis Demidov, Lin Mu, Bin Wang", "title": "Accelerating linear solvers for Stokes problems with C++ metaprogramming", "comments": null, "journal-ref": null, "doi": "10.1016/j.jocs.2020.101285", "report-no": null, "categories": "cs.MS cs.DC cs.DS physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient solution of large sparse saddle point systems is very important\nin computational fluid mechanics. The discontinuous Galerkin finite element\nmethods have become increasingly popular for incompressible flow problems but\ntheir application is limited due to high computational cost. We describe the\nC++ programming techniques that may help to accelerate linear solvers for such\nproblems. The approach is based on the policy-based design pattern and partial\ntemplate specialization, and is implemented in the open source AMGCL library.\nThe efficiency is demonstrated with the example of accelerating an iterative\nsolver of a discontinuous Galerkin finite element method for the Stokes\nproblem. The implementation allows selecting algorithmic components of the\nsolver by adjusting template parameters without any changes to the codebase. It\nis possible to switch the system matrix to use small statically sized blocks to\nstore the nonzero values, or use a mixed precision solution, which results in\nup to 4 times speedup, and reduces the memory footprint of the algorithm by\nabout 40\\%. We evaluate both monolithic and composite preconditioning\nstrategies for the 3 benchmark problems. The performance of the proposed\nsolution is compared with a multithreaded direct Pardiso solver and a parallel\niterative PETSc solver.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 20:20:05 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 15:39:23 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 06:02:43 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Demidov", "Denis", ""], ["Mu", "Lin", ""], ["Wang", "Bin", ""]]}, {"id": "2006.06070", "submitter": "Mohammad Saidur Rahman", "authors": "Mohammad Saidur Rahman", "title": "Optimizing Smart Grid Aggregators and Measuring Degree of Privacy in a\n  Distributed Trust Based Anonymous Aggregation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A smart grid is an advanced method for supplying electricity to the consumers\nalleviating the limitations of the existing system. It causes frequent meter\nreading transmission from the end-user to the supplier. This frequent data\ntransmission poses privacy risks. Several works have been proposed to solve\nthis problem but cannot ensure privacy at the optimal level. This work is based\non a distributed trust-based data aggregation system leveraging a secret\nsharing mechanism. In this work, we show that {\\em three aggregators} are\nenough for ensuring consumer's privacy in a distributed trust-based system. We\nleverage the idea of anonymity in our research and show that neither an active\nattacker nor a passive attacker can breach consumer's privacy. We show proof of\nour concept mathematically and in a cryptographic game based mechanism. We name\nour new proposed system \\emph{\"Distributed Trust Based Anonymous System\n(DTBAS)\"}.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:20:30 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Rahman", "Mohammad Saidur", ""]]}, {"id": "2006.06296", "submitter": "Felix Lorenz", "authors": "Felix Lorenz, Lauritz Thamsen, Andreas Wilke, Ilja Behnke, Jens\n  Waldm\\\"uller-Littke, Ilya Komarov, Odej Kao, Manfred Paeschke", "title": "Fingerprinting Analog IoT Sensors for Secret-Free Authentication", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Especially in context of critical urban infrastructures, trust in IoT data is\nof utmost importance. While most technology stacks provide means for\nauthentication and encryption of device-to-cloud traffic, there are currently\nno mechanisms to rule out physical tampering with an IoT device's sensors.\nAddressing this gap, we introduce a new method for extracting a hardware\nfingerprint of an IoT sensor which can be used for secret-free authentication.\nBy comparing the fingerprint against reference measurements recorded prior to\ndeployment, we can tell whether the sensing hardware connected to the IoT\ndevice has been changed by environmental effects or with malicious intent. Our\napproach exploits the characteristic behavior of analog circuits, which is\nrevealed by applying a fixed-frequency alternating current to the sensor, while\nrecording its output voltage. To demonstrate the general feasibility of our\nmethod, we apply it to four commercially available temperature sensors using\nlaboratory equipment and evaluate the accuracy. The results indicate that with\na sensible configuration of the two hyperparameters we can identify individual\nsensors with high probability, using only a few recordings from the target\ndevice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:03:24 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Lorenz", "Felix", ""], ["Thamsen", "Lauritz", ""], ["Wilke", "Andreas", ""], ["Behnke", "Ilja", ""], ["Waldm\u00fcller-Littke", "Jens", ""], ["Komarov", "Ilya", ""], ["Kao", "Odej", ""], ["Paeschke", "Manfred", ""]]}, {"id": "2006.06377", "submitter": "Shuheng Shen", "authors": "Shuheng Shen, Yifei Cheng, Jingchang Liu and Linli Xu", "title": "STL-SGD: Speeding Up Local SGD with Stagewise Communication Period", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed parallel stochastic gradient descent algorithms are workhorses\nfor large scale machine learning tasks. Among them, local stochastic gradient\ndescent (Local SGD) has attracted significant attention due to its low\ncommunication complexity. Previous studies prove that the communication\ncomplexity of Local SGD with a fixed or an adaptive communication period is in\nthe order of $O (N^{\\frac{3}{2}} T^{\\frac{1}{2}})$ and $O (N^{\\frac{3}{4}}\nT^{\\frac{3}{4}})$ when the data distributions on clients are identical (IID) or\notherwise (Non-IID), where $N$ is the number of clients and $T$ is the number\nof iterations. In this paper, to accelerate the convergence by reducing the\ncommunication complexity, we propose \\textit{ST}agewise \\textit{L}ocal\n\\textit{SGD} (STL-SGD), which increases the communication period gradually\nalong with decreasing learning rate. We prove that STL-SGD can keep the same\nconvergence rate and linear speedup as mini-batch SGD. In addition, as the\nbenefit of increasing the communication period, when the objective is strongly\nconvex or satisfies the Polyak-\\L ojasiewicz condition, the communication\ncomplexity of STL-SGD is $O (N \\log{T})$ and $O (N^{\\frac{1}{2}}\nT^{\\frac{1}{2}})$ for the IID case and the Non-IID case respectively, achieving\nsignificant improvements over Local SGD. Experiments on both convex and\nnon-convex problems demonstrate the superior performance of STL-SGD.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 12:48:17 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 15:19:54 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Shen", "Shuheng", ""], ["Cheng", "Yifei", ""], ["Liu", "Jingchang", ""], ["Xu", "Linli", ""]]}, {"id": "2006.06513", "submitter": "Klaus-Tycho Foerster", "authors": "Klaus-Tycho Foerster, Juho Hirvonen, Yvonne-Anne Pignolet, Stefan\n  Schmid, Gilles Tredan", "title": "On the Feasibility of Perfect Resilience with Local Fast Failover", "comments": "To appear in the proceedings of the 2nd Symposium on Algorithmic\n  Principles of Computer Systems (APOCS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to provide a high resilience and to react quickly to link failures,\nmodern computer networks support fully decentralized flow rerouting, also known\nas local fast failover. In a nutshell, the task of a local fast failover\nalgorithm is to pre-define fast failover rules for each node using locally\navailable information only. These rules determine for each incoming link from\nwhich a packet may arrive and the set of local link failures (i.e., the failed\nlinks incident to a node), on which outgoing link a packet should be forwarded.\nIdeally, such a local fast failover algorithm provides a perfect resilience\ndeterministically: a packet emitted from any source can reach any target, as\nlong as the underlying network remains connected. Feigenbaum et al. (ACM PODC\n2012) and also Chiesa et al. (IEEE/ACM Trans. Netw. 2017) showed that it is not\nalways possible to provide perfect resilience. Interestingly, not much more is\nknown currently about the feasibility of perfect resilience.\n  This paper revisits perfect resilience with local fast failover, both in a\nmodel where the source can and cannot be used for forwarding decisions. We\nfirst derive several fairly general impossibility results: By establishing a\nconnection between graph minors and resilience, we prove that it is impossible\nto achieve perfect resilience on any non-planar graph; furthermore, while\nplanarity is necessary, it is also not sufficient for perfect resilience. On\nthe positive side, we show that graph families closed under link subdivision\nallow for simple and efficient failover algorithms which simply skip failed\nlinks. We demonstrate this technique by deriving perfect resilience for\nouterplanar graphs and related scenarios, as well as for scenarios where the\nsource and target are topologically close after failures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:30:08 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:55:15 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Foerster", "Klaus-Tycho", ""], ["Hirvonen", "Juho", ""], ["Pignolet", "Yvonne-Anne", ""], ["Schmid", "Stefan", ""], ["Tredan", "Gilles", ""]]}, {"id": "2006.06608", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Gushu Li, Shuangchen Li, Lei Deng, Yuan Xie,\n  Yufei Ding", "title": "GNNAdvisor: An Adaptive and Efficient Runtime System for GNN\n  Acceleration on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the emerging trend of graph-based deep learning, Graph Neural Networks\n(GNNs) excel for their capability to generate high-quality node feature vectors\n(embeddings). However, the existing one-size-fits-all GNN implementations are\ninsufficient to catch up with the evolving GNN architectures, the\never-increasing graph sizes, and the diverse node embedding dimensionalities.\nTo this end, we propose \\textbf{GNNAdvisor}, an adaptive and efficient runtime\nsystem to accelerate various GNN workloads on GPU platforms. First, GNNAdvisor\nexplores and identifies several performance-relevant features from both the GNN\nmodel and the input graph, and uses them as a new driving force for GNN\nacceleration. Second, GNNAdvisor implements a novel and highly-efficient 2D\nworkload management, tailored for GNN computation to improve GPU utilization\nand performance under different application settings. Third, GNNAdvisor\ncapitalizes on the GPU memory hierarchy for acceleration by gracefully\ncoordinating the execution of GNNs according to the characteristics of the GPU\nmemory structure and GNN workloads. Furthermore, to enable automatic runtime\noptimization, GNNAdvisor incorporates a lightweight analytical model for an\neffective design parameter search. Extensive experiments show that GNNAdvisor\noutperforms the state-of-the-art GNN computing frameworks, such as Deep Graph\nLibrary ($3.02\\times$ faster on average) and NeuGraph (up to $4.10\\times$\nfaster), on mainstream GNN architectures across various datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:58:10 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 06:12:57 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 04:49:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Li", "Gushu", ""], ["Li", "Shuangchen", ""], ["Deng", "Lei", ""], ["Xie", "Yuan", ""], ["Ding", "Yufei", ""]]}, {"id": "2006.06771", "submitter": "Xing Hu", "authors": "Vassos Hadzilacos, Xing Hu, Sam Toueg", "title": "Randomized Consensus with Regular Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known randomized consensus algorithm by Aspnes and Herlihy for\nasynchronous shared-memory systems was proved to work, even against a strong\nadversary, under the assumption that the registers that it uses are atomic\nregisters. With atomic registers, every read or write operation is\ninstantaneous (and thus indivisible). As pointed out by Golab et al. (2011),\nhowever, a randomized algorithm that works with atomic registers does not\nnecessarily work if we replace the atomic registers that it uses with\nlinearizable implementations of registers.\n  This raises the following question: does the randomized consensus algorithm\nby Aspnes and Herlihy still work against a strong adversary if we replace its\natomic registers with linearizable registers? We show that the answer is\naffirmative, in fact, we show that even linearizable registers are not\nnecessary. More precisely, we prove that the algorithm by Aspnes and Herlihy\nworks against a strong adversary even if the algorithm uses only regular\nregisters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:49:46 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Hadzilacos", "Vassos", ""], ["Hu", "Xing", ""], ["Toueg", "Sam", ""]]}, {"id": "2006.06775", "submitter": "Lukas Breitwieser", "authors": "Lukas Breitwieser, Ahmad Hesam, Jean de Montigny, Vasileios\n  Vavourakis, Alexandros Iosif, Jack Jennings, Marcus Kaiser, Marco Manca,\n  Alberto Di Meglio, Zaid Al-Ars, Fons Rademakers, Onur Mutlu, Roman Bauer", "title": "BioDynaMo: a general platform for scalable agent-based simulation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Agent-based modeling is an indispensable tool for studying\ncomplex biological systems. However, existing simulators do not always take\nfull advantage of modern hardware and often have a field-specific software\ndesign.\n  Results: We present a novel simulation platform called BioDynaMo that\nalleviates both of these problems. BioDynaMo features a general-purpose and\nhigh-performance simulation engine. We demonstrate that BioDynaMo can be used\nto simulate use cases in: neuroscience, oncology, and epidemiology. For each\nuse case we validate our findings with experimental data or an analytical\nsolution. Our performance results show that BioDynaMo performs up to three\norders of magnitude faster than the state-of-the-art baseline. This improvement\nmakes it feasible to simulate each use case with one billion agents on a single\nserver, showcasing the potential BioDynaMo has for computational biology\nresearch.\n  Availability: BioDynaMo is an open-source project under the Apache 2.0\nlicense and is available at www.biodynamo.org. Instructions to reproduce the\nresults are available in supplementary information.\n  Contact: lukas.breitwieser@inf.ethz.ch, a.s.hesam@tudelft.nl, omutlu@ethz.ch,\nr.bauer@surrey.ac.uk\n  Supplementary information: Available at\nhttps://doi.org/10.5281/zenodo.4501515\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:55:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 13:24:41 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Breitwieser", "Lukas", ""], ["Hesam", "Ahmad", ""], ["de Montigny", "Jean", ""], ["Vavourakis", "Vasileios", ""], ["Iosif", "Alexandros", ""], ["Jennings", "Jack", ""], ["Kaiser", "Marcus", ""], ["Manca", "Marco", ""], ["Di Meglio", "Alberto", ""], ["Al-Ars", "Zaid", ""], ["Rademakers", "Fons", ""], ["Mutlu", "Onur", ""], ["Bauer", "Roman", ""]]}, {"id": "2006.06890", "submitter": "Seung Won Min", "authors": "Seung Won Min, Vikram Sharma Mailthody, Zaid Qureshi, Jinjun Xiong,\n  Eiman Ebrahimi, Wen-mei Hwu", "title": "EMOGI: Efficient Memory-access for Out-of-memory Graph-traversal In GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern analytics and recommendation systems are increasingly based on graph\ndata that capture the relations between entities being analyzed. Practical\ngraphs come in huge sizes, offer massive parallelism, and are stored in\nsparse-matrix formats such as CSR. To exploit the massive parallelism,\ndevelopers are increasingly interested in using GPUs for graph traversal.\nHowever, due to their sizes, graphs often do not fit into the GPU memory. Prior\nworks have either used input data pre-processing/partitioning or UVM to migrate\nchunks of data from the host memory to the GPU memory. However, the large,\nmulti-dimensional, and sparse nature of graph data presents a major challenge\nto these schemes and results in significant amplification of data movement and\nreduced effective data throughput. In this work, we propose EMOGI, an\nalternative approach to traverse graphs that do not fit in GPU memory using\ndirect cacheline-sized access to data stored in host memory. This paper\naddresses the open question of whether a sufficiently large number of\noverlapping cacheline-sized accesses can be sustained to 1) tolerate the long\nlatency to host memory, 2) fully utilize the available bandwidth, and 3)\nachieve favorable execution performance. We analyze the data access patterns of\nseveral graph traversal applications in GPU over PCIe using an FPGA to\nunderstand the cause of poor external bandwidth utilization. By carefully\ncoalescing and aligning external memory requests, we show that we can minimize\nthe number of PCIe transactions and nearly fully utilize the PCIe bandwidth\neven with direct cache-line accesses to the host memory. EMOGI achieves\n2.92$\\times$ speedup on average compared to the optimized UVM implementations\nin various graph traversal applications. We also show that EMOGI scales better\nthan a UVM-based solution when the system uses higher bandwidth interconnects\nsuch as PCIe 4.0.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 00:37:51 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 23:04:37 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Min", "Seung Won", ""], ["Mailthody", "Vikram Sharma", ""], ["Qureshi", "Zaid", ""], ["Xiong", "Jinjun", ""], ["Ebrahimi", "Eiman", ""], ["Hwu", "Wen-mei", ""]]}, {"id": "2006.06943", "submitter": "Rajkumar Buyya", "authors": "Adarsh Kumar, Kriti Sharma, Harvinder Singh, Sagar Gupta Naugriya,\n  Sukhpal Singh Gill, and Rajkumar Buyya", "title": "A Drone-based Networked System and Methods for Combating Coronavirus\n  Disease (COVID-19) Pandemic", "comments": "25 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease (COVID-19) is an infectious disease caused by a newly\ndiscovered coronavirus. It is similar to influenza viruses and raises concerns\nthrough alarming levels of spread and severity resulting in an ongoing pandemic\nworldwide. Within eight months (by August 2020), it infected 24.0 million\npersons worldwide and over 824 thousand have died. Drones or Unmanned Aerial\nVehicles (UAVs) are very helpful in handling the COVID-19 pandemic. This work\ninvestigates the drone-based systems, COVID-19 pandemic situations, and\nproposes an architecture for handling pandemic situations in different\nscenarios using real-time and simulation-based scenarios. The proposed\narchitecture uses wearable sensors to record the observations in Body Area\nNetworks (BANs) in a push-pull data fetching mechanism. The proposed\narchitecture is found to be useful in remote and highly congested pandemic\nareas where either the wireless or Internet connectivity is a major issue or\nchances of COVID-19 spreading are high. It collects and stores the substantial\namount of data in a stipulated period and helps to take appropriate action as\nand when required. In real-time drone-based healthcare system implementation\nfor COVID-19 operations, it is observed that a large area can be covered for\nsanitization, thermal image collection, and patient identification within a\nshort period (2 KMs within 10 minutes approx.) through aerial route. In the\nsimulation, the same statistics are observed with an addition of\ncollision-resistant strategies working successfully for indoor and outdoor\nhealthcare operations. Further, open challenges are identified and promising\nresearch directions are highlighted.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 04:48:29 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 04:45:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kumar", "Adarsh", ""], ["Sharma", "Kriti", ""], ["Singh", "Harvinder", ""], ["Naugriya", "Sagar Gupta", ""], ["Gill", "Sukhpal Singh", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2006.06983", "submitter": "Chengxu Yang", "authors": "Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian,\n  Yunxin Liu, Xuanzhe Liu", "title": "Characterizing Impacts of Heterogeneity in Federated Learning upon\n  Large-Scale Smartphone Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is an emerging, privacy-preserving machine learning\nparadigm, drawing tremendous attention in both academia and industry. A unique\ncharacteristic of FL is heterogeneity, which resides in the various hardware\nspecifications and dynamic states across the participating devices.\nTheoretically, heterogeneity can exert a huge influence on the FL training\nprocess, e.g., causing a device unavailable for training or unable to upload\nits model updates. Unfortunately, these impacts have never been systematically\nstudied and quantified in existing FL literature.\n  In this paper, we carry out the first empirical study to characterize the\nimpacts of heterogeneity in FL. We collect large-scale data from 136k\nsmartphones that can faithfully reflect heterogeneity in real-world settings.\nWe also build a heterogeneity-aware FL platform that complies with the standard\nFL protocol but with heterogeneity in consideration. Based on the data and the\nplatform, we conduct extensive experiments to compare the performance of\nstate-of-the-art FL algorithms under heterogeneity-aware and\nheterogeneity-unaware settings. Results show that heterogeneity causes\nnon-trivial performance degradation in FL, including up to 9.2% accuracy drop,\n2.32x lengthened training time, and undermined fairness. Furthermore, we\nanalyze potential impact factors and find that device failure and participant\nbias are two potential factors for performance degradation. Our study provides\ninsightful implications for FL practitioners. On the one hand, our findings\nsuggest that FL algorithm designers consider necessary heterogeneity during the\nevaluation. On the other hand, our findings urge system providers to design\nspecific mechanisms to mitigate the impacts of heterogeneity.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:49:21 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 13:20:53 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 05:38:49 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 08:45:11 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Yang", "Chengxu", ""], ["Wang", "Qipeng", ""], ["Xu", "Mengwei", ""], ["Chen", "Zhenpeng", ""], ["Bian", "Kaigui", ""], ["Liu", "Yunxin", ""], ["Liu", "Xuanzhe", ""]]}, {"id": "2006.07026", "submitter": "Chien-Lun Chen", "authors": "Chien-Lun Chen, Leana Golubchik, Marco Paolieri", "title": "Backdoor Attacks on Federated Meta-Learning", "comments": "13 pages, 19 figures, NeurIPS Workshop on Scalability, Privacy, and\n  Security in Federated Learning (NeurIPS-SpicyFL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning allows multiple users to collaboratively train a shared\nclassification model while preserving data privacy. This approach, where model\nupdates are aggregated by a central server, was shown to be vulnerable to\npoisoning backdoor attacks: a malicious user can alter the shared model to\narbitrarily classify specific inputs from a given class. In this paper, we\nanalyze the effects of backdoor attacks on federated meta-learning, where users\ntrain a model that can be adapted to different sets of output classes using\nonly a few examples. While the ability to adapt could, in principle, make\nfederated learning frameworks more robust to backdoor attacks (when new\ntraining examples are benign), we find that even 1-shot~attacks can be very\nsuccessful and persist after additional training. To address these\nvulnerabilities, we propose a defense mechanism inspired by matching networks,\nwhere the class of an input is predicted from the similarity of its features\nwith a support set of labeled examples. By removing the decision logic from the\nmodel shared with the federation, success and persistence of backdoor attacks\nare greatly reduced.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 09:23:24 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 16:15:58 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Chen", "Chien-Lun", ""], ["Golubchik", "Leana", ""], ["Paolieri", "Marco", ""]]}, {"id": "2006.07066", "submitter": "Rosa M Badia", "authors": "Rosa M Badia, Jorge Ejarque, Francesc Lordan, Daniele Lezzi, Javier\n  Conejero, Javier \\'Alvarez Cid-Fuentes, Yolanda Becerra, and Anna Queralt", "title": "Workflow environments for advanced cyberinfrastructure platforms", "comments": "10 pages, 6 figures, in proceedings of 2019 IEEE 39th International\n  Conference on Distributed Computing Systems (ICDCS)", "journal-ref": "Proceedings of 2019 IEEE 39th International Conference on\n  Distributed Computing Systems (ICDCS)", "doi": "10.1109/ICDCS.2019.00171", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in science is deeply bound to the effective use of high-performance\ncomputing infrastructures and to the efficient extraction of knowledge from\nvast amounts of data. Such data comes from different sources that follow a\ncycle composed of pre-processing steps for data curation and preparation for\nsubsequent computing steps, and later analysis and analytics steps applied to\nthe results. However, scientific workflows are currently fragmented in multiple\ncomponents, with different processes for computing and data management, and\nwith gaps in the viewpoints of the user profiles involved. Our vision is that\nfuture workflow environments and tools for the development of scientific\nworkflows should follow a holistic approach, where both data and computing are\nintegrated in a single flow built on simple, high-level interfaces. The topics\nof research that we propose involve novel ways to express the workflows that\nintegrate the different data and compute processes, dynamic runtimes to support\nthe execution of the workflows in complex and heterogeneous computing\ninfrastructures in an efficient way, both in terms of performance and energy.\nThese infrastructures include highly distributed resources, from sensors and\ninstruments, and devices in the edge, to High-Performance Computing and Cloud\ncomputing resources. This paper presents our vision to develop these workflow\nenvironments and also the steps we are currently following to achieve it.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:41:48 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Badia", "Rosa M", ""], ["Ejarque", "Jorge", ""], ["Lordan", "Francesc", ""], ["Lezzi", "Daniele", ""], ["Conejero", "Javier", ""], ["Cid-Fuentes", "Javier \u00c1lvarez", ""], ["Becerra", "Yolanda", ""], ["Queralt", "Anna", ""]]}, {"id": "2006.07086", "submitter": "Sahil Dhoked", "authors": "Sahil Dhoked and Neeraj Mittal", "title": "An Adaptive Approach to Recoverable Mutual Exlcusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual exclusion (ME) is one of the most commonly used techniques to handle\nconflicts in concurrent systems. Traditionally, mutual exclusion algorithms\nhave been designed under the assumption that a process does not fail while\nacquiring/releasing a lock or while executing its critical section. However,\nfailures do occur in real life, potentially leaving the lock in an inconsistent\nstate. This gives rise to the problem of \\emph{recoverable mutual exclusion\n(RME)} that involves designing a mutual exclusion algorithm that can tolerate\nfailures, while maintaining safety and liveness properties.\n  One of the important measures of performance of any ME algorithm, including\nan RME algorithm, is the number of \\emph{remote memory references (RMRs)} made\nby a process (for acquiring and releasing a lock as well as recovering the lock\nstructure after a failure). The best known RME algorithm solves the problem for\n$n$ processes in sub-logarithmic number of RMRs, given by\n$\\mathcal{O}(\\frac{\\log n}{\\log \\log n})$, irrespective of the number of\nfailures in the system.\n  In this work, we present a new algorithm for solving the RME problem whose\nRMR complexity gradually \\emph{adapts} to the number of failures that have\noccurred in the system \"recently\". In the absence of failures, our algorithm\ngenerates only $\\mathcal{O}(1)$ RMRs. Furthermore, its RMR complexity is given\nby $\\mathcal{O}(\\min\\{ \\sqrt{F}, \\frac{\\log n}{\\log \\log n} \\})$ where $F$ is\nthe total number of failures in the \"recent\" past. In addition to read and\nwrite instructions, our algorithm uses compare-and-swap (\\CAS{}) and\nfetch-and-store (\\FAS{}) hardware instructions, both of which are commonly\navailable in most modern processors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 11:18:04 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 06:04:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dhoked", "Sahil", ""], ["Mittal", "Neeraj", ""]]}, {"id": "2006.07163", "submitter": "Wolfgang John", "authors": "Mina Sedaghat, Pontus Sk\\\"oldstr\\\"om, Daniel Turull, Vinay Yadhav,\n  Joacim Hal\\'en, Madhubala Ganesan, Amardeep Mehta and Wolfgang John", "title": "Nefele: Process Orchestration for the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtualization, either at OS- or hardware level, plays an important role in\ncloud computing. It enables easier automation and faster deployment in\ndistributed environments. While virtualized infrastructures provide a level of\nmanagement flexibility, they lack practical abstraction of the distributed\nresources. A developer in such an environment still needs to deal with all the\ncomplications of building a distributed software system. Different\norchestration systems are built to provide that abstraction; however, they do\nnot solve the inherent challenges of distributed systems, such as\nsynchronization issues or resilience to failures. This paper introduces Nefele,\na decentralized process orchestration system that automatically deploys and\nmanages individual processes, rather than containers/VMs, within a cluster.\nNefele is inspired by the Single System Image (SSI) vision of mitigating the\nintricacies of remote execution, yet it maintains the flexibility and\nperformance of virtualized infrastructures. Nefele offers a set of APIs for\nbuilding cloud-native applications that lets the developer easily build,\ndeploy, and scale applications in a cloud environment. We have implemented and\ndeployed Nefele on a cluster in our datacenter and evaluated its performance.\nOur evaluations show that Nefele can effectively deploy, scale, and monitor\nprocesses across a distributed environment, while it incorporates essential\nprimitives to build a distributed software system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:21:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 08:32:14 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Sedaghat", "Mina", ""], ["Sk\u00f6ldstr\u00f6m", "Pontus", ""], ["Turull", "Daniel", ""], ["Yadhav", "Vinay", ""], ["Hal\u00e9n", "Joacim", ""], ["Ganesan", "Madhubala", ""], ["Mehta", "Amardeep", ""], ["John", "Wolfgang", ""]]}, {"id": "2006.07218", "submitter": "Aur\\'elien Bellet", "authors": "C\\'esar Sabater, Aur\\'elien Bellet, Jan Ramon", "title": "Distributed Differentially Private Averaging with Improved Utility and\n  Robustness to Malicious Parties", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from data owned by several parties, as in federated learning, raises\nchallenges regarding the privacy guarantees provided to participants and the\ncorrectness of the computation in the presence of malicious parties. We tackle\nthese challenges in the context of distributed averaging, an essential building\nblock of distributed and federated learning. Our first contribution is a novel\ndistributed differentially private protocol which naturally scales with the\nnumber of parties. The key idea underlying our protocol is to exchange\ncorrelated Gaussian noise along the edges of a network graph, complemented by\nindependent noise added by each party. We analyze the differential privacy\nguarantees of our protocol and the impact of the graph topology, showing that\nwe can match the accuracy of the trusted curator model even when each party\ncommunicates with only a logarithmic number of other parties chosen at random.\nThis is in contrast with protocols in the local model of privacy (with lower\naccuracy) or based on secure aggregation (where all pairs of users need to\nexchange messages). Our second contribution is to enable users to prove the\ncorrectness of their computations without compromising the efficiency and\nprivacy guarantees of the protocol. Our construction relies on standard\ncryptographic primitives like commitment schemes and zero knowledge proofs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:21:10 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Sabater", "C\u00e9sar", ""], ["Bellet", "Aur\u00e9lien", ""], ["Ramon", "Jan", ""]]}, {"id": "2006.07273", "submitter": "Georgios Damaskinos", "authors": "Georgios Damaskinos, Rachid Guerraoui, Anne-Marie Kermarrec, Vlad\n  Nitu, Rhicheek Patra, Francois Taiani", "title": "FLeet: Online Federated Learning via Staleness Awareness and Performance\n  Prediction", "comments": null, "journal-ref": null, "doi": "10.1145/3423211.3425685", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is very appealing for its privacy benefits:\nessentially, a global model is trained with updates computed on mobile devices\nwhile keeping the data of users local. Standard FL infrastructures are however\ndesigned to have no energy or performance impact on mobile devices, and are\ntherefore not suitable for applications that require frequent (online) model\nupdates, such as news recommenders.\n  This paper presents FLeet, the first Online FL system, acting as a middleware\nbetween the Android OS and the machine learning application. FLeet combines the\nprivacy of Standard FL with the precision of online learning thanks to two core\ncomponents: (i) I-Prof, a new lightweight profiler that predicts and controls\nthe impact of learning tasks on mobile devices, and (ii) AdaSGD, a new adaptive\nlearning algorithm that is resilient to delayed updates.\n  Our extensive evaluation shows that Online FL, as implemented by FLeet, can\ndeliver a 2.3x quality boost compared to Standard FL, while only consuming\n0.036% of the battery per day. I-Prof can accurately control the impact of\nlearning tasks by improving the prediction accuracy up to 3.6x (computation\ntime) and up to 19x (energy). AdaSGD outperforms alternative FL approaches by\n18.4% in terms of convergence speed on heterogeneous data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 15:43:38 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 11:19:50 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Damaskinos", "Georgios", ""], ["Guerraoui", "Rachid", ""], ["Kermarrec", "Anne-Marie", ""], ["Nitu", "Vlad", ""], ["Patra", "Rhicheek", ""], ["Taiani", "Francois", ""]]}, {"id": "2006.07357", "submitter": "Rolando Garcia", "authors": "Rolando Garcia, Eric Liu, Vikram Sreekanti, Bobby Yan, Anusha\n  Dandamudi, Joseph E. Gonzalez, Joseph M. Hellerstein, Koushik Sen", "title": "Hindsight Logging for Model Training", "comments": null, "journal-ref": null, "doi": "10.14778/3436905.3436925", "report-no": null, "categories": "cs.DC cs.DB cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In modern Machine Learning, model training is an iterative, experimental\nprocess that can consume enormous computation resources and developer time. To\naid in that process, experienced model developers log and visualize program\nvariables during training runs. Exhaustive logging of all variables is\ninfeasible. Optimistic logging can be accompanied by program checkpoints; this\nallows developers to add log statements post-hoc, and \"replay\" desired log\nstatements from checkpoint -- a process we refer to as hindsight logging.\nUnfortunately, hindsight logging raises tricky problems in data management and\nsoftware engineering. Done poorly, hindsight logging can waste resources and\ngenerate technical debt embodied in multiple variants of training code.\n  In this paper, we present methodologies for efficient and effective logging\npractices for model training, with a focus on techniques for hindsight logging.\nOur goal is for experienced model developers to learn and adopt these\npractices. To make this easier, we provide an open-source suite of tools for\nFast Low-Overhead Recovery (flor) that embodies our design across three tasks:\n(i) efficient background logging in Python, (ii) adaptable periodic\ncheckpointing, and (iii) an instrumentation library that codifies hindsight\nlogging for efficient and automatic record-replay of model-training. Model\ndevelopers can use each flor tool separately as they see fit, or they can use\nflor in hands-free mode, entrusting it to instrument their code end-to-end for\nefficient record-replay. Our solutions leverage techniques from physiological\ntransaction logs and recovery in database systems. Evaluations on modern ML\nbenchmarks demonstrate that flor can produce fast checkpointing with small\nuser-specifiable overheads (e.g. 7%), and still provide hindsight log replay\ntimes orders of magnitude faster than restarting training from scratch.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 17:47:32 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 05:14:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Garcia", "Rolando", ""], ["Liu", "Eric", ""], ["Sreekanti", "Vikram", ""], ["Yan", "Bobby", ""], ["Dandamudi", "Anusha", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""], ["Sen", "Koushik", ""]]}, {"id": "2006.07402", "submitter": "Umair Mohammad", "authors": "Umair Mohammad, Sameh Sorour and Mohamed Hefeida", "title": "Jointly Optimizing Dataset Size and Local Updates in Heterogeneous\n  Mobile Edge Learning", "comments": "7 pages, 3 figures, This paper has been submitted to the IEEE for\n  possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes to maximize the accuracy of a distributed machine\nlearning (ML) model trained on learners connected via the resource-constrained\nwireless edge. We jointly optimize the number of local/global updates and the\ntask size allocation to minimize the loss while taking into account\nheterogeneous communication and computation capabilities of each learner. By\nleveraging existing bounds on the difference between the training loss at any\ngiven iteration and the theoretically optimal loss, we derive an expression for\nthe objective function in terms of the number of local updates. The resulting\nconvex program is solved to obtain the optimal number of local updates which is\nused to obtain the total updates and batch sizes for each learner. The merits\nof the proposed solution, which is heterogeneity aware (HA), are exhibited by\ncomparing its performance to the heterogeneity unaware (HU) approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:19:20 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 17:30:57 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 05:17:27 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mohammad", "Umair", ""], ["Sorour", "Sameh", ""], ["Hefeida", "Mohamed", ""]]}, {"id": "2006.07405", "submitter": "Subhadeep Bhattacharya", "authors": "Subhadeep Bhattacharya, Weikuan Yu and Fahim Tahmid Chowdhury", "title": "O(1) Communication for Distributed SGD through Two-Level Gradient\n  Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large neural network models present a hefty communication challenge to\ndistributed Stochastic Gradient Descent (SGD), with a communication complexity\nof O(n) per worker for a model of n parameters. Many sparsification and\nquantization techniques have been proposed to compress the gradients, some\nreducing the communication complexity to O(k), where k << n. In this paper, we\nintroduce a strategy called two-level gradient averaging (A2SGD) to consolidate\nall gradients down to merely two local averages per worker before the\ncomputation of two global averages for an updated model. A2SGD also retains\nlocal errors to maintain the variance for fast convergence. Our theoretical\nanalysis shows that A2SGD converges similarly like the default distributed SGD\nalgorithm. Our evaluation validates the theoretical conclusion and demonstrates\nthat A2SGD significantly reduces the communication traffic per worker, and\nimproves the overall training time of LSTM-PTB by 3.2x and 23.2x, respectively,\ncompared to Top-K and QSGD. To the best of our knowledge, A2SGD is the first to\nachieve O(1) communication complexity per worker for distributed SGD.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 18:20:52 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 01:49:28 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bhattacharya", "Subhadeep", ""], ["Yu", "Weikuan", ""], ["Chowdhury", "Fahim Tahmid", ""]]}, {"id": "2006.07449", "submitter": "Soumyottam Chatterjee", "authors": "Soumyottam Chatterjee and Robert Gmyr and Gopal Pandurangan", "title": "Sleeping is Efficient: MIS in $O(1)$-rounds Node-averaged Awake\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal Independent Set (MIS) is one of the fundamental problems in\ndistributed computing. The round (time) complexity of distributed MIS has\ntraditionally focused on the \\emph{worst-case time} for all nodes to finish.\nThe best-known (randomized) MIS algorithms take $O(\\log{n})$ worst-case rounds\non general graphs (where $n$ is the number of nodes). Motivated by the goal to\nreduce \\emph{total} energy consumption in energy-constrained networks such as\nsensor and ad hoc wireless networks, we take an alternative approach to\nmeasuring performance. We focus on minimizing the total (or equivalently, the\n\\emph{average}) time for all nodes to finish. It is not clear whether the\ncurrently best-known algorithms yield constant-round (or even $o(\\log{n})$)\nnode-averaged round complexity for MIS in general graphs. We posit the\n\\emph{sleeping model}, a generalization of the traditional model, that allows\nnodes to enter either ``sleep'' or ``waking'' states at any round. While waking\nstate corresponds to the default state in the traditional model, in sleeping\nstate a node is ``offline'', i.e., it does not send or receive messages (and\nmessages sent to it are dropped as well) and does not incur any time,\ncommunication, or local computation cost. Hence, in this model, only rounds in\nwhich a node is awake are counted and we are interested in minimizing the\naverage as well as the worst-case number of rounds a node spends in the awake\nstate.\n  Our main result is that we show that {\\em MIS can be solved in (expected)\n$O(1)$ rounds under node-averaged awake complexity measure} in the sleeping\nmodel. In particular, we present a randomized distributed algorithm for MIS\nthat has expected {\\em $O(1)$-rounds node-averaged awake complexity} and, with\nhigh probability has {\\em $O(\\log{n})$-rounds worst-case awake complexity} and\n{\\em $O(\\log^{3.41}n)$-rounds worst-case complexity}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 20:05:01 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Chatterjee", "Soumyottam", ""], ["Gmyr", "Robert", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "2006.07453", "submitter": "Sameh Sorour", "authors": "Sameh Sorour, Umair Mohammad, Amr Abutuleb, Hossam Hassanein", "title": "Returning the Favor: What Wireless Networking Can Offer to AI and Edge\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) and artificial intelligence (AI) have recently made a\nsignificant impact on improving the operations of wireless networks and\nestablishing intelligence at the edge. In return, rare efforts were made to\nexplore how adapting, optimizing, and arranging wireless networks can\ncontribute to implementing ML/AI at the edge. This article aims to address this\nvoid by setting a vision on how wireless networking researchers can leverage\ntheir expertise to return the favor to edge learning. It will review the\nenabling technologies, summarize the inaugural works on this path, and shed\nlight on different directions to establish a comprehensive framework for mobile\nedge learning (MEL).\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 20:25:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Sorour", "Sameh", ""], ["Mohammad", "Umair", ""], ["Abutuleb", "Amr", ""], ["Hassanein", "Hossam", ""]]}, {"id": "2006.07478", "submitter": "Stephen Timcheck", "authors": "Stephen Timcheck and Jeremy Buhler", "title": "Streaming Computations with Region-Based State on SIMD Architectures", "comments": "Presented at the 13th International Workshop on Programmability and\n  Architectures for Heterogeneous Multicores, 2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "MULTIPROG/2020/1", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Streaming computations on massive data sets are an attractive candidate for\nparallelization, particularly when they exhibit independence (and hence data\nparallelism) between items in the stream. However, some streaming computations\nare stateful, which disrupts independence and can limit parallelism. In this\nwork, we consider how to extract data parallelism from streaming computations\nwith a common, limited form of statefulness. The stream is assumed to be\ndivided into variably-sized regions, and items in the same region are processed\nin a common context of state. In general, the computation to be performed on a\nstream is also irregular, with each item potentially undergoing different,\ndata-dependent processing.\n  This work describes mechanisms to implement such computations efficiently on\na SIMD-parallel architecture such as a GPU. We first develop a low-level\nprotocol by which a data stream can be augmented with control signals that are\ndelivered to each stage of a computation at precise points in the stream. We\nthen describe an abstraction, enumeration and aggregation, by which an\napplication developer can specify the behavior of a streaming application with\nregion-based state. Finally, we study an implementation of our ideas as part of\nthe MERCATOR system for irregular streaming computations on GPUs, investigating\nhow the frequency of region boundaries in a stream impacts SIMD occupancy and\nhence application performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 21:18:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Timcheck", "Stephen", ""], ["Buhler", "Jeremy", ""]]}, {"id": "2006.07521", "submitter": "Harris Niavis", "authors": "Harris Niavis, Nikolaos Papadis, Leandros Tassiulas", "title": "A Blockchain-based Decentralized Data Sharing Infrastructure for\n  Off-grid Networking", "comments": "An abridged version of this work appeared in ICBC 2020, fixed minor\n  typos and layout issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-grid networks are recently emerging as a solution to connect the\nunconnected or provide alternative services to networks of possibly untrusted\nparticipants. The systems currently used, however, exhibit limitations due to\ntheir centralized nature and thus prove inadequate to secure trust. Blockchain\ntechnology can be the tool that will enable trust and transparency in such\nnetworks. In this paper, we introduce a platform for secure and\nprivacy-respecting decentralized data sharing among untrusted participants in\noff-grid networks. The proposed architecture realizes this goal via the\nintegration of existing blockchain frameworks (Hyperledger Fabric, Indy, Aries)\nwith an off-grid network device and a distributed file system. We evaluate the\nproposed platform through experiments and show results for its throughput and\nlatency, which indicate its adequate performance for supporting off-grid\ndecentralized applications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 00:37:56 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 16:52:08 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Niavis", "Harris", ""], ["Papadis", "Nikolaos", ""], ["Tassiulas", "Leandros", ""]]}, {"id": "2006.07578", "submitter": "Jorge Pe\\~na Queralta", "authors": "Jorge Pe\\~na Queralta and Tomi Westerlund", "title": "Blockchain for Mobile Edge Computing: Consensus Mechanisms and\n  Scalability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) and next-generation mobile networks are set to\ndisrupt the way intelligent and autonomous systems are interconnected. This\nwill have an effect on a wide range of domains, from the Internet of Things to\nautonomous mobile robots. The integration of such a variety of MEC services in\na inherently distributed architecture requires a robust system for managing\nhardware resources, balancing the network load and securing the distributed\napplications. Blockchain technology has emerged a solution for managing MEC\nservices, with consensus protocols and data integrity checks that enable\ntransparent and efficient distributed decision-making. In addition to\ntransparency, the benefits from a security point of view are evident.\nNonetheless, blockchain technology faces significant challenges in terms of\nscalability. In this chapter, we review existing consensus protocols and\nscalability techniques in both well-established and next-generation blockchain\narchitectures. From this, we evaluate the most suitable solutions for managing\nMEC services and discuss the benefits and drawbacks of the available\nalternatives.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 06:47:17 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Queralta", "Jorge Pe\u00f1a", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2006.08067", "submitter": "Victor Zakhary", "authors": "Victor Zakhary, Lawrence Lim, Divyakant Agrawal, Amr El Abbadi", "title": "CoT: Decentralized Elastic Caches for Cloud Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed caches are widely deployed to serve social networks and web\napplications at billion-user scales. This paper presents Cache-on-Track (CoT),\na decentralized, elastic, and predictive caching framework for cloud\nenvironments. CoT proposes a new cache replacement policy specifically tailored\nfor small front-end caches that serve skewed workloads. Front-end servers use a\nheavy hitter tracking algorithm to continuously track the top-k hot keys. CoT\ndynamically caches the hottest C keys out of the tracked keys. Our experiments\nshow that CoT's replacement policy consistently outperforms the hit-rates of\nLRU, LFU, and ARC for the same cache size on different skewed workloads. Also,\n\\algoname slightly outperforms the hit-rate of LRU-2 when both policies are\nconfigured with the same tracking (history) size. CoT achieves server size\nload-balance with 50\\% to 93.75\\% less front-end cache in comparison to other\nreplacement policies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:03:47 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 04:22:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Zakhary", "Victor", ""], ["Lim", "Lawrence", ""], ["Agrawal", "Divyakant", ""], ["Abbadi", "Amr El", ""]]}, {"id": "2006.08134", "submitter": "Jing Liu", "authors": "Jing Liu, Guochu Shou, Qingtian Wang, Yaqiong Liu, Yihong Hu, and\n  Zhigang Guo", "title": "Load-balanced Service Function Chaining in Edge Computing over FiWi\n  Access Networks for Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service function chaining (SFC) is promising to implement flexible and\nscalable virtual network infrastructure for the Internet of Things (IoT). Edge\ncomputing is envisioned to be an effective solution to process huge amount of\nIoT application data. In order to uniformly provide services to IoT\napplications among the distributed edge computing nodes (ECNs), we present a\nunified SFC orchestration framework based on the coordination of SDN and NFV,\nwhich provides a synergic edge cloud platform by exploiting the connectivity of\nFiWi access networks. In addition, we study the VNF deployment problem under\nour synergic framework, and we formulate it as a mixed-integer nonlinear\nprogramming (MINLP) problem jointly considering the load balancing of\nnetworking and computing for chaining VNFs. We also propose two approximation\noptimal deployment algorithms named Greedy-Bisection Multi-Path (GBMP) and KSP\nMultiPath (KSMP) taking advantage of the multi-instance virtual network\nfunctions (VNFs) deployed in ECNs and the multipath capacity in FiWi access\nnetworks. Extensive simulations are conducted in two types of IoT application\nscenarios in the EC over FiWi access networks. The numerical results show that\nour proposed algorithms are superior to single path and ECMP based deployment\nalgorithms in terms of load balancing, service acceptance ratio, and network\nutilization in both two typical scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 05:09:06 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Liu", "Jing", ""], ["Shou", "Guochu", ""], ["Wang", "Qingtian", ""], ["Liu", "Yaqiong", ""], ["Hu", "Yihong", ""], ["Guo", "Zhigang", ""]]}, {"id": "2006.08211", "submitter": "Ahmad Slo", "authors": "Ahmad Slo, Sukanya Bhowmik, Kurt Rothermel", "title": "hSPICE: State-Aware Event Shedding in Complex Event Processing", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex event processing (CEP), load shedding is performed to maintain a\ngiven latency bound during overload situations when there is a limitation on\nresources. However, shedding load implies degradation in the quality of results\n(QoR). Therefore, it is crucial to perform load shedding in a way that has the\nlowest impact on QoR. Researchers, in the CEP domain, propose to drop either\nevents or partial matches (PMs) in overload cases. They assign utilities to\nevents or PMs by considering either the importance of events or the importance\nof PMs but not both together. In this paper, we propose a load shedding\napproach for CEP systems that combines these approaches by assigning a utility\nto an event by considering both the event importance and the importance of PMs.\nWe adopt a probabilistic model that uses the type and position of an event in a\nwindow and the state of a PM to assign a utility to an event corresponding to\neach PM. We, also, propose an approach to predict a utility threshold that is\nused to drop the required amount of events to maintain a given latency bound.\nBy extensive evaluations on two real-world datasets and several representative\nqueries, we show that, in the majority of cases, our load shedding approach\noutperforms state-of-the-art load shedding approaches, w.r.t. QoR.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:24:56 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 12:21:53 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Slo", "Ahmad", ""], ["Bhowmik", "Sukanya", ""], ["Rothermel", "Kurt", ""]]}, {"id": "2006.08408", "submitter": "Philipp Schneider", "authors": "Fabian Kuhn and Philipp Schneider", "title": "Computing Shortest Paths and Diameter in the Hybrid Network Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $\\mathsf{HYBRID}$ model, introduced in [Augustine et al., SODA '20],\nprovides a theoretical foundation for networks that allow multiple\ncommunication modes. The model follows the principles of synchronous message\npassing, whereas nodes are allowed to use \\textit{two} fundamentally different\ncommunication modes. First, a local mode where nodes may exchange arbitrary\ninformation per round over edges of a local communication graph $G$ (akin to\nthe $\\mathsf{LOCAL}$ model). Second, a global mode where every node may\nexchange $O(\\log n)$ messages of size $O(\\log n)$ bits per round with arbitrary\nnodes in the network. The $\\mathsf{HYBRID}$ model intends to reflect the\nconditions of many real hybrid networks, where high-bandwidth but inherently\nlocal communication is combined with highly flexible global communication with\nrestricted bandwidth.\n  We continue to explore the power and limitations of the $\\mathsf{HYBRID}$\nmodel by investigating the complexity of computing shortest paths and diameter\nof the local communication graph $G$. We improve on the known upper bound for\nthe exact all pairs shortest paths problem (APSP) from [Augustine et al., SODA\n'20] and provide algorithms to approximate solutions for the $k$ source\nshortest paths problem ($k$-SSSP). We demonstrate that our results for APSP and\n$k$-SSP are almost tight (up to poly-logarithmic factors). Furthermore, we give\nan improved algorithm for the exact single source shortest paths problem for\ngraphs with large diameter. For the problem of approximating the diameter of\nthe local communication network we give the first non-trivial upper bound. This\nupper bound is complemented by a lower bound for the exact diameter problem.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 13:59:38 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 14:02:07 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 14:34:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Kuhn", "Fabian", ""], ["Schneider", "Philipp", ""]]}, {"id": "2006.08435", "submitter": "Arjun Ramaswami", "authors": "Arjun Ramaswami, Tobias Kenter, Thomas D. K\\\"uhne, Christian Plessl", "title": "Efficient Ab-Initio Molecular Dynamic Simulations by Offloading Fast\n  Fourier Transformations to FPGAs", "comments": "2 pages, 3 figures, to be published in FPL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A large share of today's HPC workloads is used for Ab-Initio Molecular\nDynamics (AIMD) simulations, where the interatomic forces are computed\non-the-fly by means of accurate electronic structure calculations. They are\ncomputationally intensive and thus constitute an interesting application class\nfor energy-efficient hardware accelerators such as FPGAs. In this paper, we\ninvestigate the potential of offloading 3D Fast Fourier Transformations (FFTs)\nas a critical routine of plane-wave-based electronic structure calculations to\nFPGA and in conjunction demonstrate the tolerance of these simulations to lower\nprecision computations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 14:32:56 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ramaswami", "Arjun", ""], ["Kenter", "Tobias", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "2006.08481", "submitter": "Ahmet-Serdar Karakaya", "authors": "Ahmet-Serdar Karakaya, Jonathan Hasenburg and David Bermbach", "title": "SimRa: Using Crowdsourcing to Identify Near Miss Hotspots in Bicycle\n  Traffic", "comments": "Accepted for publication in Elsevier Pervasive and Mobile Computing", "journal-ref": null, "doi": "10.1016/j.pmcj.2020.101197", "report-no": null, "categories": "cs.CY cs.DC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An increased modal share of bicycle traffic is a key mechanism to reduce\nemissions and solve traffic-related problems. However, a lack of (perceived)\nsafety keeps people from using their bikes more frequently. To improve safety\nin bicycle traffic, city planners need an overview of accidents, near miss\nincidents, and bike routes. Such information, however, is currently not\navailable. In this paper, we describe SimRa, a platform for collecting data on\nbicycle routes and near miss incidents using smartphone-based crowdsourcing. We\nalso describe how we identify dangerous near miss hotspots based on the\ncollected data and propose a scoring model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:29:52 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:46:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Karakaya", "Ahmet-Serdar", ""], ["Hasenburg", "Jonathan", ""], ["Bermbach", "David", ""]]}, {"id": "2006.08487", "submitter": "Priyank Faldu", "authors": "Priyank Faldu", "title": "Addressing Variability in Reuse Prediction for Last-Level Caches", "comments": "PhD Thesis submitted to the School of Informatics, The University of\n  Edinburgh (Advisor: Prof. Boris Grot, Examiners: Prof. Michael O'Boyle and\n  Dr. Gabriel Loh)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Last-Level Cache (LLC) represents the bulk of a modern CPU processor's\ntransistor budget and is essential for application performance as LLC enables\nfast access to data in contrast to much slower main memory. However,\napplications with large working set size often exhibit streaming and/or\nthrashing access patterns at LLC. As a result, a large fraction of the LLC\ncapacity is occupied by dead blocks that will not be referenced again, leading\nto inefficient utilization of the LLC capacity. To improve cache efficiency,\nthe state-of-the-art cache management techniques employ prediction mechanisms\nthat learn from the past access patterns with an aim to accurately identify as\nmany dead blocks as possible. Once identified, dead blocks are evicted from LLC\nto make space for potentially high reuse cache blocks.\n  In this thesis, we identify variability in the reuse behavior of cache blocks\nas the key limiting factor in maximizing cache efficiency for state-of-the-art\npredictive techniques. Variability in reuse prediction is inevitable due to\nnumerous factors that are outside the control of LLC. The sources of\nvariability include control-flow variation, speculative execution and\ncontention from cores sharing the cache, among others. Variability in reuse\nprediction challenges existing techniques in reliably identifying the end of a\nblock's useful lifetime, thus causing lower prediction accuracy, coverage, or\nboth. To address this challenge, this thesis aims to design robust cache\nmanagement mechanisms and policies for LLC in the face of variability in reuse\nprediction to minimize cache misses, while keeping the cost and complexity of\nthe hardware implementation low. To that end, we propose two cache management\ntechniques, one domain-agnostic and one domain-specialized, to improve cache\nefficiency by addressing variability in reuse prediction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:43:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Faldu", "Priyank", ""]]}, {"id": "2006.08498", "submitter": "Edoardo di Napoli", "authors": "Xiao Zhang (1), Sebastian Achilles (2 and 3), Jan Winkelmann (3),\n  Roland Haas (1), Andr\\'e Schleife (1) and Edoardo Di Napoli (2) ((1)\n  University of Illinois at Urbana-Champaign, (2) Forschungszentrum J\\\"ulich,\n  (3) RWTH Aachen University)", "title": "Solving the Bethe-Salpeter equation on massively parallel architectures", "comments": "17 Pages plus 7 pages of supplemental information, 6 figures and 3\n  tables. To be submitted to Computer Physics Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2021.108081", "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last ten years have witnessed fast spreading of massively parallel\ncomputing clusters, from leading supercomputing facilities down to the average\nuniversity computing center. Many companies in the private sector have\nundergone a similar evolution. In this scenario, the seamless integration of\nsoftware and middleware libraries is a key ingredient to ensure portability of\nscientific codes and guarantees them an extended lifetime. In this work, we\ndescribe the integration of the ChASE library, a modern parallel eigensolver,\ninto an existing legacy code for the first-principles computation of optical\nproperties of materials via solution of the Bethe-Salpeter equation for the\noptical polarization function. Our numerical tests show that, as a result of\nintegrating ChASE and parallelizing the reading routine, the code experiences a\nremarkable speedup and greatly improved scaling behavior on both multi- and\nmany-core architectures. We demonstrate that such a modernized BSE code will,\nby fully exploiting parallel computing architectures and file systems, enable\ndomain scientists to accurately study complex material systems that were not\naccessible before.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:55:52 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zhang", "Xiao", "", "2 and 3"], ["Achilles", "Sebastian", "", "2 and 3"], ["Winkelmann", "Jan", ""], ["Haas", "Roland", ""], ["Schleife", "Andr\u00e9", ""], ["Di Napoli", "Edoardo", ""]]}, {"id": "2006.08517", "submitter": "Yang You", "authors": "Yang You and Yuhui Wang and Huan Zhang and Zhao Zhang and James Demmel\n  and Cho-Jui Hsieh", "title": "The Limit of the Batch Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-batch training is an efficient approach for current distributed deep\nlearning systems. It has enabled researchers to reduce the ImageNet/ResNet-50\ntraining from 29 hours to around 1 minute. In this paper, we focus on studying\nthe limit of the batch size. We think it may provide a guidance to AI\nsupercomputer and algorithm designers. We provide detailed numerical\noptimization instructions for step-by-step comparison. Moreover, it is\nimportant to understand the generalization and optimization performance of huge\nbatch training. Hoffer et al. introduced \"ultra-slow diffusion\" theory to\nlarge-batch training. However, our experiments show contradictory results with\nthe conclusion of Hoffer et al. We provide comprehensive experimental results\nand detailed analysis to study the limitations of batch size scaling and\n\"ultra-slow diffusion\" theory. For the first time we scale the batch size on\nImageNet to at least a magnitude larger than all previous work, and provide\ndetailed studies on the performance of many state-of-the-art optimization\nschemes under this setting. We propose an optimization recipe that is able to\nimprove the top-1 test accuracy by 18% compared to the baseline.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:18:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["You", "Yang", ""], ["Wang", "Yuhui", ""], ["Zhang", "Huan", ""], ["Zhang", "Zhao", ""], ["Demmel", "James", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2006.08654", "submitter": "Pedro Garc\\'ia-L\\'opez", "authors": "Pedro Garc\\'ia-L\\'opez, Aitor Arjona, Josep Sampe, Aleksander\n  Slominski, Lionel Villard", "title": "Triggerflow: Trigger-based Orchestration of Serverless Workflows", "comments": "The 14th ACM International Conference on Distributed and Event-based\n  Systems (DEBS 2020)", "journal-ref": null, "doi": "10.1145/3401025.3401731", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more applications are being moved to the Cloud thanks to serverless\ncomputing, it is increasingly necessary to support native life cycle execution\nof those applications in the data center. But existing systems either focus on\nshort-running workflows (like IBM Composer or Amazon Express Workflows) or\nimpose considerable overheads for synchronizing massively parallel jobs (Azure\nDurable Functions, Amazon Step Functions, Google Cloud Composer). None of them\nare open systems enabling extensible interception and optimization of custom\nworkflows. We present Triggerflow: an extensible Trigger-based Orchestration\narchitecture for serverless workflows built on top of Knative Eventing and\nKubernetes technologies. We demonstrate that Triggerflow is a novel serverless\nbuilding block capable of constructing different reactive schedulers (State\nMachines, Directed Acyclic Graphs, Workflow as code). We also validate that it\ncan support high-volume event processing workloads, auto-scale on demand and\ntransparently optimize scientific workflows.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:04:33 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 14:18:34 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Garc\u00eda-L\u00f3pez", "Pedro", ""], ["Arjona", "Aitor", ""], ["Sampe", "Josep", ""], ["Slominski", "Aleksander", ""], ["Villard", "Lionel", ""]]}, {"id": "2006.08737", "submitter": "Raj Kumar Maity", "authors": "Avishek Ghosh, Raj Kumar Maity and Arya Mazumdar", "title": "Distributed Newton Can Communicate Less and Resist Byzantine Workers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a distributed second order optimization algorithm that is\ncommunication-efficient as well as robust against Byzantine failures of the\nworker machines. We propose COMRADE (COMunication-efficient and Robust\nApproximate Distributed nEwton), an iterative second order algorithm, where the\nworker machines communicate only once per iteration with the center machine.\nThis is in sharp contrast with the state-of-the-art distributed second order\nalgorithms like GIANT [34] and DINGO[7], where the worker machines send\n(functions of) local gradient and Hessian sequentially; thus ending up\ncommunicating twice with the center machine per iteration. Moreover, we show\nthat the worker machines can further compress the local information before\nsending it to the center. In addition, we employ a simple norm based\nthresholding rule to filter-out the Byzantine worker machines. We establish the\nlinear-quadratic rate of convergence of COMRADE and establish that the\ncommunication savings and Byzantine resilience result in only a small\nstatistical error rate for arbitrary convex loss functions. To the best of our\nknowledge, this is the first work that addresses the issue of Byzantine\nresilience in second order distributed optimization. Furthermore, we validate\nour theoretical results with extensive experiments on synthetic and benchmark\nLIBSVM [5] data-sets and demonstrate convergence guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:16:15 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ghosh", "Avishek", ""], ["Maity", "Raj Kumar", ""], ["Mazumdar", "Arya", ""]]}, {"id": "2006.08848", "submitter": "The Canh Dinh", "authors": "Canh T. Dinh, Nguyen H. Tran, Tuan Dung Nguyen", "title": "Personalized Federated Learning with Moreau Envelopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) is a decentralized and privacy-preserving machine\nlearning technique in which a group of clients collaborate with a server to\nlearn a global model without sharing clients' data. One challenge associated\nwith FL is statistical diversity among clients, which restricts the global\nmodel from delivering good performance on each client's task. To address this,\nwe propose an algorithm for personalized FL (pFedMe) using Moreau envelopes as\nclients' regularized loss functions, which help decouple personalized model\noptimization from the global model learning in a bi-level problem stylized for\npersonalized FL. Theoretically, we show that pFedMe's convergence rate is\nstate-of-the-art: achieving quadratic speedup for strongly convex and sublinear\nspeedup of order 2/3 for smooth nonconvex objectives. Experimentally, we verify\nthat pFedMe excels at empirical performance compared with the vanilla FedAvg\nand Per-FedAvg, a meta-learning based personalized FL algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:55:23 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 03:07:46 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Dinh", "Canh T.", ""], ["Tran", "Nguyen H.", ""], ["Nguyen", "Tuan Dung", ""]]}, {"id": "2006.08905", "submitter": "Masahito Ohue", "authors": "Masahito Ohue, Kento Aoyama, Yutaka Akiyama", "title": "High-performance cloud computing for exhaustive protein-protein docking", "comments": "11 pages, 2 figures, and 3 tables. To be published in in Proceedings\n  of The 26th International Conference on Parallel & Distributed Processing\n  Techniques and Applications (PDPTA'20), Transactions on Computational Science\n  and Computational Intelligence (Springer)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-bio.BM q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public cloud computing environments, such as Amazon AWS, Microsoft Azure, and\nthe Google Cloud Platform, have achieved remarkable improvements in\ncomputational performance in recent years, and are also expected to be able to\nperform massively parallel computing. As the cloud enables users to use\nthousands of CPU cores and GPU accelerators casually, and various software\ntypes can be used very easily by cloud images, the cloud is beginning to be\nused in the field of bioinformatics. In this study, we ported the original\nprotein-protein interaction prediction (protein-protein docking) software,\nMEGADOCK, into Microsoft Azure as an example of an HPC cloud environment. A\ncloud parallel computing environment with up to 1,600 CPU cores and 960 GPUs\nwas constructed using four CPU instance types and two GPU instance types, and\nthe parallel computing performance was evaluated. Our MEGADOCK on Azure system\nshowed a strong scaling value of 0.93 for the CPU instance when H16 instance\nwith 100 instances were used compared to 50, and a strong scaling value of 0.89\nfor the GPU instance when NC24 instance with 20 were used compared to 5.\nMoreover, the results of the usage fee and total computation time supported\nthat using a GPU instance reduced the computation time of MEGADOCK and the\ncloud usage fee required for the computation. The developed environment\ndeployed on the cloud is highly portable, making it suitable for applications\nin which an on-demand and large-scale HPC environment is desirable.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 03:37:50 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Ohue", "Masahito", ""], ["Aoyama", "Kento", ""], ["Akiyama", "Yutaka", ""]]}, {"id": "2006.08950", "submitter": "Honglin Yuan", "authors": "Honglin Yuan, Tengyu Ma", "title": "Federated Accelerated Stochastic Gradient Descent", "comments": "Accepted to NeurIPS 2020. Best paper in International Workshop on\n  Federated Learning for User Privacy and Data Confidentiality in Conjunction\n  with ICML 2020 (FL-ICML'20). Code repository see\n  https://github.com/hongliny/FedAc-NeurIPS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Federated Accelerated Stochastic Gradient Descent (FedAc), a\nprincipled acceleration of Federated Averaging (FedAvg, also known as Local\nSGD) for distributed optimization. FedAc is the first provable acceleration of\nFedAvg that improves convergence speed and communication efficiency on various\ntypes of convex functions. For example, for strongly convex and smooth\nfunctions, when using $M$ workers, the previous state-of-the-art FedAvg\nanalysis can achieve a linear speedup in $M$ if given $M$ rounds of\nsynchronization, whereas FedAc only requires $M^{\\frac{1}{3}}$ rounds.\nMoreover, we prove stronger guarantees for FedAc when the objectives are\nthird-order smooth. Our technique is based on a potential-based perturbed\niterate analysis, a novel stability analysis of generalized accelerated SGD,\nand a strategic tradeoff between acceleration and stability.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 06:58:07 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 17:55:44 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 04:00:01 GMT"}, {"version": "v4", "created": "Sat, 5 Jun 2021 06:22:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yuan", "Honglin", ""], ["Ma", "Tengyu", ""]]}, {"id": "2006.09083", "submitter": "Roberto L. Castro", "authors": "Roberto L. Castro, Diego Andrade, Basilio Fraguela", "title": "Reusing Trained Layers of Convolutional Neural Networks to Shorten\n  Hyperparameters Tuning Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters tuning is a time-consuming approach, particularly when the\narchitecture of the neural network is decided as part of this process. For\ninstance, in convolutional neural networks (CNNs), the selection of the number\nand the characteristics of the hidden (convolutional) layers may be decided.\nThis implies that the search process involves the training of all these\ncandidate network architectures.\n  This paper describes a proposal to reuse the weights of hidden\n(convolutional) layers among different trainings to shorten this process. The\nrationale is that if a set of convolutional layers have been trained to solve a\ngiven problem, the weights calculated in this training may be useful when a new\nconvolutional layer is added to the network architecture.\n  This idea has been tested using the CIFAR-10 dataset, testing different CNNs\narchitectures with up to 3 convolutional layers and up to 3 fully connected\nlayers. The experiments compare the training time and the validation loss when\nreusing and not reusing convolutional layers. They confirm that this strategy\nreduces the training time while it even increases the accuracy of the resulting\nneural network. This finding opens up the future possibility of integrating\nthis strategy in existing AutoML methods with the purpose of reducing the total\nsearch time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:39:39 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 15:30:27 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Castro", "Roberto L.", ""], ["Andrade", "Diego", ""], ["Fraguela", "Basilio", ""]]}, {"id": "2006.09141", "submitter": "Javier Ferrando", "authors": "Javier Ferrando and Juan Luis Dominguez and Jordi Torres and Raul\n  Garcia and David Garcia and Daniel Garrido and Jordi Cortada and Mateo Valero", "title": "Improving accuracy and speeding up Document Image Classification through\n  parallel systems", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-50417-5_29", "report-no": null, "categories": "cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a study showing the benefits of the EfficientNet models\ncompared with heavier Convolutional Neural Networks (CNNs) in the Document\nClassification task, essential problem in the digitalization process of\ninstitutions. We show in the RVL-CDIP dataset that we can improve previous\nresults with a much lighter model and present its transfer learning\ncapabilities on a smaller in-domain dataset such as Tobacco3482. Moreover, we\npresent an ensemble pipeline which is able to boost solely image input by\ncombining image model predictions with the ones generated by BERT model on\nextracted text by OCR. We also show that the batch size can be effectively\nincreased without hindering its accuracy so that the training process can be\nsped up by parallelizing throughout multiple GPUs, decreasing the computational\ntime needed. Lastly, we expose the training performance differences between\nPyTorch and Tensorflow Deep Learning frameworks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 13:36:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Ferrando", "Javier", ""], ["Dominguez", "Juan Luis", ""], ["Torres", "Jordi", ""], ["Garcia", "Raul", ""], ["Garcia", "David", ""], ["Garrido", "Daniel", ""], ["Cortada", "Jordi", ""], ["Valero", "Mateo", ""]]}, {"id": "2006.09167", "submitter": "Szil\\'ard P\\'all", "authors": "Szil\\'ard P\\'all, Artem Zhmurov, Paul Bauer, Mark Abraham, Magnus\n  Lundborg, Alan Gray, Berk Hess, Erik Lindahl", "title": "Heterogeneous Parallelization and Acceleration of Molecular Dynamics\n  Simulations in GROMACS", "comments": "The following article has been submitted to the Journal of Chemical\n  Physics", "journal-ref": null, "doi": "10.1063/5.0018516", "report-no": null, "categories": "physics.comp-ph cs.DC cs.DS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The introduction of accelerator devices such as graphics processing units\n(GPUs) has had profound impact on molecular dynamics simulations and has\nenabled order-of-magnitude performance advances using commodity hardware. To\nfully reap these benefits, it has been necessary to reformulate some of the\nmost fundamental algorithms, including the Verlet list, pair searching and\ncut-offs. Here, we present the heterogeneous parallelization and acceleration\ndesign of molecular dynamics implemented in the GROMACS codebase over the last\ndecade. The setup involves a general cluster-based approach to pair lists and\nnon-bonded pair interactions that utilizes both GPUs and CPU SIMD acceleration\nefficiently, including the ability to load-balance tasks between CPUs and GPUs.\nThe algorithm work efficiency is tuned for each type of hardware, and to use\naccelerators more efficiently we introduce dual pair lists with rolling pruning\nupdates. Combined with new direct GPU-GPU communication as well as GPU\nintegration, this enables excellent performance from single GPU simulations\nthrough strong scaling across multiple GPUs and efficient multi-node\nparallelization.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:19:26 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 22:54:40 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["P\u00e1ll", "Szil\u00e1rd", ""], ["Zhmurov", "Artem", ""], ["Bauer", "Paul", ""], ["Abraham", "Mark", ""], ["Lundborg", "Magnus", ""], ["Gray", "Alan", ""], ["Hess", "Berk", ""], ["Lindahl", "Erik", ""]]}, {"id": "2006.09182", "submitter": "Rafael Copstein", "authors": "R. Copstein, F. Dotti", "title": "Distributed File System for an Edge-Based Environment", "comments": "10 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the industry of personal computing led to a greater\nnumber of the so-called edge devices. Such devices typically do not collaborate\nor foresee the possibility of collaboration to offer aggregated storage and\ncomputing capabilities. The concept of distributed file system (DFS) is not new\nto the field of distributed systems, in fact, it is widely used in dedicated\ninfrastructures, for example, in cloud computing applications.\n  In this work, we discuss reasonable assumptions for an environment composed\nof edge devices, the main design issues and implementation challenges of a DFS\nin the given environment and how they would impact this application. Thereafter\nwe define a system model for an environment composed of edge devices while\ntaking into consideration their high mobility and common cases of network\npartitioning. Next, we describe an architecture for a DFS that withstands the\nproposed system model while offering most capabilities that a DFS at a\ndedicated infrastructure would.\n  We conclude that the development of a distributed file system is a very\ncomplex task and, given the broad assumptions of the system model, also hard to\nverify. Some important aspects of the development lie as future work, but we\nbelieve that the developed DFS can be used not only as a tool on it's own, but\nalso as a reference for further development of distributed file systems and,\nspecially, of systems for infrastructures composed of edge devices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:27:13 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Copstein", "R.", ""], ["Dotti", "F.", ""]]}, {"id": "2006.09320", "submitter": "Carlos Pedroso", "authors": "Carlos Pedroso, Yan Uehara de Moraes, Michele Nogueira, Aldri Santos", "title": "Managing Consensus-Based Cooperative Task Allocation for IIoT Networks", "comments": "This work has been accept to the IEEE ISCC2020. Copyright\n  978-1-7281-8086-1/20/$31.00 2020 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current IoT services include industry-oriented services, which often require\nobjects to run more than one task. However, the exponential growth of objects\nin IoT poses the challenge of distributing and managing task allocation among\nobjects. One of the main goals of task allocation is to improve the quality of\ninformation and maximize the tasks to be performed. Although there are\napproaches that optimize and manage the dynamics of nodes, not all consider the\nquality of information and the distributed allocation over the cluster service.\nThis paper proposes the mechanism CONTASKI for task allocation in IIoT networks\nto distribute tasks among objects. It relies on collaborative consensus to\nallocate tasks and similarity capabilities to know which objects can play in\naccomplishing those tasks. CONTASKI was evaluated on NS-3 and achieved 100% of\nallocated tasks incases with 75 and 100 nodes, and, on average, more than 80%\nclusters performed tasks in a low response time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:05:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Pedroso", "Carlos", ""], ["de Moraes", "Yan Uehara", ""], ["Nogueira", "Michele", ""], ["Santos", "Aldri", ""]]}, {"id": "2006.09473", "submitter": "Kewen Meng", "authors": "Kewen Meng and Boyana Norris", "title": "Guiding Optimizations with Meliora: A Deep Walk down Memory Lane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance models can be very useful for understanding the behavior of\napplications and hence can help guide design and optimization decisions.\nUnfortunately, performance modeling of nontrivial computations typically\nrequires significant expertise and human effort. Moreover, even when performed\nby experts, it is necessarily limited in scope, accuracy, or both. However,\nsince models are not typically available, programmers, compilers or autotuners\ncannot use them easily to guide optimizations and are limited to\nheuristic-based methods that potentially take a lot of time to perform\nunnecessary transformations. We believe that streamlining model generation and\nmaking it scalable (both in terms of human effort and code size) would enable\ndramatic improvements in compilation techniques, as well as manual optimization\nand autotuning. To that end, we are building the Meliora code analysis\ninfrastructure for machine learning-based performance model generation of\narbitrary codes based on static analysis of intermediate language\nrepresentations. We demonstrate good accuracy in matching known codes and show\nhow Meliora can be used to optimize new codes though reusing optimization\nknowledge, either manually or in conjunction with an autotuner. When\nautotuning, Meliora eliminates or dramatically reduces the empirical search\nspace, while generally achieving competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 03:59:31 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Meng", "Kewen", ""], ["Norris", "Boyana", ""]]}, {"id": "2006.09503", "submitter": "Deepak Narayanan", "authors": "Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, Matei Zaharia", "title": "Memory-Efficient Pipeline-Parallel DNN Training", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art ML results have been obtained by scaling up the number\nof parameters in existing models. However, parameters and activations for such\nlarge models often do not fit in the memory of a single accelerator device;\nthis means that it is necessary to distribute training of large models over\nmultiple accelerators. In this work, we propose PipeDream-2BW, a system that\nsupports memory-efficient pipeline parallelism. PipeDream-2BW uses a novel\npipelining and weight gradient coalescing strategy, combined with the double\nbuffering of weights, to ensure high throughput, low memory footprint, and\nweight update semantics similar to data parallelism. In addition, PipeDream-2BW\nautomatically partitions the model over the available hardware resources, while\nrespecting hardware constraints such as memory capacities of accelerators and\ninterconnect topologies. PipeDream-2BW can accelerate the training of large GPT\nand BERT language models by up to 20$\\times$ with similar final model accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:33:54 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 05:01:32 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 17:25:58 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Narayanan", "Deepak", ""], ["Phanishayee", "Amar", ""], ["Shi", "Kaiyu", ""], ["Chen", "Xie", ""], ["Zaharia", "Matei", ""]]}, {"id": "2006.09637", "submitter": "Kavya Kopparapu", "authors": "Kavya Kopparapu, Eric Lin, Jessica Zhao", "title": "FedCD: Improving Performance in non-IID Federated Learning", "comments": "Accepted for Oral Presentation at ACM SIGKDD International Conference\n  on Knowledge Discovery and Data Mining (KDD 2020) International workshop on\n  Artificial Intelligence of Things", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has been widely applied to enable decentralized devices,\nwhich each have their own local data, to learn a shared model. However,\nlearning from real-world data can be challenging, as it is rarely identically\nand independently distributed (IID) across edge devices (a key assumption for\ncurrent high-performing and low-bandwidth algorithms). We present a novel\napproach, FedCD, which clones and deletes models to dynamically group devices\nwith similar data. Experiments on the CIFAR-10 dataset show that FedCD achieves\nhigher accuracy and faster convergence compared to a FedAvg baseline on non-IID\ndata while incurring minimal computation, communication, and storage overheads.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 03:45:25 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 02:18:15 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 04:55:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kopparapu", "Kavya", ""], ["Lin", "Eric", ""], ["Zhao", "Jessica", ""]]}, {"id": "2006.09638", "submitter": "Margalit Glasgow", "authors": "Margalit Glasgow, Mary Wootters", "title": "Approximate Gradient Coding with Optimal Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed optimization problems, a technique called gradient coding,\nwhich involves replicating data points, has been used to mitigate the effect of\nstraggling machines. Recent work has studied approximate gradient coding, which\nconcerns coding schemes where the replication factor of the data is too low to\nrecover the full gradient exactly. Our work is motivated by the challenge of\ncreating approximate gradient coding schemes that simultaneously work well in\nboth the adversarial and stochastic models. To that end, we introduce novel\napproximate gradient codes based on expander graphs, in which each machine\nreceives exactly two blocks of data points. We analyze the decoding error both\nin the random and adversarial straggler setting, when optimal decoding\ncoefficients are used. We show that in the random setting, our schemes achieve\nan error to the gradient that decays exponentially in the replication factor.\nIn the adversarial setting, the error is nearly a factor of two smaller than\nany existing code with similar performance in the random setting. We show\nconvergence bounds both in the random and adversarial setting for gradient\ndescent under standard assumptions using our codes. In the random setting, our\nconvergence rate improves upon block-box bounds. In the adversarial setting, we\nshow that gradient descent can converge down to a noise floor that scales\nlinearly with the adversarial error to the gradient. We demonstrate empirically\nthat our schemes achieve near-optimal error in the random setting and converge\nfaster than algorithms which do not use the optimal decoding coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 03:46:27 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 05:32:31 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 05:02:52 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Glasgow", "Margalit", ""], ["Wootters", "Mary", ""]]}, {"id": "2006.09710", "submitter": "Xu Chen", "authors": "Huirong Ma and Zhi Zhou and Xu Chen", "title": "Leveraging the Power of Prediction: Predictive Service Placement for\n  Latency-Sensitive Mobile Edge Computing", "comments": "Accepted by IEEE Transactions on Wireless Communications, June 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) is emerging to support delay-sensitive 5G\napplications at the edge of mobile networks. When a user moves erratically\namong multiple MEC nodes, the challenge of how to dynamically migrate its\nservice to maintain service performance (i.e., user-perceived latency) arises.\nHowever, frequent service migration can significantly increase operational\ncost, incurring the conflict between improving performance and reducing cost.\nTo address these mis-aligned objectives, this paper studies the performance\noptimization of mobile edge service placement under the constraint of long-term\ncost budget. It is challenging because the budget involves the future uncertain\ninformation (e.g., user mobility). To overcome this difficulty, we devote to\nleveraging the power of prediction and advocate predictive service placement\nwith predicted near-future information. By using two-timescale Lyapunov\noptimization method, we propose a T-slot predictive service placement (PSP)\nalgorithm to incorporate the prediction of user mobility based on a frame-based\ndesign. We characterize the performance bounds of PSP in terms of cost-delay\ntrade-off theoretically. Furthermore, we propose a new weight adjustment scheme\nfor the queue in each frame named PSP-WU to exploit the historical queue\ninformation, which greatly reduces the length of queue while improving the\nquality of user-perceived latency. Rigorous theoretical analysis and extensive\nevaluations using realistic data traces demonstrate the superior performance of\nthe proposed predictive schemes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 08:09:52 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ma", "Huirong", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "2006.09791", "submitter": "Jos\\'e Cano", "authors": "Perry Gibson, Jos\\'e Cano, Jack Turner, Elliot J. Crowley, Michael\n  O'Boyle, Amos Storkey", "title": "Optimizing Grouped Convolutions on Edge Devices", "comments": "Camera ready version to be published at ASAP 2020 - The 31st IEEE\n  International Conference on Application-specific Systems, Architectures and\n  Processors. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When deploying a deep neural network on constrained hardware, it is possible\nto replace the network's standard convolutions with grouped convolutions. This\nallows for substantial memory savings with minimal loss of accuracy. However,\ncurrent implementations of grouped convolutions in modern deep learning\nframeworks are far from performing optimally in terms of speed. In this paper\nwe propose Grouped Spatial Pack Convolutions (GSPC), a new implementation of\ngrouped convolutions that outperforms existing solutions. We implement GSPC in\nTVM, which provides state-of-the-art performance on edge devices. We analyze a\nset of networks utilizing different types of grouped convolutions and evaluate\ntheir performance in terms of inference time on several edge devices. We\nobserve that our new implementation scales well with the number of groups and\nprovides the best inference times in all settings, improving the existing\nimplementations of grouped convolutions in TVM, PyTorch and TensorFlow Lite by\n3.4x, 8x and 4x on average respectively. Code is available at\nhttps://github.com/gecLAB/tvm-GSPC/\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 11:48:37 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Gibson", "Perry", ""], ["Cano", "Jos\u00e9", ""], ["Turner", "Jack", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""], ["Storkey", "Amos", ""]]}, {"id": "2006.09823", "submitter": "Taylor Blau", "authors": "Taylor Blau", "title": "Verifying Strong Eventual Consistency in $\\delta$-CRDTs", "comments": "66 pages, 27 figures. Senior thesis report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conflict-free replicated data types (CRDTs) are a natural structure with\nwhich to communicate information about a shared computation in a distributed\nsetting where coordination overhead may not be tolerated, and individual\nparticipants are allowed to temporarily diverge from the overall computation.\nWithin this setting, there are two classical approaches: state- and\noperation-based CRDTs. The former define a commutative, associative, and\nidempotent join operation, and their states a monotone join semi-lattice.\nState-based CRDTs may be further distinguished into classical- and\n$\\delta$-state CRDTs. The former communicate their full state after each\nupdate, whereas the latter communicate only the changed state. Op-based CRDTs\ncommunicate operations (not state), thus making their updates non-idempotent.\nWhereas op-based CRDTs require little information to be exchanged, they demand\nrelatively strong network guarantees (exactly-once message delivery), and\nstate-based CRDTs suffer the opposite problem. Both satisfy strong eventual\nconsistency (SEC).\n  We posit that $\\delta$-state CRDTs both (1) require less communication\noverhead from payload size, and (2) tolerate relatively weak network\nenvironments, making them an ideal candidate for real-world use of CRDTs. Our\ncentral intuition is a pair of reductions between state-, $\\delta$-state, and\nop-based CRDTs. We formalize this intuition in the Isabelle interactive theorem\nprover and show that state-based CRDTs achieve SEC. We present a relaxed\nnetwork model in Isabelle and show that state-based CRDTs still maintain SEC.\nFinally, we extend our work to show that $\\delta$-state CRDTs maintain SEC when\nonly communicating $\\delta$-state fragments, even under relatively weak network\nconditions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 04:32:29 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Blau", "Taylor", ""]]}, {"id": "2006.09864", "submitter": "Matheus Saldanha", "authors": "Matheus Henrique Junqueira Saldanha", "title": "Probabilistic Models for the Execution Time in Stochastic Scheduling", "comments": "2nd ver comments: Included changes requested by the thesis reviewers.\n  Results do NOT change. Some figures were changed. Abstract also changes. 1st\n  ver comments: Bachelor's thesis. Advisor Prof. Dr. Adriano Kamimura Suzuki", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The execution time of programs is a key element in many areas of computer\nscience, mainly those where achieving good performance (e.g., scheduling in\ncloud computing) or a predictable one (e.g., meeting deadlines in embedded\nsystems) is the objective. Despite being random variables, execution times are\nmost often treated as deterministic in the literature, with few works taking\nadvantage of their randomness; even in those, the underlying distributions are\nassumed as being normal or uniform for no particular reason. In this work we\ninvestigate these distributions in various machines and algorithms. A\nmathematical problem arises when dealing with samples whose populational\nminimum is unknown, so a significant portion of this monograph is dedicated to\nsuch problem. We propose several different effective or computationally cheap\nways to overcome the problem, which also apply to execution times. These\nmethods are tested experimentally, and results point to the superiority of our\nproposed inference methods. We demonstrate the existence of execution time\ndistributions with long tails, and also conclude that two particular\nprobability distributions were the most suitable for modelling all execution\ntimes. While we do not discuss direct applications to stochastic scheduling, we\nhope to promote the usage of probabilistic execution times to yield better\nresults in, for example, task scheduling.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 23:02:48 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 07:08:27 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Saldanha", "Matheus Henrique Junqueira", ""]]}, {"id": "2006.09895", "submitter": "J\\'ozsef D\\'aniel G\\'asp\\'ar", "authors": "J\\'ozsef D\\'aniel G\\'asp\\'ar, Martin Horv\\'ath, Gy\\H{o}z\\H{o}\n  Horv\\'ath and Zolt\\'an Zvara", "title": "Ranking and benchmarking framework for sampling algorithms on synthetic\n  data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fields of big data, AI, and streaming processing, we work with large\namounts of data from multiple sources. Due to memory and network limitations,\nwe process data streams on distributed systems to alleviate computational and\nnetwork loads. When data streams with non-uniform distributions are processed,\nwe often observe overloaded partitions due to the use of simple hash\npartitioning. To tackle this imbalance, we can use dynamic partitioning\nalgorithms that require a sampling algorithm to precisely estimate the\nunderlying distribution of the data stream. There is no standardized way to\ntest these algorithms. We offer an extensible ranking framework with benchmark\nand hyperparameter optimization capabilities and supply our framework with a\ndata generator that can handle concept drifts.\n  Our work includes a generator for dynamic micro-bursts that we can apply to\nany data stream. We provide algorithms that react to concept drifts and compare\nthose against the state-of-the-art algorithms using our framework.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 14:25:07 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["G\u00e1sp\u00e1r", "J\u00f3zsef D\u00e1niel", ""], ["Horv\u00e1th", "Martin", ""], ["Horv\u00e1th", "Gy\u0151z\u0151", ""], ["Zvara", "Zolt\u00e1n", ""]]}, {"id": "2006.10091", "submitter": "Junyi Li", "authors": "Junyi Li, Heng Huang", "title": "Faster Secure Data Mining via Distributed Homomorphic Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rising privacy demand in data mining, Homomorphic Encryption (HE)\nis receiving more and more attention recently for its capability to do\ncomputations over the encrypted field. By using the HE technique, it is\npossible to securely outsource model learning to the not fully trustful but\npowerful public cloud computing environments. However, HE-based training scales\nbadly because of the high computation complexity. It is still an open problem\nwhether it is possible to apply HE to large-scale problems. In this paper, we\npropose a novel general distributed HE-based data mining framework towards one\nstep of solving the scaling problem. The main idea of our approach is to use\nthe slightly more communication overhead in exchange of shallower computational\ncircuit in HE, so as to reduce the overall complexity. We verify the efficiency\nand effectiveness of our new framework by testing over various data mining\nalgorithms and benchmark data-sets. For example, we successfully train a\nlogistic regression model to recognize the digit 3 and 8 within around 5\nminutes, while a centralized counterpart needs almost 2 hours.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 18:14:30 GMT"}], "update_date": "2020-06-20", "authors_parsed": [["Li", "Junyi", ""], ["Huang", "Heng", ""]]}, {"id": "2006.10103", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Chaokun Chang, Haibin Lin, Yida Wang, Raman Arora, Xin Jin", "title": "Is Network the Bottleneck of Distributed Training?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a surge of research on improving the communication\nefficiency of distributed training. However, little work has been done to\nsystematically understand whether the network is the bottleneck and to what\nextent.\n  In this paper, we take a first-principles approach to measure and analyze the\nnetwork performance of distributed training. As expected, our measurement\nconfirms that communication is the component that blocks distributed training\nfrom linear scale-out. However, contrary to the common belief, we find that the\nnetwork is running at low utilization and that if the network can be fully\nutilized, distributed training can achieve a scaling factor of close to one.\nMoreover, while many recent proposals on gradient compression advocate over\n100x compression ratio, we show that under full network utilization, there is\nno need for gradient compression in 100 Gbps network. On the other hand, a\nlower speed network like 10 Gbps requires only 2x--5x gradients compression\nratio to achieve almost linear scale-out. Compared to application-level\ntechniques like gradient compression, network-level optimizations do not\nrequire changes to applications and do not hurt the performance of trained\nmodels. As such, we advocate that the real challenge of distributed training is\nfor the network community to develop high-performance network transport to\nfully utilize the network capacity and achieve linear scale-out.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:00:31 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 02:41:04 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 19:23:26 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhang", "Zhen", ""], ["Chang", "Chaokun", ""], ["Lin", "Haibin", ""], ["Wang", "Yida", ""], ["Arora", "Raman", ""], ["Jin", "Xin", ""]]}, {"id": "2006.10226", "submitter": "Animesh Jain", "authors": "Animesh Jain, Shoubhik Bhattacharya, Masahiro Masuda, Vin Sharma and\n  Yida Wang", "title": "Efficient Execution of Quantized Deep Learning Models: A Compiler\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of applications implement predictive functions using deep\nlearning models, which require heavy use of compute and memory. One popular\ntechnique for increasing resource efficiency is 8-bit integer quantization, in\nwhich 32-bit floating point numbers (fp32) are represented using shorter 8-bit\ninteger numbers. Although deep learning frameworks such as TensorFlow, TFLite,\nMXNet, and PyTorch enable developers to quantize models with only a small drop\nin accuracy, they are not well suited to execute quantized models on a variety\nof hardware platforms. For example, TFLite is optimized to run inference on ARM\nCPU edge devices but it does not have efficient support for Intel CPUs and\nNvidia GPUs. In this paper, we address the challenges of executing quantized\ndeep learning models on diverse hardware platforms by proposing an augmented\ncompiler approach. A deep learning compiler such as Apache TVM can enable the\nefficient execution of model from various frameworks on various targets. Many\ndeep learning compilers today, however, are designed primarily for fp32\ncomputation and cannot optimize a pre-quantized INT8 model. To address this\nissue, we created a new dialect called Quantized Neural Network (QNN) that\nextends the compiler's internal representation with a quantization context.\nWith this quantization context, the compiler can generate efficient code for\npre-quantized models on various hardware platforms. As implemented in Apache\nTVM, we observe that the QNN-augmented deep learning compiler achieves speedups\nof 2.35x, 2.15x, 1.35x and 1.40x on Intel Xeon Cascade Lake CPUs, Nvidia Tesla\nT4 GPUs, ARM Raspberry Pi3 and Pi4 respectively against well optimized fp32\nexecution, and comparable performance to the state-of-the-art\nframework-specific solutions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:38:10 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Jain", "Animesh", ""], ["Bhattacharya", "Shoubhik", ""], ["Masuda", "Masahiro", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "2006.10443", "submitter": "Milos Stankovic", "authors": "Milos S. Stankovic, Marko Beko, Srdjan S. Stankovic", "title": "Distributed Value Function Approximation for Collaborative Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose several novel distributed gradient-based temporal\ndifference algorithms for multi-agent off-policy learning of linear\napproximation of the value function in Markov decision processes with strict\ninformation structure constraints, limiting inter-agent communications to small\nneighborhoods. The algorithms are composed of: 1) local parameter updates based\non single-agent off-policy gradient temporal difference learning algorithms,\nincluding eligibility traces with state dependent parameters, and 2) linear\nstochastic time varying consensus schemes, represented by directed graphs. The\nproposed algorithms differ by their form, definition of eligibility traces,\nselection of time scales and the way of incorporating consensus iterations. The\nmain contribution of the paper is a convergence analysis based on the general\nproperties of the underlying Feller-Markov processes and the stochastic time\nvarying consensus model. We prove, under general assumptions, that the\nparameter estimates generated by all the proposed algorithms weakly converge to\nthe corresponding ordinary differential equations (ODE) with precisely defined\ninvariant sets. It is demonstrated how the adopted methodology can be applied\nto temporal-difference algorithms under weaker information structure\nconstraints. The variance reduction effect of the proposed algorithms is\ndemonstrated by formulating and analyzing an asymptotic stochastic differential\nequation. Specific guidelines for communication network design are provided.\nThe algorithms' superior properties are illustrated by characteristic\nsimulation results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 11:46:09 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 21:14:53 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 20:00:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Stankovic", "Milos S.", ""], ["Beko", "Marko", ""], ["Stankovic", "Srdjan S.", ""]]}, {"id": "2006.10494", "submitter": "Stephen Dolan", "authors": "Stephen Dolan", "title": "The Only Undoable CRDTs are Counters", "comments": null, "journal-ref": null, "doi": "10.1145/3382734.3405749", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In comparing well-known CRDTs representing sets that can grow and shrink, we\nfind caveats. In one, the removal of an element cannot be reliably undone. In\nanother, undesirable states are attainable, such as when an element is present\n-1 times (and so must be added for the set to become empty). The first lacks a\ngeneral-purpose undo, while the second acts less like a set and more like a\ntuple of counters, one per possible element.\n  Using some group theory, we show that this trade-off is unavoidable: every\nundoable CRDT is a tuple of counters.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 13:18:43 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Dolan", "Stephen", ""]]}, {"id": "2006.10504", "submitter": "Tanuj Kr Aasawat", "authors": "Xiufeng Yang and Tanuj Kr Aasawat and Kazuki Yoshizoe", "title": "Practical Massively Parallel Monte-Carlo Tree Search Applied to\n  Molecular Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common practice to use large computational resources to train neural\nnetworks, as is known from many examples, such as reinforcement learning\napplications. However, while massively parallel computing is often used for\ntraining models, it is rarely used for searching solutions for combinatorial\noptimization problems. In this paper, we propose a novel massively parallel\nMonte-Carlo Tree Search (MP-MCTS) algorithm that works efficiently for 1,000\nworker scale, and apply it to molecular design. This is the first work that\napplies distributed MCTS to a real-world and non-game problem. Existing work on\nlarge-scale parallel MCTS show efficient scalability in terms of the number of\nrollouts up to 100 workers, but suffer from the degradation in the quality of\nthe solutions. MP-MCTS maintains the search quality at larger scale, and by\nrunning MP-MCTS on 256 CPU cores for only 10 minutes, we obtained candidate\nmolecules having similar score to non-parallel MCTS running for 42 hours.\nMoreover, our results based on parallel MCTS (combined with a simple RNN model)\nsignificantly outperforms existing state-of-the-art work. Our method is generic\nand is expected to speed up other applications of MCTS.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 13:23:40 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 09:12:04 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 06:33:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yang", "Xiufeng", ""], ["Aasawat", "Tanuj Kr", ""], ["Yoshizoe", "Kazuki", ""]]}, {"id": "2006.10587", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky, Tomer Golomb, Yuval Elovici", "title": "Lightweight Collaborative Anomaly Detection for the IoT using Blockchain", "comments": "Preprint of accepted publication, June 2020: Journal of Parallel and\n  Distributed Computing, Elsevier, ISSN: 0743-7315", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their rapid growth and deployment, the Internet of things (IoT) have\nbecome a central aspect of our daily lives. Unfortunately, IoT devices tend to\nhave many vulnerabilities which can be exploited by an attacker. Unsupervised\ntechniques, such as anomaly detection, can be used to secure these devices in a\nplug-and-protect manner.\n  However, anomaly detection models must be trained for a long time in order to\ncapture all benign behaviors. Furthermore, the anomaly detection model is\nvulnerable to adversarial attacks since, during the training phase, all\nobservations are assumed to be benign. In this paper, we propose (1) a novel\napproach for anomaly detection and (2) a lightweight framework that utilizes\nthe blockchain to ensemble an anomaly detection model in a distributed\nenvironment.\n  Blockchain framework incrementally updates a trusted anomaly detection model\nvia self-attestation and consensus among the IoT devices. We evaluate our\nmethod on a distributed IoT simulation platform, which consists of 48 Raspberry\nPis. The simulation demonstrates how the approach can enhance the security of\neach device and the security of the network as a whole.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:50:08 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Mirsky", "Yisroel", ""], ["Golomb", "Tomer", ""], ["Elovici", "Yuval", ""]]}, {"id": "2006.10664", "submitter": "X. Buffat", "authors": "X. Buffat", "title": "Computing techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.acc-ph cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This lecture aims at providing a user's perspective on the main concepts used\nnowadays for the implementation of numerical algorithm on common computing\narchitecture. In particular, the concepts and applications of Central\nProcessing Units (CPUs), vectorisation, multithreading, hyperthreading and\nGraphical Processing Units (GPUs), as well as computer clusters and grid\ncomputing will be discussed. Few examples of source codes illustrating the\nusage of these technologies are provided.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:43:49 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Buffat", "X.", ""]]}, {"id": "2006.10672", "submitter": "Mohammad Mohammadi Amiri Dr.", "authors": "Mohammad Mohammadi Amiri, Deniz Gunduz, Sanjeev R. Kulkarni, H.\n  Vincent Poor", "title": "Federated Learning With Quantized Global Model Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study federated learning (FL), which enables mobile devices to utilize\ntheir local datasets to collaboratively train a global model with the help of a\ncentral server, while keeping data localized. At each iteration, the server\nbroadcasts the current global model to the devices for local training, and\naggregates the local model updates from the devices to update the global model.\nPrevious work on the communication efficiency of FL has mainly focused on the\naggregation of model updates from the devices, assuming perfect broadcasting of\nthe global model. In this paper, we instead consider broadcasting a compressed\nversion of the global model. This is to further reduce the communication cost\nof FL, which can be particularly limited when the global model is to be\ntransmitted over a wireless medium. We introduce a lossy FL (LFL) algorithm, in\nwhich both the global model and the local model updates are quantized before\nbeing transmitted. We analyze the convergence behavior of the proposed LFL\nalgorithm assuming the availability of accurate local model updates at the\nserver. Numerical experiments show that the proposed LFL scheme, which\nquantizes the global model update (with respect to the global model estimate at\nthe devices) rather than the global model itself, significantly outperforms\nother existing schemes studying quantization of the global model at the\nPS-to-device direction. Also, the performance loss of the proposed scheme is\nmarginal compared to the fully lossless approach, where the PS and the devices\ntransmit their messages entirely without any quantization.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 16:55:20 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 01:15:06 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2006.10698", "submitter": "Tim Roughgarden", "authors": "Andrew Lewis-Pye and Tim Roughgarden", "title": "Resource Pools and the CAP Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain protocols differ in fundamental ways, including the mechanics of\nselecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and\nthe method to establish consensus (e.g., longest chain rules vs. BFT-inspired\nprotocols). These fundamental differences have hindered \"apples-to-apples\"\ncomparisons between different categories of blockchain protocols and, in turn,\nthe development of theory to formally discuss their relative merits.\n  This paper presents a parsimonious abstraction sufficient for capturing and\ncomparing properties of many well-known permissionless blockchain protocols,\nsimultaneously capturing essential properties of both proof-of-work and\nproof-of-stake protocols, and of both longest-chain-type and BFT-type\nprotocols. Our framework blackboxes the precise mechanics of the user selection\nprocess, allowing us to isolate the properties of the selection process which\nare significant for protocol design.\n  We illustrate our framework's utility with two results. First, we prove an\nanalog of the CAP theorem from distributed computing for our framework in a\npartially synchronous setting. This theorem shows that a fundamental dichotomy\nholds between protocols (such as Bitcoin) that are adaptive, in the sense that\nthey can function given unpredictable levels of participation, and protocols\n(such as Algorand) that have certain finality properties. Second, we formalize\nthe idea that proof-of-work (PoW) protocols and non-PoW protocols can be\ndistinguished by the forms of permission that users are given to carry out\nupdates to the state.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:33:07 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Lewis-Pye", "Andrew", ""], ["Roughgarden", "Tim", ""]]}, {"id": "2006.10901", "submitter": "Trevor Gale", "authors": "Trevor Gale, Matei Zaharia, Cliff Young, Erich Elsen", "title": "Sparse GPU Kernels for Deep Learning", "comments": "Updated to match camera-ready for SC20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific workloads have traditionally exploited high levels of sparsity to\naccelerate computation and reduce memory requirements. While deep neural\nnetworks can be made sparse, achieving practical speedups on GPUs is difficult\nbecause these applications have relatively moderate levels of sparsity that are\nnot sufficient for existing sparse kernels to outperform their dense\ncounterparts. In this work, we study sparse matrices from deep learning\napplications and identify favorable properties that can be exploited to\naccelerate computation. Based on these insights, we develop high-performance\nGPU kernels for two sparse matrix operations widely applicable in neural\nnetworks: sparse matrix-dense matrix multiplication and sampled dense-dense\nmatrix multiplication. Our kernels reach 27% of single-precision peak on Nvidia\nV100 GPUs. Using our kernels, we demonstrate sparse Transformer and MobileNet\nmodels that achieve 1.2-2.1x speedups and up to 12.8x memory savings without\nsacrificing accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 23:59:11 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 18:35:07 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Gale", "Trevor", ""], ["Zaharia", "Matei", ""], ["Young", "Cliff", ""], ["Elsen", "Erich", ""]]}, {"id": "2006.10937", "submitter": "Kavya Kopparapu", "authors": "Kavya Kopparapu, Eric Lin", "title": "FedFMC: Sequential Efficient Federated Learning on Non-iid Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a mechanism for devices to update a global model without sharing data,\nfederated learning bridges the tension between the need for data and respect\nfor privacy. However, classic FL methods like Federated Averaging struggle with\nnon-iid data, a prevalent situation in the real world. Previous solutions are\nsub-optimal as they either employ a small shared global subset of data or\ngreater number of models with increased communication costs. We propose FedFMC\n(Fork-Merge-Consolidate), a method that dynamically forks devices into updating\ndifferent global models then merges and consolidates separate models into one.\nWe first show the soundness of FedFMC on simple datasets, then run several\nexperiments comparing against baseline approaches. These experiments show that\nFedFMC substantially improves upon earlier approaches to non-iid data in the\nfederated learning context without using a globally shared subset of data nor\nincrease communication costs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 02:36:17 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kopparapu", "Kavya", ""], ["Lin", "Eric", ""]]}, {"id": "2006.11062", "submitter": "Sebastian Litzinger", "authors": "J\\\"org Keller and Sebastian Litzinger", "title": "Influence of Incremental Constraints on Energy Consumption and Static\n  Scheduling Time for Moldable Tasks with Deadline", "comments": "Presented at the 13th International Workshop on Programmability and\n  Architectures for Heterogeneous Multicores, 2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "Report-no: MULTIPROG/2020/5", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static scheduling of independent, moldable tasks on parallel machines with\nfrequency scaling comprises decisions on core allocation, assignment, frequency\nscaling and ordering, to meet a deadline and minimize energy consumption.\nConstraining some of these decisions reduces the solution space, i.e. may\nincrease energy consumption, but may also reduce scheduling time or give the\nchance to tackle larger task sets. We investigate the influence of different\nconstraints that lead from an unrestricted scheduler via two intermediate steps\nto the crown scheduler, by presenting integer linear programs for all four\nschedulers. We compare scheduling time and energy consumption for a benchmark\nsuite of synthetic task sets of different sizes. Our results indicate that the\nfinal step towards the crown scheduler -- the execution order constraint -- is\nresponsible for faster scheduling when task sets are small, and lower energy\nconsumption when we deal with large task sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 10:39:20 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Keller", "J\u00f6rg", ""], ["Litzinger", "Sebastian", ""]]}, {"id": "2006.11256", "submitter": "Alexander Stolyar", "authors": "Seva Shneer, Alexander Stolyar", "title": "Large-scale parallel server system with multi-component jobs", "comments": "21 pages, 1 figure. Revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad class of parallel server systems is considered, for which we prove\nthe steady-state asymptotic independence of server workloads, as the number of\nservers goes to infinity, while the system load remains sub-critical. Arriving\njobs consist of multiple components. There are multiple job classes, and each\nclass may be of one of two types, which determines the rule according to which\nthe job components add workloads to the servers. The model is broad enough to\ninclude as special cases some popular queueing models with redundancy, such as\ncancel-on-start and cancel-on-completion redundancy.\n  Our analysis uses mean-field process representation and the corresponding\nmean-field limits. In essence, our approach relies almost exclusively on three\nfundamental properties of the model: (a) monotonicity, (b) work conservation,\n(c) the property that, on average, \"new arriving workload prefers to go to\nservers with lower workloads.\"\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:42:44 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 19:39:39 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Shneer", "Seva", ""], ["Stolyar", "Alexander", ""]]}, {"id": "2006.11522", "submitter": "Mayra Samaniego Mrs", "authors": "Mayra Samaniego, Sara Hosseinzadeh Kassani, Cristian Espana, Ralph\n  Deters", "title": "Access Control Management for Computer-Aided Diagnosis Systems using\n  Blockchain", "comments": "5 pages, 7 figures, 1 table, 2020 IEEE International Conference on\n  Smart Internet of Things (SmartIoT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-Aided Diagnosis (CAD) systems have emerged to support clinicians in\ninterpreting medical images. CAD systems are traditionally combined with\nartificial intelligence (AI), computer vision, and data augmentation to\nevaluate suspicious structures in medical images. This evaluation generates\nvast amounts of data. Traditional CAD systems belong to a single institution\nand handle data access management centrally. However, the advent of CAD systems\nfor research among multiple institutions demands distributed access management.\nThis research proposes a blockchain-based solution to enable distributed data\naccess management in CAD systems. This solution has been developed as a\ndistributed application (DApp) using Ethereum in a consortium network.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 08:42:20 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Samaniego", "Mayra", ""], ["Kassani", "Sara Hosseinzadeh", ""], ["Espana", "Cristian", ""], ["Deters", "Ralph", ""]]}, {"id": "2006.11601", "submitter": "Chee Seng Chan", "authors": "Lixin Fan, Kam Woh Ng, Ce Ju, Tianyu Zhang, Chang Liu, Chee Seng Chan,\n  Qiang Yang", "title": "Rethinking Privacy Preserving Deep Learning: How to Evaluate and Thwart\n  Privacy Attacks", "comments": "under review, 36 pages (updated Eq. 3 and Fig. 8)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates capabilities of Privacy-Preserving Deep Learning\n(PPDL) mechanisms against various forms of privacy attacks. First, we propose\nto quantitatively measure the trade-off between model accuracy and privacy\nlosses incurred by reconstruction, tracing and membership attacks. Second, we\nformulate reconstruction attacks as solving a noisy system of linear equations,\nand prove that attacks are guaranteed to be defeated if condition (2) is\nunfulfilled. Third, based on theoretical analysis, a novel Secret Polarization\nNetwork (SPN) is proposed to thwart privacy attacks, which pose serious\nchallenges to existing PPDL methods. Extensive experiments showed that model\naccuracies are improved on average by 5-20% compared with baseline mechanisms,\nin regimes where data privacy are satisfactorily protected.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 15:48:57 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 14:45:58 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Fan", "Lixin", ""], ["Ng", "Kam Woh", ""], ["Ju", "Ce", ""], ["Zhang", "Tianyu", ""], ["Liu", "Chang", ""], ["Chan", "Chee Seng", ""], ["Yang", "Qiang", ""]]}, {"id": "2006.11711", "submitter": "Yuan Wang", "authors": "Yuan Wang, Hideaki Ishii, Fran\\c{c}ois Bonnet, Xavier D\\'efago", "title": "Resilient Consensus Against Mobile Malicious Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses novel consensus problems in the presence of adversaries\nthat can move within the network and induce faulty behaviors in the attacked\nagents. By adopting several mobile adversary models from the computer science\nliterature, we develop protocols which can mitigate the influence of such\nmalicious agents. The algorithms follow the class of mean subsequence reduced\n(MSR) algorithms, under which agents ignore the suspicious values received from\nneighbors during their state updates. Different from the static adversary\nmodels, even after the adversaries move away, the infected agents may remain\nfaulty in their values, whose effects must be taken into account. We develop\nconditions on the network structures for both the complete and non-complete\ngraph cases, under which the proposed algorithms are guaranteed to attain\nresilient consensus. Extensive simulations are carried out over random graphs\nto verify the effectiveness of our approach under uncertainties in the systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 04:44:47 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Wang", "Yuan", ""], ["Ishii", "Hideaki", ""], ["Bonnet", "Fran\u00e7ois", ""], ["D\u00e9fago", "Xavier", ""]]}, {"id": "2006.11869", "submitter": "Gabor Elek", "authors": "G\\'abor Elek", "title": "Planarity is (almost) locally checkable in constant-time", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally checkable proofs for graph properties were introduced by G\\\"o\\\"os and\nSuomela \\cite{GS}. Roughly speaking, a graph property $\\cP$ is locally\ncheckable in constant-time, if the vertices of a graph having the property can\nbe convinced, in a short period of time not depending on the size of the graph,\nthat they are indeed vertices of a graph having the given property.\n  For a given $\\eps>0$, we call a property $\\cP$ $\\eps$-locally checkable in\nconstant-time if the vertices of a graph having the given property can be\nconvinced at least that they are in a graph $\\eps$-close to the given property.\nWe say that a property $\\cP$ is almost locally checkable in constant-time, if\nfor all $\\eps>0$, $\\cP$ is $\\eps$-locally checkable in constant-time. It is not\nhard to see that in the universe of bounded degree graphs planarity is not\nlocally checkable in constant-time. However, the main result of this paper is\nthat planarity of bounded degree graphs is almost locally checkable in\nconstant-time. The proof is based on the surprising fact that although graphs\ncannot be convinced by their planarity or hyperfiniteness, planar graphs can be\nconvinced by their own hyperfiniteness. The reason behind this fact is that the\nclass of planar graphs are not only hyperfinite but possesses Property A of Yu.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 18:34:42 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Elek", "G\u00e1bor", ""]]}, {"id": "2006.12144", "submitter": "Shady Issa", "authors": "Alex Kogan, Dave Dice, Shady Issa", "title": "Scalable Range Locks for Scalable Address Spaces and Beyond", "comments": "17 pages, 9 figures, Eurosys 2020", "journal-ref": null, "doi": "10.1145/3342195.3387533", "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Range locks are a synchronization construct designed to provide concurrent\naccess to multiple threads (or processes) to disjoint parts of a shared\nresource. Originally conceived in the file system context, range locks are\ngaining increasing interest in the Linux kernel community seeking to alleviate\nbottlenecks in the virtual memory management subsystem. The existing\nimplementation of range locks in the kernel, however, uses an internal spin\nlock to protect the underlying tree structure that keeps track of acquired and\nrequested ranges. This spin lock becomes a point of contention on its own when\nthe range lock is frequently acquired. Furthermore, where and exactly how\nspecific (refined) ranges can be locked remains an open question.\n  In this paper, we make two independent, but related contributions. First, we\npropose an alternative approach for building range locks based on linked lists.\nThe lists are easy to maintain in a lock-less fashion, and in fact, our range\nlocks do not use any internal locks in the common case. Second, we show how the\nrange of the lock can be refined in the mprotect operation through a\nspeculative mechanism. This refinement, in turn, allows concurrent execution of\nmprotect operations on non-overlapping memory regions. We implement our new\nalgorithms and demonstrate their effectiveness in user-space and kernel-space,\nachieving up to 9$\\times$ speedup compared to the stock version of the Linux\nkernel. Beyond the virtual memory management subsystem, we discuss other\napplications of range locks in parallel software. As a concrete example, we\nshow how range locks can be used to facilitate the design of scalable\nconcurrent data structures, such as skip lists.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 11:12:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kogan", "Alex", ""], ["Dice", "Dave", ""], ["Issa", "Shady", ""]]}, {"id": "2006.12203", "submitter": "Alessandro Patti", "authors": "Luca Tonti and Alessandro Patti", "title": "Fast overlap detection between hard-core colloidal cuboids and spheres.\n  The OCSI algorithm", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision between rigid three-dimensional objects is a very common modelling\nproblem in a wide spectrum of scientific disciplines, including Computer\nScience and Physics. It spans from realistic animation of polyhedral shapes for\ncomputer vision to the description of thermodynamic and dynamic properties in\nsimple and complex fluids. For instance, colloidal particles of especially\nexotic shapes are commonly modelled as hard-core objects, whose collision test\nis key to correctly determine their phase and aggregation behaviour. In this\nwork, we propose the OpenMP Cuboid Sphere Intersection (OCSI) algorithm to\ndetect collisions between prolate or oblate cuboids and spheres. We investigate\nOCSI's performance by bench-marking it against a number of algorithms commonly\nemployed in computer graphics and colloidal science: Quick Rejection First\n(QRI), Quick Rejection Intertwined (QRF) and SIMD Streaming Extensions (SSE).\nWe observed that QRI and QRF significantly depend on the specific cuboid\nanisotropy and sphere radius, while SSE and OCSI maintain their speed\nindependently of the objects' geometry. While OCSI and SSE, both based on SIMD\nparallelization, show excellent and very similar performance, the former\nprovides a more accessible coding and user-friendly implementation as it\nexploits OpenMP directives for automatic vectorization.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:52:19 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Tonti", "Luca", ""], ["Patti", "Alessandro", ""]]}, {"id": "2006.12238", "submitter": "Shaocheng Huang", "authors": "Shaocheng Huang, Yu Ye, Ming Xiao, H. Vincent Poor and Mikael Skoglund", "title": "Decentralized Beamforming Design for Intelligent Reflecting\n  Surface-enhanced Cell-free Networks", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell-free networks are considered as a promising distributed network\narchitecture to satisfy the increasing number of users and high rate\nexpectations in beyond-5G systems. However, to further enhance network\ncapacity, an increasing number of high-cost base stations (BSs) are required.\nTo address this problem and inspired by the cost-effective intelligent\nreflecting surface (IRS) technique, we propose a fully decentralized design\nframework for cooperative beamforming in IRS-aided cell-free networks. We first\ntransform the centralized weighted sum-rate maximization problem into a\ntractable consensus optimization problem, and then an incremental alternating\ndirection method of multipliers (ADMM) algorithm is proposed to locally update\nthe beamformer. The complexity and convergence of the proposed method are\nanalyzed, and these results show that the performance of the new scheme can\nasymptotically approach that of the centralized one as the number of iterations\nincreases. Results also show that IRSs can significantly increase the system\nsum-rate of cell-free networks and the proposed method outperforms existing\ndecentralized methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 13:33:12 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Shaocheng", ""], ["Ye", "Yu", ""], ["Xiao", "Ming", ""], ["Poor", "H. Vincent", ""], ["Skoglund", "Mikael", ""]]}, {"id": "2006.12274", "submitter": "Andreas Bytyn", "authors": "Andreas Bytyn, Ren\\'e Ahlsdorf, Rainer Leupers, Gerd Ascheid", "title": "Dataflow Aware Mapping of Convolutional Neural Networks Onto Many-Core\n  Platforms With Network-on-Chip Interconnect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine intelligence, especially using convolutional neural networks (CNNs),\nhas become a large area of research over the past years. Increasingly\nsophisticated hardware accelerators are proposed that exploit e.g. the sparsity\nin computations and make use of reduced precision arithmetic to scale down the\nenergy consumption. However, future platforms require more than just energy\nefficiency: Scalability is becoming an increasingly important factor. The\nrequired effort for physical implementation grows with the size of the\naccelerator making it more difficult to meet target constraints. Using\nmany-core platforms consisting of several homogeneous cores can alleviate the\naforementioned limitations with regard to physical implementation at the\nexpense of an increased dataflow mapping effort. While the dataflow in CNNs is\ndeterministic and can therefore be optimized offline, the problem of finding a\nsuitable scheme that minimizes both runtime and off-chip memory accesses is a\nchallenging task which becomes even more complex if an interconnect system is\ninvolved. This work presents an automated mapping strategy starting at the\nsingle-core level with different optimization targets for minimal runtime and\nminimal off-chip memory accesses. The strategy is then extended towards a\nsuitable many-core mapping scheme and evaluated using a scalable system-level\nsimulation with a network-on-chip interconnect. Design space exploration is\nperformed by mapping the well-known CNNs AlexNet and VGG-16 to platforms of\ndifferent core counts and computational power per core in order to investigate\nthe trade-offs. Our mapping strategy and system setup is scaled starting from\nthe single core level up to 128 cores, thereby showing the limits of the\nselected approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:13:18 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bytyn", "Andreas", ""], ["Ahlsdorf", "Ren\u00e9", ""], ["Leupers", "Rainer", ""], ["Ascheid", "Gerd", ""]]}, {"id": "2006.12276", "submitter": "Francois Taiani", "authors": "Alex Auvolat (Wide, Univ-Rennes, Irisa, Di-Ens, Cnrs), Davide Frey\n  (Wide, Univ-Rennes, Irisa, Cnrs), Michel Raynal (Wide, Univ-Rennes, Irisa,\n  Polyu, Cnrs), Fran\\c{c}ois Ta\\\"iani (Wide, Irisa, Cnrs, Univ-Rennes)", "title": "Money Transfer Made Simple: a Specification, a Generic Algorithm, and\n  its Proof", "comments": null, "journal-ref": "Bulletin of Association for Theoretical Computer Science (BEATCS),\n  Kazuo Iwama, RIMS, Kyoto University Kitashirakawa-Oiwakecho, Kyoto 606-8502,\n  Japan, 2020, 132", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that, contrarily to a common belief, money\ntransfer in the presence of faulty (Byzantine) processes does not require\nstrong agreement such as consensus. This article goes one step further: namely,\nit first proposes a non-sequential specification of the money-transfer object,\nand then presents a generic algorithm based on a simple FIFO order between each\npair of processes that implements it. The genericity dimension lies in the\nunderlying reliable broadcast abstraction which must be suited to the\nappropriate failure model. Interestingly, whatever the failure model, the money\ntransfer algorithm only requires adding a single sequence number to its\nmessages as control information. Moreover, as a side effect of the proposed\nalgorithm, it follows that money transfer is a weaker problem than the\nconstruction of a safe/regular/atomic read/write register in the asynchronous\nmessage-passing crash-prone model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:01:34 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 13:53:00 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Auvolat", "Alex", "", "Wide, Univ-Rennes, Irisa, Di-Ens, Cnrs"], ["Frey", "Davide", "", "Wide, Univ-Rennes, Irisa, Cnrs"], ["Raynal", "Michel", "", "Wide, Univ-Rennes, Irisa,\n  Polyu, Cnrs"], ["Ta\u00efani", "Fran\u00e7ois", "", "Wide, Irisa, Cnrs, Univ-Rennes"]]}, {"id": "2006.12560", "submitter": "Jashwant Raj Gunasekaran", "authors": "Jashwant Raj Gunasekaran, Michael Cui, Prashanth Thinakaran, Josh\n  Simons, Mahmut Taylan Kandemir and Chita R. Das", "title": "Multiverse: Dynamic VM Provisioning for Virtualized High Performance\n  Computing Clusters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, HPC workloads have been deployed in bare-metal clusters; but\nthe advances in virtualization have led the pathway for these workloads to be\ndeployed in virtualized clusters. However, HPC cluster administrators/providers\nstill face challenges in terms of resource elasticity and virtual machine (VM)\nprovisioning at large-scale, due to the lack of coordination between a\ntraditional HPC scheduler and the VM hypervisor (resource management layer).\nThis lack of interaction leads to low cluster utilization and job completion\nthroughput. Furthermore, the VM provisioning delays directly impact the overall\nperformance of jobs in the cluster. Hence, there is a need for effectively\nprovisioning virtualized HPC clusters, which can best-utilize the physical\nhardware with minimal provisioning overheads.\n  Towards this, we propose Multiverse, a VM provisioning framework, which can\ndynamically spawn VMs for incoming jobs in a virtualized HPC cluster, by\nintegrating the HPC scheduler along with VM resource manager. We have\nimplemented this framework on the Slurm} scheduler along with the vSphere VM\nresource manager. In order to reduce the VM provisioning overheads, we use\ninstant cloning which shares both the disk and memory with the parent VM, when\ncompared to full VM cloning which has to boot-up a new VM from scratch.\nMeasurements with real-world HPC workloads demonstrate that, instant cloning is\n2.5x faster than full cloning in terms of VM provisioning time. Further, it\nimproves resource utilization by up to 40%, and cluster throughput by up to\n1.5x, when compared to full clone for bursty job arrival scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 18:46:40 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Gunasekaran", "Jashwant Raj", ""], ["Cui", "Michael", ""], ["Thinakaran", "Prashanth", ""], ["Simons", "Josh", ""], ["Kandemir", "Mahmut Taylan", ""], ["Das", "Chita R.", ""]]}, {"id": "2006.12575", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Can Zhao, Wenqi Li, Holger Roth, Ziyue Xu, Daguang Xu", "title": "LAMP: Large Deep Nets with Automated Model Parallelism for Image\n  Segmentation", "comments": "MICCAI 2020 Early Accepted paper. Code is\n  available\\footnote{https://monai.io/research/lamp-automated-model-parallelism}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) models are becoming larger, because the increase in model\nsize might offer significant accuracy gain. To enable the training of large\ndeep networks, data parallelism and model parallelism are two well-known\napproaches for parallel training. However, data parallelism does not help\nreduce memory footprint per device. In this work, we introduce Large deep 3D\nConvNets with Automated Model Parallelism (LAMP) and investigate the impact of\nboth input's and deep 3D ConvNets' size on segmentation accuracy. Through\nautomated model parallelism, it is feasible to train large deep 3D ConvNets\nwith a large input patch, even the whole image. Extensive experiments\ndemonstrate that, facilitated by the automated model parallelism, the\nsegmentation accuracy can be improved through increasing model size and input\ncontext size, and large input yields significant inference speedup compared\nwith sliding window of small patches in the inference. Code is\navailable\\footnote{https://monai.io/research/lamp-automated-model-parallelism}.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:20:35 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 19:41:26 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 17:51:41 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zhu", "Wentao", ""], ["Zhao", "Can", ""], ["Li", "Wenqi", ""], ["Roth", "Holger", ""], ["Xu", "Ziyue", ""], ["Xu", "Daguang", ""]]}, {"id": "2006.12587", "submitter": "Thomas Rausch", "authors": "Thomas Rausch and Waldemar Hummer and Vinod Muthusamy", "title": "PipeSim: Trace-driven Simulation of Large-Scale AI Operations Platforms", "comments": "11 pages, 13 figures, extended version of OpML'20 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operationalizing AI has become a major endeavor in both research and\nindustry. Automated, operationalized pipelines that manage the AI application\nlifecycle will form a significant part of tomorrow's infrastructure workloads.\nTo optimize operations of production-grade AI workflow platforms we can\nleverage existing scheduling approaches, yet it is challenging to fine-tune\noperational strategies that achieve application-specific cost-benefit tradeoffs\nwhile catering to the specific domain characteristics of machine learning (ML)\nmodels, such as accuracy, robustness, or fairness. We present a trace-driven\nsimulation-based experimentation and analytics environment that allows\nresearchers and engineers to devise and evaluate such operational strategies\nfor large-scale AI workflow systems. Analytics data from a production-grade AI\nplatform developed at IBM are used to build a comprehensive simulation model.\nOur simulation model describes the interaction between pipelines and system\ninfrastructure, and how pipeline tasks affect different ML model metrics. We\nimplement the model in a standalone, stochastic, discrete event simulator, and\nprovide a toolkit for running experiments. Synthetic traces are made available\nfor ad-hoc exploration as well as statistical analysis of experiments to test\nand examine pipeline scheduling, cluster resource allocation, and similar\noperational mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 19:55:37 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Rausch", "Thomas", ""], ["Hummer", "Waldemar", ""], ["Muthusamy", "Vinod", ""]]}, {"id": "2006.12608", "submitter": "Francesco Silvestri", "authors": "Thomas D. Ahle and Francesco Silvestri", "title": "Similarity Search with Tensor Core Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Core Units (TCUs) are hardware accelerators developed for deep neural\nnetworks, which efficiently support the multiplication of two dense\n$\\sqrt{m}\\times \\sqrt{m}$ matrices, where $m$ is a given hardware parameter. In\nthis paper, we show that TCUs can speed up similarity search problems as well.\nWe propose algorithms for the Johnson-Lindenstrauss dimensionality reduction\nand for similarity join that, by leveraging TCUs, achieve a $\\sqrt{m}$ speedup\nup with respect to traditional approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 20:47:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Ahle", "Thomas D.", ""], ["Silvestri", "Francesco", ""]]}, {"id": "2006.12727", "submitter": "Xiangqiang Gao", "authors": "Xiangqiang Gao, Rongke liu, Aryan Kaushik", "title": "Service Chaining Placement Based on Satellite Mission Planning in Ground\n  Station Networks", "comments": "IEEE Transactions on Network and Service Management (2020)", "journal-ref": null, "doi": "10.1109/TNSM.2020.3045432", "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the increase in satellite number and variety, satellite ground stations\nshould be required to offer user services in a flexible and efficient manner.\nNetwork function virtualization (NFV) can provide a new paradigm to allocate\nnetwork resources on-demand for user services over the underlying network.\nHowever, most of the existing work focuses on the virtual network function\n(VNF) placement and routing traffic problem for enterprise data center\nnetworks, the issue needs to further study in satellite communication\nscenarios. In this paper, we investigate the VNF placement and routing traffic\nproblem in satellite ground station networks. We formulate the problem of\nresource allocation as an integer nonlinear programming (INLP) model and the\nobjective is to minimize the link resource utilization and the number of\nservers used. Considering the information about satellite orbit fixation and\nmission planning, we propose location-aware resource allocation (LARA)\nalgorithms based on Greedy and IBM CPLEX 12.10, respectively. The proposed LARA\nalgorithm can assist in deploying VNFs and routing traffic flows by predicting\nthe running conditions of user services. We evaluate the performance of our\nproposed LARA algorithm in three networks of Fat-Tree, BCube, and VL2.\nSimulation results show that our proposed LARA algorithm performs better than\nthat without prediction, and can effectively decrease the average resource\nutilization of satellite ground station networks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 03:52:15 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:07:48 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 08:50:21 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Gao", "Xiangqiang", ""], ["liu", "Rongke", ""], ["Kaushik", "Aryan", ""]]}, {"id": "2006.12819", "submitter": "Zhaokang Wang", "authors": "Zhaokang Wang, Weiwei Hu, Chunfeng Yuan, Rong Gu, Yihua Huang", "title": "Distributed Subgraph Enumeration via Backtracking-based Framework", "comments": "Modify some terms;Fix typos; Unify line styles in Fig. 13 and Fig. 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding or monitoring subgraph instances that are isomorphic to a given\npattern graph in a data graph is a fundamental query operation in many graph\nanalytic applications, such as network motif mining and fraud detection. The\nstate-of-the-art distributed methods are inefficient in communication. They\nhave to shuffle partial matching results during the distributed multiway join.\nThe partial matching results may be much larger than the data graph itself. To\novercome the drawback, we develop the Batch-BENU framework (B-BENU) for\ndistributed subgraph enumeration. B-BENU executes a group of local search tasks\nin parallel. Each task enumerates subgraphs around a vertex in the data graph,\nguided by a backtracking-based execution plan. B-BENU does not shuffle any\npartial matching result. Instead, it stores the data graph in a distributed\ndatabase. Each task queries adjacency sets of the data graph on demand. To\nsupport dynamic data graphs, we propose the concept of incremental pattern\ngraphs and turn continuous subgraph enumeration into enumerating incremental\npattern graphs at each time step. We develop the Streaming-BENU framework\n(S-BENU) to enumerate their matches efficiently. We implement B-BENU and S-BENU\nwith the local database cache and the task splitting techniques. The extensive\nexperiments show that B-BENU and S-BENU can scale to big data graphs and\ncomplex pattern graphs. They outperform the state-of-the-art methods by up to\none and two orders of magnitude, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 08:23:59 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:18:47 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 03:23:51 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Wang", "Zhaokang", ""], ["Hu", "Weiwei", ""], ["Yuan", "Chunfeng", ""], ["Gu", "Rong", ""], ["Huang", "Yihua", ""]]}, {"id": "2006.12889", "submitter": "Arik Rinberg", "authors": "Arik Rinberg and Idit Keidar", "title": "Intermediate Value Linearizability: A Quantitative Correctness Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data processing systems often employ batched updates and data sketches to\nestimate certain properties of large data. For example, a CountMin sketch\napproximates the frequencies at which elements occur in a data stream, and a\nbatched counter counts events in batches. This paper focuses on the correctness\nof concurrent implementations of such objects. Specifically, we consider\nquantitative objects, whose return values are from a totally ordered domain,\nwith an emphasis on $(e,d)$-bounded objects that estimate a quantity with an\nerror of at most $e$ with probability at least $1 - d$.\n  The de facto correctness criterion for concurrent objects is linearizability.\nUnder linearizability, when a read overlaps an update, it must return the\nobject's value either before the update or after it. Consider, for example, a\nsingle batched increment operation that counts three new events, bumping a\nbatched counter's value from $7$ to $10$. In a linearizable implementation of\nthe counter, an overlapping read must return one of these. We observe, however,\nthat in typical use cases, any intermediate value would also be acceptable. To\ncapture this degree of freedom, we propose Intermediate Value Linearizability\n(IVL), a new correctness criterion that relaxes linearizability to allow\nreturning intermediate values, for instance $8$ in the example above. Roughly\nspeaking, IVL allows reads to return any value that is bounded between two\nreturn values that are legal under linearizability. A key feature of IVL is\nthat concurrent IVL implementations of $(e,d)$-bounded objects remain\n$(e,d)$-bounded. To illustrate the power of this result, we give a\nstraightforward and efficient concurrent implementation of an $(e, d)$-bounded\nCountMin sketch, which is IVL (albeit not linearizable). Finally, we show that\nIVL allows for inherently cheaper implementations than linearizable ones.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 10:45:24 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Rinberg", "Arik", ""], ["Keidar", "Idit", ""]]}, {"id": "2006.13041", "submitter": "Deepesh Data", "authors": "Deepesh Data and Suhas Diggavi", "title": "Byzantine-Resilient High-Dimensional Federated Learning", "comments": "33 pages; title change; improved bound on the approximation error by\n  the factor of H", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study stochastic gradient descent (SGD) with local iterations in the\npresence of malicious/Byzantine clients, motivated by the federated learning.\nThe clients, instead of communicating with the central server in every\niteration, maintain their local models, which they update by taking several SGD\niterations based on their own datasets and then communicate the net update with\nthe server, thereby achieving communication-efficiency. Furthermore, only a\nsubset of clients communicate with the server, and this subset may be different\nat different synchronization times. The Byzantine clients may collaborate and\nsend arbitrary vectors to the server to disrupt the learning process. To combat\nthe adversary, we employ an efficient high-dimensional robust mean estimation\nalgorithm from Steinhardt et al.~\\cite[ITCS 2018]{Resilience_SCV18} at the\nserver to filter-out corrupt vectors; and to analyze the outlier-filtering\nprocedure, we develop a novel matrix concentration result that may be of\nindependent interest.\n  We provide convergence analyses for strongly-convex and non-convex smooth\nobjectives in the heterogeneous data setting, where different clients may have\ndifferent local datasets, and we do not make any probabilistic assumptions on\ndata generation. We believe that ours is the first Byzantine-resilient\nalgorithm and analysis with local iterations. We derive our convergence results\nunder minimal assumptions of bounded variance for SGD and bounded gradient\ndissimilarity (which captures heterogeneity among local datasets). We also\nextend our results to the case when clients compute full-batch gradients.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 03:54:36 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 21:24:14 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Data", "Deepesh", ""], ["Diggavi", "Suhas", ""]]}, {"id": "2006.13087", "submitter": "Marko Vukoli\\'c", "authors": "Marko Vukolic", "title": "On the Interoperability of Decentralized Exposure Notification Systems", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report summarizes the requirements and proposes a high-level solution\nfor interoperability across recently proposed COVID-19 exposure notification\nefforts. Our focus is on interoperability across exposure notification (EN)\napplications which are based on the decentralized Bluetooth Low Energy (BLE)\nprotocol driven by Google/Apple Exposure Notifications API (including DP3T and\nsimilar protocols). We distinguish different interoperability use cases, such\nas worldwide public EN interoperability, as well as interoperability in the\nenterprise EN systems. This report also proposes an API and a backend\nimplementation architecture for EN interoperability. Finally, we propose using\na permissioned blockchain-based solution for managing EN backend certificates\nand configurations (without storing any users' data on the blockchain) for\nhelping address EN interoperability challenges across different vendors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:24:14 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Vukolic", "Marko", ""]]}, {"id": "2006.13109", "submitter": "Saurabh Deochake", "authors": "Saurabh Deochake and Debajyoti Mukhopadhyay", "title": "An Agent-based Cloud Service Negotiation in Hybrid Cloud Computing", "comments": "Fifth International Conference on ICT for Sustainable Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of evolution of cloud computing, large organizations have\nbeen scaling the on-premise IT infrastructure to the cloud. Although this being\na popular practice, it lacks comprehensive efforts to study the aspects of\nautomated negotiation of resources among cloud customers and providers. This\npaper proposes a full-fledged framework for the multi-party, multi-issue\nnegotiation system for cloud resources. It introduces a robust cloud\nmarketplace system to buy and sell cloud resources. The Belief-Desire-Intention\n(BDI) model-based cloud customer and provider agents concurrently negotiate on\nmultiple issues, pursuing a hybrid tactic of time and resource-based dynamic\ndeadline algorithms to generate offers and counter-offers. The cloud\nmarketplace-based system is further augmented with the assignment of behavior\nnorm score and reputation index to the agents to establish trust among them.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 05:23:38 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Deochake", "Saurabh", ""], ["Mukhopadhyay", "Debajyoti", ""]]}, {"id": "2006.13112", "submitter": "Andreas Jocksch", "authors": "Andreas Jocksch and Noe Ohana and Emmanuel Lanti and Vasileios\n  Karakasis and Laurent Villard", "title": "Optimised allgatherv, reduce_scatter and allreduce communication in\n  message-passing systems", "comments": "accepted at PASC 2020/2021 conference as a poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective communications, namely the patterns allgatherv, reduce_scatter,\nand allreduce in message-passing systems are optimised based on measurements at\nthe installation time of the library. The algorithms used are set up in an\ninitialisation phase of the communication, similar to the method used in\nso-called persistent collective communication introduced in the literature. For\nallgatherv and reduce_scatter the existing algorithms, recursive\nmultiply/divide and cyclic shift (Bruck's algorithm) are applied with a\nflexible number of communication ports per node. The algorithms for equal\nmessage sizes are used with non-equal message sizes together with a heuristic\nfor rank reordering. The two communication patterns are applied in a plasma\nphysics application that uses a specialised matrix-vector multiplication. For\nthe allreduce pattern the cyclic shift algorithm is applied with a prefix\noperation. The data is gathered and scattered by the cores within the node and\nthe communication algorithms are applied across the nodes. In general our\nroutines outperform the non-persistent counterparts in established MPI\nlibraries by up to one order of magnitude or show equal performance, with a few\nexceptions of number of nodes and message sizes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:05:04 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Jocksch", "Andreas", ""], ["Ohana", "Noe", ""], ["Lanti", "Emmanuel", ""], ["Karakasis", "Vasileios", ""], ["Villard", "Laurent", ""]]}, {"id": "2006.13378", "submitter": "Tianyi Liu", "authors": "Tianyi Liu, Sen He, Sunzhou Huang, Danny Tsang, Lingjia Tang, Jason\n  Mars, and Wei Wang", "title": "A Benchmarking Framework for Interactive 3D Applications in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of cloud gaming and cloud virtual reality (VR),\ninteractive 3D applications have become a major type of workloads for the\ncloud. However, despite their growing importance, there is limited public\nresearch on how to design cloud systems to efficiently support these\napplications, due to the lack of an open and reliable research infrastructure,\nincluding benchmarks and performance analysis tools. The challenges of\ngenerating human-like inputs under various system/application randomness and\ndissecting the performance of complex graphics systems make it very difficult\nto design such an infrastructure. In this paper, we present the design of a\nnovel cloud graphics rendering research infrastructure, Pictor. Pictor employs\nAI to mimic human interactions with complex 3D applications. It can also\nprovide in-depth performance measurements for the complex software and hardware\nstack used for cloud 3D graphics rendering. With Pictor, we designed a\nbenchmark suite with six interactive 3D applications. Performance analyses were\nconducted with these benchmarks to characterize 3D applications in the cloud\nand reveal new performance bottlenecks. To demonstrate the effectiveness of\nPictor, we also implemented two optimizations to address two performance\nbottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering\nsystem, which improved the frame rate by 57.7% on average.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:11:30 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 16:06:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Tianyi", ""], ["He", "Sen", ""], ["Huang", "Sunzhou", ""], ["Tsang", "Danny", ""], ["Tang", "Lingjia", ""], ["Mars", "Jason", ""], ["Wang", "Wei", ""]]}, {"id": "2006.13421", "submitter": "Jayanth Regatti", "authors": "Jayanth Regatti, Hao Chen and Abhishek Gupta", "title": "ByGARS: Byzantine SGD with Arbitrary Number of Attackers", "comments": "16 pages, 8 figures. Added more empirical results against\n  state-of-the-art attacks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel stochastic gradient descent algorithms, ByGARS and\nByGARS++, for distributed machine learning in the presence of any number of\nByzantine adversaries. In these algorithms, reputation scores of workers are\ncomputed using an auxiliary dataset at the server. This reputation score is\nthen used for aggregating the gradients for stochastic gradient descent. The\ncomputational complexity of ByGARS++ is the same as the usual distributed\nstochastic gradient descent method with only an additional inner product\ncomputation in every iteration. We show that using these reputation scores for\ngradient aggregation is robust to any number of multiplicative noise Byzantine\nadversaries and use two-timescale stochastic approximation theory to prove\nconvergence for strongly convex loss functions. We demonstrate the\neffectiveness of the algorithms for non-convex learning problems using MNIST\nand CIFAR-10 datasets against almost all state-of-the-art Byzantine attacks. We\nalso show that the proposed algorithms are robust to multiple different types\nof attacks at the same time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 01:47:13 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 20:59:12 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Regatti", "Jayanth", ""], ["Chen", "Hao", ""], ["Gupta", "Abhishek", ""]]}, {"id": "2006.13432", "submitter": "Mauro da Silva", "authors": "Mauro R. C. da Silva and Rafael C. S. Schouery", "title": "Local-Search Based Heuristics for Advertisement Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the MAXSPACE problem, given a set of ads A, one wants to place a subset A'\nof A into K slots B_1, ..., B_K of size L. Each ad A_i in A has a size s_i and\na frequency w_i. A schedule is feasible if the total size of ads in any slot is\nat most L, and each ad A_i in A' appears in exactly w_i slots. The goal is to\nfind a feasible schedule that maximizes the sum of the space occupied by all\nslots. We introduce MAXSPACE-RDWV, which is a MAXSPACE generalization that has\nrelease dates, deadlines, variable frequency, and generalized profit. In\nMAXSPACE-RDWV each ad A_i has a release date r_i >= 1, a deadline d_i >= r_i, a\nprofit v_i that may not be related with s_i*w_i and lower and upper bounds\nw^min_i and w^max_i for frequency. In this problem, an ad may only appear in a\nslot B_j with r_i <= j <= d_i. In this paper, we present some algorithms based\non meta-heuristics Greedy Randomized Adaptive Search Procedure (GRASP),\nVariable Neighborhood Search (VNS), Local Search and Tabu Search for MAXSPACE\nand MAXSPACE-RDWV. We compare our proposed algorithms with Hybrid-GA proposed\nby Kumar et al. (2006). We also create a version of Hybrid-GA for MAXSPACE-RDWV\nand compare it with our meta-heuristics for this problem. Some meta-heuristics,\nlike GRASP, have had better results than Hybrid-GA for both problems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 02:35:52 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:20:15 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["da Silva", "Mauro R. C.", ""], ["Schouery", "Rafael C. S.", ""]]}, {"id": "2006.13484", "submitter": "Shuai Zheng", "authors": "Shuai Zheng and Haibin Lin and Sheng Zha and Mu Li", "title": "Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes", "comments": "Technical Report (not under reviewed in any venue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BERT has recently attracted a lot of attention in natural language\nunderstanding (NLU) and achieved state-of-the-art results in various NLU tasks.\nHowever, its success requires large deep neural networks and huge amount of\ndata, which result in long training time and impede development progress. Using\nstochastic gradient methods with large mini-batch has been advocated as an\nefficient tool to reduce the training time. Along this line of research, LAMB\nis a prominent example that reduces the training time of BERT from 3 days to 76\nminutes on a TPUv3 Pod. In this paper, we propose an accelerated gradient\nmethod called LANS to improve the efficiency of using large mini-batches for\ntraining. As the learning rate is theoretically upper bounded by the inverse of\nthe Lipschitz constant of the function, one cannot always reduce the number of\noptimization iterations by selecting a larger learning rate. In order to use\nlarger mini-batch size without accuracy loss, we develop a new learning rate\nscheduler that overcomes the difficulty of using large learning rate. Using the\nproposed LANS method and the learning rate scheme, we scaled up the mini-batch\nsizes to 96K and 33K in phases 1 and 2 of BERT pretraining, respectively. It\ntakes 54 minutes on 192 AWS EC2 P3dn.24xlarge instances to achieve a target F1\nscore of 90.5 or higher on SQuAD v1.1, achieving the fastest BERT training time\nin the cloud.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:00:41 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 08:46:52 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zheng", "Shuai", ""], ["Lin", "Haibin", ""], ["Zha", "Sheng", ""], ["Li", "Mu", ""]]}, {"id": "2006.13486", "submitter": "Dharma Teja Vooturi", "authors": "Dharma Teja Vooturi, Girish Varma, Kishore Kothapalli", "title": "Ramanujan Bipartite Graph Products for Efficient Block Sparse Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse neural networks are shown to give accurate predictions competitive to\ndenser versions, while also minimizing the number of arithmetic operations\nperformed. However current hardware like GPU's can only exploit structured\nsparsity patterns for better efficiency. Hence the run time of a sparse neural\nnetwork may not correspond to the arithmetic operations required.\n  In this work, we propose RBGP( Ramanujan Bipartite Graph Product) framework\nfor generating structured multi level block sparse neural networks by using the\ntheory of Graph products. We also propose to use products of Ramanujan graphs\nwhich gives the best connectivity for a given level of sparsity. This\nessentially ensures that the i.) the networks has the structured block sparsity\nfor which runtime efficient algorithms exists ii.) the model gives high\nprediction accuracy, due to the better expressive power derived from the\nconnectivity of the graph iii.) the graph data structure has a succinct\nrepresentation that can be stored efficiently in memory. We use our framework\nto design a specific connectivity pattern called RBGP4 which makes efficient\nuse of the memory hierarchy available on GPU. We benchmark our approach by\nexperimenting on image classification task over CIFAR dataset using VGG19 and\nWideResnet-40-4 networks and achieve 5-9x and 2-5x runtime gains over\nunstructured and block sparsity patterns respectively, while achieving the same\nlevel of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:08:17 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 12:22:52 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Vooturi", "Dharma Teja", ""], ["Varma", "Girish", ""], ["Kothapalli", "Kishore", ""]]}, {"id": "2006.13591", "submitter": "Celestine Mendler-D\\\"unner", "authors": "Celestine Mendler-D\\\"unner, Aurelien Lucchi", "title": "Randomized Block-Diagonal Preconditioning for Parallel Learning", "comments": "improvement in Theorem 3 compared to ICML 2020 version", "journal-ref": "PMLR 119:6841-6851 (2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study preconditioned gradient-based optimization methods where the\npreconditioning matrix has block-diagonal form. Such a structural constraint\ncomes with the advantage that the update computation is block-separable and can\nbe parallelized across multiple independent tasks. Our main contribution is to\ndemonstrate that the convergence of these methods can significantly be improved\nby a randomization technique which corresponds to repartitioning coordinates\nacross tasks during the optimization procedure. We provide a theoretical\nanalysis that accurately characterizes the expected convergence gains of\nrepartitioning and validate our findings empirically on various traditional\nmachine learning tasks. From an implementation perspective, block-separable\nmodels are well suited for parallelization and, when shared memory is\navailable, randomization can be implemented on top of existing methods very\nefficiently to improve convergence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:12:36 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 09:33:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mendler-D\u00fcnner", "Celestine", ""], ["Lucchi", "Aurelien", ""]]}, {"id": "2006.13603", "submitter": "Andrea Valassi", "authors": "Federico Stagni, Andrea Valassi, Vladimir Romanovskiy", "title": "Integrating LHCb workflows on HPC resources: status and strategies", "comments": "9 pages, submitted to CHEP2019 proceedings in EPJ Web of Conferences", "journal-ref": "EPJ Web of Conferences 245, 09002 (2020)", "doi": "10.1051/epjconf/202024509002", "report-no": null, "categories": "cs.DC hep-ex physics.ins-det", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High Performance Computing (HPC) supercomputers are expected to play an\nincreasingly important role in HEP computing in the coming years. While HPC\nresources are not necessarily the optimal fit for HEP workflows, computing time\nat HPC centers on an opportunistic basis has already been available to the LHC\nexperiments for some time, and it is also possible that part of the pledged\ncomputing resources will be offered as CPU time allocations at HPC centers in\nthe future. The integration of the experiment workflows to make the most\nefficient use of HPC resources is therefore essential. This paper describes the\nwork that has been necessary to integrate LHCb workflows at a specific HPC\nsite, the Marconi-A2 system at CINECA in Italy, where LHCb benefited from a\njoint PRACE (Partnership for Advanced Computing in Europe) allocation with the\nother Large Hadron Collider (LHC) experiments. This has required addressing two\ntypes of challenges: on the software application workloads, for optimising\ntheir performance on a many-core hardware architecture that differs\nsignificantly from those traditionally used in WLCG (Worldwide LHC Computing\nGrid), by reducing memory footprint using a multi-process approach; and in the\ndistributed computing area, for submitting these workloads using more than one\nlogical processor per job, which had never been done yet in LHCb.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 10:26:44 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Stagni", "Federico", ""], ["Valassi", "Andrea", ""], ["Romanovskiy", "Vladimir", ""]]}, {"id": "2006.13829", "submitter": "Diederik Vink", "authors": "Diederik Adriaan Vink, Aditya Rajagopal, Stylianos I. Venieris,\n  Christos-Savvas Bouganis", "title": "Caffe Barista: Brewing Caffe with FPGAs in the Training Loop", "comments": "Published as short paper at FPL2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the complexity of deep learning (DL) models increases, their compute\nrequirements increase accordingly. Deploying a Convolutional Neural Network\n(CNN) involves two phases: training and inference. With the inference task\ntypically taking place on resource-constrained devices, a lot of research has\nexplored the field of low-power inference on custom hardware accelerators. On\nthe other hand, training is both more compute- and memory-intensive and is\nprimarily performed on power-hungry GPUs in large-scale data centres. CNN\ntraining on FPGAs is a nascent field of research. This is primarily due to the\nlack of tools to easily prototype and deploy various hardware and/or\nalgorithmic techniques for power-efficient CNN training. This work presents\nBarista, an automated toolflow that provides seamless integration of FPGAs into\nthe training of CNNs within the popular deep learning framework Caffe. To the\nbest of our knowledge, this is the only tool that allows for such versatile and\nrapid deployment of hardware and algorithms for the FPGA-based training of\nCNNs, providing the necessary infrastructure for further research and\ndevelopment.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:56:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Vink", "Diederik Adriaan", ""], ["Rajagopal", "Aditya", ""], ["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "2006.13878", "submitter": "K. R. Jayaram", "authors": "Vaibhav Saxena, K. R. Jayaram, Saurav Basu, Yogish Sabharwal and\n  Ashish Verma", "title": "Effective Elastic Scaling of Deep Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased use of deep learning (DL) in academia, government and industry\nhas, in turn, led to the popularity of on-premise and cloud-hosted deep\nlearning platforms, whose goals are to enable organizations utilize expensive\nresources effectively, and to share said resources among multiple teams in a\nfair and effective manner.\n  In this paper, we examine the elastic scaling of Deep Learning (DL) jobs over\nlarge-scale training platforms and propose a novel resource allocation strategy\nfor DL training jobs, resulting in improved job run time performance as well as\nincreased cluster utilization. We begin by analyzing DL workloads and exploit\nthe fact that DL jobs can be run with a range of batch sizes without affecting\ntheir final accuracy. We formulate an optimization problem that explores a\ndynamic batch size allocation to individual DL jobs based on their scaling\nefficiency, when running on multiple nodes. We design a fast dynamic\nprogramming based optimizer to solve this problem in real-time to determine\njobs that can be scaled up/down, and use this optimizer in an autoscaler to\ndynamically change the allocated resources and batch sizes of individual DL\njobs.\n  We demonstrate empirically that our elastic scaling algorithm can complete up\nto $\\approx 2 \\times$ as many jobs as compared to a strong baseline algorithm\nthat also scales the number of GPUs but does not change the batch size. We also\ndemonstrate that the average completion time with our algorithm is up to\n$\\approx 10 \\times$ faster than that of the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:01:09 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Saxena", "Vaibhav", ""], ["Jayaram", "K. R.", ""], ["Basu", "Saurav", ""], ["Sabharwal", "Yogish", ""], ["Verma", "Ashish", ""]]}, {"id": "2006.13893", "submitter": "Oluwasegun Adedugbe", "authors": "Oluwasegun Adedugbe, Elhadj Benkhelifa and Anoud Bani-Hani", "title": "A Cloud Computing Capability Model for Large-Scale Semantic Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic technologies are designed to facilitate context-awareness for web\ncontent, enabling machines to understand and process them. However, this has\nbeen faced with several challenges, such as disparate nature of existing\nsolutions and lack of scalability in proportion to web scale. With a holistic\nperspective to web content semantic annotation, this paper focuses on\nleveraging cloud computing for these challenges. To achieve this, a set of\nrequirements towards holistic semantic annotation on the web is defined and\nmapped with cloud computing mechanisms to facilitate them. Technical\nspecification for the requirements is critically reviewed and examined against\neach of the cloud computing mechanisms, in relation to their technical\nfunctionalities. Hence, a mapping is established if the cloud computing\nmechanism's functionalities proffer a solution for implementation of a\nrequirement's technical specification. The result is a cloud computing\ncapability model for holistic semantic annotation which presents an approach\ntowards delivering large scale semantic annotation on the web via a cloud\nplatform.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:23:06 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 14:02:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Adedugbe", "Oluwasegun", ""], ["Benkhelifa", "Elhadj", ""], ["Bani-Hani", "Anoud", ""]]}, {"id": "2006.14346", "submitter": "Alex Shamis", "authors": "Alex Shamis, Matthew Renzelmann, Stanko Novakovic, Georgios\n  Chatzopoulos, Anders T. Gjerdrum, Dan Alistarh, Aleksandar Dragojevic,\n  Dushyanth Narayanan, and Miguel Castro", "title": "Fast General Distributed Transactions with Opacity using Global Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactions can simplify distributed applications by hiding data\ndistribution, concurrency, and failures from the application developer. Ideally\nthe developer would see the abstraction of a single large machine that runs\ntransactions sequentially and never fails. This requires the transactional\nsubsystem to provide opacity (strict serializability for both committed and\naborted transactions), as well as transparent fault tolerance with high\navailability. As even the best abstractions are unlikely to be used if they\nperform poorly, the system must also provide high performance.\n  Existing distributed transactional designs either weaken this abstraction or\nare not designed for the best performance within a data center. This paper\nextends the design of FaRM - which provides strict serializability only for\ncommitted transactions - to provide opacity while maintaining FaRM's high\nthroughput, low latency, and high availability within a modern data center. It\nuses timestamp ordering based on real time with clocks synchronized to within\ntens of microseconds across a cluster, and a failover protocol to ensure\ncorrectness across clock master failures. FaRM with opacity can commit 5.4\nmillion neworder transactions per second when running the TPC-C transaction mix\non 90 machines with 3-way replication.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 12:42:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Shamis", "Alex", ""], ["Renzelmann", "Matthew", ""], ["Novakovic", "Stanko", ""], ["Chatzopoulos", "Georgios", ""], ["Gjerdrum", "Anders T.", ""], ["Alistarh", "Dan", ""], ["Dragojevic", "Aleksandar", ""], ["Narayanan", "Dushyanth", ""], ["Castro", "Miguel", ""]]}, {"id": "2006.14384", "submitter": "Hadrien Hendrikx", "authors": "Hadrien Hendrikx, Francis Bach, Laurent Massouli\\'e", "title": "Dual-Free Stochastic Decentralized Optimization with Variance Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training machine learning models on distributed\ndata in a decentralized way. For finite-sum problems, fast single-machine\nalgorithms for large datasets rely on stochastic updates combined with variance\nreduction. Yet, existing decentralized stochastic algorithms either do not\nobtain the full speedup allowed by stochastic updates, or require oracles that\nare more expensive than regular gradients. In this work, we introduce a\nDecentralized stochastic algorithm with Variance Reduction called DVR. DVR only\nrequires computing stochastic gradients of the local functions, and is\ncomputationally as fast as a standard stochastic variance-reduced algorithms\nrun on a $1/n$ fraction of the dataset, where $n$ is the number of nodes. To\nderive DVR, we use Bregman coordinate descent on a well-chosen dual problem,\nand obtain a dual-free algorithm using a specific Bregman divergence. We give\nan accelerated version of DVR based on the Catalyst framework, and illustrate\nits effectiveness with simulations on real data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:22:04 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hendrikx", "Hadrien", ""], ["Bach", "Francis", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "2006.14528", "submitter": "Ellis Solaiman", "authors": "B. Awaji, E. Solaiman, A. Albshri", "title": "Blockchain-Based Applications in Higher Education: A Systematic Mapping\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilisation of blockchain has moved beyond digital currency to other\nfields such as health, the Internet of Things, and education. In this paper, we\npresent a systematic mapping study to collect and analyse relevant research on\nblockchain technology related to the higher education field. The paper\nconcentrates on two main themes. First, it examines state of the art in\nblockchain-based applications that have been developed for educational\npurposes. Second, it summarises the challenges and research gaps that need to\nbe addressed in future studies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:24:28 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Awaji", "B.", ""], ["Solaiman", "E.", ""], ["Albshri", "A.", ""]]}, {"id": "2006.14652", "submitter": "Ojas Parekh", "authors": "Ojas Parekh, Cynthia A. Phillips, Conrad D. James, James B. Aimone", "title": "Constant-Depth and Subcubic-Size Threshold Circuits for Matrix\n  Multiplication", "comments": "Appears in the proceedings of the ACM Symposium on Parallelism in\n  Algorithms and Architectures (SPAA), 2018", "journal-ref": null, "doi": "10.1145/3210377.3210410", "report-no": null, "categories": "cs.DS cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean circuits of McCulloch-Pitts threshold gates are a classic model of\nneural computation studied heavily in the late 20th century as a model of\ngeneral computation. Recent advances in large-scale neural computing hardware\nhas made their practical implementation a near-term possibility. We describe a\ntheoretical approach for multiplying two $N$ by $N$ matrices that integrates\nthreshold gate logic with conventional fast matrix multiplication algorithms,\nthat perform $O(N^\\omega)$ arithmetic operations for a positive constant\n$\\omega < 3$. Our approach converts such a fast matrix multiplication algorithm\ninto a constant-depth threshold circuit with approximately $O(N^\\omega)$ gates.\nPrior to our work, it was not known whether the $\\Theta(N^3)$-gate barrier for\nmatrix multiplication was surmountable by constant-depth threshold circuits.\n  Dense matrix multiplication is a core operation in convolutional neural\nnetwork training. Performing this work on a neural architecture instead of\noff-loading it to a GPU may be an appealing option.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:28:10 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Parekh", "Ojas", ""], ["Phillips", "Cynthia A.", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "2006.14784", "submitter": "Peter Vaillancourt", "authors": "Peter Z. Vaillancourt, J. Eric Coulter, Richard Knepper, Brandon\n  Barker", "title": "Self-Scaling Clusters and Reproducible Containers to Enable Scientific\n  Computing", "comments": "Accepted for publication in the IEEE conference proceedings for HPEC\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Container technologies such as Docker have become a crucial component of many\nsoftware industry practices especially those pertaining to reproducibility and\nportability. The containerization philosophy has influenced the scientific\ncomputing community, which has begun to adopt - and even develop - container\ntechnologies (such as Singularity). Leveraging containers for scientific\nsoftware often poses challenges distinct from those encountered in industry,\nand requires different methodologies. This is especially true for HPC. With an\nincreasing number of options for HPC in the cloud (including NSF-funded cloud\nprojects), there is strong motivation to seek solutions that provide\nflexibility to develop and deploy scientific software on a variety of\ncomputational infrastructures in a portable and reproducible way. The\nflexibility offered by cloud services enables virtual HPC clusters that scale\non-demand, and the Cyberinfrastructure Resource Integration team in the XSEDE\nproject has developed a set of tools which provides scalable infrastructure in\nthe cloud. We now present a solution which uses the Nix package manager in an\nMPI-capable Docker container that is converted to Singularity. It provides\nconsistent installations, dependencies, and environments in each image that are\nreproducible and portable across scientific computing infrastructures. We\ndemonstrate the utility of these containers with cluster benchmark runs in a\nself-scaling virtual cluster using the Slurm scheduler deployed in the\nJetstream and Aristotle Red Cloud OpenStack clouds. We conclude this technique\nis useful as a template for scientific software application containers to be\nused in the XSEDE compute environment, other Singularity HPC environments, and\ncloud computing environments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:57:19 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 23:40:15 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Vaillancourt", "Peter Z.", ""], ["Coulter", "J. Eric", ""], ["Knepper", "Richard", ""], ["Barker", "Brandon", ""]]}, {"id": "2006.14793", "submitter": "Francisco Romero", "authors": "Francisco Romero, Benjamin Braun, David Cheriton", "title": "The TRaCaR Ratio: Selecting the Right Storage Technology for Active\n  Dataset-Serving Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Main memory database systems aim to provide users with low latency and high\nthroughput access to data. Most data resides in secondary storage, which is\nlimited by the access speed of the technology. For hot content, data resides in\nDRAM, which has become increasingly expensive as datasets grow in size and\naccess demand. With the emergence of low-latency storage solutions such as\nFlash and Intel's 3D XPoint (3DXP), there is an opportunity for these systems\nto give users high Quality-of-Service while reducing the cost for providers. To\nachieve high performance, providers must provision the server hosts for these\ndatasets with the proper amount of DRAM and secondary storage, as well as\nselecting a storage technology. The growth of capacity and transaction load\novertime makes it expensive to flip back-and-forth between different storage\ntechnologies and memory-storage combinations. Servers set up for one storage\ntechnology must now be reconfigured, repartitioned, and potentially replaced\naltogether. As more low-latency storage solutions become available, how does\none decide on the right memory-storage combination, as well as selecting a\nstorage technology, given a predicted trend in dataset growth and offered load?\nIn this paper, we describe and make the case for using the TRaCaR ratio - the\ntransaction rate divided by the storage capacity needed for a workload - for\nallowing providers to choose the most cost-effective memory-storage combination\nand storage technology given their predicted dataset trend and load\nrequirement. We explore how the TRaCaR ratio can be used with 3DXP and Flash\nwith a highly-zipfian b-tree database, and discuss potential research\ndirections that can leverage the ratio.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 04:30:30 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Romero", "Francisco", ""], ["Braun", "Benjamin", ""], ["Cheriton", "David", ""]]}, {"id": "2006.15213", "submitter": "Sumanas Sarma", "authors": "Serge Plata, Sumanas Sarma, Melvin Lancelot, Kristine Bagrova, David\n  Romano-Critchley", "title": "Simulating human interactions in supermarkets to measure the risk of\n  COVID-19 contagion at scale", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the context of simulating a retail environment using agent based\nmodelling, a theoretical model is presented that describes the probability\ndistribution of customer \"collisions\" using a novel space transformation to the\nTorus $Tor^2$. A method for generating the distribution of customer paths based\non historical basket data is developed. Finally a calculation of the number of\nsimulations required for statistical significance is developed. An\nimplementation of this modelling approach to run simulations on multiple store\ngeometries at industrial scale is being developed with current progress\ndetailed in the technical appendix.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 21:04:09 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Plata", "Serge", ""], ["Sarma", "Sumanas", ""], ["Lancelot", "Melvin", ""], ["Bagrova", "Kristine", ""], ["Romano-Critchley", "David", ""]]}, {"id": "2006.15234", "submitter": "Yuchen Pang", "authors": "Yuchen Pang, Tianyi Hao, Annika Dugad, Yiqing Zhou, Edgar Solomonik", "title": "Efficient 2D Tensor Network Simulation of Quantum Systems", "comments": "to be published in SC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of quantum systems is challenging due to the exponential size of\nthe state space. Tensor networks provide a systematically improvable\napproximation for quantum states. 2D tensor networks such as Projected\nEntangled Pair States (PEPS) are well-suited for key classes of physical\nsystems and quantum circuits. However, direct contraction of PEPS networks has\nexponential cost, while approximate algorithms require computations with large\ntensors. We propose new scalable algorithms and software abstractions for\nPEPS-based methods, accelerating the bottleneck operation of contraction and\nrefactorization of a tensor subnetwork. We employ randomized SVD with an\nimplicit matrix to reduce cost and memory footprint asymptotically. Further, we\ndevelop a distributed-memory PEPS library and study accuracy and efficiency of\nalternative algorithms for PEPS contraction and evolution on the Stampede2\nsupercomputer. We also simulate a popular near-term quantum algorithm, the\nVariational Quantum Eigensolver (VQE), and benchmark Imaginary Time Evolution\n(ITE), which compute ground states of Hamiltonians.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 22:36:56 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 15:57:34 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Pang", "Yuchen", ""], ["Hao", "Tianyi", ""], ["Dugad", "Annika", ""], ["Zhou", "Yiqing", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2006.15254", "submitter": "Aman Khalid", "authors": "Aman Khalid", "title": "Optimizing Cuckoo Filter for high burst tolerance,low latency, and high\n  throughput", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an implementation of a cuckoo filter for membership\ntesting, optimized for distributed data stores operating in high workloads. In\nlarge databases, querying becomes inefficient using traditional search methods.\nTo achieve optimal performance it is necessary to use probabilistic data\nstructures to test the membership of a given key, at the cost of getting false\npositives while querying data. The widely used bloom filters can be used for\nthis, but they have limitations like no support for deletes. To improve upon\nthis we use a modified version of the cuckoo filter that gives better amortized\ntimes for search, with less false positives.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 01:58:36 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Khalid", "Aman", ""]]}, {"id": "2006.15259", "submitter": "Ramtin Afshar", "authors": "Ramtin Afshar, Michael T. Goodrich, Pedro Matias, Martha C. Osegueda", "title": "Reconstructing Biological and Digital Phylogenetic Trees in Parallel", "comments": null, "journal-ref": "Leibniz International Proceedings in Informatics (LIPICS) 173\n  (2020) 3:1-3:24", "doi": "10.4230/LIPIcs.ESA.2020.3", "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the parallel query complexity of reconstructing\nbiological and digital phylogenetic trees from simple queries involving their\nnodes. This is motivated from computational biology, data protection, and\ncomputer security settings, which can be abstracted in terms of two parties, a\nresponder, Alice, who must correctly answer queries of a given type regarding a\ndegree-d tree, T, and a querier, Bob, who issues batches of queries, with each\nquery in a batch being independent of the others, so as to eventually infer the\nstructure of T. We show that a querier can efficiently reconstruct an n-node\ndegree-d tree, T, with a logarithmic number of rounds and quasilinear number of\nqueries, with high probability, for various types of queries, including\nrelative-distance queries and path queries. Our results are all asymptotically\noptimal and improve the asymptotic (sequential) query complexity for one of the\nproblems we study. Moreover, through an experimental analysis using both\nreal-world and synthetic data, we provide empirical evidence that our\nalgorithms provide significant parallel speedups while also improving the total\nquery complexities for the problems we study.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 02:28:44 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 20:40:23 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 17:58:57 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Afshar", "Ramtin", ""], ["Goodrich", "Michael T.", ""], ["Matias", "Pedro", ""], ["Osegueda", "Martha C.", ""]]}, {"id": "2006.15314", "submitter": "Ellis Solaiman", "authors": "A. Alzubaidi, K. Mitra, P. Patel, E. Solaiman", "title": "A Blockchain-based Approach for Assessing Compliance with SLA-guaranteed\n  IoT Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within cloud-based internet of things (IoT) applications, typically cloud\nproviders employ Service Level Agreements (SLAs) to ensure the quality of their\nprovisioned services. Similar to any other contractual method, an SLA is not\nimmune to breaches. Ideally, an SLA stipulates consequences (e.g. penalties)\nimposed on cloud providers when they fail to conform to SLA terms. The current\npractice assumes trust in service providers to acknowledge SLA breach incidents\nand executing associated consequences. Recently, the Blockchain paradigm has\nintroduced compelling capabilities that may enable us to address SLA\nenforcement more elegantly. This paper proposes and implements a\nblockchain-based approach for assessing SLA compliance and enforcing\nconsequences. It employs a diagnostic accuracy method for validating the\ndependability of the proposed solution. The paper also benchmarks Hyperledger\nFabric to investigate its feasibility as an underlying blockchain\ninfrastructure concerning latency and transaction success/fail rates.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 08:08:45 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Alzubaidi", "A.", ""], ["Mitra", "K.", ""], ["Patel", "P.", ""], ["Solaiman", "E.", ""]]}, {"id": "2006.15367", "submitter": "Michael Lingg", "authors": "Michael P. Lingg, Stephen M. Hughey, Hasan Metin Aktulga,\n  Balasubramaniam Shanker", "title": "High Performance Evaluation of Helmholtz Potentials using the\n  Multi-Level Fast Multipole Algorithm", "comments": "Submitted to ACM Transactions on Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of pair potentials is critical in a number of areas of physics.\nThe classicalN-body problem has its root in evaluating the Laplace potential,\nand has spawned tree-algorithms, the fast multipole method (FMM), as well as\nkernel independent approaches. Over the years, FMM for Laplace potential has\nhad a profound impact on a number of disciplines as it has been possible to\ndevelop highly scalable parallel algorithm for these potential evaluators. This\nis in stark contrast to parallel algorithms for the Helmholtz (oscillatory)\npotentials. The principal bottleneck to scalable parallelism are operations\nnecessary to traverse up, across and down the tree, affecting both computation\nand communication. In this paper, we describe techniques to overcome\nbottlenecks and achieve high performance evaluation of the Helmholtz potential\nfor a wide spectrum of geometries. We demonstrate that the resulting\nimplementation has a load balancing effect that significantly reduces the\ntime-to-solution and enhances the scale of problems that can be treated using\nfull wave physics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 13:50:28 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 11:46:33 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lingg", "Michael P.", ""], ["Hughey", "Stephen M.", ""], ["Aktulga", "Hasan Metin", ""], ["Shanker", "Balasubramaniam", ""]]}, {"id": "2006.15481", "submitter": "Vanderson Martins do Rosario", "authors": "Vanderson Martins Do Rosario, Thais A. Silva Camacho, Ot\\'avio O.\n  Napoli and Edson Borin", "title": "Fast and Low-cost Search for Efficient Cloud Configurations for HPC\n  Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of cloud computational resources has become increasingly important\nfor companies and researchers to access on-demand and at any moment\nhigh-performance resources. However, given the wide variety of virtual machine\ntypes, network configurations, number of instances, among others, finding the\nbest configuration that reduces costs and resource waste while achieving\nacceptable performance is a hard task even for specialists. Thus, many\napproaches to find these good or optimal configurations for a given program\nhave been proposed. Observing the performance of an application in some\nconfiguration takes time and money. Therefore, most of the approaches aim not\nonly to find good solutions but also to reduce the search cost. One approach is\nthe use of Bayesian Optimization to observe the least amount possible of\nconfigurations, reducing the search cost while still finding good solutions.\nAnother approach is the use of a technique named Paramount Iteration to make\nperformance assumptions of HPC workloads without entirely executing them\n(early-stopping), reducing the cost of making one observation, and making it\nfeasible to grid search solutions. In this work, we show that both techniques\ncan be used together to do fewer and low-cost observations. We show that such\nan approach can recommend Pareto-optimal solutions that are on average 1.68x\nbetter than Random Searching and with a 6-time cheaper search.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 01:17:03 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Rosario", "Vanderson Martins Do", ""], ["Camacho", "Thais A. Silva", ""], ["Napoli", "Ot\u00e1vio O.", ""], ["Borin", "Edson", ""]]}, {"id": "2006.15594", "submitter": "Karim Sonbol", "authors": "Karim Sonbol, \\\"Oznur \\\"Ozkasap, Ibrahim Al-Oqily, Moayad Aloqaily", "title": "EdgeKV: Decentralized, scalable, and consistent storage for the edge", "comments": "13 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge computing moves the computation closer to the data and the data closer\nto the user to overcome the high latency communication of cloud computing.\nStorage at the edge allows data access with high speeds that enable\nlatency-sensitive applications in areas such as autonomous driving and smart\ngrid. However, several distributed services are typically designed for the\ncloud and building an efficient edge-enabled storage system is challenging\nbecause of the distributed and heterogeneous nature of the edge and its limited\nresources. In this paper, we propose EdgeKV, a decentralized storage system\ndesigned for the network edge. EdgeKV offers fast and reliable storage,\nutilizing data replication with strong consistency guarantees. With a\nlocation-transparent and interface-based design, EdgeKV can scale with a\nheterogeneous system of edge nodes. We implement a prototype of the EdgeKV\nmodules in Golang and evaluate it in both the edge and cloud settings on the\nGrid'5000 testbed. We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to\nanalyze the system's performance under realistic workloads. Our evaluation\nresults show that EdgeKV outperforms the cloud storage setting with both local\nand global data access with an average write response time and throughput\nimprovements of 26% and 19% respectively under the same settings. Our\nevaluations also show that EdgeKV can scale with the number of clients, without\nsacrificing performance. Finally, we discuss the energy efficiency improvement\nwhen utilizing edge resources with EdgeKV instead of a centralized cloud.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 13:08:34 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Sonbol", "Karim", ""], ["\u00d6zkasap", "\u00d6znur", ""], ["Al-Oqily", "Ibrahim", ""], ["Aloqaily", "Moayad", ""]]}, {"id": "2006.15698", "submitter": "Andrew Kirby", "authors": "Andrew C. Kirby, Dimitri J. Mavriplis", "title": "GPU-Accelerated Discontinuous Galerkin Methods: 30x Speedup on 345\n  Billion Unknowns", "comments": "7 pages, 4 figures, 40 references. Accepted to 2020 IEEE HPEC", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA cs.PF math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discontinuous Galerkin method for the discretization of the compressible\nEuler equations, the governing equations of inviscid fluid dynamics, on\nCartesian meshes is developed for use of Graphical Processing Units via OCCA, a\nunified approach to performance portability on multi-threaded hardware\narchitectures. A 30x time-to-solution speedup over CPU-only implementations\nusing non-CUDA-Aware MPI communications is demonstrated up to 1,536 NVIDIA V100\nGPUs and parallel strong scalability is shown up to 6,144 NVIDIA V100 GPUs for\na problem containing 345 billion unknowns. A comparison of CUDA-Aware MPI\ncommunication to non-GPUDirect communication is performed demonstrating an\nadditional 24% speedup on eight nodes composed of 32 NVIDIA V100 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:14:38 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:13:49 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 00:27:29 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kirby", "Andrew C.", ""], ["Mavriplis", "Dimitri J.", ""]]}, {"id": "2006.15703", "submitter": "Anton Bernshteyn", "authors": "Anton Bernshteyn", "title": "A Fast Distributed Algorithm for $(\\Delta + 1)$-Edge-Coloring", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deterministic distributed algorithm in the LOCAL model that\nfinds a proper $(\\Delta + 1)$-edge-coloring of an $n$-vertex graph of maximum\ndegree $\\Delta$ in $\\mathrm{poly}(\\Delta, \\log n)$ rounds. This is the first\nnontrivial distributed edge-coloring algorithm that uses only $\\Delta+1$ colors\n(matching the bound given by Vizing's theorem). Our approach is inspired by the\nrecent proof of the measurable version of Vizing's theorem due to Greb\\'ik and\nPikhurko.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:35:54 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 22:08:34 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 20:18:19 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Bernshteyn", "Anton", ""]]}, {"id": "2006.15704", "submitter": "Shen Li", "authors": "Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis,\n  Teng Li, Adam Paszke, Jeff Smith, Brian Vaughan, Pritam Damania, Soumith\n  Chintala", "title": "PyTorch Distributed: Experiences on Accelerating Data Parallel Training", "comments": "To appear in VLDB 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the design, implementation, and evaluation of the PyTorch\ndistributed data parallel module. PyTorch is a widely-adopted scientific\ncomputing package used in deep learning research and applications. Recent\nadvances in deep learning argue for the value of large datasets and large\nmodels, which necessitates the ability to scale out model training to more\ncomputational resources. Data parallelism has emerged as a popular solution for\ndistributed training thanks to its straightforward principle and broad\napplicability. In general, the technique of distributed data parallelism\nreplicates the model on every computational resource to generate gradients\nindependently and then communicates those gradients at each iteration to keep\nmodel replicas consistent. Despite the conceptual simplicity of the technique,\nthe subtle dependencies between computation and communication make it\nnon-trivial to optimize the distributed training efficiency. As of v1.5,\nPyTorch natively provides several techniques to accelerate distributed data\nparallel, including bucketing gradients, overlapping computation with\ncommunication, and skipping gradient synchronization. Evaluations show that,\nwhen configured appropriately, the PyTorch distributed data parallel module\nattains near-linear scalability using 256 GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 20:39:45 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Li", "Shen", ""], ["Zhao", "Yanli", ""], ["Varma", "Rohan", ""], ["Salpekar", "Omkar", ""], ["Noordhuis", "Pieter", ""], ["Li", "Teng", ""], ["Paszke", "Adam", ""], ["Smith", "Jeff", ""], ["Vaughan", "Brian", ""], ["Damania", "Pritam", ""], ["Chintala", "Soumith", ""]]}, {"id": "2006.15824", "submitter": "Tianbo Gu", "authors": "Jinyue Song, Tianbo Gu, Yunjie Ge, Prasant Mohapatra", "title": "Smart Contract-based Computing ResourcesTrading in Edge Computing", "comments": "8 pages, 9 figures, to appear in the 2020 Annual IEEE International\n  Symposium on Personal, Indoor and Mobile Radio Communications (IEEE PIMRC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is an emerging trend that some computing services are\nmoving from cloud to the edge of the networks. Compared to cloud computing,\nedge computing can provide services with faster response, lower expense, and\nmore security. The massive idle computing resources closing to the edge also\nenhance the deployment of edge services. Instead of using cloud services from\nsome primary providers, edge computing provides people with a great chance to\nactively join the market of computing resources. However, edge computing also\nhas some critical impediments that we have to overcome.\n  In this paper, we design an edge computing service platform that can receive\nand distribute the computing resources from the end-users in a decentralized\nway. Without centralized trade control, we propose a novel hierarchical smart\ncontract-based decentralized technique to establish the trading trust among\nusers and provide flexible smart contract interfaces to satisfy users. Our\nsystem also considers and resolves a variety of security and privacy challenges\nwhen utilizing the encryption and distributed access control mechanism. We\nimplement our system and conduct extensive experiments to show the feasibility\nand effectiveness of our proposed system.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 06:09:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Song", "Jinyue", ""], ["Gu", "Tianbo", ""], ["Ge", "Yunjie", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "2006.15918", "submitter": "Christopher Goes", "authors": "Christopher Goes", "title": "The Interblockchain Communication Protocol: An Overview", "comments": "16 pages, 15 code listings in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interblockchain communication protocol (IBC) is an end-to-end,\nconnection-oriented, stateful protocol for reliable, ordered, and authenticated\ncommunication between modules on separate distributed ledgers. IBC is designed\nfor interoperation between heterogenous ledgers arranged in an unknown, dynamic\ntopology, operating with varied consensus algorithms and state machines. The\nprotocol realises this by specifying the sufficient set of data structures,\nabstractions, and semantics of a communication protocol which once implemented\nby participating ledgers will allow them to safely communicate. IBC is\npayload-agnostic and provides a cross-ledger asynchronous communication\nprimitive which can be used as a constituent building block by a wide variety\nof applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 10:17:31 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Goes", "Christopher", ""]]}, {"id": "2006.15980", "submitter": "Yuanhang Yu", "authors": "Yuanhang Yu, Dong Wen, Ying Zhang, Xiaoyang Wang, Wenjie Zhang and\n  Xuemin Lin", "title": "Efficient Matrix Factorization on Heterogeneous CPU-GPU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix Factorization (MF) has been widely applied in machine learning and\ndata mining. A large number of algorithms have been studied to factorize\nmatrices. Among them, stochastic gradient descent (SGD) is a commonly used\nmethod. Heterogeneous systems with multi-core CPUs and GPUs have become more\nand more promising recently due to the prevalence of GPUs in general-purpose\ndata-parallel applications. Due to the large computational cost of MF, we aim\nto improve the efficiency of SGD-based MF computation by utilizing the massive\nparallel processing power of heterogeneous multiprocessors. The main challenge\nin parallel SGD algorithms on heterogeneous CPU-GPU systems lies in the\ngranularity of the matrix division and the strategy to assign tasks. We design\na novel strategy to divide the matrix into a set of blocks by considering two\naspects. First, we observe that the matrix should be divided nonuniformly, and\nrelatively large blocks should be assigned to GPUs to saturate the computing\npower of GPUs. In addition to exploiting the characteristics of hardware, the\nworkloads assigned to two types of hardware should be balanced. Aiming at the\nfinal division strategy, we design a cost model tailored for our problem to\naccurately estimate the performance of hardware on different data sizes. A\ndynamic scheduling policy is also used to further balance workloads in\npractice. Extensive experiments show that our proposed algorithm achieves high\nefficiency with a high quality of training quality.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:33:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Yu", "Yuanhang", ""], ["Wen", "Dong", ""], ["Zhang", "Ying", ""], ["Wang", "Xiaoyang", ""], ["Zhang", "Wenjie", ""], ["Lin", "Xuemin", ""]]}, {"id": "2006.16191", "submitter": "Diego Ram\\'irez-Romero", "authors": "Pedro Montealegre, Diego Ram\\'irez-Romero, Ivan Rapaport", "title": "Shared vs Private Randomness in Distributed Interactive Proofs", "comments": null, "journal-ref": "31st International Symposium on Algorithms and Computation (ISAAC\n  2020)", "doi": "10.4230/LIPIcs.ISAAC.2020.51", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed interactive proofs, the nodes of a graph G interact with a\npowerful but untrustable prover who tries to convince them, in a small number\nof rounds and through short messages, that G satisfies some property. This\nseries of interactions is followed by a phase of distributed verification,\nwhich may be either deterministic or randomized, where nodes exchange messages\nwith their neighbors.\n  The nature of this last verification round defines the two types of\ninteractive protocols. We say that the protocol is of Arthur-Merlin type if the\nverification round is deterministic. We say that the protocol is of\nMerlin-Arthur type if, in the verification round, the nodes are allowed to use\na fresh set of random bits.\n  In the original model introduced by Kol, Oshman, and Saxena [PODC 2018], the\nrandomness was private in the sense that each node had only access to an\nindividual source of random coins. Crescenzi, Fraigniaud, and Paz [DISC 2019]\ninitiated the study of the impact of shared randomness (the situation where the\ncoin tosses are visible to all nodes) in the distributed interactive model.\n  In this work, we continue that research line by showing that the impact of\nthe two forms of randomness is very different depending on whether we are\nconsidering Arthur-Merlin protocols or Merlin-Arthur protocols. While private\nrandomness gives more power to the first type of protocols, shared randomness\nprovides more power to the second. Our results also connect shared randomness\nin distributed interactive proofs with distributed verification, and new lower\nbounds are obtained.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 16:58:07 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Montealegre", "Pedro", ""], ["Ram\u00edrez-Romero", "Diego", ""], ["Rapaport", "Ivan", ""]]}, {"id": "2006.16284", "submitter": "Thorsten Sch\\\"utt", "authors": "Thorsten Sch\\\"utt and Florian Schintke and Jan Skrzypczak", "title": "Transactions on Red-black and AVL trees in NVRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Byte-addressable non-volatile memory (NVRAM) supports persistent storage with\nlow latency and high bandwidth. Complex data structures in it ought to be\nupdated transactionally, so that they remain recoverable at all times.\nTraditional database technologies such as keeping a separate log, a journal, or\nshadow data work on a coarse-grained level, where the whole transaction is made\nvisible using a final atomic update operation. These methods typically need\nsignificant additional space overhead and induce non-trivial overhead for log\npruning, state maintenance, and resource (de-)allocation. Thus, they are not\nnecessarily the best choice for NVRAM, which supports fine-grained,\nbyte-addressable access.\n  We present a generic transaction mechanism to update dynamic complex data\nstructures `in-place' with a constant memory overhead. It is independent of the\nsize of the data structure. We demonstrate and evaluate our approach on\nRed-Black Trees and AVL Trees with a redo log of constant size (4 resp. 2 cache\nlines). The redo log guarantees that each accepted (started) transaction is\nexecuted eventually despite arbitrary many system crashes and recoveries in the\nmeantime. We update complex data structures in local and remote NVRAM providing\nexactly once semantics and durable linearizability for multi-reader\nsingle-writer access. To persist data, we use the available processor\ninstructions for NVRAM in the local case and remote direct memory access (RDMA)\ncombined with a software agent in the remote case.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 18:05:37 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Sch\u00fctt", "Thorsten", ""], ["Schintke", "Florian", ""], ["Skrzypczak", "Jan", ""]]}, {"id": "2006.16339", "submitter": "Yongli Zhu", "authors": "Yongli Zhu, Renchang Dai, Guangyi Liu", "title": "Parallel Betweenness Computation in Graph Database for Contingency\n  Selection", "comments": "This paper has been accepted by the 2020 IEEE PES General Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel betweenness computation algorithms are proposed and implemented in a\ngraph database for power system contingency selection. Principles of the graph\ndatabase and graph computing are investigated for both node and edge\nbetweenness computation. Experiments on the 118-bus system and a real power\nsystem show that speed-up can be achieved for both node and edge betweenness\ncomputation while the speeding effect on the latter is more remarkable due to\nthe data retrieving advantages of the graph database on the power network data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 19:45:51 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhu", "Yongli", ""], ["Dai", "Renchang", ""], ["Liu", "Guangyi", ""]]}, {"id": "2006.16393", "submitter": "Ali Hadian", "authors": "Ali Hadian, Behzad Ghaffari, Taiyi Wang, Thomas Heinis", "title": "COAX: Correlation-Aware Indexing on Multidimensional Data with Soft\n  Functional Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work proposed learned index structures, which learn the distribution\nof the underlying dataset to improve performance. The initial work on learned\nindexes has shown that by learning the cumulative distribution function of the\ndata, index structures such as the B-Tree can improve their performance by one\norder of magnitude while having a smaller memory footprint.\n  In this paper, we present COAX, a learned index for multidimensional data\nthat, instead of learning the distribution of keys, learns the correlations\nbetween attributes of the dataset. Our approach is driven by the observation\nthat in many datasets, values of two (or multiple) attributes are correlated.\nCOAX exploits these correlations to reduce the dimensionality of the datasets.\n  More precisely, we learn how to infer one (or multiple) attribute $C_d$ from\nthe remaining attributes and hence no longer need to index attribute $C_d$.\nThis reduces the dimensionality and hence makes the index smaller and more\nefficient.\n  We theoretically investigate the effectiveness of the proposed technique\nbased on the predictability of the FD attributes. We further show\nexperimentally that by predicting correlated attributes in the data, we can\nimprove the query execution time and reduce the memory overhead of the index.\nIn our experiments, we reduce the execution time by 25% while reducing the\nmemory footprint of the index by four orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:22:15 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 20:47:00 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 15:43:57 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Hadian", "Ali", ""], ["Ghaffari", "Behzad", ""], ["Wang", "Taiyi", ""], ["Heinis", "Thomas", ""]]}, {"id": "2006.16423", "submitter": "Jakub Tarnawski", "authors": "Jakub Tarnawski, Amar Phanishayee, Nikhil R. Devanur, Divya Mahajan,\n  Fanny Nina Paravecino", "title": "Efficient Algorithms for Device Placement of DNN Graph Operators", "comments": "Accepted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning workloads use large models, with complex structures,\nthat are very expensive to execute. The devices that execute complex models are\nbecoming increasingly heterogeneous as we see a flourishing of domain-specific\naccelerators being offered as hardware accelerators in addition to CPUs. These\ntrends necessitate distributing the workload across multiple devices. Recent\nwork has shown that significant gains can be obtained with model parallelism,\ni.e, partitioning a neural network's computational graph onto multiple devices.\nIn particular, this form of parallelism assumes a pipeline of devices, which is\nfed a stream of samples and yields high throughput for training and inference\nof DNNs. However, for such settings (large models and multiple heterogeneous\ndevices), we require automated algorithms and toolchains that can partition the\nML workload across devices. In this paper, we identify and isolate the\nstructured optimization problem at the core of device placement of DNN\noperators, for both inference and training, especially in modern pipelined\nsettings. We then provide algorithms that solve this problem to optimality. We\ndemonstrate the applicability and efficiency of our approaches using several\ncontemporary DNN computation graphs.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 22:45:01 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 19:07:35 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tarnawski", "Jakub", ""], ["Phanishayee", "Amar", ""], ["Devanur", "Nikhil R.", ""], ["Mahajan", "Divya", ""], ["Paravecino", "Fanny Nina", ""]]}, {"id": "2006.16441", "submitter": "Mohsin Ur Rahman", "authors": "Mohsin Ur Rahman", "title": "Investigating the Effects of Mobility Metrics in Mobile Ad Hoc Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Ad Hoc Networks (MANETs) are formed by a collection of mobile nodes\n(MNs) that are capable of moving from one location to another location. These\nnetworks are widely identified by their unique characteristics such as lack of\ninfrastructure, mobility and multi-hop communication. Unlike traditional\n(wired) networks, MNs in MANETs do not rely on any infrastructure or central\nmanagement. Mobility allows MNs to move at different values of speed. Multi-hop\ncommunication is used to deliver data across the entire network. Due to\nmobility and changing network topology, the performance of these networks is\nsignificantly affected by the choice of mobility models and routing protocols.\nOur research work aims to evaluate the effects of mobility metrics on\ndistinguishing between entity and group mobility models in MANETs. In addition,\nwe demonstrate the interactions between mobility metrics and performance\nmetrics. We also investigate how effective are the mobility metrics and which\nmetrics can clearly distinguish between entity and group mobility models?\n  We performed extensive simulations using network simulator, ns-2.35, to\ncapture mobility metrics as well as different performance metrics. Simulation\nresults reveal the efficiency of mobility metrics on distinguishing between\nentity and group mobility models. We also obtain useful interactions between\nmobility metrics and performance metrics. The results presented in this paper\nprovide new insights into the variability of mobility metrics in MANETs.\nFurthermore, our simulation results reveal better understanding of the\nrelationships between mobility metrics (e.g., relative speed, node degree,\nnetwork partitions etc.) and performance metrics (e.g., packet delivery ratio,\nend-to-end delay and normalized routing load\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 00:25:18 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Rahman", "Mohsin Ur", ""]]}, {"id": "2006.16529", "submitter": "Jia Zou", "authors": "Jia Zou, Amitabh Das, Pratik Barhate, Arun Iyengar, Binhang Yuan,\n  Dimitrije Jankov, Chris Jermaine", "title": "Lachesis: Automatic Partitioning for UDF-Centric Analytics", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent partitioning is effective in avoiding expensive shuffling\noperations. However it remains a significant challenge to automate this process\nfor Big Data analytics workloads that extensively use user defined functions\n(UDFs), where sub-computations are hard to be reused for partitionings compared\nto relational applications. In addition, functional dependency that is widely\nutilized for partitioning selection is often unavailable in the unstructured\ndata that is ubiquitous in UDF-centric analytics. We propose the Lachesis\nsystem, which represents UDF-centric workloads as workflows of analyzable and\nreusable sub-computations. Lachesis further adopts a deep reinforcement\nlearning model to infer which sub-computations should be used to partition the\nunderlying data. This analysis is then applied to automatically optimize the\nstorage of the data across applications to improve the performance and users'\nproductivity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 04:49:44 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 02:15:27 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 00:25:31 GMT"}, {"version": "v4", "created": "Sat, 10 Oct 2020 08:43:46 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 08:21:08 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zou", "Jia", ""], ["Das", "Amitabh", ""], ["Barhate", "Pratik", ""], ["Iyengar", "Arun", ""], ["Yuan", "Binhang", ""], ["Jankov", "Dimitrije", ""], ["Jermaine", "Chris", ""]]}, {"id": "2006.16578", "submitter": "Ang Li", "authors": "Ang Li and Simon Su", "title": "Accelerating Binarized Neural Networks via Bit-Tensor-Cores in Turing\n  GPUs", "comments": "This work has already been accepted by IEEE Transactions on Parallel\n  and Distributed Systems (TPDS) Special Section on Parallel and Distributed\n  Computing Techniques for AI/ML/DL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite foreseeing tremendous speedups over conventional deep neural\nnetworks, the performance advantage of binarized neural networks (BNNs) has\nmerely been showcased on general-purpose processors such as CPUs and GPUs. In\nfact, due to being unable to leverage bit-level-parallelism with a word-based\narchitecture, GPUs have been criticized for extremely low utilization (1%) when\nexecuting BNNs. Consequently, the latest tensorcores in NVIDIA Turing GPUs\nstart to experimentally support bit computation. In this work, we look into\nthis brand new bit computation capability and characterize its unique features.\nWe show that the stride of memory access can significantly affect performance\ndelivery and a data-format co-design is highly desired to support the\ntensorcores for achieving superior performance than existing software solutions\nwithout tensorcores. We realize the tensorcore-accelerated BNN design,\nparticularly the major functions for fully-connect and convolution layers --\nbit matrix multiplication and bit convolution. Evaluations on two NVIDIA Turing\nGPUs show that, with ResNet-18, our BTC-BNN design can process ImageNet at a\nrate of 5.6K images per second, 77% faster than state-of-the-art. Our BNN\napproach is released on https://github.com/pnnl/TCBNN.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:32:02 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 00:13:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Li", "Ang", ""], ["Su", "Simon", ""]]}, {"id": "2006.16606", "submitter": "Marius Appel", "authors": "Marius Appel and Edzer Pebesma", "title": "Spatiotemporal Multi-Resolution Approximations for Analyzing Global\n  Environmental Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological developments and open data policies have made large, global\nenvironmental datasets accessible to everyone. For analysing such datasets,\nincluding spatiotemporal correlations using traditional models based on\nGaussian processes does not scale with data volume and requires strong\nassumptions about stationarity, separability, and distance measures of\ncovariance functions that are often unrealistic for global data. Only very few\nmodeling approaches suitably model spatiotemporal correlations while addressing\nboth computational scalability as well as flexible covariance models. In this\npaper, we provide an extension to the multi-resolution approximation (MRA)\napproach for spatiotemporal modeling of global datasets. MRA has been shown to\nbe computationally scalable in distributed computing environments and allows\nfor integrating arbitrary user-defined covariance functions. Our extension adds\na spatiotemporal partitioning, and fitting of complex covariance models\nincluding nonstationarity with kernel convolutions and spherical distances. We\nevaluate the effect of the MRA parameters on estimation and spatiotemporal\nprediction using simulated data, where computation times reduced around two\norders of magnitude with an increase of the root-mean-square prediction error\nof around five percent. This allows for trading off computation times against\nprediction errors, and we derive a practical strategy for selecting the MRA\nparameters. We demonstrate how the approach can be practically used for\nanalyzing daily sea surface temperature and precipitation data on global scale\nand compare models with different complexities in the covariance function.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:40:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Appel", "Marius", ""], ["Pebesma", "Edzer", ""]]}, {"id": "2006.16616", "submitter": "Marcos Maro\\~nas", "authors": "Marcos Maro\\~nas, Sergi Mateo, Kai Keller, Leonardo Bautista-Gomez,\n  Eduard Ayguad\\'e, Vicen\\c{c} Beltran", "title": "Extending the OpenCHK Model with Advanced Checkpoint Features", "comments": null, "journal-ref": "Future Generation Computer Systems, Volume 112, 2020, Pages\n  738-750", "doi": "10.1016/j.future.2020.06.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in using extreme scale systems efficiently is to\nmitigate the impact of faults. Application-level checkpoint/restart (CR)\nmethods provide the best trade-off between productivity, robustness, and\nperformance. There are many solutions implementing CR at the application level.\nThey all provide advanced I/O capabilities to minimize the overhead introduced\nby CR. Nevertheless, there is still room for improvement in terms of\nprogrammability and flexibility, because end-users must manually serialize and\ndeserialize application state using low-level APIs, modify the flow of the\napplication to consider restarts, or rewrite CR code whenever the backend\nlibrary changes. In this work, we propose a set of compiler directives and\nclauses that allow users to specify CR operations in a simple way. Our approach\nsupports the common CR features provided by all the CR libraries. However, it\ncan also be extended to support advanced features that are only available in\nsome CR libraries, such as differential checkpointing, the use of HDF5 format,\nand the possibility of using fault-tolerance-dedicated threads. The result of\nour evaluation revealed a high increase in programmability. On average, we\nreduced the number of lines of code by 71%, 94%, and 64% for FTI, SCR, and\nVeloC, respectively, and no additional overhead was perceived using our\nsolution compared to using the backend libraries directly. Finally, portability\nis enhanced because our programming model allows the use of any backend library\nwithout changing any code.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:14:45 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:18:06 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Maro\u00f1as", "Marcos", ""], ["Mateo", "Sergi", ""], ["Keller", "Kai", ""], ["Bautista-Gomez", "Leonardo", ""], ["Ayguad\u00e9", "Eduard", ""], ["Beltran", "Vicen\u00e7", ""]]}, {"id": "2006.16686", "submitter": "Gilad Stern", "authors": "Ittai Abraham, Danny Dolev, Gilad Stern", "title": "Revisiting Asynchronous Fault Tolerant Computation with Optimal\n  Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated result of Fischer, Lynch and Paterson is the fundamental lower\nbound for asynchronous fault tolerant computation: any 1-crash resilient\nasynchronous agreement protocol must have some (possibly measure zero)\nprobability of not terminating. In 1994, Ben-Or, Kelmer and Rabin published a\nproof-sketch of a lesser known lower bound for asynchronous fault tolerant\ncomputation with optimal resilience against a Byzantine adversary: if $n\\le 4t$\nthen any t-resilient asynchronous verifiable secret sharing protocol must have\nsome non-zero probability of not terminating.\n  Our main contribution is to revisit this lower bound and provide a rigorous\nand more general proof. Our second contribution is to show how to avoid this\nlower bound. We provide a protocol with optimal resilience that is almost\nsurely terminating for a strong common coin functionality. Using this new\nprimitive we provide an almost surely terminating protocol with optimal\nresilience for asynchronous Byzantine agreement that has a new fair validity\nproperty. To the best of our knowledge this is the first asynchronous Byzantine\nagreement with fair validity in the information theoretic setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 11:09:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 21:51:05 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Abraham", "Ittai", ""], ["Dolev", "Danny", ""], ["Stern", "Gilad", ""]]}, {"id": "2006.16744", "submitter": "Qiang Wu", "authors": "Hongwei Sun (University of Jinan) and Qiang Wu (Middle Tennessee State\n  University)", "title": "Optimal Rates of Distributed Regression with Imperfect Kernels", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning systems have been receiving increasing\nattentions for their efficiency to process large scale data. Many distributed\nframeworks have been proposed for different machine learning tasks. In this\npaper, we study the distributed kernel regression via the divide and conquer\napproach. This approach has been proved asymptotically minimax optimal if the\nkernel is perfectly selected so that the true regression function lies in the\nassociated reproducing kernel Hilbert space. However, this is usually, if not\nalways, impractical because kernels that can only be selected via prior\nknowledge or a tuning process are hardly perfect. Instead it is more common\nthat the kernel is good enough but imperfect in the sense that the true\nregression can be well approximated by but does not lie exactly in the kernel\nspace. We show distributed kernel regression can still achieves capacity\nindependent optimal rate in this case. To this end, we first establish a\ngeneral framework that allows to analyze distributed regression with response\nweighted base algorithms by bounding the error of such algorithms on a single\ndata set, provided that the error bounds has factored the impact of the\nunexplained variance of the response variable. Then we perform a leave one out\nanalysis of the kernel ridge regression and bias corrected kernel ridge\nregression, which in combination with the aforementioned framework allows us to\nderive sharp error bounds and capacity independent optimal rates for the\nassociated distributed kernel regression algorithms. As a byproduct of the\nthorough analysis, we also prove the kernel ridge regression can achieve rates\nfaster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting\nwhich, to our best knowledge, are first observed and novel in regression\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:00:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Sun", "Hongwei", "", "University of Jinan"], ["Wu", "Qiang", "", "Middle Tennessee State\n  University"]]}, {"id": "2006.16767", "submitter": "Chao Yang", "authors": "Min Li and Yulong Ao and Chao Yang", "title": "Adaptive SpMV/SpMSpV on GPUs for Input Vectors of Varied Sparsity", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite numerous efforts for optimizing the performance of Sparse Matrix and\nVector Multiplication (SpMV) on modern hardware architectures, few works are\ndone to its sparse counterpart, Sparse Matrix and Sparse Vector Multiplication\n(SpMSpV), not to mention dealing with input vectors of varied sparsity. The key\nchallenge is that depending on the sparsity levels, distribution of data, and\ncompute platform, the optimal choice of SpMV/SpMSpV kernel can vary, and a\nstatic choice does not suffice. In this paper, we propose an adaptive\nSpMV/SpMSpV framework, which can automatically select the appropriate\nSpMV/SpMSpV kernel on GPUs for any sparse matrix and vector at the runtime.\nBased on systematic analysis on key factors such as computing pattern, workload\ndistribution and write-back strategy, eight candidate SpMV/SpMSpV kernels are\nencapsulated into the framework to achieve high performance in a seamless\nmanner. A comprehensive study on machine learning based kernel selector is\nperformed to choose the kernel and adapt with the varieties of both the input\nand hardware from both accuracy and overhead perspectives. Experiments\ndemonstrate that the adaptive framework can substantially outperform the\nprevious state-of-the-art in real-world applications on NVIDIA Tesla K40m, P100\nand V100 GPUs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:20:02 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 01:32:10 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 12:28:15 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Li", "Min", ""], ["Ao", "Yulong", ""], ["Yang", "Chao", ""]]}]