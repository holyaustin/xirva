[{"id": "1810.00104", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Joseph G. Peters, Jason Schoeters", "title": "Temporal Cliques Admit Sparse Spanners", "comments": "This version of the article will appear in JCSS and a short version\n  with the same title was presented at ICALP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E)$ be an undirected graph on $n$ vertices and $\\lambda:E\\to\n2^{\\mathbb{N}}$ a mapping that assigns to every edge a non-empty set of integer\nlabels (times). Such a graph is {\\em temporally connected} if a path exists\nwith non-decreasing times from every vertex to every other vertex. In a seminal\npaper, Kempe, Kleinberg, and Kumar \\cite{KKK02} asked whether, given such a\ntemporal graph, a {\\em sparse} subset of edges always exists whose labels\nsuffice to preserve temporal connectivity -- a {\\em temporal spanner}. Axiotis\nand Fotakis \\cite{AF16} answered negatively by exhibiting a family of\n$\\Theta(n^2)$-dense temporal graphs which admit no temporal spanner of density\n$o(n^2)$. In this paper, we give the first positive answer as to the existence\nof $o(n^2)$-sparse spanners in a dense class of temporal graphs, by showing\n(constructively) that if $G$ is a complete graph, then one can always find a\ntemporal spanner of density $O(n \\log n)$.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 22:08:39 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 15:27:09 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 23:39:58 GMT"}, {"version": "v4", "created": "Sun, 31 May 2020 13:08:30 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 07:08:32 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Peters", "Joseph G.", ""], ["Schoeters", "Jason", ""]]}, {"id": "1810.00179", "submitter": "Daniele Santoro", "authors": "Daniele Santoro, Daniel Zozin, Daniele Pizzolli, Francesco De\n  Pellegrini, Silvio Cretti", "title": "Foggy: A Platform for Workload Orchestration in a Fog Computing\n  Environment", "comments": "The article has been accepted for publication by IEEE", "journal-ref": null, "doi": "10.1109/CloudCom.2017.62", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Foggy, an architectural framework and software\nplatform based on Open Source technologies. Foggy orchestrates application\nworkload, negotiates resources and supports IoT operations for multi-tier,\ndistributed, heterogeneous and decentralized Cloud Computing systems. Foggy is\ntailored for emerging domains such as 5G Networks and IoT, which demand\nresources and services to be distributed and located close to data sources and\nusers following the Fog Computing paradigm. Foggy provides a platform for\ninfrastructure owners and tenants (i.e., application providers) offering\nfunctionality of negotiation, scheduling and workload placement taking into\naccount traditional requirements (e.g. based on RAM, CPU, disk) and\nnon-traditional ones (e.g. based on networking) as well as diversified\nconstraints on location and access rights. Economics and pricing of resources\ncan also be considered by the Foggy model in a near future. The ability of\nFoggy to find a trade-off between infrastructure owners' and tenants' needs, in\nterms of efficient and optimized use of the infrastructure while satisfying the\napplication requirements, is demonstrated through three use cases in the video\nsurveillance and vehicle tracking contexts.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2018 09:43:35 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Santoro", "Daniele", ""], ["Zozin", "Daniel", ""], ["Pizzolli", "Daniele", ""], ["De Pellegrini", "Francesco", ""], ["Cretti", "Silvio", ""]]}, {"id": "1810.00305", "submitter": "Cheol-Ho Hong", "authors": "Cheol-Ho Hong and Blesson Varghese", "title": "Resource Management in Fog/Edge Computing: A Survey", "comments": "22 pages", "journal-ref": "ACM Computing Surveys (CSUR) 52.5 (2019) 1-37", "doi": "10.1145/3326066", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrary to using distant and centralized cloud data center resources,\nemploying decentralized resources at the edge of a network for processing data\ncloser to user devices, such as smartphones and tablets, is an upcoming\ncomputing paradigm, referred to as fog/edge computing. Fog/edge resources are\ntypically resource-constrained, heterogeneous, and dynamic compared to the\ncloud, thereby making resource management an important challenge that needs to\nbe addressed. This article reviews publications as early as 1991, with 85% of\nthe publications between 2013-2018, to identify and classify the architectures,\ninfrastructure, and underlying algorithms for managing resources in fog/edge\ncomputing.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 03:14:59 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hong", "Cheol-Ho", ""], ["Varghese", "Blesson", ""]]}, {"id": "1810.00401", "submitter": "Raphael Hiesgen", "authors": "Raphael Hiesgen, Dominik Charousset, Thomas C. Schmidt", "title": "A Configurable Transport Layer for CAF", "comments": null, "journal-ref": null, "doi": "10.1145/3281366.3281369", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The message-driven nature of actors lays a foundation for developing scalable\nand distributed software. While the actor itself has been thoroughly modeled,\nthe message passing layer lacks a common definition. Properties and guarantees\nof message exchange often shift with implementations and contexts. This adds\ncomplexity to the development process, limits portability, and removes\ntransparency from distributed actor systems.\n  In this work, we examine actor communication, focusing on the implementation\nand runtime costs of reliable and ordered delivery. Both guarantees are often\nbased on TCP for remote messaging, which mixes network transport with the\nsemantics of messaging. However, the choice of transport may follow different\nconstraints and is often governed by deployment. As a first step towards\nre-architecting actor-to-actor communication, we decouple the messaging\nguarantees from the transport protocol. We validate our approach by redesigning\nthe network stack of the C++ Actor Framework (CAF) so that it allows to combine\nan arbitrary transport protocol with additional functions for remote messaging.\nAn evaluation quantifies the cost of composability and the impact of individual\nlayers on the entire stack.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:23:14 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Hiesgen", "Raphael", ""], ["Charousset", "Dominik", ""], ["Schmidt", "Thomas C.", ""]]}, {"id": "1810.00596", "submitter": "Gabriele D'Angelo", "authors": "Gabriele D'Angelo, Stefano Ferretti, Moreno Marzolla", "title": "Fault Tolerant Adaptive Parallel and Distributed Simulation through\n  Functional Replication", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07310", "journal-ref": "Simulation Modelling Practice and Theory, Elsevier, vol. 93 (May\n  2019)", "doi": "10.1016/j.simpat.2018.09.012", "report-no": null, "categories": "cs.DC cs.MA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents FT-GAIA, a software-based fault-tolerant parallel and\ndistributed simulation middleware. FT-GAIA has being designed to reliably\nhandle Parallel And Distributed Simulation (PADS) models, which are needed to\nproperly simulate and analyze complex systems arising in any kind of scientific\nor engineering field. PADS takes advantage of multiple execution units run in\nmulticore processors, cluster of workstations or HPC systems. However, large\ncomputing systems, such as HPC systems that include hundreds of thousands of\ncomputing nodes, have to handle frequent failures of some components. To cope\nwith this issue, FT-GAIA transparently replicates simulation entities and\ndistributes them on multiple execution nodes. This allows the simulation to\ntolerate crash-failures of computing nodes. Moreover, FT-GAIA offers some\nprotection against Byzantine failures, since interaction messages among the\nsimulated entities are replicated as well, so that the receiving entity can\nidentify and discard corrupted messages. Results from an analytical model and\nfrom an experimental evaluation show that FT-GAIA provides a high degree of\nfault tolerance, at the cost of a moderate increase in the computational load\nof the execution units.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 09:51:46 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 08:56:49 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""], ["Marzolla", "Moreno", ""]]}, {"id": "1810.00645", "submitter": "Laurent Orgogozo", "authors": "Laurent Orgogozo (GET), Prokushkin Prokushkin, Oleg S. Pokrovsky\n  (GET), Christophe Grenier (LSCE), Michel Quintard (IMFT), Jerome Viers (GET),\n  St\\'ephane Audry (GET)", "title": "Numerical investigation of evapotranspiration processes in a forested\n  watershed of Central Siberia", "comments": null, "journal-ref": "5th European Conference On Permafrost, Jun 2018, Chamonix, France", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evapotranspiration has a major control on continental surfaces dynamics in\nforested boreal environments. For example, it has strong impacts on active\nlayer thickness, mainly because of its effect on water content within the upper\nlayers of the soil column. These are complex processes, depending not only on\nclimate forcings (atmospheric water demand) but also on physical,\ngeo-pedological and biological properties of the considered areas. Here we\npropose a numerical investigation of evapotranspiration processes in the active\nlayer of slopes of a forested watershed in Central Siberia. The effect on\nactual evapotranspiration of the spatial contrasts in terms of exposure, root\nlayer thickness and tree stand density within the watershed are simulated using\na recently developed high performance computing cryohydrogeological model,\npermaFoam.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 12:07:32 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Orgogozo", "Laurent", "", "GET"], ["Prokushkin", "Prokushkin", "", "GET"], ["Pokrovsky", "Oleg S.", "", "GET"], ["Grenier", "Christophe", "", "LSCE"], ["Quintard", "Michel", "", "IMFT"], ["Viers", "Jerome", "", "GET"], ["Audry", "St\u00e9phane", "", "GET"]]}, {"id": "1810.00722", "submitter": "Daniel Ziener", "authors": "Thorbj\\\"orn Posewsky and Daniel Ziener", "title": "Throughput Optimizations for FPGA-based Deep Neural Network Inference", "comments": null, "journal-ref": "Microprocessors and Microsystems 60C (2018) pp. 151-161", "doi": "10.1016/j.micpro.2018.04.004", "report-no": null, "categories": "cs.DC cs.AR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks are an extremely successful and widely used technique\nfor various pattern recognition and machine learning tasks. Due to power and\nresource constraints, these computationally intensive networks are difficult to\nimplement in embedded systems. Yet, the number of applications that can benefit\nfrom the mentioned possibilities is rapidly rising. In this paper, we propose\nnovel architectures for the inference of previously learned and arbitrary deep\nneural networks on FPGA-based SoCs that are able to overcome these limitations.\nOur key contributions include the reuse of previously transferred weight\nmatrices across multiple input samples, which we refer to as batch processing,\nand the usage of compressed weight matrices, also known as pruning. An\nextensive evaluation of these optimizations is presented. Both techniques allow\na significant mitigation of data transfers and speed-up the network inference\nby one order of magnitude. At the same time, we surpass the data throughput of\nfully-featured x86-based systems while only using a fraction of their energy\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 17:08:22 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Posewsky", "Thorbj\u00f6rn", ""], ["Ziener", "Daniel", ""]]}, {"id": "1810.01051", "submitter": "Parth Shah", "authors": "Parth Shah and Rachana Oza", "title": "Improved Parallel Rabin-Karp Algorithm Using Compute Unified Device\n  Architecture", "comments": "Information and Communication Technology for Intelligent Systems\n  (ICTIS 2017)", "journal-ref": "Smart Innovation, Systems and Technologies, vol 84. 2017 Springer,\n  Cham", "doi": "10.1007/978-3-319-63645-0_26", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  String matching algorithms are among one of the most widely used algorithms\nin computer science. Traditional string matching algorithms efficiency of\nunderlaying string matching algorithm will greatly increase the efficiency of\nany application. In recent years, Graphics processing units are emerged as\nhighly parallel processor. They out perform best of the central processing\nunits in scientific computation power. By combining recent advancement in\ngraphics processing units with string matching algorithms will allows to speed\nup process of string matching. In this paper we proposed modified parallel\nversion of Rabin-Karp algorithm using graphics processing unit. Based on that,\nresult of CPU as well as parallel GPU implementations are compared for\nevaluating effect of varying number of threads, cores, file size as well as\npattern size.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 03:21:28 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Shah", "Parth", ""], ["Oza", "Rachana", ""]]}, {"id": "1810.01489", "submitter": "Paul Liu", "authors": "Paul Liu, Jan Vondrak", "title": "Submodular Optimization in the MapReduce Model", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Submodular optimization has received significant attention in both practice\nand theory, as a wide array of problems in machine learning, auction theory,\nand combinatorial optimization have submodular structure. In practice, these\nproblems often involve large amounts of data, and must be solved in a\ndistributed way. One popular framework for running such distributed algorithms\nis MapReduce. In this paper, we present two simple algorithms for cardinality\nconstrained submodular optimization in the MapReduce model: the first is a\n$(1/2-o(1))$-approximation in 2 MapReduce rounds, and the second is a\n$(1-1/e-\\epsilon)$-approximation in $\\frac{1+o(1)}{\\epsilon}$ MapReduce rounds.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 20:08:27 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Liu", "Paul", ""], ["Vondrak", "Jan", ""]]}, {"id": "1810.01540", "submitter": "Julio A. Reyes-Munoz", "authors": "Julio A. Reyes-Munoz and Michael McGarry", "title": "The Effect of Data Marshalling on Computation Offloading Decisions", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conducted an extensive set of experiments with an offloading testbed to\nunderstand the impact that data marshalling techniques have on computation\noffloading decisions. We find that the popular JSON format to marshall data\nbetween client and server comes at a significant computational expense compared\nto a minimalistic raw data transfer. The computational time is significant in\nthat it affects computation offloading decisions in a variety of conditions. We\noutline some of these conditions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 23:32:32 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Reyes-Munoz", "Julio A.", ""], ["McGarry", "Michael", ""]]}, {"id": "1810.01547", "submitter": "Nasheen Nur", "authors": "Nasheen Nur, Wenwen Dou, Xi Niu, Siddharth Krishnan and Noseong Park", "title": "GI-OHMS: Graphical Inference to Detect Overlapping Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discovery of communities in complex networks is a topic of considerable\nrecent interest within the complex systems community. Due to the dynamic and\nrapidly evolving nature of large-scale networks, like online social networks,\nthe notion of stronger local and global interactions among the nodes in\ncommunities has become harder to capture. In this paper, we present a novel\ngraphical inference method - GI-OHMS (Graphical Inference in Observed-Hidden\nvariable Merged Seeded network) to solve the problem of overlapping community\ndetection. The novelty of our approach is in transforming the complex and dense\nnetwork of interest into an observed-hidden merged seeded(OHMS) network, which\npreserves the important community properties of the network. We further utilize\na graphical inference method (Bayesian Markov Random Field) to extract\ncommunities. The superiority of our approach lies in two main observations: 1)\nThe extracted OHMS network excludes many weaker connections, thus leading to a\nhigher accuracy of inference 2) The graphical inference step operates on a\nsmaller network, thus having much lower execution time. We demonstrate that our\nmethod outperforms the accuracy of other baseline algorithms like OSLOM, DEMON,\nand LEMON. To further improve execution time, we have a multi-threaded\nimplementation and demonstrate significant speed-up compared to\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 00:24:46 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Nur", "Nasheen", ""], ["Dou", "Wenwen", ""], ["Niu", "Xi", ""], ["Krishnan", "Siddharth", ""], ["Park", "Noseong", ""]]}, {"id": "1810.01609", "submitter": "Arjun Gambhir", "authors": "Evan Berkowitz, M.A. Clark, Arjun Gambhir, Ken McElvain, Amy\n  Nicholson, Enrico Rinaldi, Pavlos Vranas, Andr\\'e Walker-Loud, Chia Cheng\n  Chang, B\\'alint Jo\\'o, Thorsten Kurth, Kostas Orginos", "title": "Simulating the weak death of the neutron in a femtoscale universe with\n  near-Exascale computing", "comments": "2018 Gordon Bell Finalist: 9 pages, 9 figures; v2: fixed 2 typos and\n  appended acknowledgements", "journal-ref": "Supercomputing 2018, pp. 697-705", "doi": "10.1109/SC.2018.00058", "report-no": "LLNL-JRNL-749850, RIKEN-iTHEMS-Report-18", "categories": "hep-lat cs.DC nucl-th physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fundamental particle theory called Quantum Chromodynamics (QCD) dictates\neverything about protons and neutrons, from their intrinsic properties to\ninteractions that bind them into atomic nuclei. Quantities that cannot be fully\nresolved through experiment, such as the neutron lifetime (whose precise value\nis important for the existence of light-atomic elements that make the sun shine\nand life possible), may be understood through numerical solutions to QCD. We\ndirectly solve QCD using Lattice Gauge Theory and calculate nuclear observables\nsuch as neutron lifetime. We have developed an improved algorithm that\nexponentially decreases the time-to solution and applied it on the new CORAL\nsupercomputers, Sierra and Summit. We use run-time autotuning to distribute GPU\nresources, achieving 20% performance at low node count. We also developed\noptimal application mapping through a job manager, which allows CPU and GPU\njobs to be interleaved, yielding 15% of peak performance when deployed across\nlarge fractions of CORAL.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 07:14:18 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 23:59:25 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Berkowitz", "Evan", ""], ["Clark", "M. A.", ""], ["Gambhir", "Arjun", ""], ["McElvain", "Ken", ""], ["Nicholson", "Amy", ""], ["Rinaldi", "Enrico", ""], ["Vranas", "Pavlos", ""], ["Walker-Loud", "Andr\u00e9", ""], ["Chang", "Chia Cheng", ""], ["Jo\u00f3", "B\u00e1lint", ""], ["Kurth", "Thorsten", ""], ["Orginos", "Kostas", ""]]}, {"id": "1810.01698", "submitter": "Marc Shapiro", "authors": "Alejandro Z. Tomsic (DELYS), Manuel Bravo, Marc Shapiro (DELYS)", "title": "Distributed transactional reads: the strong, the quick, the fresh \\& the\n  impossible", "comments": null, "journal-ref": "ACM. 2018 ACM/IFIP/USENIX International Middleware Conference, Dec\n  2018, Rennes, France. ACM, 2018 ACM/IFIP/USENIX International Middleware\n  Conference, pp.14, 0018, Proceedings of 2018 ACM/IFIP/USENIX International\n  Middleware Conference. http://2018.middleware-conference.org", "doi": "10.1145/3274808.3274818", "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the costs and trade-offs of providing transactional\nconsistent reads in a distributed storage system. We identify the following\ndimensions: read consistency, read delay (latency), and data freshness. We show\nthat there is a three-way trade-off between them, which can be summarised as\nfollows: (i) it is not possible to ensure at the same time order-preserving\n(e.g., causally-consistent) or atomic reads, Minimal Delay, and maximal\nfreshness; thus, reading data that is the most fresh without delay is possible\nonly in a weakly-isolated mode; (ii) to ensure atomic or order-preserving reads\nat Minimal Delay imposes to read data from the past (not fresh); (iii) however,\norder-preserving minimal-delay reads can be fresher than atomic; (iv) reading\natomic or order-preserving data at maximal freshness may block reads or writes\nindefinitely. Our impossibility results hold independently of other features of\nthe database, such as update semantics (totally ordered or not) or data model\n(structured or unstructured). Guided by these results, we modify an existing\nprotocol to ensure minimal-delay reads (at the cost of freshness) under\natomic-visibility and causally-consistent semantics. Our experimental\nevaluation supports the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 11:51:44 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Tomsic", "Alejandro Z.", "", "DELYS"], ["Bravo", "Manuel", "", "DELYS"], ["Shapiro", "Marc", "", "DELYS"]]}, {"id": "1810.01839", "submitter": "Daniele Santoro", "authors": "Daniele Pizzolli, Giuseppe Cossu, Daniele Santoro, Luca Capra,\n  Corentin Dupont, Dukas Charalampos, Francesco De Pellegrini, Fabio Antonelli\n  and Silvio Cretti", "title": "Cloud4IoT: a heterogeneous, distributed and autonomic cloud platform for\n  the IoT", "comments": "The article has been accepted for publication by IEEE", "journal-ref": null, "doi": "10.1109/CloudCom.2016.0082", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Cloud4IoT, a platform offering automatic deployment,\norchestration and dynamic configuration of IoT support software components and\ndata-intensive applications for data processing and analytics, thus enabling\nplug-and-play integration of new sensor objects and dynamic workload\nscalability. Cloud4IoT enables the concept of Infrastructure as Code in the IoT\ncontext: it empowers IoT operations with the flexibility and elasticity of\nCloud services. Furthermore it shifts traditionally centralized Cloud\narchitectures towards a more distributed and decentralized computation\nparadigm, as required by IoT technologies, bridging the gap between Cloud\nComputing and IoT ecosystems. Thus, Cloud4IoT is playing a role similar to the\none covered by solutions like Fog Computing, Cloudlets or Mobile Edge Cloud.\nThe hierarchical architecture of Cloud4IoThosts a central Cloud platform and\nmultiple remote edge Cloud modules supporting dedicated devices, namely the IoT\nGateways, through which new sensor objects are made accessible to the platform.\nOverall, the platform is designed in order to support systems where IoT-based\nand data intensive applications may pose specific requirements for low latency,\nrestricted available bandwidth, or data locality. Cloud4IoT is built on several\nOpen Source technologies for containerisation and implementations of standards,\nprotocols and services for the IoT. We present the implementation of the\nplatform and demonstrate it in two different use cases.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 16:55:12 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Pizzolli", "Daniele", ""], ["Cossu", "Giuseppe", ""], ["Santoro", "Daniele", ""], ["Capra", "Luca", ""], ["Dupont", "Corentin", ""], ["Charalampos", "Dukas", ""], ["De Pellegrini", "Francesco", ""], ["Antonelli", "Fabio", ""], ["Cretti", "Silvio", ""]]}, {"id": "1810.01973", "submitter": "Feng Shi", "authors": "Feng Shi, Haochen Li, Yuhe Gao, Benjamin Kuschner, Song-Chun Zhu", "title": "Sparse Winograd Convolutional neural networks on small-scale systolic\n  arrays", "comments": "submitted to FPGA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconfigurability, energy-efficiency, and massive parallelism on FPGAs\nmake them one of the best choices for implementing efficient deep learning\naccelerators. However, state-of-art implementations seldom consider the balance\nbetween high throughput of computation power and the ability of the memory\nsubsystem to support it. In this paper, we implement an accelerator on FPGA by\ncombining the sparse Winograd convolution, clusters of small-scale systolic\narrays, and a tailored memory layout design. We also provide an analytical\nmodel analysis for the general Winograd convolution algorithm as a design\nreference. Experimental results on VGG16 show that it achieves very high\ncomputational resource utilization, 20x ~ 30x energy efficiency, and more than\n5x speedup compared with the dense implementation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 21:01:51 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Shi", "Feng", ""], ["Li", "Haochen", ""], ["Gao", "Yuhe", ""], ["Kuschner", "Benjamin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1810.01993", "submitter": "Mayur Mudigonda", "authors": "Thorsten Kurth, Sean Treichler, Joshua Romero, Mayur Mudigonda, Nathan\n  Luehr, Everett Phillips, Ankur Mahesh, Michael Matheson, Jack Deslippe,\n  Massimiliano Fatica, Prabhat, Michael Houston", "title": "Exascale Deep Learning for Climate Analytics", "comments": "12 pages, 5 tables, 4, figures, Super Computing Conference November\n  11-16, 2018, Dallas, TX, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We extract pixel-level masks of extreme weather patterns using variants of\nTiramisu and DeepLabv3+ neural networks. We describe improvements to the\nsoftware frameworks, input pipeline, and the network training algorithms\nnecessary to efficiently scale deep learning on the Piz Daint and Summit\nsystems. The Tiramisu network scales to 5300 P100 GPUs with a sustained\nthroughput of 21.0 PF/s and parallel efficiency of 79.0%. DeepLabv3+ scales up\nto 27360 V100 GPUs with a sustained throughput of 325.8 PF/s and a parallel\nefficiency of 90.7% in single precision. By taking advantage of the FP16 Tensor\nCores, a half-precision version of the DeepLabv3+ network achieves a peak and\nsustained throughput of 1.13 EF/s and 999.0 PF/s respectively.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2018 22:45:53 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Kurth", "Thorsten", ""], ["Treichler", "Sean", ""], ["Romero", "Joshua", ""], ["Mudigonda", "Mayur", ""], ["Luehr", "Nathan", ""], ["Phillips", "Everett", ""], ["Mahesh", "Ankur", ""], ["Matheson", "Michael", ""], ["Deslippe", "Jack", ""], ["Fatica", "Massimiliano", ""], ["Prabhat", "", ""], ["Houston", "Michael", ""]]}, {"id": "1810.02053", "submitter": "EPTCS", "authors": "Massimo Bartoletti (University of Cagliari, Italy), Sophia Knight\n  (Uppsala University, Sweden)", "title": "Proceedings 11th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 279, 2018", "doi": "10.4204/EPTCS.279", "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE'18, the 11th Interaction and\nConcurrency Experience, which was held in Madrid, Spain on the 20th and 21st of\nJune 2018 as a satellite event of DisCoTec'18.\n  The ICE workshop series features a distinguishing review and selection\nprocedure, allowing PC members to interact anonymously with authors. As in the\npast ten editions, this interaction considerably improved the accuracy of the\nfeedback from the reviewers and the quality of accepted papers, and offered the\nbasis for lively discussion during the workshop. For the second time, the 2018\nedition of ICE included double blind reviewing of original research papers, in\norder to increase fairness and avoid bias in reviewing.\n  Each paper was reviewed by three PC members, and altogether six papers were\naccepted for publication (the workshop also featured four oral presentations\nwhich are not part of this volume). We were proud to host three invited talks,\nby Elvira Albert, Silvia Crafa, and Alexey Gotsman. The abstracts of these\ntalks are included in this volume together with the regular papers. Final\nversions of the contributions, taking into account the discussion at the\nworkshop, are included.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 04:44:13 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Bartoletti", "Massimo", "", "University of Cagliari, Italy"], ["Knight", "Sophia", "", "Uppsala University, Sweden"]]}, {"id": "1810.02106", "submitter": "Mika\\\"el Rabie", "authors": "Keren Censor-Hillel, Mika\\\"el Rabie", "title": "Distributed Reconfiguration of Maximal Independent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a distributed maximal independent set (MIS)\nreconfiguration problem, in which there are two maximal independent sets for\nwhich every node is given its membership status, and the nodes need to\ncommunicate with their neighbors in order to find a reconfiguration schedule\nthat switches from the first MIS to the second. Such a schedule is a list of\nindependent sets that is restricted by forbidding two neighbors to change their\nmembership status at the same step. In addition, these independent sets should\nprovide some covering guarantee. We show that obtaining an actual MIS (and even\na 3-dominating set) in each intermediate step is impossible. However, we\nprovide efficient solutions when the intermediate sets are only required to be\nindependent and 4-dominating, which is almost always possible, as we fully\ncharacterize. Consequently, our goal is to pin down the tradeoff between the\npossible length of the schedule and the number of communication rounds. We\nprove that a constant length schedule can be found in\n$O(\\texttt{MIS}+\\texttt{R32})$ rounds, where $\\texttt{MIS}$ is the complexity\nof finding an MIS in a worst-case graph and $\\texttt{R32}$ is the complexity of\nfinding a $(3,2)$-ruling set. For bounded degree graphs, this is $O(\\log^*n)$\nrounds and we show that it is necessary. On the other extreme, we show that\nwith a constant number of rounds we can find a linear length schedule.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 09:07:07 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 11:55:59 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Rabie", "Mika\u00ebl", ""]]}, {"id": "1810.02137", "submitter": "Chengzheng Sun", "authors": "Chengzheng Sun, David Sun, Agustina, and Weiwei Cai", "title": "Real Differences between OT and CRDT for Co-Editors", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OT (Operational Transformation) was invented for supporting real-time\nco-editors in the late 1980s and has evolved to become a core technique used in\ntoday's working co-editors and adopted in major industrial products. CRDT\n(Commutative Replicated Data Type) for co-editors was first proposed around\n2006, under the name of WOOT (WithOut Operational Transformation). Follow-up\nCRDT variations are commonly labeled as \"post-OT\" techniques capable of making\nconcurrent operations natively commutative, and have made broad claims of\nsuperiority over OT solutions, in terms of correctness, time and space\ncomplexity, simplicity, etc. Over one decade later, however, CRDT solutions are\nrarely found in working co-editors, while OT solutions remain the choice for\nbuilding the vast majority of co-editors. Contradictions between this reality\nand CRDT's purported advantages have been the source of much debate and\nconfusion in co-editing research and developer communities. What is CRDT really\nto co-editing? What are the real differences between OT and CRDT for\nco-editors? What are the key factors that may have affected the adoption of and\nchoice between OT and CRDT for co-editors in the real world? In this paper, we\nreport our discoveries, in relation to these questions and beyond, from a\ncomprehensive review and comparison study on representative OT and CRDT\nsolutions and working co-editors based on them. Moreover, this work reveals\nfacts and presents evidences that refute CRDT claimed advantages over OT. We\nhope the results reported in this paper will help clear up common myths,\nmisconceptions, and confusions surrounding alternative co-editing techniques,\nand accelerate progress in co-editing technology for real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 10:22:06 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Sun", "Chengzheng", ""], ["Sun", "David", ""], ["Agustina", "", ""], ["Cai", "Weiwei", ""]]}, {"id": "1810.02174", "submitter": "Philipp Hoenisch", "authors": "Dr. Julian Hosp and Toby Hoenisch and Paul Kittiwongsunthorn", "title": "COMIT - Cryptographically-secure Off-chain Multi-asset Instant\n  Transaction Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the creation of Bitcoin in 2009 we have seen a great push towards\npublic and private blockchains. In order to avoid fragmentation, a global\nnetwork connecting all these blockchains is envisioned. Just like the Internet\nfacilitates communication and the transfer of information, we propose a system,\nsimilar in size and reach for payments and transactions: A\ncryptographically-secure off-chain multi-asset instant transaction network\n(COMIT) can connect and exchange any asset on any blockchain to any other\nblockchain using a cross-chain routing protocol (CRP). COMIT is a super\nblockchain network that allows for instant transactions which are enforced\nusing off-chain smart contracts. It leverages Payment Channels and Hashed\nTimelock Contracts (HTLC) across chains to solve the problem of double spending\nattacks without requiring a settlement onto the underlying blockchains. COMIT's\nconnectivity is provided by Liquidity Providers (LP), who operate on one or\nmore blockchains, acting as payment hubs and nodes on a single chain and market\nmakers in a decentralized network for cross-chain asset conversions. This paper\nlays out how COMIT works, the benefits for Users, Liquidity Providers and\nBusinesses; and how this does not only accelerate the adoption of blockchain\ntechnology, but furthermore allows for an integration with the traditional\nbanking system.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 12:42:04 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hosp", "Dr. Julian", ""], ["Hoenisch", "Toby", ""], ["Kittiwongsunthorn", "Paul", ""]]}, {"id": "1810.02186", "submitter": "Quan Nguyen Hoang", "authors": "Sang-Min Choi, Jiho Park, Quan Nguyen, Andre Cronje, Kiyoung Jang,\n  Hyunjoon Cheon, Yo-Sub Han, Byung-Ik Ahn", "title": "OPERA: Reasoning about continuous common knowledge in asynchronous\n  distributed systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new family of consensus protocols, namely\n\\emph{Lachesis-class} denoted by $\\mathcal{L}$, for distributed networks with\nguaranteed Byzantine fault tolerance. Each Lachesis protocol $L$ in\n$\\mathcal{L}$ has complete asynchrony, is leaderless, has no round robin, no\nproof-of-work, and has eventual consensus.\n  The core concept of our technology is the \\emph{OPERA chain}, generated by\nthe Lachesis protocol. In the most general form, each node in Lachesis has a\nset of $k$ neighbours of most preference. When receiving transactions a node\ncreates and shares an event block with all neighbours. Each event block is\nsigned by the hashes of the creating node and its $k$ peers. The OPERA chain of\nthe event blocks is a Directed Acyclic Graph (DAG); it guarantees practical\nByzantine fault tolerance (pBFT). Our framework is then presented using Lamport\ntimestamps and concurrent common knowledge.\n  Further, we present an example of Lachesis consensus protocol $L_0$ of our\nframework. Our $L_0$ protocol can reach consensus upon 2/3 of all participants'\nagreement to an event block without any additional communication overhead.\n$L_0$ protocol relies on a cost function to identify $k$ peers and to generate\nthe DAG-based OPERA chain. By creating a binary flag table that stores\nconnection information and share information between blocks, Lachesis achieves\nconsensus in fewer steps than pBFT protocol for consensus.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 13:08:10 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Choi", "Sang-Min", ""], ["Park", "Jiho", ""], ["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Jang", "Kiyoung", ""], ["Cheon", "Hyunjoon", ""], ["Han", "Yo-Sub", ""], ["Ahn", "Byung-Ik", ""]]}, {"id": "1810.02470", "submitter": "EPTCS", "authors": "Eduard Kamburjan (Technische Universit\\\"at Darmstadt, Germany), Reiner\n  H\\\"ahnle (Technische Universit\\\"at Darmstadt, Germany)", "title": "Prototyping Formal System Models with Active Objects", "comments": "In Proceedings ICE 2018, arXiv:1810.02053", "journal-ref": "EPTCS 279, 2018, pp. 52-67", "doi": "10.4204/EPTCS.279.7", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose active object languages as a development tool for formal system\nmodels of distributed systems. Additionally to a formalization based on a term\nrewriting system, we use established Software Engineering concepts, including\nsoftware product lines and object orientation that come with extensive tool\nsupport. We illustrate our modeling approach by prototyping a weak memory\nmodel. The resulting executable model is modular and has clear interfaces\nbetween communicating participants through object-oriented modeling.\nRelaxations of the basic memory model are expressed as self-contained variants\nof a software product line. As a modeling language we use the formal active\nobject language ABS which comes with an extensive tool set. This permits rapid\nformalization of core ideas, early validity checks in terms of formal invariant\nproofs, and debugging support by executing test runs. Hence, our approach\nsupports the prototyping of formal system models with early feedback.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 00:35:00 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Kamburjan", "Eduard", "", "Technische Universit\u00e4t Darmstadt, Germany"], ["H\u00e4hnle", "Reiner", "", "Technische Universit\u00e4t Darmstadt, Germany"]]}, {"id": "1810.02474", "submitter": "Zhongyuan Zhao", "authors": "Zhongyuan Zhao", "title": "Meeting Real-Time Constraint of Spectrum Management in TV Black-Space\n  Access", "comments": "9 pages, 7 figures, Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TV set feedback feature standardized in the next generation TV system,\nATSC 3.0, would enable opportunistic access of active TV channels in future\nCognitive Radio Networks. This new dynamic spectrum access approach is named as\nblack-space access, as it is complementary of current TV white space, which\nstands for inactive TV channels. TV black-space access can significantly\nincrease the available spectrum of Cognitive Radio Networks in populated urban\nmarkets, where spectrum shortage is most severe while TV whitespace is very\nlimited. However, to enable TV black-space access, secondary user has to\nevacuate a TV channel in a timely manner when TV user comes in. Such strict\nreal-time constraint is an unique challenge of spectrum management\ninfrastructure of Cognitive Radio Networks. In this paper, the real-time\nperformance of spectrum management with regard to the degree of centralization\nof infrastructure is modeled and tested. Based on collected empirical network\nlatency and database response time, we analyze the average evacuation time\nunder four structures of spectrum management infrastructure: fully\ndistribution, city-wide centralization, national-wide centralization, and\nsemi-national centralization. The results show that national wide\ncentralization may not meet the real-time requirement, while semi-national\ncentralization that use multiple co-located independent spectrum manager can\nachieve real-time performance while keep most of the operational advantage of\nfully centralized structure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 00:46:23 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 21:12:45 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 16:57:26 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhao", "Zhongyuan", ""]]}, {"id": "1810.02660", "submitter": "Hadrien Hendrikx", "authors": "Hadrien Hendrikx, Francis Bach and Laurent Massouli\\'e", "title": "Accelerated Decentralized Optimization with Local Updates for Smooth and\n  Strongly Convex Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of minimizing a sum of smooth and\nstrongly convex functions split over the nodes of a network in a decentralized\nfashion. We propose the algorithm $ESDACD$, a decentralized accelerated\nalgorithm that only requires local synchrony. Its rate depends on the condition\nnumber $\\kappa$ of the local functions as well as the network topology and\ndelays. Under mild assumptions on the topology of the graph, $ESDACD$ takes a\ntime $O((\\tau_{\\max} +\n\\Delta_{\\max})\\sqrt{{\\kappa}/{\\gamma}}\\ln(\\epsilon^{-1}))$ to reach a precision\n$\\epsilon$ where $\\gamma$ is the spectral gap of the graph, $\\tau_{\\max}$ the\nmaximum communication delay and $\\Delta_{\\max}$ the maximum computation time.\nTherefore, it matches the rate of $SSDA$, which is optimal when $\\tau_{\\max} =\n\\Omega\\left(\\Delta_{\\max}\\right)$. Applying $ESDACD$ to quadratic local\nfunctions leads to an accelerated randomized gossip algorithm of rate $O(\n\\sqrt{\\theta_{\\rm gossip}/n})$ where $\\theta_{\\rm gossip}$ is the rate of the\nstandard randomized gossip. To the best of our knowledge, it is the first\nasynchronous gossip algorithm with a provably improved rate of convergence of\nthe second moment of the error. We illustrate these results with experiments in\nidealized settings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:06:43 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 17:30:01 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 13:01:21 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Hendrikx", "Hadrien", ""], ["Bach", "Francis", ""], ["Massouli\u00e9", "Laurent", ""]]}, {"id": "1810.02679", "submitter": "Giovanni Iacca Dr.", "authors": "Giovanni Iacca", "title": "Distributed optimization in wireless sensor networks: an island-model\n  framework", "comments": null, "journal-ref": "Soft Computing, Volume 17, pp 2257-2277, 2013", "doi": "10.1007/s00500-013-1091-x", "report-no": null, "categories": "cs.NE cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Networks (WSNs) is an emerging technology in several\napplication domains, ranging from urban surveillance to environmental and\nstructural monitoring. Computational Intelligence (CI) techniques are\nparticularly suitable for enhancing these systems. However, when embedding CI\ninto wireless sensors, severe hardware limitations must be taken into account.\nIn this paper we investigate the possibility to perform an online, distributed\noptimization process within a WSN. Such a system might be used, for example, to\nimplement advanced network features like distributed modelling, self-optimizing\nprotocols, and anomaly detection, to name a few. The proposed approach, called\nDOWSN (Distributed Optimization for WSN) is an island-model infrastructure in\nwhich each node executes a simple, computationally cheap (both in terms of CPU\nand memory) optimization algorithm, and shares promising solutions with its\nneighbors. We perform extensive tests of different DOWSN configurations on a\nbenchmark made up of continuous optimization problems; we analyze the influence\nof the network parameters (number of nodes, inter-node communication period and\nprobability of accepting incoming solutions) on the optimization performance.\nFinally, we profile energy and memory consumption of DOWSN to show the\nefficient usage of the limited hardware resources available on the sensor\nnodes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 13:44:59 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Iacca", "Giovanni", ""]]}, {"id": "1810.02749", "submitter": "Ellis Solaiman", "authors": "Awatif Alqahtani and Pankesh Patel and Ellis Solaiman and Rajiv Ranjan", "title": "Demonstration Abstract: A Toolkit for Specifying Service Level\n  Agreements for IoT applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today we see the use of the Internet of Things (IoT) in various application\ndomains such as healthcare, smart homes, smart cars, and smart-x applications\nin smart cities. The number of applications based on IoT and cloud computing is\nprojected to increase rapidly over the next few years. IoT-based services must\nmeet the guaranteed levels of quality of service (QoS) to match users'\nexpectations. Ensuring QoS through specifying the QoS constraints using Service\nLevel Agreements (SLAs) is crucial. Therefore, as a first step toward SLA\nmanagement, it is essential to provide an SLA specification in a\nmachine-readable format. In this paper, we demonstrate a toolkit for creating\nSLA specifications for IoT applications. The toolkit is used to simplify the\nprocess of capturing the requirements of IoT applications. We present a\ndemonstration of the toolkit using a Remote Health Monitoring Service (RHMS)\nusecase. The toolkit supports the following: (1) specifying the Service-Level\nObjectives (SLO) of an IoT application at the application level; (2) specifying\nthe workflow activities of the IoT application; (3) mapping each activity to\nthe required software and hardware resources and specifying the constraints of\nSLOs and other configuration- related metrics of the required hardware and\nsoftware; and (4) creating the composed SLA in JSON format.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 15:32:17 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Alqahtani", "Awatif", ""], ["Patel", "Pankesh", ""], ["Solaiman", "Ellis", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1810.02762", "submitter": "Roman Nikiforov", "authors": "Roman Nikiforov", "title": "Clustering-based Anomaly Detection for microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is an important step in the management and monitoring of\ndata centers and cloud computing platforms. The ability to detect anomalous\nvirtual machines before real failures occur results in reduced downtime while\noperations engineers urgently recover malfunctioning virtual machines,\nefficient root cause analysis, and improved customer optics in the event said\nmalfunction lead to an outage. Virtual machines could fail at any time, whether\nin a lab or production system. If there is no anomaly detection system, and a\nvirtual machine in a lab environment fails, the QA and DEV team will have to\nswitch to another environment while the OPS team fixes the failure. The\npotential impact of failing to detect anomalous virtual machines can result in\nfinancial ramifications, both when developing new features and servicing\nexisting ones. This paper presents a model that can efficiently detect\nanomalous virtual machines both in production and testing environments.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 17:12:37 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Nikiforov", "Roman", ""]]}, {"id": "1810.02780", "submitter": "Jian Shi", "authors": "Jian Shi, Brian Sullivan, Mike Mazzola, Babak Saravi, Uttam Adhikari,\n  Tomaz Haupt", "title": "A Relaxation-based Network Decomposition Algorithm for Parallel\n  Transient Stability Simulation with Improved Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient stability simulation of a large-scale and interconnected electric\npower system involves solving a large set of differential algebraic equations\n(DAEs) at every simulation time-step. With the ever-growing size and complexity\nof power grids, dynamic simulation becomes more time-consuming and\ncomputationally difficult using conventional sequential simulation techniques.\nTo cope with this challenge, this paper aims to develop a fully distributed\napproach intended for implementation on High Performance Computer (HPC)\nclusters. A novel, relaxation-based domain decomposition algorithm known as\nParallel-General-Norton with Multiple-port Equivalent (PGNME) is proposed as\nthe core technique of a two-stage decomposition approach to divide the overall\ndynamic simulation problem into a set of subproblems that can be solved\nconcurrently to exploit parallelism and scalability. While the convergence\nproperty has traditionally been a concern for relaxation-based decomposition,\nan estimation mechanism based on multiple-port network equivalent is adopted as\nthe preconditioner to enhance the convergence of the proposed algorithm. The\nproposed algorithm is illustrated using rigorous mathematics and validated both\nin terms of speed-up and capability. Moreover, a complexity analysis is\nperformed to support the observation that PGNME scales well when the size of\nthe subproblems are sufficiently large.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 16:28:08 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Shi", "Jian", ""], ["Sullivan", "Brian", ""], ["Mazzola", "Mike", ""], ["Saravi", "Babak", ""], ["Adhikari", "Uttam", ""], ["Haupt", "Tomaz", ""]]}, {"id": "1810.02781", "submitter": "Miguel Coimbra", "authors": "Miguel E. Coimbra, S\\'ergio Esteves, Alexandre P. Francisco, Lu\\'is\n  Veiga", "title": "VeilGraph: Streaming Graph Approximations", "comments": "10 pages, 3 algorithm, 7 figures, 1 table, 5 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs are found in a plethora of domains, including online social networks,\nthe World Wide Web and the study of epidemics, to name a few. With the advent\nof greater volumes of information and the need for continuously updated results\nunder temporal constraints, it is necessary to explore novel approaches that\nfurther enable performance improvements.\n  In the scope of stream processing over graphs, we research the trade-offs\nbetween result accuracy and the speedup of approximate computation techniques.\nWe see this as a natural path towards these performance improvements. Herein we\npresent \\name, through which we conducted our research. We showcase an\ninnovative model for approximate graph processing, implemented in\n\\texttt{Apache Flink}.\n  We analyze our model and evaluate it with the case study of the PageRank\nalgorithm \\cite{pageRank}, perhaps the most famous measure of vertex centrality\nused to rank websites in search engine results. %In light of our model, we\ndiscuss the challenges driven by relations between result accuracy and\npotential performance gains. Our experiments, even when set up for favoring\n\\texttt{Flink} for comparability, show that \\name can improve performance up to\n3X speedups, while achieving result quality above 95\\% when compared to results\nof the traditional version of PageRank without any summarization or\napproximation techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 16:29:51 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 17:51:30 GMT"}, {"version": "v3", "created": "Fri, 13 Dec 2019 15:27:07 GMT"}, {"version": "v4", "created": "Tue, 17 Dec 2019 22:54:35 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Coimbra", "Miguel E.", ""], ["Esteves", "S\u00e9rgio", ""], ["Francisco", "Alexandre P.", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1810.02848", "submitter": "Peter Robinson", "authors": "Calvin Newport and Peter Robinson", "title": "Fault-Tolerant Consensus with an Abstract MAC Layer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study fault-tolerant distributed consensus in wireless\nsystems. In more detail, we produce two new randomized algorithms that solve\nthis problem in the abstract MAC layer model, which captures the basic\ninterface and communication guarantees provided by most wireless MAC layers.\nOur algorithms work for any number of failures, require no advance knowledge of\nthe network participants or network size, and guarantee termination with high\nprobability after a number of broadcasts that are polynomial in the network\nsize. Our first algorithm satisfies the standard agreement property, while our\nsecond trades a faster termination guarantee in exchange for a looser agreement\nproperty in which most nodes agree on the same value. These are the first known\nfault-tolerant consensus algorithms for this model. In addition to our main\nupper bound results, we explore the gap between the abstract MAC layer and the\nstandard asynchronous message passing model by proving fault-tolerant consensus\nis impossible in the latter in the absence of information regarding the network\nparticipants, even if we assume no faults, allow randomized solutions, and\nprovide the algorithm a constant-factor approximation of the network size.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 18:51:19 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Newport", "Calvin", ""], ["Robinson", "Peter", ""]]}, {"id": "1810.02911", "submitter": "George Teodoro", "authors": "Luis F. R. Taveira, Tahsin Kurc, Alba C. M. A. Melo, Jun Kong, Erich\n  Bremer, Joel H. Saltz, George Teodoro", "title": "Tuning for Tissue Image Segmentation Workflows for Accuracy and\n  Performance", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a software platform that integrates methods and tools for\nmulti-objective parameter auto- tuning in tissue image segmentation workflows.\nThe goal of our work is to provide an approach for improving the accuracy of\nnucleus/cell segmentation pipelines by tuning their input parameters. The\nshape, size and texture features of nuclei in tissue are important biomarkers\nfor disease prognosis, and accurate computation of these features depends on\naccurate delineation of boundaries of nuclei. Input parameters in many nucleus\nsegmentation workflows affect segmentation accuracy and have to be tuned for\noptimal performance. This is a time-consuming and computationally expensive\nprocess; automating this step facilitates more robust image segmentation\nworkflows and enables more efficient application of image analysis in large\nimage datasets. Our software platform adjusts the parameters of a nuclear\nsegmentation algorithm to maximize the quality of image segmentation results\nwhile minimizing the execution time. It implements several optimization methods\nto search the parameter space efficiently. In addition, the methodology is\ndeveloped to execute on high performance computing systems to reduce the\nexecution time of the parameter tuning phase. Our results using three\nreal-world image segmentation workflows demonstrate that the proposed solution\nis able to (1) search a small fraction (about 100 points) of the parameter\nspace, which contains billions to trillions of points, and improve the quality\nof segmentation output by 1.20x, 1.29x, and 1.29x, on average; (2) decrease the\nexecution time of a segmentation workflow by up to 11.79x while improving\noutput quality; and (3) effectively use parallel systems to accelerate\nparameter tuning and segmentation phases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 23:39:02 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Taveira", "Luis F. R.", ""], ["Kurc", "Tahsin", ""], ["Melo", "Alba C. M. A.", ""], ["Kong", "Jun", ""], ["Bremer", "Erich", ""], ["Saltz", "Joel H.", ""], ["Teodoro", "George", ""]]}, {"id": "1810.02974", "submitter": "Vero Estrada-Galinanes", "authors": "Vero Estrada-Gali\\~nanes (1 and 2), Ethan Miller (2), Pascal Felber\n  (1), and Jehan-Fran\\c{c}ois P\\^aris (3) ((1) University of Neuch\\^atel, 2000\n  Neuch\\^atel, Switzerland, (2) University of California, Santa Cruz, CA 95064,\n  USA, (3) University of Houston, Houston, TX 77204-3010, USA)", "title": "Alpha Entanglement Codes: Practical Erasure Codes to Archive Data in\n  Unreliable Environments", "comments": "The publication has 12 pages and 13 figures. This work was partially\n  supported by Swiss National Science Foundation SNSF Doc.Mobility 162014, 2018\n  48th Annual IEEE/IFIP International Conference on Dependable Systems and\n  Networks (DSN)", "journal-ref": null, "doi": "10.1109/DSN.2018.00030", "report-no": null, "categories": "cs.DC cs.CR cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data centres that use consumer-grade disks drives and distributed\npeer-to-peer systems are unreliable environments to archive data without enough\nredundancy. Most redundancy schemes are not completely effective for providing\nhigh availability, durability and integrity in the long-term. We propose alpha\nentanglement codes, a mechanism that creates a virtual layer of highly\ninterconnected storage devices to propagate redundant information across a\nlarge scale storage system. Our motivation is to design flexible and practical\nerasure codes with high fault-tolerance to improve data durability and\navailability even in catastrophic scenarios. By flexible and practical, we mean\ncode settings that can be adapted to future requirements and practical\nimplementations with reasonable trade-offs between security, resource usage and\nperformance. The codes have three parameters. Alpha increases storage overhead\nlinearly but increases the possible paths to recover data exponentially. Two\nother parameters increase fault-tolerance even further without the need of\nadditional storage. As a result, an entangled storage system can provide high\navailability, durability and offer additional integrity: it is more difficult\nto modify data undetectably. We evaluate how several redundancy schemes perform\nin unreliable environments and show that alpha entanglement codes are flexible\nand practical codes. Remarkably, they excel at code locality, hence, they\nreduce repair costs and become less dependent on storage locations with poor\navailability. Our solution outperforms Reed-Solomon codes in many disaster\nrecovery scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 10:32:33 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Estrada-Gali\u00f1anes", "Vero", "", "1 and 2"], ["Miller", "Ethan", ""], ["Felber", "Pascal", ""], ["P\u00e2ris", "Jehan-Fran\u00e7ois", ""]]}, {"id": "1810.03035", "submitter": "Steven Wei-der Chien", "authors": "Steven W. D. Chien, Stefano Markidis, Chaitanya Prasad Sishtla, Luis\n  Santos, Pawel Herman, Sai Narasimhamurthy, Erwin Laure", "title": "Characterizing Deep-Learning I/O Workloads in TensorFlow", "comments": "Accepted for publication at pdsw-DISCS 2018", "journal-ref": null, "doi": "10.1109/PDSW-DISCS.2018.00011", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Deep-Learning (DL) computing frameworks rely on the\nperformance of data ingestion and checkpointing. In fact, during the training,\na considerable high number of relatively small files are first loaded and\npre-processed on CPUs and then moved to accelerator for computation. In\naddition, checkpointing and restart operations are carried out to allow DL\ncomputing frameworks to restart quickly from a checkpoint. Because of this, I/O\naffects the performance of DL applications. In this work, we characterize the\nI/O performance and scaling of TensorFlow, an open-source programming framework\ndeveloped by Google and specifically designed for solving DL problems. To\nmeasure TensorFlow I/O performance, we first design a micro-benchmark to\nmeasure TensorFlow reads, and then use a TensorFlow mini-application based on\nAlexNet to measure the performance cost of I/O and checkpointing in TensorFlow.\nTo improve the checkpointing performance, we design and implement a burst\nbuffer. We find that increasing the number of threads increases TensorFlow\nbandwidth by a maximum of 2.3x and 7.8x on our benchmark environments. The use\nof the tensorFlow prefetcher results in a complete overlap of computation on\naccelerator and input pipeline on CPU eliminating the effective cost of I/O on\nthe overall performance. The use of a burst buffer to checkpoint to a fast\nsmall capacity storage and copy asynchronously the checkpoints to a slower\nlarge capacity storage resulted in a performance improvement of 2.6x with\nrespect to checkpointing directly to slower storage on our benchmark\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 18:31:51 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chien", "Steven W. D.", ""], ["Markidis", "Stefano", ""], ["Sishtla", "Chaitanya Prasad", ""], ["Santos", "Luis", ""], ["Herman", "Pawel", ""], ["Narasimhamurthy", "Sai", ""], ["Laure", "Erwin", ""]]}, {"id": "1810.03056", "submitter": "Eliu Huerta", "authors": "E. A. Huerta, Roland Haas, Shantenu Jha, Mark Neubauer, Daniel S. Katz", "title": "Supporting High-Performance and High-Throughput Computing for\n  Experimental Science", "comments": "13 pages, 7 figures. Accepted to Computing and Software for Big\n  Science", "journal-ref": "Comput Softw Big Sci (2019) 3: 5", "doi": "10.1007/s41781-019-0022-7", "report-no": null, "categories": "cs.DC astro-ph.HE gr-qc hep-ex hep-ph hep-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of experimental science facilities-instruments and observatories,\nsuch as the Large Hadron Collider, the Laser Interferometer Gravitational Wave\nObservatory, and the upcoming Large Synoptic Survey Telescope-has brought about\nchallenging, large-scale computational and data processing requirements.\nTraditionally, the computing infrastructure to support these facility's\nrequirements were organized into separate infrastructure that supported their\nhigh-throughput needs and those that supported their high-performance computing\nneeds. We argue that to enable and accelerate scientific discovery at the scale\nand sophistication that is now needed, this separation between high-performance\ncomputing and high-throughput computing must be bridged and an integrated,\nunified infrastructure provided. In this paper, we discuss several case studies\nwhere such infrastructure has been implemented. These case studies span\ndifferent science domains, software systems, and application requirements as\nwell as levels of sustainability. A further aim of this paper is to provide a\nbasis to determine the common characteristics and requirements of such\ninfrastructure, as well as to begin a discussion of how best to support the\ncomputing requirements of existing and future experimental science facilities.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 21:13:01 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 21:03:43 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Huerta", "E. A.", ""], ["Haas", "Roland", ""], ["Jha", "Shantenu", ""], ["Neubauer", "Mark", ""], ["Katz", "Daniel S.", ""]]}, {"id": "1810.03120", "submitter": "Andrzej Pelc", "authors": "Andrzej Pelc, Ram Narayan Yadav", "title": "Using Time to Break Symmetry: Universal Deterministic Anonymous\n  Rendezvous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two anonymous mobile agents navigate synchronously in an anonymous graph and\nhave to meet at a node, using a deterministic algorithm. This is a symmetry\nbreaking task called rendezvous, equivalent to the fundamental task of leader\nelection between the agents. When is this feasible in a completely anonymous\nenvironment? It is known that agents can always meet if their initial positions\nare nonsymmetric, and that if they are symmetric and agents start\nsimultaneously then rendezvous is impossible. What happens for symmetric\ninitial positions with non-simultaneous start? Can symmetry between the agents\nbe broken by the delay between their starting times?\n  In order to answer these questions, we consider {\\em space-time initial\nconfigurations} (abbreviated by STIC). A STIC is formalized as\n$[(u,v),\\delta]$, where $u$ and $v$ are initial nodes of the agents in some\ngraph and $\\delta$ is a non-negative integer that represents the difference\nbetween their starting times. A STIC is {\\em feasible} if there exists a\ndeterministic algorithm, even dedicated to this particular STIC, which\naccomplishes rendezvous for it. Our main result is a characterization of all\nfeasible STICs and the design of a universal deterministic algorithm that\naccomplishes rendezvous for all of them without {\\em any } a priori knowledge\nof the agents. Thus, as far as feasibility is concerned, we completely solve\nthe problem of symmetry breaking between two anonymous agents in anonymous\ngraphs. Moreover, we show that such a universal algorithm cannot work for all\nfeasible STICs in time polynomial in the initial distance between the agents.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2018 11:16:59 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Pelc", "Andrzej", ""], ["Yadav", "Ram Narayan", ""]]}, {"id": "1810.03264", "submitter": "Wei Dai", "authors": "Wei Dai, Yi Zhou, Nanqing Dong, Hao Zhang, Eric P. Xing", "title": "Toward Understanding the Impact of Staleness in Distributed Machine\n  Learning", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many distributed machine learning (ML) systems adopt the non-synchronous\nexecution in order to alleviate the network communication bottleneck, resulting\nin stale parameters that do not reflect the latest updates. Despite much\ndevelopment in large-scale ML, the effects of staleness on learning are\ninconclusive as it is challenging to directly monitor or control staleness in\ncomplex distributed environments. In this work, we study the convergence\nbehaviors of a wide array of ML models and algorithms under delayed updates.\nOur extensive experiments reveal the rich diversity of the effects of staleness\non the convergence of ML algorithms and offer insights into seemingly\ncontradictory reports in the literature. The empirical findings also inspire a\nnew convergence analysis of stochastic gradient descent in non-convex\noptimization under staleness, matching the best-known convergence rate of\nO(1/\\sqrt{T}).\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 03:57:39 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Dai", "Wei", ""], ["Zhou", "Yi", ""], ["Dong", "Nanqing", ""], ["Zhang", "Hao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1810.03278", "submitter": "Aerin Kim", "authors": "Rohit Pandey, Yifan Chang, Cameron White, Gaurav Jagtiani, Aerin Young\n  Kim, Gil Lapid Shafriri, Sathya Singh", "title": "Optimizing Waiting Thresholds Within A State Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Azure (the cloud service provided by Microsoft) is composed of physical\ncomputing units which are called nodes. These nodes are controlled by a\nsoftware component called Fabric Controller (FC), which can consider the nodes\nto be in one of many different states such as Ready, Unhealthy, Booting, etc.\nSome of these states correspond to a node being unresponsive to FCs requests.\nWhen a node goes unresponsive for more than a set threshold, FC intervenes and\nreboots the node. We minimized the downtime caused by the intervention\nthreshold when a node switches to the Unhealthy state by fitting various\nheavy-tail probability distributions. We consider using features of the node to\ncustomize the organic recovery model to the individual nodes that go unhealthy.\nThis regression approach allows us to use information about the node like\nhardware, software versions, historical performance indicators, etc. to inform\nthe organic recovery model and hence the optimal threshold. In another\ndirection, we consider generalizing this to an arbitrary number of thresholds\nwithin the node state machine (or Markov chain). When the states become\nintertwined in ways that different thresholds start affecting each other, we\ncan't simply optimize each of them in isolation. For best results, we must\nconsider this as an optimization problem in many variables (the number of\nthresholds). We no longer have a nice closed form solution for this more\ncomplex problem like we did with one threshold, but we can still use numerical\ntechniques (gradient descent) to solve it.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 06:25:38 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Pandey", "Rohit", ""], ["Chang", "Yifan", ""], ["White", "Cameron", ""], ["Jagtiani", "Gaurav", ""], ["Kim", "Aerin Young", ""], ["Shafriri", "Gil Lapid", ""], ["Singh", "Sathya", ""]]}, {"id": "1810.03298", "submitter": "Won-Yong Shin", "authors": "Huifa Lin, Won-Yong Shin, Bang Chul Jung", "title": "Multi-Stream Opportunistic Network Decoupling: Relay Selection and\n  Interference Management", "comments": "14 pages, 6 figures, 2 tables, to appear in IEEE Transactions on\n  Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.MA cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study multi-stream transmission in the $K \\times N \\times K$ channel with\ninterfering relay nodes, consisting of $K$ multi-antenna source--destination\n(S--D) pairs and $N$ single-antenna half-duplex relay nodes between the S--D\npairs. We propose a new achievable scheme operating with partial effective\nchannel gain, termed multi-stream opportunistic network decoupling (MS-OND),\nwhich achieves the optimal degrees of freedom (DoF) under a certain relay\nscaling law. Our protocol is built upon the conventional OND that leads to\nvirtual full-duplex mode with one data stream transmission per S--D pair,\ngeneralizing the idea of OND to multi-stream scenarios by leveraging relay\nselection and interference management. Specifically, two subsets of relay nodes\nare opportunistically selected using alternate relaying in terms of producing\nor receiving the minimum total interference level. For interference management,\neach source node sends $S \\,(1 \\le S \\le M)$ data streams to selected relay\nnodes with random beamforming for the first hop, while each destination node\nreceives its desired $S$ streams from the selected relay nodes via\nopportunistic interference alignment for the second hop, where $M$ is the\nnumber of antennas at each source or destination node. Our analytical results\nare validated by numerical evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 07:58:57 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Lin", "Huifa", ""], ["Shin", "Won-Yong", ""], ["Jung", "Bang Chul", ""]]}, {"id": "1810.03357", "submitter": "Abdul Wahab Mr", "authors": "Abdul Wahab and Waqas Mehmood", "title": "Survey of Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed ledger technology has gained wide popularity and adoption since\nthe emergence of bitcoin in 2008 which is based on proof of work (PoW). It is a\ndistributed, transparent and immutable database of records of all the\ntransactions or events that have been shared and executed among the\nparticipants. All the transactions are verified and maintained by multiple\nnodes across a network without a central authority through a distributed\ncryptographic mechanism, a consensus protocol. It forms the core of this\ntechnology that not only validates the information appended to the ledger but\nalso ensures the order in which it is appended across all the nodes. It is the\nfoundation of its security, accountability and trust. While many researchers\nare working on improving the current protocol to be quantum resistant,\nfault-tolerant, and energy-efficient. Others are focused on developing\ndifferent variants of the protocol, best suited for specific use cases. In this\npaper, we shall review different consensus protocols of distributed ledger\ntechnologies and their implementations. We shall also review their properties,\nconcept and similar-work followed by a brief analysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 10:04:39 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 05:14:58 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Wahab", "Abdul", ""], ["Mehmood", "Waqas", ""]]}, {"id": "1810.03417", "submitter": "Arda Aytekin", "authors": "Arda Aytekin and Martin Biel and Mikael Johansson", "title": "POLO: a POLicy-based Optimization library", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present POLO --- a C++ library for large-scale parallel optimization\nresearch that emphasizes ease-of-use, flexibility and efficiency in algorithm\ndesign. It uses multiple inheritance and template programming to decompose\nalgorithms into essential policies and facilitate code reuse. With its clear\nseparation between algorithm and execution policies, it provides researchers\nwith a simple and powerful platform for prototyping ideas, evaluating them on\ndifferent parallel computing architectures and hardware platforms, and\ngenerating compact and efficient production code. A C-API is included for\ncustomization and data loading in high-level languages. POLO enables users to\nmove seamlessly from serial to multi-threaded shared-memory and multi-node\ndistributed-memory executors. We demonstrate how POLO allows users to implement\nstate-of-the-art asynchronous parallel optimization algorithms in just a few\nlines of code and report experiment results from shared and distributed-memory\ncomputing architectures. We provide both POLO and POLO.jl, a wrapper around\nPOLO written in the Julia language, at https://github.com/pologrp under the\npermissive MIT license.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 12:58:26 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Aytekin", "Arda", ""], ["Biel", "Martin", ""], ["Johansson", "Mikael", ""]]}, {"id": "1810.03488", "submitter": "Albin Severinson", "authors": "Albin Severinson, Alexandre Graell i Amat, Eirik Rosnes, Francisco\n  Lazaro, and Gianluigi Liva", "title": "A Droplet Approach Based on Raptor Codes for Distributed Computing With\n  Straggling Servers", "comments": "Accepted at the 2018 International Symposium on Turbo Codes &\n  Iterative Information Processing, Hong Kong", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG cs.PF math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a coded distributed computing scheme based on Raptor codes to\naddress the straggler problem. In particular, we consider a scheme where each\nserver computes intermediate values, referred to as droplets, that are either\nstored locally or sent over the network. Once enough droplets are collected,\nthe computation can be completed. Compared to previous schemes in the\nliterature, our proposed scheme achieves lower computational delay when the\ndecoding time is taken into account.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:23:27 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Severinson", "Albin", ""], ["Amat", "Alexandre Graell i", ""], ["Rosnes", "Eirik", ""], ["Lazaro", "Francisco", ""], ["Liva", "Gianluigi", ""]]}, {"id": "1810.03506", "submitter": "Eric Neiva", "authors": "Eric Neiva and Santiago Badia and Alberto F. Mart\\'in and Michele\n  Chiumenti", "title": "A scalable parallel finite element framework for growing geometries.\n  Application to metal additive manufacturing", "comments": null, "journal-ref": null, "doi": "10.1002/nme.6085", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces an innovative parallel, fully-distributed finite element\nframework for growing geometries and its application to metal additive\nmanufacturing. It is well-known that virtual part design and qualification in\nadditive manufacturing requires highly-accurate multiscale and multiphysics\nanalyses. Only high performance computing tools are able to handle such\ncomplexity in time frames compatible with time-to-market. However, efficiency,\nwithout loss of accuracy, has rarely held the centre stage in the numerical\ncommunity. Here, in contrast, the framework is designed to adequately exploit\nthe resources of high-end distributed-memory machines. It is grounded on three\nbuilding blocks: (1) Hierarchical adaptive mesh refinement with octree-based\nmeshes; (2) a parallel strategy to model the growth of the geometry; (3)\nstate-of-the-art parallel iterative linear solvers. Computational experiments\nconsider the heat transfer analysis at the part scale of the printing process\nby powder-bed technologies. After verification against a 3D benchmark, a\nstrong-scaling analysis assesses performance and identifies major sources of\nparallel overhead. A third numerical example examines the efficiency and\nrobustness of (2) in a curved 3D shape. Unprecedented parallelism and\nscalability were achieved in this work. Hence, this framework contributes to\ntake on higher complexity and/or accuracy, not only of part-scale simulations\nof metal or polymer additive manufacturing, but also in welding, sedimentation,\natherosclerosis, or any other physical problem where the physical domain of\ninterest grows in time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 14:51:31 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 09:21:34 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 10:20:37 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Neiva", "Eric", ""], ["Badia", "Santiago", ""], ["Mart\u00edn", "Alberto F.", ""], ["Chiumenti", "Michele", ""]]}, {"id": "1810.03530", "submitter": "Michael Kamp", "authors": "Michael Kamp and Mario Boley and Olana Missura and Thomas G\\\"artner", "title": "Effective Parallelisation for Machine Learning", "comments": "Advances in Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel parallelisation scheme that simplifies the adaptation of\nlearning algorithms to growing amounts of data as well as growing needs for\naccurate and confident predictions in critical applications. In contrast to\nother parallelisation techniques, it can be applied to a broad class of\nlearning algorithms without further mathematical derivations and without\nwriting dedicated code, while at the same time maintaining theoretical\nperformance guarantees. Moreover, our parallelisation scheme is able to reduce\nthe runtime of many learning algorithms to polylogarithmic time on\nquasi-polynomially many processing units. This is a significant step towards a\ngeneral answer to an open question on the efficient parallelisation of machine\nlearning algorithms in the sense of Nick's Class (NC). The cost of this\nparallelisation is in the form of a larger sample complexity. Our empirical\nstudy confirms the potential of our parallelisation scheme with fixed numbers\nof processors and instances in realistic application scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 15:35:07 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Kamp", "Michael", ""], ["Boley", "Mario", ""], ["Missura", "Olana", ""], ["G\u00e4rtner", "Thomas", ""]]}, {"id": "1810.03702", "submitter": "Michel Fliess", "authors": "Maria Bekcheva, Michel Fliess, C\\'edric Join, Alireza Moradi, Hugues\n  Mounier", "title": "Improving resource elasticity in cloud computing thanks to model-free\n  control", "comments": "in French", "journal-ref": "ISTE OpenScience Automatique/Control, vol. 2, 2018", "doi": null, "report-no": null, "categories": "cs.DC cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing management, the dynamic adaptation of computing resource\nallocations under time-varying workload is an active domain of investigation.\nSeveral control strategies were already proposed. Here the model-free control\nsetting and the corresponding \"intelligent\" controllers, which are most\nsuccessful in many concrete engineering situations, are employed for the\n\"horizontal elasticity.\" When compared to the commercial \"Auto-Scaling\"\nalgorithms, our easily implementable approach, behaves better even with sharp\nworkload fluctuations. This is confirmed by experiments on Amazon Web Services\n(AWS).\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 21:16:54 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Bekcheva", "Maria", ""], ["Fliess", "Michel", ""], ["Join", "C\u00e9dric", ""], ["Moradi", "Alireza", ""], ["Mounier", "Hugues", ""]]}, {"id": "1810.03723", "submitter": "Damien Imbs", "authors": "Zahra Aghazadeh, Damien Imbs, Michel Raynal, Gadi Taubenfeld, Philipp\n  Woelfel", "title": "Optimal Memory-Anonymous Symmetric Deadlock-Free Mutual Exclusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of an anonymous shared memory (recently introduced in PODC 2017)\nconsiders that processes use different names for the same memory location.\nHence, there is permanent disagreement on the location names among processes.\nIn this context, the PODC paper presented -among other results- a symmetric\ndeadlock-free mutual exclusion (mutex) algorithm for two processes and a\nnecessary condition on the size $m$ of the anonymous memory for the existence\nof a symmetric deadlock-free mutex algorithm in an $n$-process system. This\ncondition states that $m$ must be greater than $1$ and belong to the set\n$M(n)=\\{m:\\forall~\\ell:1<\\ell\\leq n:~\\gcd(\\ell,m)=1\\}$ (symmetric means that,\nwhile each process has its own identity, process identities can only be\ncompared with equality).\n  The present paper answers several open problems related to symmetric\ndeadlock-free mutual exclusion in an $n$-process system ($n\\geq 2$) where the\nprocesses communicate through $m$ registers. It first presents two algorithms.\nThe first considers that the registers are anonymous read/write atomic\nregisters and works for any $m$ greater than $1$ and belonging to the set\n$M(n)$. It thus shows that this condition on $m$ is both necessary and\nsufficient. The second algorithm considers anonymous read/modify/write atomic\nregisters. It assumes that $m\\in M(n)$. These algorithms differ in their design\nprinciples and their costs (measured as the number of registers which must\ncontain the identity of a process to allow it to enter the critical section).\nThe paper also shows that the condition $m\\in M(n)$ is necessary for\ndeadlock-free mutex on top of anonymous read/modify/write atomic registers. It\nfollows that, when $m>1$, $m\\in M(n)$ is a tight characterization of the size\nof the anonymous shared memory needed to solve deadlock-free mutex, be the\nanonymous registers read/write or read/modify/write.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 22:06:48 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Aghazadeh", "Zahra", ""], ["Imbs", "Damien", ""], ["Raynal", "Michel", ""], ["Taubenfeld", "Gadi", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1810.03931", "submitter": "Ferenc Heged\\H{u}s Dr.", "authors": "Ferenc Heged\\H{u}s", "title": "Modular, general purpose ODE integration package to solve large number\n  of independent ODE systems on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general purpose, modular program package for the integration of large\nnumber of independent ordinary differential equation systems capable of using\nprofessional graphics cards is presented. The available numerical schemes are\nthe explicit and adaptive Runge--Kutta--Cash--Karp algorithm and the explicit\nfourth order Runge--Kutta method with fixed time step. In order to harness the\nhuge processing power of graphics cards, the intermediate points of the\ncomputed trajectories are not stored. As a compensate, with pre-declared device\nfunctions, the required special features or properties of a solution can be\neasily extracted and stored each into a dedicated variable. For instance, the\nmaximum and minimum values and/or their time instances. Event handling is also\nincorporated into the package in order to detect special points which can be\nstored as well. Moreover, again with pre-declared device function calls at such\nspecial points, the efficient handling of non-smooth dynamics---e.g. impact\ndynamics---is possible. Several test cases are presented to demonstrate the\nflexibility of the pre-declared device functions and the strength of the\nprogram package. The applied models are the simple Duffing oscillator, the more\ncomplex Keller--Miksis equation known in bubble dynamics, and a system\ndescribing the behaviour of a pressure relief valve that can exhibit impact\ndynamics.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 12:07:25 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Heged\u0171s", "Ferenc", ""]]}, {"id": "1810.03955", "submitter": "Shaun D'Souza", "authors": "Shaun C. D'Souza, Hussain Boxwala, Sudharshan Reddy, Annapurna\n  Patcharla, Suman Mishra, Swaminathan Gopalakrishnan, Harsha Jawagal", "title": "Holistic generational offsets: Fostering a primitive online abstraction\n  for human vs. machine cognition", "comments": "11 pages, extended architecture details, added references. arXiv\n  admin note: text overlap with arXiv:1809.07794", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a unified architecture for next generation cognitive, low cost,\nmobile internet. The end user platform is able to scale as per the application\nand network requirements. It takes computing out of the data center and into\nend user platform. Internet enables open standards, accessible computing and\napplications programmability on a commodity platform. The architecture is a\nsuper-set to present day infrastructure web computing. The Java virtual machine\n(JVM) derives from the stack architecture. Applications can be developed and\ndeployed on a multitude of host platforms. O(1) <-> O(N). Computing and the\ninternet today are more accessible and available to the larger community.\nMachine learning has made extensive advances with the availability of modern\ncomputing. It is used widely in NLP, Computer Vision, Deep learning and AI. A\nprototype device for mobile could contain N compute and N MB of memory.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 18:11:41 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 18:01:55 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["D'Souza", "Shaun C.", ""], ["Boxwala", "Hussain", ""], ["Reddy", "Sudharshan", ""], ["Patcharla", "Annapurna", ""], ["Mishra", "Suman", ""], ["Gopalakrishnan", "Swaminathan", ""], ["Jawagal", "Harsha", ""]]}, {"id": "1810.04063", "submitter": "Vajira Thambawita", "authors": "Vajira Thambawita, Roshan G. Ragel, Dhammike Elkaduwe", "title": "To Use or Not to Use: CPUs' Cache Optimization Techniques on GPGPUs", "comments": "6 pages, 15 Figures", "journal-ref": "ICIAfS 2016- IEEE International Conference on Information and\n  Automation for Sustainability", "doi": "10.1109/ICIAFS.2016.7946534", "report-no": null, "categories": "cs.DC cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Purpose Graphic Processing Unit(GPGPU) is used widely for achieving\nhigh performance or high throughput in parallel programming. This capability of\nGPGPUs is very famous in the new era and mostly used for scientific computing\nwhich requires more processing power than normal personal computers. Therefore,\nmost of the programmers, researchers and industry use this new concept for\ntheir work. However, achieving high-performance or high-throughput using GPGPUs\nare not an easy task compared with conventional programming concepts in the CPU\nside. In this research, the CPU's cache memory optimization techniques have\nbeen adopted to the GPGPU's cache memory to identify rare performance\nimprovement techniques compared to GPGPU's best practices. The cache\noptimization techniques of blocking, loop fusion, array merging and array\ntranspose were tested on GPGPUs for finding suitability of these techniques.\nFinally, we identified that some of the CPU cache optimization techniques go\nwell with the cache memory system of the GPGPU and shows performance\nimprovements while some others show the opposite effect on the GPGPUs compared\nwith the CPUs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 15:10:33 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Thambawita", "Vajira", ""], ["Ragel", "Roshan G.", ""], ["Elkaduwe", "Dhammike", ""]]}, {"id": "1810.04110", "submitter": "Sergio Rivas-Gomez", "authors": "Sergio Rivas-Gomez, Roberto Gioiosa, Ivy Bo Peng, Gokcen Kestor, Sai\n  Narasimhamurthy, Erwin Laure, Stefano Markidis", "title": "MPI Windows on Storage for HPC Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upcoming HPC clusters will feature hybrid memories and storage devices per\ncompute node. In this work, we propose to use the MPI one-sided communication\nmodel and MPI windows as unique interface for programming memory and storage.\nWe describe the design and implementation of MPI storage windows, and present\nits benefits for out-of-core execution, parallel I/O and fault-tolerance. In\naddition, we explore the integration of heterogeneous window allocations, where\nmemory and storage share a unified virtual address space. When performing\nlarge, irregular memory operations, we verify that MPI windows on local storage\nincurs a 55% performance penalty on average. When using a Lustre parallel file\nsystem, asymmetric performance is observed with over 90% degradation in writing\noperations. Nonetheless, experimental results of a Distributed Hash Table, the\nHACC I/O kernel mini-application, and a novel MapReduce implementation based on\nthe use of MPI one-sided communication, indicate that the overall penalty of\nMPI windows on storage can be negligible in most cases in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:35:48 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Rivas-Gomez", "Sergio", ""], ["Gioiosa", "Roberto", ""], ["Peng", "Ivy Bo", ""], ["Kestor", "Gokcen", ""], ["Narasimhamurthy", "Sai", ""], ["Laure", "Erwin", ""], ["Markidis", "Stefano", ""]]}, {"id": "1810.04146", "submitter": "Sergio Rivas-Gomez", "authors": "Sergio Rivas-Gomez, Sai Narasimhamurthy, Keeran Brabazon, Oliver\n  Perks, Erwin Laure, Stefano Markidis", "title": "Decoupled Strategy for Imbalanced Workloads in MapReduce Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the integration of MPI one-sided communication and\nnon-blocking I/O in HPC-centric MapReduce frameworks. Using a decoupled\nstrategy, we aim to overlap the Map and Reduce phases of the algorithm by\nallowing processes to communicate and synchronize using solely one-sided\noperations. Hence, we effectively increase the performance in situations where\nthe workload per process is unexpectedly unbalanced. Using a Word-Count\nimplementation and a large dataset from the Purdue MapReduce Benchmarks Suite\n(PUMA), we demonstrate that our approach can provide up to 23% performance\nimprovement on average compared to a reference MapReduce implementation that\nuses state-of-the-art MPI collective communication and I/O.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:26:26 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Rivas-Gomez", "Sergio", ""], ["Narasimhamurthy", "Sai", ""], ["Brabazon", "Keeran", ""], ["Perks", "Oliver", ""], ["Laure", "Erwin", ""], ["Markidis", "Stefano", ""]]}, {"id": "1810.04150", "submitter": "Sergio Rivas-Gomez", "authors": "Sergio Rivas-Gomez, Antonio J. Pe\\~na, David Moloney, Erwin Laure,\n  Stefano Markidis", "title": "Exploring the Vision Processing Unit as Co-processor for Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of the exascale supercomputer is largely debated to remain\ndependent on novel breakthroughs in technology that effectively reduce the\npower consumption and thermal dissipation requirements. In this work, we\nconsider the integration of co-processors in high-performance computing (HPC)\nto enable low-power, seamless computation offloading of certain operations. In\nparticular, we explore the so-called Vision Processing Unit (VPU), a\nhighly-parallel vector processor with a power envelope of less than 1W. We\nevaluate this chip during inference using a pre-trained GoogLeNet convolutional\nnetwork model and a large image dataset from the ImageNet ILSVRC challenge.\nPreliminary results indicate that a multi-VPU configuration provides similar\nperformance compared to reference CPU and GPU implementations, while reducing\nthe thermal-design power (TDP) up to 8x in comparison.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 17:37:14 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Rivas-Gomez", "Sergio", ""], ["Pe\u00f1a", "Antonio J.", ""], ["Moloney", "David", ""], ["Laure", "Erwin", ""], ["Markidis", "Stefano", ""]]}, {"id": "1810.04201", "submitter": "Grzegorz Korcyl", "authors": "Grzegorz Korcyl and Piotr Korcyl", "title": "Towards Lattice Quantum Chromodynamics on FPGA devices", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2019.107029", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a single-node, double precision Field Programmable\nGate Array (FPGA) implementation of the Conjugate Gradient algorithm in the\ncontext of Lattice Quantum Chromodynamics. As a benchmark of our proposal we\ninvert numerically the Dirac-Wilson operator on a 4-dimensional grid on three\nXilinx hardware solutions: Zynq Ultrascale+ evaluation board, the Alveo U250\naccelerator and the largest device available on the market, the VU13P device.\nIn our implementation we separate software/hardware parts in such a way that\nthe entire multiplication by the Dirac operator is performed in hardware, and\nthe rest of the algorithm runs on the host. We find out that the FPGA\nimplementation can offer a performance comparable with that obtained using\ncurrent CPU or Intel's many core Xeon Phi accelerators. A possible multiple\nnode FPGA-based system is discussed and we argue that power-efficient High\nPerformance Computing (HPC) systems can be implemented using FPGA devices only.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 13:03:16 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 07:16:09 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Korcyl", "Grzegorz", ""], ["Korcyl", "Piotr", ""]]}, {"id": "1810.04254", "submitter": "Tharun Kumar Reddy Medini", "authors": "Qixuan Huang, Yiqiu Wang, Tharun Medini, Anshumali Shrivastava", "title": "Extreme Classification in Log Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Merged-Averaged Classifiers via Hashing (MACH) for\nK-classification with ultra-large values of K. Compared to traditional\none-vs-all classifiers that require O(Kd) memory and inference cost, MACH only\nneed O(d log K) (d is dimensionality )memory while only requiring O(K log K + d\nlog K) operation for inference. MACH is a generic K-classification algorithm,\nwith provably theoretical guarantees, which requires O(log K) memory without\nany assumption on the relationship between classes. MACH uses universal hashing\nto reduce classification with a large number of classes to few independent\nclassification tasks with small (constant) number of classes. We provide\ntheoretical quantification of discriminability-memory tradeoff. With MACH we\ncan train ODP dataset with 100,000 classes and 400,000 features on a single\nTitan X GPU, with the classification accuracy of 19.28%, which is the\nbest-reported accuracy on this dataset. Before this work, the best performing\nbaseline is a one-vs-all classifier that requires 40 billion parameters (160 GB\nmodel size) and achieves 9% accuracy. In contrast, MACH can achieve 9% accuracy\nwith 480x reduction in the model size (of mere 0.3GB). With MACH, we also\ndemonstrate complete training of fine-grained imagenet dataset (compressed size\n104GB), with 21,000 classes, on a single GPU. To the best of our knowledge,\nthis is the first work to demonstrate complete training of these extreme-class\ndatasets on a single Titan X.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 21:43:57 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Huang", "Qixuan", ""], ["Wang", "Yiqiu", ""], ["Medini", "Tharun", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1810.04289", "submitter": "Fande Kong", "authors": "Fande Kong, Vitaly Kheyfets, Ender Finol, and Xiao-Chuan Cai", "title": "Simulation of unsteady blood flows in a patient-specific compliant\n  pulmonary artery with a highly parallel monolithically coupled\n  fluid-structure interaction algorithm", "comments": "24 pages, 15 figures. Submitted to International Journal of Numerical\n  Methods in Biomedical Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational fluid dynamics (CFD) is increasingly used to study blood flows\nin patient-specific arteries for understanding certain cardiovascular diseases.\nThe techniques work quite well for relatively simple problems, but need\nimprovements when the problems become harder in the case when (1) the geometry\nbecomes complex (from a few branches to a full pulmonary artery), (2) the model\nbecomes more complex (from fluid-only calculation to coupled fluid-structure\ninteraction calculation), (3) both the fluid and wall models become highly\nnonlinear, and (4) the computer on which we run the simulation is a\nsupercomputer with tens of thousands of processor cores. To push the limit of\nCFD in all four fronts, in this paper, we develop and study a highly parallel\nalgorithm for solving a monolithically coupled fluid-structure system for the\nmodeling of the interaction of the blood flow and the arterial wall. As a case\nstudy, we consider a patient-specific, full size pulmonary artery obtained from\nCT (Computed Tomography) images, with an artificially added layer of wall with\na fixed thickness. The fluid is modeled with a system of incompressible\nNavier-Stokes equations and the wall is modeled by a geometrically nonlinear\nelasticity equation. As far as we know this is the first time the unsteady\nblood flow in a full pulmonary artery is simulated without assuming a rigid\nwall. The proposed numerical algorithm and software scale well beyond 10,000\nprocessor cores on a supercomputer for solving the fluid-structure interaction\nproblem discretized with a stabilized finite element method in space and an\nimplicit scheme in time involving hundreds of millions of unknowns.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 22:47:42 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Kong", "Fande", ""], ["Kheyfets", "Vitaly", ""], ["Finol", "Ender", ""], ["Cai", "Xiao-Chuan", ""]]}, {"id": "1810.04329", "submitter": "Muhammad Hilman", "authors": "Muhammad H. Hilman and Maria A. Rodriguez and Rajkumar Buyya", "title": "Task Runtime Prediction in Scientific Workflows Using an Online\n  Incremental Learning Approach", "comments": "Accepted for presentation at main conference track of 11th IEEE/ACM\n  International Conference on Utility and Cloud Computing", "journal-ref": null, "doi": "10.1109/UCC.2018.00018", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms in workflow scheduling and resource provisioning rely on the\nperformance estimation of tasks to produce a scheduling plan. A profiler that\nis capable of modeling the execution of tasks and predicting their runtime\naccurately, therefore, becomes an essential part of any Workflow Management\nSystem (WMS). With the emergence of multi-tenant Workflow as a Service (WaaS)\nplatforms that use clouds for deploying scientific workflows, task runtime\nprediction becomes more challenging because it requires the processing of a\nsignificant amount of data in a near real-time scenario while dealing with the\nperformance variability of cloud resources. Hence, relying on methods such as\nprofiling tasks' execution data using basic statistical description (e.g.,\nmean, standard deviation) or batch offline regression techniques to estimate\nthe runtime may not be suitable for such environments. In this paper, we\npropose an online incremental learning approach to predict the runtime of tasks\nin scientific workflows in clouds. To improve the performance of the\npredictions, we harness fine-grained resources monitoring data in the form of\ntime-series records of CPU utilization, memory usage, and I/O activities that\nare reflecting the unique characteristics of a task's execution. We compare our\nsolution to a state-of-the-art approach that exploits the resources monitoring\ndata based on regression machine learning technique. From our experiments, the\nproposed strategy improves the performance, in terms of the error, up to\n29.89%, compared to the state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 01:59:08 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Hilman", "Muhammad H.", ""], ["Rodriguez", "Maria A.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1810.04334", "submitter": "Peng Sun", "authors": "Peng Sun, Yonggang Wen, Ta Nguyen Binh Duong, Xiaokui Xiao", "title": "GraphMP: I/O-Efficient Big Graph Analytics on a Single Commodity Machine", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.02557", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies showed that single-machine graph processing systems can be as\nhighly competitive as cluster-based approaches on large-scale problems. While\nseveral out-of-core graph processing systems and computation models have been\nproposed, the high disk I/O overhead could significantly reduce performance in\nmany practical cases. In this paper, we propose GraphMP to tackle big graph\nanalytics on a single machine. GraphMP achieves low disk I/O overhead with\nthree techniques. First, we design a vertex-centric sliding window (VSW)\ncomputation model to avoid reading and writing vertices on disk. Second, we\npropose a selective scheduling method to skip loading and processing\nunnecessary edge shards on disk. Third, we use a compressed edge cache\nmechanism to fully utilize the available memory of a machine to reduce the\namount of disk accesses for edges. Extensive evaluations have shown that\nGraphMP could outperform existing single-machine out-of-core systems such as\nGraphChi, X-Stream and GridGraph by up to 51, and can be as highly competitive\nas distributed graph engines like Pregel+, PowerGraph and Chaos.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 05:50:52 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 07:40:14 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sun", "Peng", ""], ["Wen", "Yonggang", ""], ["Duong", "Ta Nguyen Binh", ""], ["Xiao", "Xiaokui", ""]]}, {"id": "1810.04597", "submitter": "Luigi Iapichino", "authors": "Matteo Bugli (CEA Saclay), Luigi Iapichino (LRZ) and Fabio Baruffa\n  (Intel)", "title": "ECHO-3DHPC: Advance the performance of astrophysics simulations with\n  code modernization", "comments": "7 pages, 6 figures. Accepted for publication on The Parallel Universe\n  Magazine ( https://software.intel.com/en-us/parallel-universe-magazine )", "journal-ref": "Parallel Universe Magazine 34 (2018), 49", "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present recent developments in the parallelization scheme of ECHO-3DHPC,\nan efficient astrophysical code used in the modelling of relativistic plasmas.\nWith the help of the Intel Software Development Tools, like Fortran compiler\nand Profile-Guided Optimization (PGO), Intel MPI library, VTune Amplifier and\nInspector we have investigated the performance issues and improved the\napplication scalability and the time to solution. The node-level performance is\nimproved by $2.3 \\times$ and, thanks to the improved threading parallelisation,\nthe hybrid MPI-OpenMP version of the code outperforms the MPI-only, thus\nlowering the MPI communication overhead.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:40:03 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Bugli", "Matteo", "", "CEA Saclay"], ["Iapichino", "Luigi", "", "LRZ"], ["Baruffa", "Fabio", "", "Intel"]]}, {"id": "1810.04608", "submitter": "Nan Wang", "authors": "Nan Wang, Michail Matthaiou, Dimitrios S. Nikolopoulos and Blesson\n  Varghese", "title": "DYVERSE: DYnamic VERtical Scaling in Multi-tenant Edge Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-tenancy in resource-constrained environments is a key challenge in Edge\ncomputing. In this paper, we develop 'DYVERSE: DYnamic VERtical Scaling in\nEdge' environments, which is the first light-weight and dynamic vertical\nscaling mechanism for managing resources allocated to applications for\nfacilitating multi-tenancy in Edge environments. To enable dynamic vertical\nscaling, one static and three dynamic priority management approaches that are\nworkload-aware, community-aware and system-aware, respectively are proposed.\nThis research advocates that dynamic vertical scaling and priority management\napproaches reduce Service Level Objective (SLO) violation rates. An online-game\nand a face detection workload in a Cloud-Edge test-bed are used to validate the\nresearch. The merits of DYVERSE is that there is only a sub-second overhead per\nEdge server when 32 Edge servers are deployed on a single Edge node. When\ncompared to executing applications on the Edge servers without dynamic vertical\nscaling, static priorities and dynamic priorities reduce SLO violation rates of\nrequests by up to 4% and 12% for the online game, respectively, and in both\ncases 6% for the face detection workload. Moreover, for both workloads, the\nsystem-aware dynamic vertical scaling method effectively reduces the latency of\nnon-violated requests, when compared to other methods.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 15:38:48 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 09:29:05 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Wang", "Nan", ""], ["Matthaiou", "Michail", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Varghese", "Blesson", ""]]}, {"id": "1810.04609", "submitter": "Muhammad Hunain Memon", "authors": "Syeda Munazza Marium, Liaquat Ali Thebo, Syed Naveed Ahmed jaffari and\n  Muhammad Hunain Memon", "title": "Time Efficient Data Migration among Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is one of the chief requirement of modern IT trade. Today's\ncloud industry progressively dependent on it, which lead mutually abundant\nsolutions and challenges. Among the numerous challenges of cloud computing,\ncloud migration is one of the major concern, and it is necessity to design\noptimize solutions to advance it with time. Data migration researcher attempt\nto move data concerning various geographical locations, which contain huge data\nvolumes, compact time limit and problematical architectures. Researchers aim to\ntransfer data with minimal transmission cost and used various efficient\nscheduling methods and other techniques to achieve this objective. In former\nresearch struggles, numerous solution have proposed. In our proposed work, we\nhave explored the contextual factor to accomplish shorter transmission time.\nEntity Framework Core technology is utilizing for conceptual modelling, mapping\nand sortage modelling. Meant for minimum transmission cost Object Related\nMapping is designated. Desired objective to achieve time efficiency during data\nmigration has been accomplished. Results obtained when data transmission occur\namong azure and gear host cloud implementation of proposed framework with some\nsize limitations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 05:51:36 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Marium", "Syeda Munazza", ""], ["Thebo", "Liaquat Ali", ""], ["jaffari", "Syed Naveed Ahmed", ""], ["Memon", "Muhammad Hunain", ""]]}, {"id": "1810.04718", "submitter": "Seyedakbar Mostafavi", "authors": "Seyedakbar Mostafavi, Vesal Hakami", "title": "A Stochastic Approximation Approach for Foresighted Task Scheduling in\n  Cloud Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing and elastic demand for cloud resources, finding an\noptimal task scheduling mechanism become a challenge for cloud service\nproviders. Due to the time-varying nature of resource demands in length and\nprocessing over time and dynamics and heterogeneity of cloud resources,\nexisting myopic task scheduling solutions intended to maximize the performance\nof task scheduling are inefficient and sacrifice the long-time system\nperformance in terms of resource utilization and response time. In this paper,\nwe propose an optimal solution for performing foresighted task scheduling in a\ncloud environment. Since a-priori knowledge from the dynamics in queue length\nof virtual machines is not known in run time, an online reinforcement learning\napproach is proposed for foresighted task allocation. The evaluation results\nshow that our method not only reduce the response time and makespan of\nsubmitted tasks, but also increase the resource efficiency. So in this thesis a\nscheduling method based on reinforcement learning is proposed. Adopting with\nenvironment conditions and responding to unsteady requests, reinforcement\nlearning can cause a long-term increase in system's performance. The results\nshow that this proposed method can not only reduce the response time and\nmakespan but also increase resource efficiency as a minor goal.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 19:20:36 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 05:57:36 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mostafavi", "Seyedakbar", ""], ["Hakami", "Vesal", ""]]}, {"id": "1810.04758", "submitter": "Michael Gowanlock", "authors": "Michael Gowanlock", "title": "Hybrid KNN-Join: Parallel Nearest Neighbor Searches Exploiting CPU and\n  GPU Architectural Features", "comments": "22 pages, 20 figures, 9 tables. Accepted for publication in Journal\n  of Parallel and Distributed Computing (JPDC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K Nearest Neighbor (KNN) joins are used in scientific domains for data\nanalysis, and are building blocks of several well-known algorithms. KNN-joins\nfind the KNN of all points in a dataset. This paper focuses on a hybrid CPU/GPU\napproach for low-dimensional KNN-joins, where the GPU may not yield substantial\nperformance gains over parallel CPU algorithms. We utilize a work queue that\nprioritizes computing data points in high density regions on the GPU, and low\ndensity regions on the CPU, thereby taking advantage of each architecture's\nrelative strengths. Our approach, HybridKNN-Join, effectively augments a\nstate-of-the-art multi-core CPU algorithm. We propose optimizations that $(i)$\nmaximize GPU query throughput by assigning the GPU large batches of work;\n$(ii)$ increase workload granularity to optimize GPU utilization; and, $(iii)$\nlimit load imbalance between CPU and GPU architectures. We compare\nHybridKNN-Join to one GPU and two parallel CPU reference implementations.\nCompared to the reference implementations, we find that the hybrid algorithm\nperforms best on larger workloads (dataset size and K). The methods employed in\nthis paper show promise for the general division of work in other hybrid\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 21:55:15 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 22:07:03 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gowanlock", "Michael", ""]]}, {"id": "1810.04822", "submitter": "Dmitry Kolomenskiy", "authors": "Dmitry Kolomenskiy, Ryo Onishi and Hitoshi Uehara", "title": "WaveRange: Wavelet-based data compression for three-dimensional\n  numerical simulations on regular grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC physics.ao-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wavelet-based method for compression of three-dimensional simulation data\nis presented and its software framework is described. It uses wavelet\ndecomposition and subsequent range coding with quantization suitable for\nfloating-point data. The effectiveness of this method is demonstrated by\napplying it to example numerical tests, ranging from idealized configurations\nto realistic global-scale simulations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 02:33:04 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 06:05:38 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Kolomenskiy", "Dmitry", ""], ["Onishi", "Ryo", ""], ["Uehara", "Hitoshi", ""]]}, {"id": "1810.04831", "submitter": "Seyedakbar Mostafavi", "authors": "Seyedakbar Mostafavi, Vesal Hakami", "title": "A new rank-order clustering algorithm for prolonging the lifetime of\n  wireless sensor networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy efficient resource management is critical for prolonging the lifetime\nof wireless sensor networks (WSN). Clustering of sensor nodes with the aim of\ndistributing the traffic loads in the network is a proven approach for balanced\nenergy consumption in WSN. The main body of literature in this topic can be\nclassified as hierarchical and distance-based clustering techniques in which\nmulti-hop, multi-level forwarding and distance-based criteria, respectively,\nare utilized for categorization of sensor nodes. In this study, we propose the\nApproximate Rank-Order Wireless Sensor Networks (ARO-WSN) clustering algorithm\nas a combined hierarchical and distance-based clustering approach. ARO-WSN\nalgorithm which has been extensively used in the field of image processing,\nruns in the order of O(n) for a large data set, therefore it can be applied on\nWSN. The results shows that ARO-WSN outperforms the classical LEACH, LEACH-C\nand K-means clustering algorithms in the terms of energy consumption and\nnetwork lifetime.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 03:26:15 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 11:40:46 GMT"}, {"version": "v3", "created": "Fri, 4 Oct 2019 08:19:39 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mostafavi", "Seyedakbar", ""], ["Hakami", "Vesal", ""]]}, {"id": "1810.05088", "submitter": "Noman Islam Dr.", "authors": "Noman Islam, Zeeshan Islam", "title": "An economic perspective on major cloud computing providers", "comments": null, "journal-ref": "Journal of Information and Communication Technology, 2018", "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing can be defined as the outsourcing / renting of resources over\nthe network. It has been regarded as an essential utility similar to\nelectricity, water, telephone and sewerage utilities. Over the years, a number\nof providers have emerged that provide basic and advanced form of services for\ncloud computing. However, these providers employ their own nomenclature for\nmarketing their specification and the range of services offered by them. This\nmakes it very difficult for a common user to select a platform based on their\nrequirements. This paper provides a comparison of different cloud service\nproviders from an economic perspective. It performs the cost analysis of\nservices provided by providers for different scenarios. Based on the analysis,\nconclusions have been drawn and directions for further research have been\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 14:16:51 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Islam", "Noman", ""], ["Islam", "Zeeshan", ""]]}, {"id": "1810.05256", "submitter": "Micha{\\l} \\'Swi\\k{e}tek Ms", "authors": "Adam G\\k{a}gol and Micha{\\l} \\'Swi\\k{e}tek", "title": "Aleph: A Leaderless, Asynchronous, Byzantine Fault Tolerant Consensus\n  Protocol", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose Aleph, a leaderless, fully asynchronous, Byzantine\nfault tolerant consensus protocol for ordering messages exchanged among\nprocesses. It is based on a distributed construction of a partially ordered set\nand the algorithm for reaching a consensus on its extension to a total order.\nTo achieve the consensus, the processes perform computations based only on a\nlocal copy of the data structure, however, they are bound to end with the same\nresults. Our algorithm uses a dual-threshold coin-tossing scheme as a\nrandomization strategy and establishes the agreement in an expected constant\nnumber of rounds. In addition, we introduce a fast way of validating messages\nthat can occur prior to determining the total ordering.\n  This version of the protocol is deprecated. For current version see\narXiv:1908.05156.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 21:27:45 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 16:51:01 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["G\u0105gol", "Adam", ""], ["\u015awi\u0119tek", "Micha\u0142", ""]]}, {"id": "1810.05291", "submitter": "Jeremy Bernstein", "authors": "Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, Anima\n  Anandkumar", "title": "signSGD with Majority Vote is Communication Efficient And Fault Tolerant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks on large datasets can be accelerated by distributing\nthe workload over a network of machines. As datasets grow ever larger, networks\nof hundreds or thousands of machines become economically viable. The time cost\nof communicating gradients limits the effectiveness of using such large machine\ncounts, as may the increased chance of network faults. We explore a\nparticularly simple algorithm for robust, communication-efficient\nlearning---signSGD. Workers transmit only the sign of their gradient vector to\na server, and the overall update is decided by a majority vote. This algorithm\nuses $32\\times$ less communication per iteration than full-precision,\ndistributed SGD. Under natural conditions verified by experiment, we prove that\nsignSGD converges in the large and mini-batch settings, establishing\nconvergence for a parameter regime of Adam as a byproduct. Aggregating sign\ngradients by majority vote means that no individual worker has too much power.\nWe prove that unlike SGD, majority vote is robust when up to 50% of workers\nbehave adversarially. The class of adversaries we consider includes as special\ncases those that invert or randomise their gradient estimate. On the practical\nside, we built our distributed training system in Pytorch. Benchmarking against\nthe state of the art collective communications library (NCCL), our\nframework---with the parameter server housed entirely on one machine---led to a\n25% reduction in time for training resnet50 on Imagenet when using 15 AWS\np3.2xlarge machines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 23:50:32 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 02:53:43 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2019 19:55:48 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Bernstein", "Jeremy", ""], ["Zhao", "Jiawei", ""], ["Azizzadenesheli", "Kamyar", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1810.05365", "submitter": "Wei Cai", "authors": "Wei Cai, Zehua Wang, Jason B. Ernst, Zhen Hong, Chen Feng, Victor C.M.\n  Leung", "title": "Decentralized Applications: The Blockchain-Empowered Software System", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2870644", "report-no": null, "categories": "cs.DC cs.CR cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Blockchain technology has attracted tremendous attention in both academia and\ncapital market. However, overwhelming speculations on thousands of available\ncryptocurrencies and numerous initial coin offering (ICO) scams have also\nbrought notorious debates on this emerging technology. This paper traces the\ndevelopment of blockchain systems to reveal the importance of decentralized\napplications (dApps) and the future value of blockchain. We survey the\nstate-of-the-art dApps and discuss the direction of blockchain development to\nfulfill the desirable characteristics of dApps. The readers will gain an\noverview of dApp research and get familiar with recent developments in the\nblockchain.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 05:41:31 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Cai", "Wei", ""], ["Wang", "Zehua", ""], ["Ernst", "Jason B.", ""], ["Hong", "Zhen", ""], ["Feng", "Chen", ""], ["Leung", "Victor C. M.", ""]]}, {"id": "1810.05453", "submitter": "Jannik Castenow", "authors": "Jannik Castenow, Christina Kolb and Christian Scheideler", "title": "A Bounding Box Overlay for Competitive Routing in Hybrid Communication\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new approach for competitive geometric routing in\nwireless ad hoc networks. In general, it is well-known that any online routing\nstrategy performs very poor in the worst case. The main difficulty are\nuncovered regions within the wireless ad hoc network, which we denote as radio\nholes. Complex shapes of radio holes, for example zig-zag-shapes, make local\ngeometric routing even more difficult, i.e., forwarded messages in direction to\nthe destination might get stuck in a dead end or are routed along very long\ndetours, when there is no knowledge about the ad hoc network. To obtain\nknowledge about the position and shape of radio holes, we make use of a hybrid\nnetwork approach. This approach assumes that we can not just make use of the ad\nhoc network but also of some cellular infrastructure, which is used to gather\nknowledge about the underlying ad hoc network. Communication via the cellular\ninfrastructure incurs costs as cell phone providers are involved. Therefore, we\nuse the cellular infrastructure only to compute routing paths in the ad hoc\nnetwork. The actual data transmission takes place in the ad hoc network. In\norder to find good routing paths we aim at computing an abstraction of the ad\nhoc network in which radio holes are abstracted by bounding boxes. The\nadvantage of bounding boxes as hole abstraction is that we only have to\nconsider a constant number of nodes per hole. We prove that bounding boxes are\na suitable hole abstraction that allows us to find $c$-competitive paths in the\nad hoc network in case of non-intersecting bounding boxes. In case of\nintersecting bounding boxes, we show via simulations that our routing strategy\nsignificantly outperforms the so far best online routing strategies for\nwireless ad hoc networks. Finally, we also present a routing strategy that is\n$c$-competitive in case of pairwise intersecting bounding boxes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 11:29:05 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 15:31:10 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 09:52:12 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Castenow", "Jannik", ""], ["Kolb", "Christina", ""], ["Scheideler", "Christian", ""]]}, {"id": "1810.05502", "submitter": "Deepa R", "authors": "Devipriya T K, Jovita Franci A, Deepa R, Godwin Sam Josh", "title": "Asynchronous Wi-Fi Control Interface (AWCI) Using Socket IO Technology", "comments": "5 pages, 5 figures, published with Global Research and Development\n  Journal for Engineering", "journal-ref": "Global Research and Development Journal for Engineering, 1(3),\n  pp.66-70, 2017", "doi": null, "report-no": null, "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is a system of interrelated computing devices to\nthe Internet that are provided with unique identifiers which has the ability to\ntransfer data over a network without requiring human-to- human or human-to-\ncomputer interaction. Raspberry pi-3 a popular, cheap, small and powerful\ncomputer with built in Wi-Fi can be used to make any devices smart by\nconnecting to that particular device and embedding the required software to\nRaspberry pi-3 and connect it to Internet. It is difficult to install a full\nLinux OS inside a small devices like light switch so in that case to connect to\na Wi-Fi connection a model was proposed known as Asynchronous Wi-Fi Control\nInterface (AWCI) which is a simple Wi-Fi connectivity software for a Debian\ncompatible Linux OS). The objective of this paper is to make the interactive\nuser interface for Wi-Fi connection in Raspberry Pi touch display by providing\nlive updates using Socket IO technology. The Socket IO technology enables\nreal-time bidirectional communication between client and server. Asynchronous\nWi-Fi Control Interface (AWCI) is compatible with every platform, browser or\ndevice.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2018 15:38:17 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["K", "Devipriya T", ""], ["A", "Jovita Franci", ""], ["R", "Deepa", ""], ["Josh", "Godwin Sam", ""]]}, {"id": "1810.05871", "submitter": "Xiong Zheng", "authors": "Xiong Zheng, Vijay K. Garg, John Kaippallimalil", "title": "Linearizable Replicated State Machines with Lattice Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the lattice agreement problem in asynchronous systems and\nexplores its application to building linearizable replicated state machines\n(RSM). First, we propose an algorithm to solve the lattice agreement problem in\n$O(\\log f)$ asynchronous rounds, where $f$ is the number of crash failures that\nthe system can tolerate. This is an exponential improvement over the previous\nbest upper bound. Second, Faleiro et al have shown in [Faleiro et al. PODC,\n2012] that combination of conflict-free data types and lattice agreement\nprotocols can be applied to implement linearizable RSM. They give a Paxos style\nlattice agreement protocol, which can be adapted to implement linearizable RSM\nand guarantee that a command can be learned in at most $O(n)$ message delays,\nwhere $n$ is the number of proposers. Later on, Xiong et al in [Xiong et al.\nDISC, 2018] give a lattice agreement protocol which improves the $O(n)$\nguarantee to be $O(f)$. However, neither protocols is practical for building a\nlinearizable RSM. Thus, in the second part of the paper, we first give an\nimproved protocol based on the one proposed by Xiong et al. Then, we implement\na simple linearizable RSM using the our improved protocol and compare our\nimplementation with an open source Java implementation of Paxos. Results show\nthat better performance can be obtained by using lattice agreement based\nprotocols to implement a linearizable RSM compared to traditional consensus\nbased protocols.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 15:06:52 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Zheng", "Xiong", ""], ["Garg", "Vijay K.", ""], ["Kaippallimalil", "John", ""]]}, {"id": "1810.05937", "submitter": "Ellis Solaiman", "authors": "Awatif Alqahtani and Yinhao Li and Pankesh Patel and Ellis Solaiman\n  and Rajiv Ranjan", "title": "End-to-End Service Level Agreement Specification for IoT Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) promises to help solve a wide range of issues\nthat relate to our wellbeing within application domains that include smart\ncities, healthcare monitoring, and environmental monitoring. IoT is bringing\nnew wireless sensor use cases by taking advantage of the computing power and\nflexibility provided by Edge and Cloud Computing. However, the software and\nhardware resources used within such applications must perform correctly and\noptimally. Especially in applications where a failure of resources can be\ncritical. Service Level Agreements (SLA) where the performance requirements of\nsuch applications are defined, need to be specified in a standard way that\nreflects the end-to-end nature of IoT application domains, accounting for the\nQuality of Service (QoS) metrics within every layer including the Edge, Network\nGateways, and Cloud. In this paper, we propose a conceptual model that captures\nthe key entities of an SLA and their relationships, as a prior step for\nend-to-end SLA specification and composition. Service level objective (SLO)\nterms are also considered to express the QoS constraints. Moreover, we propose\na new SLA grammar which considers workflow activities and the multi-layered\nnature of IoT applications. Accordingly, we develop a tool for SLA\nspecification and composition that can be used as a template to generate SLAs\nin a machine-readable format. We demonstrate the effectiveness of the proposed\nspecification language through a literature survey that includes an SLA\nlanguage comparison analysis, and via reflecting the user satisfaction results\nof a usability study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Oct 2018 22:49:40 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Alqahtani", "Awatif", ""], ["Li", "Yinhao", ""], ["Patel", "Pankesh", ""], ["Solaiman", "Ellis", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1810.06046", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Carlos Reano and Federico Silla", "title": "Accelerator Virtualization in Fog Computing: Moving From the Cloud to\n  the Edge", "comments": "IEEE Cloud Computing magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware accelerators are available on the Cloud for enhanced analytics. Next\ngeneration Clouds aim to bring enhanced analytics using accelerators closer to\nuser devices at the edge of the network for improving Quality-of-Service by\nminimizing end-to-end latencies and response times. The collective computing\nmodel that utilizes resources at the Cloud-Edge continuum in a multi-tier\nhierarchy comprising the Cloud, the Edge and user devices is referred to as Fog\ncomputing. This article identifies challenges and opportunities in making\naccelerators accessible at the Edge. A holistic view of the Fog architecture is\nkey to pursuing meaningful research in this area.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 15:30:55 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Varghese", "Blesson", ""], ["Reano", "Carlos", ""], ["Silla", "Federico", ""]]}, {"id": "1810.06115", "submitter": "Yunseong Lee", "authors": "Yunseong Lee, Alberto Scolari, Byung-Gon Chun, Marco Domenico\n  Santambrogio, Markus Weimer, Matteo Interlandi", "title": "PRETZEL: Opening the Black Box of Machine Learning Prediction Serving\n  Systems", "comments": "16 pages, 14 figures, 13th USENIX Symposium on Operating Systems\n  Design and Implementation (OSDI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning models are often composed of pipelines of transformations.\nWhile this design allows to efficiently execute single model components at\ntraining time, prediction serving has different requirements such as low\nlatency, high throughput and graceful performance degradation under heavy load.\nCurrent prediction serving systems consider models as black boxes, whereby\nprediction-time-specific optimizations are ignored in favor of ease of\ndeployment. In this paper, we present PRETZEL, a prediction serving system\nintroducing a novel white box architecture enabling both end-to-end and\nmulti-model optimizations. Using production-like model pipelines, our\nexperiments show that PRETZEL is able to introduce performance improvements\nover different dimensions; compared to state-of-the-art approaches PRETZEL is\non average able to reduce 99th percentile latency by 5.5x while reducing memory\nfootprint by 25x, and increasing throughput by 4.7x.\n", "versions": [{"version": "v1", "created": "Sun, 14 Oct 2018 22:21:30 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Lee", "Yunseong", ""], ["Scolari", "Alberto", ""], ["Chun", "Byung-Gon", ""], ["Santambrogio", "Marco Domenico", ""], ["Weimer", "Markus", ""], ["Interlandi", "Matteo", ""]]}, {"id": "1810.06361", "submitter": "Amrith Setlur", "authors": "S. Jaya Nirmala, Amrith Rajagopal Setlur, Har Simrat Singh, Sudhanshu\n  Khoriya", "title": "An Efficient Fault Tolerant Workflow Scheduling Approach using\n  Replication Heuristics and Checkpointing in the Cloud", "comments": "35 pages, 9 figures", "journal-ref": "Journal of Parallel and Distributed Computing 2020", "doi": "10.1016/j.jpdc.2019.09.004", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific workflows have been predominantly used for complex and large scale\ndata analysis and scientific computation/automation and the need for robust\nworkflow scheduling techniques has grown considerably. But, most of the\nexisting workflow scheduling algorithms do not provide the required reliability\nand robustness. In this paper, a new fault tolerant workflow scheduling\nalgorithm that learns replication heuristics in an unsupervised manner has been\nproposed. Furthermore, the use of light weight synchronized checkpointing\nenables efficient resubmission of failed tasks and ensures workflow completion\neven in precarious environments. The proposed technique improves upon metrics\nlike Resource Wastage and Resource Usage in comparison to the Replicate-All\nalgorithm, while maintaining an acceptable increase in Makespan as compared to\nthe vanilla Heterogeneous Earliest Finish Time (HEFT).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 14:12:09 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 19:14:32 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Nirmala", "S. Jaya", ""], ["Setlur", "Amrith Rajagopal", ""], ["Singh", "Har Simrat", ""], ["Khoriya", "Sudhanshu", ""]]}, {"id": "1810.06653", "submitter": "Shi Pu", "authors": "Shi Pu, Wei Shi, Jinming Xu, Angelia Nedi\\'c", "title": "Push-Pull Gradient Methods for Distributed Optimization in Networks", "comments": "Parts of the results appear in Proceedings of the 57th IEEE\n  Conference on Decision and Control (see arXiv:1803.07588)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on solving a distributed convex optimization problem\nin a network, where each agent has its own convex cost function and the goal is\nto minimize the sum of the agents' cost functions while obeying the network\nconnectivity structure. In order to minimize the sum of the cost functions, we\nconsider new distributed gradient-based methods where each node maintains two\nestimates, namely, an estimate of the optimal decision variable and an estimate\nof the gradient for the average of the agents' objective functions. From the\nviewpoint of an agent, the information about the gradients is pushed to the\nneighbors, while the information about the decision variable is pulled from the\nneighbors hence giving the name \"push-pull gradient methods\". The methods\nutilize two different graphs for the information exchange among agents, and as\nsuch, unify the algorithms with different types of distributed architecture,\nincluding decentralized (peer-to-peer), centralized (master-slave), and\nsemi-centralized (leader-follower) architecture. We show that the proposed\nalgorithms and their many variants converge linearly for strongly convex and\nsmooth objective functions over a network (possibly with unidirectional data\nlinks) in both synchronous and asynchronous random-gossip settings. In\nparticular, under the random-gossip setting, \"push-pull\" is the first class of\nalgorithms for distributed optimization over directed graphs. Moreover, we\nnumerically evaluate our proposed algorithms in both scenarios, and show that\nthey outperform other existing linearly convergent schemes, especially for\nill-conditioned problems and networks that are not well balanced.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 23:58:51 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 16:16:38 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 16:49:30 GMT"}, {"version": "v4", "created": "Thu, 6 Feb 2020 06:07:43 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pu", "Shi", ""], ["Shi", "Wei", ""], ["Xu", "Jinming", ""], ["Nedi\u0107", "Angelia", ""]]}, {"id": "1810.06659", "submitter": "Yifan Wang", "authors": "Yifan Wang, Shaoshan Liu, Xiaopei Wu, Weisong Shi", "title": "CAVBench: A Benchmark Suite for Connected and Autonomous Vehicles", "comments": "13 pages, The Third ACM/IEEE Symposium on Edge Computing 2018 SEC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected and autonomous vehicles (CAVs) have recently attracted a\nsignificant amount of attention both from researchers and industry. Numerous\nstudies targeting algorithms, software frameworks, and applications on the CAVs\nscenario have emerged. Meanwhile, several pioneer efforts have focused on the\nedge computing system and architecture design for the CAVs scenario and\nprovided various heterogeneous platform prototypes for CAVs. However, a\nstandard and comprehensive application benchmark for CAVs is missing, hindering\nthe study of these emerging computing systems. To address this challenging\nproblem, we present CAVBench, the first benchmark suite for the edge computing\nsystem in the CAVs scenario. CAVBench is comprised of six typical applications\ncovering four dominate CAVs scenarios and takes four datasets as standard\ninput. CAVBench provides quantitative evaluation results via application and\nsystem perspective output metrics. We perform a series of experiments and\nacquire three systemic characteristics of the applications in CAVBench. First,\nthe operation intensity of the applications is polarized, which explains why\nheterogeneous hardware is important for a CAVs computing system. Second, all\napplications in CAVBench consume high memory bandwidth, so the system should be\nequipped with high bandwidth memory or leverage good memory bandwidth\nmanagement to avoid the performance degradation caused by memory bandwidth\ncompetition. Third, some applications have worse data/instruction locality\nbased on the cache miss observation, so the computing system targeting these\napplications should optimize the cache architecture. Last, we use the CAVBench\nto evaluate a typical edge computing platform and present the quantitative and\nqualitative analysis of the benchmarking results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 20:07:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Wang", "Yifan", ""], ["Liu", "Shaoshan", ""], ["Wu", "Xiaopei", ""], ["Shi", "Weisong", ""]]}, {"id": "1810.06835", "submitter": "Andrew Rowley", "authors": "Andrew G. D. Rowley, Christian Brenninkmeijer, Simon Davidson, Donal\n  Fellows, Andrew Gait, David R. Lester, Luis A. Plana, Oliver Rhodes, Alan B.\n  Stokes, Steve B. Furber", "title": "SpiNNTools: The Execution Engine for the SpiNNaker Platform", "comments": null, "journal-ref": null, "doi": "10.3389/fnins.2019.00231", "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems are becoming more common place, as computers typically\ncontain multiple computation processors. The SpiNNaker architecture is such a\ndistributed architecture, containing millions of cores connected with a unique\ncommunication network, making it one of the largest neuromorphic computing\nplatforms in the world. Utilising these processors efficiently usually requires\nexpert knowledge of the architecture to generate executable code. This work\nintroduces a set of tools (SpiNNTools) that can map computational work\ndescribed as a graph in to executable code that runs on this novel machine. The\nSpiNNaker architecture is highly scalable which in turn produces unique\nchallenges in loading data, executing the mapped problem and the retrieval of\ndata. In this paper we describe these challenges in detail and the solutions\nimplemented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 06:35:09 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Rowley", "Andrew G. D.", ""], ["Brenninkmeijer", "Christian", ""], ["Davidson", "Simon", ""], ["Fellows", "Donal", ""], ["Gait", "Andrew", ""], ["Lester", "David R.", ""], ["Plana", "Luis A.", ""], ["Rhodes", "Oliver", ""], ["Stokes", "Alan B.", ""], ["Furber", "Steve B.", ""]]}, {"id": "1810.06964", "submitter": "Sayed Chhattan Shah", "authors": "Jae Hyeck Lee, Myong-Soon Park and Sayed Chhattan Shah", "title": "Wi-Fi Direct Based Mobile Ad hoc Network", "comments": "The 2nd International Conference on Computer and Communication\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile ad hoc network is a wireless network of mobile devices that\ncommunicate with one another without any fixed network infrastructure. Existing\nmobile ad hoc networks are either based on IEEE 802.11 a or b standard. Both\nstandards provide limited bandwidth and therefore are not suitable for\ndata-intensive applications such as automated video surveillance. Wi-Fi Direct\nis a new technology that enables mobile devices to directly communicate with\neach other without a wireless access point. Compare to existing technologies it\nprovides data rates of up to 250 Mbps which is sufficient for several mobile\ndata-intensive applications. Wi-Fi Direct technology, however, has two main\nlimitations. It does not support communication between two client devices in a\ngroup and it also does not provide support for multi hop communication. This\npaper describes a Wi-Fi Direct based multi hop mobile ad hoc network. More\nspecifically a routing layer has been developed to support communication\nbetween Wi-Fi Direct devices in a group and multi-hop communication between\ndevices across a group. The proposed system has been implemented on a group of\nfour Wi-Fi Direct enabled Samsung mobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 12:56:26 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Lee", "Jae Hyeck", ""], ["Park", "Myong-Soon", ""], ["Shah", "Sayed Chhattan", ""]]}, {"id": "1810.07000", "submitter": "Anne Reinarz", "authors": "Anne Reinarz, Jean-Matthieu Gallard, Michael Bader", "title": "Influence of A-Posteriori Subcell Limiting on Fault Frequency in\n  Higher-Order DG Schemes", "comments": "2018 IEEE/ACM 8th Workshop on Fault Tolerance for HPC at eXtreme\n  Scale (FTXS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft error rates are increasing as modern architectures require increasingly\nsmall features at low voltages. Due to the large number of components used in\nHPC architectures, these are particularly vulnerable to soft errors. Hence,\nwhen designing applications that run for long time periods on large machines,\nalgorithmic resilience must be taken into account. In this paper we analyse the\ninherent resiliency of a-posteriori limiting procedures in the context of the\nexplicit ADER DG hyperbolic PDE solver ExaHyPE. The a-posteriori limiter checks\nelement-local high-order DG solutions for physical admissibility, and can thus\nbe expected to also detect hardware-induced errors. Algorithmically, it can be\ninterpreted as element-local checkpointing and restarting of the solver with a\nmore robust finite volume scheme on a fine subgrid. We show that the limiter\nindeed increases the resilience of the DG algorithm, detecting and correcting\nparticularly those faults which would otherwise lead to a fatal failure.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:57:37 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 09:02:14 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Reinarz", "Anne", ""], ["Gallard", "Jean-Matthieu", ""], ["Bader", "Michael", ""]]}, {"id": "1810.07026", "submitter": "Markus H\\\"ohnerbach", "authors": "Markus H\\\"ohnerbach, Paolo Bientinesi", "title": "Optimizing AIREBO: Navigating the Journey from Complex Legacy Code to\n  High Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite initiatives to improve the quality of scientific codes, there still\nis a large presence of legacy code. Such code often needs to implement a lot of\nfunctionality under time constrains, sacrificing quality. Additionally, quality\nis rarely improved by optimizations for new architectures. This development\nmodel leads to code that is increasingly difficult to work with. Our suggested\nsolution includes complexity-reducing refactoring and hardware abstraction. We\nfocus on the AIREBO potential from LAMMPS, where the challenge is that any\npotential kernel is rather large and complex, hindering systematic\noptimization. This issue is common to codes that model multiple physical\nphenomena. We present our journey from the C++ port of a previous Fortran code\nto performance-portable, KNC-hybrid, vectorized, scalable, optimized code\nsupporting full and reduced precision. The journey includes extensive testing\nthat fixed bugs in the original code. Large-scale, full-precision runs sustain\nspeedups of more than 4x (KNL) and 3x (Skylake).\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:21:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["H\u00f6hnerbach", "Markus", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1810.07042", "submitter": "Sayed Chhattan Shah", "authors": "Sayed Chhattan Shah", "title": "Recent Advances in Mobile Grid and Cloud Computing", "comments": "24 pages", "journal-ref": "Intelligent Automation and Soft Computing, February 2017", "doi": "10.1080/10798587.2017.1280995", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid and cloud computing systems have been extensively used to solve large\nand complex problems in science and engineering areas. These systems include\npowerful computing resources connected through high-speed networks. Due to\nrecent advances in mobile computing and networking technologies, it has become\nfeasible to integrate various mobile devices such as robots, aerial vehicles,\nsensors, and smartphones with grid and cloud computing systems. This\nintegration enables design and development of next generation of applications\nthrough sharing of resources in mobile environments and also introduces several\nchallenges due to dynamic and unpredictable network. This paper discusses\napplications, research challenges involved in design and development of mobile\ngrid and cloud computing systems, and recent advances in the field.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:42:04 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Shah", "Sayed Chhattan", ""]]}, {"id": "1810.07077", "submitter": "Thorsten G\\\"otte", "authors": "Thorsten G\\\"otte, Vipin Ravindran Vijayalakshmi, Christian Scheideler", "title": "Always be Two Steps Ahead of Your Enemy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the maintenance of overlay networks under massive churn, i.e.\nnodes joining and leaving the network. We assume an adversary that may churn a\nconstant fraction $\\alpha n$ of nodes over the course of $\\mathcal{O}(\\log n)$\nrounds. In particular, the adversary has an almost up-to-date information of\nthe network topology as it can observe an only slightly outdated topology that\nis at least $2$ rounds old. Other than that, we only have the provably minimal\nrestriction that new nodes can only join the network via nodes that have taken\npart in the network for at least one round.\n  Our contributions are as follows: First, we show that it is impossible to\nmaintain a connected topology if adversary has up-to-date information about the\nnodes' connections. Further, we show that our restriction concerning the join\nis also necessary. As our main result present an algorithm that constructs a\nnew overlay -- completely independent of all previous overlays -- every $2$\nrounds. Furthermore, each node sends and receives only $\\mathcal{O}(\\log^3 n)$\nmessages each round. As part of our solution we propose the Linearized DeBruijn\nSwarm (LDS), a highly churn resistant overlay, which will be maintained by the\nalgorithm. However, our approaches can be transferred to a variety of classical\nP2P Topologies where nodes are mapped into the $[0,1)$-interval.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 15:29:30 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 14:33:27 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["G\u00f6tte", "Thorsten", ""], ["Vijayalakshmi", "Vipin Ravindran", ""], ["Scheideler", "Christian", ""]]}, {"id": "1810.07338", "submitter": "Sayed Chhattan Shah", "authors": "Sayed Chhattan Shah", "title": "A Mobile Ad hoc Cloud Computing and Networking Infrastructure for\n  Automated Video Surveillance System", "comments": "Technical Reports, 14 Pages", "journal-ref": "Journal of Computer Science, Volume 13, Issue 12, Pages 767-780,\n  23-12-2017", "doi": "10.3844/jcssp.2017.767.780", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile automated video surveillance system involves application of real-time\nimage and video processing algorithms which require a vast quantity of\ncomputing and storage resources. To support the execution of mobile automated\nvideo surveillance system, a mobile ad hoc cloud computing and networking\ninfrastructure is proposed in which multiple mobile devices interconnected\nthrough a mobile ad hoc network are combined to create a virtual supercomputing\nnode. An energy efficient resource allocation scheme has also been proposed for\nallocation of realtime automated video surveillance tasks. To enable\ncommunication between mobile devices, a Wi-Fi Direct based mobile ad hoc cloud\nnetworking infrastructure has been developed. More specifically, a routing\nlayer has been developed to support communication between Wi-Fi Direct devices\nin a group and multi-hop communication between devices across the group. The\nproposed system has been implemented on a group of Wi-Fi Direct-enabled Samsung\nmobile devices.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 08:27:23 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Shah", "Sayed Chhattan", ""]]}, {"id": "1810.07423", "submitter": "Parwat Singh Anjana", "authors": "Parwat Singh Anjana, Priyanka Badiwal, Rajeev Wankar, and C.\n  Raghavendra Rao", "title": "Cloud Service Provider Evaluation System using Fuzzy Rough Set Technique", "comments": "12 pages, 7 figures, and 8 tables", "journal-ref": null, "doi": "10.1109/SOSE.2019.00033", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Service Providers (CSPs) offer a wide variety of scalable, flexible,\nand cost-efficient services to cloud users on demand and pay-per-utilization\nbasis. However, vast diversity in available cloud service providers leads to\nnumerous challenges for users to determine and select the best suitable\nservice. Also, sometimes users need to hire the required services from multiple\nCSPs which introduce difficulties in managing interfaces, accounts, security,\nsupports, and Service Level Agreements (SLAs). To circumvent such problems\nhaving a Cloud Service Broker (CSB) be aware of service offerings and users\nQuality of Service (QoS) requirements will benefit both the CSPs as well as\nusers. In this work, we proposed a Fuzzy Rough Set based Cloud Service\nBrokerage Architecture, which is responsible for ranking and selecting services\nbased on users QoS requirements, and finally monitor the service execution. We\nhave used the fuzzy rough set technique for dimension reduction. Used weighted\nEuclidean distance to rank the CSPs. To prioritize user QoS request, we\nintended to use user assign weights, also incorporated system assigned weights\nto give the relative importance to QoS attributes. We compared the proposed\nranking technique with an existing method based on the system response time.\nThe case study experiment results show that the proposed approach is scalable,\nresilience, and produce better results with less searching time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 08:12:51 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 09:24:56 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Anjana", "Parwat Singh", ""], ["Badiwal", "Priyanka", ""], ["Wankar", "Rajeev", ""], ["Rao", "C. Raghavendra", ""]]}, {"id": "1810.07458", "submitter": "Mansaf Alam Dr", "authors": "Syed Arshad Ali, Mohammad Affan, Mansaf Alam", "title": "A Study of Efficient Energy Management Techniques for Cloud Computing\n  Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overall performance of the development of computing systems has been\nengrossed on enhancing demand from the client and enterprise domains. but, the\nintake of ever-increasing energy for computing systems has commenced to bound\nin increasing overall performance due to heavy electric payments and carbon\ndioxide emission. The growth in power consumption of server is increased\ncontinuously, and many researchers proposed, if this pattern repeats\ncontinuously, then the power consumption cost of a server over its lifespan\nwould be higher than its hardware prices. The power intake troubles more for\nclusters, grids, and clouds, which encompass numerous thousand heterogeneous\nservers. Continuous efforts have been done to reduce the electricity intake of\nthese massive-scale infrastructures. To identify the challenges and required\nfuture enhancements in the field of efficient energy consumption in Cloud\nComputing, it is necessary to synthesize and categorize the research and\ndevelopment done so far. In this paper, the authors discuss the reasons and\nproblems associated with huge energy consumption by Cloud data centres and\nprepare a taxonomy of huge energy consumption problems and its related\nsolutions. The authors cover all aspects of energy consumption by Cloud data\ncenters and analyze many research papers to find the better solution for\nefficient energy consumption. This work gives an overall information regarding\nenergy-consumption problems of Cloud data centres and energy-efficient\nsolutions for this problem. The paper is concluded with a conversation of\nfuture enhancement and development in energy-efficient methods in Cloud\nComputing\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 10:19:05 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Ali", "Syed Arshad", ""], ["Affan", "Mohammad", ""], ["Alam", "Mansaf", ""]]}, {"id": "1810.07622", "submitter": "Rafael Angarita", "authors": "Merzoug Soltane, Yudith Cardinale, Rafael Angarita, Philippe Rosse,\n  Marta Rukoz, Derdour Makhlouf, Kazar Okba", "title": "A Self-adaptive Agent-based System for Cloud Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing is a model for enabling on-demand network access to a shared\npool of computing resources, that can be dynamically allocated and released\nwith minimal effort. However, this task can be complex in highly dynamic\nenvironments with various resources to allocate for an increasing number of\ndifferent users requirements. In this work, we propose a Cloud architecture\nbased on a multi-agent system exhibiting a self-adaptive behavior to address\nthe dynamic resource allocation. This self-adaptive system follows a MAPE-K\napproach to reason and act, according to QoS, Cloud service information, and\npropagated run-time information, to detect QoS degradation and make better\nresource allocation decisions. We validate our proposed Cloud architecture by\nsimulation. Results show that it can properly allocate resources to reduce\nenergy consumption, while satisfying the users demanded QoS.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 15:37:52 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Soltane", "Merzoug", ""], ["Cardinale", "Yudith", ""], ["Angarita", "Rafael", ""], ["Rosse", "Philippe", ""], ["Rukoz", "Marta", ""], ["Makhlouf", "Derdour", ""], ["Okba", "Kazar", ""]]}, {"id": "1810.07748", "submitter": "Jianguo Chen", "authors": "Jianguo Chen, Kenli Li, Zhuo Tang, Kashif Bilal, Shui Yu, Chuliang\n  Weng, Keqin Li", "title": "A Parallel Random Forest Algorithm for Big Data in a Spark Cloud\n  Computing Environment", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2017,\n  28(4): 919-933", "doi": "10.1109/TPDS.2016.2603511", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of the big data age, the issue of how to obtain valuable\nknowledge from a dataset efficiently and accurately has attracted increasingly\nattention from both academia and industry. This paper presents a Parallel\nRandom Forest (PRF) algorithm for big data on the Apache Spark platform. The\nPRF algorithm is optimized based on a hybrid approach combining data-parallel\nand task-parallel optimization. From the perspective of data-parallel\noptimization, a vertical data-partitioning method is performed to reduce the\ndata communication cost effectively, and a data-multiplexing method is\nperformed is performed to allow the training dataset to be reused and diminish\nthe volume of data. From the perspective of task-parallel optimization, a dual\nparallel approach is carried out in the training process of RF, and a task\nDirected Acyclic Graph (DAG) is created according to the parallel training\nprocess of PRF and the dependence of the Resilient Distributed Datasets (RDD)\nobjects. Then, different task schedulers are invoked for the tasks in the DAG.\nMoreover, to improve the algorithm's accuracy for large, high-dimensional, and\nnoisy data, we perform a dimension-reduction approach in the training process\nand a weighted voting approach in the prediction process prior to\nparallelization. Extensive experimental results indicate the superiority and\nnotable advantages of the PRF algorithm over the relevant algorithms\nimplemented by Spark MLlib and other studies in terms of the classification\naccuracy, performance, and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 19:40:14 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Chen", "Jianguo", ""], ["Li", "Kenli", ""], ["Tang", "Zhuo", ""], ["Bilal", "Kashif", ""], ["Yu", "Shui", ""], ["Weng", "Chuliang", ""], ["Li", "Keqin", ""]]}, {"id": "1810.07751", "submitter": "Graham Gobieski", "authors": "Graham Gobieski, Nathan Beckmann, Brandon Lucia", "title": "Intelligence Beyond the Edge: Inference on Intermittent Embedded Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-harvesting technology provides a promising platform for future IoT\napplications. However, since communication is very expensive in these devices,\napplications will require inference \"beyond the edge\" to avoid wasting precious\nenergy on pointless communication. We show that application performance is\nhighly sensitive to inference accuracy. Unfortunately, accurate inference\nrequires large amounts of computation and memory, and energy-harvesting systems\nare severely resource-constrained. Moreover, energy-harvesting systems operate\nintermittently, suffering frequent power failures that corrupt results and\nimpede forward progress.\n  This paper overcomes these challenges to present the first full-scale\ndemonstration of DNN inference on an energy-harvesting system. We design and\nimplement SONIC, an intermittence-aware software system with specialized\nsupport for DNN inference. SONIC introduces loop continuation, a new technique\nthat dramatically reduces the cost of guaranteeing correct intermittent\nexecution for loop-heavy code like DNN inference. To build a complete system,\nwe further present GENESIS, a tool that automatically compresses networks to\noptimally balance inference accuracy and energy, and TAILS, which exploits SIMD\nhardware available in some microcontrollers to improve energy efficiency. Both\nSONIC & TAILS guarantee correct intermittent execution without any hand-tuning\nor performance loss across different power systems. Across three neural\nnetworks on a commercially available microcontroller, SONIC & TAILS reduce\ninference energy by 6.9x and 12.2x, respectively, over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:24:33 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 15:24:31 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Gobieski", "Graham", ""], ["Beckmann", "Nathan", ""], ["Lucia", "Brandon", ""]]}, {"id": "1810.07753", "submitter": "Koustabh Dolui", "authors": "Koustabh Dolui and Csaba Kiraly", "title": "Towards Multi-container Deployment on IoT Gateways", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stringent latency requirements in advanced Internet of Things (IoT)\napplications as well as an increased load on cloud data centers have prompted a\nmove towards a more decentralized approach, bringing storage and processing of\nIoT data closer to the end-devices through the deployment of multi-purpose IoT\ngateways. However, the resource constrained nature and diversity of these\ngateways pose a challenge in developing applications that can be deployed\nwidely. This challenge can be overcome with containerization, a form of\nlightweight virtualization, bringing support for a wide range of hardware\narchitectures and operating system agnostic deployment of applications on IoT\ngateways. This paper discusses the architectural aspects of containerization,\nand studies the suitability of available containerization tools for\nmulti-container deployment in the context of IoT gateways. We present\ncontainerization in the context of AGILE, a multi-container and micro-service\nbased open source framework for IoT gateways, developed as part of a Horizon\n2020 project. Our study of containerized services to perform common gateway\nfunctions like device discovery, data management and cloud integration among\nothers, reveal the advantages of having a containerized environment for IoT\ngateways with regard to use of base image hierarchies and image layering for\nin-container and cross-container performance optimizations. We illustrate these\nresults in a set of benchmark experiments in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 12:43:34 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Dolui", "Koustabh", ""], ["Kiraly", "Csaba", ""]]}, {"id": "1810.07766", "submitter": "Chen Yu", "authors": "Chen Yu, Hanlin Tang, Cedric Renggli, Simon Kassing, Ankit Singla, Dan\n  Alistarh, Ce Zhang, Ji Liu", "title": "Distributed Learning over Unreliable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's distributed machine learning systems assume {\\em reliable\nnetworks}: whenever two machines exchange information (e.g., gradients or\nmodels), the network should guarantee the delivery of the message. At the same\ntime, recent work exhibits the impressive tolerance of machine learning\nalgorithms to errors or noise arising from relaxed communication or\nsynchronization. In this paper, we connect these two trends, and consider the\nfollowing question: {\\em Can we design machine learning systems that are\ntolerant to network unreliability during training?} With this motivation, we\nfocus on a theoretical problem of independent interest---given a standard\ndistributed parameter server architecture, if every communication between the\nworker and the server has a non-zero probability $p$ of being dropped, does\nthere exist an algorithm that still converges, and at what speed? The technical\ncontribution of this paper is a novel theoretical analysis proving that\ndistributed learning over unreliable network can achieve comparable convergence\nrate to centralized or distributed learning over reliable networks. Further, we\nprove that the influence of the packet drop rate diminishes with the growth of\nthe number of \\textcolor{black}{parameter servers}. We map this theoretical\nresult onto a real-world scenario, training deep neural networks over an\nunreliable network layer, and conduct network simulation to validate the system\nimprovement by allowing the networks to be unreliable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:13:05 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 06:03:49 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 15:18:11 GMT"}, {"version": "v4", "created": "Thu, 16 May 2019 03:59:32 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Yu", "Chen", ""], ["Tang", "Hanlin", ""], ["Renggli", "Cedric", ""], ["Kassing", "Simon", ""], ["Singla", "Ankit", ""], ["Alistarh", "Dan", ""], ["Zhang", "Ce", ""], ["Liu", "Ji", ""]]}, {"id": "1810.07792", "submitter": "Lucas Cassano", "authors": "Lucas Cassano, Kun Yuan, Ali H. Sayed", "title": "Multi-Agent Fully Decentralized Value Function Learning with Linear\n  Convergence Rates", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a fully decentralized multi-agent algorithm for policy\nevaluation. The proposed scheme can be applied to two distinct scenarios. In\nthe first scenario, a collection of agents have distinct datasets gathered\nfollowing different behavior policies (none of which is required to explore the\nfull state space) in different instances of the same environment and they all\ncollaborate to evaluate a common target policy. The network approach allows for\nefficient exploration of the state space and allows all agents to converge to\nthe optimal solution even in situations where neither agent can converge on its\nown without cooperation. The second scenario is that of multi-agent games, in\nwhich the state is global and rewards are local. In this scenario, agents\ncollaborate to estimate the value function of a target team policy. The\nproposed algorithm combines off-policy learning, eligibility traces and linear\nfunction approximation. The proposed algorithm is of the variance-reduced kind\nand achieves linear convergence with $O(1)$ memory requirements. The linear\nconvergence of the algorithm is established analytically, and simulations are\nused to illustrate the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 20:54:47 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 16:41:28 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 15:15:39 GMT"}, {"version": "v4", "created": "Wed, 31 Jul 2019 13:15:34 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2019 09:44:11 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cassano", "Lucas", ""], ["Yuan", "Kun", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1810.07852", "submitter": "Xiangyu Guo", "authors": "Xiangyu Guo, Shi Li", "title": "Distributed $k$-Clustering for Data with Heavy Noise", "comments": "slightly improve the comm cost over the version accepted into\n  NeurIPS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the $k$-center/median/means clustering with\noutliers problems (or the $(k, z)$-center/median/means problems) in the\ndistributed setting. Most previous distributed algorithms have their\ncommunication costs linearly depending on $z$, the number of outliers. Recently\nGuha et al. overcame this dependence issue by considering bi-criteria\napproximation algorithms that output solutions with $2z$ outliers. For the case\nwhere $z$ is large, the extra $z$ outliers discarded by the algorithms might be\ntoo large, considering that the data gathering process might be costly. In this\npaper, we improve the number of outliers to the best possible $(1+\\epsilon)z$,\nwhile maintaining the $O(1)$-approximation ratio and independence of\ncommunication cost on $z$. The problems we consider include the $(k, z)$-center\nproblem, and $(k, z)$-median/means problems in Euclidean metrics.\nImplementation of the our algorithm for $(k, z)$-center shows that it\noutperforms many previous algorithms, both in terms of the communication cost\nand quality of the output solution.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 01:04:14 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:41:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Guo", "Xiangyu", ""], ["Li", "Shi", ""]]}, {"id": "1810.08038", "submitter": "EPTCS", "authors": "Eric Fabre (INRIA Rennes - Bretagne Atlantique, France), G. Michele\n  Pinna (Universit\\`a degli Studi di Cagliari, Italy)", "title": "Toward a Uniform Approach to the Unfolding of Nets", "comments": "In Proceedings ICE 2018, arXiv:1810.02053", "journal-ref": "EPTCS 279, 2018, pp. 21-36", "doi": "10.4204/EPTCS.279.5", "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the notion of spread net. Spread nets are (safe)\nPetri nets equipped with vector clocks on places and with ticking functions on\ntransitions, and are such that vector clocks are consistent with the ticking of\ntransitions. Such nets generalize previous families of nets like unfoldings,\nmerged processes and trellis processes, and can thus be used to represent runs\nof a net in a true concurrency semantics through an operation called the\nspreading of a net. By contrast with previous constructions, which may identify\nconflicts, spread nets allow loops in time\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 00:34:11 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Fabre", "Eric", "", "INRIA Rennes - Bretagne Atlantique, France"], ["Pinna", "G. Michele", "", "Universit\u00e0 degli Studi di Cagliari, Italy"]]}, {"id": "1810.08092", "submitter": "Vivek Bagaria", "authors": "Vivek Bagaria, Sreeram Kannan, David Tse, Giulia Fanti, Pramod\n  Viswanath", "title": "Deconstructing the Blockchain to Approach Physical Limits", "comments": "Computer and Communications Security, 2019", "journal-ref": "Computer and Communications Security, 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transaction throughput, confirmation latency and confirmation reliability are\nfundamental performance measures of any blockchain system in addition to its\nsecurity. In a decentralized setting, these measures are limited by two\nunderlying physical network attributes: communication capacity and\nspeed-of-light propagation delay. Existing systems operate far away from these\nphysical limits. In this work we introduce Prism, a new proof-of-work\nblockchain protocol, which can achieve 1) security against up to 50%\nadversarial hashing power; 2) optimal throughput up to the capacity C of the\nnetwork; 3) confirmation latency for honest transactions proportional to the\npropagation delay D, with confirmation error probability exponentially small in\nCD ; 4) eventual total ordering of all transactions. Our approach to the design\nof this protocol is based on deconstructing the blockchain into its basic\nfunctionalities and systematically scaling up these functionalities to approach\ntheir physical limits.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 14:55:40 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 17:40:56 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 19:40:17 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 00:47:25 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Bagaria", "Vivek", ""], ["Kannan", "Sreeram", ""], ["Tse", "David", ""], ["Fanti", "Giulia", ""], ["Viswanath", "Pramod", ""]]}, {"id": "1810.08260", "submitter": "Srivatsan Ravi Mr", "authors": "Ryan Goodfellow, Lincoln Thurlow, Srivatsan Ravi", "title": "Merge: An Architecture for Interconnected Testbed Ecosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the cybersecurity research community, there is no one-size-fits-all\nsolution for merging large numbers of heterogeneous resources and\nexperimentation capabilities from disparate specialized testbeds into\nintegrated experiments. The current landscape for cyber-experimentation is\ndiverse, encompassing many fields including critical infrastructure, enterprise\nIT, cyber-physical systems, cellular networks, automotive platforms, IoT and\nindustrial control systems. Existing federated testbeds are constricted in\ndesign to predefined domains of applicability, lacking the systematic ability\nto integrate the burgeoning number of heterogeneous devices or tools that\nenable their effective use for experimentation. We have developed the Merge\narchitecture to dynamically integrate disparate testbeds in a logically\ncentralized way that allows researchers to effectively discover, and use the\nresources and capabilities provided the by evolving ecosystem of distributed\ntestbeds for the development of rigorous and high-fidelity cybersecurity\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:54:36 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 18:16:57 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Goodfellow", "Ryan", ""], ["Thurlow", "Lincoln", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1810.08313", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Gauri Joshi", "title": "Adaptive Communication Strategies to Achieve the Best Error-Runtime\n  Trade-off in Local-Update SGD", "comments": "Accepted to SysML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale machine learning training, in particular distributed stochastic\ngradient descent, needs to be robust to inherent system variability such as\nnode straggling and random communication delays. This work considers a\ndistributed training framework where each worker node is allowed to perform\nlocal model updates and the resulting models are averaged periodically. We\nanalyze the true speed of error convergence with respect to wall-clock time\n(instead of the number of iterations), and analyze how it is affected by the\nfrequency of averaging. The main contribution is the design of AdaComm, an\nadaptive communication strategy that starts with infrequent averaging to save\ncommunication delay and improve convergence speed, and then increases the\ncommunication frequency in order to achieve a low error floor. Rigorous\nexperiments on training deep neural networks show that AdaComm can take $3\n\\times$ less time than fully synchronous SGD, and still reach the same final\ntraining loss.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 00:04:05 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:45:02 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wang", "Jianyu", ""], ["Joshi", "Gauri", ""]]}, {"id": "1810.08342", "submitter": "Hyeonjoong Cho", "authors": "Hyeonjoong Cho, Arvind Easwaran", "title": "Flow Network Models for Online Scheduling Real-time Tasks on\n  Multiprocessors", "comments": "33 pages, 12 figures, submitted to Real-Time Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the flow network model to solve the multiprocessor real-time task\nscheduling problems. Using the flow network model or its generic form, linear\nprogramming (LP) formulation, for the problems is not new. However, the\nprevious works have limitations, for example, that they are classified as\noffline scheduling techniques since they establish a flow network model or an\nLP problem considering a very long time interval. In this study, we propose how\nto construct the flow network model for online scheduling periodic real-time\ntasks on multiprocessors. Our key idea is to construct the flow network only\nfor the active instances of tasks at the current scheduling time, while\nguaranteeing the existence of an optimal schedule for the future instances of\nthe tasks. The optimal scheduling is here defined to ensure that all real-time\ntasks meet their deadlines when the total utilization demand of the given tasks\ndoes not exceed the total processing capacity. We then propose the flow network\nmodel-based polynomial-time scheduling algorithms. Advantageously, the flow\nnetwork model allows the task workload to be collected unfairly within a\ncertain time interval without losing the optimality. It thus leads us to\ndesigning three unfair-but-optimal scheduling algorithms on both continuous and\ndiscrete-time models. Especially, our unfair-but-optimal scheduling algorithm\non a discrete-time model is, to the best of our knowledge, the first in the\nproblem domain. We experimentally demonstrate that it significantly alleviates\nthe scheduling overheads, i.e., the reduced number of preemptions with the\ncomparable number of task migrations across processors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 03:06:36 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Cho", "Hyeonjoong", ""], ["Easwaran", "Arvind", ""]]}, {"id": "1810.08403", "submitter": "Lingxiao Ma", "authors": "Lingxiao Ma, Zhi Yang, Youshan Miao, Jilong Xue, Ming Wu, Lidong Zhou,\n  Yafei Dai", "title": "Towards Efficient Large-Scale Graph Neural Network Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning models have moved beyond low-dimensional regular grids\nsuch as image, video, and speech, to high-dimensional graph-structured data,\nsuch as social networks, brain connections, and knowledge graphs. This\nevolution has led to large graph-based irregular and sparse models that go\nbeyond what existing deep learning frameworks are designed for. Further, these\nmodels are not easily amenable to efficient, at scale, acceleration on parallel\nhardwares (e.g. GPUs). We introduce NGra, the first parallel processing\nframework for graph-based deep neural networks (GNNs). NGra presents a new\nSAGA-NN model for expressing deep neural networks as vertex programs with each\nlayer in well-defined (Scatter, ApplyEdge, Gather, ApplyVertex) graph operation\nstages. This model not only allows GNNs to be expressed intuitively, but also\nfacilitates the mapping to an efficient dataflow representation. NGra addresses\nthe scalability challenge transparently through automatic graph partitioning\nand chunk-based stream processing out of GPU core or over multiple GPUs, which\ncarefully considers data locality, data movement, and overlapping of parallel\nprocessing and data movement. NGra further achieves efficiency through highly\noptimized Scatter/Gather operators on GPUs despite its sparsity. Our evaluation\nshows that NGra scales to large real graphs that none of the existing\nframeworks can handle directly, while achieving up to about 4 times speedup\neven at small scales over the multiple-baseline design on TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 08:50:58 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Ma", "Lingxiao", ""], ["Yang", "Zhi", ""], ["Miao", "Youshan", ""], ["Xue", "Jilong", ""], ["Wu", "Ming", ""], ["Zhou", "Lidong", ""], ["Dai", "Yafei", ""]]}, {"id": "1810.08544", "submitter": "Udit Agarwal", "authors": "Udit Agarwal and Vijaya Ramachandran", "title": "New and Simplified Distributed Algorithms for Weighted All Pairs\n  Shortest Paths", "comments": "arXiv admin note: text overlap with arXiv:1807.08824", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing all pairs shortest paths (APSP) and\nshortest paths for k sources in a weighted graph in the distributed CONGEST\nmodel. For graphs with non-negative integer edge weights (including zero\nweights) we build on a recent pipelined algorithm to obtain\n$\\tilde{O}(\\lambda^{1/4}\\cdot n^{5/4})$ in graphs with non-negative integer\nedge-weight at most $\\lambda$, and $\\tilde{O}(n \\cdot \\bigtriangleup^{1/3})$\nrounds for shortest path distances at most $\\bigtriangleup$. Additionally, we\nsimplify some of the procedures in the earlier APSP algorithms for non-negative\nedge weights in [HNS17,ARKP18]. We also present results for computing h-hop\nshortest paths and shortest paths from $k$ given sources.\n  In other results, we present a randomized exact APSP algorithm for graphs\nwith arbitrary edge weights that runs in $\\tilde{O}(n^{4/3})$ rounds w.h.p. in\nn, which improves the previous best $\\tilde{O}(n^{3/2})$ bound, which is\ndeterministic. We also present an $\\tilde{O}(n/\\epsilon^2)$-round deterministic\n$(1+\\epsilon)$ approximation algorithm for graphs with non-negative $poly(n)$\ninteger weights (including zero edge-weights), improving results in\n[Nanongkai14,LP15] that hold only for positive integer weights.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 15:53:23 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Agarwal", "Udit", ""], ["Ramachandran", "Vijaya", ""]]}, {"id": "1810.08675", "submitter": "Andrew McGough", "authors": "A. Stephen McGough and Matthew Forshaw and John Brennan and Noura Al\n  Moubayed and Stephen Bonner", "title": "Using Machine Learning to reduce the energy wasted in Volunteer\n  Computing Environments", "comments": "Accepted for publication at THE 9th international Green and\n  sustainable computing Conference, Technically Co-sponsored by IEEE Computer\n  Society & STC Sustainable Computing, October 22-24, Pittsburgh, PA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Throughput Computing (HTC) provides a convenient mechanism for running\nthousands of tasks. Many HTC systems exploit computers which are provisioned\nfor other purposes by utilising their idle time - volunteer computing. This has\ngreat advantages as it gives access to vast quantities of computational power\nfor little or no cost. The downside is that running tasks are sacrificed if the\ncomputer is needed for its primary use. Normally terminating the task which\nmust be restarted on a different computer - leading to wasted energy and an\nincrease in task completion time. We demonstrate, through the use of\nsimulation, how we can reduce this wasted energy by targeting tasks at\ncomputers less likely to be needed for primary use, predicting this idle time\nthrough machine learning. By combining two machine learning approaches, namely\nRandom Forest and MultiLayer Perceptron, we save 51.4% of the energy without\nsignificantly affecting the time to complete tasks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2018 20:17:41 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["McGough", "A. Stephen", ""], ["Forshaw", "Matthew", ""], ["Brennan", "John", ""], ["Moubayed", "Noura Al", ""], ["Bonner", "Stephen", ""]]}, {"id": "1810.08744", "submitter": "Mark Hamilton", "authors": "Mark Hamilton, Sudarshan Raghunathan, Ilya Matiach, Andrew\n  Schonhoffer, Anand Raman, Eli Barzilay, Karthik Rajendran, Dalitso Banda,\n  Casey Jisoo Hong, Manon Knoertzer, Ben Brodsky, Minsoo Thigpen, Janhavi\n  Suresh Mahajan, Courtney Cochrane, Abhiram Eswaran, Ari Green", "title": "MMLSpark: Unifying Machine Learning Ecosystems at Massive Scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Microsoft Machine Learning for Apache Spark (MMLSpark), an\necosystem of enhancements that expand the Apache Spark distributed computing\nlibrary to tackle problems in Deep Learning, Micro-Service Orchestration,\nGradient Boosting, Model Interpretability, and other areas of modern\ncomputation. Furthermore, we present a novel system called Spark Serving that\nallows users to run any Apache Spark program as a distributed, sub-millisecond\nlatency web service backed by their existing Spark Cluster. All MMLSpark\ncontributions have the same API to enable simple composition across frameworks\nand usage across batch, streaming, and RESTful web serving scenarios on static,\nelastic, or serverless clusters. We showcase MMLSpark by creating a method for\ndeep object detection capable of learning without human labeled data and\ndemonstrate its effectiveness for Snow Leopard conservation.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 03:12:59 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 15:39:52 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Hamilton", "Mark", ""], ["Raghunathan", "Sudarshan", ""], ["Matiach", "Ilya", ""], ["Schonhoffer", "Andrew", ""], ["Raman", "Anand", ""], ["Barzilay", "Eli", ""], ["Rajendran", "Karthik", ""], ["Banda", "Dalitso", ""], ["Hong", "Casey Jisoo", ""], ["Knoertzer", "Manon", ""], ["Brodsky", "Ben", ""], ["Thigpen", "Minsoo", ""], ["Mahajan", "Janhavi Suresh", ""], ["Cochrane", "Courtney", ""], ["Eswaran", "Abhiram", ""], ["Green", "Ari", ""]]}, {"id": "1810.08869", "submitter": "Ryan Kim", "authors": "Biresh Kumar Joardar, Ryan Gary Kim, Janardhan Rao Doppa, Partha\n  Pratim Pande, Diana Marculescu, and Radu Marculescu", "title": "Learning-based Application-Agnostic 3D NoC Design for Heterogeneous\n  Manycore Systems", "comments": "Published in IEEE Transactions on Computers", "journal-ref": "IEEE Transactions on Computers, vol. 68, no. 6, June 2019", "doi": "10.1109/TC.2018.2889053", "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rising use of deep learning and other big-data algorithms has led to an\nincreasing demand for hardware platforms that are computationally powerful, yet\nenergy-efficient. Due to the amount of data parallelism in these algorithms,\nhigh-performance 3D manycore platforms that incorporate both CPUs and GPUs\npresent a promising direction. However, as systems use heterogeneity (e.g., a\ncombination of CPUs, GPUs, and accelerators) to improve performance and\nefficiency, it becomes more pertinent to address the distinct and likely\nconflicting communication requirements (e.g., CPU memory access latency or GPU\nnetwork throughput) that arise from such heterogeneity. Unfortunately, it is\ndifficult to quickly explore the hardware design space and choose appropriate\ntradeoffs between these heterogeneous requirements. To address these\nchallenges, we propose the design of a 3D Network-on-Chip (NoC) for\nheterogeneous manycore platforms that considers the appropriate design\nobjectives for a 3D heterogeneous system and explores various tradeoffs using\nan efficient ML-based multi-objective optimization technique. The proposed\ndesign space exploration considers the various requirements of its\nheterogeneous components and generates a set of 3D NoC architectures that\nefficiently trades off these design objectives. Our findings show that by\njointly considering these requirements (latency, throughput, temperature, and\nenergy), we can achieve 9.6% better Energy-Delay Product on average at nearly\niso-temperature conditions when compared to a thermally-optimized design for 3D\nheterogeneous NoCs. More importantly, our results suggest that our 3D NoCs\noptimized for a few applications can be generalized for unknown applications as\nwell. Our results show that these generalized 3D NoCs only incur a 1.8%\n(36-tile system) and 1.1% (64-tile system) average performance loss compared to\napplication-specific NoCs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 23:46:14 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 15:06:54 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Joardar", "Biresh Kumar", ""], ["Kim", "Ryan Gary", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Marculescu", "Diana", ""], ["Marculescu", "Radu", ""]]}, {"id": "1810.08899", "submitter": "Jie Ren", "authors": "Qing Qin, Jie Ren, Jialong Yu, Ling Gao, Hai Wang, Jie Zheng, Yansong\n  Feng, Jianbin Fang, Zheng Wang", "title": "To Compress, or Not to Compress: Characterizing Deep Learning Model\n  Compression for Embedded Inference", "comments": "8 pages, To appear in ISPA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep neural networks (DNNs) make them attractive for\nembedded systems. However, it can take a long time for DNNs to make an\ninference on resource-constrained computing devices. Model compression\ntechniques can address the computation issue of deep inference on embedded\ndevices. This technique is highly attractive, as it does not rely on\nspecialized hardware, or computation-offloading that is often infeasible due to\nprivacy concerns or high latency. However, it remains unclear how model\ncompression techniques perform across a wide range of DNNs. To design efficient\nembedded deep learning solutions, we need to understand their behaviors. This\nwork develops a quantitative approach to characterize model compression\ntechniques on a representative embedded deep learning architecture, the NVIDIA\nJetson Tx2. We perform extensive experiments by considering 11 influential\nneural network architectures from the image classification and the natural\nlanguage processing domains. We experimentally show that how two mainstream\ncompression techniques, data quantization and pruning, perform on these network\narchitectures and the implications of compression techniques to the model\nstorage size, inference time, energy consumption and performance metrics. We\ndemonstrate that there are opportunities to achieve fast deep inference on\nembedded systems, but one must carefully choose the compression settings. Our\nresults provide insights on when and how to apply model compression techniques\nand guidelines for designing efficient embedded deep learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 05:09:45 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Qin", "Qing", ""], ["Ren", "Jie", ""], ["Yu", "Jialong", ""], ["Gao", "Ling", ""], ["Wang", "Hai", ""], ["Zheng", "Jie", ""], ["Feng", "Yansong", ""], ["Fang", "Jianbin", ""], ["Wang", "Zheng", ""]]}, {"id": "1810.08955", "submitter": "Jiawen Liu", "authors": "Jiawen Liu, Dong Li, Gokcen Kestor, Jeffrey Vetter", "title": "Runtime Concurrency Control and Operation Scheduling for High\n  Performance Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural network often uses a machine learning framework such as\nTensorFlow and Caffe2. These frameworks employ a dataflow model where the NN\ntraining is modeled as a directed graph composed of a set of nodes. Operations\nin neural network training are typically implemented by the frameworks as\nprimitives and represented as nodes in the dataflow graph. Training NN models\nin a dataflow-based machine learning framework involves a large number of\nfine-grained operations. Those operations have diverse memory access patterns\nand computation intensity. How to manage and schedule those operations is\nchallenging, because we have to decide the number of threads to run each\noperation (concurrency control) and schedule those operations for good hardware\nutilization and system throughput.\n  In this paper, we extend an existing runtime system (the TensorFlow runtime)\nto enable automatic concurrency control and scheduling of operations. We\nexplore performance modeling to predict the performance of operations with\nvarious thread-level parallelism. Our performance model is highly accurate and\nlightweight. Leveraging the performance model, our runtime system employs a set\nof scheduling strategies that co-run operations to improve hardware utilization\nand system throughput. Our runtime system demonstrates a big performance\nbenefit. Comparing with using the recommended configurations for concurrency\ncontrol and operation scheduling in TensorFlow, our approach achieves 33%\nperformance (execution time) improvement on average (up to 49%) for three\nneural network models, and achieves high performance closing to the optimal one\nmanually obtained by the user.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 14:18:03 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 01:15:36 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Liu", "Jiawen", ""], ["Li", "Dong", ""], ["Kestor", "Gokcen", ""], ["Vetter", "Jeffrey", ""]]}, {"id": "1810.09007", "submitter": "Sanket Vaibhav Mehta", "authors": "Sanket Vaibhav Mehta, Shagun Sodhani, Dhaval Patel", "title": "Spatial Co-location Pattern Mining - A new perspective using Graph\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial co-location pattern mining refers to the task of discovering the\ngroup of objects or events that co-occur at many places. Extracting these\npatterns from spatial data is very difficult due to the complexity of spatial\ndata types, spatial relationships, and spatial auto-correlation. These patterns\nhave applications in domains including public safety, geo-marketing, crime\nprediction and ecology. Prior work focused on using the spatial join. While\nthese approaches provide state-of-the-art results, they are very expensive to\ncompute due to the multiway spatial join and scaling them to real-world\ndatasets is an open problem. We address these limitations by formulating the\nco-location pattern discovery as a clique enumeration problem over a\nneighborhood graph (which is materialized using a distributed graph database).\nWe propose three new traversal based algorithms, namely $CliqueEnum_G$,\n$CliqueEnum_K$ and $CliqueExtend$. We provide the empirical evidence for the\neffectiveness of our proposed algorithms by evaluating them for a large\nreal-life dataset. The three algorithms allow for a trade-off between time and\nmemory requirements and support interactive data analysis without having to\nrecompute all the intermediate results. These attributes make our algorithms\napplicable to a wide range of use cases for different data sizes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 19:05:33 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Mehta", "Sanket Vaibhav", ""], ["Sodhani", "Shagun", ""], ["Patel", "Dhaval", ""]]}, {"id": "1810.09027", "submitter": "Yasamin Nazari", "authors": "Michael Dinitz, Yasamin Nazari", "title": "Massively Parallel Approximate Distance Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data structures that allow efficient distance estimation (distance oracles,\ndistance sketches, etc.) have been extensively studied, and are particularly\nwell studied in centralized models and classical distributed models such as\nCONGEST. We initiate their study in newer (and arguably more realistic) models\nof distributed computation: the Congested Clique model and the Massively\nParallel Computation (MPC) model. We provide efficient constructions in both of\nthese models, but our core results are for MPC. In MPC we give two main\nresults: an algorithm that constructs stretch/space optimal distance sketches\nbut takes a (small) polynomial number of rounds, and an algorithm that\nconstructs distance sketches with worse stretch but that only takes\npolylogarithmic rounds.\n  Along the way, we show that other useful combinatorial structures can also be\ncomputed in MPC. In particular, one key component we use to construct distance\nsketches are an MPC construction of the hopsets of Elkin and Neiman (2016).\nThis result has additional applications such as the first polylogarithmic time\nalgorithm for constant approximate single-source shortest paths for weighted\ngraphs in the low memory MPC setting.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 21:07:24 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 01:46:41 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 01:38:44 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 15:56:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Dinitz", "Michael", ""], ["Nazari", "Yasamin", ""]]}, {"id": "1810.09300", "submitter": "Srinivasan Keshav", "authors": "S. Keshav, W. Golab, B. Wong, S. Rizvi, and S. Gorbunov", "title": "RCanopus: Making Canopus Resilient to Failures and Byzantine Faults", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed consensus is a key enabler for many distributed systems including\ndistributed databases and blockchains. Canopus is a scalable distributed\nconsensus protocol that ensures that live nodes in a system agree on an ordered\nsequence of operations (called transactions). Unlike most prior consensus\nprotocols, Canopus does not rely on a single leader. Instead, it uses a virtual\ntree overlay for message dissemination to limit network traffic across\noversubscribed links. It leverages hardware redundancies, both within a rack\nand inside the network fabric, to reduce both protocol complexity and\ncommunication overhead. These design decisions enable Canopus to support large\ndeployments without significant performance degradation.\n  The existing Canopus protocol is resilient in the face of node and\ncommunication failures, but its focus is primarily on performance, so does not\nrespond well to other types of failures. For example, the failure of a single\nrack of servers causes all live nodes to stall. The protocol is also open to\nattack by Byzantine nodes, which can cause different live nodes to conclude the\nprotocol with different transaction orders. In this paper, we describe RCanopus\n(`resilent Canopus') which extends Canopus to add liveness, that is, allowing\nlive nodes to make progress, when possible, despite many types of failures.\nThis requires RCanopus to accurately detect and recover from failure despite\nusing unreliable failure detectors, and tolerance of Byzantine attacks. Second,\nRCanopus guarantees safety, that is, agreement amongst live nodes of\ntransaction order, in the presence of Byzantine attacks and network\npartitioning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:08:16 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 18:07:36 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 22:30:07 GMT"}, {"version": "v4", "created": "Sun, 16 Jun 2019 12:51:20 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Keshav", "S.", ""], ["Golab", "W.", ""], ["Wong", "B.", ""], ["Rizvi", "S.", ""], ["Gorbunov", "S.", ""]]}, {"id": "1810.09330", "submitter": "Mohamed Wahib", "authors": "Jens Domke, Kazuaki Matsumura, Mohamed Wahib, Haoyu Zhang, Keita\n  Yashima, Toshiki Tsuchikawa, Yohei Tsuji, Artur Podobas, Satoshi Matsuoka", "title": "Double-precision FPUs in High-Performance Computing: an Embarrassment of\n  Riches?", "comments": "IEEE International Parallel and Distributed Processing Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the (uncontended) common wisdom in High-Performance Computing (HPC) is\nthe applications' need for large amount of double-precision support in\nhardware. Hardware manufacturers, the TOP500 list, and (rarely revisited)\nlegacy software have without doubt followed and contributed to this view.\n  In this paper, we challenge that wisdom, and we do so by exhaustively\ncomparing a large number of HPC proxy application on two processors: Intel's\nKnights Landing (KNL) and Knights Mill (KNM). Although similar, the KNM and KNL\narchitecturally deviate at one important point: the silicon area devoted to\ndouble-precision arithmetic's. This fortunate discrepancy allows us to\nempirically quantify the performance impact in reducing the amount of hardware\ndouble-precision arithmetic.\n  Our analysis shows that this common wisdom might not always be right. We find\nthat the investigated HPC proxy applications do allow for a (significant)\nreduction in double-precision with little-to-no performance implications. With\nthe advent of a failing of Moore's law, our results partially reinforce the\nview taken by modern industry (e.g. upcoming Fujitsu ARM64FX) to integrate\nhybrid-precision hardware units.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 14:51:44 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:12:54 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 02:03:29 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Domke", "Jens", ""], ["Matsumura", "Kazuaki", ""], ["Wahib", "Mohamed", ""], ["Zhang", "Haoyu", ""], ["Yashima", "Keita", ""], ["Tsuchikawa", "Toshiki", ""], ["Tsuji", "Yohei", ""], ["Podobas", "Artur", ""], ["Matsuoka", "Satoshi", ""]]}, {"id": "1810.09360", "submitter": "Arash Tavakkol", "authors": "Arash Tavakkol, Aasheesh Kolli, Stanko Novakovic, Kaveh Razavi, Juan\n  Gomez-Luna, Hasan Hassan, Claude Barthels, Yaohua Wang, Mohammad Sadrosadati,\n  Saugata Ghose, Ankit Singla, Pratap Subrahmanyam, and Onur Mutlu", "title": "Enabling Efficient RDMA-based Synchronous Mirroring of Persistent Memory\n  Transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Synchronous Mirroring (SM) is a standard approach to building\nhighly-available and fault-tolerant enterprise storage systems. SM ensures\nstrong data consistency by maintaining multiple exact data replicas and\nsynchronously propagating every update to all of them. Such strong consistency\nprovides fault tolerance guarantees and a simple programming model coveted by\nenterprise system designers. For current storage devices, SM comes at modest\nperformance overheads. This is because performing both local and remote updates\nsimultaneously is only marginally slower than performing just local updates,\ndue to the relatively slow performance of accesses to storage in today's\nsystems. However, emerging persistent memory and ultra-low-latency network\ntechnologies necessitate a careful re-evaluation of the existing SM techniques,\nas these technologies present fundamentally different latency characteristics\ncompared than their traditional counterparts. In addition to that, existing\nlow-latency network technologies, such as Remote Direct Memory Access (RDMA),\nprovide limited ordering guarantees and do not provide durability guarantees\nnecessary for SM. To evaluate the performance implications of RDMA-based SM, we\ndevelop a rigorous testing framework that is based on emulated persistent\nmemory. Our testing framework makes use of two different tools: (i) a\nconfigurable microbenchmark and (ii) a modified version of the WHISPER\nbenchmark suite, which comprises a set of common cloud applications. Using this\nframework, we find that recently proposed RDMA primitives, such as remote\ncommit, provide correctness guarantees, but do not take full advantage of the\nasynchronous nature of RDMA hardware. To this end, we propose new primitives\nenabling efficient and correct SM over RDMA, and use these primitives to\ndevelop two new techniques delivering high-performance SM of persistent\nmemories.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:26:08 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Tavakkol", "Arash", ""], ["Kolli", "Aasheesh", ""], ["Novakovic", "Stanko", ""], ["Razavi", "Kaveh", ""], ["Gomez-Luna", "Juan", ""], ["Hassan", "Hasan", ""], ["Barthels", "Claude", ""], ["Wang", "Yaohua", ""], ["Sadrosadati", "Mohammad", ""], ["Ghose", "Saugata", ""], ["Singla", "Ankit", ""], ["Subrahmanyam", "Pratap", ""], ["Mutlu", "Onur", ""]]}, {"id": "1810.09376", "submitter": "Wanling Gao", "authors": "Wanling Gao, Jianfeng Zhan, Lei Wang, Chunjie Luo, Zhen Jia, Daoyi\n  Zheng, Chen Zheng, Xiwen He, Hainan Ye, Haibin Wang, and Rui Ren", "title": "Data Motif-based Proxy Benchmarks for Big Data and AI Workloads", "comments": "The paper will be published on 2018 IEEE International Symposium on\n  Workload Characterization (IISWC 2018). arXiv admin note: text overlap with\n  arXiv:1802.00699, arXiv:1711.03229", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the architecture community, reasonable simulation time is a strong\nrequirement in addition to performance data accuracy. However, emerging big\ndata and AI workloads are too huge at binary size level and prohibitively\nexpensive to run on cycle-accurate simulators. The concept of data motif, which\nis identified as a class of units of computation performed on initial or\nintermediate data, is the first step towards building proxy benchmark to mimic\nthe real-world big data and AI workloads. However, there is no practical way to\nconstruct a proxy benchmark based on the data motifs to help simulation-based\nresearch. In this paper, we embark on a study to bridge the gap between data\nmotif and a practical proxy benchmark. We propose a data motif-based proxy\nbenchmark generating methodology by means of machine learning method, which\ncombine data motifs with different weights to mimic the big data and AI\nworkloads. Furthermore, we implement various data motifs using light-weight\nstacks and apply the methodology to five real-world workloads to construct a\nsuite of proxy benchmarks, considering the data types, patterns, and\ndistributions. The evaluation results show that our proxy benchmarks shorten\nthe execution time by 100s times on real systems while maintaining the average\nsystem and micro-architecture performance data accuracy above 90%, even\nchanging the input data sets or cluster configurations. Moreover, the generated\nproxy benchmarks reflect consistent performance trends across different\narchitectures. To facilitate the community, we will release the proxy\nbenchmarks on the project homepage http://prof.ict.ac.cn/BigDataBench.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 13:58:00 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gao", "Wanling", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Luo", "Chunjie", ""], ["Jia", "Zhen", ""], ["Zheng", "Daoyi", ""], ["Zheng", "Chen", ""], ["He", "Xiwen", ""], ["Ye", "Hainan", ""], ["Wang", "Haibin", ""], ["Ren", "Rui", ""]]}, {"id": "1810.09438", "submitter": "Amro Awad", "authors": "Amro Awad, Laurent Njilla and Mao Ye", "title": "Triad-NVM: Persistent-Security for Integrity-Protected and Encrypted\n  Non-Volatile Memories (NVMs)", "comments": "This paper is currently under submission. We arXiv our paper to\n  establish credit for inventing this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging Non-Volatile Memories (NVMs) are promising contenders for building\nfuture memory systems. On the other side, unlike DRAM systems, NVMs can retain\ndata even after power loss and thus enlarge the attack surface. While data\nencryption and integrity verification have been proposed earlier for DRAM\nsystems, protecting and recovering secure memories becomes more challenging\nwith persistent memory. Specifically, security metadata, e.g., encryption\ncounters and Merkle Tree data, should be securely persisted and recovered\nacross system reboots and during recovery from crashes. Not persisting updates\nto security metadata can lead to data inconsistency, in addition to serious\nsecurity vulnerabilities.\n  In this paper, we pioneer a new direction that explores persistency of both\nMerkle Tree and encryption counters to enable secure recovery of\ndata-verifiable and encrypted memory systems. To this end, we coin a new\nconcept that we call Persistent-Security. We discuss the requirements for such\npersistently secure systems, propose novel optimizations, and evaluate the\nimpact of the proposed relaxation schemes and optimizations on performance,\nresilience and recovery time. To the best of our knowledge, our paper is the\nfirst to discuss the persistence of security metadata in integrity-protected\nNVM systems and provide corresponding optimizations. We define a set of\nrelaxation schemes that bring trade-offs between performance and recovery time\nfor large capacity NVM systems. Our results show that our proposed design,\nTriad-NVM, can improve the throughput by an average of ~2x (relative to strict\npersistence). Moreover, Triad-NVM maintains a recovery time of less than 4\nseconds for an 8TB NVM system (30.6 seconds for 64TB), which is ~3648x faster\nthan a system without security metadata persistence.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 21:21:28 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Awad", "Amro", ""], ["Njilla", "Laurent", ""], ["Ye", "Mao", ""]]}, {"id": "1810.09442", "submitter": "Suryanarayana Murthy Durbhakula", "authors": "Suryanarayana Murthy Durbhakula", "title": "OS Scheduling Algorithms for Improving the Performance of Multithreaded\n  Workloads", "comments": "arXiv admin note: text overlap with arXiv:1809.08628", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major chip manufacturers have all introduced multicore microprocessors.\nMulti-socket systems built from these processors are used for running various\nserver applications. However to the best of our knowledge current commercial\noperating systems are not optimized for multi-threaded workloads running on\nsuch servers. Cache-to-cache transfers and remote memory accesses impact the\nperformance of such workloads. This paper presents a unified approach to\noptimizing OS scheduling algorithms for both cache-to-cache transfers and\nremote DRAM accesses that also takes cache affinity into account. By observing\nthe patterns of local and remote cache-to-cache transfers as well as local and\nremote DRAM accesses for every thread in each scheduling quantum and applying\ndifferent algorithms, we come up with a new schedule of threads for the next\nquantum taking cache affinity into account. This new schedule cuts down both\nremote cache-to-cache transfers and remote DRAM accesses for the next\nscheduling quantum and improves overall performance. We present two algorithms\nof varying complexity for optimizing cache-to-cache transfers. One of these is\na new algorithm which is relatively simpler and performs better when combined\nwith algorithms that optimize remote DRAM accesses. For optimizing remote DRAM\naccesses we present two algorithms. Though both algorithms differ in\nalgorithmic complexity we find that for our workloads they perform equally\nwell. We used three different synthetic workloads to evaluate these algorithms.\nWe also performed sensitivity analysis with respect to varying remote\ncache-to-cache transfer latency and remote DRAM latency. We show that these\nalgorithms can cut down overall latency by up to 16.79% depending on the\nalgorithm used.\n", "versions": [{"version": "v1", "created": "Sun, 21 Oct 2018 18:02:58 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Durbhakula", "Suryanarayana Murthy", ""]]}, {"id": "1810.09679", "submitter": "Vaishaal Shankar", "authors": "Vaishaal Shankar, Karl Krauth, Qifan Pu, Eric Jonas, Shivaram\n  Venkataraman, Ion Stoica, Benjamin Recht, Jonathan Ragan-Kelley", "title": "numpywren: serverless linear algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear algebra operations are widely used in scientific computing and machine\nlearning applications. However, it is challenging for scientists and data\nanalysts to run linear algebra at scales beyond a single machine. Traditional\napproaches either require access to supercomputing clusters, or impose\nconfiguration and cluster management challenges. In this paper we show how the\ndisaggregation of storage and compute resources in so-called \"serverless\"\nenvironments, combined with compute-intensive workload characteristics, can be\nexploited to achieve elastic scalability and ease of management.\n  We present numpywren, a system for linear algebra built on a serverless\narchitecture. We also introduce LAmbdaPACK, a domain-specific language designed\nto implement highly parallel linear algebra algorithms in a serverless setting.\nWe show that, for certain linear algebra algorithms such as matrix multiply,\nsingular value decomposition, and Cholesky decomposition, numpywren's\nperformance (completion time) is within 33% of ScaLAPACK, and its compute\nefficiency (total CPU-hours) is up to 240% better due to elasticity, while\nproviding an easier to use interface and better fault tolerance. At the same\ntime, we show that the inability of serverless runtimes to exploit locality\nacross the cores in a machine fundamentally limits their network efficiency,\nwhich limits performance on other algorithms such as QR factorization. This\nhighlights how cloud providers could better support these types of computations\nthrough small changes in their infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 06:32:10 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Shankar", "Vaishaal", ""], ["Krauth", "Karl", ""], ["Pu", "Qifan", ""], ["Jonas", "Eric", ""], ["Venkataraman", "Shivaram", ""], ["Stoica", "Ion", ""], ["Recht", "Benjamin", ""], ["Ragan-Kelley", "Jonathan", ""]]}, {"id": "1810.09683", "submitter": "Giuseppe Antonio Di Luna", "authors": "Roberto Baldoni, Giuseppe Antonio Di Luna, Luca Massarelli, Fabio\n  Petroni, Leonardo Querzoni", "title": "Unsupervised Features Extraction for Binary Similarity Using Graph\n  Embedding Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the binary similarity problem that consists in\ndetermining if two binary functions are similar only considering their compiled\nform. This problem is know to be crucial in several application scenarios, such\nas copyright disputes, malware analysis, vulnerability detection, etc. The\ncurrent state-of-the-art solutions in this field work by creating an embedding\nmodel that maps binary functions into vectors in $\\mathbb{R}^{n}$. Such\nembedding model captures syntactic and semantic similarity between binaries,\ni.e., similar binary functions are mapped to points that are close in the\nvector space. This strategy has many advantages, one of them is the possibility\nto precompute embeddings of several binary functions, and then compare them\nwith simple geometric operations (e.g., dot product). In [32] functions are\nfirst transformed in Annotated Control Flow Graphs (ACFGs) constituted by\nmanually engineered features and then graphs are embedded into vectors using a\ndeep neural network architecture. In this paper we propose and test several\nways to compute annotated control flow graphs that use unsupervised approaches\nfor feature learning, without incurring a human bias. Our methods are inspired\nafter techniques used in the natural language processing community (e.g., we\nuse word2vec to encode assembly instructions). We show that our approach is\nindeed successful, and it leads to better performance than previous\nstate-of-the-art solutions. Furthermore, we report on a qualitative analysis of\nfunctions embeddings. We found interesting cases in which embeddings are\nclustered according to the semantic of the original binary function.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 06:45:54 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 13:26:55 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Baldoni", "Roberto", ""], ["Di Luna", "Giuseppe Antonio", ""], ["Massarelli", "Luca", ""], ["Petroni", "Fabio", ""], ["Querzoni", "Leonardo", ""]]}, {"id": "1810.09773", "submitter": "Hamid Reza Zohouri", "authors": "Hamid Reza Zohouri", "title": "High Performance Computing with FPGAs and OpenCL", "comments": "PhD Thesis, August 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work we evaluate the potential of FPGAs for accelerating HPC\nworkloads as a more power-efficient alternative to GPUs. Using High-Level\nSynthesis and a large set of optimization techniques, we show that FPGAs can\nachieve better performance than CPUs, and better power efficiency than both\nCPUs and GPUs for typical HPC workloads. Furthermore, we show that for the\nspecific case of stencil computation, the unique architectural advantages of\nFPGAs allow them to surpass high-end CPU, Xeon Phi and GPU devices. Unlike\nprevious work, our FPGA-based stencil accelerator combines spatial blocking\nwith temporal blocking to achieve high performance without restricting input\nsize. With support for high-order stencils, we achieve the highest single-FPGA\nperformance for 2D and 3D stencil computation of any order, to this day.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 11:04:01 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 17:37:14 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 02:05:02 GMT"}, {"version": "v4", "created": "Sun, 15 Sep 2019 12:51:07 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zohouri", "Hamid Reza", ""]]}, {"id": "1810.09830", "submitter": "Francesco Salvadore PhD", "authors": "Raffaele Ponzini, Francesco Salvadore", "title": "LincoSim: a web based HPC-cloud platform for automatic virtual towing\n  tank analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new web based HPC-cloud platform for automatic\nvirtual towing tank analysis. It is well known that the design project of a new\nhull requires a continuous integration of shape hypothesis and hydrodynamics\nverifications using analytical tools, 3D computational methods, experimental\nfacilities and sea keeping trial tests. The complexity and the cost of such\ndesign tools increase considerably moving from analytical tools to sea keeping\ntrials. In order to perform a meaningful trade-off between costs and high\nquality data acquiring during the last decade the usage of 3D computational\nmodels has grown pushed also by well-known technological factors. Nevertheless,\nin the past, there were several limiting factors on the wide diffusion of 3D\ncomputational models to perform virtual towing tank data acquiring, from\nhardware and software costs to the specific technological skills needed. In\nthis work we propose an innovative high-level approach which is embodied in the\nso-called LincoSim web application in which a hypothetical designer user can\ncarry out the simulation only starting from its own geometry and a set of\nmeaningful physical parameters. LincoSim automatically manages and hides to the\nuser all the necessary details of CFD modelling and of HPC infrastructure usage\nallowing them to access, visualize and analyze the outputs from the same single\naccess point made up from the web browser. In addition to the web interface,\nthe platform includes a back-end server which implements a Cloud logic and can\nbe connected to multiple HPC machines for computing. LincoSim is currently set\nup with finite volume Open-FOAM CFD engine. A preliminary validation campaign\nhas been performed to assess the robustness and the reliability of the tool and\nis proposed as a novel approach for the development of Computer Aided\nEngineering (CAE) applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 13:11:58 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Ponzini", "Raffaele", ""], ["Salvadore", "Francesco", ""]]}, {"id": "1810.09957", "submitter": "Hanjoo Kim", "authors": "Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park,\n  Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, Nako\n  Sung, Jung-Woo Ha", "title": "NSML: Meet the MLaaS platform with a real-world case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boom of deep learning induced many industries and academies to introduce\nmachine learning based approaches into their concern, competitively. However,\nexisting machine learning frameworks are limited to sufficiently fulfill the\ncollaboration and management for both data and models. We proposed NSML, a\nmachine learning as a service (MLaaS) platform, to meet these demands. NSML\nhelps machine learning work be easily launched on a NSML cluster and provides a\ncollaborative environment which can afford development at enterprise scale.\nFinally, NSML users can deploy their own commercial services with NSML cluster.\nIn addition, NSML furnishes convenient visualization tools which assist the\nusers in analyzing their work. To verify the usefulness and accessibility of\nNSML, we performed some experiments with common examples. Furthermore, we\nexamined the collaborative advantages of NSML through three competitions with\nreal-world use cases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 04:30:44 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kim", "Hanjoo", ""], ["Kim", "Minkyu", ""], ["Seo", "Dongjoo", ""], ["Kim", "Jinwoong", ""], ["Park", "Heungseok", ""], ["Park", "Soeun", ""], ["Jo", "Hyunwoo", ""], ["Kim", "KyungHyun", ""], ["Yang", "Youngil", ""], ["Kim", "Youngkwan", ""], ["Sung", "Nako", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "1810.09958", "submitter": "Jason Knight", "authors": "Matthew Sotoudeh, Anand Venkat, Michael Anderson, Evangelos Georganas,\n  Alexander Heinecke, Jason Knight", "title": "ISA Mapper: A Compute and Hardware Agnostic Deep Learning Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain specific accelerators present new challenges and opportunities for\ncode generation onto novel instruction sets, communication fabrics, and memory\narchitectures.\n  In this paper we introduce an intermediate representation (IR) which enables\nboth deep learning computational kernels and hardware capabilities to be\ndescribed in the same IR. We then formulate and apply instruction mapping to\ndetermine the possible ways a computation can be performed on a hardware\nsystem. Next, our scheduler chooses a specific mapping and determines the data\nmovement and computation order. In order to manage the large search space of\nmappings and schedules, we developed a flexible framework that allows\nheuristics, cost models, and potentially machine learning to facilitate this\nsearch problem.\n  With this system, we demonstrate the automated extraction of matrix\nmultiplication kernels out of recent deep learning kernels such as\ndepthwise-separable convolution. In addition, we demonstrate two to five times\nbetter performance on DeepBench sized GEMMs and GRU RNN execution when compared\nto state-of-the-art (SOTA) implementations on new hardware and up to 85% of the\nperformance for SOTA implementations on existing hardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 23:54:00 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Venkat", "Anand", ""], ["Anderson", "Michael", ""], ["Georganas", "Evangelos", ""], ["Heinecke", "Alexander", ""], ["Knight", "Jason", ""]]}, {"id": "1810.09992", "submitter": "Mohammad Mohammadi Amiri Mr.", "authors": "Mohammad Mohammadi Amiri and Deniz Gunduz", "title": "Computation Scheduling for Distributed Machine Learning with Straggling\n  Workers", "comments": "Submitted for publication", "journal-ref": null, "doi": "10.1109/TSP.2019.2952051", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study scheduling of computation tasks across n workers in a large scale\ndistributed learning problem with the help of a master. Computation and\ncommunication delays are assumed to be random, and redundant computations are\nassigned to workers in order to tolerate stragglers. We consider sequential\ncomputation of tasks assigned to a worker, while the result of each computation\nis sent to the master right after its completion. Each computation round, which\ncan model an iteration of the stochastic gradient descent (SGD) algorithm, is\ncompleted once the master receives k distinct computations, referred to as the\ncomputation target. Our goal is to characterize the average completion time as\na function of the computation load, which denotes the portion of the dataset\navailable at each worker, and the computation target. We propose two\ncomputation scheduling schemes that specify the tasks assigned to each worker,\nas well as their computation schedule, i.e., the order of execution. Assuming a\ngeneral statistical model for computation and communication delays, we derive\nthe average completion time of the proposed schemes. We also establish a lower\nbound on the minimum average completion time by assuming prior knowledge of the\nrandom delays. Experimental results carried out on Amazon EC2 cluster show a\nsignificant reduction in the average completion time over existing coded and\nuncoded computing schemes. It is also shown numerically that the gap between\nthe proposed scheme and the lower bound is relatively small, confirming the\nefficiency of the proposed scheduling design.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 17:49:13 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 17:33:21 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 13:41:20 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Amiri", "Mohammad Mohammadi", ""], ["Gunduz", "Deniz", ""]]}, {"id": "1810.10194", "submitter": "Hiroki Watanabe", "authors": "Hiroki Watanabe, Shigenori Ohashi, Shigeru Fujimura, Atsushi\n  Nakadaira, Kota Hidaka, Jay Kishigami", "title": "Niji: Bitcoin Bridge Utilizing Payment Channels", "comments": "Presented at Scaling Bitcoin 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin's enormous success has inspired the development of alternative\nblockchains, such as consortium chains. Several cross-chain protocols have been\nproposed as ways of connecting these universes of individual blockchains in a\ndistributed and secure manner. In this paper, we present Niji, a new\ncross-chain protocol that allows parties to perform virtual Bitcoin payment\nsecurely on a consortium chain, without any trusted third-party or mediators.\nOur work focuses on the issue that it is difficult for a consortium chain's\ntoken to hold a stable market value, and Niji makes it possible for smart\ncontract services to acquire means of payment in the consortium chain. With the\nBitcoin payment channel built on the consortium chain, the process from payment\nto service provision runs autonomously without any interaction between parties.\nNiji introduces the concept of a transaction template to validate Bitcoin\npayments efficiently on different blockchains, and it allows a service provider\nto delegate all of its tasks for verifying state updates to a smart contract on\nthe consortium chain. We also propose a novel bi-directional payment channel\nadapted for design of the Niji protocol, which can update payments\nnon-interactively between parties. We implemented a prototype of the Niji\nprotocol and conducted an experiment measuring the computational cost and\nlatency that demonstrates the protocol's feasibility on practical platforms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 05:43:16 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Watanabe", "Hiroki", ""], ["Ohashi", "Shigenori", ""], ["Fujimura", "Shigeru", ""], ["Nakadaira", "Atsushi", ""], ["Hidaka", "Kota", ""], ["Kishigami", "Jay", ""]]}, {"id": "1810.10263", "submitter": "Dean Koro\\v{s}ak", "authors": "Emilija Stojmenova Duh, Andrej Duh, Uro\\v{s} Droftina, Tim Kos, Urban\n  Duh, Tanja Simoni\\v{c} Koro\\v{s}ak, Dean Koro\\v{s}ak", "title": "Publish-and-Flourish: decentralized co-creation and curation of\n  scholarly content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scholarly communication is today immersed in publish or perish culture that\npropels noncooperative behaviour in the sense of strategic games played by\nresearchers. Here we introduce and describe a blockchain based platform for\ndecentralized scholarly communication. The design of the platform rests on\ncommunity driven publishing reviewing processes and implements incentives that\npromote cooperative user behaviour. Key to achieve cooperation in blockchain\nbased scholarly communication is to transform a static research paper into a\nmodifiable research paper under continuous peer review process. We describe and\ndiscuss the implementation of a modifiable research paper as a smart contract\non the blockchain.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 09:22:55 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Duh", "Emilija Stojmenova", ""], ["Duh", "Andrej", ""], ["Droftina", "Uro\u0161", ""], ["Kos", "Tim", ""], ["Duh", "Urban", ""], ["Koro\u0161ak", "Tanja Simoni\u010d", ""], ["Koro\u0161ak", "Dean", ""]]}, {"id": "1810.10279", "submitter": "Luan Teylo", "authors": "Luan Teylo, L\\'ucia Maria de A. Drummond, Luciana Arantes, Pierre Sens", "title": "A Bag-of-Tasks Scheduler Tolerant to Temporal Failures in Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud platforms have emerged as a prominent environment to execute high\nperformance computing (HPC) applications providing on-demand resources as well\nas scalability. They usually offer different classes of Virtual Machines (VMs)\nwhich ensure different guarantees in terms of availability and volatility,\nprovisioning the same resource through multiple pricing models. For instance,\nin Amazon EC2 cloud, the user pays per hour for on-demand VMs while spot VMs\nare unused instances available for lower price. Despite the monetary\nadvantages, a spot VM can be terminated, stopped, or hibernated by EC2 at any\nmoment.\n  Using both hibernation-prone spot VMs (for cost sake) and on-demand VMs, we\npropose in this paper a static scheduling for HPC applications which are\ncomposed by independent tasks (bag-of-task) with deadline constraints. However,\nif a spot VM hibernates and it does not resume within a time which guarantees\nthe application's deadline, a temporal failure takes place. Our scheduling,\nthus, aims at minimizing monetary costs of bag-of-tasks applications in EC2\ncloud, respecting its deadline and avoiding temporal failures. To this end, our\nalgorithm statically creates two scheduling maps: (i) the first one contains,\nfor each task, its starting time and on which VM (i.e., an available spot or\non-demand VM with the current lowest price) the task should execute; (ii) the\nsecond one contains, for each task allocated on a VM spot in the first map, its\nstarting time and on which on-demand VM it should be executed to meet the\napplication deadline in order to avoid temporal failures. The latter will be\nused whenever the hibernation period of a spot VM exceeds a time limit.\n  Performance results from simulation with task execution traces, configuration\nof Amazon EC2 VM classes, and VMs market history confirms the effectiveness of\nour scheduling and that it tolerates temporal failures.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 10:16:19 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Teylo", "Luan", ""], ["Drummond", "L\u00facia Maria de A.", ""], ["Arantes", "Luciana", ""], ["Sens", "Pierre", ""]]}, {"id": "1810.10360", "submitter": "Quan Nguyen Hoang", "authors": "Sang-Min Choi, Jiho Park, Quan Nguyen, Andre Cronje", "title": "Fantom: A scalable framework for asynchronous distributed systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.02186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe \\emph{Fantom}, a framework for asynchronous distributed systems.\n\\emph{Fantom} is based on the Lachesis Protocol~\\cite{lachesis01}, which uses\nasynchronous event transmission for practical Byzantine fault tolerance (pBFT)\nto create a leaderless, scalable, asynchronous Directed Acyclic Graph (DAG).\n  We further optimize the \\emph{Lachesis Protocol} by introducing a\npermission-less network for dynamic participation. Root selection cost is\nfurther optimized by the introduction of an n-row flag table, as well as\noptimizing path selection by introducing domination relationships.\n  We propose an alternative framework for distributed ledgers, based on\nasynchronous partially ordered sets with logical time ordering instead of\nblockchains.\n  This paper builds upon the original proposed family of \\emph{Lachesis-class}\nconsensus protocols. We formalize our proofs into a model that can be applied\nto abstract asynchronous distributed system.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 22:32:05 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Choi", "Sang-Min", ""], ["Park", "Jiho", ""], ["Nguyen", "Quan", ""], ["Cronje", "Andre", ""]]}, {"id": "1810.10498", "submitter": "Elena Pastorelli", "authors": "Cristiano Capone and Elena Pastorelli and Bruno Golosio and Pier\n  Stanislao Paolucci", "title": "Sleep-like slow oscillations improve visual classification through\n  synaptic homeostasis and memory association in a thalamo-cortical model", "comments": "11 pages, 5 figures, v5 is the final version published on Scientific\n  Reports journal", "journal-ref": "Sci Rep 9, 8990 (2019)", "doi": "10.1038/s41598-019-45525-0", "report-no": null, "categories": "q-bio.NC cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The occurrence of sleep passed through the evolutionary sieve and is\nwidespread in animal species. Sleep is known to be beneficial to cognitive and\nmnemonic tasks, while chronic sleep deprivation is detrimental. Despite the\nimportance of the phenomenon, a complete understanding of its functions and\nunderlying mechanisms is still lacking. In this paper, we show interesting\neffects of deep-sleep-like slow oscillation activity on a simplified\nthalamo-cortical model which is trained to encode, retrieve and classify images\nof handwritten digits. During slow oscillations,\nspike-timing-dependent-plasticity (STDP) produces a differential homeostatic\nprocess. It is characterized by both a specific unsupervised enhancement of\nconnections among groups of neurons associated to instances of the same class\n(digit) and a simultaneous down-regulation of stronger synapses created by the\ntraining. This hierarchical organization of post-sleep internal representations\nfavours higher performances in retrieval and classification tasks. The\nmechanism is based on the interaction between top-down cortico-thalamic\npredictions and bottom-up thalamo-cortical projections during deep-sleep-like\nslow oscillations. Indeed, when learned patterns are replayed during sleep,\ncortico-thalamo-cortical connections favour the activation of other neurons\ncoding for similar thalamic inputs, promoting their association. Such mechanism\nhints at possible applications to artificial learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 17:06:00 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:38:12 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 15:24:40 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 15:51:14 GMT"}, {"version": "v5", "created": "Mon, 18 Nov 2019 13:01:01 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Capone", "Cristiano", ""], ["Pastorelli", "Elena", ""], ["Golosio", "Bruno", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "1810.10615", "submitter": "Guilherme Miguel Teixeira Rito", "authors": "Guilherme Rito and Herv\\'e Paulino", "title": "Scheduling computations with provably low synchronization overheads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work Stealing has been a very successful algorithm for scheduling parallel\ncomputations, and is known to achieve high performances even for computations\nexhibiting fine-grained parallelism. We present a variant of \\ws\\ that provably\navoids most synchronization overheads by keeping processors' deques entirely\nprivate by default, and only exposing work when requested by thieves. This is\nthe first paper that obtains bounds on the synchronization overheads that are\n(essentially) independent of the total amount of work, thus corresponding to a\ngreat improvement, in both algorithm design and theory, over state-of-the-art\n\\ws\\ algorithms. Consider any computation with work $T_{1}$ and critical-path\nlength $T_{\\infty}$ executed by $P$ processors using our scheduler. Our\nanalysis shows that the expected execution time is $O\\left(\\frac{T_{1}}{P} +\nT_{\\infty}\\right)$, and the expected synchronization overheads incurred during\nthe execution are at most $O\\left(\\left(C_{CAS} +\nC_{MFence}\\right)PT_{\\infty}\\right)$, where $C_{CAS}$ and $C_{MFence}$\nrespectively denote the maximum cost of executing a Compare-And-Swap\ninstruction and a Memory Fence instruction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 20:54:48 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 18:32:59 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Rito", "Guilherme", ""], ["Paulino", "Herv\u00e9", ""]]}, {"id": "1810.10632", "submitter": "Guilherme Miguel Teixeira Rito", "authors": "Guilherme Rito and Herv\\'e Paulino", "title": "On the analysis of scheduling algorithms for structured parallel\n  computations", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for scheduling structured parallel computations have been widely\nstudied in the literature. For some time now, Work Stealing is one of the most\npopular for scheduling such computations, and its performance has been studied\nin both theory and practice. Although it delivers provably good performances,\nthe effectiveness of its underlying load balancing strategy is known to be\nlimited for certain classes of computations, particularly the ones exhibiting\nirregular parallelism (e.g. depth first searches). Many studies have addressed\nthis limitation from a purely load balancing perspective, viewing computations\nas sets of independent tasks, and then analyzing the expected amount of work\nattached to each processor as the execution progresses. However, these studies\nmake strong assumptions regarding work generation which, despite being standard\nfrom a queuing theory perspective --- where work generation can be assumed to\nfollow some random distribution --- do not match the reality of structured\nparallel computations --- where the work generation is not random, only\ndepending on the structure of a computation.\n  In this paper, we introduce a formal framework for studying the performance\nof structured computation schedulers, define a criterion that is appropriate\nfor measuring their performance, and present a methodology for analyzing the\nperformance of randomized schedulers. We demonstrate the convenience of this\nmethodology by using it to prove that the performance of Work Stealing is\nlimited, and to analyze the performance of a Work Stealing and Spreading\nalgorithm, which overcomes Work Stealing's limitation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 21:31:25 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Rito", "Guilherme", ""], ["Paulino", "Herv\u00e9", ""]]}, {"id": "1810.10838", "submitter": "Francois Le Gall", "authors": "Fran\\c{c}ois Le Gall, Harumichi Nishimura and Ansis Rosmanis", "title": "Quantum Advantage for the LOCAL Model in Distributed Computing", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two central models considered in (fault-free synchronous)\ndistributed computing: the CONGEST model, in which communication channels have\nlimited bandwidth, and the LOCAL model, in which communication channels have\nunlimited bandwidth. Very recently, Le Gall and Magniez (PODC 2018) showed the\nsuperiority of quantum distributed computing over classical distributed\ncomputing in the CONGEST model. In this work we show the superiority of quantum\ndistributed computing in the LOCAL model: we exhibit a computational task that\ncan be solved in a constant number of rounds in the quantum setting but\nrequires $\\Omega(n)$ rounds in the classical setting, where $n$ denotes the\nsize of the network.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 12:03:35 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Gall", "Fran\u00e7ois Le", ""], ["Nishimura", "Harumichi", ""], ["Rosmanis", "Ansis", ""]]}, {"id": "1810.11112", "submitter": "Ammar Ahmad Awan", "authors": "Ammar Ahmad Awan, Jeroen Bedorf, Ching-Hsiang Chu, Hari Subramoni, and\n  Dhabaleswar K. Panda", "title": "Scalable Distributed DNN Training using TensorFlow and CUDA-Aware MPI:\n  Characterization, Designs, and Performance Evaluation", "comments": "10 pages, 9 figures, submitted to IEEE IPDPS 2019 for peer-review", "journal-ref": "IEEE CCGrid, 2019", "doi": "10.1109/CCGRID.2019.00064", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow has been the most widely adopted Machine/Deep Learning framework.\nHowever, little exists in the literature that provides a thorough understanding\nof the capabilities which TensorFlow offers for the distributed training of\nlarge ML/DL models that need computation and communication at scale. Most\ncommonly used distributed training approaches for TF can be categorized as\nfollows: 1) Google Remote Procedure Call (gRPC), 2) gRPC+X: X=(InfiniBand\nVerbs, Message Passing Interface, and GPUDirect RDMA), and 3) No-gRPC: Baidu\nAllreduce with MPI, Horovod with MPI, and Horovod with NVIDIA NCCL. In this\npaper, we provide an in-depth performance characterization and analysis of\nthese distributed training approaches on various GPU clusters including the Piz\nDaint system (6 on Top500). We perform experiments to gain novel insights along\nthe following vectors: 1) Application-level scalability of DNN training, 2)\nEffect of Batch Size on scaling efficiency, 3) Impact of the MPI library used\nfor no-gRPC approaches, and 4) Type and size of DNN architectures. Based on\nthese experiments, we present two key insights: 1) Overall, No-gRPC designs\nachieve better performance compared to gRPC-based approaches for most\nconfigurations, and 2) The performance of No-gRPC is heavily influenced by the\ngradient aggregation using Allreduce. Finally, we propose a truly CUDA-Aware\nMPI Allreduce design that exploits CUDA kernels and pointer caching to perform\nlarge reductions efficiently. Our proposed designs offer 5-17X better\nperformance than NCCL2 for small and medium messages, and reduces latency by\n29% for large messages. The proposed optimizations help Horovod-MPI to achieve\napproximately 90% scaling efficiency for ResNet-50 training on 64 GPUs.\nFurther, Horovod-MPI achieves 1.8X and 3.2X higher throughput than the native\ngRPC method for ResNet-50 and MobileNet, respectively, on the Piz Daint\ncluster.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 21:25:26 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Awan", "Ammar Ahmad", ""], ["Bedorf", "Jeroen", ""], ["Chu", "Ching-Hsiang", ""], ["Subramoni", "Hari", ""], ["Panda", "Dhabaleswar K.", ""]]}, {"id": "1810.11126", "submitter": "Kush Varshney", "authors": "Nelson Kibichii Bore, Ravi Kiran Raman, Isaac M. Markus, Sekou L.\n  Remy, Oliver Bent, Michael Hind, Eleftheria K. Pissadaki, Biplav Srivastava,\n  Roman Vaculin, Kush R. Varshney, and Komminist Weldemariam", "title": "Promoting Distributed Trust in Machine Learning and Computational\n  Simulation via a Blockchain Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy decisions are increasingly dependent on the outcomes of simulations\nand/or machine learning models. The ability to share and interact with these\noutcomes is relevant across multiple fields and is especially critical in the\ndisease modeling community where models are often only accessible and workable\nto the researchers that generate them. This work presents a blockchain-enabled\nsystem that establishes a decentralized trust between parties involved in a\nmodeling process. Utilizing the OpenMalaria framework, we demonstrate the\nability to store, share and maintain auditable logs and records of each step in\nthe simulation process, showing how to validate results generated by computing\nworkers. We also show how the system monitors worker outputs to rank and\nidentify faulty workers via comparison to nearest neighbors or historical\nreward spaces as a means of ensuring model quality.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 22:22:40 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Bore", "Nelson Kibichii", ""], ["Raman", "Ravi Kiran", ""], ["Markus", "Isaac M.", ""], ["Remy", "Sekou L.", ""], ["Bent", "Oliver", ""], ["Hind", "Michael", ""], ["Pissadaki", "Eleftheria K.", ""], ["Srivastava", "Biplav", ""], ["Vaculin", "Roman", ""], ["Varshney", "Kush R.", ""], ["Weldemariam", "Komminist", ""]]}, {"id": "1810.11131", "submitter": "Lei Zhang", "authors": "Lei Zhang, Diego Lai, Andriy V. Miranskyy", "title": "The Impact of Position Errors on Crowd Simulation", "comments": "37 pages, 12 figures, accepted for publication in: Simulation\n  Modelling Practice and Theory, Elsevier", "journal-ref": null, "doi": "10.1016/j.simpat.2018.10.010", "report-no": null, "categories": "cs.MA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large crowd events, there is always a potential possibility that a\nstampede accident will occur. The accident may cause injuries or even death.\nApproaches for controlling crowd flows and predicting dangerous congestion\nspots would be a boon to on-site authorities to manage the crowd and to prevent\nfatal accidents. One of the most popular approaches is real-time crowd\nsimulation based on position data from personal Global Positioning System (GPS)\ndevices. However, the accuracy of spatial data varies for different GPS\ndevices, and it is also affected by an environment in which an event takes\nplace. In this paper, we would like to assess the effect of position errors on\nstampede prediction. We propose an Automatic Real-time dEtection of Stampedes\n(ARES) method to predict stampedes for large events. We implement three\ndifferent stampede assessment methods in Menge framework and incorporate\nposition errors. Our analysis suggests that the probability of simulated\nstampede changes significantly with the increase of the magnitude of position\nerrors, which cannot be eliminated entirely with the help of classic\ntechniques, such as the Kalman filter. Thus, it is our position that novel\nstampede assessment methods should be developed, focusing on the detection of\nposition noise and the elimination of its effect.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 23:12:29 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Zhang", "Lei", ""], ["Lai", "Diego", ""], ["Miranskyy", "Andriy V.", ""]]}, {"id": "1810.11199", "submitter": "Jia Yan", "authors": "Jia Yan, Suzhi Bi, Ying-Jun Angela Zhang, Meixia Tao", "title": "Optimal Task Offloading and Resource Allocation in Mobile-Edge Computing\n  with Inter-user Task Dependency", "comments": "This paper has been accepted for publication in IEEE Transactions on\n  Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile-edge computing (MEC) has recently emerged as a cost-effective paradigm\nto enhance the computing capability of hardware-constrained wireless devices\n(WDs). In this paper, we first consider a two-user MEC network, where each WD\nhas a sequence of tasks to execute. In particular, we consider task dependency\nbetween the two WDs, where the input of a task at one WD requires the final\ntask output at the other WD. Under the considered task-dependency model, we\nstudy the optimal task offloading policy and resource allocation (e.g., on\noffloading transmit power and local CPU frequencies) that minimize the weighted\nsum of the WDs' energy consumption and task execution time. The problem is\nchallenging due to the combinatorial nature of the offloading decisions among\nall tasks and the strong coupling with resource allocation. To tackle this\nproblem, we first assume that the offloading decisions are given and derive the\nclosed-form expressions of the optimal offloading transmit power and local CPU\nfrequencies. Then, an efficient bi-section search method is proposed to obtain\nthe optimal solutions. Furthermore, we prove that the optimal offloading\ndecisions follow an one-climb policy, based on which a reduced-complexity Gibbs\nSampling algorithm is proposed to obtain the optimal offloading decisions. We\nthen extend the investigation to a general multi-user scenario, where the input\nof a task at one WD requires the final task outputs from multiple other WDs.\nNumerical results show that the proposed method can significantly outperform\nthe other representative benchmarks and efficiently achieve low complexity with\nrespect to the call graph size.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 06:32:43 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 13:11:10 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Yan", "Jia", ""], ["Bi", "Suzhi", ""], ["Zhang", "Ying-Jun Angela", ""], ["Tao", "Meixia", ""]]}, {"id": "1810.11208", "submitter": "Alessio Netti", "authors": "Alessio Netti, Zeynep Kiziltan, Ozalp Babaoglu, Alina Sirbu, Andrea\n  Bartolini and Andrea Borghesi", "title": "Online Fault Classification in HPC Systems through Machine Learning", "comments": "Accepted for publication at the Euro-Par 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As High-Performance Computing (HPC) systems strive towards the exascale goal,\nstudies suggest that they will experience excessive failure rates. For this\nreason, detecting and classifying faults in HPC systems as they occur and\ninitiating corrective actions before they can transform into failures will be\nessential for continued operation. In this paper, we propose a fault\nclassification method for HPC systems based on machine learning that has been\ndesigned specifically to operate with live streamed data. We cast the problem\nand its solution within realistic operating constraints of online use. Our\nresults show that almost perfect classification accuracy can be reached for\ndifferent fault types with low computational overhead and minimal delay. We\nhave based our study on a local dataset, which we make publicly available, that\nwas acquired by injecting faults to an in-house experimental HPC system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 07:27:56 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 14:55:30 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Netti", "Alessio", ""], ["Kiziltan", "Zeynep", ""], ["Babaoglu", "Ozalp", ""], ["Sirbu", "Alina", ""], ["Bartolini", "Andrea", ""], ["Borghesi", "Andrea", ""]]}, {"id": "1810.11226", "submitter": "Frank Berghaus", "authors": "Frank Berghaus, Kevin Casteels, Alessandro Di Girolamo, Colson\n  Driemel, Marcus Ebert, Fabrizio Furano, Fernado Galindo, Mario Lassnig, Colin\n  Leavett-Brown, Michael Paterson, Cedric Serfon, Rolf Seuster, Randall Sobie,\n  Reda Tafirout and Ryan Paul Taylor", "title": "Federating distributed storage for clouds in ATLAS", "comments": "5 pages, 2 figures, 1 table, ACAT2017 proceedings", "journal-ref": "J.Phys.Conf.Ser. 1085 (2018) no.3, 032027", "doi": "10.1088/1742-6596/1085/3/032027", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Input data for applications that run in cloud computing centres can be stored\nat distant repositories, often with multiple copies of the popular data stored\nat many sites. Locating and retrieving the remote data can be challenging, and\nwe believe that federating the storage can address this problem. A federation\nwould locate the closest copy of the data on the basis of GeoIP information.\nCurrently we are using the dynamic data federation Dynafed, a software solution\ndeveloped by CERN IT. Dynafed supports several industry standards for\nconnection protocols like Amazon's S3, Microsoft's Azure, as well as WebDAV and\nHTTP. Dynafed functions as an abstraction layer under which protocol-dependent\nauthentication details are hidden from the user, requiring the user to only\nprovide an X509 certificate. We have setup an instance of Dynafed and\nintegrated it into the ATLAS data distribution management system. We report on\nthe challenges faced during the installation and integration. We have tested\nATLAS analysis jobs submitted by the PanDA production system and we report on\nour first experiences with its operation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 08:34:27 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Berghaus", "Frank", ""], ["Casteels", "Kevin", ""], ["Di Girolamo", "Alessandro", ""], ["Driemel", "Colson", ""], ["Ebert", "Marcus", ""], ["Furano", "Fabrizio", ""], ["Galindo", "Fernado", ""], ["Lassnig", "Mario", ""], ["Leavett-Brown", "Colin", ""], ["Paterson", "Michael", ""], ["Serfon", "Cedric", ""], ["Seuster", "Rolf", ""], ["Sobie", "Randall", ""], ["Tafirout", "Reda", ""], ["Taylor", "Ryan Paul", ""]]}, {"id": "1810.11268", "submitter": "Cristian Ramon-Cortes", "authors": "Cristian Ramon-Cortes and Ramon Amela and Jorge Ejarque and Philippe\n  Clauss and Rosa M. Badia", "title": "AutoParallel: A Python module for automatic parallelization and\n  distributed execution of affine loop nests", "comments": "Accepted to the 8th Workshop on Python for High-Performance and\n  Scientific Computing (PyHPC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last improvements in programming languages, programming models, and\nframeworks have focused on abstracting the users from many programming issues.\nAmong others, recent programming frameworks include simpler syntax, automatic\nmemory management and garbage collection, which simplifies code re-usage\nthrough library packages, and easily configurable tools for deployment. For\ninstance, Python has risen to the top of the list of the programming languages\ndue to the simplicity of its syntax, while still achieving a good performance\neven being an interpreted language. Moreover, the community has helped to\ndevelop a large number of libraries and modules, tuning them to obtain great\nperformance.\n  However, there is still room for improvement when preventing users from\ndealing directly with distributed and parallel computing issues. This paper\nproposes and evaluates AutoParallel, a Python module to automatically find an\nappropriate task-based parallelization of affine loop nests to execute them in\nparallel in a distributed computing infrastructure. This parallelization can\nalso include the building of data blocks to increase task granularity in order\nto achieve a good execution performance. Moreover, AutoParallel is based on\nsequential programming and only contains a small annotation in the form of a\nPython decorator so that anyone with little programming skills can scale up an\napplication to hundreds of cores.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 11:17:21 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Ramon-Cortes", "Cristian", ""], ["Amela", "Ramon", ""], ["Ejarque", "Jorge", ""], ["Clauss", "Philippe", ""], ["Badia", "Rosa M.", ""]]}, {"id": "1810.11287", "submitter": "Csaba Kiraly", "authors": "Rom\\'an Sosa, Csaba Kiraly, Juan D. Parra Rodriguez", "title": "Offloading Execution from Edge to Cloud: a Dynamic Node-RED Based\n  Approach", "comments": "The 10th IEEE International Conference on Cloud Computing Technology\n  and Science (CloudCom 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing enables use cases where data produced in end devices are\nstored, processed, and acted on directly at the edges of the network, yet\ncomputation can be offloaded to more powerful instances through the edge to\ncloud continuum. Such offloading mechanism is especially needed in case of\nmodern multi-purpose IoT gateways, where both demand and operation conditions\ncan vary largely between deployments. To facilitate the development and\noperations of gateways, we implement offloading directly as part of the IoT\nrapid prototyping process embedded in the software stack, based on Node-RED. We\nevaluate the implemented method using an image processing example, and compare\nvarious offloading strategies based on resource consumption and other system\nmetrics, highlighting the differences in handling demand and service levels\nreached.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:26:18 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Sosa", "Rom\u00e1n", ""], ["Kiraly", "Csaba", ""], ["Rodriguez", "Juan D. Parra", ""]]}, {"id": "1810.11319", "submitter": "Ruben Mayer", "authors": "Christian Mayer and Ruben Mayer and Sukanya Bhowmik and Lukas Epple\n  and Kurt Rothermel", "title": "HYPE: Massive Hypergraph Partitioning with Neighborhood Expansion", "comments": "To appear in Proceedings of IEEE 2018 International Conference on Big\n  Data (BigData '18), 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important real-world applications-such as social networks or distributed\ndata bases-can be modeled as hypergraphs. In such a model, vertices represent\nentities-such as users or data records-whereas hyperedges model a group\nmembership of the vertices-such as the authorship in a specific topic or the\nmembership of a data record in a specific replicated shard. To optimize such\napplications, we need an efficient and effective solution to the NP-hard\nbalanced k-way hypergraph partitioning problem. However, existing hypergraph\npartitioners that scale to very large graphs do not effectively exploit the\nhypergraph structure when performing the partitioning decisions. We propose\nHYPE, a hypergraph partitionier that exploits the neighborhood relations\nbetween vertices in the hypergraph using an efficient implementation of\nneighborhood expansion. HYPE improves partitioning quality by up to 95% and\nreduces runtime by up to 39% compared to streaming partitioning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 13:35:55 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 09:22:01 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 11:42:15 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 10:59:29 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Mayer", "Christian", ""], ["Mayer", "Ruben", ""], ["Bhowmik", "Sukanya", ""], ["Epple", "Lukas", ""], ["Rothermel", "Kurt", ""]]}, {"id": "1810.11441", "submitter": "Bogdan Chlebus", "authors": "Bogdan S. Chlebus and Elijah Hradovich and Tomasz Jurdzinski and Marek\n  Klonowski and Dariusz R. Kowalski", "title": "Energy Efficient Adversarial Routing in Shared Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate routing on networks modeled as multiple access channels, when\npackets are injected continually. There is an energy cap understood as a bound\non the number of stations that can be switched on simultaneously. Each packet\nis injected into some station and needs to be delivered to its destination\nstation via the channel. A station has to be switched on in order to receive a\npacket when it is heard on the channel. Each station manages when it is\nswitched on and off by way of a programmable wakeup mechanism, which is\nscheduled by a routing algorithm. Packet injection is governed by adversarial\nmodels that determine upper bounds on injection rates and burstiness. We\ndevelop deterministic distributed routing algorithms and assess their\nperformance in the worst-case sense. One of the algorithms maintains bounded\nqueues for the maximum injection rate 1 subject only to the energy cap 3. This\nenergy cap is provably optimal, in that obtaining the same throughput with the\nenergy cap 2 is impossible. We give algorithms subject to the minimum energy\ncap 2 that have latency polynomial in the total number of stations n for each\nfixed adversary of injection rate less than 1. An algorithm is\nk-energy-oblivious if at most k stations are switched on in a round and for\neach station the rounds when it will be switched on are determined in advance.\nWe give a k-energy-oblivious algorithm that has packet delay O(n) for\nadversaries of injection rates less than (k-1)/(n-1), and show that there is no\nk-energy-oblivious stable algorithm against adversaries with injection rates\ngreater than k/n. We give a k-energy-oblivious algorithm routing directly that\nhas latency O(n^2/k) for adversaries of sufficiently small injection rates that\nare O(k^2/n^2). We show that no k-energy-oblivious algorithm routing directly\ncan be stable against adversaries with injection rates greater than\nk(k-1)/n(n-1).\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 17:44:48 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 21:20:38 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Chlebus", "Bogdan S.", ""], ["Hradovich", "Elijah", ""], ["Jurdzinski", "Tomasz", ""], ["Klonowski", "Marek", ""], ["Kowalski", "Dariusz R.", ""]]}, {"id": "1810.11451", "submitter": "Ga\\'etan Hains", "authors": "G. Hains, W. Suijlen, W. Liang and Z. Wu", "title": "5Gperf: signal processing performance for 5G", "comments": null, "journal-ref": null, "doi": null, "report-no": "PADAL-TR-2018-2", "categories": "eess.SP cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 5Gperf project was conducted by Huawei research teams in 2016-17. It was\nconcerned with the acceleration of signal-processing algorithms for a 5G\nbase-station prototype. It improved on already optimized SIMD-parallel CPU\nalgorithms and designed a new software tool for higher programmer productivity\nwhen converting MATLAB code to optimized C\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 11:14:01 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Hains", "G.", ""], ["Suijlen", "W.", ""], ["Liang", "W.", ""], ["Wu", "Z.", ""]]}, {"id": "1810.11482", "submitter": "Patrick Diehl", "authors": "Patrick Diehl and Madhavan Seshadri and Thomas Heller and Hartmut\n  Kaiser", "title": "Integration of CUDA Processing within the C++ library for parallelism\n  and concurrency (HPX)", "comments": null, "journal-ref": null, "doi": "10.1109/ESPM2.2018.00006", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience shows that on today's high performance systems the utilization of\ndifferent acceleration cards in conjunction with a high utilization of all\nother parts of the system is difficult. Future architectures, like exascale\nclusters, are expected to aggravate this issue as the number of cores are\nexpected to increase and memory hierarchies are expected to become deeper. One\nbig aspect for distributed applications is to guarantee high utilization of all\navailable resources, including local or remote acceleration cards on a cluster\nwhile fully using all the available CPU resources and the integration of the\nGPU work into the overall programming model.\n  For the integration of CUDA code we extended HPX, a general purpose C++ run\ntime system for parallel and distributed applications of any scale, and enabled\nasynchronous data transfers from and to the GPU device and the asynchronous\ninvocation of CUDA kernels on this data. Both operations are well integrated\ninto the general programming model of HPX which allows to seamlessly overlap\nany GPU operation with work on the main cores. Any user defined CUDA kernel can\nbe launched on any (local or remote) GPU device available to the distributed\napplication.\n  We present asynchronous implementations for the data transfers and kernel\nlaunches for CUDA code as part of a HPX asynchronous execution graph. Using\nthis approach we can combine all remotely and locally available acceleration\ncards on a cluster to utilize its full performance capabilities. Overhead\nmeasurements show, that the integration of the asynchronous operations (data\ntransfer + launches of the kernels) as part of the HPX execution graph imposes\nno additional computational overhead and significantly eases orchestrating\ncoordinated and concurrent work on the main cores and the used GPU devices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:23:35 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Diehl", "Patrick", ""], ["Seshadri", "Madhavan", ""], ["Heller", "Thomas", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "1810.11613", "submitter": "Tianyi Chen", "authors": "Tianyi Chen, Sergio Barbarossa, Xin Wang, Georgios B. Giannakis, and\n  Zhi-Li Zhang", "title": "Learning and Management for Internet-of-Things: Accounting for\n  Adaptivity and Scalability", "comments": "Submitted on June 15 to Proceeding of IEEE Special Issue on Adaptive\n  and Scalable Communication Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-of-Things (IoT) envisions an intelligent infrastructure of networked\nsmart devices offering task-specific monitoring and control services. The\nunique features of IoT include extreme heterogeneity, massive number of\ndevices, and unpredictable dynamics partially due to human interaction. These\ncall for foundational innovations in network design and management. Ideally, it\nshould allow efficient adaptation to changing environments, and low-cost\nimplementation scalable to massive number of devices, subject to stringent\nlatency constraints. To this end, the overarching goal of this paper is to\noutline a unified framework for online learning and management policies in IoT\nthrough joint advances in communication, networking, learning, and\noptimization. From the network architecture vantage point, the unified\nframework leverages a promising fog architecture that enables smart devices to\nhave proximity access to cloud functionalities at the network edge, along the\ncloud-to-things continuum. From the algorithmic perspective, key innovations\ntarget online approaches adaptive to different degrees of nonstationarity in\nIoT dynamics, and their scalable model-free implementation under limited\nfeedback that motivates blind or bandit approaches. The proposed framework\naspires to offer a stepping stone that leads to systematic designs and analysis\nof task-specific learning and management schemes for IoT, along with a host of\nnew research directions to build on.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 07:33:17 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chen", "Tianyi", ""], ["Barbarossa", "Sergio", ""], ["Wang", "Xin", ""], ["Giannakis", "Georgios B.", ""], ["Zhang", "Zhi-Li", ""]]}, {"id": "1810.11655", "submitter": "Sabine Bertram", "authors": "Sabine Bertram and Co-Pierre Georg", "title": "A privacy-preserving system for data ownership using blockchain and\n  distributed databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain has the potential to revolutionize the way we store, use, and\nprocess data. Information on most blockchains can be viewed by every node\nhosting the blockchain, which means that most blockchains cannot handle private\ndata. Decentralized databases exist that guarantee privacy by encrypting user\ndata with the user's private key, but this prevents easy data sharing. However,\nin many real world applications, from student data to medical records, it is\ndesirable that user data is anonymously searchable. In this paper we present a\nnovel system that gives users ownership over their data while at the same time\nenabling them to make their data searchable within previously agreed upon\nlimits. Our system implements a strong notion of ownership using a\nself-sovereign identity system and a weak notion of ownership using multiple\ncentralized databases together with a blockchain and a tumbling process. We\ndiscuss applications of our methods to university's student records and medical\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 14:44:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Bertram", "Sabine", ""], ["Georg", "Co-Pierre", ""]]}, {"id": "1810.11772", "submitter": "Huda Ibeid", "authors": "Huda Ibeid, Siping Meng, Oliver Dobon, Luke Olson, William Gropp", "title": "Learning with Analytical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand and predict the performance of scientific applications, several\nanalytical and machine learning approaches have been proposed, each having its\nadvantages and disadvantages. In this paper, we propose and validate a hybrid\napproach for performance modeling and prediction, which combines analytical and\nmachine learning models. The proposed hybrid model aims to minimize prediction\ncost while providing reasonable prediction accuracy. Our validation results\nshow that the hybrid model is able to learn and correct the analytical models\nto better match the actual performance. Furthermore, the proposed hybrid model\nimproves the prediction accuracy in comparison to pure machine learning\ntechniques while using small training datasets, thus making it suitable for\nhardware and workload changes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 07:19:23 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 00:20:20 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Ibeid", "Huda", ""], ["Meng", "Siping", ""], ["Dobon", "Oliver", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1810.11787", "submitter": "Karanbir Chahal", "authors": "Karanbir Chahal, Manraj Singh Grover, Kuntal Dey", "title": "A Hitchhiker's Guide On Distributed Training of Deep Neural Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has led to tremendous advancements in the field of Artificial\nIntelligence. One caveat however is the substantial amount of compute needed to\ntrain these deep learning models. Training a benchmark dataset like ImageNet on\na single machine with a modern GPU can take upto a week, distributing training\non multiple machines has been observed to drastically bring this time down.\nRecent work has brought down ImageNet training time to a time as low as 4\nminutes by using a cluster of 2048 GPUs. This paper surveys the various\nalgorithms and techniques used to distribute training and presents the current\nstate of the art for a modern distributed training framework. More\nspecifically, we explore the synchronous and asynchronous variants of\ndistributed Stochastic Gradient Descent, various All Reduce gradient\naggregation strategies and best practices for obtaining higher throughout and\nlower latency over a cluster such as mixed precision training, large batch\ntraining and gradient compression.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 09:37:47 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chahal", "Karanbir", ""], ["Grover", "Manraj Singh", ""], ["Dey", "Kuntal", ""]]}, {"id": "1810.11871", "submitter": "Jinwook Lee Ph.D.", "authors": "Jinwook Lee, Paul Moon Sub Choi", "title": "Chain of Antichains: An Efficient and Secure Distributed Ledger\n  Technology and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the inception of blockchain and Bitcoin (Nakamoto (2008)), a\ndecentralized-distributed ledger system and its associated cryptocurrency,\nrespectively, the world has witnessed a slew of newer adaptations and\napplications. Although the original distributed ledger technology (DLT) of\nblockchain is deemed secure and decentralized, the confirmation of transactions\nis inefficient by design. Recently adopted, directed acyclic graph (DAG)-based\ndistributed ledgers validate transactions efficiently without the physically\nand environmentally costly building process of blocks (Lerner (2015)). However,\ncentrally-controlled confirmation against the odds of multiple validation\ndisqualifies the DAG as a decentralized-distributed ledger. In this regard, we\nintroduce an innovative DLT by reconstructing a chain of antichains based on a\ngiven DAG-pool of transactions. Each antichain (box) contains distinct nodes\nwhose approved transactions are recursively validated by subsequently\naugmenting nodes. The boxer node closes the box and keeps the hash of all\ntransactions confirmed by the box-genesis node. Designation of boxers and\nbox-geneses is conditionally randomized for decentralization. The boxes are\nserially concatenated with recursive confirmation (boxchain) without incurring\nthe cost of box generation. Rewards (boxcoin) are paid to the contributing\nnodes of the ecosystem whose trust is built on the doubly-secure protocol of\nconfirmation. A value-preserving medium of payment (boxdollar) is among\nnumerous practical applications discussed herein.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 19:52:02 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 00:51:18 GMT"}, {"version": "v3", "created": "Tue, 1 Jan 2019 23:24:56 GMT"}, {"version": "v4", "created": "Fri, 11 Jan 2019 15:00:02 GMT"}, {"version": "v5", "created": "Mon, 14 Jan 2019 18:47:26 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Lee", "Jinwook", ""], ["Choi", "Paul Moon Sub", ""]]}, {"id": "1810.11883", "submitter": "Huda Ibeid", "authors": "Huda Ibeid, Luke Olson, William Gropp", "title": "FFT, FMM, and Multigrid on the Road to Exascale: performance challenges\n  and opportunities", "comments": null, "journal-ref": null, "doi": "10.1016/j.jpdc.2019.09.014", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FFT, FMM, and multigrid methods are widely used fast and highly scalable\nsolvers for elliptic PDEs. However, emerging large-scale computing systems are\nintroducing challenges in comparison to current petascale computers. Recent\nefforts (Dongarra et al. 2011) have identified several constraints in the\ndesign of exascale software that includes massive concurrency, resilience\nmanagement, exploiting the high performance of heterogeneous systems, energy\nefficiency, and utilizing the deeper and more complex memory hierarchy expected\nat exascale. In this paper, we perform a model-based comparison of the FFT,\nFMM, and multigrid methods in the context of these projected constraints. In\naddition, we use performance models to offer predictions about the expected\nperformance on upcoming exascale system configurations based on current\ntechnology trends.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 21:05:20 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 19:12:01 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ibeid", "Huda", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1810.12092", "submitter": "Anna Engelmann", "authors": "Anna Engelmann, Admela Jukan, Rastin Pries", "title": "On Coding for Reliable VNF Chaining in DCNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how erasure coding can improve service reliability in Data Center\nNetworks (DCN). To this end, we find that coding can be best deployed in\nsystems, where i) traffic is split into multiple parallel sub-flows, ii) each\nsub-flow is encoded; iii) SFC along with their corresponding Virtual Network\nFunctions (VNF) concatenated are replicated into at least as many VNF instances\nas there are sub-flows, resulting in parallel sub- SFCs; and iv) all coded\nsub-flows are distributed over parallel paths and processed in parallel. We\nstudy service reliability as function of the level of parallelization within\nDCN and the resulting amount of redundancy. Based on the probability theory and\nby considering failures of path segments, VNF and server failures, we\nanalytically derive the probability that parallel subflows are successfully\nprocessed by the parallelized SFC and that the original serial traffic can be\nsuccessfully recovered without service interruptions.We compare the proposed\nfailure protection with coding and the standard backup protection and evaluate\nthe related overhead of both methods, including decoding, traffic redirection\nand VNF migration. The results not only show the benefit of our scheme for\nreliability, but also a reduced overhead required in comparison to backup\nprotection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 13:16:39 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Engelmann", "Anna", ""], ["Jukan", "Admela", ""], ["Pries", "Rastin", ""]]}, {"id": "1810.12137", "submitter": "Jianlei Yang", "authors": "Wenzhi Fu, Jianlei Yang, Pengcheng Dai, Yiran Chen, Weisheng Zhao", "title": "A Scalable Pipelined Dataflow Accelerator for Object Region Proposals on\n  FPGA Platform", "comments": "accepted by FPT 2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Region proposal is critical for object detection while it usually poses a\nbottleneck in improving the computation efficiency on traditional control-flow\narchitectures. We have observed region proposal tasks are potentially suitable\nfor performing pipelined parallelism by exploiting dataflow driven\nacceleration. In this paper, a scalable pipelined dataflow accelerator is\nproposed for efficient region proposals on FPGA platform. The accelerator\nprocesses image data by a streaming manner with three sequential stages:\nresizing, kernel computing and sorting. First, Ping-Pong cache strategy is\nadopted for rotation loading in resize module to guarantee continuous output\nstreaming. Then, a multiple pipelines architecture with tiered memory is\nutilized in kernel computing module to complete the main computation tasks.\nFinally, a bubble-pushing heap sort method is exploited in sorting module to\nfind the top-k largest candidates efficiently. Our design is implemented with\nhigh level synthesis on FPGA platforms, and experimental results on VOC2007\ndatasets show that it could achieve about 3.67X speedups than traditional\ndesktop CPU platform and >250X energy efficiency improvement than embedded ARM\nplatform.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 12:40:31 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Fu", "Wenzhi", ""], ["Yang", "Jianlei", ""], ["Dai", "Pengcheng", ""], ["Chen", "Yiran", ""], ["Zhao", "Weisheng", ""]]}, {"id": "1810.12292", "submitter": "Mansaf Alam Dr", "authors": "Preeti Agarwal, Mansaf Alam", "title": "Investigating IoT Middleware Platforms for Smart Application Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing number of Internet of Things (IoT) devices, the data\ngenerated through these devices is also increasing. By 2030, it is been\npredicted that the number of IoT devices will exceed the number of human beings\non earth. This gives rise to the requirement of middleware platform that can\nmanage IoT devices, intelligently store and process gigantic data generated for\nbuilding smart applications such as Smart Cities, Smart Healthcare, Smart\nIndustry, and others. At present, market is overwhelming with the number of IoT\nmiddleware platforms with specific features. This raises one of the most\nserious and least discussed challenge for application developer to choose\nsuitable platform for their application development. Across the literature,\nvery little attempt is done in classifying or comparing IoT middleware\nplatforms for the applications. This paper categorizes IoT platforms into four\ncategories namely-publicly traded, open source, developer friendly and\nend-to-end connectivity. Some of the popular middleware platforms in each\ncategory are investigated based on general IoT architecture. Comparison of IoT\nmiddleware platforms in each category, based on basic, sensing, communication\nand application development features is presented. This study can be useful for\nIoT application developers to select the most appropriate platform according to\ntheir application requirement.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 10:02:06 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 06:11:34 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Agarwal", "Preeti", ""], ["Alam", "Mansaf", ""]]}, {"id": "1810.12297", "submitter": "Shoumik Palkar", "authors": "Shoumik Palkar and Matei Zaharia", "title": "Optimizing Data-Intensive Computations in Existing Libraries with Split\n  Annotations", "comments": "Appearing in SOSP 2019, Huntsville, ON, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data movement between main memory and the CPU is a major bottleneck in\nparallel data-intensive applications. In response, researchers have proposed\nusing compilers and intermediate representations (IRs) that apply optimizations\nsuch as loop fusion under existing high-level APIs such as NumPy and\nTensorFlow. Even though these techniques generally do not require changes to\nuser applications, they require intrusive changes to the library itself: often,\nlibrary developers must rewrite each function using a new IR. In this paper, we\npropose a new technique called split annotations (SAs) that enables key data\nmovement optimizations over unmodified library functions. SAs only require\ndevelopers to annotate functions and implement an API that specifies how to\npartition data in the library. The annotation and API describe how to enable\ncross-function data pipelining and parallelization, while respecting each\nfunction's correctness constraints. We implement a parallel runtime for SAs in\na system called Mozart. We show that Mozart can accelerate workloads in\nlibraries such as Intel MKL and Pandas by up to 15x, with no library\nmodifications. Mozart also provides performance gains competitive with\nsolutions that require rewriting libraries, and can sometimes outperform these\nsystems by up to 2x by leveraging existing hand-optimized code.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 17:30:35 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 23:13:24 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Palkar", "Shoumik", ""], ["Zaharia", "Matei", ""]]}, {"id": "1810.12457", "submitter": "Milind Rao", "authors": "Milind Rao, Stefano Rini, Andrea Goldsmith", "title": "Distributed Convex Optimization With Limited Communications", "comments": "Extended version of submission to IEEE ICASSP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a distributed convex optimization algorithm, termed\n\\emph{distributed coordinate dual averaging} (DCDA) algorithm, is proposed. The\nDCDA algorithm addresses the scenario of a large distributed optimization\nproblem with limited communication among nodes in the network. Currently known\ndistributed subgradient methods, such as the distributed dual averaging or the\ndistributed alternating direction method of multipliers algorithms, assume that\nnodes can exchange messages of large cardinality. Such network communication\ncapabilities are not valid in many scenarios of practical relevance. In the\nDCDA algorithm, on the other hand, communication of each coordinate of the\noptimization variable is restricted over time. For the proposed algorithm, we\nbound the rate of convergence under different communication protocols and\nnetwork architectures. We also consider the extensions to the case of imperfect\ngradient knowledge and the case in which transmitted messages are corrupted by\nadditive noise or are quantized. Relevant numerical simulations are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 23:42:37 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Rao", "Milind", ""], ["Rini", "Stefano", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1810.12596", "submitter": "Zhijie Ren", "authors": "Zhijie Ren and Zekeriya Erkin", "title": "VAPOR: a Value-Centric Blockchain that is Scale-out, Decentralized, and\n  Flexible by Design", "comments": "To be appeared in Financial Crypto 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains is a special type of distributed systems that operates in unsafe\nnetworks. In most blockchains, all nodes should reach consensus on all state\ntransitions with Byzantine fault tolerant algorithms, which creates bottlenecks\nin performance. In this paper, we propose a new type of blockchains, namely\nValue-Centric Blockchains (VCBs), in which the states are specified as values\n(or more comprehensively, coins) with owners and the state transition records\nare then specified as proofs of the ownerships of individual values. We then\nformalize the \"rational\" assumptions that have been used in most blockchains.\nWe further propose a VCB, VAPOR, that guarantees secure value transfers if all\nnodes are rational and keep the proofs of the values they owned, which is\nmerely parts of the whole state transition record. As a result, we show that\nVAPOR enjoys significant benefits in throughput, decentralization, and\nflexibility without compromising security.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 09:19:00 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 09:59:37 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Ren", "Zhijie", ""], ["Erkin", "Zekeriya", ""]]}, {"id": "1810.12640", "submitter": "Lyes Khacef", "authors": "Lyes Khacef, Bernard Girau, Nicolas Rougier, Andres Upegui, Benoit\n  Miramond", "title": "Neuromorphic hardware as a self-organizing computing system", "comments": "Published in IEEE World Congress on Computational Intelligence\n  (WCCI), International Workshop: Neuromorphic Hardware in Practice and Use\n  (NHPU), Jul. 2018, Rio de Janeiro, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the self-organized neuromorphic architecture named SOMA.\nThe objective is to study neural-based self-organization in computing systems\nand to prove the feasibility of a self-organizing hardware structure.\nConsidering that these properties emerge from large scale and fully connected\nneural maps, we will focus on the definition of a self-organizing hardware\narchitecture based on digital spiking neurons that offer hardware efficiency.\nFrom a biological point of view, this corresponds to a combination of the\nso-called synaptic and structural plasticities. We intend to define\ncomputational models able to simultaneously self-organize at both computation\nand communication levels, and we want these models to be hardware-compliant,\nfault tolerant and scalable by means of a neuro-cellular structure.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:35:07 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Khacef", "Lyes", ""], ["Girau", "Bernard", ""], ["Rougier", "Nicolas", ""], ["Upegui", "Andres", ""], ["Miramond", "Benoit", ""]]}, {"id": "1810.12795", "submitter": "Jia Kan", "authors": "Jia Kan and Lingyi Zou and Bella Liu and Xin Huang", "title": "Boost Blockchain Broadcast Propagation with Tree Routing", "comments": "10 pages, SmartBlock 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the rapid development and popularization of BitCoin,\nthe research of blockchain technology has also shown growth. It has gradually\nbecome a new generation of distributed, non-centralized and trust-based\ntechnology solution. However, the blockchain operation is expensive and\ntransaction is delayed. Take BitCoin as an example. On the one hand, a block is\nproduced every ten minute. On the other hand, once the new block is generated,\nit takes a certain time to propagate world wide. The slow speed of propagation\ndetermines that BitCoin can not use too small block interval time. Ethereum\nalso faces similar problems, so the concept of uncle block was introduced to\nreduce blockchain forks. This paper introduces a new tree structure based\nbroadcast propagation routing model, providing a novel method to organize\nnetwork nodes and message propagation mechanism. In oder to avoid the single\nnode failure problem, the tree cluster routing is proposed. The research shows\nthat the tree based routing can accelerate broadcast convergence time and\nreduce redundant traffic.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 15:04:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Kan", "Jia", ""], ["Zou", "Lingyi", ""], ["Liu", "Bella", ""], ["Huang", "Xin", ""]]}, {"id": "1810.12910", "submitter": "Muhammad Abdullah Hanif", "authors": "Muhammad Abdullah Hanif, Rachmad Vidya Wicaksana Putra, Muhammad\n  Tanvir, Rehan Hafiz, Semeen Rehman, Muhammad Shafique", "title": "MPNA: A Massively-Parallel Neural Array Accelerator with Dataflow\n  Optimization for Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art accelerators for Convolutional Neural Networks (CNNs)\ntypically focus on accelerating only the convolutional layers, but do not\nprioritize the fully-connected layers much. Hence, they lack a synergistic\noptimization of the hardware architecture and diverse dataflows for the\ncomplete CNN design, which can provide a higher potential for\nperformance/energy efficiency. Towards this, we propose a novel\nMassively-Parallel Neural Array (MPNA) accelerator that integrates two\nheterogeneous systolic arrays and respective highly-optimized dataflow patterns\nto jointly accelerate both the convolutional (CONV) and the fully-connected\n(FC) layers. Besides fully-exploiting the available off-chip memory bandwidth,\nthese optimized dataflows enable high data-reuse of all the data types (i.e.,\nweights, input and output activations), and thereby enable our MPNA to achieve\nhigh energy savings. We synthesized our MPNA architecture using the ASIC design\nflow for a 28nm technology, and performed functional and timing validation\nusing multiple real-world complex CNNs. MPNA achieves 149.7GOPS/W at 280MHz and\nconsumes 239mW. Experimental results show that our MPNA architecture provides\n1.7x overall performance improvement compared to state-of-the-art accelerator,\nand 51% energy saving compared to the baseline architecture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 17:20:24 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Hanif", "Muhammad Abdullah", ""], ["Putra", "Rachmad Vidya Wicaksana", ""], ["Tanvir", "Muhammad", ""], ["Hafiz", "Rehan", ""], ["Rehman", "Semeen", ""], ["Shafique", "Muhammad", ""]]}, {"id": "1810.13029", "submitter": "Benjamin Brock", "authors": "Benjamin Brock, Ayd{\\i}n Bulu\\c{c}, Katherine Yelick", "title": "BCL: A Cross-Platform Distributed Container Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-sided communication is a useful paradigm for irregular parallel\napplications, but most one-sided programming environments, including MPI's\none-sided interface and PGAS programming languages, lack application level\nlibraries to support these applications. We present the Berkeley Container\nLibrary, a set of generic, cross-platform, high-performance data structures for\nirregular applications, including queues, hash tables, Bloom filters and more.\nBCL is written in C++ using an internal DSL called the BCL Core that provides\none-sided communication primitives such as remote get and remote put\noperations. The BCL Core has backends for MPI, OpenSHMEM, GASNet-EX, and UPC++,\nallowing BCL data structures to be used natively in programs written using any\nof these programming environments. Along with our internal DSL, we present the\nBCL ObjectContainer abstraction, which allows BCL data structures to\ntransparently serialize complex data types while maintaining efficiency for\nprimitive types. We also introduce the set of BCL data structures and evaluate\ntheir performance across a number of high-performance computing systems,\ndemonstrating that BCL programs are competitive with hand-optimized code, even\nwhile hiding many of the underlying details of message aggregation,\nserialization, and synchronization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 23:17:34 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 18:04:21 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Brock", "Benjamin", ""], ["Bulu\u00e7", "Ayd\u0131n", ""], ["Yelick", "Katherine", ""]]}, {"id": "1810.13084", "submitter": "Nicolas Loizou", "authors": "Nicolas Loizou, Michael Rabbat, Peter Richt\\'arik", "title": "Provably Accelerated Randomized Gossip Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present novel provably accelerated gossip algorithms for\nsolving the average consensus problem. The proposed protocols are inspired from\nthe recently developed accelerated variants of the randomized Kaczmarz method -\na popular method for solving linear systems. In each gossip iteration all nodes\nof the network update their values but only a pair of them exchange their\nprivate information. Numerical experiments on popular wireless sensor networks\nshowing the benefits of our protocols are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 02:59:02 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Loizou", "Nicolas", ""], ["Rabbat", "Michael", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1810.13132", "submitter": "Jie Xu", "authors": "Jie Xu", "title": "Cardinalities estimation under sliding time window by sharing\n  HyperLogLog Counter", "comments": "2 figures. arXiv admin note: text overlap with arXiv:1807.01527", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinalities estimation is an important research topic in network management\nand security. How to solve this problem under sliding time window is a hot\ntopic. HyperLogLog is a memory efficient algorithm work under a fixed time\nwindow. A sliding version of HyperLogLog can work under sliding time window by\nreplacing every counter of HyperLogLog with a list of feature possible maxim\n(LFPM). But LFPM is a dynamic structure whose size is variable at running time.\nThis paper proposes a novel counter for HyperLogLog which consumes smaller size\nof memory than that of LFPM. Our counter is called bit distance recorder BDR,\nbecause it maintains the distance of every left most \"1\" bit position. The size\nof BDR is fixed. Based on BDR, we design a multi hosts' cardinalities\nestimation algorithm under sliding time window, virtual bit distance recorder\nVBDR. VBDR allocate a virtual vector of BDR for every host and every physical\nBDR is shared by several hosts to improve the memory usage. After a small\nmodifcation, we propose another two parallel versions of VBDR which can run on\nGPU to handle high speed traffic. One of these parallel VBDR is fast in IP pair\nscanning and the other one is memory efficient. BDR is also suitable for other\ncardinality estimation algorithms such as PCSA, LogLog.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 07:19:32 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Xu", "Jie", ""]]}, {"id": "1810.13177", "submitter": "Felix Martin Schuhknecht", "authors": "Ankur Sharma, Felix Martin Schuhknecht, Divya Agrawal, Jens Dittrich", "title": "How to Databasify a Blockchain: the Case of Hyperledger Fabric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Within the last few years, a countless number of blockchain systems have\nemerged on the market, each one claiming to revolutionize the way of\ndistributed transaction processing in one way or the other. Many blockchain\nfeatures, such as byzantine fault tolerance (BFT), are indeed valuable\nadditions in modern environments. However, despite all the hype around the\ntechnology, many of the challenges that blockchain systems have to face are\nfundamental transaction management problems. These are largely shared with\ntraditional database systems, which have been around for decades already.\n  These similarities become especially visible for systems, that blur the lines\nbetween blockchain systems and classical database systems. A great example of\nthis is Hyperledger Fabric, an open-source permissioned blockchain system under\ndevelopment by IBM. By having a relaxed view on BFT, the transaction pipeline\nof Fabric highly resembles the workflow of classical distributed databases\nsystems.\n  This raises two questions: (1) Which conceptual similarities and differences\ndo actually exist between a system such as Fabric and a classical distributed\ndatabase system? (2) Is it possible to improve on the performance of Fabric by\ntransitioning technology from the database world to blockchains and thus\nblurring the lines between these two types of systems even further? To tackle\nthese questions, we first explore Fabric from the perspective of database\nresearch, where we observe weaknesses in the transaction pipeline. We then\nsolve these issues by transitioning well-understood database concepts to\nFabric, namely transaction reordering as well as early transaction abort. Our\nexperimental evaluation shows that our improved version Fabric++ significantly\nincreases the throughput of successful transactions over the vanilla version by\nup to a factor of 3x.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:40:26 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Sharma", "Ankur", ""], ["Schuhknecht", "Felix Martin", ""], ["Agrawal", "Divya", ""], ["Dittrich", "Jens", ""]]}, {"id": "1810.13186", "submitter": "Benny Van Houdt", "authors": "Benny Van Houdt", "title": "Randomized Work Stealing versus Sharing in Large-scale Systems with\n  Non-exponential Job Sizes", "comments": "This paper was accepted in IEEE/ACM Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work sharing and work stealing are two scheduling paradigms to redistribute\nwork when performing distributed computations. In work sharing, processors\nattempt to migrate pending jobs to other processors in the hope of reducing\nresponse times. In work stealing, on the other hand, underutilized processors\nattempt to steal jobs from other processors. Both paradigms generate a certain\ncommunication overhead and the question addressed in this paper is which of the\ntwo reduces the response time the most given that they use the same amount of\ncommunication overhead.\n  Prior work presented explicit bounds, for large scale systems, on when\nrandomized work sharing outperforms randomized work stealing in case of Poisson\narrivals and exponential job durations and indicated that work sharing is best\nwhen the load is below $\\phi -1 \\approx 0.6180$, with $\\phi$ being the golden\nratio.\n  In this paper we revisit this problem and study the impact of the job size\ndistribution using a mean field model. We present an efficient method to\ndetermine the boundary between the regions where sharing or stealing is best\nfor a given job size distribution, as well as bounds that apply to any\n(phase-type) job size distribution. The main insight is that work stealing\nbenefits significantly from having more variable job sizes and work sharing may\nbecome inferior to work stealing for loads as small as $1/2 + \\epsilon$ for any\n$\\epsilon > 0$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 09:49:52 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 12:44:03 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Van Houdt", "Benny", ""]]}, {"id": "1810.13325", "submitter": "Muktikanta Sa", "authors": "Sathya Peri, Chandra Kiran Reddy, Muktikanta Sa", "title": "A Concurrent Unbounded Wait-Free Graph", "comments": "20 pages, 9 figs. arXiv admin note: text overlap with\n  arXiv:1611.03947", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient concurrent wait-free algorithm to\nconstruct an unbounded directed graph for shared memory architecture. To the\nbest of our knowledge that this is the first wait-free algorithm for an\nunbounded directed graph where insertion and deletion of vertices and/or edges\ncan happen concurrently. To achieve wait-freedom in a dynamic setting, threads\nhelp each other to perform the desired tasks using operator descriptors by\nother threads. To enhance performance, we also developed an optimized wait-free\ngraph based on the principle of fast-path-slow-path. We also prove that all\ngraph operations are wait-free and linearizable. We implemented our algorithms\nin C++ and tested its performance through several micro-benchmarks. Our\nexperimental results show an average of 9x improvement over the global\nlock-based implementation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:08:18 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 08:54:35 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Peri", "Sathya", ""], ["Reddy", "Chandra Kiran", ""], ["Sa", "Muktikanta", ""]]}, {"id": "1810.13432", "submitter": "Daniel Milroy", "authors": "Daniel J. Milroy, Allison H. Baker, Dorit M. Hammerling, Youngsung\n  Kim, Elizabeth R. Jessup, Thomas Hauser", "title": "Making root cause analysis feasible for large code bases: a solution\n  approach for a climate model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale simulation codes with huge and complex code bases, where\nbit-for-bit comparisons are too restrictive, finding the source of\nstatistically significant discrepancies (e.g., from a previous version,\nalternative hardware or supporting software stack) in output is non-trivial at\nbest. Although there are many tools for program comprehension through debugging\nor slicing, few (if any) scale to a model as large as the Community Earth\nSystem Model (CESM; trademarked), which consists of more than 1.5 million lines\nof Fortran code. Currently for the CESM, we can easily determine whether a\ndiscrepancy exists in the output using a by now well-established statistical\nconsistency testing tool. However, this tool provides no information as to the\npossible cause of the detected discrepancy, leaving developers in a seemingly\nimpossible (and frustrating) situation. Therefore, our aim in this work is to\nprovide the tools to enable developers to trace a problem detected through the\nCESM output to its source. To this end, our strategy is to reduce the search\nspace for the root cause(s) to a tractable size via a series of techniques that\ninclude creating a directed graph of internal CESM variables, extracting a\nsubgraph (using a form of hybrid program slicing), partitioning into\ncommunities, and ranking nodes by centrality. Runtime variable sampling then\nbecomes feasible in this reduced search space. We demonstrate the utility of\nthis process on multiple examples of CESM simulation output by illustrating how\nsampling can be performed as part of an efficient parallel iterative refinement\nprocedure to locate error sources, including sensitivity to CPU instructions.\nBy providing CESM developers with tools to identify and understand the reason\nfor statistically distinct output, we have positively impacted the CESM\nsoftware development cycle and, in particular, its focus on quality assurance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 17:45:10 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 05:33:50 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Milroy", "Daniel J.", ""], ["Baker", "Allison H.", ""], ["Hammerling", "Dorit M.", ""], ["Kim", "Youngsung", ""], ["Jessup", "Elizabeth R.", ""], ["Hauser", "Thomas", ""]]}]