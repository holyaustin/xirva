[{"id": "1402.0121", "submitter": "Alexandre Maurer", "authors": "Alexandre Maurer (EPFL), S\\'ebastien Tixeuil (LINCS, NPA), Xavier\n  D\\'efago (JAIST)", "title": "Reliable Communication in a Dynamic Network in the Presence of Byzantine\n  Faults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: two nodes want to reliably communicate in\na dynamic multihop network where some nodes have been compromised, and may have\na totally arbitrary and unpredictable behavior. These nodes are called\nByzantine. We consider the two cases where cryptography is available and not\navailable. We prove the necessary and sufficient condition (that is, the\nweakest possible condition) to ensure reliable communication in this context.\nOur proof is constructive, as we provide Byzantine-resilient algorithms for\nreliable communication that are optimal with respect to our impossibility\nresults. In a second part, we investigate the impact of our conditions in three\ncase studies: participants interacting in a conference, robots moving on a grid\nand agents in the subway. Our simulations indicate a clear benefit of using our\nalgorithms for reliable communication in those contexts.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2014 20:01:57 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2014 07:53:09 GMT"}, {"version": "v3", "created": "Tue, 27 May 2014 18:55:43 GMT"}, {"version": "v4", "created": "Mon, 16 Feb 2015 17:34:12 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Maurer", "Alexandre", "", "EPFL"], ["Tixeuil", "S\u00e9bastien", "", "LINCS, NPA"], ["D\u00e9fago", "Xavier", "", "JAIST"]]}, {"id": "1402.0239", "submitter": "Wojciech Mazurczyk", "authors": "Bartosz Lipinski, Wojciech Mazurczyk, Krzysztof Szczypiorski", "title": "Improving Hard Disk Contention-based Covert Channel in Cloud Computing\n  Environment", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steganographic methods allow the covert exchange of secret data between\nparties aware of the procedure. The cloud computing environment is a new and\nhot target for steganographers, and currently not many solutions have been\nproposed. This paper proposes CloudSteg which is a steganographic method that\nallows the creation of a covert channel based on hard disk contention between\nthe two cloud instances that reside on the same physical machine. Experimental\nresults conducted using open source cloud environment OpenStack, show that\nCloudSteg is able to achieve a bandwidth of about 0.1 bps which is 1000 times\nhigher than is known from the state-of-the-art version.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 20:01:30 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Lipinski", "Bartosz", ""], ["Mazurczyk", "Wojciech", ""], ["Szczypiorski", "Krzysztof", ""]]}, {"id": "1402.0264", "submitter": "Ning Xie", "authors": "Sardar Anisul Haque, Marc Moreno Maza and Ning Xie", "title": "A Many-core Machine Model for Designing Algorithms with Minimum\n  Parallelism Overheads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model of multithreaded computation, combining fork-join and\nsingle-instruction-multiple-data parallelisms, with an emphasis on estimating\nparallelism overheads of programs written for modern many-core architectures.\nWe establish a Graham-Brent theorem for this model so as to estimate execution\ntime of programs running on a given number of streaming multiprocessors. We\nevaluate the benefits of our model with four fundamental algorithms from\nscientific computing. In each case, our model is used to minimize parallelism\noverheads by determining an appropriate value range for a given program\nparameter; moreover experimentation confirms the model's prediction.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 00:22:59 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Haque", "Sardar Anisul", ""], ["Maza", "Marc Moreno", ""], ["Xie", "Ning", ""]]}, {"id": "1402.0485", "submitter": "Mustazee Rahman", "authors": "Mustazee Rahman and Balint Virag", "title": "Local algorithms for independent sets are half-optimal", "comments": "Exposition has been clarified in the new version", "journal-ref": "Ann. Probab. 45 (2017), no. 3, 1543-1577", "doi": "10.1214/16-AOP1094", "report-no": null, "categories": "math.PR cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the largest density of factor of i.i.d. independent sets on the\nd-regular tree is asymptotically at most (log d)/d as d tends to infinity. This\nmatches the lower bound given by previous constructions. It follows that the\nlargest independent sets given by local algorithms on random d-regular graphs\nhave the same asymptotic density. In contrast, the density of the largest\nindependent sets on these graphs is asymptotically 2(log d)/d. We also prove\nanalogous results for Poisson-Galton-Watson trees, which yield bounds for local\nalgorithms on sparse Erdos-Renyi graphs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 19:48:35 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 20:57:34 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Rahman", "Mustazee", ""], ["Virag", "Balint", ""]]}, {"id": "1402.0696", "submitter": "Shafi'i Muhammad Abdulhamid Mr", "authors": "Shafii Muhammad Abdulhamid, Muhammad Shafie Abd Latiff, Mohammed Bakri\n  Bashir", "title": "On-Demand Grid Provisioning Using Cloud Infrastructures and Related\n  Virtualization Tools: A Survey and Taxonomy", "comments": "11 page, 6 figures, 1 table", "journal-ref": "International Journal of Advanced Studies in Computer Science and\n  Engineering IJASCSE, Volume 3, Issue 1, 2014", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent researches have shown that grid resources can be accessed by client\non-demand, with the help of virtualization technology in the Cloud. The virtual\nmachines hosted by the hypervisors are being utilized to build the grid network\nwithin the cloud environment. The aim of this study is to survey some concepts\nused for the on-demand grid provisioning using Infrastructure as a Service\nCloud and the taxonomy of its related components. This paper, discusses the\ndifferent approaches for on-demand grid using infrastructural Cloud, the issues\nit tries to address and the implementation tools. The paper also, proposed an\nextended classification for the virtualization technology used and a new\nclassification for the Grid-Cloud integration which was based on the\narchitecture, communication flow and the user demand for the Grid resources.\nThis survey, tools and taxonomies presented here will contribute as a guide in\nthe design of future architectures for further researches.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 11:27:56 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Abdulhamid", "Shafii Muhammad", ""], ["Latiff", "Muhammad Shafie Abd", ""], ["Bashir", "Mohammed Bakri", ""]]}, {"id": "1402.0804", "submitter": "Antonio Fern\\'andez Anta", "authors": "Jordi Arjona, Angelos Chatzipapas, Antonio Fernandez Anta, Vincenzo\n  Mancuso", "title": "A Measurement-based Analysis of the Energy Consumption of Data Center\n  Servers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption is a growing issue in data centers, impacting their\neconomic viability and their public image. In this work we empirically\ncharacterize the power and energy consumed by different types of servers. In\nparticular, in order to understand the behavior of their energy and power\nconsumption, we perform measurements in different servers. In each of them, we\nexhaustively measure the power consumed by the CPU, the disk, and the network\ninterface under different configurations, identifying the optimal operational\nlevels. One interesting conclusion of our study is that the curve that defines\nthe minimal CPU power as a function of the load is neither linear nor purely\nconvex as has been previously assumed. Moreover, we find that the efficiency of\nthe various server components can be maximized by tuning the CPU frequency and\nthe number of active cores as a function of the system and network load, while\nthe block size of I/O operations should be always maximized by applications. We\nalso show how to estimate the energy consumed by an application as a function\nof some simple parameters, like the CPU load, and the disk and network\nactivity. We validate the proposed approach by accurately estimating the energy\nof a map-reduce computation in a Hadoop platform.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 17:40:38 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Arjona", "Jordi", ""], ["Chatzipapas", "Angelos", ""], ["Anta", "Antonio Fernandez", ""], ["Mancuso", "Vincenzo", ""]]}, {"id": "1402.1285", "submitter": "Jorge Gonz\\'alez-Dom\\'inguez", "authors": "Jorge Gonz\\'alez-Dom\\'inguez, Evangelos Georganas, Yili Zheng, Mar\\'ia\n  J. Mart\\'in", "title": "Constructing Performance Models for Dense Linear Algebra Algorithms on\n  Cray XE Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hiding or minimizing the communication cost is key in order to obtain good\nperformance on large-scale systems. While communication overlapping attempts to\nhide communications cost, 2.5D communication avoiding algorithms improve\nperformance scalability by reducing the volume of data transfers at the cost of\nextra memory usage. Both approaches can be used together or separately and the\nbest choice depends on the machine, the algorithm and the problem size. Thus,\nthe development of performance models is crucial to determine the best option\nfor each scenario. In this paper, we present a methodology for constructing\nperformance models for parallel numerical routines on Cray XE systems. Our\nmodels use portable benchmarks that measure computational cost and network\ncharacteristics, as well as performance degradation caused by simultaneous\naccesses to the network. We validate our methodology by constructing the\nperformance models for the 2D and 2.5D approaches, with and without\noverlapping, of two matrix multiplication algorithms (Cannon's and SUMMA),\ntriangular solve (TRSM) and Cholesky. We compare the estimations provided by\nthese models with the experimental results using up to 24,576 cores of a Cray\nXE6 system and predict the performance of the algorithms on larger systems.\nResults prove that the estimations significantly improve when taking into\naccount network contention.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 09:16:38 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Gonz\u00e1lez-Dom\u00ednguez", "Jorge", ""], ["Georganas", "Evangelos", ""], ["Zheng", "Yili", ""], ["Mart\u00edn", "Mar\u00eda J.", ""]]}, {"id": "1402.1309", "submitter": "Yanik Ngoko", "authors": "Yanik Ngoko, Christophe C\\'erin, Alfredo Goldman, and Dejan Milojicic", "title": "Backtracking algorithms for service selection", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the automation of services' compositions. We focus\non the service selection problem. In the formulation that we consider, the\nproblem's inputs are constituted by a behavioral composition whose abstract\nservices must be bound to concrete ones. The objective is to find the binding\nthat optimizes the {\\it utility} of the composition under some services level\nagreements. We propose a complete solution. Firstly, we show that the service\nselection problem can be mapped onto a Constraint Satisfaction Problem (CSP).\nThe benefit of this mapping is that the large know-how in the resolution of the\nCSP can be used for the service selection problem. Among the existing\ntechniques for solving CSP, we consider the backtracking. Our second\ncontribution is to propose various backtracking-based algorithms for the\nservice selection problem. The proposed variants are inspired by existing\nheuristics for the CSP. We analyze the runtime gain of our framework over an\nintuitive resolution based on exhaustive search. Our last contribution is an\nexperimental evaluation in which we demonstrate that there is an effective gain\nin using backtracking instead of some comparable approaches. The experiments\nalso show that our proposal can be used for finding in real time, optimal\nsolutions on small and medium services' compositions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 10:38:26 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Ngoko", "Yanik", ""], ["C\u00e9rin", "Christophe", ""], ["Goldman", "Alfredo", ""], ["Milojicic", "Dejan", ""]]}, {"id": "1402.1515", "submitter": "Jianshu Chen", "authors": "Jianshu Chen, Zaid J. Towfic, Ali H. Sayed", "title": "Dictionary Learning over Distributed Models", "comments": "16 pages, 8 figures. To appear in IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2385045", "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider learning dictionary models over a network of\nagents, where each agent is only in charge of a portion of the dictionary\nelements. This formulation is relevant in Big Data scenarios where large\ndictionary models may be spread over different spatial locations and it is not\nfeasible to aggregate all dictionaries in one location due to communication and\nprivacy considerations. We first show that the dual function of the inference\nproblem is an aggregation of individual cost functions associated with\ndifferent agents, which can then be minimized efficiently by means of diffusion\nstrategies. The collaborative inference step generates dual variables that are\nused by the agents to update their dictionaries without the need to share these\ndictionaries or even the coefficient models for the training data. This is a\npowerful property that leads to an effective distributed procedure for learning\ndictionaries over large networks (e.g., hundreds of agents in our experiments).\nFurthermore, the proposed learning strategy operates in an online manner and is\nable to respond to streaming data, where each data sample is presented to the\nnetwork once.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 22:19:19 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 05:40:44 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chen", "Jianshu", ""], ["Towfic", "Zaid J.", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1402.1841", "submitter": "Nikhita Reddy Gade", "authors": "G.Nikhita Reddy, G.J.Ugander Reddy", "title": "Study of Cloud Computing in HealthCare Industry", "comments": "4 pages", "journal-ref": "International Journal of Science and Engineering Research - France\n  - ISSN : 2229-5518 Volume 4 Issue 9 - September 2013 Pages: 68-71", "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Todays real world technology has become a domiant crucial component in\nevery industry including healthcare industry. The benefits of storing\nelectronically the records of patients have increased the productivity of\npatient care and easy accessibility and usage. The recent technological\ninnovations in the health care is the invention of cloud based Technology. But\nmany fears and security measures regarding patient records storing remotely is\na concern for many in health care industry. One needs to understand the\nbenefits and fears of implementation of cloud computing its advantages and\ndisadvantages of this new technology.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 11:51:13 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Reddy", "G. Nikhita", ""], ["Reddy", "G. J. Ugander", ""]]}, {"id": "1402.1932", "submitter": "Prince Jain mr", "authors": "Dr. Rahul Malhotra and Prince Jain", "title": "An EMUSIM Technique and its Components in Cloud Computing- A Review", "comments": "6 pages, 3 figures and \"Published with International Journal of\n  Computer Trends and Technology (IJCTT)\"", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  4(8): 2435-2440 August 2013", "doi": "10.14445/2231-2803", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent efforts to design and develop Cloud technologies focus on defining\nnovel methods, policies and mechanisms for efficiently managing Cloud\ninfrastructures. One key challenge potential Cloud customers have before\nrenting resources is to know how their services will behave in a set of\nresources and the costs involved when growing and shrinking their resource\npool. Most of the studies in this area rely on simulation-based experiments,\nwhich consider simplified modeling of applications and computing environment.\nIn order to better predict service's behavior on Cloud platforms, an integrated\narchitecture that is based on both simulation and emulation. The proposed\narchitecture, named EMUSIM, automatically extracts information from application\nbehavior via emulation and then uses this information to generate the\ncorresponding simulation model. This paper presents brief overview of the\nEMUSIM technique and its components. The work in this paper focuses on\narchitecture and operation details of Automated Emulation Framework (AEF),\nQAppDeployer and proposes Cloud Sim Application for Simulation techniques.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2014 10:10:48 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Malhotra", "Dr. Rahul", ""], ["Jain", "Prince", ""]]}, {"id": "1402.2090", "submitter": "Piotr Skowron", "authors": "Piotr Skowron and Krzysztof Rzadca", "title": "We Are Impatient: Algorithms for Geographically Distributed Load\n  Balancing with (Almost) Arbitrary Load Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geographically-distributed systems, communication latencies are\nnon-negligible. The perceived processing time of a request is thus composed of\nthe time needed to route the request to the server and the true processing\ntime. Once a request reaches a target server, the processing time depends on\nthe total load of that server; this dependency is described by a load function.\nWe consider a broad class of load functions; we just require that they are\nconvex and two times differentiable. In particular our model can be applied to\nheterogeneous systems in which every server has a different load function. This\napproach allows us not only to generalize results for queuing theory and for\nbatches of requests, but also to use empirically-derived load functions,\nmeasured in a system under stress-testing. The optimal assignment of requests\nto servers is communication-balanced, i.e. for any pair of non\nperfectly-balanced servers, the reduction of processing time resulting from\nmoving a single request from the overloaded to underloaded server is smaller\nthan the additional communication latency. We present a centralized and a\ndecentralized algorithm for optimal load balancing. We prove bounds on the\nalgorithms' convergence. To the best of our knowledge these bounds were not\nknown even for the special cases studied previously (queuing theory and batches\nof requests). Both algorithms are any-time algorithms. In the decentralized\nalgorithm, each server balances the load with a randomly chosen peer. Such\nalgorithm is very robust to failures. We prove that the decentralized algorithm\nperforms locally-optimal steps. Our work extends the currently known results by\nconsidering a broad class of load functions and by establishing theoretical\nbounds on the algorithms' convergence. These results are applicable for servers\nwhose characteristics under load cannot be described by a standard mathematical\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 10:33:52 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Skowron", "Piotr", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1402.2107", "submitter": "Mario Pastorelli", "authors": "Mario Pastorelli, Matteo Dell'Amico, Pietro Michiardi", "title": "OS-Assisted Task Preemption for Hadoop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a new task preemption primitive for Hadoop, that allows\ntasks to be suspended and resumed exploiting existing memory management\nmechanisms readily available in modern operating systems. Our technique fills\nthe gap that exists between the two extremes cases of killing tasks (which\nwaste work) or waiting for their completion (which introduces latency):\nexperimental results indicate superior performance and very small overheads\nwhen compared to existing alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 11:14:19 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Pastorelli", "Mario", ""], ["Dell'Amico", "Matteo", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1402.2140", "submitter": "Aybars Ural", "authors": "Vedat Kavalci, Aybars Ural and Orhan Dagdeviren", "title": "Distributed Vertex Cover Algorithms For Wireless Sensor Networks", "comments": null, "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.6, No.1, January 2014", "doi": "10.5121/ijcnc.2014.6107", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex covering has important applications for wireless sensor networks such\nas monitoring link failures, facility location, clustering, and data\naggregation. In this study, we designed three algorithms for constructing\nvertex cover in wireless sensor networks. The first algorithm, which is an\nadaption of the Parnas & Ron's algorithm, is a greedy approach that finds a\nvertex cover by using the degrees of the nodes. The second algorithm finds a\nvertex cover from graph matching where Hoepman's weighted matching algorithm is\nused. The third algorithm firstly forms a breadth-first search tree and then\nconstructs a vertex cover by selecting nodes with predefined levels from\nbreadth-first tree. We show the operation of the designed algorithms, analyze\nthem, and provide the simulation results in the TOSSIM environment. Finally we\nhave implemented, compared and assessed all these approaches. The transmitted\nmessage count of the first algorithm is smallest among other algorithms where\nthe third algorithm has turned out to be presenting the best results in vertex\ncover approximation ratio.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 13:30:25 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Kavalci", "Vedat", ""], ["Ural", "Aybars", ""], ["Dagdeviren", "Orhan", ""]]}, {"id": "1402.2266", "submitter": "Christian Lavault", "authors": "Sidi Mohamed Sedjelmaci (LIPN), Christian Lavault (LIPN)", "title": "Improvements on the accelerated integer GCD algorithm", "comments": "6 pages", "journal-ref": "Information Processing Letters 61, 1 (1997) 31--36", "doi": null, "report-no": null, "categories": "cs.DC cs.DM math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper analyses and presents several improvements to the algorithm\nfor finding the $(a,b)$-pairs of integers used in the $k$-ary reduction of the\nright-shift $k$-ary integer GCD algorithm. While the worst-case complexity of\nWeber's \"Accelerated integer GCD algorithm\" is $\\cO\\l(\\log_\\phi(k)^2\\r)$, we\nshow that the worst-case number of iterations of the while loop is exactly\n$\\tfrac 12 \\l\\lfloor \\log_{\\phi}(k)\\r\\rfloor$, where $\\phi := \\tfrac 12\n\\l(1+\\sqrt{5}\\r)$.\\par We suggest improvements on the average complexity of the\nlatter algorithm and also present two new faster residual algorithms: the\nsequential and the parallel one. A lower bound on the probability of avoiding\nthe while loop in our parallel residual algorithm is also given.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 20:43:46 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Sedjelmaci", "Sidi Mohamed", "", "LIPN"], ["Lavault", "Christian", "", "LIPN"]]}, {"id": "1402.2446", "submitter": "Petr  Kuznetsov", "authors": "Zohir Bouzid, Eli Gafni, Petr Kuznetsov", "title": "Strong Equivalence Relations for Iterated Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iterated Immediate Snapshot model (IIS), due to its elegant geometrical\nrepresentation, has become standard for applying topological reasoning to\ndistributed computing. Its modular structure makes it easier to analyze than\nthe more realistic (non-iterated) read-write Atomic-Snapshot memory model (AS).\nIt is known that AS and IIS are equivalent with respect to \\emph{wait-free\ntask} computability: a distributed task is solvable in AS if and only if it\nsolvable in IIS. We observe, however, that this equivalence is not sufficient\nin order to explore solvability of tasks in \\emph{sub-models} of AS (i.e.\nproper subsets of its runs) or computability of \\emph{long-lived} objects, and\na stronger equivalence relation is needed. In this paper, we consider\n\\emph{adversarial} sub-models of AS and IIS specified by the sets of processes\nthat can be \\emph{correct} in a model run. We show that AS and IIS are\nequivalent in a strong way: a (possibly long-lived) object is implementable in\nAS under a given adversary if and only if it is implementable in IIS under the\nsame adversary. %This holds whether the object is one-shot or long-lived.\nTherefore, the computability of any object in shared memory under an\nadversarial AS scheduler can be equivalently investigated in IIS.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 11:10:39 GMT"}, {"version": "v2", "created": "Tue, 20 May 2014 16:29:29 GMT"}], "update_date": "2014-05-21", "authors_parsed": [["Bouzid", "Zohir", ""], ["Gafni", "Eli", ""], ["Kuznetsov", "Petr", ""]]}, {"id": "1402.2491", "submitter": "Uthaya Banu", "authors": "M.Uthaya Banu, K.Saravanan", "title": "Optimizing the Cost for Resource Subscription Policy in IaaS Cloud", "comments": "6 pages,8 figures,\"Published with International Journal of\n  Engineering Trends and Technology (IJETT)\". M.Uthaya Banu, K.Saravanan.\n  Article:Optimizing the Cost for Resource Subscription Policy in IaaS Cloud", "journal-ref": "International Journal of Engineering Trends and Technology (IJETT)\n  6(6):296-301, December 2013", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing allow the users to efficiently and dynamically provision\ncomputing resource to meet their IT needs. Cloud Provider offers two\nsubscription plan to the customer namely reservation and on-demand. The\nreservation plan is typically cheaper than on-demand plan. If the actual\ncomputing demand is known in advance reserving the resource would be\nstraightforward. The challenge is how to make properly resource provisioning\nand how the customers efficiently purchase the provisioning options under\nreservation and on-demand. To address this issue, two-phase algorithm are\nproposed to minimize service provision cost in both reservation and on-demand\nplan. To reserve the correct and optimal amount of resources during\nreservation, proposed a mathematical formulae in the first phase. To predict\nresource demand, use kalman filter in the second phase. The evaluation result\nshows that the two-phase algorithm can significantly reduce the provision cost\nand the prediction is of reasonable accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:11:18 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Banu", "M. Uthaya", ""], ["Saravanan", "K.", ""]]}, {"id": "1402.2496", "submitter": "Pierre Fraigniaud", "authors": "L\\'elia Blin and Pierre Fraigniaud", "title": "Polynomial-Time Space-Optimal Silent Self-Stabilizing Minimum-Degree\n  Spanning Tree Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to sensor networks, as well as to many other areas,\nthis paper studies the construction of minimum-degree spanning trees. We\nconsider the classical node-register state model, with a weakly fair scheduler,\nand we present a space-optimal \\emph{silent} self-stabilizing construction of\nminimum-degree spanning trees in this model. Computing a spanning tree with\nminimum degree is NP-hard. Therefore, we actually focus on constructing a\nspanning tree whose degree is within one from the optimal. Our algorithm uses\nregisters on $O(\\log n)$ bits, converges in a polynomial number of rounds, and\nperforms polynomial-time computation at each node. Specifically, the algorithm\nconstructs and stabilizes on a special class of spanning trees, with degree at\nmost $OPT+1$. Indeed, we prove that, unless NP $=$ coNP, there are no\nproof-labeling schemes involving polynomial-time computation at each node for\nthe whole family of spanning trees with degree at most $OPT+1$. Up to our\nknowledge, this is the first example of the design of a compact silent\nself-stabilizing algorithm constructing, and stabilizing on a subset of optimal\nsolutions to a natural problem for which there are no time-efficient\nproof-labeling schemes. On our way to design our algorithm, we establish a set\nof independent results that may have interest on their own. In particular, we\ndescribe a new space-optimal silent self-stabilizing spanning tree\nconstruction, stabilizing on \\emph{any} spanning tree, in $O(n)$ rounds, and\nusing just \\emph{one} additional bit compared to the size of the labels used to\ncertify trees. We also design a silent loop-free self-stabilizing algorithm for\ntransforming a tree into another tree. Last but not least, we provide a silent\nself-stabilizing algorithm for computing and certifying the labels of a\nNCA-labeling scheme.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:19:00 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Blin", "L\u00e9lia", ""], ["Fraigniaud", "Pierre", ""]]}, {"id": "1402.2509", "submitter": "Subha", "authors": "M.Subha, K.Saravanan", "title": "Achieve Better Ranking Accuracy Using CloudRank Framework for Cloud\n  Services", "comments": "6 pages, 10 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology (IJETT)\n  6(6):307-312, December 2013", "doi": null, "report-no": null, "categories": "cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building high quality cloud applications becomes an urgently required\nresearch problem. Nonfunctional performance of cloud services is usually\ndescribed by quality-of-service (QoS). In cloud applications, cloud services\nare invoked remotely by internet connections. The QoS Ranking of cloud services\nfor a user cannot be transferred directly to another user, since the locations\nof the cloud applications are quite different. Personalized QoS Ranking is\nrequired to evaluate all candidate services at the user - side but it is\nimpractical in reality. To get QoS values, the service candidates are usually\nrequired and it is very expensive. To avoid time consuming and expensive\nrealworld service invocations, this paper proposes a CloudRank framework which\npredicts the QoS ranking directly without predicting the corresponding QoS\nvalues. This framework provides an accurate ranking but the QoS values are same\nin both algorithms so, an optimal VM allocation policy is used to improve the\nQoS performance of cloud services and it also provides better ranking accuracy\nthan CloudRank2 algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:51:11 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Subha", "M.", ""], ["Saravanan", "K.", ""]]}, {"id": "1402.2543", "submitter": "Jukka Suomela", "authors": "Juho Hirvonen, Joel Rybicki, Stefan Schmid, Jukka Suomela", "title": "Large Cuts with Local Algorithms on Triangle-Free Graphs", "comments": "1+17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of finding large cuts in $d$-regular triangle-free\ngraphs. In prior work, Shearer (1992) gives a randomised algorithm that finds a\ncut of expected size $(1/2 + 0.177/\\sqrt{d})m$, where $m$ is the number of\nedges. We give a simpler algorithm that does much better: it finds a cut of\nexpected size $(1/2 + 0.28125/\\sqrt{d})m$. As a corollary, this shows that in\nany $d$-regular triangle-free graph there exists a cut of at least this size.\n  Our algorithm can be interpreted as a very efficient randomised distributed\nalgorithm: each node needs to produce only one random bit, and the algorithm\nruns in one synchronous communication round. This work is also a case study of\napplying computational techniques in the design of distributed algorithms: our\nalgorithm was designed by a computer program that searched for optimal\nalgorithms for small values of $d$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 16:06:35 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Hirvonen", "Juho", ""], ["Rybicki", "Joel", ""], ["Schmid", "Stefan", ""], ["Suomela", "Jukka", ""]]}, {"id": "1402.2549", "submitter": "Jukka Suomela", "authors": "Miikka Hilke, Christoph Lenzen, Jukka Suomela", "title": "Local Approximability of Minimum Dominating Set on Planar Graphs", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is no deterministic local algorithm (constant-time\ndistributed graph algorithm) that finds a $(7-\\epsilon)$-approximation of a\nminimum dominating set on planar graphs, for any positive constant $\\epsilon$.\nIn prior work, the best lower bound on the approximation ratio has been\n$5-\\epsilon$; there is also an upper bound of $52$.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 16:24:05 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Hilke", "Miikka", ""], ["Lenzen", "Christoph", ""], ["Suomela", "Jukka", ""]]}, {"id": "1402.2552", "submitter": "Jukka Suomela", "authors": "Juhana Laurinharju, Jukka Suomela", "title": "Linial's Lower Bound Made Easy", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linial's seminal result shows that any deterministic distributed algorithm\nthat finds a $3$-colouring of an $n$-cycle requires at least $\\log^*(n)/2 - 1$\ncommunication rounds. We give a new simpler proof of this theorem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 16:33:07 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Laurinharju", "Juhana", ""], ["Suomela", "Jukka", ""]]}, {"id": "1402.2626", "submitter": "Jan Verschelde", "authors": "Jan Verschelde and Xiangcheng Yu", "title": "GPU acceleration of Newton's method for large systems of polynomial\n  equations in double double and quad double arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to compensate for the higher cost of double double and quad double\narithmetic when solving large polynomial systems, we investigate the\napplication of NVIDIA Tesla K20C general purpose graphics processing unit. The\nfocus on this paper is on Newton's method, which requires the evaluation of the\npolynomials, their derivatives, and the solution of a linear system to compute\nthe update to the current approximation for the solution. The reverse mode of\nalgorithmic differentiation for a product of variables is rewritten in a binary\ntree fashion so all threads in a block can collaborate in the computation. For\ndouble arithmetic, the evaluation and differentiation problem is memory bound,\nwhereas for complex quad double arithmetic the problem is compute bound. With\nacceleration we can double the dimension and get results that are twice as\naccurate in about the same time.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 20:18:31 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 13:38:42 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Verschelde", "Jan", ""], ["Yu", "Xiangcheng", ""]]}, {"id": "1402.2676", "submitter": "Parameswaran Raman", "authors": "Hyokun Yun, Parameswaran Raman, S.V.N. Vishwanathan", "title": "Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:39:54 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 21:08:34 GMT"}, {"version": "v3", "created": "Fri, 11 Apr 2014 06:19:04 GMT"}, {"version": "v4", "created": "Thu, 21 Aug 2014 06:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Yun", "Hyokun", ""], ["Raman", "Parameswaran", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1402.2680", "submitter": "Marcos Manzano Mr.", "authors": "Marc Manzano, Anna Manolova Fagertun, Sarah Ruepp, Eusebi Calle,\n  Caterina Scoglio, Ali Sydney, Antonio de la Oliva, and Alfonso Mu\\~noz", "title": "Unveiling Potential Failure Propagation Scenarios in Core Transport\n  Networks", "comments": "Submitted to IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contemporary society has become more dependent on telecommunication\nnetworks. Novel services and technologies supported by such networks, such as\ncloud computing or e-Health, hold a vital role in modern day living.\nLarge-scale failures are prone to occur, thus being a constant threat to\nbusiness organizations and individuals. To the best of our knowledge, there are\nno publicly available reports regarding failure propagation in core transport\nnetworks. Furthermore, Software Defined Networking (SDN) is becoming more\nprevalent in our society and we can envision more SDN-controlled Backbone\nTransport Networks (BTNs) in the future. For this reason, we investigate the\nmain motivations that could lead to epidemic-like failures in BTNs and SDNTNs.\nTo do so, we enlist the expertise of several research groups with significant\nbackground in epidemics, network resiliency, and security. In addition, we\nconsider the experiences of three network providers. Our results illustrate\nthat Dynamic Transport Networks (DTNs) are prone to epidemic-like failures.\nMoreover, we propose different situations in which a failure can propagate in\nSDNTNs. We believe that the key findings will aid network engineers and the\nscientific community to predict this type of disastrous failure scenario and\nplan adequate survivability strategies.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:57:55 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Manzano", "Marc", ""], ["Fagertun", "Anna Manolova", ""], ["Ruepp", "Sarah", ""], ["Calle", "Eusebi", ""], ["Scoglio", "Caterina", ""], ["Sydney", "Ali", ""], ["de la Oliva", "Antonio", ""], ["Mu\u00f1oz", "Alfonso", ""]]}, {"id": "1402.2698", "submitter": "Mateus de Oliveira Oliveira", "authors": "Mateus de Oliveira Oliveira", "title": "Automated Verification, Synthesis and Correction of Concurrent Systems\n  via MSO Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide algorithmic solutions to five fundamental problems\nconcerning the verification, synthesis and correction of concurrent systems\nthat can be modeled by bounded p/t-nets. We express concurrency via partial\norders and assume that behavioral specifications are given via monadic second\norder logic. A c-partial-order is a partial order whose Hasse diagram can be\ncovered by c paths. For a finite set T of transitions, we let P(c,T,\\phi)\ndenote the set of all T-labelled c-partial-orders satisfying \\phi. If N=(P,T)\nis a p/t-net we let P(N,c) denote the set of all c-partially-ordered runs of N.\nA (b, r)-bounded p/t-net is a b-bounded p/t-net in which each place appears\nrepeated at most r times. We solve the following problems:\n  1. Verification: given an MSO formula \\phi and a bounded p/t-net N determine\nwhether P(N,c)\\subseteq P(c,T,\\phi), whether P(c,T,\\phi)\\subseteq P(N,c), or\nwhether P(N,c)\\cap P(c,T,\\phi)=\\emptyset.\n  2. Synthesis from MSO Specifications: given an MSO formula \\phi, synthesize a\nsemantically minimal (b,r)-bounded p/t-net N satisfying P(c,T,\\phi)\\subseteq\nP(N, c).\n  3. Semantically Safest Subsystem: given an MSO formula \\phi defining a set of\nsafe partial orders, and a b-bounded p/t-net N, possibly containing unsafe\nbehaviors, synthesize the safest (b,r)-bounded p/t-net N' whose behavior lies\nin between P(N,c)\\cap P(c,T,\\phi) and P(N,c).\n  4. Behavioral Repair: given two MSO formulas \\phi and \\psi, and a b-bounded\np/t-net N, synthesize a semantically minimal (b,r)-bounded p/t net N' whose\nbehavior lies in between P(N,c) \\cap P(c,T,\\phi) and P(c,T,\\psi).\n  5. Synthesis from Contracts: given an MSO formula \\phi^yes specifying a set\nof good behaviors and an MSO formula \\phi^no specifying a set of bad behaviors,\nsynthesize a semantically minimal (b,r)-bounded p/t-net N such that\nP(c,T,\\phi^yes) \\subseteq P(N,c) but P(c,T,\\phi^no ) \\cap P(N,c)=\\emptyset.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 23:50:39 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Oliveira", "Mateus de Oliveira", ""]]}, {"id": "1402.2701", "submitter": "Bernhard Haeupler", "authors": "Bernhard Haeupler, Dahlia Malkhi", "title": "Optimal Gossip with Direct Addressing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gossip algorithms spread information by having nodes repeatedly forward\ninformation to a few random contacts. By their very nature, gossip algorithms\ntend to be distributed and fault tolerant. If done right, they can also be fast\nand message-efficient. A common model for gossip communication is the random\nphone call model, in which in each synchronous round each node can PUSH or PULL\ninformation to or from a random other node. For example, Karp et al. [FOCS\n2000] gave algorithms in this model that spread a message to all nodes in\n$\\Theta(\\log n)$ rounds while sending only $O(\\log \\log n)$ messages per node\non average.\n  Recently, Avin and Els\\\"asser [DISC 2013], studied the random phone call\nmodel with the natural and commonly used assumption of direct addressing.\nDirect addressing allows nodes to directly contact nodes whose ID (e.g., IP\naddress) was learned before. They show that in this setting, one can \"break the\n$\\log n$ barrier\" and achieve a gossip algorithm running in $O(\\sqrt{\\log n})$\nrounds, albeit while using $O(\\sqrt{\\log n})$ messages per node.\n  We study the same model and give a simple gossip algorithm which spreads a\nmessage in only $O(\\log \\log n)$ rounds. We also prove a matching $\\Omega(\\log\n\\log n)$ lower bound which shows that this running time is best possible. In\nparticular we show that any gossip algorithm takes with high probability at\nleast $0.99 \\log \\log n$ rounds to terminate. Lastly, our algorithm can be\ntweaked to send only $O(1)$ messages per node on average with only $O(\\log n)$\nbits per message. Our algorithm therefore simultaneously achieves the optimal\nround-, message-, and bit-complexity for this setting. As all prior gossip\nalgorithms, our algorithm is also robust against failures. In particular, if in\nthe beginning an oblivious adversary fails any $F$ nodes our algorithm still,\nwith high probability, informs all but $o(F)$ surviving nodes.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 00:14:58 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Haeupler", "Bernhard", ""], ["Malkhi", "Dahlia", ""]]}, {"id": "1402.2810", "submitter": "Georgios Zois", "authors": "Evripidis Bampis, Vincent Chau, Dimitrios Letsios, Giorgio Lucarelli,\n  Ioannis Milis and Georgios Zois", "title": "Energy Efficient Scheduling of MapReduce Jobs", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MapReduce is emerged as a prominent programming model for data-intensive\ncomputation. In this work, we study power-aware MapReduce scheduling in the\nspeed scaling setting first introduced by Yao et al. [FOCS 1995]. We focus on\nthe minimization of the total weighted completion time of a set of MapReduce\njobs under a given budget of energy. Using a linear programming relaxation of\nour problem, we derive a polynomial time constant-factor approximation\nalgorithm. We also propose a convex programming formulation that we combine\nwith standard list scheduling policies, and we evaluate their performance using\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 13:16:16 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Bampis", "Evripidis", ""], ["Chau", "Vincent", ""], ["Letsios", "Dimitrios", ""], ["Lucarelli", "Giorgio", ""], ["Milis", "Ioannis", ""], ["Zois", "Georgios", ""]]}, {"id": "1402.2880", "submitter": "Anthony Scemama", "authors": "Anthony Scemama, Nicolas Renon and Mathias Rapacioli", "title": "A Sparse SCF algorithm and its parallel implementation: Application to\n  DFTB", "comments": "13 pages, 11 figures", "journal-ref": "J. Chem. Theory Comput., 2014, 10 (6), 2344-2354", "doi": "10.1021/ct500115v", "report-no": null, "categories": "physics.chem-ph cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm and its parallel implementation for solving a self\nconsistent problem as encountered in Hartree Fock or Density Functional Theory.\nThe algorithm takes advantage of the sparsity of matrices through the use of\nlocal molecular orbitals. The implementation allows to exploit efficiently\nmodern symmetric multiprocessing (SMP) computer architectures. As a first\napplication, the algorithm is used within the density functional based tight\nbinding method, for which most of the computational time is spent in the linear\nalgebra routines (diagonalization of the Fock/Kohn-Sham matrix). We show that\nwith this algorithm (i) single point calculations on very large systems\n(millions of atoms) can be performed on large SMP machines (ii) calculations\ninvolving intermediate size systems (1~000--100~000 atoms) are also strongly\naccelerated and can run efficiently on standard servers (iii) the error on the\ntotal energy due to the use of a cut-off in the molecular orbital coefficients\ncan be controlled such that it remains smaller than the SCF convergence\ncriterion.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 16:27:00 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Scemama", "Anthony", ""], ["Renon", "Nicolas", ""], ["Rapacioli", "Mathias", ""]]}, {"id": "1402.3010", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural, Cevdet Aykanat", "title": "1-D and 2-D Parallel Algorithms for All-Pairs Similarity Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-pairs similarity problem asks to find all vector pairs in a set of\nvectors the similarities of which surpass a given similarity threshold, and it\nis a computational kernel in data mining and information retrieval for several\ntasks. We investigate the parallelization of a recent fast sequential\nalgorithm. We propose effective 1-D and 2-D data distribution strategies that\npreserve the essential optimizations in the fast algorithm. 1-D parallel\nalgorithms distribute either dimensions or vectors, whereas the 2-D parallel\nalgorithm distributes data both ways. Additional contributions to the 1-D\nvertical distribution include a local pruning strategy to reduce the number of\ncandidates, a recursive pruning algorithm, and block processing to reduce\nimbalance. The parallel algorithms were programmed in OCaml which affords much\nconvenience. Our experiments indicate that the performance depends on the\ndataset, therefore a variety of parallelizations is useful.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 00:14:33 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["\u00d6zkural", "Eray", ""], ["Aykanat", "Cevdet", ""]]}, {"id": "1402.3034", "submitter": "Sukhpal  Singh", "authors": "Sukhpal Singh and Inderveer Chana", "title": "Formal Specification Language Based IaaS Cloud Workload Regression\n  Analysis", "comments": "6 Pages including 2 figures, Published by IEEE Joint Chapter of\n  IE/PEL/CS under IEEE UP section", "journal-ref": "Presented in the IEEE International Conference on Control,\n  Computing, Communication and Materials (ICCCCM-2013) held at UIT, Allahabad,\n  India on August 03-04, 2013", "doi": "10.1109/ICCCCM.2013.6648901", "report-no": "978-1?4799?1375?6/13/$31.00 {copyright}2013 IEEE", "categories": "cs.DC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Cloud Computing is an emerging area for accessing computing resources. In\ngeneral, Cloud service providers offer services that can be clustered into\nthree categories: SaaS, PaaS and IaaS. This paper discusses the Cloud workload\nanalysis. The efficient Cloud workload resource mapping technique is proposed.\nThis paper aims to provide a means of understanding and investigating IaaS\nCloud workloads and the resources. In this paper, regression analysis is used\nto analyze the Cloud workloads and identifies the relationship between Cloud\nworkloads and available resources. The effective organization of dynamic nature\nresources can be done with the help of Cloud workloads. Till Cloud workload is\nconsidered a vital talent, the Cloud resources cannot be consumed in an\neffective style. The proposed technique has been validated by Z Formal\nspecification language. This approach is effective in minimizing the cost and\nsubmission burst time of Cloud workloads.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 05:24:37 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Singh", "Sukhpal", ""], ["Chana", "Inderveer", ""]]}, {"id": "1402.3281", "submitter": "Christian Schulz", "authors": "Henning Meyerhenke and Peter Sanders and Christian Schulz", "title": "Partitioning Complex Networks via Size-constrained Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most commonly used method to tackle the graph partitioning problem in\npractice is the multilevel approach. During a coarsening phase, a multilevel\ngraph partitioning algorithm reduces the graph size by iteratively contracting\nnodes and edges until the graph is small enough to be partitioned by some other\nalgorithm. A partition of the input graph is then constructed by successively\ntransferring the solution to the next finer graph and applying a local search\nalgorithm to improve the current solution.\n  In this paper, we describe a novel approach to partition graphs effectively\nespecially if the networks have a highly irregular structure. More precisely,\nour algorithm provides graph coarsening by iteratively contracting\nsize-constrained clusterings that are computed using a label propagation\nalgorithm. The same algorithm that provides the size-constrained clusterings\ncan also be used during uncoarsening as a fast and simple local search\nalgorithm.\n  Depending on the algorithm's configuration, we are able to compute partitions\nof very high quality outperforming all competitors, or partitions that are\ncomparable to the best competitor in terms of quality, hMetis, while being\nnearly an order of magnitude faster on average. The fastest configuration\npartitions the largest graph available to us with 3.3 billion edges using a\nsingle machine in about ten minutes while cutting less than half of the edges\nthan the fastest competitor, kMetis.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 20:46:31 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 10:50:28 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Meyerhenke", "Henning", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""]]}, {"id": "1402.3305", "submitter": "Martin Klein", "authors": "Martin Klein, Robert Sanderson, Herbert Van de Sompel, Michael L.\n  Nelson", "title": "Real-Time Notification for Resource Synchronization", "comments": "12 pages, 5 figues, 1 table The experiments were conducted in 2012 as\n  part of the authors' work in the ResourceSync project. For further\n  information and the ResourceSync framework specification please see\n  http://www.openarchives.org/rs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web applications frequently leverage resources made available by remote web\nservers. As resources are created, updated, deleted, or moved, these\napplications face challenges to remain in lockstep with the server's change\ndynamics. Several approaches exist to help meet this challenge for use cases\nwhere \"good enough\" synchronization is acceptable. But when strict resource\ncoverage or low synchronization latency is required, commonly accepted\nWeb-based solutions remain elusive. This paper details characteristics of an\napproach that aims at decreasing synchronization latency while maintaining\ndesired levels of accuracy. The approach builds on pushing change notifications\nand pulling changed resources and it is explored with an experiment based on a\nDBpedia Live instance.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:04:54 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Klein", "Martin", ""], ["Sanderson", "Robert", ""], ["Van de Sompel", "Herbert", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1402.3319", "submitter": "Boris \\v{S}kori\\'c", "authors": "Boris Skoric, Sebastiaan J.A. de Hoogh, Nicola Zannone", "title": "Flow-based reputation with uncertainty: Evidence-Based Subjective Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of reputation is widely used as a measure of trustworthiness\nbased on ratings from members in a community. The adoption of reputation\nsystems, however, relies on their ability to capture the actual trustworthiness\nof a target. Several reputation models for aggregating trust information have\nbeen proposed in the literature. The choice of model has an impact on the\nreliability of the aggregated trust information as well as on the procedure\nused to compute reputations. Two prominent models are flow-based reputation\n(e.g., EigenTrust, PageRank) and Subjective Logic based reputation. Flow-based\nmodels provide an automated method to aggregate trust information, but they are\nnot able to express the level of uncertainty in the information. In contrast,\nSubjective Logic extends probabilistic models with an explicit notion of\nuncertainty, but the calculation of reputation depends on the structure of the\ntrust network and often requires information to be discarded. These are severe\ndrawbacks.\n  In this work, we observe that the `opinion discounting' operation in\nSubjective Logic has a number of basic problems. We resolve these problems by\nproviding a new discounting operator that describes the flow of evidence from\none party to another. The adoption of our discounting rule results in a\nconsistent Subjective Logic algebra that is entirely based on the handling of\nevidence. We show that the new algebra enables the construction of an automated\nreputation assessment procedure for arbitrary trust networks, where the\ncalculation no longer depends on the structure of the network, and does not\nneed to throw away any information. Thus, we obtain the best of both worlds:\nflow-based reputation and consistent handling of uncertainties.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 21:49:52 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 15:55:37 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Skoric", "Boris", ""], ["de Hoogh", "Sebastiaan J. A.", ""], ["Zannone", "Nicola", ""]]}, {"id": "1402.3501", "submitter": "Jean-Guillaume Dumas", "authors": "Jean-Guillaume Dumas (LJK), Thierry Gautier (INRIA Grenoble\n  Rh\\^one-Alpes / LIG Laboratoire d'Informatique de Grenoble), Cl\\'ement Pernet\n  (INRIA Grenoble Rh\\^one-Alpes / LIG Laboratoire d'Informatique de Grenoble),\n  Ziad Sultan (LJK, INRIA Grenoble Rh\\^one-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble)", "title": "Parallel computation of echelon forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose efficient parallel algorithms and implementations on shared memory\narchitectures of LU factorization over a finite field. Compared to the\ncorresponding numerical routines, we have identified three main difficulties\nspecific to linear algebra over finite fields. First, the arithmetic complexity\ncould be dominated by modular reductions. Therefore, it is mandatory to delay\nas much as possible these reductions while mixing fine-grain parallelizations\nof tiled iterative and recursive algorithms. Second, fast linear algebra\nvariants, e.g., using Strassen-Winograd algorithm, never suffer from\ninstability and can thus be widely used in cascade with the classical\nalgorithms. There, trade-offs are to be made between size of blocks well suited\nto those fast variants or to load and communication balancing. Third, many\napplications over finite fields require the rank profile of the matrix (quite\noften rank deficient) rather than the solution to a linear system. It is thus\nimportant to design parallel algorithms that preserve and compute this rank\nprofile. Moreover, as the rank profile is only discovered during the algorithm,\nblock size has then to be dynamic. We propose and compare several block\ndecomposition: tile iterative with left-looking, right-looking and Crout\nvariants, slab and tile recursive. Experiments demonstrate that the tile\nrecursive variant performs better and matches the performance of reference\nnumerical software when no rank deficiency occur. Furthermore, even in the most\nheterogeneous case, namely when all pivot blocks are rank deficient, we show\nthat it is possbile to maintain a high efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 15:35:37 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Dumas", "Jean-Guillaume", "", "LJK"], ["Gautier", "Thierry", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LIG Laboratoire d'Informatique de Grenoble"], ["Pernet", "Cl\u00e9ment", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LIG Laboratoire d'Informatique de Grenoble"], ["Sultan", "Ziad", "", "LJK, INRIA Grenoble Rh\u00f4ne-Alpes / LIG Laboratoire\n  d'Informatique de Grenoble"]]}, {"id": "1402.3545", "submitter": "Eike Hermann M\\\"uller", "authors": "Eike Hermann M\\\"uller, Robert Scheichl, Eero Vainikko", "title": "Petascale elliptic solvers for anisotropic PDEs on GPU clusters", "comments": "20 pages, 6 figures. Additional explanations and clarifications of\n  the characteristics of the PDE; discussion and estimate of the condition\n  number. Added section and figure on the robustness of both the single-level\n  and the multigrid method under variations of the Courant number. Clarified\n  the terminology in the performance analysis. Added section on preliminary\n  strong scaling results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory bound applications such as solvers for large sparse systems of\nequations remain a challenge for GPUs. Fast solvers should be based on\nnumerically efficient algorithms and implemented such that global memory access\nis minimised. To solve systems with up to one trillion ($10^{12}$) unknowns the\ncode has to make efficient use of several million individual processor cores on\nlarge GPU clusters. We describe the multi-GPU implementation of two\nalgorithmically optimal iterative solvers for anisotropic elliptic PDEs which\nare encountered in atmospheric modelling. In this application the condition\nnumber is large but independent of the grid resolution and both methods are\nasymptotically optimal, albeit with different absolute performance. We\nparallelise the solvers and adapt them to the specific features of GPU\narchitectures, paying particular attention to efficient global memory access.\nWe achieve a performance of up to 0.78 PFLOPs when solving an equation with\n$0.55\\cdot 10^{12}$ unknowns on 16384 GPUs; this corresponds to about $3\\%$ of\nthe theoretical peak performance of the machine and we use more than $40\\%$ of\nthe peak memory bandwidth with a Conjugate Gradient (CG) solver. Although the\nother solver, a geometric multigrid algorithm, has a slightly worse performance\nin terms of FLOPs per second, overall it is faster as it needs less iterations\nto converge; the multigrid algorithm can solve a linear PDE with half a\ntrillion unknowns in about one second.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 18:30:04 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 10:56:36 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["M\u00fcller", "Eike Hermann", ""], ["Scheichl", "Robert", ""], ["Vainikko", "Eero", ""]]}, {"id": "1402.3549", "submitter": "Xiajun Wang", "authors": "Xiajun Wang, Song Huang, Song Fu and Krishna Kavi", "title": "Characterizing Workload of Web Applications on Virtualized Servers", "comments": "8 pages, 8 figures, The Fourth Workshop on Big Data Benchmarks,\n  Performance Optimization, and Emerging Hardware in conjunction with the 19th\n  ACM International Conference on Architectural Support for Programming\n  Languages and Operating Systems (ASPLOS-2014), Salt Lake City, Utah, USA,\n  March 1-5, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With the ever increasing demands of cloud computing services, planning and\nmanagement of cloud resources has become a more and more important issue which\ndirected affects the resource utilization and SLA and customer satisfaction.\nBut before any management strategy is made, a good understanding of\napplications' workload in virtualized environment is the basic fact and\nprinciple to the resource management methods. Unfortunately, little work has\nbeen focused on this area. Lack of raw data could be one reason; another reason\nis that people still use the traditional models or methods shared under\nnon-virtualized environment. The study of applications' workload in virtualized\nenvironment should take on some of its peculiar features comparing to the\nnon-virtualized environment. In this paper, we are open to analyze the workload\ndemands that reflect applications' behavior and the impact of virtualization.\nThe results are obtained from an experimental cloud testbed running web\napplications, specifically the RUBiS benchmark application. We profile the\nworkload dynamics on both virtualized and non-virtualized environments and\ncompare the findings. The experimental results are valuable for us to estimate\nthe performance of applications on computer architectures, to predict SLA\ncompliance or violation based on the projected application workload and to\nguide the decision making to support applications with the right hardware.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 18:51:38 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Wang", "Xiajun", ""], ["Huang", "Song", ""], ["Fu", "Song", ""], ["Kavi", "Krishna", ""]]}, {"id": "1402.3781", "submitter": "Haytham Alzeini Mr.", "authors": "H I Alzeini, Sh A Hameed and M H Habaebi", "title": "A Framework for Developing Real-Time OLAP algorithm using Multi-core\n  processing and GPU: Heterogeneous Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overwhelmingly increasing amount of stored data has spurred researchers\nseeking different methods in order to optimally take advantage of it which\nmostly have faced a response time problem as a result of this enormous size of\ndata. Most of solutions have suggested materialization as a favourite solution.\nHowever, such a solution cannot attain Real- Time answers anyhow. In this paper\nwe propose a framework illustrating the barriers and suggested solutions in the\nway of achieving Real-Time OLAP answers that are significantly used in decision\nsupport systems and data warehouses.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 10:11:57 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Alzeini", "H I", ""], ["Hameed", "Sh A", ""], ["Habaebi", "M H", ""]]}, {"id": "1402.3788", "submitter": "Natalya Litvinenko", "authors": "Natalya Litvinenko", "title": "Using of GPUs for cluster analysis of large data by K-means method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This problem was solved within the framework of the grant project \"Solving of\nproblems of cluster analysis with application of parallel algorithms and cloud\ntechnologies\" in the Institute of Mathematics and Mathematical Modelling in\nAlmaty. The problem of cluster analysis for the large amount of data is very\nimportant in different areas of science - genetics, biology, sociology etc. At\nthe same time, such statistical known packages as STATISTICA, STADIA, SYSTAT\nand others do not allow to solve large problems. The new algorithm that uses\nthe high processing power of GPUs for solving clustering problems by the\nK-means method was developed. This algorithm is implemented as a C++\napplication in Microsoft Visual Studio 2010 with using the GPU Nvidia GeForce\n660. The developed software package for solving clustering problems by the\nmethod of K - means with using GPUs allows us to handle up to 2 million records\nwith number of features up to 25. The gain in the computing time is in factor\n5. We plan to increase this factor up to 20-30 after improving the algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 11:36:39 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Litvinenko", "Natalya", ""]]}, {"id": "1402.3789", "submitter": "Natalya Litvinenko", "authors": "Natalya Litvinenko", "title": "Parallel algorithms for problems of cluster analysis with very large\n  amount of data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we solve on GPUs massive problems with large amount of data,\nwhich are not appropriate for solution with the SIMD technology. For the given\nproblem we consider a three-level parallelization. The multithreading of CPU is\nused at the top level and graphic processors for massive computing. For solving\nproblems of cluster analysis on GPUs the nearest neighbor method (NNM) is\ndeveloped. This algorithm allows us to handle up to 2 millions records with\nnumber of features up to 25. Since sequential and parallel algorithms are\nfundamentally different, it is difficult to compare the computation times.\nHowever, some comparisons are made. The gain in the computing time is about 10\ntimes. We plan to increase this factor up to 50-100 after fine tuning of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 11:53:30 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Litvinenko", "Natalya", ""]]}, {"id": "1402.3809", "submitter": "Michael Heroux", "authors": "Michael A. Heroux", "title": "Toward Resilient Algorithms and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the high performance computing community has become\nincreasingly concerned that preserving the reliable, digital machine model will\nbecome too costly or infeasible. In this paper we discuss four approaches for\ndeveloping new algorithms that are resilient to hard and soft failures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 15:43:05 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 17:55:31 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Heroux", "Michael A.", ""]]}, {"id": "1402.4010", "submitter": "Lucas Benedi\\v{c}i\\v{c}", "authors": "Lucas Benedi\\v{c}i\\v{c}, Felipe A. Cruz, Tsuyoshi Hamada and Peter\n  Koro\\v{s}ec", "title": "A GRASS GIS parallel module for radio-propagation predictions", "comments": "13 pages, 12 figures and 2 tables. International Journal of\n  Geographical Information Science, 2014", "journal-ref": null, "doi": "10.1080/13658816.2013.879151", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographical information systems are ideal candidates for the application of\nparallel programming techniques, mainly because they usually handle large data\nsets. To help us deal with complex calculations over such data sets, we\ninvestigated the performance constraints of a classic master-worker parallel\nparadigm over a message-passing communication model. To this end, we present a\nnew approach that employs an external database in order to improve the\ncalculation/communication overlap, thus reducing the idle times for the worker\nprocesses. The presented approach is implemented as part of a parallel\nradio-coverage prediction tool for the GRASS environment. The prediction\ncalculation employs digital elevation models and land-usage data in order to\nanalyze the radio coverage of a geographical area. We provide an extended\nanalysis of the experimental results, which are based on real data from an LTE\nnetwork currently deployed in Slovenia. Based on the results of the\nexperiments, which were performed on a computer cluster, the new approach\nexhibits better scalability than the traditional master-worker approach. We\nsuccessfully tackled real-world data sets, while greatly reducing the\nprocessing time and saturating the hardware utilization.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 14:10:12 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Benedi\u010di\u010d", "Lucas", ""], ["Cruz", "Felipe A.", ""], ["Hamada", "Tsuyoshi", ""], ["Koro\u0161ec", "Peter", ""]]}, {"id": "1402.4247", "submitter": "Jae-Hyeon Parq", "authors": "Jae-Hyeon Parq, Erik Sevre and Sang-Mook Lee", "title": "Effects of Easy Hybrid Parallelization with CUDA for\n  Numerical-Atomic-Orbital Density Functional Theory Calculation", "comments": "20 pages, 3 figures", "journal-ref": "International Journal of Computer Applications, Volume 98, No.13,\n  pp. 20-27 (2014)", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We modified a MPI-friendly density functional theory (DFT) source code within\nhybrid parallelization including CUDA. Our objective is to find out how simple\nconversions within the hybrid parallelization with mid-range GPUs affect DFT\ncode not originally suitable to CUDA. We settled several rules of hybrid\nparallelization for numerical-atomic-orbital (NAO) DFT codes. The test was\nperformed on a magnetite material system with OpenMX code by utilizing a\nhardware system containing 2 Xeon E5606 CPUs and 2 Quadro 4000 GPUs. 3-way\nhybrid routines obtained a speedup of 7.55 while 2-way hybrid speedup by 10.94.\nGPUs with CUDA complement the efficiency of OpenMP and compensate CPUs'\nexcessive competition within MPI.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 08:25:47 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Parq", "Jae-Hyeon", ""], ["Sevre", "Erik", ""], ["Lee", "Sang-Mook", ""]]}, {"id": "1402.4623", "submitter": "Dario Berzano", "authors": "Dario Berzano, Jakob Blomer, Predrag Buncic, Ioannis Charalampidis,\n  Gerardo Ganis, Georgios Lestaris and Ren\\'e Meusel", "title": "PROOF as a Service on the Cloud: a Virtual Analysis Facility based on\n  the CernVM ecosystem", "comments": "Talk from Computing in High Energy and Nuclear Physics 2013\n  (CHEP2013), Amsterdam (NL), October 2013, 7 pages, 4 figures", "journal-ref": null, "doi": "10.1088/1742-6596/513/3/032007", "report-no": null, "categories": "cs.DC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PROOF, the Parallel ROOT Facility, is a ROOT-based framework which enables\ninteractive parallelism for event-based tasks on a cluster of computing nodes.\nAlthough PROOF can be used simply from within a ROOT session with no additional\nrequirements, deploying and configuring a PROOF cluster used to be not as\nstraightforward. Recently great efforts have been spent to make the\nprovisioning of generic PROOF analysis facilities with zero configuration, with\nthe added advantages of positively affecting both stability and scalability,\nmaking the deployment operations feasible even for the end user. Since a\ngrowing amount of large-scale computing resources are nowadays made available\nby Cloud providers in a virtualized form, we have developed the Virtual\nPROOF-based Analysis Facility: a cluster appliance combining the solid CernVM\necosystem and PoD (PROOF on Demand), ready to be deployed on the Cloud and\nleveraging some peculiar Cloud features such as elasticity. We will show how\nthis approach is effective both for sysadmins, who will have little or no\nconfiguration to do to run it on their Clouds, and for the end users, who are\nultimately in full control of their PROOF cluster and can even easily restart\nit by themselves in the unfortunate event of a major failure. We will also show\nhow elasticity leads to a more optimal and uniform usage of Cloud resources.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 11:14:53 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Berzano", "Dario", ""], ["Blomer", "Jakob", ""], ["Buncic", "Predrag", ""], ["Charalampidis", "Ioannis", ""], ["Ganis", "Gerardo", ""], ["Lestaris", "Georgios", ""], ["Meusel", "Ren\u00e9", ""]]}, {"id": "1402.4662", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Evgeniy Nikulchev, Simon Payain", "title": "Optimal Control of Applications for Hybrid Cloud Services", "comments": "4 pages, Proc. conf. (not published). arXiv admin note: text overlap\n  with arXiv:1402.1469", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Development of cloud computing enables to move Big Data in the hybrid cloud\nservices. This requires research of all processing systems and data structures\nfor provide QoS. Due to the fact that there are many bottlenecks requires\nmonitoring and control system when performing a query. The models and\noptimization criteria for the design of systems in a hybrid cloud\ninfrastructures are created. In this article suggested approaches and the\nresults of this build.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 13:49:50 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Nikulchev", "Evgeniy", ""], ["Payain", "Simon", ""]]}, {"id": "1402.4663", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Evgeniy Nikulchev, Simon Payain", "title": "Concept of Feedback in Future Computing Models to Cloud Systems", "comments": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:1402.1469", "journal-ref": "World Applied Sciences Journal 32 (7): 1394-1399, 2014", "doi": "10.5829/idosi.wasj.2014.32.07.588", "report-no": null, "categories": "cs.DC cs.NI cs.SY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Currently, it is urgent to ensure QoS in distributed computing systems. This\nbecame especially important to the development and spread of cloud services.\nBig data structures become heavily distributed. Necessary to consider the\ncommunication channels and data transmission systems and virtualization and\nscalability in future design of computational models in problems of designing\ncloud systems, evaluating the effectiveness of the algorithms, the assessment\nof economic performance data centers. Requires not only the monitoring of data\nflows and computing resources, but also the operational management of these\nresources to QoS provide. Such a tool may be just the introduction of feedback\nin computational models. The article presents the main dynamic model with\nfeedback as a basis for a new model of distributed computing processes. The\nresearch results are presented here. Formulated in this work can be used for\nother complex tasks - estimation of structural complexity of distributed\ndatabases, evaluation of dynamic characteristics of systems operating in the\nhybrid cloud, etc.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 13:53:22 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Nikulchev", "Evgeniy", ""], ["Payain", "Simon", ""]]}, {"id": "1402.4707", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "Standard protocol complexes for the immediate snapshot read/write model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a family of abstract simplicial complexes which we\ncall immediate snapshot complexes. Their definition is motivated by theoretical\ndistributed computing. Specifically, these complexes appear as protocol\ncomplexes in the general immediate snapshot execution model.\n  In order to define and to analyze the immediate snapshot complexes we use the\nnovel language of witness structures. We develop the rigorous mathematical\ntheory of witness structures and use it to prove several combinatorial as well\nas topological properties of the immediate snapshot complexes. In particular,\nwe prove that these complexes are simplicially homeomorphic to simplices.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 15:51:48 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1402.4758", "submitter": "Robert Green II", "authors": "Rachel Householder, Scott Arnold, and Robert Green", "title": "On Cloud-based Oversubscription", "comments": "7 pages, 3 figures", "journal-ref": "International Journal of Engineering Trends and Technology(IJETT),\n  V8(8),425-431 February 2014. ISSN:2231-5381", "doi": "10.14445/22315381/IJETT-V8P273", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rising trends in the number of customers turning to the cloud for their\ncomputing needs has made effective resource allocation imperative for cloud\nservice providers. In order to maximize profits and reduce waste, providers\nhave started to explore the role of oversubscribing cloud resources. However,\nthe benefits of cloud-based oversubscription are not without inherent risks.\nThis paper attempts to unveil the incentives, risks, and techniques behind\noversubscription in a cloud infrastructure. Additionally, an overview of the\ncurrent research that has been completed on this highly relevant topic is\nreviewed, and suggestions are made regarding potential avenues for future work.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 18:25:54 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 16:15:00 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Householder", "Rachel", ""], ["Arnold", "Scott", ""], ["Green", "Robert", ""]]}, {"id": "1402.4876", "submitter": "Ruibang Luo", "authors": "Sze-Hang Chan, Jeanno Cheung, Edward Wu, Heng Wang, Chi-Man Liu,\n  Xiaoqian Zhu, Shaoliang Peng, Ruibang Luo, Tak-Wah Lam", "title": "MICA: A fast short-read aligner that takes full advantage of Intel Many\n  Integrated Core Architecture (MIC)", "comments": "2 pages, 1 figure, 2 tables, submitted to Bioinformatics as an\n  application note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Short-read aligners have recently gained a lot of speed by\nexploiting the massive parallelism of GPU. An uprising alternative to GPU is\nIntel MIC; supercomputers like Tianhe-2, currently top of TOP500, is built with\n48,000 MIC boards to offer ~55 PFLOPS. The CPU-like architecture of MIC allows\nCPU-based software to be parallelized easily; however, the performance is often\ninferior to GPU counterparts as an MIC board contains only ~60 cores (while a\nGPU board typically has over a thousand cores). Results: To better utilize\nMIC-enabled computers for NGS data analysis, we developed a new short-read\naligner MICA that is optimized in view of MICs limitation and the extra\nparallelism inside each MIC core. Experiments on aligning 150bp paired-end\nreads show that MICA using one MIC board is 4.9 times faster than the BWA-MEM\n(using 6-core of a top-end CPU), and slightly faster than SOAP3-dp (using a\nGPU). Furthermore, MICAs simplicity allows very efficient scale-up when\nmultiple MIC boards are used in a node (3 cards give a 14.1-fold speedup over\nBWA-MEM). Summary: MICA can be readily used by MIC-enabled supercomputers for\nproduction purpose. We have tested MICA on Tianhe-2 with 90 WGS samples (17.47\nTera-bases), which can be aligned in an hour less than 400 nodes. MICA has\nimpressive performance even though the current MIC is at its initial stage of\ndevelopment (the next generation of MIC has been announced to release in late\n2014).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 03:19:50 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chan", "Sze-Hang", ""], ["Cheung", "Jeanno", ""], ["Wu", "Edward", ""], ["Wang", "Heng", ""], ["Liu", "Chi-Man", ""], ["Zhu", "Xiaoqian", ""], ["Peng", "Shaoliang", ""], ["Luo", "Ruibang", ""], ["Lam", "Tak-Wah", ""]]}, {"id": "1402.4958", "submitter": "Dan Dobre Dan Dobre", "authors": "Elli Androulaki, Christian Cachin, Dan Dobre, Marko Vukolic", "title": "Erasure-Coded Byzantine Storage with Separate Metadata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many distributed storage protocols have been introduced, a solution\nthat combines the strongest properties in terms of availability, consistency,\nfault-tolerance, storage complexity and the supported level of concurrency, has\nbeen elusive for a long time. Combining these properties is difficult,\nespecially if the resulting solution is required to be efficient and incur low\ncost. We present AWE, the first erasure-coded distributed implementation of a\nmulti-writer multi-reader read/write storage object that is, at the same time:\n(1) asynchronous, (2) wait-free, (3) atomic, (4) amnesic, (i.e., with data\nnodes storing a bounded number of values) and (5) Byzantine fault-tolerant\n(BFT) using the optimal number of nodes. Furthermore, AWE is efficient since it\ndoes not use public-key cryptography and requires data nodes that support only\nreads and writes, further reducing the cost of deployment and ownership of a\ndistributed storage solution. Notably, AWE stores metadata separately from\n$k$-out-of-$n$ erasure-coded fragments. This enables AWE to be the first BFT\nprotocol that uses as few as $2t+k$ data nodes to tolerate $t$ Byzantine nodes,\nfor any $k \\ge 1$.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 10:53:07 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Androulaki", "Elli", ""], ["Cachin", "Christian", ""], ["Dobre", "Dan", ""], ["Vukolic", "Marko", ""]]}, {"id": "1402.4986", "submitter": "Gang Mei", "authors": "Gang Mei and Hong Tian", "title": "Performance Impact of Data Layout on the GPU-accelerated IDW\n  Interpolation", "comments": "Preprint version for reviewing. 9 pages", "journal-ref": "SpringerPlus.2016, 5:104", "doi": "10.1186/s40064-016-1731-6", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on evaluating the performance impact of different data\nlayouts on the GPU-accelerated IDW interpolation. First, we redesign and\nimprove our previous GPU implementation that was performed by exploiting the\nfeature CUDA Dynamic Parallel (CDP). And then, we implement three versions of\nGPU implementations, i.e., the naive version, the tiled version, and the\nimproved CDP version, based on five layouts including the Structure of Arrays\n(SoA), the Array of Sturcutes (AoS), the Array of aligned Sturcutes (AoaS), the\nStructure of Arrays of aligned Structures (SoAoS), and the Hybrid layout.\nExperimental results show that: the layouts AoS and AoaS achieve better\nperformance than the layout SoA for both the naive version and tiled version,\nwhile the layout SoA is the best choice for the improved CDP version. We also\nobserve that: for the two combined data layouts (the SoAoS and the Hybrid),\nthere are no notable performance gains when compared to other three basic\nlayouts. We recommend that: in practical applications, the layout AoaS is the\nbest choice since the tiled version is the fastest one among the three versions\nof GPU implementations, especially on single precision.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 12:44:09 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Mei", "Gang", ""], ["Tian", "Hong", ""]]}, {"id": "1402.5205", "submitter": "Thilagavathi Jayamurugan", "authors": "D. Thilagavathi, Antony Selvadoss Thanamani", "title": "A Survey on Dynamic Job Scheduling in Grid Environment Based on\n  Heuristic Algorithms", "comments": "6 Pages, 1 FiguerE, \"Published with International Journal of Computer\n  Trends and Technology (IJCTT)\". arXiv admin note: contains excessive text\n  overlap with other internet sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Grids are a new trend in distributed computing systems. They\nallow the sharing of geographically distributed resources in an efficient way,\nextending the boundaries of what we perceive as distributed computing. Various\nsciences can benefit from the use of grids to solve CPU-intensive problems,\ncreating potential benefits to the entire society. Job scheduling is an\nintegrated part of parallel and distributed computing. It allows selecting\ncorrect match of resource for a particular job and thus increases the job\nthroughput and utilization of resources. Job should be scheduled in an\nautomatic way to make the system more reliable, accessible and less sensitive\nto subsystem failures. This paper provides a survey on various heuristic\nalgorithms, used for scheduling in grid.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 05:01:24 GMT"}], "update_date": "2014-08-24", "authors_parsed": [["Thilagavathi", "D.", ""], ["Thanamani", "Antony Selvadoss", ""]]}, {"id": "1402.5207", "submitter": "Maxwell Young", "authors": "Michael A. Bender and Jeremy T. Fineman and Seth Gilbert and Maxwell\n  Young", "title": "How to Scale Exponential Backoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized exponential backoff is a widely deployed technique for\ncoordinating access to a shared resource. A good backoff protocol should,\narguably, satisfy three natural properties: (i) it should provide constant\nthroughput, wasting as little time as possible; (ii) it should require few\nfailed access attempts, minimizing the amount of wasted effort; and (iii) it\nshould be robust, continuing to work efficiently even if some of the access\nattempts fail for spurious reasons. Unfortunately, exponential backoff has some\nwell-known limitations in two of these areas: it provides poor (sub-constant)\nthroughput (in the worst case), and is not robust (to resource acquisition\nfailures).\n  The goal of this paper is to \"fix\" exponential backoff by making it scalable,\nparticularly focusing on the case where processes arrive in an on-line,\nworst-case fashion. We present a relatively simple backoff\nprotocol~Re-Backoff~that has, at its heart, a version of exponential backoff.\nIt guarantees expected constant throughput with dynamic process arrivals and\nrequires only an expected polylogarithmic number of access attempts per\nprocess.\n  Re-Backoff is also robust to periods where the shared resource is unavailable\nfor a period of time. If it is unavailable for $D$ time slots, Re-Backoff\nprovides the following guarantees. When the number of packets is a finite $n$,\nthe average expected number of access attempts for successfully sending a\npacket is $O(\\log^2( n + D))$. In the infinite case, the average expected\nnumber of access attempts for successfully sending a packet is $O( \\log^2(\\eta)\n+ \\log^2(D) )$ where $\\eta$ is the maximum number of processes that are ever in\nthe system concurrently.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 05:47:48 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 14:51:16 GMT"}, {"version": "v3", "created": "Fri, 14 Nov 2014 06:09:09 GMT"}, {"version": "v4", "created": "Sun, 12 Jul 2015 06:09:28 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Bender", "Michael A.", ""], ["Fineman", "Jeremy T.", ""], ["Gilbert", "Seth", ""], ["Young", "Maxwell", ""]]}, {"id": "1402.5521", "submitter": "Gesualdo Scutari", "authors": "Francisco Facchinei and Gesualdo Scutari and Simone Sagratella", "title": "Parallel Selective Algorithms for Big Data Optimization", "comments": "This work is an extended version of the conference paper that has\n  been presented at IEEE ICASSP'14. The first and the second author contributed\n  equally to the paper. This revised version contains new numerical results on\n  non convex quadratic problems", "journal-ref": null, "doi": "10.1109/TSP.2015.2399858", "report-no": null, "categories": "cs.DC cs.IT cs.NA math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a decomposition framework for the parallel optimization of the sum\nof a differentiable (possibly nonconvex) function and a (block) separable\nnonsmooth, convex one. The latter term is usually employed to enforce structure\nin the solution, typically sparsity. Our framework is very flexible and\nincludes both fully parallel Jacobi schemes and Gauss- Seidel (i.e.,\nsequential) ones, as well as virtually all possibilities \"in between\" with only\na subset of variables updated at each iteration. Our theoretical convergence\nresults improve on existing ones, and numerical results on LASSO, logistic\nregression, and some nonconvex quadratic problems show that the new method\nconsistently outperforms existing algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 16:01:50 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2014 14:04:41 GMT"}, {"version": "v3", "created": "Thu, 6 Mar 2014 13:41:01 GMT"}, {"version": "v4", "created": "Fri, 8 Aug 2014 23:26:02 GMT"}, {"version": "v5", "created": "Tue, 9 Dec 2014 00:44:39 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Facchinei", "Francisco", ""], ["Scutari", "Gesualdo", ""], ["Sagratella", "Simone", ""]]}, {"id": "1402.5524", "submitter": "Olga Ohrimenko", "authors": "Olga Ohrimenko, Michael T. Goodrich, Roberto Tamassia, Eli Upfal", "title": "The Melbourne Shuffle: Improving Oblivious Storage in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, efficient, and secure data-oblivious randomized shuffle\nalgorithm. This is the first secure data-oblivious shuffle that is not based on\nsorting. Our method can be used to improve previous oblivious storage solutions\nfor network-based outsourcing of data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 16:29:22 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Ohrimenko", "Olga", ""], ["Goodrich", "Michael T.", ""], ["Tamassia", "Roberto", ""], ["Upfal", "Eli", ""]]}, {"id": "1402.5642", "submitter": "Ankur Sahai", "authors": "Ankur Sahai", "title": "VM Power Prediction in Distributed Systems for Maximizing Renewable\n  Energy Usage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of GreenPAD project it is important to predict the energy\nconsumption of individual (and mixture of) VMs / workload for optimal\nscheduling (running those VMs which require higher energy when there is more\ngreen energy available and vice-versa) in order to maximize green energy\nutilization.\n  For this we execute the following experiments on an Openstack cloud testbed\nconsisting of Fujitsu servers: VM energy measurement for different\nconfigurations (flavor + workload) and VM energy prediction for a new\nconfiguration. The automation framework for running these experiments uses bash\nscripts which call tools like 'stress' (simulating workloads), 'collected'\n(resource usage) and 'IPMI' (power measurement).\n  We propose a linear model for predicting the power usage of the VMs based on\nregression. We first collect the resource usage (using collected) and the\nassociated power usage (using IPMI) for different VM configurations and use\nthis to build a (multi-) regression model (between resource usage and VM energy\nconsumption). Then we use the information about the resource usage patterns of\nthe new workload to predict the power usage. For predicting power for mix of\nworkloads we execute (build a regression model based on) experiments with\nrandom workloads. We observe the highest energy usage for CPU-intensive\nworkloads followed by memory-intensive workloads.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 17:56:00 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Sahai", "Ankur", ""]]}, {"id": "1402.5748", "submitter": "Aarti", "authors": "Aarti, Sanjiv Kumar Tomar", "title": "A Parametric Chain based Routing Approach for Underwater Sensor Network", "comments": "4 pages,1 figure", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT) ,\n  4(1): 1492-1495, May 2013, Published by Seventh Sense Research Group", "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sensor network is one of the critical networks that is based on hardware\ncomponents as well the energy parameters. Because of this, such network\nrequires the optimization in all kind communication to improve the network\nlife. In case of underwater sensor network, the criticality of network is also\nincreased because of the random floating movement of the nodes. In this work, a\ncomposition of the multicast or broadcast communication is presented by the\ngeneration of aggregative path. The presented work is about to define a new\nchain based aggregative routing approach to provide the effective communication\nover the network. In this work, an effective aggregative path is suggested\nunder the different parameters of energy, distance and congestion analysis.\nBased on these parameters a trustful aggregative route will be generated so\nthat the network life will be improved.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 08:44:46 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Aarti", "", ""], ["Tomar", "Sanjiv Kumar", ""]]}, {"id": "1402.5770", "submitter": "Richard McClatchey", "authors": "Theo Lynn, Philip Healy, Richard McClatchey, John Morrison, Claus Pahl\n  and Brian Lee", "title": "The Case for Cloud Service Trustmarks and Assurance-as-a-Service", "comments": "6 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": "3rd Int Conf on Cloud Computing and Services Science (CLOSER).\n  Aachen, Germany May 2013. SciTePress", "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing represents a significant economic opportunity for Europe.\nHowever, this growth is threatened by adoption barriers largely related to\ntrust. This position paper examines trust and confidence issues in cloud\ncomputing and advances a case for addressing them through the implementation of\na novel trustmark scheme for cloud service providers. The proposed trustmark\nwould be both active and dynamic featuring multi-modal information about the\nperformance of the underlying cloud service. The trustmarks would be informed\nby live performance data from the cloud service provider, or ideally an\nindependent third-party accountability and assurance service that would\ncommunicate up-to-date information relating to service performance and\ndependability. By combining assurance measures with a remediation scheme, cloud\nservice providers could both signal dependability to customers and the wider\nmarketplace and provide customers, auditors and regulators with a mechanism for\ndetermining accountability in the event of failure or non-compliance. As a\nresult, the trustmarks would convey to consumers of cloud services and other\nstakeholders that strong assurance and accountability measures are in place for\nthe service in question and thereby address trust and confidence issues in\ncloud computing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 10:05:04 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Lynn", "Theo", ""], ["Healy", "Philip", ""], ["McClatchey", "Richard", ""], ["Morrison", "John", ""], ["Pahl", "Claus", ""], ["Lee", "Brian", ""]]}, {"id": "1402.5830", "submitter": "Luca Vassio Mr", "authors": "Enrico Ampellio and Luca Vassio", "title": "A hybrid swarm-based algorithm for single-objective optimization\n  problems involving high-cost analyses", "comments": "19 pages, 4 figures, Springer Swarm Intelligence", "journal-ref": "Swarm Intelligence 10, 99-121 (2016)", "doi": "10.1007/s11721-016-0121-6", "report-no": null, "categories": "math.OC cs.AI cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many technical fields, single-objective optimization procedures in\ncontinuous domains involve expensive numerical simulations. In this context, an\nimprovement of the Artificial Bee Colony (ABC) algorithm, called the Artificial\nsuper-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide\nfast convergence speed, high solution accuracy and robust performance over a\nwide range of problems. It implements enhancements of the ABC structure and\nhybridizations with interpolation strategies. The latter are inspired by the\nquadratic trust region approach for local investigation and by an efficient\nglobal optimizer for separable problems. Each modification and their combined\neffects are studied with appropriate metrics on a numerical benchmark, which is\nalso used for comparing AsBeC with some effective ABC variants and other\nderivative-free algorithms. In addition, the presented algorithm is validated\non two recent benchmarks adopted for competitions in international conferences.\nResults show remarkable competitiveness and robustness for AsBeC.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 14:06:52 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 14:55:37 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ampellio", "Enrico", ""], ["Vassio", "Luca", ""]]}, {"id": "1402.6208", "submitter": "Thomas Lansdall-Welfare", "authors": "Ilias Flaounas, Thomas Lansdall-Welfare, Panagiota Antonakaki, Nello\n  Cristianini", "title": "The Anatomy of a Modular System for Media Content Analysis", "comments": "Updated to include previously missing figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent systems for the annotation of media content are increasingly\nbeing used for the automation of parts of social science research. In this\ndomain the problem of integrating various Artificial Intelligence (AI)\nalgorithms into a single intelligent system arises spontaneously. As part of\nour ongoing effort in automating media content analysis for the social\nsciences, we have built a modular system by combining multiple AI modules into\na flexible framework in which they can cooperate in complex tasks. Our system\ncombines data gathering, machine translation, topic classification, extraction\nand annotation of entities and social networks, as well as many other tasks\nthat have been perfected over the past years of AI research. Over the last few\nyears, it has allowed us to realise a series of scientific studies over a vast\nrange of applications including comparative studies between news outlets and\nmedia content in different countries, modelling of user preferences, and\nmonitoring public mood. The framework is flexible and allows the design and\nimplementation of modular agents, where simple modules cooperate in the\nannotation of a large dataset without central coordination.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 15:45:16 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:25:07 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Flaounas", "Ilias", ""], ["Lansdall-Welfare", "Thomas", ""], ["Antonakaki", "Panagiota", ""], ["Cristianini", "Nello", ""]]}, {"id": "1402.6601", "submitter": "Gr\\'egory Mouni\\'e", "authors": "Rapha\\\"el Bleuse and Thierry Gautier and Jo\\~ao V. F. Lima and\n  Gr\\'egory Mouni\\'e and Denis Trystram", "title": "Scheduling data flow program in xkaapi: A new affinity based Algorithm\n  for Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-09873-9_47", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Efficient implementations of parallel applications on heterogeneous hybrid\narchitectures require a careful balance between computations and communications\nwith accelerator devices. Even if most of the communication time can be\noverlapped by computations, it is essential to reduce the total volume of\ncommunicated data. The literature therefore abounds with ad-hoc methods to\nreach that balance, but that are architecture and application dependent. We\npropose here a generic mechanism to automatically optimize the scheduling\nbetween CPUs and GPUs, and compare two strategies within this mechanism: the\nclassical Heterogeneous Earliest Finish Time (HEFT) algorithm and our new,\nparametrized, Distributed Affinity Dual Approximation algorithm (DADA), which\nconsists in grouping the tasks by affinity before running a fast dual\napproximation. We ran experiments on a heterogeneous parallel machine with six\nCPU cores and eight NVIDIA Fermi GPUs. Three standard dense linear algebra\nkernels from the PLASMA library have been ported on top of the Xkaapi runtime.\nWe report their performances. It results that HEFT and DADA perform well for\nvarious experimental conditions, but that DADA performs better for larger\nsystems and number of GPUs, and, in most cases, generates much lower data\ntransfers than HEFT to achieve the same performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 16:37:01 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 13:05:23 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Bleuse", "Rapha\u00ebl", ""], ["Gautier", "Thierry", ""], ["Lima", "Jo\u00e3o V. F.", ""], ["Mouni\u00e9", "Gr\u00e9gory", ""], ["Trystram", "Denis", ""]]}, {"id": "1402.6942", "submitter": "Jakub Nalepa", "authors": "Jakub Nalepa and Zbigniew J. Czech", "title": "A Parallel Memetic Algorithm to Solve the Vehicle Routing Problem with\n  Time Windows", "comments": "15 pages", "journal-ref": "Studia Informatica 33 (1), pp 91-106 (2012)", "doi": null, "report-no": null, "categories": "cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parallel memetic algorithm for solving the vehicle\nrouting problem with time windows (VRPTW). The VRPTW is a well-known NP-hard\ndiscrete optimization problem with two objectives. The main objective is to\nminimize the number of vehicles serving customers scattered on the map, and the\nsecond one is to minimize the total distance traveled by the vehicles. Here,\nthe fleet size is minimized in the first phase of the proposed method using the\nparallel heuristic algorithm (PHA), and the traveled distance is minimized in\nthe second phase by the parallel memetic algorithm (PMA). In both parallel\nalgorithms, the parallel components co-operate periodically in order to\nexchange the best solutions found so far. An extensive experimental study\nperformed on the Gehring and Homberger's benchmark proves the high convergence\ncapabilities and robustness of both PHA and PMA. Also, we present the speedup\nanalysis of the PMA.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 15:41:05 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Nalepa", "Jakub", ""], ["Czech", "Zbigniew J.", ""]]}, {"id": "1402.6964", "submitter": "Austin Benson", "authors": "Austin R. Benson, Jason D. Lee, Bartek Rajwa, David F. Gleich", "title": "Scalable methods for nonnegative matrix factorizations of near-separable\n  tall-and-skinny matrices", "comments": null, "journal-ref": "Proceedings of Neural Information Processing Systems, 2014", "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous algorithms are used for nonnegative matrix factorization under the\nassumption that the matrix is nearly separable. In this paper, we show how to\nmake these algorithms efficient for data matrices that have many more rows than\ncolumns, so-called \"tall-and-skinny matrices\". One key component to these\nimproved methods is an orthogonal matrix transformation that preserves the\nseparability of the NMF problem. Our final methods need a single pass over the\ndata matrix and are suitable for streaming, multi-core, and MapReduce\narchitectures. We demonstrate the efficacy of these algorithms on\nterabyte-sized synthetic matrices and real-world matrices from scientific\ncomputing and bioinformatics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 16:41:26 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Lee", "Jason D.", ""], ["Rajwa", "Bartek", ""], ["Gleich", "David F.", ""]]}, {"id": "1402.7216", "submitter": "Jana Paz\\'urikov\\'a", "authors": "Jana Paz\\'urikov\\'a", "title": "Large-Scale Molecular Dynamics Simulations for Highly Parallel\n  Infrastructures", "comments": "thesis proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Computational chemistry allows researchers to experiment in sillico: by\nrunning a computer simulations of a biological or chemical processes of\ninterest. Molecular dynamics with molecular mechanics model of interactions\nsimulates N-body problem of atoms$-$it computes movements of atoms according to\nNewtonian physics and empirical descriptions of atomic electrostatic\ninteractions. These simulations require high performance computing resources,\nas evaluations within each step are computationally demanding and billions of\nsteps are needed to reach interesting timescales. Current methods decompose the\nspatial domain of the problem and calculate on parallel/distributed\ninfrastructures. Even the methods with the highest strong scaling hit the limit\nat half a million cores: they are not able to cut the time to result if\nprovided with more processors. At the dawn of exascale computing with massively\nparallel computational resources, we want to increase the level of parallelism\nby incorporating parallel-in-time computation to molecular dynamics\nsimulations. Calculation of results in several successive time points\nsimultaneously without a priori knowledge has been examined with no major\nsuccess. We will study and implement a novel combinations of methods that\naccording to our theoretical analyses should achieve promising speed-up\ncompared to sequential-in-time calculation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 11:59:08 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Paz\u00farikov\u00e1", "Jana", ""]]}, {"id": "1402.7223", "submitter": "Henning Hasemann", "authors": "Dennis Boldt, Henning Hasemann, Alexander Kr\\\"oller, Marcel Karnstedt,\n  Christian von der Weth", "title": "SPARQL for Networks of Embedded Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web (or Web of Data) represents the successful efforts towards\nlinking and sharing data over the Web. The cornerstones of the Web of Data are\nRDF as data format and SPARQL as de-facto standard query language. Recent\ntrends show the evolution of the Web of Data towards the Web of Things,\nintegrating embedded devices and smart objects. Data stemming from such devices\ndo not share a common format, making the integration and querying impossible.\nTo overcome this problem, we present our approach to make embedded systems\nfirst-class citizens of the Web of Things. Our framework abstracts from\nindividual deployments to represent them as common data sources in line with\nthe ideas behind the Semantic Web. This includes the execution of arbitrary\nSPARQL queries over the data from a pool of embedded devices and/or external\ndata sources. Handling verbose RDF data and executing SPARQL queries in an\nembedded network poses major challenges to minimize the involved processing and\ncommunication cost. We therefore present an in-network query processor aiming\nto push processing steps onto devices. We demonstrate the practical application\nand the potential benefits of our framework in a comprehensive evaluation using\na real-world deployment and a range of SPARQL queries stemming from a common\nuse case of the Web of Things.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 12:36:12 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Boldt", "Dennis", ""], ["Hasemann", "Henning", ""], ["Kr\u00f6ller", "Alexander", ""], ["Karnstedt", "Marcel", ""], ["von der Weth", "Christian", ""]]}, {"id": "1402.7228", "submitter": "Henning Hasemann", "authors": "Henning Hasemann, Alexander Kr\\\"oller, Max Pagel", "title": "The Wiselib TupleStore: A Modular RDF Database for the Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things movement provides self-configuring and universally\ninteroperable devices. While such devices are often built with a specific\napplication in mind, they often turn out to be useful in other contexts as\nwell. We claim that by describing the devices' knowledge in a universal way,\nIoT devices can become first-class citizens in the Internet. They can then\nexchange data between heterogeneous hardware, different applications and large\ndata sources on the Web. Our key idea --- in contrast to most existing\napproaches --- is to not restrict the domain of knowledge that can be expressed\non the device in any way and, at the same time, allow this knowledge to be\nmachine-understandable and linkable across different locations.\n  We propose an architecture that allows to connect embedded devices to the\nSemantic Web by expressing their knowledge in the Resource Description\nFramework (RDF). We present the Wiselib TupleStore, a modular embedded database\ntailored specifically for the storage of RDF. The Wiselib TupleStore is\nportable to many platforms including Contiki and TinyOS and allows a variety of\ntrade-offs, making it able to scale to a large variety of hardware scenarios.\nWe discuss the applicability of RDF to heterogeneous resource-constrained\ndevices and compare our system to the existing embedded tuple stores Antelope\nand TeenyLIME.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 12:52:47 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Hasemann", "Henning", ""], ["Kr\u00f6ller", "Alexander", ""], ["Pagel", "Max", ""]]}, {"id": "1402.7254", "submitter": "Shenggen Zheng", "authors": "Jozef Gruska, Daowen Qiu, Shenggen Zheng", "title": "Generalizations of the distributed Deutsch-Jozsa promise problem", "comments": "we correct some errors of and improve the presentation the previous\n  version. arXiv admin note: substantial text overlap with arXiv:1309.7739", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the {\\em distributed Deutsch-Jozsa promise problem}, two parties are to\ndetermine whether their respective strings $x,y\\in\\{0,1\\}^n$ are at the {\\em\nHamming distance} $H(x,y)=0$ or $H(x,y)=\\frac{n}{2}$. Buhrman et al. (STOC' 98)\nproved that the exact {\\em quantum communication complexity} of this problem is\n${\\bf O}(\\log {n})$ while the {\\em deterministic communication complexity} is\n${\\bf \\Omega}(n)$. This was the first impressive (exponential) gap between\nquantum and classical communication complexity.\n  In this paper, we generalize the above distributed Deutsch-Jozsa promise\nproblem to determine, for any fixed $\\frac{n}{2}\\leq k\\leq n$, whether\n$H(x,y)=0$ or $H(x,y)= k$, and show that an exponential gap between exact\nquantum and deterministic communication complexity still holds if $k$ is an\neven such that $\\frac{1}{2}n\\leq k<(1-\\lambda) n$, where $0<\n\\lambda<\\frac{1}{2}$ is given. We also deal with a promise version of the\nwell-known {\\em disjointness} problem and show also that for this promise\nproblem there exists an exponential gap between quantum (and also\nprobabilistic) communication complexity and deterministic communication\ncomplexity of the promise version of such a disjointness problem. Finally, some\napplications to quantum, probabilistic and deterministic finite automata of the\nresults obtained are demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 14:21:06 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 08:47:19 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2015 14:12:18 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gruska", "Jozef", ""], ["Qiu", "Daowen", ""], ["Zheng", "Shenggen", ""]]}]