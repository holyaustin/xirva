[{"id": "1701.00180", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz, Marco A. Contreras, and Jun Zhang", "title": "A scalable approach for tree segmentation within small-footprint\n  airborne LiDAR data", "comments": "The replacement version is exactly the same and only the journal\n  biblio information and the DOI of the published version was added", "journal-ref": "Computers and Geosciences 102 (pp. 139-147): Elsevier (2017)", "doi": "10.1016/j.cageo.2017.02.017", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a distributed approach that scales up to segment tree\ncrowns within a LiDAR point cloud representing an arbitrarily large forested\narea. The approach uses a single-processor tree segmentation algorithm as a\nbuilding block in order to process the data delivered in the shape of tiles in\nparallel. The distributed processing is performed in a master-slave manner, in\nwhich the master maintains the global map of the tiles and coordinates the\nslaves that segment tree crowns within and across the boundaries of the tiles.\nA minimal bias was introduced to the number of detected trees because of trees\nlying across the tile boundaries, which was quantified and adjusted for.\nTheoretical and experimental analyses of the runtime of the approach revealed a\nnear linear speedup. The estimated number of trees categorized by crown class\nand the associated error margins as well as the height distribution of the\ndetected trees aligned well with field estimations, verifying that the\ndistributed approach works correctly. The approach enables providing\ninformation of individual tree locations and point cloud segments for a\nforest-level area in a timely manner, which can be used to create detailed\nremotely sensed forest inventories. Although the approach was presented for\ntree segmentation within LiDAR point clouds, the idea can also be generalized\nto scale up processing other big spatial datasets.\n  Highlights: - A scalable distributed approach for tree segmentation was\ndeveloped and theoretically analyzed. - ~2 million trees in a 7440 ha forest\nwas segmented in 2.5 hours using 192 cores. - 2% false positive trees were\nidentified as a result of the distributed run. - The approach can be used to\nscale up processing other big spatial data\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 00:10:42 GMT"}, {"version": "v2", "created": "Sun, 19 Mar 2017 21:13:31 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""], ["Zhang", "Jun", ""]]}, {"id": "1701.00186", "submitter": "Bogdan Chlebus", "authors": "Lakshmi Anantharamu and Bogdan S. Chlebus and Dariusz R. Kowalski and\n  Mariusz A. Rokicki", "title": "Packet Latency of Deterministic Broadcasting in Adversarial Multiple\n  Access Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study broadcasting in multiple access channels with dynamic packet\narrivals and jamming. Communication environments are represented by adversarial\nmodels that specify constraints on packet arrivals and jamming. We consider\ndeterministic distributed broadcast algorithms and give upper bounds on the\nworst-case packet latency and the number of queued packets in relation to the\nparameters defining adversaries. Packet arrivals are determined by a rate of\ninjections and a number of packets that can be generated in one round. Jamming\nis constrained by a rate with which an adversary can jam rounds and by a number\nof consecutive rounds that can be jammed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 02:05:41 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 17:27:59 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Anantharamu", "Lakshmi", ""], ["Chlebus", "Bogdan S.", ""], ["Kowalski", "Dariusz R.", ""], ["Rokicki", "Mariusz A.", ""]]}, {"id": "1701.00335", "submitter": "Philippe Robert S.", "authors": "Wen Sun and V\\'eronique Simon and S\\'ebastien Monnet and Philippe\n  Robert and Pierre Sens", "title": "Analysis of a Stochastic Model of Replication in Large Distributed\n  Storage Systems: A Mean-Field Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems such as Hadoop File System or Google File System\n(GFS) ensure data availability and durability using replication. This paper is\nfocused on the analysis of the efficiency of replication mechanism that\ndetermines the location of the copies of a given file at some server. The\nvariability of the loads of the nodes of the network is investigated for\nseveral policies. Three replication mechanisms are tested against simulations\nin the context of a real implementation of a such a system: Random, Least\nLoaded and Power of Choice.\n  The simulations show that some of these policies may lead to quite unbalanced\nsituations: if $\\beta$ is the average number of copies per node it turns out\nthat, at equilibrium, the load of the nodes may exhibit a high variability. It\nis shown in this paper that a simple variant of a power of choice type\nalgorithm has a striking effect on the loads of the nodes: at equilibrium, the\ndistribution of the load of a node has a bounded support, most of nodes have a\nload less than $2\\beta$ which is an interesting property for the design of the\nstorage space of these systems.\n  Mathematical models are introduced and investigated to explain this\ninteresting phenomenon. The analysis of these systems turns out to be quite\ncomplicated mainly because of the large dimensionality of the state spaces\ninvolved. Our study relies on probabilistic methods, mean-field analysis, to\nanalyze the asymptotic behavior of an arbitrary node of the network when the\ntotal number of nodes gets large. An additional ingredient is the use of\nstochastic calculus with marked Poisson point processes to establish some of\nour results.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 08:39:51 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 06:51:33 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Sun", "Wen", ""], ["Simon", "V\u00e9ronique", ""], ["Monnet", "S\u00e9bastien", ""], ["Robert", "Philippe", ""], ["Sens", "Pierre", ""]]}, {"id": "1701.00383", "submitter": "Adnan Ashraf", "authors": "Adnan Ashraf and Ivan Porres", "title": "Multi-objective dynamic virtual machine consolidation in the cloud using\n  ant colony system", "comments": "The manuscript has been accepted for publication in the International\n  Journal of Parallel, Emergent and Distributed Systems", "journal-ref": null, "doi": "10.1080/17445760.2017.1278601", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel multi-objective ant colony system algorithm\nfor virtual machine (VM) consolidation in cloud data centers. The proposed\nalgorithm builds VM migration plans, which are then used to minimize\nover-provisioning of physical machines (PMs) by consolidating VMs on\nunder-utilized PMs. It optimizes two objectives that are ordered by their\nimportance. The first and foremost objective in the proposed algorithm is to\nmaximize the number of released PMs. Moreover, since VM migration is a\nresource-intensive operation, it also tries to minimize the number of VM\nmigrations. The proposed algorithm is empirically evaluated in a series of\nexperiments. The experimental results show that the proposed algorithm provides\nan efficient solution for VM consolidation in cloud data centers. Moreover, it\noutperforms two existing ant colony optimization based VM consolidation\nalgorithms in terms of number of released PMs and number of VM migrations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 13:30:00 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Ashraf", "Adnan", ""], ["Porres", "Ivan", ""]]}, {"id": "1701.00384", "submitter": "Chathura Sarathchandra Magurawalage", "authors": "Chathura Sarathchandra Magurawalage, Kun Yang, Ritosa Patrik, Michael\n  Georgiades, Kezhi Wang", "title": "A Resource Management Protocol for Mobile Cloud Using Auto-Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud radio access networks (C-RAN) and Mobile Cloud Computing (MCC) have\nemerged as promising candidates for the next generation access network\ntechniques. MCC enables resource limited mobile devices to offload\ncomputationally intensive tasks to the cloud, while C-RAN offers a technology\nthat addresses the increasing mobile traffic. In this paper, we propose a\nprotocol for task offloading and for managing resources in both C-RAN and\nmobile cloud together using a centralised controller. Experiments on resource\nmanagement using cloud auto-scaling shows that resource (CPU, RAM, Storage)\nscaling times vary.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 13:36:53 GMT"}, {"version": "v2", "created": "Sun, 15 Jan 2017 16:09:18 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2017 12:59:02 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Magurawalage", "Chathura Sarathchandra", ""], ["Yang", "Kun", ""], ["Patrik", "Ritosa", ""], ["Georgiades", "Michael", ""], ["Wang", "Kezhi", ""]]}, {"id": "1701.00503", "submitter": "George M Slota", "authors": "George M Slota and Sivasankaran Rajamanickam and Kamesh Madduri", "title": "Distributed Graph Layout for Scalable Small-world Network Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The in-memory graph layout or organization has a considerable impact on the\ntime and energy efficiency of distributed memory graph computations. It affects\nmemory locality, inter-task load balance, communication time, and overall\nmemory utilization. Graph layout could refer to partitioning or replication of\nvertex and edge arrays, selective replication of data structures that hold\nmeta-data, and reordering vertex and edge identifiers. In this work, we present\nDGL, a fast, parallel, and memory-efficient distributed graph layout strategy\nthat is specifically designed for small-world networks (low-diameter graphs\nwith skewed vertex degree distributions). Label propagation-based partitioning\nand a scalable BFS-based ordering are the main steps in the layout strategy. We\nshow that the DGL layout can significantly improve end-to-end performance of\nfive challenging graph analytics workloads: PageRank, a parallel subgraph\nenumeration program, tuned implementations of breadth-first search and\nsingle-source shortest paths, and RDF3X-MPI, a distributed SPARQL query\nprocessing engine. Using these benchmarks, we additionally offer a\ncomprehensive analysis on how graph layout affects the performance of graph\nanalytics with variable computation and communication characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 19:17:55 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Slota", "George M", ""], ["Rajamanickam", "Sivasankaran", ""], ["Madduri", "Kamesh", ""]]}, {"id": "1701.00546", "submitter": "Sabeur Aridhi", "authors": "Sabeur Aridhi, Alberto Montresor, Yannis Velegrakis", "title": "BLADYG: A Graph Processing Framework for Large Dynamic Graphs", "comments": null, "journal-ref": null, "doi": "10.1016/j.bdr.2017.05.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distributed processing of large dynamic graphs has become very\npopular, especially in certain domains such as social network analysis, Web\ngraph analysis and spatial network analysis. In this context, many\ndistributed/parallel graph processing systems have been proposed, such as\nPregel, GraphLab, and Trinity. These systems can be divided into two\ncategories: (1) vertex-centric and (2) block-centric approaches. In\nvertex-centric approaches, each vertex corresponds to a process, and message\nare exchanged among vertices. In block-centric approaches, the unit of\ncomputation is a block, a connected subgraph of the graph, and message\nexchanges occur among blocks. In this paper, we are considering the issues of\nscale and dynamism in the case of block-centric approaches. We present bladyg,\na block-centric framework that addresses the issue of dynamism in large-scale\ngraphs. We present an implementation of BLADYG on top of akka framework. We\nexperimentally evaluate the performance of the proposed framework.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 22:25:33 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Aridhi", "Sabeur", ""], ["Montresor", "Alberto", ""], ["Velegrakis", "Yannis", ""]]}, {"id": "1701.00609", "submitter": "Shuai Li", "authors": "Shuai Li", "title": "Akid: A Library for Neural Network Research and Production from a\n  Dataism Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a revolutionary but immature technique that is fast\nevolving and heavily relies on data. To benefit from the newest development and\nnewly available data, we want the gap between research and production as small\nas possibly. On the other hand, differing from traditional machine learning\nmodels, neural network is not just yet another statistic model, but a model for\nthe natural processing engine --- the brain. In this work, we describe a neural\nnetwork library named {\\texttt akid}. It provides higher level of abstraction\nfor entities (abstracted as blocks) in nature upon the abstraction done on\nsignals (abstracted as tensors) by Tensorflow, characterizing the dataism\nobservation that all entities in nature processes input and emit out in some\nways. It includes a full stack of software that provides abstraction to let\nresearchers focus on research instead of implementation, while at the same time\nthe developed program can also be put into production seamlessly in a\ndistributed environment, and be production ready. At the top application stack,\nit provides out-of-box tools for neural network applications. Lower down, akid\nprovides a programming paradigm that lets user easily build customized models.\nThe distributed computing stack handles the concurrency and communication, thus\nletting models be trained or deployed to a single GPU, multiple GPUs, or a\ndistributed environment without affecting how a model is specified in the\nprogramming paradigm stack. Lastly, the distributed deployment stack handles\nhow the distributed computing is deployed, thus decoupling the research\nprototype environment with the actual production environment, and is able to\ndynamically allocate computing resources, so development (Devs) and operations\n(Ops) could be separated. Please refer to http://akid.readthedocs.io/en/latest/\nfor documentation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 09:18:22 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Li", "Shuai", ""]]}, {"id": "1701.00773", "submitter": "Mark Strembeck", "authors": "Ema Ku\\v{s}en, Mark Strembeck", "title": "Security-related Research in Ubiquitous Computing -- Results of a\n  Systematic Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In an endeavor to reach the vision of ubiquitous computing where users are\nable to use pervasive services without spatial and temporal constraints, we are\nwitnessing a fast growing number of mobile and sensor-enhanced devices becoming\navailable. However, in order to take full advantage of the numerous benefits\noffered by novel mobile devices and services, we must address the related\nsecurity issues. In this paper, we present results of a systematic literature\nreview (SLR) on security-related topics in ubiquitous computing environments.\nIn our study, we found 5165 scientific contributions published between 2003 and\n2015. We applied a systematic procedure to identify the threats,\nvulnerabilities, attacks, as well as corresponding defense mechanisms that are\ndiscussed in those publications. While this paper mainly discusses the results\nof our study, the corresponding SLR protocol which provides all details of the\nSLR is also publicly available for download.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 18:56:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Ku\u0161en", "Ema", ""], ["Strembeck", "Mark", ""]]}, {"id": "1701.00854", "submitter": "Paul McKenney", "authors": "Paul E. McKenney", "title": "Is Parallel Programming Hard, And, If So, What Can You Do About It?\n  (Second Edition)", "comments": "595 pages. For a summary of changes since the 2014 First Edition,\n  please see https://paulmck.livejournal.com/60291.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this book is to help you program shared-memory parallel\nsystems without risking your sanity. Nevertheless, you should think of the\ninformation in this book as a foundation on which to build, rather than as a\ncompleted cathedral. Your mission, if you choose to accept, is to help make\nfurther progress in the exciting field of parallel programming-progress that\nwill in time render this book obsolete.\n  Parallel programming in the 21st century is no longer focused solely on\nscience, research, and grand-challenge projects. And this is all to the good,\nbecause it means that parallel programming is becoming an engineering\ndiscipline. Therefore, as befits an engineering discipline, this book examines\nspecific parallel-programming tasks and describes how to approach them. In some\nsurprisingly common cases, these tasks can be automated.\n  This book is written in the hope that presenting the engineering discipline\nunderlying successful parallel-programming projects will free a new generation\nof parallel hackers from the need to slowly and painstakingly reinvent old\nwheels, enabling them to instead focus their energy and creativity on new\nfrontiers. However, what you get from this book will be determined by what you\nput into it. It is hoped that simply reading this book will be helpful, and\nthat working the Quick Quizzes will be even more helpful. However, the best\nresults come from applying the techniques taught in this book to real-life\nproblems. As always, practice makes perfect.\n  But no matter how you approach it, we sincerely hope that parallel\nprogramming brings you at least as much fun, excitement, and challenge that it\nhas brought to us!\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 22:35:03 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 16:20:37 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 16:09:12 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["McKenney", "Paul E.", ""]]}, {"id": "1701.00981", "submitter": "Marcus Brandenburger", "authors": "Marcus Brandenburger, Christian Cachin, Matthias Lorenz, R\\\"udiger\n  Kapitza", "title": "Rollback and Forking Detection for Trusted Execution Environments using\n  Lightweight Collective Memory", "comments": "This is the authors' version of an article published in the\n  Proceedings of the 47th International Conference on Dependable Systems and\n  Networks (DSN'17), Denver, USA, 26-29 June 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel hardware-aided trusted execution environments, as provided by Intel's\nSoftware Guard Extensions (SGX), enable to execute applications in a secure\ncontext that enforces confidentiality and integrity of the application state\neven when the host system is misbehaving. While this paves the way towards\nsecure and trustworthy cloud computing, essential system support to protect\npersistent application state against rollback and forking attacks is missing.\n  In this paper we present LCM - a lightweight protocol to establish a\ncollective memory amongst all clients of a remote application to detect\nintegrity and consistency violations. LCM enables the detection of rollback\nattacks against the remote application, enforces the consistency notion of\nfork-linearizability and notifies clients about operation stability. The\nprotocol exploits the trusted execution environment, complements it with simple\nclient-side operations, and maintains only small, constant storage at the\nclients. This simplifies the solution compared to previous approaches, where\nthe clients had to verify all operations initiated by other clients. We have\nimplemented LCM and demonstrated its advantages with a key-value store\napplication. The evaluation shows that it introduces low network and\ncomputation overhead; in particular, a LCM-protected key-value store achieves\n0.72x - 0.98x of a SGX-secured key-value store throughput.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 12:18:18 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:14:54 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Brandenburger", "Marcus", ""], ["Cachin", "Christian", ""], ["Lorenz", "Matthias", ""], ["Kapitza", "R\u00fcdiger", ""]]}, {"id": "1701.00997", "submitter": "Severin Sadjina", "authors": "Severin Sadjina, Lars T. Kyllingstad, Martin Rindar{\\o}y, Stian\n  Skjong, Vilmar {\\AE}s{\\o}y, Dariusz Eirik Fathi, Vahid Hassani, Trond\n  Johnsen, J{\\o}rgen Bremnes Nielsen, Eilif Pedersen", "title": "Distributed Co-Simulation of Maritime Systems and Operations", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we present the concept of an open virtual prototyping framework for\nmaritime systems and operations that enables its users to develop re-usable\ncomponent or subsystem models, and combine them in full-system simulations for\nprototyping, verification, training, and performance studies. This framework\nconsists of a set of guidelines for model coupling, high-level and low-level\ncoupling interfaces to guarantee interoperability, a full-system simulation\nsoftware, and example models and demonstrators. We discuss the requirements for\nsuch a framework, address the challenges and the possibilities in fulfilling\nthem, and aim to give a list of best practices for modular and efficient\nvirtual prototyping and full-system simulation. The context of our work is\nwithin maritime systems and operations, but the issues and solutions we present\nhere are general enough to be of interest to a much broader audience, both\nindustrial and scientific.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:23:04 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Sadjina", "Severin", ""], ["Kyllingstad", "Lars T.", ""], ["Rindar\u00f8y", "Martin", ""], ["Skjong", "Stian", ""], ["\u00c6s\u00f8y", "Vilmar", ""], ["Fathi", "Dariusz Eirik", ""], ["Hassani", "Vahid", ""], ["Johnsen", "Trond", ""], ["Nielsen", "J\u00f8rgen Bremnes", ""], ["Pedersen", "Eilif", ""]]}, {"id": "1701.01170", "submitter": "Yangzihao Wang", "authors": "Yangzihao Wang, Yuechao Pan, Andrew Davidson, Yuduo Wu, Carl Yang,\n  Leyuan Wang, Muhammad Osama, Chenshan Yuan, Weitang Liu, Andy T. Riffel and\n  John D. Owens", "title": "Gunrock: GPU Graph Analytics", "comments": "52 pages, invited paper to ACM Transactions on Parallel Computing\n  (TOPC), an extended version of PPoPP'16 paper \"Gunrock: A High-Performance\n  Graph Processing Library on the GPU\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For large-scale graph analytics on the GPU, the irregularity of data access\nand control flow, and the complexity of programming GPUs, have presented two\nsignificant challenges to developing a programmable high-performance graph\nlibrary. \"Gunrock\", our graph-processing system designed specifically for the\nGPU, uses a high-level, bulk-synchronous, data-centric abstraction focused on\noperations on a vertex or edge frontier. Gunrock achieves a balance between\nperformance and expressiveness by coupling high performance GPU computing\nprimitives and optimization strategies with a high-level programming model that\nallows programmers to quickly develop new graph primitives with small code size\nand minimal GPU programming knowledge. We characterize the performance of\nvarious optimization strategies and evaluate Gunrock's overall performance on\ndifferent GPU architectures on a wide range of graph primitives that span from\ntraversal-based algorithms and ranking algorithms, to triangle counting and\nbipartite-graph-based algorithms. The results show that on a single GPU,\nGunrock has on average at least an order of magnitude speedup over Boost and\nPowerGraph, comparable performance to the fastest GPU hardwired primitives and\nCPU shared-memory graph libraries such as Ligra and Galois, and better\nperformance than any other GPU high-level graph library.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 22:16:07 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Wang", "Yangzihao", ""], ["Pan", "Yuechao", ""], ["Davidson", "Andrew", ""], ["Wu", "Yuduo", ""], ["Yang", "Carl", ""], ["Wang", "Leyuan", ""], ["Osama", "Muhammad", ""], ["Yuan", "Chenshan", ""], ["Liu", "Weitang", ""], ["Riffel", "Andy T.", ""], ["Owens", "John D.", ""]]}, {"id": "1701.01189", "submitter": "Saman Ashkiani", "authors": "Saman Ashkiani, Andrew Davidson, Ulrich Meyer, John D. Owens", "title": "GPU Multisplit: an extended study of a parallel algorithm", "comments": "44 pages, to appear on ACM Transactions on Parallel Computing (TOPC):\n  \"Special Issue: invited papers from PPoPP 2016\". This is an extended version\n  of PPoPP'16 paper \"GPU Multisplit\"", "journal-ref": "ACM Transactions on Parallel Computing (TOPC), Volume 4, Issue 1,\n  Article No. 2, August 2017", "doi": "10.1145/3108139", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multisplit is a broadly useful parallel primitive that permutes its input\ndata into contiguous buckets or bins, where the function that categorizes an\nelement into a bucket is provided by the programmer. Due to the lack of an\nefficient multisplit on GPUs, programmers often choose to implement multisplit\nwith a sort. One way is to first generate an auxiliary array of bucket IDs and\nthen sort input data based on it. In case smaller indexed buckets possess\nsmaller valued keys, another way for multisplit is to directly sort input data.\nBoth methods are inefficient and require more work than necessary: the former\nrequires more expensive data movements while the latter spends unnecessary\neffort in sorting elements within each bucket. In this work, we provide a\nparallel model and multiple implementations for the multisplit problem. Our\nprincipal focus is multisplit for a small (up to 256) number of buckets. We use\nwarp-synchronous programming models and emphasize warp-wide communications to\navoid branch divergence and reduce memory usage. We also hierarchically reorder\ninput elements to achieve better coalescing of global memory accesses. On a\nGeForce GTX 1080 GPU, we can reach a peak throughput of 18.93 Gkeys/s (or 11.68\nGpairs/s) for a key-only (or key-value) multisplit. Finally, we demonstrate how\nmultisplit can be used as a building block for radix sort. In our\nmultisplit-based sort implementation, we achieve comparable performance to the\nfastest GPU sort routines, sorting 32-bit keys (and key-value pairs) with a\nthroughput of 3.0 G keys/s (and 2.1 Gpair/s).\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 01:00:27 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 00:44:08 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Ashkiani", "Saman", ""], ["Davidson", "Andrew", ""], ["Meyer", "Ulrich", ""], ["Owens", "John D.", ""]]}, {"id": "1701.01539", "submitter": "K. Alex Mills", "authors": "K. Alex Mills, R. Chandrasekaran, Neeraj Mittal", "title": "Algorithms for Optimal Replica Placement Under Correlated Failure in\n  Hierarchical Failure Domains", "comments": "64 pages, 9 figures. Preprint submission to Theoretical Computer\n  Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data centers, data replication is the primary method used to ensure\navailability of customer data. To avoid correlated failure, cloud storage\ninfrastructure providers model hierarchical failure domains using a tree, and\navoid placing a large number of data replicas within the same failure domain\n(i.e. on the same branch of the tree). Typical best practices ensure that\nreplicas are distributed across failure domains, but relatively little is known\nconcerning optimization algorithms for distributing data replicas. Using a\nhierarchical model, we answer how to distribute replicas across failure domains\noptimally. We formulate a novel optimization problem for replica placement in\ndata centers. As part of our problem, we formalize and explain a new criterion\nfor optimizing a replica placement. Our overall goal is to choose placements in\nwhich correlated failures disable as few replicas as possible. We provide two\noptimization algorithms for dependency models represented by trees. We first\npresent an $O(n + \\rho \\log \\rho)$ time dynamic programming algorithm for\nplacing $\\rho$ replicas of a single file on the leaves (representing servers)\nof a tree with $n$ vertices. We next consider the problem of placing replicas\nof $m$ blocks of data, where each block may have different replication factors.\nFor this problem, we give an exact algorithm which runs in polynomial time when\nthe skew, the difference in the number of replicas between the largest and\nsmallest blocks of data, is constant.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 04:05:09 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 18:43:01 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 19:40:37 GMT"}, {"version": "v4", "created": "Thu, 20 Apr 2017 00:30:50 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Mills", "K. Alex", ""], ["Chandrasekaran", "R.", ""], ["Mittal", "Neeraj", ""]]}, {"id": "1701.01575", "submitter": "Bo Xu", "authors": "Bo Xu, Changlong Li, Hang Zhuang, Jiali Wang, Qingfeng Wang, Jinhong\n  Zhou, Xuehai Zhou", "title": "DSA: Scalable Distributed Sequence Alignment System Using SIMD\n  Instructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence alignment algorithms are a basic and critical component of many\nbioinformatics fields. With rapid development of sequencing technology, the\nfast growing reference database volumes and longer length of query sequence\nbecome new challenges for sequence alignment. However, the algorithm is\nprohibitively high in terms of time and space complexity. In this paper, we\npresent DSA, a scalable distributed sequence alignment system that employs\nSpark to process sequences data in a horizontally scalable distributed\nenvironment, and leverages data parallel strategy based on Single Instruction\nMultiple Data (SIMD) instruction to parallelize the algorithm in each core of\nworker node. The experimental results demonstrate that 1) DSA has outstanding\nperformance and achieves up to 201x speedup over SparkSW. 2) DSA has excellent\nscalability and achieves near linear speedup when increasing the number of\nnodes in cluster.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 09:03:32 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Xu", "Bo", ""], ["Li", "Changlong", ""], ["Zhuang", "Hang", ""], ["Wang", "Jiali", ""], ["Wang", "Qingfeng", ""], ["Zhou", "Jinhong", ""], ["Zhou", "Xuehai", ""]]}, {"id": "1701.01587", "submitter": "Vlady Ravelomanana", "authors": "Ny Aina Andriambolamalala (MISA -- Antananarivo), Vlady Ravelomanana\n  (IRIF -- Paris)", "title": "An Optimal Randomized Broadcasting Algorithm in Radio Networks with\n  Collision Detection", "comments": "A major flaw in the paper needs to be corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized distributed algorithm that in radio networks with\ncollision detection broadcasts a single message in $O(D+\\log^2 n)$ time slots,\nwith high probability. In view of the lower-bound $\\Omega(D+\\log^2 n)$, our\nalgorithm is optimal in the considered model answering the decades-old question\nof Alon, Bar-Noy, Linial and Peleg [JCSS 1991].\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 10:28:17 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 08:36:31 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Andriambolamalala", "Ny Aina", "", "MISA -- Antananarivo"], ["Ravelomanana", "Vlady", "", "IRIF -- Paris"]]}, {"id": "1701.01648", "submitter": "Mohamed Khafagy Helmy", "authors": "Ahmed H.Abase, Mohamed H. Khafagy, Fatma A. Omara", "title": "Locality Sim: Cloud Simulator with Data Locality", "comments": "15 Pages, 10 Figures", "journal-ref": "International Journal on Cloud Computing: Services and\n  Architecture (IJCCSA) Vol. 6, No. 6, December 2016", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing (CC) is a model for enabling on-demand access to a shared\npool of configurable computing resources. Testing and evaluating the\nperformance of the cloud environment for allocating, provisioning, scheduling,\nand data allocation policy have great attention to be achieved. Therefore,\nusing cloud simulator would save time and money, and provide a flexible\nenvironment to evaluate new research work. Unfortunately, the current\nsimulators (e.g., CloudSim, NetworkCloudSim, GreenCloud, etc..) deal with the\ndata as for size only without any consideration about the data allocation\npolicy and locality. On the other hand, the NetworkCloudSim simulator is\nconsidered one of the most common used simulators because it includes different\nmodules which support needed functions to a simulated cloud environment, and it\ncould be extended to include new extra modules. According to work in this\npaper, the NetworkCloudSim simulator has been extended and modified to support\ndata locality. The modified simulator is called LocalitySim. The accuracy of\nthe proposed LocalitySim simulator has been proved by building a mathematical\nmodel. Also, the proposed simulator has been used to test the performance of\nthe three-tire data center as a case study with considering the data locality\nfeature.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 14:40:51 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Abase", "Ahmed H.", ""], ["Khafagy", "Mohamed H.", ""], ["Omara", "Fatma A.", ""]]}, {"id": "1701.01676", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Lu\\'is Veiga", "title": "SD-CPS: Taming the Challenges of Cyber-Physical Systems with a\n  Software-Defined Approach", "comments": "Pre-print submitted to SDS'17", "journal-ref": null, "doi": null, "report-no": "INESC-ID Tec. Rep. 10/2016, Dec 2016", "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems (CPS) revolutionize various application domains with\nintegration and interoperability of networking, computing systems, and\nmechanical devices. Due to its scale and variety, CPS faces a number of\nchallenges and opens up a few research questions in terms of management,\nfault-tolerance, and scalability. We propose a software-defined approach\ninspired by Software-Defined Networking (SDN), to address the challenges for a\nwider CPS adoption. We thus design a middleware architecture for the correct\nand resilient operation of CPS, to manage and coordinate the interacting\ndevices centrally in the cyberspace whilst not sacrificing the functionality\nand performance benefits inherent to a distributed execution.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 16:00:23 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1701.01772", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Rong Zhou and Nesreen K. Ahmed", "title": "Estimation of Graphlet Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC math.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphlets are induced subgraphs of a large network and are important for\nunderstanding and modeling complex networks. Despite their practical\nimportance, graphlets have been severely limited to applications and domains\nwith relatively small graphs. Most previous work has focused on exact\nalgorithms, however, it is often too expensive to compute graphlets exactly in\nmassive networks with billions of edges, and finding an approximate count is\nusually sufficient for many applications. In this work, we propose an unbiased\ngraphlet estimation framework that is (a) fast with significant speedups\ncompared to the state-of-the-art, (b) parallel with nearly linear-speedups, (c)\naccurate with <1% relative error, (d) scalable and space-efficient for massive\nnetworks with billions of edges, and (e) flexible for a variety of real-world\nsettings, as well as estimating macro and micro-level graphlet statistics\n(e.g., counts) of both connected and disconnected graphlets. In addition, an\nadaptive approach is introduced that finds the smallest sample size required to\nobtain estimates within a given user-defined error bound. On 300 networks from\n20 domains, we obtain <1% relative error for all graphlets. This is\nsignificantly more accurate than existing methods while using less data.\nMoreover, it takes a few seconds on billion edge graphs (as opposed to\ndays/weeks). These are by far the largest graphlet computations to date.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 22:37:59 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 16:18:56 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Zhou", "Rong", ""], ["Ahmed", "Nesreen K.", ""]]}, {"id": "1701.01963", "submitter": "Cong Luong Nguyen", "authors": "Nguyen Cong Luong, Ping Wang, Dusit Niyato, Wen Yonggang, and Zhu Han", "title": "Resource Management in Cloud Networking Using Economic Analysis and\n  Pricing Models: A Survey", "comments": "IEEE Communications Surveys and Tutorials, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a comprehensive literature review on applications of\neconomic and pricing models for resource management in cloud networking. To\nachieve sustainable profit advantage, cost reduction, and flexibility in\nprovisioning of cloud resources, resource management in cloud networking\nrequires adaptive and robust designs to address many issues, e.g., resource\nallocation, bandwidth reservation, request allocation, and workload allocation.\nEconomic and pricing models have received a lot of attention as they can lead\nto desirable performance in terms of social welfare, fairness, truthfulness,\nprofit, user satisfaction, and resource utilization. This paper reviews\napplications of the economic and pricing models to develop adaptive algorithms\nand protocols for resource management in cloud networking. Besides, we survey a\nvariety of incentive mechanisms using the pricing strategies in sharing\nresources in edge computing. In addition, we consider using pricing models in\ncloud-based Software Defined Wireless Networking (cloud-based SDWN). Finally,\nwe highlight important challenges, open issues and future research directions\nof applying economic and pricing models to cloud networking\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 14:32:54 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Luong", "Nguyen Cong", ""], ["Wang", "Ping", ""], ["Niyato", "Dusit", ""], ["Yonggang", "Wen", ""], ["Han", "Zhu", ""]]}, {"id": "1701.02324", "submitter": "Chenhan Yu", "authors": "Chenhan D. Yu, William B. March, George Biros", "title": "An $N \\log N$ Parallel Fast Direct Solver for Kernel Matrices", "comments": "proceeding 31st IEEE International Parallel & Distributed Processing\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel matrices appear in machine learning and non-parametric statistics.\nGiven $N$ points in $d$ dimensions and a kernel function that requires\n$\\mathcal{O}(d)$ work to evaluate, we present an $\\mathcal{O}(dN\\log N)$-work\nalgorithm for the approximate factorization of a regularized kernel matrix, a\ncommon computational bottleneck in the training phase of a learning task. With\nthis factorization, solving a linear system with a kernel matrix can be done\nwith $\\mathcal{O}(N\\log N)$ work. Our algorithm only requires kernel\nevaluations and does not require that the kernel matrix admits an efficient\nglobal low rank approximation. Instead our factorization only assumes low-rank\nproperties for the off-diagonal blocks under an appropriate row and column\nordering. We also present a hybrid method that, when the factorization is\nprohibitively expensive, combines a partial factorization with iterative\nmethods. As a highlight, we are able to approximately factorize a dense\n$11M\\times11M$ kernel matrix in 2 minutes on 3,072 x86 \"Haswell\" cores and a\n$4.5M\\times4.5M$ matrix in 1 minute using 4,352 \"Knights Landing\" cores.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 19:11:10 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Yu", "Chenhan D.", ""], ["March", "William B.", ""], ["Biros", "George", ""]]}, {"id": "1701.02408", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Philip S. Yu, Guolei Yi, Wenlong Ma, Mengying Guo, Jianxun\n  Liu", "title": "To Vote Before Decide: A Logless One-Phase Commit Protocol for\n  Highly-Available Datastores", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highly-available datastores are widely deployed for online applications.\nHowever, many online applications are not contented with the simple data access\ninterface currently provided by highly-available datastores. Distributed\ntransaction support is demanded by applications such as large-scale online\npayment used by Alipay or Paypal. Current solutions to distributed transaction\ncan spend more than half of the whole transaction processing time in\ndistributed commit. An efficient atomic commit protocol is highly desirable.\nThis paper presents the HACommit protocol, a logless one-phase commit protocol\nfor highly-available systems. HACommit has transaction participants vote for a\ncommit before the client decides to commit or abort the transaction; in\ncomparison, the state-of-the-art practice for distributed commit is to have the\nclient decide before participants vote. The change enables the removal of both\nthe participant logging and the coordinator logging steps in the distributed\ncommit process; it also makes possible that, after the client initiates the\ntransaction commit, the transaction data is visible to other transactions\nwithin one communication roundtrip time (i.e., one phase). In the evaluation\nwith extensive experiments, HACommit outperforms recent atomic commit solutions\nfor highly-available datastores under different workloads. In the best case,\nHACommit can commit in one fifth of the time 2PC does.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 02:05:49 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 03:16:37 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Zhu", "Yuqing", ""], ["Yu", "Philip S.", ""], ["Yi", "Guolei", ""], ["Ma", "Wenlong", ""], ["Guo", "Mengying", ""], ["Liu", "Jianxun", ""]]}, {"id": "1701.02555", "submitter": "Amos Korman", "authors": "Ofer Feinerman, Amos Korman (IRIF, GANG)", "title": "The ANTS problem", "comments": "arXiv admin note: text overlap with arXiv:1205.4545", "journal-ref": "Distributed Computing, Springer Verlag, 2016", "doi": "10.1007/s00446-016-0285-8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Ants Nearby Treasure Search (ANTS) problem, which models\nnatural cooperative foraging behavior such as that performed by ants around\ntheir nest. In this problem, k probabilistic agents, initially placed at a\ncentral location, collectively search for a treasure on the two-dimensional\ngrid. The treasure is placed at a target location by an adversary and the\nagents' goal is to find it as fast as possible as a function of both k and D,\nwhere D is the (unknown) distance between the central location and the target.\nWe concentrate on the case in which agents cannot communicate while searching.\nIt is straightforward to see that the time until at least one agent finds the\ntarget is at least $\\Omega$(D + D 2 /k), even for very sophisticated agents,\nwith unrestricted memory. Our algorithmic analysis aims at establishing\nconnections between the time complexity and the initial knowledge held by\nagents (e.g., regarding their total number k), as they commence the search. We\nprovide a range of both upper and lower bounds for the initial knowledge\nrequired for obtaining fast running time. For example, we prove that log log k\n+ $\\Theta$(1) bits of initial information are both necessary and sufficient to\nobtain asymptotically optimal running time, i.e., O(D +D 2 /k). We also we\nprove that for every 0 \\textless{} \\textless{} 1, running in time O(log 1-- k\n$\\times$(D +D 2 /k)) requires that agents have the capacity for storing\n$\\Omega$(log k) different states as they leave the nest to start the search. To\nthe best of our knowledge, the lower bounds presented in this paper provide the\nfirst non-trivial lower bounds on the memory complexity of probabilistic agents\nin the context of search problems. We view this paper as a \"proof of concept\"\nfor a new type of interdisciplinary methodology. To fully demonstrate this\nmethodology, the theoretical tradeoff presented here (or a similar one) should\nbe combined with measurements of the time performance of searching ants.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 12:38:44 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Feinerman", "Ofer", "", "IRIF, GANG"], ["Korman", "Amos", "", "IRIF, GANG"]]}, {"id": "1701.02601", "submitter": "Harishchandra Dubey", "authors": "Rabindra K. Barik, Harishchandra Dubey, Arun B. Samaddar, Rajan D.\n  Gupta, Prakash K. Ray", "title": "FogGIS: Fog Computing for Geospatial Big Data Analytics", "comments": "6 pages, 4 figures, 1 table, 3rd IEEE Uttar Pradesh Section\n  International Conference on Electrical, Computer and Electronics (09-11\n  December, 2016) Indian Institute of Technology (Banaras Hindu University)\n  Varanasi, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Geographic Information Systems (GIS) has emerged as a tool for\nanalysis, processing and transmission of geospatial data. The Fog computing is\na paradigm where Fog devices help to increase throughput and reduce latency at\nthe edge of the client. This paper developed a Fog-based framework named Fog\nGIS for mining analytics from geospatial data. We built a prototype using Intel\nEdison, an embedded microprocessor. We validated the FogGIS by doing\npreliminary analysis. including compression, and overlay analysis. Results\nshowed that Fog computing hold a great promise for analysis of geospatial data.\nWe used several open source compression techniques for reducing the\ntransmission to the cloud.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 12:59:54 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Barik", "Rabindra K.", ""], ["Dubey", "Harishchandra", ""], ["Samaddar", "Arun B.", ""], ["Gupta", "Rajan D.", ""], ["Ray", "Prakash K.", ""]]}, {"id": "1701.02628", "submitter": "Kamer Kaya Kamer Kaya", "authors": "Mustafa Kemal Ta\\c{s}, Kamer Kaya, Erik Saule", "title": "Greed is Good: Optimistic Algorithms for Bipartite-Graph Partial\n  Coloring on Multicore Architectures", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parallel computing, a valid graph coloring yields a lock-free processing\nof the colored tasks, data points, etc., without expensive synchronization\nmechanisms. However, coloring is not free and the overhead can be significant.\nIn particular, for the bipartite-graph partial coloring (BGPC) and distance-2\ngraph coloring (D2GC) problems, which have various use-cases within the\nscientific computing and numerical optimization domains, the coloring overhead\ncan be in the order of minutes with a single thread for many real-life graphs.\n  In this work, we propose parallel algorithms for bipartite-graph partial\ncoloring on shared-memory architectures. Compared to the existing shared-memory\nBGPC algorithms, the proposed ones employ greedier and more optimistic\ntechniques that yield a better parallel coloring performance. In particular, on\n16 cores, the proposed algorithms perform more than 4x faster than their\ncounterparts in the ColPack library which is, to the best of our knowledge, the\nonly publicly-available coloring library for multicore architectures. In\naddition to BGPC, the proposed techniques are employed to devise parallel\ndistance-2 graph coloring algorithms and similar performance improvements have\nbeen observed. Finally, we propose two costless balancing heuristics for BGPC\nthat can reduce the skewness and imbalance on the cardinality of color sets\n(almost) for free. The heuristics can also be used for the D2GC problem and in\ngeneral, they will probably yield a better color-based parallelization\nperformance especially on many-core architectures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:13:49 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Ta\u015f", "Mustafa Kemal", ""], ["Kaya", "Kamer", ""], ["Saule", "Erik", ""]]}, {"id": "1701.02641", "submitter": "Vladimir Savic Dr", "authors": "Vladimir Savic, Elad M. Schiller, Marina Papatriantafilou", "title": "Distributed Algorithm for Collision Avoidance at Road Intersections in\n  the Presence of Communication Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle-to-vehicle (V2V) communication is a crucial component of the future\nautonomous driving systems since it enables improved awareness of the\nsurrounding environment, even without extensive processing of sensory\ninformation. However, V2V communication is prone to failures and delays, so a\ndistributed fault-tolerant approach is required for safe and efficient\ntransportation. In this paper, we focus on the intersection crossing (IC)\nproblem with autonomous vehicles that cooperate via V2V communications, and\npropose a novel distributed IC algorithm that can handle an unknown number of\ncommunication failures. Our analysis shows that both safety and liveness\nrequirements are satisfied in all realistic situations. We also found, based on\na real data set, that the crossing delay is only slightly increased even in the\npresence of highly correlated failures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:41:28 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Savic", "Vladimir", ""], ["Schiller", "Elad M.", ""], ["Papatriantafilou", "Marina", ""]]}, {"id": "1701.02991", "submitter": "Zaid Hussain", "authors": "Zaid Hussain, Bader AlBdaiwi, Anton Cerny", "title": "Node-Independent Spanning Trees in Gaussian Networks", "comments": null, "journal-ref": "Journal of Parallel and Distributed Computing, Volume 109,\n  November 2017, Pages 324-332", "doi": "10.1016/j.jpdc.2017.06.018", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Message broadcasting in networks could be carried over spanning trees. A set\nof spanning trees in the same network is node independent if two conditions are\nsatisfied. First, all trees are rooted at node $r$. Second, for every node $u$\nin the network, all trees' paths from $r$ to $u$ are node-disjoint, excluding\nthe end nodes $r$ and $u$. Independent spanning trees have applications in\nfault-tolerant communications and secure message distributions. Gaussian\nnetworks and two-dimensional toroidal networks share similar topological\ncharacteristics. They are regular of degree four, symmetric, and\nnode-transitive. Gaussian networks, however, have relatively lesser network\ndiameter that could result in a better performance. This promotes Gaussian\nnetworks to be a potential alternative for two-dimensional toroidal networks.\nIn this paper, we present constructions for node independent spanning trees in\ndense Gaussian networks. Based on these constructions, we design routing\nalgorithms that can be used in fault-tolerant routing and secure message\ndistribution. We also design fault-tolerant algorithms to construct these trees\nin parallel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 14:43:05 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hussain", "Zaid", ""], ["AlBdaiwi", "Bader", ""], ["Cerny", "Anton", ""]]}, {"id": "1701.03004", "submitter": "Massimo Cafaro", "authors": "Massimo Cafaro, Marco Pulimeno and Italo Epicoco", "title": "Parallel mining of time-faded heavy hitters", "comments": "arXiv admin note: text overlap with arXiv:1601.03892", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PFDCMSS, a novel message-passing based parallel algorithm for\nmining time-faded heavy hitters. The algorithm is a parallel version of the\nrecently published FDCMSS sequential algorithm. We formally prove its\ncorrectness by showing that the underlying data structure, a sketch augmented\nwith a Space Saving stream summary holding exactly two counters, is mergeable.\nWhilst mergeability of traditional sketches derives immediately from theory, we\nshow that merging our augmented sketch is non trivial. Nonetheless, the\nresulting parallel algorithm is fast and simple to implement. To the best of\nour knowledge, PFDCMSS is the first parallel algorithm solving the problem of\nmining time-faded heavy hitters on message-passing parallel architectures.\nExtensive experimental results confirm that PFDCMSS retains the extreme\naccuracy and error bound provided by FDCMSS whilst providing excellent parallel\nscalability.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 15:07:38 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Cafaro", "Massimo", ""], ["Pulimeno", "Marco", ""], ["Epicoco", "Italo", ""]]}, {"id": "1701.03038", "submitter": "Arturo Argueta", "authors": "Arturo Argueta, David Chiang", "title": "Decoding with Finite-State Transducers on GPUs", "comments": "accepted at EACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted finite automata and transducers (including hidden Markov models and\nconditional random fields) are widely used in natural language processing (NLP)\nto perform tasks such as morphological analysis, part-of-speech tagging,\nchunking, named entity recognition, speech recognition, and others.\nParallelizing finite state algorithms on graphics processing units (GPUs) would\nbenefit many areas of NLP. Although researchers have implemented GPU versions\nof basic graph algorithms, limited previous work, to our knowledge, has been\ndone on GPU algorithms for weighted finite automata. We introduce a GPU\nimplementation of the Viterbi and forward-backward algorithm, achieving\ndecoding speedups of up to 5.2x over our serial implementation running on\ndifferent computer architectures and 6093x over OpenFST.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:07:27 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 14:48:24 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Argueta", "Arturo", ""], ["Chiang", "David", ""]]}, {"id": "1701.03043", "submitter": "Xiaohan Wei", "authors": "Manxi Wang, Yongcheng Li, Xiaohan Wei, Qing Ling", "title": "Robust Group LASSO Over Decentralized Networks", "comments": "IEEE GlobalSIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the recovery of group sparse signals over a multi-agent\nnetwork, where the measurements are subject to sparse errors. We first\ninvestigate the robust group LASSO model and its centralized algorithm based on\nthe alternating direction method of multipliers (ADMM), which requires a\ncentral fusion center to compute a global row-support detector. To implement it\nin a decentralized network environment, we then adopt dynamic average consensus\nstrategies that enable dynamic tracking of the global row-support detector.\nNumerical experiments demonstrate the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:22:28 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Wang", "Manxi", ""], ["Li", "Yongcheng", ""], ["Wei", "Xiaohan", ""], ["Ling", "Qing", ""]]}, {"id": "1701.03053", "submitter": "David Castells-Rufas", "authors": "David Castells-Rufas, C\\'edric Bastoul", "title": "Proceedings of the Workshop on High Performance Energy Efficient\n  Embedded Systems (HIP3ES) 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proceedings of the Workshop on High Performance Energy Efficient Embedded\nSystems (HIP3ES) 2017. Stockholm, Sweden, January 25th. Collocated with HIPEAC\n2017 Conference.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 16:44:32 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Castells-Rufas", "David", ""], ["Bastoul", "C\u00e9dric", ""]]}, {"id": "1701.03295", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Automatic Cloud Resource Scaling Algorithm based on Long Short-Term\n  Memory Recurrent Neural Network", "comments": null, "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (IJACSA), 7(12), 2016", "doi": "10.14569/IJACSA.2016.071236", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scalability is an important characteristic of cloud computing. With\nscalability, cost is minimized by provisioning and releasing resources\naccording to demand. Most of current Infrastructure as a Service (IaaS)\nproviders deliver threshold-based auto-scaling techniques. However, setting up\nthresholds with right values that minimize cost and achieve Service Level\nAgreement is not an easy task, especially with variant and sudden workload\nchanges. This paper has proposed dynamic threshold based auto-scaling\nalgorithms that predict required resources using Long Short-Term Memory\nRecurrent Neural Network and auto-scale virtual resources based on predicted\nvalues. The proposed algorithms have been evaluated and compared with some of\nexisting algorithms. Experimental results show that the proposed algorithms\noutperform other algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 10:41:37 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1701.03296", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Using Multiple Seasonal Holt-Winters Exponential Smoothing to Predict\n  Cloud Resource Provisioning", "comments": null, "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications,Vol. 7, No. 11, 2016", "doi": "10.14569/IJACSA.2016.071113", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Elasticity is one of the key features of cloud computing that attracts many\nSaaS providers to minimize their services' cost. Cost is minimized by\nautomatically provision and release computational resources depend on actual\ncomputational needs. However, delay of starting up new virtual resources can\ncause Service Level Agreement violation. Consequently, predicting cloud\nresources provisioning gains a lot of attention to scale computational\nresources in advance. However, most of current approaches do not consider\nmulti-seasonality in cloud workloads. This paper proposes cloud resource\nprovisioning prediction algorithm based on Holt-Winters exponential smoothing\nmethod. The proposed algorithm extends Holt-Winters exponential smoothing\nmethod to model cloud workload with multi-seasonal cycles. Prediction accuracy\nof the proposed algorithm has been improved by employing Artificial Bee Colony\nalgorithm to optimize its parameters. Performance of the proposed algorithm has\nbeen evaluated and compared with double and triple exponential smoothing\nmethods. Our results have shown that the proposed algorithm outperforms other\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 10:48:37 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1701.03318", "submitter": "EPTCS", "authors": "Edelmira Pasarella (Universitat Politecnica de Catalunya),\n  Maria-Esther Vidal (Fraunhofer IAIS), Cristina Zoltan (Universitat\n  Politecnica de Catalunya)", "title": "Comparing MapReduce and Pipeline Implementations for Counting Triangles", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 20-33", "doi": "10.4204/EPTCS.237.2", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common method to define a parallel solution for a computational problem\nconsists in finding a way to use the Divide and Conquer paradigm in order to\nhave processors acting on its own data and scheduled in a parallel fashion.\nMapReduce is a programming model that follows this paradigm, and allows for the\ndefinition of efficient solutions by both decomposing a problem into steps on\nsubsets of the input data and combining the results of each step to produce\nfinal results. Albeit used for the implementation of a wide variety of\ncomputational problems, MapReduce performance can be negatively affected\nwhenever the replication factor grows or the size of the input is larger than\nthe resources available at each processor. In this paper we show an alternative\napproach to implement the Divide and Conquer paradigm, named dynamic pipeline.\nThe main features of dynamic pipelines are illustrated on a parallel\nimplementation of the well-known problem of counting triangles in a graph. This\nproblem is especially interesting either when the input graph does not fit in\nmemory or is dynamically generated. To evaluate the properties of pipeline, a\ndynamic pipeline of processes and an ad-hoc version of MapReduce are\nimplemented in the language Go, exploiting its ability to deal with channels\nand spawned processes. An empirical evaluation is conducted on graphs of\ndifferent topologies, sizes, and densities. Observed results suggest that\ndynamic pipelines allows for an efficient implementation of the problem of\ncounting triangles in a graph, particularly, in dense and large graphs,\ndrastically reducing the execution time with respect to the MapReduce\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:04:15 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Pasarella", "Edelmira", "", "Universitat Politecnica de Catalunya"], ["Vidal", "Maria-Esther", "", "Fraunhofer IAIS"], ["Zoltan", "Cristina", "", "Universitat\n  Politecnica de Catalunya"]]}, {"id": "1701.03319", "submitter": "EPTCS", "authors": "Salvador Tamarit (Universidad Polit\\`ecnica de Madrid), Julio Mari\\~no\n  (Universidad Polit\\`ecnica de Madrid), Guillermo Vigueras (IMDEA Software\n  Institute), Manuel Carro (IMDEA Software Institute)", "title": "Towards a Semantics-Aware Code Transformation Toolchain for\n  Heterogeneous Systems", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069. arXiv admin note:\n  substantial text overlap with arXiv:1603.03011", "journal-ref": "EPTCS 237, 2017, pp. 34-51", "doi": "10.4204/EPTCS.237.3", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining good performance when programming heterogeneous computing platforms\nposes significant challenges. We present a program transformation environment,\nimplemented in Haskell, where architecture-agnostic scientific C code with\nsemantic annotations is transformed into functionally equivalent code better\nsuited for a given platform. The transformation steps are represented as rules\nthat can be fired when certain syntactic and semantic conditions are fulfilled.\nThese rules are not hard-wired into the rewriting engine: they are written in a\nC-like language and are automatically processed and incorporated into the\nrewriting engine. That makes it possible for end-users to add their own rules\nor to provide sets of rules that are adapted to certain specific domains or\npurposes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:04:30 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Tamarit", "Salvador", "", "Universidad Polit\u00e8cnica de Madrid"], ["Mari\u00f1o", "Julio", "", "Universidad Polit\u00e8cnica de Madrid"], ["Vigueras", "Guillermo", "", "IMDEA Software\n  Institute"], ["Carro", "Manuel", "", "IMDEA Software Institute"]]}, {"id": "1701.03458", "submitter": "Tina Woolf", "authors": "Deanna Needell, Tina Woolf", "title": "An Asynchronous Parallel Approach to Sparse Recovery", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous parallel computing and sparse recovery are two areas that have\nreceived recent interest. Asynchronous algorithms are often studied to solve\noptimization problems where the cost function takes the form $\\sum_{i=1}^M\nf_i(x)$, with a common assumption that each $f_i$ is sparse; that is, each\n$f_i$ acts only on a small number of components of $x\\in\\mathbb{R}^n$. Sparse\nrecovery problems, such as compressed sensing, can be formulated as\noptimization problems, however, the cost functions $f_i$ are dense with respect\nto the components of $x$, and instead the signal $x$ is assumed to be sparse,\nmeaning that it has only $s$ non-zeros where $s\\ll n$. Here we address how one\nmay use an asynchronous parallel architecture when the cost functions $f_i$ are\nnot sparse in $x$, but rather the signal $x$ is sparse. We propose an\nasynchronous parallel approach to sparse recovery via a stochastic greedy\nalgorithm, where multiple processors asynchronously update a vector in shared\nmemory containing information on the estimated signal support. We include\nnumerical simulations that illustrate the potential benefits of our proposed\nasynchronous method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 05:14:40 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Needell", "Deanna", ""], ["Woolf", "Tina", ""]]}, {"id": "1701.03477", "submitter": "Marc Olm", "authors": "Santiago Badia and Marc Olm", "title": "Space-time balancing domain decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose two-level space-time domain decomposition\npreconditioners for parabolic problems discretized using finite elements. They\nare motivated as an extension to space-time of balancing domain decomposition\nby constraints preconditioners. The key ingredients to be defined are the\nsub-assembled space and operator, the coarse degrees of freedom (DOFs) in which\nwe want to enforce continuity among subdomains at the preconditioner level, and\nthe transfer operator from the sub-assembled to the original finite element\nspace. With regard to the sub-assembled operator, a perturbation of the time\nderivative is needed to end up with a well-posed preconditioner. The set of\ncoarse DOFs includes the time average (at the space-time subdomain) of\nclassical space constraints plus new constraints between consecutive subdomains\nin time. Numerical experiments show that the proposed schemes are weakly\nscalable in time, i.e., we can efficiently exploit increasing computational\nresources to solve more time steps in the same {total elapsed} time. Further,\nthe scheme is also weakly space-time scalable, since it leads to asymptotically\nconstant iterations when solving larger problems both in space and time.\nExcellent {wall clock} time weak scalability is achieved for space-time\nparallel solvers on some thousands of cores.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 19:32:09 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Badia", "Santiago", ""], ["Olm", "Marc", ""]]}, {"id": "1701.03507", "submitter": "Alexandre P Francisco", "authors": "Bruno Dantas, Calmenelias Fleitas, Alexandre P. Francisco, Jos\\'e\n  Sim\\~ao, C\\'atia Vaz", "title": "Beyond NGS data sharing and towards open science", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biosciences have been revolutionized by next generation sequencing (NGS)\ntechnologies in last years, leading to new perspectives in medical, industrial\nand environmental applications. And although our motivation comes from\nbiosciences, the following is true for many areas of science: published results\nare usually hard to reproduce either because data is not available or tools are\nnot readily available, which delays the adoption of new methodologies and\nhinders innovation. Our focus is on tool readiness and pipelines availability.\nEven though most tools are freely available, pipelines for data analysis are in\ngeneral barely described and their configuration is far from trivial, with many\nparameters to be tuned.\n  In this paper we discuss how to effectively build and use pipelines, relying\non state of the art computing technologies to execute them without users need\nto configure, install and manage tools, servers and complex workflow management\nsystems. We perform an in depth comparative analysis of state of the art\nframeworks and systems. The NGSPipes framework is proposed showing that we can\nhave public pipelines ready to process and analyse experimental data, produced\nfor instance by high-throughput technologies, but without relying on\ncentralized servers or Web services.\n  The NGSPipes framework and underlying architecture provides a major step\ntowards open science and true collaboration in what concerns tools and\npipelines among computational biology researchers and practitioners. We show\nthat it is possible to execute data analysis pipelines in a decentralized and\nplatform independent way. Approaches like the one proposed are crucial for\narchiving and reusing data analysis pipelines at medium/long-term. NGSPipes\nframework is freely available at http://ngspipes.github.io/.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:56:58 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Dantas", "Bruno", ""], ["Fleitas", "Calmenelias", ""], ["Francisco", "Alexandre P.", ""], ["Sim\u00e3o", "Jos\u00e9", ""], ["Vaz", "C\u00e1tia", ""]]}, {"id": "1701.03519", "submitter": "Guy Avni", "authors": "Guy Avni, Shubham Goel, Thomas A. Henzinger, Guillermo Rodriguez-Navas", "title": "Computing Scores of Forwarding Schemes in Switched Networks with\n  Probabilistic Faults", "comments": "Accepted to TACAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-triggered switched networks are a deterministic communication\ninfrastructure used by real-time distributed embedded systems. Due to the\ncriticality of the applications running over them, developers need to ensure\nthat end-to-end communication is dependable and predictable. Traditional\napproaches assume static networks that are not flexible to changes caused by\nreconfigurations or, more importantly, faults, which are dealt with in the\napplication using redundancy. We adopt the concept of handling faults in the\nswitches from non-real-time networks while maintaining the required\npredictability.\n  We study a class of forwarding schemes that can handle various types of\nfailures. We consider probabilistic failures. For a given network with a\nforwarding scheme and a constant $\\ell$, we compute the {\\em score} of the\nscheme, namely the probability (induced by faults) that at least $\\ell$\nmessages arrive on time. We reduce the scoring problem to a reachability\nproblem on a Markov chain with a \"product-like\" structure. Its special\nstructure allows us to reason about it symbolically, and reduce the scoring\nproblem to #SAT. Our solution is generic and can be adapted to different\nnetworks and other contexts. Also, we show the computational complexity of the\nscoring problem is #P-complete, and we study methods to estimate the score. We\nevaluate the effectiveness of our techniques with an implementation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 22:06:25 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Avni", "Guy", ""], ["Goel", "Shubham", ""], ["Henzinger", "Thomas A.", ""], ["Rodriguez-Navas", "Guillermo", ""]]}, {"id": "1701.03534", "submitter": "Andrew Ling", "authors": "Utku Aydonat, Shane O'Connell, Davor Capalija, Andrew C. Ling, Gordon\n  R. Chiu", "title": "An OpenCL(TM) Deep Learning Accelerator on Arria 10", "comments": "To be published at FPGA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural nets (CNNs) have become a practical means to perform\nvision tasks, particularly in the area of image classification. FPGAs are well\nknown to be able to perform convolutions efficiently, however, most recent\nefforts to run CNNs on FPGAs have shown limited advantages over other devices\nsuch as GPUs. Previous approaches on FPGAs have often been memory bound due to\nthe limited external memory bandwidth on the FPGA device. We show a novel\narchitecture written in OpenCL(TM), which we refer to as a Deep Learning\nAccelerator (DLA), that maximizes data reuse and minimizes external memory\nbandwidth. Furthermore, we show how we can use the Winograd transform to\nsignificantly boost the performance of the FPGA. As a result, when running our\nDLA on Intel's Arria 10 device we can achieve a performance of 1020 img/s, or\n23 img/s/W when running the AlexNet CNN benchmark. This comes to 1382 GFLOPs\nand is 10x faster with 8.4x more GFLOPS and 5.8x better efficiency than the\nstate-of-the-art on FPGAs. Additionally, 23 img/s/W is competitive against the\nbest publicly known implementation of AlexNet on nVidia's TitanX GPU.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 00:31:15 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Aydonat", "Utku", ""], ["O'Connell", "Shane", ""], ["Capalija", "Davor", ""], ["Ling", "Andrew C.", ""], ["Chiu", "Gordon R.", ""]]}, {"id": "1701.03616", "submitter": "Joshua Daymude", "authors": "Joshua J. Daymude, Robert Gmyr, Andrea W. Richa, Christian Scheideler,\n  Thim Strothmann", "title": "Improved Leader Election for Self-Organizing Programmable Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider programmable matter that consists of computationally limited\ndevices (called particles) that are able to self-organize in order to achieve\nsome collective goal without the need for central control or external\nintervention. We use the geometric amoebot model to describe such\nself-organizing particle systems, which defines how particles can actively move\nand communicate with one another. In this paper, we present an efficient\nlocal-control algorithm which solves the leader election problem in O(n)\nasynchronous rounds with high probability, where n is the number of particles\nin the system. Our algorithm relies only on local information --- particles do\nnot have unique identifiers, any knowledge of n, or any sort of global\ncoordinate system --- and requires only constant memory per particle.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 10:27:57 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 13:35:23 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 07:45:53 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Daymude", "Joshua J.", ""], ["Gmyr", "Robert", ""], ["Richa", "Andrea W.", ""], ["Scheideler", "Christian", ""], ["Strothmann", "Thim", ""]]}, {"id": "1701.03709", "submitter": "Christof Schlaak", "authors": "Christof Schlaak, Maher Fakih, Ralf Stemmer", "title": "Power and Execution Time Measurement Methodology for SDF Applications on\n  FPGA-based MPSoCs", "comments": "Presented at HIP3ES, 2017 7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2017/1", "categories": "cs.DC cs.AR cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Timing and power consumption play an important role in the design of embedded\nsystems. Furthermore, both properties are directly related to the safety\nrequirements of many embedded systems. With regard to availability\nrequirements, power considerations are of uttermost importance for battery\noperated systems. Validation of timing and power requires observability of\nthese properties. In many cases this is difficult, because the observability is\neither not possible or requires big extra effort in the system validation\nprocess. In this paper, we present a measurement-based approach for the joint\ntiming and power analysis of Synchronous Dataflow (SDF) applications running on\na shared memory multiprocessor systems-on-chip (MPSoC) architecture. As a\nproof-of-concept, we implement an MPSoC system with configurable power and\ntiming measurement interfaces inside a Field Programmable Gate Array (FPGA).\nOur experiments demonstrate the viability of our approach being able of\naccurately analyzing different mappings of image processing applications (Sobel\nfilter and JPEG encoder) on an FPGA-based MPSoC implementation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 16:15:53 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Schlaak", "Christof", ""], ["Fakih", "Maher", ""], ["Stemmer", "Ralf", ""]]}, {"id": "1701.03734", "submitter": "Seyed Navid Mirnouri Langeroudi", "authors": "Navid Mirnouri", "title": "Applying Data Compression Techniques on Systolic Neural Network\n  Accelerator", "comments": "6 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  New directions in computing and algorithms has lead to some new applications\nthat have tolerance to imprecision. Although, These applications are creating\nlarge volumes of data which exceeds the capability of today's computing\nsystems. Therefore, researchers are trying to find new techniques to alleviate\nthis crisis. Approximate Computing is one promising technique that uses a trade\noff between precision and efficiency of computing. Acceleration is another\nsolution that uses specialized logics in order to do computations in a way that\nis more power efficient. Another technique is Data compression which is used in\nmemory systems in order to save capacity and bandwidth.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 13:41:23 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Mirnouri", "Navid", ""]]}, {"id": "1701.03893", "submitter": "Layla Majzoobi", "authors": "Layla Majzoobi, Farshad Lahouti", "title": "Analysis of Distributed ADMM Algorithm for Consensus Optimization in\n  Presence of Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ADMM is a popular algorithm for solving convex optimization problems.\nApplying this algorithm to distributed consensus optimization problem results\nin a fully distributed iterative solution which relies on processing at the\nnodes and communication between neighbors. Local computations usually suffer\nfrom different types of errors, due to e.g., observation or quantization noise,\nwhich can degrade the performance of the algorithm. In this work, we focus on\nanalyzing the convergence behavior of distributed ADMM for consensus\noptimization in presence of additive node error. We specifically show that (a\nnoisy) ADMM converges linearly under certain conditions and also examine the\nassociated convergence point. Numerical results are provided which demonstrate\nthe effectiveness of the presented analysis.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 08:46:01 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Majzoobi", "Layla", ""], ["Lahouti", "Farshad", ""]]}, {"id": "1701.03922", "submitter": "Huaqing Zhang", "authors": "Huaqing Zhang, Yong Xiao, Shengrong Bu, Dusit Niyato, Richard Yu, and\n  Zhu Han", "title": "Computing Resource Allocation in Three-Tier IoT Fog Networks: a Joint\n  Optimization Approach Combining Stackelberg Game and Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fog computing is a promising architecture to provide economic and low latency\ndata services for future Internet of things (IoT)-based network systems. It\nrelies on a set of low-power fog nodes that are close to the end users to\noffload the services originally targeting at cloud data centers. In this paper,\nwe consider a specific fog computing network consisting of a set of data\nservice operators (DSOs) each of which controls a set of fog nodes to provide\nthe required data service to a set of data service subscribers (DSSs). How to\nallocate the limited computing resources of fog nodes (FNs) to all the DSSs to\nachieve an optimal and stable performance is an important problem. In this\npaper, we propose a joint optimization framework for all FNs, DSOs and DSSs to\nachieve the optimal resource allocation schemes in a distributed fashion. In\nthe framework, we first formulate a Stackelberg game to analyze the pricing\nproblem for the DSOs as well as the resource allocation problem for the DSSs.\nUnder the scenarios that the DSOs can know the expected amount of resource\npurchased by the DSSs, a many-to-many matching game is applied to investigate\nthe pairing problem between DSOs and FNs. Finally, within the same DSO, we\napply another layer of many-to-many matching between each of the paired FNs and\nserving DSSs to solve the FN-DSS pairing problem. Simulation results show that\nour proposed framework can significantly improve the performance of the\nIoT-based network systems.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 13:47:52 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Zhang", "Huaqing", ""], ["Xiao", "Yong", ""], ["Bu", "Shengrong", ""], ["Niyato", "Dusit", ""], ["Yu", "Richard", ""], ["Han", "Zhu", ""]]}, {"id": "1701.04217", "submitter": "Maher Fakih", "authors": "Maher Fakih and Sebastian Warsitz", "title": "Automatic SDF-based Code Generation from Simulink Models for Embedded\n  Software Development", "comments": "10 pages, 9 figures, Presented at HIP3ES, 2017", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2017/2", "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matlab/Simulink is a wide-spread tool for model-based design of embedded\nsystems. Supporting hierarchy, domain specific building blocks, functional\nsimulation and automatic code-generation, makes it well-suited for the design\nof control and signal processing systems. In this work, we propose an automated\ntranslation methodology for a subset of Simulink models to Synchronous dataflow\nGraphs (SDFGs) including the automatic code-generation of SDF-compatible\nembedded code. A translation of Simulink models to SDFGs, is very suitable due\nto Simulink actor-oriented modeling nature, allowing the application of several\noptimization techniques from the SDFG domain. Because of their well-defined\nsemantics, SDFGs can be analyzed at compiling phase to obtain deadlock-free and\nmemory-efficient schedules. In addition, several real-time analysis methods\nexist which allow throughput-optimal mappings of SDFGs to Multiprocessor on\nChip (MPSoC) while guaranteeing upper-bounded latencies. The correctness of our\ntranslation is justified by integrating the SDF generated code as a\nsoftware-in-the-loop (SIL) and comparing its results with the results of the\nmodel-in-the-loop (MIL) simulation of reference Simulink models. The\ntranslation is demonstrated with the help of two case studies: a Transmission\nController Unit (TCU) and an Automatic Climate Control.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 09:54:12 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 10:13:30 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Fakih", "Maher", ""], ["Warsitz", "Sebastian", ""]]}, {"id": "1701.04612", "submitter": "Rafael Pereira Pires", "authors": "Rafael Pires, Marcelo Pasin, Pascal Felber, Christof Fetzer", "title": "Secure Content-Based Routing Using Intel Software Guard Extensions", "comments": "Middleware '16 Trento, Italy - 10 pages", "journal-ref": null, "doi": "10.1145/2988336.2988346", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Content-based routing (CBR) is a powerful model that supports scalable\nasynchronous communication among large sets of geographically distributed\nnodes. Yet, preserving privacy represents a major limitation for the wide\nadoption of CBR, notably when the routers are located in public clouds. Indeed,\na CBR router must see the content of the messages sent by data producers, as\nwell as the filters (or subscriptions) registered by data consumers. This\nrepresents a major deterrent for companies for which data is a key asset, as\nfor instance in the case of financial markets or to conduct sensitive\nbusiness-to-business transactions. While there exists some techniques for\nprivacy-preserving computation, they are either prohibitively slow or too\nlimited to be usable in real systems. In this paper, we follow a different\nstrategy by taking advantage of trusted hardware extensions that have just been\nintroduced in off-the-shelf processors and provide a trusted execution\nenvironment. We exploit Intel's new software guard extensions (SGX) to\nimplement a CBR engine in a secure enclave. Thanks to the hardware-based\ntrusted execution environment (TEE), the compute-intensive CBR operations can\noperate on decrypted data shielded by the enclave and leverage efficient\nmatching algorithms. Extensive experimental evaluation shows that SGX adds only\nlimited overhead to insecure plaintext matching outside secure enclaves while\nproviding much better performance and more powerful filtering capabilities than\nalternative software-only solutions. To the best of our knowledge, this work is\nthe first to demonstrate the practical benefits of SGX for privacy-preserving\nCBR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 10:48:57 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Pires", "Rafael", ""], ["Pasin", "Marcelo", ""], ["Felber", "Pascal", ""], ["Fetzer", "Christof", ""]]}, {"id": "1701.04679", "submitter": "Evangelos Pournaras", "authors": "Evangelos Pournaras, Mark Yao, Dirk Helbing", "title": "Self-regulating Supply-Demand Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.future.2017.05.018", "report-no": null, "categories": "cs.SY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supply-demand systems in Smart City sectors such as energy, transportation,\ntelecommunication, are subject of unprecedented technological transformations\nby the Internet of Things. Usually, supply-demand systems involve actors that\nproduce and consume resources, e.g. energy, and they are regulated such that\nsupply meets demand, or demand meets available supply. Mismatches of supply and\ndemand may increase operational costs, can cause catastrophic damage in\ninfrastructure, for instance power blackouts, and may even lead to social\nunrest and security threats. Long-term, operationally offline and top-down\nregulatory decision-making by governmental officers, policy makers or system\noperators may turn out to be ineffective for matching supply-demand under new\ndynamics and opportunities that Internet of Things technologies bring to\nsupply-demand systems, for instance, interactive cyber-physical systems and\nsoftware agents running locally in physical assets to monitor and apply\nautomated control actions in real-time. e.g. power flow redistributions by\nsmart transformers to improve the Smart Grid reliability. Existing work on\nonline regulatory mechanisms of matching supply-demand either focuses on\ngame-theoretic solutions with assumptions that cannot be easily met in\nreal-world systems or assume centralized management entities and local access\nto global information. This paper contributes a generic decentralized\nself-regulatory framework, which, in contrast to related work, is shaped around\nstandardized control system concepts and Internet of Things technologies for an\neasier adoption and applicability. The framework involves a decentralized\ncombinatorial optimization mechanism that matches supply-demand under different\nregulatory scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 16:53:12 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 21:56:05 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Pournaras", "Evangelos", ""], ["Yao", "Mark", ""], ["Helbing", "Dirk", ""]]}, {"id": "1701.04733", "submitter": "Ahsan Humayun Mr.", "authors": "Ahsan Humayun, Dr.Muhammad Asif, Dr.Muhammmad Kashif Hanif", "title": "BTAS: A Library for Tropical Algebra", "comments": null, "journal-ref": "International Journal of Computer Science and Information Security\n  2016 Volume 14 No.12", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are dedicated processors used for complex calculations and simulations\nand they can be effectively used for tropical algebra computations. Tropical\nalgebra is based on max-plus algebra and min-plus algebra. In this paper we\nproposed and designed a library based on Tropical Algebra which is used to\nprovide standard vector and matrix operations namely Basic Tropical Algebra\nSubroutines (BTAS). The testing of BTAS library is conducted by implementing\nthe sequential version of Floyd Warshall Algorithm on CPU and furthermore\nparallel version on GPU. The developed library for tropical algebra delivered\nextensively better results on a less expensive GPU as compared to the same on\nCPU.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:44:41 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Humayun", "Ahsan", ""], ["Asif", "Dr. Muhammad", ""], ["Hanif", "Dr. Muhammmad Kashif", ""]]}, {"id": "1701.04763", "submitter": "Eugenio Gianniti", "authors": "Eugenio Gianniti, Danilo Ardagna, Michele Ciavotta, and Mauro\n  Passacantando", "title": "A Game-Theoretic Approach for Runtime Capacity Allocation in MapReduce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays many companies have available large amounts of raw, unstructured\ndata. Among Big Data enabling technologies, a central place is held by the\nMapReduce framework and, in particular, by its open source implementation,\nApache Hadoop. For cost effectiveness considerations, a common approach entails\nsharing server clusters among multiple users. The underlying infrastructure\nshould provide every user with a fair share of computational resources,\nensuring that Service Level Agreements (SLAs) are met and avoiding wastes. In\nthis paper we consider two mathematical programming problems that model the\noptimal allocation of computational resources in a Hadoop 2.x cluster with the\naim to develop new capacity allocation techniques that guarantee better\nperformance in shared data centers. Our goal is to get a substantial reduction\nof power consumption while respecting the deadlines stated in the SLAs and\navoiding penalties associated with job rejections. The core of this approach is\na distributed algorithm for runtime capacity allocation, based on Game Theory\nmodels and techniques, that mimics the MapReduce dynamics by means of\ninteracting players, namely the central Resource Manager and Class Managers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 16:57:15 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Gianniti", "Eugenio", ""], ["Ardagna", "Danilo", ""], ["Ciavotta", "Michele", ""], ["Passacantando", "Mauro", ""]]}, {"id": "1701.04907", "submitter": "Feng Wang", "authors": "Shoulin Wei, Feng Wang, Hui Deng, Cuiyin Liu, Wei Dai, Bo Liang, Ying\n  Mei, Congming Shi, Yingbo Liu, Jingping Wu", "title": "OpenCluster: A Flexible Distributed Computing Framework for Astronomical\n  Data Processing", "comments": null, "journal-ref": null, "doi": "10.1088/1538-3873/129/972/024001", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume of data generated by modern astronomical telescopes is extremely\nlarge and rapidly growing. However, current high-performance data processing\narchitectures/frameworks are not well suited for astronomers because of their\nlimitations and programming difficulties. In this paper, we therefore present\nOpenCluster, an open-source distributed computing framework to support rapidly\ndeveloping high-performance processing pipelines of astronomical big data. We\nfirst detail the OpenCluster design principles and implementations and present\nthe APIs facilitated by the framework. We then demonstrate a case in which\nOpenCluster is used to resolve complex data processing problems for developing\na pipeline for the Mingantu Ultrawide Spectral Radioheliograph. Finally, we\npresent our OpenCluster performance evaluation. Overall, OpenCluster provides\nnot only high fault tolerance and simple programming interfaces, but also a\nflexible means of scaling up the number of interacting entities. OpenCluster\nthereby provides an easily integrated distributed computing framework for\nquickly developing a high-performance data processing system of astronomical\ntelescopes and for significantly reducing software development expenses.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 00:26:32 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Wei", "Shoulin", ""], ["Wang", "Feng", ""], ["Deng", "Hui", ""], ["Liu", "Cuiyin", ""], ["Dai", "Wei", ""], ["Liang", "Bo", ""], ["Mei", "Ying", ""], ["Shi", "Congming", ""], ["Liu", "Yingbo", ""], ["Wu", "Jingping", ""]]}, {"id": "1701.05080", "submitter": "Amos Korman", "authors": "Ofer Feinerman, Amos Korman\\\"e (CNRS, IRIF, GANG)", "title": "Individual versus collective cognition in social insects", "comments": null, "journal-ref": "The Journal of Experimental Biology, The Company of Biologists\n  2017, 220, pp.73 - 82", "doi": "10.1242/jeb.143891", "report-no": null, "categories": "q-bio.NC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concerted responses of eusocial insects to environmental stimuli are\noften referred to as collective cognition on the level of the colony.To achieve\ncollective cognitiona group can draw on two different sources: individual\ncognitionand the connectivity between individuals.Computation in\nneural-networks, for example,is attributedmore tosophisticated communication\nschemes than to the complexity of individual neurons. The case of social\ninsects, however, can be expected to differ. This is since individual insects\nare cognitively capable units that are often able to process information that\nis directly relevant at the level of the colony.Furthermore, involved\ncommunication patterns seem difficult to implement in a group of insects since\nthese lack clear network structure.This review discusses links between the\ncognition of an individual insect and that of the colony. We provide examples\nfor collective cognition whose sources span the full spectrum between\namplification of individual insect cognition and emergent group-level\nprocesses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:43:17 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Feinerman", "Ofer", "", "CNRS, IRIF, GANG"], ["Korman\u00eb", "Amos", "", "CNRS, IRIF, GANG"]]}, {"id": "1701.05403", "submitter": "Do Le Quoc", "authors": "Do Le Quoc and Martin Beck and Pramod Bhatotia and Ruichuan Chen and\n  Christof Fetzer and Thorsten Strufe", "title": "Privacy Preserving Stream Analytics: The Marriage of Randomized Response\n  and Approximate Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to preserve users' privacy while supporting high-utility analytics for\nlow-latency stream processing? To answer this question: we describe the design,\nimplementation, and evaluation of PRIVAPPROX, a data analytics system for\nprivacy-preserving stream processing. PRIVAPPROX provides three properties: (i)\nPrivacy: zero-knowledge privacy guarantees for users, a privacy bound tighter\nthan the state-of-the-art differential privacy; (ii) Utility: an interface for\ndata analysts to systematically explore the trade-offs between the output\naccuracy (with error-estimation) and query execution budget; (iii) Latency:\nnear real-time stream processing based on a scalable \"synchronization-free\"\ndistributed architecture. The key idea behind our approach is to marry two\nexisting techniques together: namely, sampling (used in the context of\napproximate computing) and randomized response (used in the context of\nprivacy-preserving analytics). The resulting marriage is complementary - it\nachieves stronger privacy guarantees and also improves performance, a necessary\ningredient for achieving low-latency stream analytics.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 13:16:27 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 15:32:17 GMT"}, {"version": "v3", "created": "Wed, 8 Feb 2017 23:43:43 GMT"}, {"version": "v4", "created": "Tue, 21 Mar 2017 15:17:46 GMT"}, {"version": "v5", "created": "Mon, 5 Jun 2017 14:02:17 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Quoc", "Do Le", ""], ["Beck", "Martin", ""], ["Bhatotia", "Pramod", ""], ["Chen", "Ruichuan", ""], ["Fetzer", "Christof", ""], ["Strufe", "Thorsten", ""]]}, {"id": "1701.05431", "submitter": "Adam Larat", "authors": "Mohamed Essadki (IFPEN, FR3487, EM2C), Jonathan Jung (LMAP), Adam\n  Larat (FR3487, EM2C), Milan Pelletier (EM2C), Vincent Perrier (LMAP)", "title": "A task-driven implementation of a simple numerical solver for hyperbolic\n  conservation laws", "comments": null, "journal-ref": "ESAIM: Proceedings and Surveys, EDP Sciences, pp.1 - 10 (2017)", "doi": null, "report-no": null, "categories": "cs.DC cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the implementation of an all-in-one numerical\nprocedure within the runtime StarPU. In order to limit the complexity of the\nmethod, for the sake of clarity of the presentation of the non-classical\ntask-driven programming environnement, we have limited the numerics to first\norder in space and time. Results show that the task distribution is efficient\nif the tasks are numerous and individually large enough so that the task heap\ncan be saturated by tasks which computational time covers the task management\noverhead. Next, we also see that even though they are mostly faster on graphic\ncards, not all the tasks are suitable for GPUs, which brings forward the\nimportance of the task scheduler. Finally, we look at a more realistic system\nof conservation laws with an expensive source term, what allows us to conclude\nand open on future works involving higher local arithmetic intensity, by\nincreasing the order of the numerical method or by enriching the model\n(increased number of parameters and therefore equations).\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:29:17 GMT"}], "update_date": "2017-01-22", "authors_parsed": [["Essadki", "Mohamed", "", "IFPEN, FR3487, EM2C"], ["Jung", "Jonathan", "", "LMAP"], ["Larat", "Adam", "", "FR3487, EM2C"], ["Pelletier", "Milan", "", "EM2C"], ["Perrier", "Vincent", "", "LMAP"]]}, {"id": "1701.05451", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Nan Wang, Dimitrios S. Nikolopoulos, Rajkumar Buyya", "title": "Feasibility of Fog Computing", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As billions of devices get connected to the Internet, it will not be\nsustainable to use the cloud as a centralised server. The way forward is to\ndecentralise computations away from the cloud towards the edge of the network\ncloser to the user. This reduces the latency of communication between a user\ndevice and the cloud, and is the premise of 'fog computing' defined in this\npaper. The aim of this paper is to highlight the feasibility and the benefits\nin improving the Quality-of-Service and Experience by using fog computing. For\nan online game use-case, we found that the average response time for a user is\nimproved by 20% when using the edge of the network in comparison to using a\ncloud-only model. It was also observed that the volume of traffic between the\nedge and the cloud server is reduced by over 90% for the use-case. The\npreliminary results highlight the potential of fog computing in achieving a\nsustainable computing model and highlights the benefits of integrating the edge\nof the network into the computing ecosystem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:58:17 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Varghese", "Blesson", ""], ["Wang", "Nan", ""], ["Nikolopoulos", "Dimitrios S.", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "1701.05463", "submitter": "Artem Khyzha", "authors": "Artem Khyzha, Mike Dodds, Alexey Gotsman, Matthew Parkinson", "title": "Proving Linearizability Using Partial Orders (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the commonly accepted notion of correctness for concurrent\ndata structures. It requires that any execution of the data structure is\njustified by a linearization --- a linear order on operations satisfying the\ndata structure's sequential specification. Proving linearizability is often\nchallenging because an operation's position in the linearization order may\ndepend on future operations. This makes it very difficult to incrementally\nconstruct the linearization in a proof.\n  We propose a new proof method that can handle data structures with such\nfuture-dependent linearizations. Our key idea is to incrementally construct not\na single linear order of operations, but a partial order that describes\nmultiple linearizations satisfying the sequential specification. This allows\ndecisions about the ordering of operations to be delayed, mirroring the\nbehaviour of data structure implementations. We formalise our method as a\nprogram logic based on rely-guarantee reasoning, and demonstrate its\neffectiveness by verifying several challenging data structures: the\nHerlihy-Wing queue, the TS queue and the Optimistic set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:13:14 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:58:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 23:04:36 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 13:35:08 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Khyzha", "Artem", ""], ["Dodds", "Mike", ""], ["Gotsman", "Alexey", ""], ["Parkinson", "Matthew", ""]]}, {"id": "1701.05478", "submitter": "Kim-Anh Tran", "authors": "Anton Weber, Kim-Anh Tran, Stefanos Kaxiras and Alexandra Jimborean", "title": "Decoupled Access-Execute on ARM big.LITTLE", "comments": "Presented at HIP3ES, 2017", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2017/4", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-efficiency plays a significant role given the battery lifetime\nconstraints in embedded systems and hand-held devices. In this work we target\nthe ARM big.LITTLE, a heterogeneous platform that is dominant in the mobile and\nembedded market, which allows code to run transparently on different\nmicroarchitectures with individual energy and performance characteristics. It\nallows to se more energy efficient cores to conserve power during simple tasks\nand idle times and switch over to faster, more power hungry cores when\nperformance is needed. This proposal explores the power-savings and the\nperformance gains that can be achieved by utilizing the ARM big.LITTLE core in\ncombination with Decoupled Access-Execute (DAE). DAE is a compiler technique\nthat splits code regions into two distinct phases: a memory-bound Access phase\nand a compute-bound Execute phase. By scheduling the memory-bound phase on the\nLITTLE core, and the compute-bound phase on the big core, we conserve energy\nwhile caching data from main memory and perform computations at maximum\nperformance. Our preliminary findings show that applying DAE on ARM big.LITTLE\nhas potential. By prefetching data in Access we can achieve an IPC improvement\nof up to 37% in the Execute phase, and manage to shift more than half of the\nprogram runtime to the LITTLE core. We also provide insight into advantages and\ndisadvantages of our approach, present preliminary results and discuss\npotential solutions to overcome locking overhead.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 22:14:52 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Weber", "Anton", ""], ["Tran", "Kim-Anh", ""], ["Kaxiras", "Stefanos", ""], ["Jimborean", "Alexandra", ""]]}, {"id": "1701.05945", "submitter": "Josef Spillner", "authors": "Josef Spillner", "title": "Exploiting the Cloud Control Plane for Fun and Profit", "comments": "14 pages, 10 figures, unreviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers typically charge for their services. There are diverse\npricing models which often follow a pay-per-use paradigm. The consumers'\npayments are expected to cover all cost which incurs to the provider for\nprocessing, storage, bandwidth, data centre operation and engineering efforts,\namong others. In contrast, the consumer management interfaces are free of\ncharge as they are expected to cause only a minority of the load compared to\nthe actual computing services. With new service models and more complex and\npowerful management abilities, it is time to rethink this decision. The paper\nshows how to exploit the control plane of AWS Lambda to implement stateful\nservices practically for free and under some circumstances even guaranteed for\nfree which if widely deployed would cause a monetary loss for the provider. It\nalso elaborates on the consistency model for AWS Lambda.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 22:48:31 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Spillner", "Josef", ""]]}, {"id": "1701.05973", "submitter": "Amirhossein Reisizadeh", "authors": "Amirhossein Reisizadeh, Saurav Prakash, Ramtin Pedarsani, Amir Salman\n  Avestimehr", "title": "Coded Computation over Heterogeneous Clusters", "comments": "This work is published in IEEE Transaction on Information Theory\n  (2019). A preliminary version of this work was published in IEEE\n  International Symposium on Information Theory (ISIT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale distributed computing clusters, such as Amazon EC2, there are\nseveral types of \"system noise\" that can result in major degradation of\nperformance: bottlenecks due to limited communication bandwidth, latency due to\nstraggler nodes, etc. On the other hand, these systems enjoy abundance of\nredundancy - a vast number of computing nodes and large storage capacity. There\nhave been recent results that demonstrate the impact of coding for efficient\nutilization of computation and storage redundancy to alleviate the effect of\nstragglers and communication bottlenecks in homogeneous clusters. In this\npaper, we focus on general heterogeneous distributed computing clusters\nconsisting of a variety of computing machines with different capabilities. We\npropose a coding framework for speeding up distributed computing in\nheterogeneous clusters by trading redundancy for reducing the latency of\ncomputation. In particular, we propose Heterogeneous Coded Matrix\nMultiplication (HCMM) algorithm for performing distributed matrix\nmultiplication over heterogeneous clusters that is provably asymptotically\noptimal for a broad class of processing time distributions. Moreover, we show\nthat HCMM is unboundedly faster than any uncoded scheme. To demonstrate\npracticality of HCMM, we carry out experiments over Amazon EC2 clusters where\nHCMM is found to be up to $61\\%$, $46\\%$ and $36\\%$ respectively faster than\nthree benchmark load allocation schemes - Uniform Uncoded, Load-balanced\nUncoded, and Uniform Coded. Additionally, we provide a generalization to the\nproblem of optimal load allocation in heterogeneous settings, where we take\ninto account the monetary costs associated with the clusters. We argue that\nHCMM is asymptotically optimal for budget-constrained scenarios as well, and we\ndevelop a heuristic algorithm for (HCMM) load allocation for budget-limited\ncomputation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 03:11:47 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 23:06:31 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 21:43:11 GMT"}, {"version": "v4", "created": "Tue, 9 Oct 2018 06:03:03 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2019 23:58:44 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Prakash", "Saurav", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "Amir Salman", ""]]}, {"id": "1701.05982", "submitter": "Sudhakar Singh", "authors": "Sudhakar Singh, Rakhi Garg, P. K. Mishra", "title": "Observations on Factors Affecting Performance of MapReduce based Apriori\n  on Hadoop Cluster", "comments": "8 pages, 8 figures, International Conference on Computing,\n  Communication and Automation (ICCCA2016)", "journal-ref": "2016 International Conference on Computing, Communication and\n  Automation (ICCCA), Greater Noida, India, 2016, pp. 87-94", "doi": "10.1109/CCAA.2016.7813695", "report-no": "466", "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing fast and scalable algorithm for mining frequent itemsets is always\nbeing a most eminent and promising problem of data mining. Apriori is one of\nthe most broadly used and popular algorithm of frequent itemset mining.\nDesigning efficient algorithms on MapReduce framework to process and analyze\nbig datasets is contemporary research nowadays. In this paper, we have focused\non the performance of MapReduce based Apriori on homogeneous as well as on\nheterogeneous Hadoop cluster. We have investigated a number of factors that\nsignificantly affects the execution time of MapReduce based Apriori running on\nhomogeneous and heterogeneous Hadoop Cluster. Factors are specific to both\nalgorithmic and non-algorithmic improvements. Considered factors specific to\nalgorithmic improvements are filtered transactions and data structures.\nExperimental results show that how an appropriate data structure and filtered\ntransactions technique drastically reduce the execution time. The\nnon-algorithmic factors include speculative execution, nodes with poor\nperformance, data locality & distribution of data blocks, and parallelism\ncontrol with input split size. We have applied strategies against these factors\nand fine tuned the relevant parameters in our particular application.\nExperimental results show that if cluster specific parameters are taken care of\nthen there is a significant reduction in execution time. Also we have discussed\nthe issues regarding MapReduce implementation of Apriori which may\nsignificantly influence the performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 05:12:13 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Singh", "Sudhakar", ""], ["Garg", "Rakhi", ""], ["Mishra", "P. K.", ""]]}, {"id": "1701.05986", "submitter": "Xie Pei", "authors": "Pei Xie, Keyou You, Shiji Song and Cheng Wu", "title": "Distributed Random-Fixed Projected Algorithm for Constrained\n  Optimization Over Digraphs", "comments": "arXiv admin note: substantial text overlap with arXiv:1612.09029", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a constrained optimization problem over a\ndirected graph (digraph) of nodes, in which the cost function is a sum of local\nobjectives, and each node only knows its local objective and constraints. To\ncollaboratively solve the optimization, most of the existing works require the\ninteraction graph to be balanced or \"doubly-stochastic\", which is quite\nrestrictive and not necessary as shown in this paper. We focus on an epigraph\nform of the original optimization to resolve the \"unbalanced\" problem, and\ndesign a novel two-step recursive algorithm with a simple structure. Under\nstrongly connected digraphs, we prove that each node asymptotically converges\nto some common optimal solution. Finally, simulations are performed to\nillustrate the effectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 05:54:51 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Xie", "Pei", ""], ["You", "Keyou", ""], ["Song", "Shiji", ""], ["Wu", "Cheng", ""]]}, {"id": "1701.05996", "submitter": "Yogesh Simmhan", "authors": "Jayanth Kalyanasundaram and Yogesh Simmhan", "title": "ARM Wrestling with Big Data: A Study of Commodity ARM64 Server for Big\n  Data Workloads", "comments": "Accepted for publication in the Proceedings of the 24th IEEE\n  International Conference on High Performance Computing, Data, and Analytics\n  (HiPC), 2017", "journal-ref": "Proceedings of the IEEE 24th International Conference on High\n  Performance Computing (HiPC), Jaipur, India, 2017", "doi": "10.1109/HiPC.2017.00032", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARM processors have dominated the mobile device market in the last decade due\nto their favorable computing to energy ratio. In this age of Cloud data centers\nand Big Data analytics, the focus is increasingly on power efficient\nprocessing, rather than just high throughput computing. ARM's first commodity\nserver-grade processor is the recent AMD A1100-series processor, based on a\n64-bit ARM Cortex A57 architecture. In this paper, we study the performance and\nenergy efficiency of a server based on this ARM64 CPU, relative to a comparable\nserver running an AMD Opteron 3300-series x64 CPU, for Big Data workloads.\nSpecifically, we study these for Intel's HiBench suite of web, query and\nmachine learning benchmarks on Apache Hadoop v2.7 in a pseudo-distributed\nsetup, for data sizes up to $20GB$ files, $5M$ web pages and $500M$ tuples. Our\nresults show that the ARM64 server's runtime performance is comparable to the\nx64 server for integer-based workloads like Sort and Hive queries, and only\nlags behind for floating-point intensive benchmarks like PageRank, when they do\nnot exploit data parallelism adequately. We also see that the ARM64 server\ntakes $\\frac{1}{3}^{rd}$ the energy, and has an Energy Delay Product (EDP) that\nis $50-71\\%$ lower than the x64 server. These results hold promise for ARM64\ndata centers hosting Big Data workloads to reduce their operational costs,\nwhile opening up opportunities for further analysis.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 07:58:58 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 02:27:14 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Kalyanasundaram", "Jayanth", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1701.06356", "submitter": "Akshar Varma", "authors": "Akshar Varma (1), Yashwant Keswani (1), Yashodhan Bhatnagar (1) and\n  Bhaskar Chaudhury (1) ((1) Dhirubhai Ambani Institute of Information and\n  Communication Technology, Gandhinagar, India)", "title": "Let's HPC: A web-based interactive platform to aid High Performance\n  Computing education", "comments": "8 pages, 4 figures. Submitted to EduPar-17. This paper is regarding\n  the Let's HPC platform which can be found here: http://www.letshpc.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let's HPC (www.letshpc.org) is an open-access online platform to supplement\nconventional classroom oriented High Performance Computing (HPC) and Parallel &\nDistributed Computing (PDC) education. The web based platform provides online\nplotting and analysis tools which allow users to learn, evaluate, teach and see\nthe performance of parallel algorithms from a system's viewpoint. The user can\nquantitatively compare and understand the importance of numerous deterministic\nas well as non-deterministic factors of both the software and the hardware that\nimpact the performance of parallel programs. At the heart of this platform is a\ndatabase archiving the performance and execution environment related data of\nstandard parallel algorithms executed on different computing architectures\nusing different programming environments, this data is contributed by various\nstakeholders in the HPC community. The plotting and analysis tools of our\nplatform can be combined seamlessly with the database to aid self-learning,\nteaching, evaluation and discussion of different HPC related topics.\nInstructors of HPC/PDC related courses can use the platform's tools to\nillustrate the importance of proper analysis in understanding factors impacting\nperformance, to encourage peer learning among students, as well as to allow\nstudents to prepare a standard lab/project report aiding the instructor in\nuniform evaluation. The platform's modular design enables easy inclusion of\nperformance related data from contributors as well as addition of new features\nin the future.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 12:24:49 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Varma", "Akshar", ""], ["Keswani", "Yashwant", ""], ["Bhatnagar", "Yashodhan", ""], ["Chaudhury", "Bhaskar", ""]]}, {"id": "1701.06562", "submitter": "Qiang Cao", "authors": "Qiang Cao, Vamsi Thummala, Jeffrey S. Chase, Yuanjun Yao, Bing Xie", "title": "Certificate Linking and Caching for Logical Trust", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAFE is a data-centric platform for building multi-domain networked systems,\ni.e., systems whose participants are controlled by different principals.\nParticipants make trust decisions by issuing local queries over logic content\nexchanged in certificates. The contribution of SAFE is to address a key barrier\nto practical use of logical trust: the problem of identifying, gathering, and\nassembling the certificates that are relevant to each trust decision.\n  SAFE uses a simple linking abstraction to organize and share certificates\naccording to scripted primitives that implement the application's trust kernel\nand isolate it from logic concerns. We show that trust scripting with logical\ndata exchange yields compact trust cores for example applications: federated\nnaming, nested groups and roles, secure IP prefix delegation and routing,\nattestation-based access control, and a federated infrastructure-as-a-service\nsystem. Linking allows granular control over dynamic logic content based on\ndependency relationships, enabling a logic server to make secure inferences at\nhigh throughput.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 21:05:17 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Cao", "Qiang", ""], ["Thummala", "Vamsi", ""], ["Chase", "Jeffrey S.", ""], ["Yao", "Yuanjun", ""], ["Xie", "Bing", ""]]}, {"id": "1701.06783", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "The Autonomic Architecture of the Licas System", "comments": null, "journal-ref": "IOSR Journal of Computer Engineering (IOSR-JCE), e-ISSN:\n  2278-0661, p-ISSN: 2278-8727, Vol. 21, Issue 5, pp. 58-65, 2019", "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Licas (lightweight internet-based communication for autonomic services) is a\ndistributed framework for building service-based systems. The framework\nprovides a p2p server and more intelligent processing of information through\nits AI algorithms. Distributed communication includes XML-RPC, REST, HTTP and\nWeb Services. It can now provide a robust platform for building different types\nof system, where Microservices or SOA would be possible. However, the system\nmay be equally suited for the IoT, as it provides classes to connect with\nexternal sources and has an optional Autonomic Manager with a MAPE control loop\nintegrated into the communication process. The system is also mobile-compatible\nwith Android. This paper focuses in particular on the autonomic setup and how\nthat might be used. A novel linking mechanism has been described previously and\nis considered again, as part of the autonomous framework.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 09:36:16 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:26:21 GMT"}, {"version": "v3", "created": "Sun, 21 Jul 2019 13:00:45 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1701.06800", "submitter": "Manfred Schwarz", "authors": "Manfred Schwarz, Martin Zeiner and Ulrich Schmid", "title": "Linear-Time Data Dissemination in Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadcasting and convergecasting are pivotal services in distributed systems,\nin particular, in wireless ad-hoc and sensor networks, which are characterized\nby time- varying communication graphs. We study the question of whether it is\npossible to disseminate data available locally at some process to all n\nprocesses in sparsely connected synchronous dynamic networks with directed\nlinks in linear time. Recently, Charron-Bost, F\\\"ugger and Nowak proved an\nupper bound of O(n log n) rounds for the case where every communication graph\nis an arbitrary directed rooted tree. We present a new formalism, which not\nonly facilitates a concise proof of this result, but also allows us to prove\nthat O(n) data dissemination is possible when the number of leaves of the\nrooted trees are bounded by a constant. In the special case of rooted chains,\nonly (n-1) rounds are needed. Our approach can also be adapted for undirected\nnetworks, where only (n-1)/2 rounds in the case of arbitrary chain graphs are\nneeded.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 10:38:08 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 09:19:48 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Schwarz", "Manfred", ""], ["Zeiner", "Martin", ""], ["Schmid", "Ulrich", ""]]}, {"id": "1701.06828", "submitter": "Nidhi Rastogi", "authors": "Nidhi Rastogi, Marie Joan Kristine Gloria and James Hendler", "title": "Security and Privacy of performing Data Analytics in the cloud - A\n  three-way handshake of Technology, Policy, and Management", "comments": "28 pages, 3 figures, Journal of Information Privacy", "journal-ref": "Journal of Information Policy 5 (2015): 129-154", "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cloud platform came into existence primarily to accelerate IT delivery and to\npromote innovation. To this point, it has performed largely well to the\nexpectations of technologists, businesses and customers. The service aspect of\nthis technology has paved the road for a faster set up of infrastructure and\nrelated goals for both startups and established organizations. This has further\nled to quicker delivery of many user-friendly applications to the market while\nproving to be a commercially viable option to companies with limited resources.\nOn the technology front, the creation and adoption of this ecosystem has\nallowed easy collection of massive data from various sources at one place,\nwhere the place is sometimes referred as just the cloud. Efficient data mining\ncan be performed on raw data to extract potentially useful information, which\nwas not possible at this scale before. Targeted advertising is a common example\nthat can help businesses. Despite these promising offerings, concerns around\nsecurity and privacy of user information suppressed wider acceptance and an\nall-encompassing deployment of the cloud platform. In this paper, we discuss\nsecurity and privacy concerns that occur due to data exchanging hands between a\ncloud servicer provider (CSP) and the primary cloud user - the data collector,\nfrom the content generator. We offer solutions that encompass technology,\npolicy and sound management of the cloud service asserting that this approach\nhas the potential to provide a holistic solution.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 11:59:28 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Rastogi", "Nidhi", ""], ["Gloria", "Marie Joan Kristine", ""], ["Hendler", "James", ""]]}, {"id": "1701.07154", "submitter": "Liang Yu", "authors": "Liang Yu, Tao Jiang, Yulong Zou", "title": "Fog-Assisted Operational Cost Reduction for Cloud Data Centers", "comments": "8 pages, 13 figures, accepted by IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2017.2728624", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we intend to reduce the operational cost of cloud data centers\nwith the help of fog devices, which can avoid the revenue loss due to wide-area\nnetwork propagation delay and save network bandwidth cost by serving nearby\ncloud users. Since fog devices may not be owned by a cloud service provider,\nthey should be compensated for serving the requests of cloud users. When taking\neconomical compensation into consideration, the optimal number of requests\nprocessed locally by each fog device should be decided. As a result, existing\nload balancing schemes developed for cloud data centers can not be applied\ndirectly and it is very necessary to redesign a cost-ware load balancing\nalgorithm for the fog-cloud system. To achieve the above aim, we first\nformulate a fog-assisted operational cost minimization problem for the cloud\nservice provider. Then, we design a parallel and distributed load balancing\nalgorithm with low computational complexity based on Proximal Jacobian\nAlternating Direction Method of Multipliers (PJ-ADMM). Finally, extensive\nsimulation results show the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 04:31:06 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 07:38:20 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 05:12:51 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Yu", "Liang", ""], ["Jiang", "Tao", ""], ["Zou", "Yulong", ""]]}, {"id": "1701.07166", "submitter": "Shaowei Wang", "authors": "Shaowei Wang, Liusheng Huang, Pengzhan Wang, Hongli Xu, Wei Yang", "title": "Personalized Classifier Ensemble Pruning Framework for Mobile\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning has been widely employed by mobile applications, ranging\nfrom environmental sensing to activity recognitions. One of the fundamental\nissue in ensemble learning is the trade-off between classification accuracy and\ncomputational costs, which is the goal of ensemble pruning. During\ncrowdsourcing, the centralized aggregator releases ensemble learning models to\na large number of mobile participants for task evaluation or as the\ncrowdsourcing learning results, while different participants may seek for\ndifferent levels of the accuracy-cost trade-off. However, most of existing\nensemble pruning approaches consider only one identical level of such\ntrade-off. In this study, we present an efficient ensemble pruning framework\nfor personalized accuracy-cost trade-offs via multi-objective optimization.\nSpecifically, for the commonly used linear-combination style of the trade-off,\nwe provide an objective-mixture optimization to further reduce the number of\nensemble candidates. Experimental results show that our framework is highly\nefficient for personalized ensemble pruning, and achieves much better pruning\nperformance with objective-mixture optimization when compared to state-of-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 05:22:35 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Wang", "Shaowei", ""], ["Huang", "Liusheng", ""], ["Wang", "Pengzhan", ""], ["Xu", "Hongli", ""], ["Yang", "Wei", ""]]}, {"id": "1701.07248", "submitter": "Johan Thunberg", "authors": "Johan Thunberg, Florian Bernard, Jorge Goncalves", "title": "Distributed methods for synchronization of orthogonal matrices over\n  graphs", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": "10.1016/j.automatica.2017.02.025", "report-no": null, "categories": "math.OC cs.CV cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of synchronizing orthogonal matrices over\ndirected graphs. For synchronized transformations (or matrices), composite\ntransformations over loops equal the identity. We formulate the synchronization\nproblem as a least-squares optimization problem with nonlinear constraints. The\nsynchronization problem appears as one of the key components in applications\nranging from 3D-localization to image registration. The main contributions of\nthis work can be summarized as the introduction of two novel algorithms; one\nfor symmetric graphs and one for graphs that are possibly asymmetric. Under\ngeneral conditions, the former has guaranteed convergence to the solution of a\nspectral relaxation to the synchronization problem. The latter is stable for\nsmall step sizes when the graph is quasi-strongly connected. The proposed\nmethods are verified in numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:34:45 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 11:26:23 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 12:11:11 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Thunberg", "Johan", ""], ["Bernard", "Florian", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1701.07294", "submitter": "Manuel Lafond", "authors": "Stefan Dobrev, Evangelos Kranakis, Danny Krizanc, Manuel Lafond, Jan\n  Manuch, Lata Narayanan, Jaroslav Opatrny and Ladislav Stacho", "title": "Weak Coverage of a Rectangular Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume n wireless mobile sensors are initially dispersed in an ad hoc manner\nin a rectangular region. They are required to move to final locations so that\nthey can detect any intruder crossing the region in a direction parallel to the\nsides of the rectangle, and thus provide weak barrier coverage of the region.\nWe study three optimization problems related to the movement of sensors to\nachieve weak barrier coverage: minimizing the number of sensors moved (MinNum),\nminimizing the average distance moved by the sensors (MinSum), and minimizing\nthe maximum distance moved by the sensors (MinMax). We give an O(n^{3/2}) time\nalgorithm for the MinNum problem for sensors of diameter 1 that are initially\nplaced at integer positions; in contrast we show that the problem is NP-hard\neven for sensors of diameter 2 that are initially placed at integer positions.\nWe show that the MinSum problem is solvable in O(n log n) time for homogeneous\nrange sensors in arbitrary initial positions, while it is NP-hard for\nheterogeneous sensor ranges. Finally, we prove that even very restricted\nhomogeneous versions of the MinMax problem are NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 13:06:16 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Dobrev", "Stefan", ""], ["Kranakis", "Evangelos", ""], ["Krizanc", "Danny", ""], ["Lafond", "Manuel", ""], ["Manuch", "Jan", ""], ["Narayanan", "Lata", ""], ["Opatrny", "Jaroslav", ""], ["Stacho", "Ladislav", ""]]}, {"id": "1701.07495", "submitter": "Ivo Kubjas", "authors": "Ivo Kubjas and Vitaly Skachek", "title": "Two-Party Function Computation on the Reconciled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we initiate a study of a new problem termed function\ncomputation on the reconciled data, which generalizes a set reconciliation\nproblem in the literature. Assume a distributed data storage system with two\nusers $A$ and $B$. The users possess a collection of binary vectors $S_{A}$ and\n$S_{B}$, respectively. They are interested in computing a function $\\phi$ of\nthe reconciled data $S_{A} \\cup S_{B}$.\n  It is shown that any deterministic protocol, which computes a sum and a\nproduct of reconciled sets of binary vectors represented as nonnegative\nintegers, has to communicate at least $2^n + n - 1$ and $2^n + n - 2$ bits in\nthe worst-case scenario, respectively, where $n$ is the length of the binary\nvectors. Connections to other problems in computer science, such as set\ndisjointness and finding the intersection, are established, yielding a variety\nof additional upper and lower bounds on the communication complexity. A\nprotocol for computation of a sum function, which is based on use of a family\nof hash functions, is presented, and its characteristics are analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:41:47 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 12:59:47 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Kubjas", "Ivo", ""], ["Skachek", "Vitaly", ""]]}, {"id": "1701.07500", "submitter": "Paras Jain", "authors": "Paras Jain, Chirag Tailor, Sam Ford, Liexiao Ding, Michael Phillips,\n  Fang Liu, Nagi Gebraeel, Duen Horng Chau", "title": "Scalable Architecture for Anomaly Detection and Visualization in Power\n  Generating Assets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-generating assets (e.g., jet engines, gas turbines) are often\ninstrumented with tens to hundreds of sensors for monitoring physical and\nperformance degradation. Anomaly detection algorithms highlight deviations from\npredetermined benchmarks with the goal of detecting incipient faults. We are\ndeveloping an integrated system to address three key challenges within\nanalyzing sensor data from power-generating assets: (1) difficulty in ingesting\nand analyzing data from large numbers of machines; (2) prevalence of false\nalarms generated by anomaly detection algorithms resulting in unnecessary\ndowntime and maintenance; and (3) lack of an integrated visualization that\nhelps users understand and explore the flagged anomalies and relevant sensor\ncontext in the energy domain. We present preliminary results and our key\nfindings in addressing these challenges. Our system's scalable event ingestion\nframework, based on OpenTSDB, ingests nearly 400,000 sensor data samples per\nseconds using a 30 machine cluster. To reduce false alarm rates, we leverage\nthe False Discovery Rate (FDR) algorithm which significantly reduces the number\nof false alarms. Our visualization tool presents the anomalies and associated\ncontent flagged by the FDR algorithm to inform users and practitioners in their\ndecision making process. We believe our integrated platform will help reduce\nmaintenance costs significantly while increasing asset lifespan. We are working\nto extend our system on multiple fronts, such as scaling to more data and more\ncompute nodes (70 in total).\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:51:28 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jain", "Paras", ""], ["Tailor", "Chirag", ""], ["Ford", "Sam", ""], ["Ding", "Liexiao", ""], ["Phillips", "Michael", ""], ["Liu", "Fang", ""], ["Gebraeel", "Nagi", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1701.07512", "submitter": "Yuqing Zhu", "authors": "Yuqing Zhu, Jianxun Liu, Mengying Guo, Wenlong Ma, Guolei Yi, Yungang\n  Bao", "title": "ACIA, not ACID: Conditions, Properties and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although ACID is the previous golden rule for transaction support, durability\nis now not a basic requirement for data storage. Rather, high availability is\nbecoming the first-class property required by online applications. We show that\nhigh availability of data is almost surely a stronger property than durability.\nWe thus propose ACIA (Atomicity, Consistency, Isolation, Availability) as the\nnew standard for transaction support. Essentially, the shift from ACID to ACIA\nis due to the change of assumed conditions for data management. Four major\ncondition changes exist. With ACIA transactions, more diverse requirements can\nbe flexibly supported for applications through the specification of consistency\nlevels, isolation levels and fault tolerance levels. Clarifying the ACIA\nproperties enables the exploitation of techniques used for ACID transactions,\nas well as bringing about new challenges for research.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 22:44:31 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 09:02:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Zhu", "Yuqing", ""], ["Liu", "Jianxun", ""], ["Guo", "Mengying", ""], ["Ma", "Wenlong", ""], ["Yi", "Guolei", ""], ["Bao", "Yungang", ""]]}, {"id": "1701.07615", "submitter": "Christopher Meiklejohn", "authors": "Christopher S. Meiklejohn", "title": "On the Design of Distributed Programming Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming large-scale distributed applications requires new abstractions\nand models to be done well. We demonstrate that these models are possible.\n  Following from both the FLP result and CAP theorem, we show that concurrent\nprogramming models are necessary, but not sufficient, in the construction of\nlarge-scale distributed systems because of the problem of failure and network\npartitions: languages need to be able to capture and encode the tradeoffs\nbetween consistency and availability.\n  We present two programming models, Lasp and Austere, each of which makes a\nstrong tradeoff with respects to the CAP theorem. These two models outline the\nbounds of distributed model design: strictly AP or strictly CP. We argue that\nall possible distributed programming models must come from this design space,\nand present one practical design that allows declarative specification of\nconsistency tradeoffs, called Spry.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 08:34:11 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 14:09:31 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Meiklejohn", "Christopher S.", ""]]}, {"id": "1701.08022", "submitter": "Sebastian Deorowicz", "authors": "Marek Kokot and Maciej D{\\l}ugosz and Sebastian Deorowicz", "title": "KMC 3: counting and manipulating k-mer statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: Counting all k-mers in a given dataset is a standard procedure in\nmany bioinformatics applications. We introduce KMC3, a significant improvement\nof the former KMC2 algorithm together with KMC tools for manipulating k-mer\ndatabases. Usefulness of the tools is shown on a few real problems.\nAvailability: Program is freely available at\nhttp://sun.aei.polsl.pl/REFRESH/kmc. Contact: sebastian.deorowicz@polsl.pl\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:04:30 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kokot", "Marek", ""], ["D\u0142ugosz", "Maciej", ""], ["Deorowicz", "Sebastian", ""]]}, {"id": "1701.08084", "submitter": "Patrick P. C. Lee", "authors": "Matt M. T. Yiu, Helen H. W. Chan, Patrick P. C. Lee", "title": "Erasure Coding for Small Objects in In-Memory KV Storage", "comments": "Accepted by SYSTOR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MemEC, an erasure-coding-based in-memory key-value (KV) store that\nachieves high availability and fast recovery while keeping low data redundancy\nacross storage servers. MemEC is specifically designed for workloads dominated\nby small objects. By encoding objects in entirety, MemEC is shown to incur 60%\nless storage redundancy for small objects than existing replication- and\nerasure-coding-based approaches. It also supports graceful transitions between\ndecentralized requests in normal mode (i.e., no failures) and coordinated\nrequests in degraded mode (i.e., with failures). We evaluate our MemEC\nprototype via testbed experiments under read-heavy and update-heavy YCSB\nworkloads. We show that MemEC achieves high throughput and low latency in both\nnormal and degraded modes, and supports fast transitions between the two modes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 15:39:50 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 15:24:05 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yiu", "Matt M. T.", ""], ["Chan", "Helen H. W.", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1701.08361", "submitter": "Martin Uecker", "authors": "Sebastian Schaetz, Dirk Voit, Jens Frahm, Martin Uecker", "title": "Accelerated Computing in Magnetic Resonance Imaging -- Real-Time Imaging\n  Using Non-Linear Inverse Reconstruction", "comments": "22 pages, 8 figures, 6 tables", "journal-ref": "Computational and Mathematical Methods in Medicine, 2017:3527269\n  (2017)", "doi": "10.1155/2017/3527269", "report-no": null, "categories": "cs.DC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop generic optimization strategies for image reconstruction\nusing graphical processing units (GPUs) in magnetic resonance imaging (MRI) and\nto exemplarily report about our experience with a highly accelerated\nimplementation of the non-linear inversion algorithm (NLINV) for dynamic MRI\nwith high frame rates. Methods: The NLINV algorithm is optimized and ported to\nrun on an a multi-GPU single-node server. The algorithm is mapped to multiple\nGPUs by decomposing the data domain along the channel dimension. Furthermore,\nthe algorithm is decomposed along the temporal domain by relaxing a temporal\nregularization constraint, allowing the algorithm to work on multiple frames in\nparallel. Finally, an autotuning method is presented that is capable of\ncombining different decomposition variants to achieve optimal algorithm\nperformance in different imaging scenarios. Results: The algorithm is\nsuccessfully ported to a multi-GPU system and allows online image\nreconstruction with high frame rates. Real-time reconstruction with low latency\nand frame rates up to 30 frames per second is demonstrated. Conclusion: Novel\nparallel decomposition methods are presented which are applicable to many\niterative algorithms for dynamic MRI. Using these methods to parallelize the\nNLINV algorithm on multiple GPUs it is possible to achieve online image\nreconstruction with high frame rates.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 10:38:06 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 15:19:47 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Schaetz", "Sebastian", ""], ["Voit", "Dirk", ""], ["Frahm", "Jens", ""], ["Uecker", "Martin", ""]]}, {"id": "1701.08474", "submitter": "Prasanna Kansakar", "authors": "Arslan Munir, Prasanna Kansakar, Samee U. Khan", "title": "IFCIoT: Integrated Fog Cloud IoT Architectural Paradigm for Future\n  Internet of Things", "comments": "9 pages, 3 figures, accepted for publication in IEEE Consumer\n  Electronics Magazine, July 2017 issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel integrated fog cloud IoT (IFCIoT) architectural paradigm\nthat promises increased performance, energy efficiency, reduced latency,\nquicker response time, scalability, and better localized accuracy for future\nIoT applications. The fog nodes (e.g., edge servers, smart routers, base\nstations) receive computation offloading requests and sensed data from various\nIoT devices. To enhance performance, energy efficiency, and real-time\nresponsiveness of applications, we propose a reconfigurable and layered fog\nnode (edge server) architecture that analyzes the applications' characteristics\nand reconfigure the architectural resources to better meet the peak workload\ndemands. The layers of the proposed fog node architecture include application\nlayer, analytics layer, virtualization layer, reconfiguration layer, and\nhardware layer. The layered architecture facilitates abstraction and\nimplementation for fog computing paradigm that is distributed in nature and\nwhere multiple vendors (e.g., applications, services, data and content\nproviders) are involved. We also elaborate the potential applications of IFCIoT\narchitecture, such as smart cities, intelligent transportation systems,\nlocalized weather maps and environmental monitoring, and real-time agricultural\ndata analytics and control.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:50:54 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Munir", "Arslan", ""], ["Kansakar", "Prasanna", ""], ["Khan", "Samee U.", ""]]}, {"id": "1701.08521", "submitter": "Peter Georg", "authors": "Peter Georg, Daniel Richtmann, Tilo Wettig", "title": "pMR: A high-performance communication library", "comments": "7 pages, 2 figures, Proceedings of Lattice 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On many parallel machines, the time LQCD applications spent in communication\nis a significant contribution to the total wall-clock time, especially in the\nstrong-scaling limit. We present a novel high-performance communication library\nthat can be used as a de facto drop-in replacement for MPI in existing\nsoftware. Its lightweight nature that avoids some of the unnecessary overhead\nintroduced by MPI allows us to improve the communication performance of\napplications without any algorithmic or complicated implementation changes. As\na first real-world benchmark, we make use of the pMR library in the coarse-grid\nsolve of the Regensburg implementation of the DD-$\\alpha$AMG algorithm. On\nrealistic lattices, we see an improvement of a factor 2x in pure communication\ntime and total execution time savings of up to 20%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 09:38:57 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Georg", "Peter", ""], ["Richtmann", "Daniel", ""], ["Wettig", "Tilo", ""]]}, {"id": "1701.08530", "submitter": "Anshu Shukla", "authors": "Anshu Shukla, Shilpa Chaturvedi and Yogesh Simmhan", "title": "RIoTBench: A Real-time IoT Benchmark for Distributed Stream Processing\n  Platforms", "comments": "33 pages. arXiv admin note: substantial text overlap with\n  arXiv:1606.07621", "journal-ref": "Concurrency and Computation: Practice and Experience, Volume 29,\n  Issue 21, 10 November 2017", "doi": "10.1002/cpe.4257", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is an emerging technology paradigm where\nmillions of sensors and actuators help monitor and manage, physical,\nenvironmental and human systems in real-time. The inherent closedloop\nresponsiveness and decision making of IoT applications make them ideal\ncandidates for using low latency and scalable stream processing platforms.\nDistributed Stream Processing Systems (DSPS) hosted on Cloud data-centers are\nbecoming the vital engine for real-time data processing and analytics in any\nIoT software architecture. But the efficacy and performance of contemporary\nDSPS have not been rigorously studied for IoT applications and data streams.\nHere, we develop RIoTBench, a Realtime IoT Benchmark suite, along with\nperformance metrics, to evaluate DSPS for streaming IoT applications. The\nbenchmark includes 27 common IoT tasks classified across various functional\ncategories and implemented as reusable micro-benchmarks. Further, we propose\nfour IoT application benchmarks composed from these tasks, and that leverage\nvarious dataflow semantics of DSPS. The applications are based on common IoT\npatterns for data pre-processing, statistical summarization and predictive\nanalytics. These are coupled with four stream workloads sourced from real IoT\nobservations on smart cities and fitness, with peak streams rates that range\nfrom 500 to 10000 messages/sec and diverse frequency distributions. We validate\nthe RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure\npublic Cloud, and present empirical observations. This suite can be used by\nDSPS researchers for performance analysis and resource scheduling, and by IoT\npractitioners to evaluate DSPS platforms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 10:13:29 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Shukla", "Anshu", ""], ["Chaturvedi", "Shilpa", ""], ["Simmhan", "Yogesh", ""]]}, {"id": "1701.08547", "submitter": "Robert Lim", "authors": "Robert V. Lim, Boyana Norris, Allen D. Malony", "title": "Autotuning GPU Kernels via Static and Predictive Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the performance of GPU kernels is challenging for both human\nprogrammers and code generators. For example, CUDA programmers must set thread\nand block parameters for a kernel, but might not have the intuition to make a\ngood choice. Similarly, compilers can generate working code, but may miss\ntuning opportunities by not targeting GPU models or performing code\ntransformations. Although empirical autotuning addresses some of these\nchallenges, it requires extensive experimentation and search for optimal code\nvariants. This research presents an approach for tuning CUDA kernels based on\nstatic analysis that considers fine-grained code structure and the specific GPU\narchitecture features. Notably, our approach does not require any program runs\nin order to discover near-optimal parameter settings. We demonstrate the\napplicability of our approach in enabling code autotuners such as Orio to\nproduce competitive code variants comparable with empirical-based methods,\nwithout the high cost of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 11:23:42 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 22:27:32 GMT"}, {"version": "v3", "created": "Thu, 29 Jun 2017 11:25:02 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Lim", "Robert V.", ""], ["Norris", "Boyana", ""], ["Malony", "Allen D.", ""]]}, {"id": "1701.08680", "submitter": "Harishchandra Dubey", "authors": "Nicholas Constant, Debanjan Borthakur, Mohammadreza Abtahi,\n  Harishchandra Dubey, Kunal Mankodiya", "title": "Fog-Assisted wIoT: A Smart Fog Gateway for End-to-End Analytics in\n  Wearable Internet of Things", "comments": "5 pages, 4 figures, The 23rd IEEE Symposium on High Performance\n  Computer Architecture HPCA 2017, (Feb. 4, 2017 - Feb. 8, 2017), Austin,\n  Texas, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, wearable internet-of-things (wIoT) devices continuously flood the\ncloud data centers at an enormous rate. This increases a demand to deploy an\nedge infrastructure for computing, intelligence, and storage close to the\nusers. The emerging paradigm of fog computing could play an important role to\nmake wIoT more efficient and affordable. Fog computing is known as the cloud on\nthe ground. This paper presents an end-to-end architecture that performs data\nconditioning and intelligent filtering for generating smart analytics from\nwearable data. In wIoT, wearable sensor devices serve on one end while the\ncloud backend offers services on the other end. We developed a prototype of\nsmart fog gateway (a middle layer) using Intel Edison and Raspberry Pi. We\ndiscussed the role of the smart fog gateway in orchestrating the process of\ndata conditioning, intelligent filtering, smart analytics, and selective\ntransfer to the cloud for long-term storage and temporal variability\nmonitoring. We benchmarked the performance of developed prototypes on\nreal-world data from smart e-textile gloves. Results demonstrated the usability\nand potential of proposed architecture for converting the real-world data into\nuseful analytics while making use of knowledge-based models. In this way, the\nsmart fog gateway enhances the end-to-end interaction between wearables (sensor\ndevices) and the cloud.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 02:52:12 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Constant", "Nicholas", ""], ["Borthakur", "Debanjan", ""], ["Abtahi", "Mohammadreza", ""], ["Dubey", "Harishchandra", ""], ["Mankodiya", "Kunal", ""]]}, {"id": "1701.08800", "submitter": "Aravind Vasudevan", "authors": "Aravind Vasudevan and David Gregg", "title": "Mutual Inclusivity of the Critical Path and its Partial Schedule on\n  Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The critical path of a group of tasks is an important measure that is\ncommonly used to guide task allocation and scheduling on parallel computers.\nThe critical path is the longest chain of dependencies in an acyclic task\ndependence graph. A problem arises on heterogeneous parallel machines where\ncomputation and communication costs can vary between different types of\nprocessor. Existing solutions for heterogeneous machines attempt to estimate\nthe critical path using average values of computation and communication costs.\nHowever, this ignores opportunities to match specific tasks to specific classes\nof processor and communication links, and can result in quite misleading paths\nbeing identified as critical. We argue that an accurate critical path must\nconsider the mapping of tasks to classes of processor and communication links.\nWe formulate a polynomial time algorithm to find such a critical path. Our\nCritical Earliest Finish Time (CEFT) algorithm finds both the length of the\ncritical path and an allocation of tasks to processors on that path. We\ncompared CEFT experimentally to existing approaches such as averaging execution\ntimes across processors. The latter approach fails to accurately model the\nexecution cost of tasks, and as a result fails to identify a correct critical\npath in 83.99% of cases in our experiments. We also adapted a critical\npath-oriented scheduling algorithm (CPOP) to use our critical path algorithm\nand found that the resulting schedules are faster.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 19:54:01 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Vasudevan", "Aravind", ""], ["Gregg", "David", ""]]}, {"id": "1701.08920", "submitter": "William Pettersson", "authors": "William Pettersson and Melih Ozlen", "title": "A parallel approach to bi-objective integer programming", "comments": "7 pages", "journal-ref": null, "doi": "10.21914/anziamj.v58i0.11724", "report-no": null, "categories": "math.OC cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain a better understanding of the trade-offs between various\nobjectives, Bi-Objective Integer Programming (BOIP) algorithms calculate the\nset of all non-dominated vectors and present these as the solution to a BOIP\nproblem. Historically, these algorithms have been compared in terms of the\nnumber of single-objective IPs solved and total CPU time taken to produce the\nsolution to a problem. This is equitable, as researchers can often have access\nto widely differing amounts of computing power. However, the real world has\nrecently seen a large uptake of multi-core processors in computers, laptops,\ntablets and even mobile phones. With this in mind, we look at how to best\nutilise parallel processing to improve the elapsed time of optimisation\nalgorithms. We present two methods of parallelising the recursive algorithm\npresented by Ozlen, Burton and MacRae. Both new methods utilise two threads and\nimprove running times. One of the new methods, the Meeting algorithm, halves\nrunning time to achieve near-perfect parallelisation. The results are compared\nwith the efficiency of parallelisation within the commercial IP solver IBM ILOG\nCPLEX, and the new methods are both shown to perform better.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 05:28:25 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Pettersson", "William", ""], ["Ozlen", "Melih", ""]]}]