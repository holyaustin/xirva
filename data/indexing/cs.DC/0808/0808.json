[{"id": "0808.0920", "submitter": "Mahesh Arumugam", "authors": "Mahesh Arumugam", "title": "A Distributed and Deterministic TDMA Algorithm for\n  Write-All-With-Collision Model", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several self-stabilizing time division multiple access (TDMA) algorithms are\nproposed for sensor networks. In addition to providing a collision-free\ncommunication service, such algorithms enable the transformation of programs\nwritten in abstract models considered in distributed computing literature into\na model consistent with sensor networks, i.e., write all with collision (WAC)\nmodel. Existing TDMA slot assignment algorithms have one or more of the\nfollowing properties: (i) compute slots using a randomized algorithm, (ii)\nassume that the topology is known upfront, and/or (iii) assign slots\nsequentially. If these algorithms are used to transform abstract programs into\nprograms in WAC model then the transformed programs are probabilistically\ncorrect, do not allow the addition of new nodes, and/or converge in a\nsequential fashion. In this paper, we propose a self-stabilizing deterministic\nTDMA algorithm where a sensor is aware of only its neighbors. We show that the\nslots are assigned to the sensors in a concurrent fashion and starting from\narbitrary initial states, the algorithm converges to states where\ncollision-free communication among the sensors is restored. Moreover, this\nalgorithm facilitates the transformation of abstract programs into programs in\nWAC model that are deterministically correct.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2008 20:33:56 GMT"}], "update_date": "2008-08-08", "authors_parsed": [["Arumugam", "Mahesh", ""]]}, {"id": "0808.0962", "submitter": "Amin Ansari", "authors": "Amin Ansari", "title": "Verification of Peterson's Algorithm for Leader Election in a\n  Unidirectional Asynchronous Ring Using NuSMV", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite intrinsic nature of the most distributed algorithms gives us this\nability to use model checking tools for verification of this type of\nalgorithms. In this paper, I attempt to use NuSMV as a model checking tool for\nverifying necessary properties of Peterson's algorithm for leader election\nproblem in a unidirectional asynchronous ring topology. Peterson's algorithm\nfor an asynchronous ring supposes that each node in the ring has a unique ID\nand also a queue for dealing with storage problem. By considering that the\nqueue can have any combination of values, a constructed model for a ring with\nonly four nodes will have more than a billion states. Although it seems that\nmodel checking is not a feasible approach for this problem, I attempt to use\nseveral effective limiting assumptions for hiring formal model checking\napproach without losing the correct functionality of the Peterson's algorithm.\nThese enforced limiting assumptions target the degree of freedom in the model\nchecking process and significantly decrease the CPU time, memory usage and the\ntotal number of page faults. By deploying these limitations, the number of\nnodes can be increased from four to eight in the model checking process with\nNuSMV.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2008 06:02:59 GMT"}], "update_date": "2008-08-08", "authors_parsed": [["Ansari", "Amin", ""]]}, {"id": "0808.1431", "submitter": "Neil J. Gunther", "authors": "Neil J. Gunther", "title": "A General Theory of Computational Scalability Based on Rational\n  Functions", "comments": "14 pages, 5 figures; several typos corrected, 1 reference updated,\n  page number reduced with 10 pt font", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The universal scalability law of computational capacity is a rational\nfunction C_p = P(p)/Q(p) with P(p) a linear polynomial and Q(p) a second-degree\npolynomial in the number of physical processors p, that has been long used for\nstatistical modeling and prediction of computer system performance. We prove\nthat C_p is equivalent to the synchronous throughput bound for a\nmachine-repairman with state-dependent service rate. Simpler rational\nfunctions, such as Amdahl's law and Gustafson speedup, are corollaries of this\nqueue-theoretic bound. C_p is further shown to be both necessary and sufficient\nfor modeling all practical characteristics of computational scalability.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2008 00:06:16 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2008 16:20:42 GMT"}], "update_date": "2008-08-25", "authors_parsed": [["Gunther", "Neil J.", ""]]}, {"id": "0808.1505", "submitter": "Joseph Y. Halpern", "authors": "Ittai Abraham, Danny Dolev, and Joseph Y. Halpern", "title": "An Almost-Surely Terminating Polynomial Protocol for Asynchronous\n  Byzantine Agreement with Optimal Resilience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an asynchronous system with private channels and $n$ processes, up\nto $t$ of which may be faulty. We settle a longstanding open question by\nproviding a Byzantine agreement protocol that simultaneously achieves three\nproperties:\n  1. (optimal) resilience: it works as long as $n>3t$\n  2. (almost-sure) termination: with probability one, all nonfaulty processes\nterminate\n  3. (polynomial) efficiency: the expected computation time, memory\nconsumption, message size, and number of messages sent are all polynomial in\n$n$.\n  Earlier protocols have achieved only two of these three properties. In\nparticular, the protocol of Bracha is not polynomially efficient, the protocol\nof Feldman and Micali is not optimally resilient, and the protocol of Canetti\nand Rabin does not have almost-sure termination. Our protocol utilizes a new\nprimitive called shunning (asynchronous) verifiable secret sharing (SVSS),\nwhich ensures, roughly speaking, that either a secret is successfully shared or\na new faulty process is ignored from this point onwards by some nonfaulty\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 11 Aug 2008 12:22:12 GMT"}], "update_date": "2008-08-12", "authors_parsed": [["Abraham", "Ittai", ""], ["Dolev", "Danny", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "0808.1744", "submitter": "Alex Brodsky", "authors": "Alex Brodsky and Scott Lindenberg", "title": "Our Brothers' Keepers: Secure Routing with High Performance", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Trinity (Brodsky et al., 2007) spam classification system is based on a\ndistributed hash table that is implemented using a structured peer-to-peer\noverlay. Such an overlay must be capable of processing hundreds of messages per\nsecond, and must be able to route messages to their destination even in the\npresence of failures and malicious peers that misroute packets or inject\nfraudulent routing information into the system. Typically there is tension\nbetween the requirements to route messages securely and efficiently in the\noverlay.\n  We describe a secure and efficient routing extension that we developed within\nthe I3 (Stoica et al. 2004) implementation of the Chord (Stoica et al. 2001)\noverlay. Secure routing is accomplished through several complementary\napproaches: First, peers in close proximity form overlapping groups that police\nthemselves to identify and mitigate fraudulent routing information. Second, a\nform of random routing solves the problem of entire packet flows passing\nthrough a malicious peer. Third, a message authentication mechanism links each\nmessage to it sender, preventing spoofing. Fourth, each peer's identifier links\nthe peer to its network address, and at the same time uniformly distributes the\npeers in the key-space.\n  Lastly, we present our initial evaluation of the system, comprising a 255\npeer overlay running on a local cluster. We describe our methodology and show\nthat the overhead of our secure implementation is quite reasonable.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2008 22:03:52 GMT"}], "update_date": "2008-08-14", "authors_parsed": [["Brodsky", "Alex", ""], ["Lindenberg", "Scott", ""]]}, {"id": "0808.1802", "submitter": "Robert Grossman", "authors": "Robert L. Grossman, Yunhong Gu, Michael Sabala and Wanzhi Zhang", "title": "Compute and Storage Clouds Using Wide Area High Performance Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a cloud based infrastructure that we have developed that is\noptimized for wide area, high performance networks and designed to support data\nmining applications. The infrastructure consists of a storage cloud called\nSector and a compute cloud called Sphere. We describe two applications that we\nhave built using the cloud and some experimental studies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2008 09:48:37 GMT"}], "update_date": "2008-08-14", "authors_parsed": [["Grossman", "Robert L.", ""], ["Gu", "Yunhong", ""], ["Sabala", "Michael", ""], ["Zhang", "Wanzhi", ""]]}, {"id": "0808.3019", "submitter": "Robert Grossman", "authors": "Robert L Grossman and Yunhong Gu", "title": "Data Mining Using High Performance Data Clouds: Experimental Studies\n  Using Sector and Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the design and implementation of a high performance cloud that we\nhave used to archive, analyze and mine large distributed data sets. By a cloud,\nwe mean an infrastructure that provides resources and/or services over the\nInternet. A storage cloud provides storage services, while a compute cloud\nprovides compute services. We describe the design of the Sector storage cloud\nand how it provides the storage services required by the Sphere compute cloud.\nWe also describe the programming paradigm supported by the Sphere compute\ncloud. Sector and Sphere are designed for analyzing large data sets using\ncomputer clusters connected with wide area high performance networks (for\nexample, 10+ Gb/s). We describe a distributed data mining application that we\nhave developed using Sector and Sphere. Finally, we describe some experimental\nstudies comparing Sector/Sphere to Hadoop.\n", "versions": [{"version": "v1", "created": "Fri, 22 Aug 2008 01:24:06 GMT"}], "update_date": "2008-08-25", "authors_parsed": [["Grossman", "Robert L", ""], ["Gu", "Yunhong", ""]]}, {"id": "0808.3535", "submitter": "Ioan Raicu", "authors": "Ioan Raicu, Yong Zhao, Ian Foster, Alex Szalay", "title": "Data Diffusion: Dynamic Resource Provision and Data-Aware Scheduling for\n  Data Intensive Applications", "comments": "16 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data intensive applications often involve the analysis of large datasets that\nrequire large amounts of compute and storage resources. While dedicated compute\nand/or storage farms offer good task/data throughput, they suffer low resource\nutilization problem under varying workloads conditions. If we instead move such\ndata to distributed computing resources, then we incur expensive data transfer\ncost. In this paper, we propose a data diffusion approach that combines dynamic\nresource provisioning, on-demand data replication and caching, and data\nlocality-aware scheduling to achieve improved resource efficiency under varying\nworkloads. We define an abstract \"data diffusion model\" that takes into\nconsideration the workload characteristics, data accessing cost, application\nthroughput and resource utilization; we validate the model using a real-world\nlarge-scale astronomy application. Our results show that data diffusion can\nincrease the performance index by as much as 34X, and improve application\nresponse time by over 506X, while achieving near-optimal throughputs and\nexecution times.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 15:19:44 GMT"}], "update_date": "2008-08-27", "authors_parsed": [["Raicu", "Ioan", ""], ["Zhao", "Yong", ""], ["Foster", "Ian", ""], ["Szalay", "Alex", ""]]}, {"id": "0808.3536", "submitter": "Ioan Raicu", "authors": "Ioan Raicu, Zhao Zhang, Mike Wilde, Ian Foster", "title": "Enabling Loosely-Coupled Serial Job Execution on the IBM BlueGene/P\n  Supercomputer and the SiCortex SC5832", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work addresses the enabling of the execution of highly parallel\ncomputations composed of loosely coupled serial jobs with no modifications to\nthe respective applications, on large-scale systems. This approach allows\nnew-and potentially far larger-classes of application to leverage systems such\nas the IBM Blue Gene/P supercomputer and similar emerging petascale\narchitectures. We present here the challenges of I/O performance encountered in\nmaking this model practical, and show results using both micro-benchmarks and\nreal applications on two large-scale systems, the BG/P and the SiCortex SC5832.\nOur preliminary benchmarks show that we can scale to 4096 processors on the\nBlue Gene/P and 5832 processors on the SiCortex with high efficiency, and can\nachieve thousands of tasks/sec sustained execution rates for parallel workloads\nof ordinary serial applications. We measured applications from two domains,\neconomic energy modeling and molecular dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 16:59:41 GMT"}], "update_date": "2008-08-27", "authors_parsed": [["Raicu", "Ioan", ""], ["Zhang", "Zhao", ""], ["Wilde", "Mike", ""], ["Foster", "Ian", ""]]}, {"id": "0808.3540", "submitter": "Ioan Raicu", "authors": "Ioan Raicu, Zhao Zhang, Mike Wilde, Ian Foster, Pete Beckman, Kamil\n  Iskra, Ben Clifford", "title": "Towards Loosely-Coupled Programming on Petascale Systems", "comments": "IEEE/ACM International Conference for High Performance Computing,\n  Networking, Storage and Analysis (SuperComputing/SC) 2008", "journal-ref": null, "doi": "10.1109/SC.2008.5219768", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have extended the Falkon lightweight task execution framework to make\nloosely coupled programming on petascale systems a practical and useful\nprogramming model. This work studies and measures the performance factors\ninvolved in applying this approach to enable the use of petascale systems by a\nbroader user community, and with greater ease. Our work enables the execution\nof highly parallel computations composed of loosely coupled serial jobs with no\nmodifications to the respective applications. This approach allows a new-and\npotentially far larger-class of applications to leverage petascale systems,\nsuch as the IBM Blue Gene/P supercomputer. We present the challenges of I/O\nperformance encountered in making this model practical, and show results using\nboth microbenchmarks and real applications from two domains: economic energy\nmodeling and molecular dynamics. Our benchmarks show that we can scale up to\n160K processor-cores with high efficiency, and can achieve sustained execution\nrates of thousands of tasks per second.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 16:48:14 GMT"}, {"version": "v2", "created": "Wed, 27 Aug 2008 18:05:46 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Raicu", "Ioan", ""], ["Zhang", "Zhao", ""], ["Wilde", "Mike", ""], ["Foster", "Ian", ""], ["Beckman", "Pete", ""], ["Iskra", "Kamil", ""], ["Clifford", "Ben", ""]]}, {"id": "0808.3545", "submitter": "Ioan Raicu", "authors": "Yong Zhao, Ioan Raicu, Ian Foster", "title": "Scientific Workflow Systems for 21st Century e-Science, New Bottle or\n  New Wine?", "comments": "IEEE Workshop on Scientific Workflows 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in e-Sciences and the growing complexity of scientific\nanalyses, more and more scientists and researchers are relying on workflow\nsystems for process coordination, derivation automation, provenance tracking,\nand bookkeeping. While workflow systems have been in use for decades, it is\nunclear whether scientific workflows can or even should build on existing\nworkflow technologies, or they require fundamentally new approaches. In this\npaper, we analyze the status and challenges of scientific workflows,\ninvestigate both existing technologies and emerging languages, platforms and\nsystems, and identify the key challenges that must be addressed by workflow\nsystems for e-science in the 21st century.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 16:46:49 GMT"}], "update_date": "2008-08-27", "authors_parsed": [["Zhao", "Yong", ""], ["Raicu", "Ioan", ""], ["Foster", "Ian", ""]]}, {"id": "0808.3546", "submitter": "Ioan Raicu", "authors": "Ioan Raicu, Yong Zhao, Ian Foster, Alex Szalay", "title": "Accelerating Large-scale Data Exploration through Data Diffusion", "comments": "IEEE/ACM International Workshop on Data-Aware Distributed Computing\n  2008", "journal-ref": null, "doi": "10.1145/1383519.1383521", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive applications often require exploratory analysis of large\ndatasets. If analysis is performed on distributed resources, data locality can\nbe crucial to high throughput and performance. We propose a \"data diffusion\"\napproach that acquires compute and storage resources dynamically, replicates\ndata in response to demand, and schedules computations close to data. As demand\nincreases, more resources are acquired, thus allowing faster response to\nsubsequent requests that refer to the same data; when demand drops, resources\nare released. This approach can provide the benefits of dedicated hardware\nwithout the associated high costs, depending on workload and resource\ncharacteristics. The approach is reminiscent of cooperative caching,\nweb-caching, and peer-to-peer storage systems, but addresses different\napplication demands. Other data-aware scheduling approaches assume dedicated\nresources, which can be expensive and/or inefficient if load varies\nsignificantly. To explore the feasibility of the data diffusion approach, we\nhave extended the Falkon resource provisioning and task scheduling system to\nsupport data caching and data-aware scheduling. Performance results from both\nmicro-benchmarks and a large scale astronomy application demonstrate that our\napproach improves performance relative to alternative approaches, as well as\nprovides improved scalability as aggregated I/O bandwidth scales linearly with\nthe number of data cache nodes.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 16:02:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Raicu", "Ioan", ""], ["Zhao", "Yong", ""], ["Foster", "Ian", ""], ["Szalay", "Alex", ""]]}, {"id": "0808.3548", "submitter": "Ioan Raicu", "authors": "Yong Zhao, Ioan Raicu, Ian Foster, Mihael Hategan, Veronika Nefedova,\n  Mike Wilde", "title": "Realizing Fast, Scalable and Reliable Scientific Computations in Grid\n  Environments", "comments": "Book chapter in Grid Computing Research Progress, ISBN:\n  978-1-60456-404-4, Nova Publisher 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical realization of managing and executing large scale scientific\ncomputations efficiently and reliably is quite challenging. Scientific\ncomputations often involve thousands or even millions of tasks operating on\nlarge quantities of data, such data are often diversely structured and stored\nin heterogeneous physical formats, and scientists must specify and run such\ncomputations over extended periods on collections of compute, storage and\nnetwork resources that are heterogeneous, distributed and may change\nconstantly. We present the integration of several advanced systems: Swift,\nKarajan, and Falkon, to address the challenges in running various large scale\nscientific applications in Grid environments. Swift is a parallel programming\ntool for rapid and reliable specification, execution, and management of\nlarge-scale science and engineering workflows. Swift consists of a simple\nscripting language called SwiftScript and a powerful runtime system that is\nbased on the CoG Karajan workflow engine and integrates the Falkon light-weight\ntask execution service that uses multi-level scheduling and a streamlined\ndispatcher. We showcase the scalability, performance and reliability of the\nintegrated system using application examples drawn from astronomy, cognitive\nneuroscience and molecular dynamics, which all comprise large number of\nfine-grained jobs. We show that Swift is able to represent dynamic workflows\nwhose structures can only be determined during runtime and reduce largely the\ncode size of various workflow representations using SwiftScript; schedule the\nexecution of hundreds of thousands of parallel computations via the Karajan\nengine; and achieve up to 90% reduction in execution time when compared to\ntraditional batch schedulers.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 16:15:42 GMT"}], "update_date": "2008-08-27", "authors_parsed": [["Zhao", "Yong", ""], ["Raicu", "Ioan", ""], ["Foster", "Ian", ""], ["Hategan", "Mihael", ""], ["Nefedova", "Veronika", ""], ["Wilde", "Mike", ""]]}, {"id": "0808.3558", "submitter": "Rajkumar Buyya", "authors": "Rajkumar Buyya, Chee Shin Yeo, and Srikumar Venugopal", "title": "Market-Oriented Cloud Computing: Vision, Hype, and Reality for\n  Delivering IT Services as Computing Utilities", "comments": "9 pages; GRIDS Lab Technical Report, Aug 2008", "journal-ref": "Proceedings of the 10th IEEE International Conference on High\n  Performance Computing and Communications (HPCC-08, IEEE CS Press, Los\n  Alamitos, CA, USA), Sept. 25-27, 2008, Dalian, China", "doi": "10.1109/HPCC.2008.172", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This keynote paper: presents a 21st century vision of computing; identifies\nvarious computing paradigms promising to deliver the vision of computing\nutilities; defines Cloud computing and provides the architecture for creating\nmarket-oriented Clouds by leveraging technologies such as VMs; provides\nthoughts on market-based resource management strategies that encompass both\ncustomer-driven service management and computational risk management to sustain\nSLA-oriented resource allocation; presents some representative Cloud platforms\nespecially those developed in industries along with our current work towards\nrealising market-oriented resource allocation of Clouds by leveraging the 3rd\ngeneration Aneka enterprise Grid technology; reveals our early thoughts on\ninterconnecting Clouds for dynamically creating an atmospheric computing\nenvironment along with pointers to future community research; and concludes\nwith the need for convergence of competing IT paradigms for delivering our 21st\ncentury vision.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2008 17:16:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Buyya", "Rajkumar", ""], ["Yeo", "Chee Shin", ""], ["Venugopal", "Srikumar", ""]]}, {"id": "0808.3693", "submitter": "Xavier Grehant", "authors": "Xavier Grehant and J.M. Dana", "title": "Providing Virtual Execution Environments: A Twofold Illustration", "comments": "openlab Technical Documents and Publications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Platform virtualization helps solving major grid computing challenges: share\nresource with flexible, user-controlled and custom execution environments and\nin the meanwhile, isolate failures and malicious code. Grid resource management\ntools will evolve to embrace support for virtual resource.\n  We present two open source projects that transparently supply virtual\nexecution environments. Tycoon has been developed at HP Labs to optimise\nresource usage in creating an economy where users bid to access virtual\nmachines and compete for CPU cycles. SmartDomains provides a peer-to-peer layer\nthat automates virtual machines deployment using a description language and\ndeployment engine from HP Labs. These projects demonstrate both client-server\nand peer-to-peer approaches to virtual resource management. The first case\nmakes extensive use of virtual machines features for dynamic resource\nallocation. The second translates virtual machines capabilities into a\nsophisticated language where resource management components can be plugged in\nconfigurations and architectures defined at deployment time.\n  We propose to share our experience at CERN openlab developing SmartDomains\nand deploying Tycoon to give an illustrative introduction to emerging research\nin virtual resource management.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2008 12:48:39 GMT"}], "update_date": "2008-08-28", "authors_parsed": [["Grehant", "Xavier", ""], ["Dana", "J. M.", ""]]}]