[{"id": "2007.00156", "submitter": "Saeed Rashidi", "authors": "Saeed Rashidi, Srinivas Sridharan, Sudarshan Srinivasan, Matthew\n  Denton, Tushar Krishna", "title": "Efficient Communication Acceleration for Next-Gen Scale-up Deep Learning\n  Training Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) training platforms are built by interconnecting multiple\nDL accelerators (e.g., GPU/TPU) via fast, customized interconnects. As the size\nof DL models and the compute efficiency of the accelerators has continued to\nincrease, there has also been a corresponding steady increase in the bandwidth\nof these interconnects.Systems today provide 100s of gigabytes (GBs) of\ninter-connect bandwidth via a mix of solutions such as Multi-Chip packaging\nmodules (MCM) and proprietary interconnects(e.g., NVlink) that together from\nthe scale-up network of accelerators. However, as we identify in this work, a\nsignificant portion of this bandwidth goes under-utilized. This is because(i)\nusing compute cores for executing collective operations such as all-reduce\ndecreases overall compute efficiency, and(ii) there is memory bandwidth\ncontention between the accesses for arithmetic operations vs those for\ncollectives, and(iii) there are significant internal bus congestions that\nincrease the latency of communication operations. To address this challenge, we\npropose a novel microarchitecture, calledAccelerator Collectives Engine(ACE),\nforDL collective communication offload. ACE is a smart net-work interface (NIC)\ntuned to cope with the high-bandwidth and low latency requirements of scale-up\nnetworks and is able to efficiently drive the various scale-up network\nsystems(e.g. switch-based or point-to-point topologies). We evaluate the\nbenefits of the ACE with micro-benchmarks (e.g. single collective performance)\nand popular DL models using an end-to-end DL training simulator. For modern DL\nworkloads, ACE on average increases the net-work bandwidth utilization by\n1.97X, resulting in 2.71X and 1.44X speedup in iteration time for ResNet-50 and\nGNMT, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 23:56:41 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 01:31:50 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:04:50 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Rashidi", "Saeed", ""], ["Sridharan", "Srinivas", ""], ["Srinivasan", "Sudarshan", ""], ["Denton", "Matthew", ""], ["Krishna", "Tushar", ""]]}, {"id": "2007.00186", "submitter": "Mohammad Jalalzai", "authors": "Mohammad M. Jalalzai, Chen Feng, Costas Busch, Golden G. Richard III,\n  Jianyu Niu", "title": "The Hermes BFT for Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of partially synchronous BFT-based consensus protocols is\nhighly dependent on the primary node. All participant nodes in the network are\nblocked until they receive a proposal from the primary node to begin the\nconsensus process.Therefore, an honest but slack node (with limited bandwidth)\ncan adversely affect the performance when selected as primary. Hermes decreases\nprotocol dependency on the primary node and minimizes transmission delay\ninduced by the slack primary while keeping low message complexity and latency.\nHermes achieves these performance improvements by relaxing strong BFT agreement\n(safety) guarantees only for a specific type of Byzantine faults (also called\nequivocated faults). Interestingly, we show that in Hermes equivocating by a\nByzantine primary is unlikely, expensive and ineffective. Therefore, the safety\nof Hermes is comparable to the general BFT consensus. We deployed and tested\nHermes on 190 Amazon EC2 instances. In these tests, Hermes's performance was\ncomparable to the state-of-the-art BFT protocol for blockchains (when the\nnetwork size is large) in the absence of slack nodes. Whereas, in the presence\nof slack nodes Hermes out performed the state-of-the-art BFT protocol by more\nthan 4x in terms of throughput as well as 15x in terms of latency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 02:22:30 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Jalalzai", "Mohammad M.", ""], ["Feng", "Chen", ""], ["Busch", "Costas", ""], ["Richard", "Golden G.", "III"], ["Niu", "Jianyu", ""]]}, {"id": "2007.00232", "submitter": "Xiaorui Liu", "authors": "Xiaorui Liu, Yao Li, Rongrong Wang, Jiliang Tang, Ming Yan", "title": "Linear Convergent Decentralized Optimization with Compression", "comments": "ICLR 2021 (International Conference on Learning Representations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication compression has become a key strategy to speed up distributed\noptimization. However, existing decentralized algorithms with compression\nmainly focus on compressing DGD-type algorithms. They are unsatisfactory in\nterms of convergence rate, stability, and the capability to handle\nheterogeneous data. Motivated by primal-dual algorithms, this paper proposes\nthe first \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, LEAD. Our theory describes the coupled dynamics of\nthe inexact primal and dual update as well as compression error, and we provide\nthe first consensus error bound in such settings without assuming bounded\ngradients. Experiments on convex problems validate our theoretical analysis,\nand empirical study on deep neural nets shows that LEAD is applicable to\nnon-convex problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 04:35:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 20:46:05 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Xiaorui", ""], ["Li", "Yao", ""], ["Wang", "Rongrong", ""], ["Tang", "Jiliang", ""], ["Yan", "Ming", ""]]}, {"id": "2007.00324", "submitter": "Zhenghai Chen", "authors": "Zhenghai Chen, Tiow-Seng Tan and Hong-Yang Ong", "title": "On Designing GPU Algorithms with Applications to Mesh Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of rules to guide the design of GPU algorithms. These rules\nare grounded on the principle of reducing waste in GPU utility to achieve good\nspeed up. In accordance to these rules, we propose GPU algorithms for 2D\nconstrained, 3D constrained and 3D Restricted Delaunay refinement problems\nrespectively. Our algorithms take a 2D planar straight line graph (PSLG) or 3D\npiecewise linear complex (PLC) $\\mathcal{G}$ as input, and generate quality\nmeshes conforming or approximating to $\\mathcal{G}$. The implementation of our\nalgorithms shows that they are the first to run an order of magnitude faster\nthan current state-of-the-art counterparts in sequential and parallel manners\nwhile using similar numbers of Steiner points to produce triangulations of\ncomparable qualities. It thus reduces the computing time of mesh refinement\nfrom possibly hours to a few seconds or minutes for possible use in interactive\ngraphics applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 08:43:12 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Zhenghai", ""], ["Tan", "Tiow-Seng", ""], ["Ong", "Hong-Yang", ""]]}, {"id": "2007.00415", "submitter": "Quinten Stokkink", "authors": "Quinten Stokkink, Dick Epema and Johan Pouwelse", "title": "A Truly Self-Sovereign Identity System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital identity is essential to access services such as: online banking,\nincome tax portals, and online higher education. Digital identity is often\noutsourced to central digital identity providers, introducing a critical\ndependency. Self-Sovereign Identity gives citizens the ownership back of their\nown identity. However, proposed solutions concentrate on data disclosure\nprotocols and are unable to produce identity with legal status. We identify how\nrelated work attempts to legalize identity by reintroducing centralization and\ndisregards common attacks on peer-to-peer interactions, missing out on the\nstrong privacy guarantees offered by the data disclosure protocols. To address\nthis problem we present IPv8, a complete system for passport-grade\nSelf-Sovereign Identity. Our design consists of a hierarchy of middleware\nlayers which are minimally required to establish legal viability. IPv8 is\ncomprised of a peer-to-peer middleware stack with Sybil attack resilience and\nstrong privacy through onion routing. No other work has offered an operational\nprototype of an academically pure identity solution without any trusted third\nparties, critical external services, or any server in general.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:14:04 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Stokkink", "Quinten", ""], ["Epema", "Dick", ""], ["Pouwelse", "Johan", ""]]}, {"id": "2007.00433", "submitter": "Xiang Yang", "authors": "Xiang Yang", "title": "Shuffle-Exchange Brings Faster: Reduce the Idle Time During\n  Communication for Decentralized Neural Network Training", "comments": "NeurIPS 2020 Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial scheme to accelerate the deep neural network (DNN) training,\ndistributed stochastic gradient descent (DSGD) is widely adopted in many\nreal-world applications. In most distributed deep learning (DL) frameworks,\nDSGD is implemented with Ring-AllReduce architecture (Ring-SGD) and uses a\ncomputation-communication overlap strategy to address the overhead of the\nmassive communications required by DSGD. However, we observe that although\n$O(1)$ gradients are needed to be communicated per worker in Ring-SGD, the\n$O(n)$ handshakes required by Ring-SGD limits its usage when training with many\nworkers or in high latency network. In this paper, we propose Shuffle-Exchange\nSGD (SESGD) to solve the dilemma of Ring-SGD. In the cluster of 16 workers with\n0.1ms Ethernet latency, SESGD can accelerate the DNN training to $1.7 \\times$\nwithout losing model accuracy. Moreover, the process can be accelerated up to\n$5\\times$ in high latency networks (5ms).\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 12:38:48 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 03:24:11 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Yang", "Xiang", ""]]}, {"id": "2007.00625", "submitter": "Matthew Connor", "authors": "Matthew Connor, Othon Michail, Paul Spirakis", "title": "On the Distributed Construction of Stable Networks in Polylogarithmic\n  Parallel Time", "comments": "19 Pages 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the class of networks which can be created in polylogarithmic\nparallel time by network constructors: groups of anonymous agents that interact\nrandomly under a uniform random scheduler with the ability to form connections\nbetween each other. Starting from an empty network, the goal is to construct a\nstable network which belongs to a given family. We prove that the class of\ntrees where each node has any k >= 2 children can be constructed in O(log n)\nparallel time with high probability. We show that constructing networks which\nare k-regular is Omega(n) time, but a minimal relaxation to (l, k)-regular\nnetworks, where l = k - 1 can be constructed in polylogarithmic parallel time\nfor any fixed k, where k > 2. We further demonstrate that when the finite-state\nassumption is relaxed and k is allowed to grow with n, then k = log log n acts\nas a threshold above which network construction is again polynomial time. We\nuse this to provide a partial characterisation of the class of polylogarithmic\ntime network constructors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:21:39 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Connor", "Matthew", ""], ["Michail", "Othon", ""], ["Spirakis", "Paul", ""]]}, {"id": "2007.00745", "submitter": "Bhanuka Manesha Samarasekara Vitharana Gamage", "authors": "Bhanuka Manesha Samarasekara Vitharana Gamage and Vishnu Monn Baskaran", "title": "Efficient Generation of Mandelbrot Set using Message Passing Interface", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing need for safer and reliable systems, Mandelbrot Set's use\nin the encryption world is evident to everyone. This document aims to provide\nan efficient method to generate this set using data parallelism. First\nBernstein's conditions are used to ensure that the Data is parallelizable when\ngenerating the Mandelbrot Set. Then Amdhal's Law is used to calculate the\ntheoretical speed up, to be used to compare three partition schemes. The three\npartition schemes discussed in this document are the Na\\\"ive Row Segmentation,\nthe First Come First Served Row Segmentation and the Alternating Row\nSegmentation. The Message Parsing Interface (MPI) library in C is used for all\nof the communication. After testing all the implementation on MonARCH, the\nresults demonstrate that the Na\\\"ive Row Segmentation approach did not perform\nas par. But the Alternating Row Segmentation approach performs better when the\nnumber of tasks are $< 16$, where as the First Come First Served approach\nperforms better when the number of tasks is $\\ge 16$.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:45:23 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gamage", "Bhanuka Manesha Samarasekara Vitharana", ""], ["Baskaran", "Vishnu Monn", ""]]}, {"id": "2007.00754", "submitter": "Bhanuka Manesha Samarasekara Vitharana Gamage", "authors": "Bhanuka Manesha Samarasekara Vitharana Gamage and Vishnu Monn Baskaran", "title": "Simulation and Analysis of Distributed Wireless Sensor Network using\n  Message Passing Interface", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Networks (WSN) are used by many industries from environment\nmonitoring systems to NASA's space exploration programs, as it has allowed\nsociety to monitor and prevent problems before they occur with less cost and\nmaintenance. This document aims to propose and analyze an efficient inter\nprocess communication (IPC) architecture using a nearest neighbor/grid based\nsocket architecture. A parallelized version of the AES encryption algorithm is\nalso used in order to increase the security of the WSN. First the proposed\narchitecture is compared and contrasted against other well established\narchitectures. Next, the benefits and drawbacks of the AES encryption algorithm\nis elucidated. The Message Parsing Interface (MPI) library in C is used for the\ncommunication while OpenMP is used for parallelizing the encryption algorithm.\nNext an analysis is performed on the results obtained from multiple\nsimulations. Finally a conclusion is made that the grid based IPC architecture\nwith AES parallel encryption helps WSNs maintain security in communication\nwhile being cost and power efficient to operate.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 21:02:10 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gamage", "Bhanuka Manesha Samarasekara Vitharana", ""], ["Baskaran", "Vishnu Monn", ""]]}, {"id": "2007.00784", "submitter": "J. Gregory Pauloski", "authors": "J. Gregory Pauloski, Zhao Zhang, Lei Huang, Weijia Xu and Ian T.\n  Foster", "title": "Convolutional Neural Network Training with Distributed K-FAC", "comments": "To be published in the proceedings of the International Conference\n  for High Performance Computing, Networking, Storage and Analysis (SC20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training neural networks with many processors can reduce time-to-solution;\nhowever, it is challenging to maintain convergence and efficiency at large\nscales. The Kronecker-factored Approximate Curvature (K-FAC) was recently\nproposed as an approximation of the Fisher Information Matrix that can be used\nin natural gradient optimizers. We investigate here a scalable K-FAC design and\nits applicability in convolutional neural network (CNN) training at scale. We\nstudy optimization techniques such as layer-wise distribution strategies,\ninverse-free second-order gradient evaluation, and dynamic K-FAC update\ndecoupling to reduce training time while preserving convergence. We use\nresidual neural networks (ResNet) applied to the CIFAR-10 and ImageNet-1k\ndatasets to evaluate the correctness and scalability of our K-FAC gradient\npreconditioner. With ResNet-50 on the ImageNet-1k dataset, our distributed\nK-FAC implementation converges to the 75.9% MLPerf baseline in 18-25% less time\nthan does the classic stochastic gradient descent (SGD) optimizer across scales\non a GPU cluster.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 22:00:53 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Pauloski", "J. Gregory", ""], ["Zhang", "Zhao", ""], ["Huang", "Lei", ""], ["Xu", "Weijia", ""], ["Foster", "Ian T.", ""]]}, {"id": "2007.00840", "submitter": "Anil Gaihre", "authors": "Anil Gaihre, Xiaoye S. Li, Hang Liu", "title": "GSoFa: Scalable Sparse Symbolic LU Factorization on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing matrix A into a lower matrix L and an upper matrix U, which is\nalso known as LU decomposition, is an essential operation in numerical linear\nalgebra. For a sparse matrix, LU decomposition often introduces more nonzero\nentries in the L and U factors than in the original matrix. A symbolic\nfactorization step is needed to identify the nonzero structures of L and U\nmatrices. Attracted by the enormous potentials of the Graphics Processing Units\n(GPUs), an array of efforts have surged to deploy various LU factorization\nsteps except for the symbolic factorization, to the best of our knowledge, on\nGPUs. This paper introduces gSoFa, the first GPU-based Symbolic factorization\ndesign with the following three optimizations to enable scalable LU symbolic\nfactorization for nonsymmetric pattern sparse matrices on GPUs. First, we\nintroduce a novel fine-grained parallel symbolic factorization algorithm that\nis well suited for the Single Instruction Multiple Thread (SIMT) architecture\nof GPUs. Second, we tailor supernode detection into a SIMT friendly process and\nstrive to balance the workload, minimize the communication and saturate the GPU\ncomputing resources during supernode detection. Third, we introduce a\nthree-pronged optimization to reduce the excessive space consumption problem\nfaced by multi-source concurrent symbolic factorization. Taken together, gSoFa\nachieves up to 31x speedup from 1 to 44 Summit nodes (6 to 264 GPUs) and\noutperforms the state-of-the-art CPU project, on average, by 5x. Notably, gSoFa\nalso achieves {up to 47%} of the peak memory throughput of a V100 GPU in\nSummit.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 02:21:03 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 02:01:58 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 04:01:40 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 23:58:58 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gaihre", "Anil", ""], ["Li", "Xiaoye S.", ""], ["Liu", "Hang", ""]]}, {"id": "2007.00864", "submitter": "Shail Dave", "authors": "Shail Dave, Riyadh Baghdadi, Tony Nowatzki, Sasikanth Avancha, Aviral\n  Shrivastava, Baoxin Li", "title": "Hardware Acceleration of Sparse and Irregular Tensor Computations of ML\n  Models: A Survey and Insights", "comments": "Accepted for publication in Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are widely used in many important domains. For\nefficiently processing these computational- and memory-intensive applications,\ntensors of these over-parameterized models are compressed by leveraging\nsparsity, size reduction, and quantization of tensors. Unstructured sparsity\nand tensors with varying dimensions yield irregular computation, communication,\nand memory access patterns; processing them on hardware accelerators in a\nconventional manner does not inherently leverage acceleration opportunities.\nThis paper provides a comprehensive survey on the efficient execution of sparse\nand irregular tensor computations of ML models on hardware accelerators. In\nparticular, it discusses enhancement modules in the architecture design and the\nsoftware support; categorizes different hardware designs and acceleration\ntechniques and analyzes them in terms of hardware and execution costs; analyzes\nachievable accelerations for recent DNNs; highlights further opportunities in\nterms of hardware/software/model co-design optimizations (inter/intra-module).\nThe takeaways from this paper include: understanding the key challenges in\naccelerating sparse, irregular-shaped, and quantized tensors; understanding\nenhancements in accelerator systems for supporting their efficient\ncomputations; analyzing trade-offs in opting for a specific design choice for\nencoding, storing, extracting, communicating, computing, and load-balancing the\nnon-zeros; understanding how structured sparsity can improve storage efficiency\nand balance computations; understanding how to compile and map models with\nsparse tensors on the accelerators; understanding recent design trends for\nefficient accelerations and further opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 04:08:40 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:41:44 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dave", "Shail", ""], ["Baghdadi", "Riyadh", ""], ["Nowatzki", "Tony", ""], ["Avancha", "Sasikanth", ""], ["Shrivastava", "Aviral", ""], ["Li", "Baoxin", ""]]}, {"id": "2007.01045", "submitter": "Shiqing Fan", "authors": "Shiqing Fan, Yi Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng,\n  Chuan Wu, Guoping Long, Jun Yang, Lixue Xia, Lansong Diao, Xiaoyong Liu, Wei\n  Lin", "title": "DAPPLE: A Pipelined Data Parallel Approach for Training Large Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenging task to train large DNN models on sophisticated GPU\nplatforms with diversified interconnect capabilities. Recently, pipelined\ntraining has been proposed as an effective approach for improving device\nutilization. However, there are still several tricky issues to address:\nimproving computing efficiency while ensuring convergence, and reducing memory\nusage without incurring additional computing costs. We propose DAPPLE, a\nsynchronous training framework which combines data parallelism and pipeline\nparallelism for large DNN models. It features a novel parallelization strategy\nplanner to solve the partition and placement problems, and explores the optimal\nhybrid strategy of data and pipeline parallelism. We also propose a new runtime\nscheduling algorithm to reduce device memory usage, which is orthogonal to\nre-computation approach and does not come at the expense of training\nthroughput. Experiments show that DAPPLE planner consistently outperforms\nstrategies generated by PipeDream's planner by up to 3.23x under synchronous\ntraining scenarios, and DAPPLE runtime outperforms GPipe by 1.6x speedup of\ntraining throughput and reduces the memory consumption of 12% at the same time.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:06:15 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Fan", "Shiqing", ""], ["Rong", "Yi", ""], ["Meng", "Chen", ""], ["Cao", "Zongyan", ""], ["Wang", "Siyu", ""], ["Zheng", "Zhen", ""], ["Wu", "Chuan", ""], ["Long", "Guoping", ""], ["Yang", "Jun", ""], ["Xia", "Lixue", ""], ["Diao", "Lansong", ""], ["Liu", "Xiaoyong", ""], ["Lin", "Wei", ""]]}, {"id": "2007.01046", "submitter": "Saar Tochner", "authors": "Maya Dotan and Saar Tochner", "title": "Proofs of Useless Work -- Positive and Negative Results for Wasteless\n  Mining Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many blockchain systems today, including Bitcoin, rely on Proof of Work\n(PoW). Proof of work is crucial to the liveness and security of\ncryptocurrencies. The assumption when using PoW is that a lot of trial and\nerror is required on average before a valid block is generated. One of the main\nconcerns raised with regard to this kind of system is the inherent need to\n\"waste\" energy on \"meaningless\" problems. In fact, the Bitcoin system is\nbelieved to consume more electricity than several small countries [5]. In this\nwork we formally define three properties that are necessary for wasteless PoW\nsystems: (1) solve \"meaningful\" problems (2) solve them efficiently and (3) be\nsecure against double-spend attacks. We analyze these properties and deduce\nconstraints that impose on PoW systems. In particular, we conclude that under\nrealistic assumptions, the set of allowed functions for mining must be preimage\nresistant functions. Finally, we propose a modification to the Bitcoin\nconsensus rule that allows users to upload a certain subset of preimage\nresistant problems and let the mining process solve them. We prove security\nagainst Double-Spend attacks identical to the existing security guarantee in\nBitcoin today.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 12:07:26 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Dotan", "Maya", ""], ["Tochner", "Saar", ""]]}, {"id": "2007.01154", "submitter": "Mohammad Mahdi Kamani", "authors": "Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari and Mehrdad\n  Mahdavi", "title": "Federated Learning with Compression: Unified Analysis and Sharp\n  Guarantees", "comments": "version 2. more experiments and comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In federated learning, communication cost is often a critical bottleneck to\nscale up distributed optimization algorithms to collaboratively learn a model\nfrom millions of devices with potentially unreliable or limited communication\nand heterogeneous data distributions. Two notable trends to deal with the\ncommunication overhead of federated algorithms are gradient compression and\nlocal computation with periodic communication. Despite many attempts,\ncharacterizing the relationship between these two approaches has proven\nelusive. We address this by proposing a set of algorithms with periodical\ncompressed (quantized or sparsified) communication and analyze their\nconvergence properties in both homogeneous and heterogeneous local data\ndistribution settings. For the homogeneous setting, our analysis improves\nexisting bounds by providing tighter convergence rates for both strongly convex\nand non-convex objective functions. To mitigate data heterogeneity, we\nintroduce a local gradient tracking scheme and obtain sharp convergence rates\nthat match the best-known communication complexities without compression for\nconvex, strongly convex, and nonconvex settings. We complement our theoretical\nresults and demonstrate the effectiveness of our proposed methods by several\nexperiments on real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 14:44:07 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 04:33:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Haddadpour", "Farzin", ""], ["Kamani", "Mohammad Mahdi", ""], ["Mokhtari", "Aryan", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2007.01156", "submitter": "Jorge Pe\\~na Queralta", "authors": "Jorge Pe\\~na Queralta, Li Qingqing, Zhuo Zou, Tomi Westerlund", "title": "Enhancing Autonomy with Blockchain and Multi-Access Edge Computing in\n  Distributed Robotic Systems", "comments": "Accepted to the Fifth International Conference on Fog and Mobile Edge\n  Computing (FMEC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This conceptual paper discusses how different aspects involving the\nautonomous operation of robots and vehicles will change when they have access\nto next-generation mobile networks. 5G and beyond connectivity is bringing\ntogether a myriad of technologies and industries under its umbrella.\nHigh-bandwidth, low-latency edge computing services through network slicing\nhave the potential to support novel application scenarios in different domains\nincluding robotics, autonomous vehicles, and the Internet of Things. In\nparticular, multi-tenant applications at the edge of the network will boost the\ndevelopment of autonomous robots and vehicles offering computational resources\nand intelligence through reliable offloading services. The integration of more\ndistributed network architectures with distributed robotic systems can increase\nthe degree of intelligence and level of autonomy of connected units. We argue\nthat the last piece to put together a services framework with third-party\nintegration will be next-generation low-latency blockchain networks.\nBlockchains will enable a transparent and secure way of providing services and\nmanaging resources at the Multi-Access Edge Computing (MEC) layer. We overview\nthe state-of-the-art in MEC slicing, distributed robotic systems and blockchain\ntechnology to define a framework for services the MEC layer that will enhance\nthe autonomous operations of connected robots and vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:41:36 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Queralta", "Jorge Pe\u00f1a", ""], ["Qingqing", "Li", ""], ["Zou", "Zhuo", ""], ["Westerlund", "Tomi", ""]]}, {"id": "2007.01191", "submitter": "Kristian Hinnenthal", "authors": "Michael Feldmann and Kristian Hinnenthal and Christian Scheideler", "title": "Fast Hybrid Network Algorithms for Shortest Paths in Sparse Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing shortest paths in hybrid networks, in\nwhich nodes can make use of different communication modes. For example, mobile\nphones may use ad-hoc connections via Bluetooth or Wi-Fi in addition to the\ncellular network to solve tasks more efficiently. Like in this case, the\ndifferent communication modes may differ considerably in range, bandwidth, and\nflexibility. We build upon the model of Augustine et al. [SODA '20], which\ncaptures these differences by a local and a global mode. Specifically, the\nlocal edges model a fixed communication network in which $O(1)$ messages of\nsize $O(\\log n)$ can be sent over every edge in each synchronous round. The\nglobal edges form a clique, but nodes are only allowed to send and receive a\ntotal of at most $O(\\log n)$ messages over global edges, which restricts the\nnodes to use these edges only very sparsely. We demonstrate the power of hybrid\nnetworks by presenting algorithms to compute Single-Source Shortest Paths and\nthe diameter very efficiently in sparse graphs. Specifically, we present exact\n$O(\\log n)$ time algorithms for cactus graphs (i.e., graphs in which each edge\nis contained in at most one cycle), and $3$-approximations for graphs that have\nat most $n + O(n^{1/3})$ edges and arboricity $O(\\log n)$. For these graph\nclasses, our algorithms provide exponentially faster solutions than the best\nknown algorithms for general graphs in this model. Beyond shortest paths, we\nalso provide a variety of useful tools and techniques for hybrid networks,\nwhich may be of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:23:51 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 09:40:05 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 11:05:32 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Feldmann", "Michael", ""], ["Hinnenthal", "Kristian", ""], ["Scheideler", "Christian", ""]]}, {"id": "2007.01199", "submitter": "Lukas Gianinazzi", "authors": "Lukas Gianinazzi, Torsten Hoefler", "title": "Parallel Planar Subgraph Isomorphism and Vertex Connectivity", "comments": "To appear in: Proceedings of the 32nd ACM Symposium on Parallelism in\n  Algorithms and Architectures (SPAA '20), July 15-17, 2020, Virtual Event,\n  USA. ACM, New York, NY, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first parallel fixed-parameter algorithm for subgraph\nisomorphism in planar graphs, bounded-genus graphs, and, more generally, all\nminor-closed graphs of locally bounded treewidth. Our randomized low depth\nalgorithm has a near-linear work dependency on the size of the target graph.\nExisting low depth algorithms do not guarantee that the work remains\nasymptotically the same for any constant-sized pattern. By using a connection\nto certain separating cycles, our subgraph isomorphism algorithm can decide the\nvertex connectivity of a planar graph (with high probability) in asymptotically\nnear-linear work and poly-logarithmic depth. Previously, no sub-quadratic work\nand poly-logarithmic depth bound was known in planar graphs (in particular for\ndistinguishing between four-connected and five-connected planar graphs).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 15:30:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Gianinazzi", "Lukas", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2007.01260", "submitter": "Nicolas Kourtellis Ph.D.", "authors": "Nicolas Kourtellis and Herodotos Herodotou and Maciej Grzenda and\n  Piotr Wawrzyniak and Albert Bifet", "title": "S2CE: A Hybrid Cloud and Edge Orchestrator for Mining Exascale\n  Distributed Streams", "comments": "11 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive increase in volume, velocity, variety, and veracity of data\ngenerated by distributed and heterogeneous nodes such as IoT and other devices,\ncontinuously challenge the state of art in big data processing platforms and\nmining techniques. Consequently, it reveals an urgent need to address the\never-growing gap between this expected exascale data generation and the\nextraction of insights from these data. To address this need, this paper\nproposes Stream to Cloud & Edge (S2CE), a first of its kind, optimized,\nmulti-cloud and edge orchestrator, easily configurable, scalable, and\nextensible. S2CE will enable machine and deep learning over voluminous and\nheterogeneous data streams running on hybrid cloud and edge settings, while\noffering the necessary functionalities for practical and scalable processing:\ndata fusion and preprocessing, sampling and synthetic stream generation, cloud\nand edge smart resource management, and distributed processing.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:14:57 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Herodotou", "Herodotos", ""], ["Grzenda", "Maciej", ""], ["Wawrzyniak", "Piotr", ""], ["Bifet", "Albert", ""]]}, {"id": "2007.01277", "submitter": "Ao Li", "authors": "Ao Li, Bojian Zheng, Gennady Pekhimenko, and Fan Long", "title": "Automatic Horizontal Fusion for GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present automatic horizontal fusion, a novel optimization technique that\ncomplements the standard kernel fusion techniques for GPU programs. Unlike the\nstandard fusion, whose goal is to eliminate intermediate data round trips, our\nhorizontal fusion technique aims to increase the thread-level parallelism to\nhide instruction latencies. We also present HFuse, a new source to source CUDA\ncompiler that implements automatic horizontal fusion. Our experimental results\nshow that horizontal fusion can speed up the running time by 2.5%-60.8%. Our\nresults reveal that the horizontal fusion is especially beneficial for fusing\nkernels with instructions that require different kinds of GPU resources (e.g.,\na memory-intensive kernel and a compute-intensive kernel).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:34:07 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Li", "Ao", ""], ["Zheng", "Bojian", ""], ["Pekhimenko", "Gennady", ""], ["Long", "Fan", ""]]}, {"id": "2007.01395", "submitter": "Suraj Padmanaban Kesavan", "authors": "Suraj P.Kesavan, Harsh Bhatia, Abhinav Bhatele, Todd Gamblin,\n  Peer-Timo Bremer and Kwan-Liu Ma", "title": "Scalable Comparative Visualization of Ensembles of Call Graphs", "comments": "12 pages, 6 figures, Submitted to IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimizing the performance of large-scale parallel codes is critical for\nefficient utilization of computing resources. Code developers often explore\nvarious execution parameters, such as hardware configurations, system software\nchoices, and application parameters, and are interested in detecting and\nunderstanding bottlenecks in different executions. They often collect\nhierarchical performance profiles represented as call graphs, which combine\nperformance metrics with their execution contexts. The crucial task of\nexploring multiple call graphs together is tedious and challenging because of\nthe many structural differences in the execution contexts and significant\nvariability in the collected performance metrics (e.g., execution runtime). In\nthis paper, we present an enhanced version of CallFlow to support the\nexploration of ensembles of call graphs using new types of visualizations,\nanalysis, graph operations, and features. We introduce ensemble-Sankey, a new\nvisual design that combines the strengths of resource-flow (Sankey) and\nbox-plot visualization techniques. Whereas the resource-flow visualization can\neasily and intuitively describe the graphical nature of the call graph, the box\nplots overlaid on the nodes of Sankey convey the performance variability within\nthe ensemble. Our interactive visual interface provides linked views to help\nexplore ensembles of call graphs, e.g., by facilitating the analysis of\nstructural differences, and identifying similar or distinct call graphs. We\ndemonstrate the effectiveness and usefulness of our design through case studies\non large-scale parallel codes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 00:42:45 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Kesavan", "Suraj P.", ""], ["Bhatia", "Harsh", ""], ["Bhatele", "Abhinav", ""], ["Gamblin", "Todd", ""], ["Bremer", "Peer-Timo", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2007.01397", "submitter": "Vitaliy Chiley", "authors": "Abhinav Venigalla and Atli Kosson and Vitaliy Chiley and Urs K\\\"oster", "title": "Adaptive Braking for Mitigating Gradient Delay", "comments": "In Beyond First Order Methods in ML Systems workshop at the 37th\n  International Conference on Machine Learning, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network training is commonly accelerated by using multiple\nsynchronized workers to compute gradient updates in parallel. Asynchronous\nmethods remove synchronization overheads and improve hardware utilization at\nthe cost of introducing gradient delay, which impedes optimization and can lead\nto lower final model performance. We introduce Adaptive Braking (AB), a\nmodification for momentum-based optimizers that mitigates the effects of\ngradient delay. AB dynamically scales the gradient based on the alignment of\nthe gradient and the velocity. This can dampen oscillations along high\ncurvature directions of the loss surface, stabilizing and accelerating\nasynchronous training. We show that applying AB on top of SGD with momentum\nenables training ResNets on CIFAR-10 and ImageNet-1k with delays $D \\geq$ 32\nupdate steps with minimal drop in final test accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 21:26:27 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 17:12:25 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Venigalla", "Abhinav", ""], ["Kosson", "Atli", ""], ["Chiley", "Vitaliy", ""], ["K\u00f6ster", "Urs", ""]]}, {"id": "2007.01408", "submitter": "Derek Weitzel", "authors": "Edgar Fajardo, Marian Zvada, Derek Weitzel, Mats Rynge, John Hicks,\n  Mat Selmeci, Brian Lin, Pascal Paschos, Brian Bockelman, Igor Sfiligoi,\n  Andrew Hanushevsky, and Frank W\\\"urthwein", "title": "Creating a content delivery network for general science on the internet\n  backbone using XCaches", "comments": null, "journal-ref": null, "doi": "10.1051/epjconf/202024504041", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general problem faced by computing on the grid for opportunistic users is\nthat delivering cycles is simpler than delivering data to those cycles. In this\nproject we show how we integrated XRootD caches placed on the internet backbone\nto implement a content delivery network for general science workflows. We will\nshow that for some workflows on different science domains like high energy\nphysics, gravitational waves, and others the combination of data reuse from the\nworkflows together with the use of caches increases CPU efficiency while\ndecreasing network bandwidth use.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:06:09 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 13:14:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Fajardo", "Edgar", ""], ["Zvada", "Marian", ""], ["Weitzel", "Derek", ""], ["Rynge", "Mats", ""], ["Hicks", "John", ""], ["Selmeci", "Mat", ""], ["Lin", "Brian", ""], ["Paschos", "Pascal", ""], ["Bockelman", "Brian", ""], ["Sfiligoi", "Igor", ""], ["Hanushevsky", "Andrew", ""], ["W\u00fcrthwein", "Frank", ""]]}, {"id": "2007.01442", "submitter": "Ronshee Chawla", "authors": "Ronshee Chawla, Abishek Sankararaman and Sanjay Shakkottai", "title": "Multi-Agent Low-Dimensional Linear Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-agent stochastic linear bandit with side information,\nparameterized by an unknown vector $\\theta^* \\in \\mathbb{R}^d$. The side\ninformation consists of a finite collection of low-dimensional subspaces, one\nof which contains $\\theta^*$. In our setting, agents can collaborate to reduce\nregret by sending recommendations across a communication graph connecting them.\nWe present a novel decentralized algorithm, where agents communicate subspace\nindices with each other and each agent plays a projected variant of LinUCB on\nthe corresponding (low-dimensional) subspace. By distributing the search for\nthe optimal subspace across users and learning of the unknown vector by each\nagent in the corresponding low-dimensional subspace, we show that the per-agent\nfinite-time regret is much smaller than the case when agents do not\ncommunicate. We finally complement these results through simulations.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 23:54:56 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 01:10:50 GMT"}, {"version": "v3", "created": "Sun, 16 May 2021 04:55:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chawla", "Ronshee", ""], ["Sankararaman", "Abishek", ""], ["Shakkottai", "Sanjay", ""]]}, {"id": "2007.01459", "submitter": "Quan-Lin Li", "authors": "Quan-Lin Li, Yan-Xia Chang, Xiaole Wu and Guoqing Zhang", "title": "A New Theoretical Framework of Pyramid Markov Processes for Blockchain\n  Selfish Mining", "comments": "76 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PF math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a new theoretical framework of pyramid Markov\nprocesses to solve some open and fundamental problems of blockchain selfish\nmining. To this end, we first describe a more general blockchain selfish mining\nwith both a two-block leading competitive criterion and a new economic\nincentive, and establish a pyramid Markov process to express the dynamic\nbehavior of the selfish mining from both consensus protocol and economic\nincentive. Then we show that the pyramid Markov process is stable and so is the\nblockchain, and its stationary probability vector is matrix-geometric with an\nexplicitly representable rate matrix. Furthermore, we use the stationary\nprobability vector to be able to analyze the waste of computational resource\ndue to generating a lot of orphan (or stale) blocks. Nextly, we set up a\npyramid Markov reward process to investigate the long-run average profits of\nthe honest and dishonest mining pools, respectively. Specifically, we show that\nthe long-run average profits are multivariate linear such that we can measure\nthe improvement of mining efficiency of the dishonest mining pool comparing to\nthe honest mining pool. As a by-product, we build three approximative Markov\nprocesses when the system states are described as the block-number difference\nof two forked block branches. Also, by using their special cases with non\nnetwork latency, we can further provide some useful interpretation for both the\nMarkov chain (Figure 1) and the revenue analysis ((1) to (3)) of the seminal\nwork by Eyal and Sirer (2014). Finally, we use some numerical examples to\nverify the correctness and computability of our theoretical results. We hope\nthat the methodology and results developed in this paper shed light on the\nblockchain selfish mining such that a series of promising research can be\nproduced potentially.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 02:02:35 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 16:58:57 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Quan-Lin", ""], ["Chang", "Yan-Xia", ""], ["Wu", "Xiaole", ""], ["Zhang", "Guoqing", ""]]}, {"id": "2007.01560", "submitter": "Alistair Stewart", "authors": "Alistair Stewart and Eleftherios Kokoris-Kogia", "title": "GRANDPA: a Byzantine Finality Gadget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic Byzantine fault-tolerant consensus protocols forfeit liveness in the\nface of asynchrony in order to preserve safety, whereas most deployed\nblockchain protocols forfeit safety in order to remain live. In this work, we\nachieve the best of both worlds by proposing a novel abstractions called the\nfinality gadget. A finality gadget allows for transactions to always\noptimistically commit but informs the clients that these transactions might be\nunsafe. As a result, a blockchain can execute transactions optimistically and\nonly commit them after they have been sufficiently and provably audited. In\nthis work, we formally model the finality gadget abstraction, prove that it is\nimpossible to solve it deterministically in full asynchrony (even though it is\nstronger than consensus) and provide a partially synchronous protocol which is\ncurrently securing a major blockchain. This way we show that the protocol\ndesigner can decouple safety and liveness in order to speed up recovery from\nfailures. We believe that there can be other types of finality gadgets that\nprovide weaker safety (e.g., probabilistic) in order to gain more efficiency\nand this can depend on the probability that the network is not in synchrony.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:05:40 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Stewart", "Alistair", ""], ["Kokoris-Kogia", "Eleftherios", ""]]}, {"id": "2007.01562", "submitter": "Xu Chen", "authors": "Shuai Yu and Xu Chen and Shuai Wang and Lingjun Pu and Di Wu", "title": "An Edge Computing-based Photo Crowdsourcing Framework for Real-time 3D\n  Reconstruction", "comments": "Accepted by IEEE Transactions on Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based three-dimensional (3D) reconstruction utilizes a set of photos to\nbuild 3D model and can be widely used in many emerging applications such as\naugmented reality (AR) and disaster recovery. Most of existing 3D\nreconstruction methods require a mobile user to walk around the target area and\nreconstruct objectives with a hand-held camera, which is inefficient and\ntime-consuming. To meet the requirements of delay intensive and resource hungry\napplications in 5G, we propose an edge computing-based photo crowdsourcing\n(EC-PCS) framework in this paper. The main objective is to collect a set of\nrepresentative photos from ubiquitous mobile and Internet of Things (IoT)\ndevices at the network edge for real-time 3D model reconstruction, with network\nresource and monetary cost considerations. Specifically, we first propose a\nphoto pricing mechanism by jointly considering their freshness, resolution and\ndata size. Then, we design a novel photo selection scheme to dynamically select\na set of photos with the required target coverage and the minimum monetary\ncost. We prove the NP-hardness of such problem, and develop an efficient\ngreedy-based approximation algorithm to obtain a near-optimal solution.\nMoreover, an optimal network resource allocation scheme is presented, in order\nto minimize the maximum uploading delay of the selected photos to the edge\nserver. Finally, a 3D reconstruction algorithm and a 3D model caching scheme\nare performed by the edge server in real time. Extensive experimental results\nbased on real-world datasets demonstrate the superior performance of our EC-PCS\nsystem over the existing mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:16:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Yu", "Shuai", ""], ["Chen", "Xu", ""], ["Wang", "Shuai", ""], ["Pu", "Lingjun", ""], ["Wu", "Di", ""]]}, {"id": "2007.01789", "submitter": "Carlos Maltzahn", "authors": "Xiaowei (Aaron) Chu, Jeff LeFevre, Aldrin Montana, Dana Robinson,\n  Quincey Koziol, Peter Alvaro, Carlos Maltzahn", "title": "Mapping Datasets to Object Storage System", "comments": null, "journal-ref": "In 24th International Conference on Computing in High Energy &\n  Nuclear Physics, Adelaide, Australia, November 4-8 2019", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access libraries such as ROOT and HDF5 allow users to interact with datasets\nusing high level abstractions, like coordinate systems and associated slicing\noperations. Unfortunately, the implementations of access libraries are based on\noutdated assumptions about storage systems interfaces and are generally unable\nto fully benefit from modern fast storage devices. The situation is getting\nworse with rapidly evolving storage devices such as non-volatile memory and\never larger datasets. This project explores distributed dataset mapping\ninfrastructures that can integrate and scale out existing access libraries\nusing Ceph's extensible object model, avoiding re-implementation or even\nmodifications of these access libraries as much as possible. These programmable\nstorage extensions coupled with our distributed dataset mapping techniques\nenable: 1) access library operations to be offloaded to storage system servers,\n2) the independent evolution of access libraries and storage systems and 3)\nfully leveraging of the existing load balancing, elasticity, and failure\nmanagement of distributed storage systems like Ceph. They also create more\nopportunities to conduct storage server-local optimizations specific to storage\nservers. For example, storage servers might include local key/value stores\ncombined with chunk stores that require different optimizations than a local\nfile system. As storage servers evolve to support new storage devices like\nnon-volatile memory, these server-local optimizations can be implemented while\nminimizing disruptions to applications. We will report progress on the means by\nwhich distributed dataset mapping can be abstracted over particular access\nlibraries, including access libraries for ROOT data, and how we address some of\nthe challenges revolving around data partitioning and composability of access\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:31:00 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Xiaowei", "", "", "Aaron"], ["Chu", "", ""], ["LeFevre", "Jeff", ""], ["Montana", "Aldrin", ""], ["Robinson", "Dana", ""], ["Koziol", "Quincey", ""], ["Alvaro", "Peter", ""], ["Maltzahn", "Carlos", ""]]}, {"id": "2007.01791", "submitter": "Wen Guan", "authors": "Wen Guan, Tadashi Maeno, Gancho Dimitrov, Brian Paul Bockelman, Torre\n  Wenaus, Vakhtang Tsulaia, Nicolo Magini", "title": "Towards an Intelligent Data Delivery Service", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": "10.1051/epjconf/202024504015", "report-no": null, "categories": "cs.DC hep-ex physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ATLAS Event Streaming Service (ESS) at the LHC is an approach to\npreprocess and deliver data for Event Service (ES) that has implemented a\nfine-grained approach for ATLAS event processing. The ESS allows one to\nasynchronously deliver only the input events required by ES processing, with\nthe aim to decrease data traffic over WAN and improve overall data processing\nthroughput. A prototype of ESS was developed to deliver streaming events to\nfine-grained ES jobs. Based on it, an intelligent Data Delivery Service (iDDS)\nis under development to decouple the \"cold format\" and the processing format of\nthe data, which also opens the opportunity to include the production systems of\nother HEP experiments. Here we will at first present the ESS model view and its\nmotivations for iDDS system. Then we will also present the iDDS schema,\narchitecture and the applications of iDDS.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:31:33 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Guan", "Wen", ""], ["Maeno", "Tadashi", ""], ["Dimitrov", "Gancho", ""], ["Bockelman", "Brian Paul", ""], ["Wenaus", "Torre", ""], ["Tsulaia", "Vakhtang", ""], ["Magini", "Nicolo", ""]]}, {"id": "2007.01811", "submitter": "Chris von Csefalvay", "authors": "Tamas Foldi, Chris von Csefalvay and Nicolas A. Perez", "title": "JAMPI: efficient matrix multiplication in Spark using Barrier Execution\n  Mode", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new barrier mode in Apache Spark allows embedding distributed deep\nlearning training as a Spark stage to simplify the distributed training\nworkflow. In Spark, a task in a stage does not depend on any other tasks in the\nsame stage, and hence it can be scheduled independently. However, several\nalgorithms require more sophisticated inter-task communications, similar to the\nMPI paradigm. By combining distributed message passing (using asynchronous\nnetwork IO), OpenJDK's new auto-vectorization and Spark's barrier execution\nmode, we can add non-map/reduce based algorithms, such as Cannon's distributed\nmatrix multiplication to Spark. We document an efficient distributed matrix\nmultiplication using Cannon's algorithm, which improves significantly on the\nperformance of the existing MLlib implementation. Used within a barrier task,\nthe algorithm described herein results in an up to 24 percent performance\nincrease on a 10,000x10,000 square matrix with a significantly lower memory\nfootprint. Applications of efficient matrix multiplication include, among\nothers, accelerating the training and implementation of deep convolutional\nneural network based workloads, and thus such efficient algorithms can play a\nground-breaking role in faster, more efficient execution of even the most\ncomplicated machine learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 17:31:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Foldi", "Tamas", ""], ["von Csefalvay", "Chris", ""], ["Perez", "Nicolas A.", ""]]}, {"id": "2007.02081", "submitter": "Amirhossein Sayyadabdi", "authors": "Amirhossein Sayyadabdi and Mohsen Sharifi", "title": "Avoiding Register Overflow in the Bakery Algorithm", "comments": "To be presented at the international workshop on Scheduling and\n  Resource Management for Parallel and Distributed Systems (SRMPDS '20)", "journal-ref": null, "doi": "10.1145/3409390.3409411", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer systems are designed to make resources available to users and users\nmay be interested in some resources more than others, therefore, a coordination\nscheme is required to satisfy the users' requirements. This scheme may\nimplement certain policies such as \"never allocate more than X units of\nresource Z\". One policy that is of particular interest is the inability of\nusers to access a single resource at the same time, which is called the problem\nof mutual exclusion. Resource management concerns the coordination and\ncollaboration of users, and it is usually based on making a decision. In the\ncase of mutual exclusion, that decision is about granting access to a resource.\nTherefore, mutual exclusion is useful for supporting resource access\nmanagement. The first true solution to the mutual exclusion problem is known as\nthe Bakery algorithm that does not rely on any lower-lever mutual exclusion. We\nexamine the problem of register overflow in real-world implementations of the\nBakery algorithm and present a variant algorithm named Bakery++ that prevents\noverflows from ever happening. Bakery++ avoids overflows without allowing a\nprocess to write into other processes' memory and without using additional\nmemory or complex arithmetic or redefining the operations and functions used in\nBakery. Bakery++ is almost as simple as Bakery and it is straightforward to\nimplement in real systems. With Bakery++, there is no reason to keep\nimplementing Bakery in real computers because Bakery++ eliminates the\nprobability of overflows and hence it is more practical than Bakery. Previous\napproaches to circumvent the problem of register overflow included introducing\nnew variables or redefining the operations or functions used in the original\nBakery algorithm, while Bakery++ avoids overflows by using simple conditional\nstatements. (the abstract does not end here.)\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 12:19:58 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sayyadabdi", "Amirhossein", ""], ["Sharifi", "Mohsen", ""]]}, {"id": "2007.02191", "submitter": "Mehmet Emre Ozfatura", "authors": "Emre Ozfatura and Sennur Ulukus and Deniz Gunduz", "title": "Coded Distributed Computing with Partial Recovery", "comments": "Presented in part at the 2019 IEEE International Conference on\n  Acoustics, Speech and Signal Processing in Brighton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded computation techniques provide robustness against straggling workers in\ndistributed computing. However, most of the existing schemes require exact\nprovisioning of the straggling behaviour and ignore the computations carried\nout by straggling workers. Moreover, these schemes are typically designed to\nrecover the desired computation results accurately, while in many machine\nlearning and iterative optimization algorithms, faster approximate solutions\nare known to result in an improvement in the overall convergence time. In this\npaper, we first introduce a novel coded matrix-vector multiplication scheme,\ncalled coded computation with partial recovery (CCPR), which benefits from the\nadvantages of both coded and uncoded computation schemes, and reduces both the\ncomputation time and the decoding complexity by allowing a trade-off between\nthe accuracy and the speed of computation. We then extend this approach to\ndistributed implementation of more general computation tasks by proposing a\ncoded communication scheme with partial recovery, where the results of subtasks\ncomputed by the workers are coded before being communicated. Numerical\nsimulations on a large linear regression task confirm the benefits of the\nproposed distributed computation scheme with partial recovery in terms of the\ntrade-off between the computation accuracy and latency.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:34:49 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ozfatura", "Emre", ""], ["Ulukus", "Sennur", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2007.02372", "submitter": "Yuanhao Wei", "authors": "Yuanhao Wei, Naama Ben-David, Guy E. Blelloch, Panagiota Fatourou,\n  Eric Ruppert, Yihan Sun", "title": "Constant-Time Snapshots with Applications to Concurrent Data Structures", "comments": "To appear in PPoPP'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for efficiently taking snapshots of the state of a\ncollection of CAS objects. Taking a snapshot allows later operations to read\nthe value that each CAS object had at the time the snapshot was taken. Taking a\nsnapshot requires a constant number of steps and returns a handle to the\nsnapshot. Reading a snapshotted value of an individual CAS object using this\nhandle is wait-free, taking time proportional to the number of successful CASes\non the object since the snapshot was taken. Our fast, flexible snapshots yield\nsimple, efficient implementations of atomic multi-point queries on concurrent\ndata structures built from CAS objects. For example, in a search tree where\nchild pointers are updated using CAS, once a snapshot is taken, one can\natomically search for ranges of keys, find the first key that matches some\ncriteria, or check if a collection of keys are all present, simply by running a\nstandard sequential algorithm on a snapshot of the tree.\n  To evaluate the performance of our approach, we apply it to two search trees,\none balanced and one not. Experiments show that the overhead of supporting\nsnapshots is low across a variety of workloads. Moreover, in almost all cases,\nrange queries on the trees built from our snapshots perform as well as or\nbetter than state-of-the-art concurrent data structures that support atomic\nrange queries.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 16:09:00 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 11:00:56 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2020 18:24:16 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wei", "Yuanhao", ""], ["Ben-David", "Naama", ""], ["Blelloch", "Guy E.", ""], ["Fatourou", "Panagiota", ""], ["Ruppert", "Eric", ""], ["Sun", "Yihan", ""]]}, {"id": "2007.02696", "submitter": "Paul Pop", "authors": "Paul Pop, Bahram Zarrin, Mohammadreza Barzegaran, Stefan Schulte,\n  Sasikumar Punnekkat, Jan Ruh, Wilfried Steiner", "title": "The FORA Fog Computing Platform for Industrial IoT", "comments": "arXiv admin note: text overlap with arXiv:1906.10718 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industry 4.0 will only become a reality through the convergence of\nOperational and Information Technologies (OT & IT), which use different\ncomputation and communication technologies. Cloud Computing cannot be used for\nOT involving industrial applications, since it cannot guar-antee stringent\nnon-functional requirements, e.g., dependability, trustworthiness and\ntimeliness. Instead, a new computing paradigm, called Fog Computing, is\nenvisioned as an architectural means to realize the IT/OT convergence. In this\npaper we propose a Fog Computing Platform (FCP) reference architecture\ntargeting Industrial IoT applications. The FCP is based on: deter-ministic\nvirtualization that reduces the effort required for safety and security\nassurance; middle-ware for supporting both critical control and dynamic Fog\napplications; deterministic networking and interoperability, using open\nstandards such as IEEE 802.1 Time-Sensitive Networking (TSN) and OPC Unified\nArchitecture (OPC UA); mechanisms for resource management and or-chestration;\nand services for security, fault tolerance and distributed machine learning. We\npro-pose a methodology for the definition and the evaluation of the reference\narchitecture. We use the Architecture Analysis Design Language (AADL) to model\nthe FCP reference architecture, and a set of industrial use cases to evaluate\nits suitability for the Industrial IoT area.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:06:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pop", "Paul", ""], ["Zarrin", "Bahram", ""], ["Barzegaran", "Mohammadreza", ""], ["Schulte", "Stefan", ""], ["Punnekkat", "Sasikumar", ""], ["Ruh", "Jan", ""], ["Steiner", "Wilfried", ""]]}, {"id": "2007.02753", "submitter": "Matteo Lucchi", "authors": "Matteo Lucchi, Friedemann Zindler, Stephan M\\\"uhlbacher-Karrer, Horst\n  Pichler", "title": "robo-gym -- An Open Source Toolkit for Distributed Deep Reinforcement\n  Learning on Real and Simulated Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying Deep Reinforcement Learning (DRL) to complex tasks in the field of\nrobotics has proven to be very successful in the recent years. However, most of\nthe publications focus either on applying it to a task in simulation or to a\ntask in a real world setup. Although there are great examples of combining the\ntwo worlds with the help of transfer learning, it often requires a lot of\nadditional work and fine-tuning to make the setup work effectively. In order to\nincrease the use of DRL with real robots and reduce the gap between simulation\nand real world robotics, we propose an open source toolkit: robo-gym. We\ndemonstrate a unified setup for simulation and real environments which enables\na seamless transfer from training in simulation to application on the robot. We\nshowcase the capabilities and the effectiveness of the framework with two real\nworld applications featuring industrial robots: a mobile robot and a robot arm.\nThe distributed capabilities of the framework enable several advantages like\nusing distributed algorithms, separating the workload of simulation and\ntraining on different physical machines as well as enabling the future\nopportunity to train in simulation and real world at the same time. Finally we\noffer an overview and comparison of robo-gym with other frequently used\nstate-of-the-art DRL frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 13:51:33 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 17:00:34 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Lucchi", "Matteo", ""], ["Zindler", "Friedemann", ""], ["M\u00fchlbacher-Karrer", "Stephan", ""], ["Pichler", "Horst", ""]]}, {"id": "2007.02800", "submitter": "David Carrera", "authors": "Nicolas Poggi, Alejandro Montero, David Carrera", "title": "Characterizing BigBench queries, Hive, and Spark in multi-cloud\n  environments", "comments": "Partially funded by European Research Council (ERC) under the\n  European Union's Horizon 2020 research and innovation programme (grant\n  agreement No 639595) - HiEST Project Published in the 9th TPC Technology\n  Conference, TPCTC 2017, Munich, Germany, August 28, 2017. Performance\n  Evaluation and Benchmarking for the Analytics Era", "journal-ref": null, "doi": "10.1007/978-3-319-72401-0_5", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BigBench is the new standard (TPCx-BB) for benchmarking and testing Big Data\nsystems. The TPCx-BB specification describes several business use cases --\nqueries -- which require a broad combination of data extraction techniques\nincluding SQL, Map/Reduce (M/R), user code (UDF), and Machine Learning to\nfulfill them. However, currently, there is no widespread knowledge of the\ndifferent resource requirements and expected performance of each query, as is\nthe case to more established benchmarks. At the same time, cloud providers\ncurrently offer convenient on-demand managed big data clusters (PaaS) with a\npay-as-you-go model. In PaaS, analytical engines such as Hive and Spark come\nready to use, with a general-purpose configuration and upgrade management. The\nstudy characterizes both the BigBench queries and the out-of-the-box\nperformance of Spark and Hive versions in the cloud. At the same time,\ncomparing popular PaaS offerings in terms of reliability, data scalability (1GB\nto 10TB), versions, and settings from Azure HDinsight, Amazon Web Services EMR,\nand Google Cloud Dataproc. The query characterization highlights the\nsimilarities and differences in Hive an Spark frameworks, and which queries are\nthe most resource consuming according to CPU, memory, and I/O. Scalability\nresults show how there is a need for configuration tuning in most cloud\nproviders as data scale grows, especially with Sparks memory usage. These\nresults can help practitioners to quickly test systems by picking a subset of\nthe queries which stresses each of the categories. At the same time, results\nshow how Hive and Spark compare and what performance can be expected of each in\nPaaS.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:04:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Poggi", "Nicolas", ""], ["Montero", "Alejandro", ""], ["Carrera", "David", ""]]}, {"id": "2007.02802", "submitter": "David Carrera", "authors": "\\'Alvaro Villalba, David Carrera", "title": "Multi-tenant Pub/Sub Processing for Real-time Data Streams", "comments": "Partially funded by European Research Council (ERC) under the\n  European Union's Horizon 2020 research and innovation programme (grant\n  agreement No 639595) - HiEST Project Published Euro-Par 2018: Euro-Par 2018:\n  Parallel Processing Workshops pp 251-262", "journal-ref": "Euro-Par 2018: Parallel Processing Workshops", "doi": "10.1007/978-3-030-10549-5_20", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devices and sensors generate streams of data across a diversity of locations\nand protocols. That data usually reaches a central platform that is used to\nstore and process the streams. Processing can be done in real time, with\ntransformations and enrichment happening on-the-fly, but it can also happen\nafter data is stored and organized in repositories. In the former case, stream\nprocessing technologies are required to operate on the data; in the latter\nbatch analytics and queries are of common use.\n  This paper introduces a runtime to dynamically construct data stream\nprocessing topologies based on user-supplied code. These dynamic topologies are\nbuilt on-the-fly using a data subscription model defined by the applications\nthat consume data. Each user-defined processing unit is called a Service\nObject. Every Service Object consumes input data streams and may produce output\nstreams that others can consume. The subscription-based programing model\nenables multiple users to deploy their own data-processing services. The\nruntime does the dynamic forwarding of data and execution of Service Objects\nfrom different users. Data streams can originate in real-world devices or they\ncan be the outputs of Service Objects.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:05:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Villalba", "\u00c1lvaro", ""], ["Carrera", "David", ""]]}, {"id": "2007.02813", "submitter": "David Carrera", "authors": "Aaron Call, Jord\\`a Polo, David Carrera, Francesc Guim, Sujoy Sen", "title": "Disaggregating Non-Volatile Memory for Throughput-Oriented Genomics\n  Workloads", "comments": "Partially funded by European Research Council (ERC) under the\n  European Union's Horizon 2020 research and innovation programme (grant\n  agreement No 639595) - HiEST Project Published Euro-Par 2018: Euro-Par 2018:\n  Parallel Processing Workshops pp 613-625", "journal-ref": "Euro-Par 2018: Euro-Par 2018: Parallel Processing Workshops", "doi": "10.1007/978-3-030-10549-5_48", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive exploitation of next-generation sequencing technologies requires\ndealing with both: huge amounts of data and complex bioinformatics pipelines.\nComputing architectures have evolved to deal with these problems, enabling\napproaches that were unfeasible years ago: accelerators and Non-Volatile\nMemories (NVM) are becoming widely used to enhance the most demanding\nworkloads. However, bioinformatics workloads are usually part of bigger\npipelines with different and dynamic needs in terms of resources. The\nintroduction of Software Defined Infrastructures (SDI) for data centers\nprovides roots to dramatically increase the efficiency in the management of\ninfrastructures. SDI enables new ways to structure hardware resources through\ndisaggregation, and provides new hardware composability and sharing mechanisms\nto deploy workloads in more flexible ways. In this paper we study a\nstate-of-the-art genomics application, SMUFIN, aiming to address the challenges\nof future HPC facilities.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 15:16:21 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Call", "Aaron", ""], ["Polo", "Jord\u00e0", ""], ["Carrera", "David", ""], ["Guim", "Francesc", ""], ["Sen", "Sujoy", ""]]}, {"id": "2007.02984", "submitter": "Nitzan Zamir", "authors": "Nitzan Zamir and Yoram Moses", "title": "Probably Approximately Knowing", "comments": "23 pages, 2 figures, a full version of a paper whose extended\n  abstract appears in the proceeding of PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas deterministic protocols are typically guaranteed to obtain particular\ngoals of interest, probabilistic protocols typically provide only probabilistic\nguarantees. This paper initiates an investigation of the interdependence\nbetween actions and subjective beliefs of agents in a probabilistic setting. In\nparticular, we study what probabilistic beliefs an agent should have when\nperforming actions, in a protocol that satisfies a probabilistic constraint of\nthe form: 'Condition C should hold with probability at least p when action a is\nperformed'. Our main result is that the expected degree of an agent's belief in\nC when it performs a equals the probability that C holds when a is performed.\nIndeed, if the threshold of the probabilistic constraint should hold with\nprobaility p=1-x^2 for some small value of x then, with probability 1-x, when\nthe agent acts it will assign a probabilistic belief no smaller than 1-x to the\npossibility that C holds. In other words, viewing strong belief as,\nintuitively, approximate knowledge, the agent must probably approximately know\n(PAK-know) that C is true when it acts.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 18:12:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zamir", "Nitzan", ""], ["Moses", "Yoram", ""]]}, {"id": "2007.03041", "submitter": "William Sands", "authors": "Andrew J. Christlieb, Pierson T. Guthrey, William A. Sands,\n  Mathialakan Thavappiragasm", "title": "Parallel Algorithms for Successive Convolution", "comments": "36 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider alternative discretizations for PDEs which use\nexpansions involving integral operators to approximate spatial derivatives.\nThese constructions use explicit information within the integral terms, but\ntreat boundary data implicitly, which contributes to the overall speed of the\nmethod. This approach is provably unconditionally stable for linear problems\nand stability has been demonstrated experimentally for nonlinear problems.\nAdditionally, it is matrix-free in the sense that it is not necessary to invert\nlinear systems and iteration is not required for nonlinear terms. Moreover, the\nscheme employs a fast summation algorithm that yields a method with a\ncomputational complexity of $\\mathcal{O}(N)$, where $N$ is the number of mesh\npoints along a direction. While much work has been done to explore the theory\nbehind these methods, their practicality in large scale computing environments\nis a largely unexplored topic. In this work, we explore the performance of\nthese methods by developing a domain decomposition algorithm suitable for\ndistributed memory systems along with shared memory algorithms. As a first\npass, we derive an artificial CFL condition that enforces a nearest-neighbor\ncommunication pattern and briefly discuss possible generalizations. We also\nanalyze several approaches for implementing the parallel algorithms by\noptimizing predominant loop structures and maximizing data reuse. Using a\nhybrid design that employs MPI and Kokkos for the distributed and shared memory\ncomponents of the algorithms, respectively, we show that our methods are\nefficient and can sustain an update rate $> 1\\times10^8$ DOF/node/s. We provide\nresults that demonstrate the scalability and versatility of our algorithms\nusing several different PDE test problems, including a nonlinear example, which\nemploys an adaptive time-stepping rule.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:59:38 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 23:56:12 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Christlieb", "Andrew J.", ""], ["Guthrey", "Pierson T.", ""], ["Sands", "William A.", ""], ["Thavappiragasm", "Mathialakan", ""]]}, {"id": "2007.03097", "submitter": "Julien Loiseau", "authors": "Julien Loiseau, Hyun Lim, Mark Alexander Kaltenborn, Oleg Korobkin,\n  Christopher M. Mauney, Irina Sagert, Wesley P. Even, Benjamin K. Bergen", "title": "FleCSPH: The Next Generation FleCSIble Parallel Computational\n  Infrastructure for Smoothed Particle Hydrodynamics", "comments": null, "journal-ref": null, "doi": "10.1016/j.softx.2020.100602", "report-no": null, "categories": "physics.comp-ph astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FleCSPH is a smoothed particle hydrodynamics simulation tool, based on the\ncompile-time configurable framework FleCSI. The asynchronous distributed tree\ntopology combined with a fast multipole method allows FleCSPH to efficiently\ncompute hydrodynamics and long range particle-particle interactions. FleCSPH\nprovides initial data generators, particle relaxation techniques, and standard\nevolution drivers, which can be easily modified and extended to user-specific\nsetups. Data input/output uses the H5part format, compatible with modern\nvisualization software.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 22:21:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Loiseau", "Julien", ""], ["Lim", "Hyun", ""], ["Kaltenborn", "Mark Alexander", ""], ["Korobkin", "Oleg", ""], ["Mauney", "Christopher M.", ""], ["Sagert", "Irina", ""], ["Even", "Wesley P.", ""], ["Bergen", "Benjamin K.", ""]]}, {"id": "2007.03131", "submitter": "Amel Awadelkarim", "authors": "Amel Awadelkarim, Johan Ugander", "title": "Prioritized Restreaming Algorithms for Balanced Graph Partitioning", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balanced graph partitioning is a critical step for many large-scale\ndistributed computations with relational data. As graph datasets have grown in\nsize and density, a range of highly-scalable balanced partitioning algorithms\nhave appeared to meet varied demands across different domains. As the starting\npoint for the present work, we observe that two recently introduced families of\niterative partitioners---those based on restreaming and those based on balanced\nlabel propagation (including Facebook's Social Hash Partitioner)---can be\nviewed through a common modular framework of design decisions. With the help of\nthis modular perspective, we find that a key combination of design decisions\nleads to a novel family of algorithms with notably better empirical performance\nthan any existing highly-scalable algorithm on a broad range of real-world\ngraphs. The resulting prioritized restreaming algorithms employ a constraint\nmanagement strategy based on multiplicative weights, borrowed from the\nrestreaming literature, while adopting notions of priority from balanced label\npropagation to optimize the ordering of the streaming process. Our experimental\nresults consider a range of stream orders, where a dynamic ordering based on\nwhat we call ambivalence is broadly the most performative in terms of the cut\nquality of the resulting balanced partitions, with a static ordering based on\ndegree being nearly as good.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 23:50:58 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Awadelkarim", "Amel", ""], ["Ugander", "Johan", ""]]}, {"id": "2007.03143", "submitter": "David Williams-Young", "authors": "David B. Williams-Young and Wibe A. de Jong and Hubertus J.J. van Dam\n  and Chao Yang", "title": "On the Efficient Evaluation of the Exchange Correlation Potential on\n  Graphics Processing Unit Clusters", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominance of Kohn-Sham density functional theory (KS-DFT) for the\ntheoretical treatment of large experimentally relevant systems in molecular\nchemistry and materials science relies primarily on the existence of efficient\nsoftware implementations which are capable of leveraging the latest advances in\nmodern high performance computing (HPC). With recent trends in HPC leading\ntowards in increasing reliance on heterogeneous accelerator based architectures\nsuch as graphics processing units (GPU), existing code bases must embrace these\narchitectural advances to maintain the high-levels of performance which have\ncome to be expected for these methods. In this work, we purpose a three-level\nparallelism scheme for the distributed numerical integration of the\nexchange-correlation (XC) potential in the Gaussian basis set discretization of\nthe Kohn-Sham equations on large computing clusters consisting of multiple GPUs\nper compute node. In addition, we purpose and demonstrate the efficacy of the\nuse of batched kernels, including batched level-3 BLAS operations, in achieving\nhigh-levels of performance on the GPU. We demonstrate the performance and\nscalability of the implementation of the purposed method in the NWChemEx\nsoftware package by comparing to the existing scalable CPU XC integration in\nNWChem.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 00:43:07 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Williams-Young", "David B.", ""], ["de Jong", "Wibe A.", ""], ["van Dam", "Hubertus J. J.", ""], ["Yang", "Chao", ""]]}, {"id": "2007.03179", "submitter": "Guyue Huang", "authors": "Guyue Huang, Guohao Dai, Yu Wang and Huazhong Yang", "title": "GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for\n  Graph Neural Networks", "comments": "To appear in International Conference for High Performance Computing,\n  Networking, Storage, and Analysis (SC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have achieved significant improvements in\nvarious domains. Sparse Matrix-Matrix multiplication (SpMM) is a fundamental\noperator in GNNs, which performs a multiplication between a sparse matrix and a\ndense matrix. Accelerating SpMM on parallel hardware like GPUs can face the\nfollowing challenges: From the GNN application perspective, the compatibility\nneeds to be considered. General GNN algorithms require SpMM-like operations\n(e.g., pooling) between matrices, which are not supported in current\nhigh-performance GPU libraries (e.g., Nvidia cuSPARSE). Moreover, the\nsophisticated preprocessing in previous implementations will lead to heavy data\nformat conversion overheads in GNN frameworks. From the GPU hardware\nperspective, optimizations in SpMV (Sparse Matrix-Vector) designs on GPUs do\nnot apply well to SpMM. SpMM exposes the column-wise parallelism in the dense\noutput matrix, but straightforward generalization from SpMV leads to\ninefficient, uncoalesced access to sparse matrix in global memory. The sparse\nrow data can be reused among GPU threads, which is neither possible in SpMM\ndesigns inherited from SpMV.\n  To tackle these challenges, we propose GE-SpMM. GE-SpMM performs SpMM-like\noperation on sparse matrices represented in the most common Compressed Sparse\nRow (CSR) format, so it can be embedded in GNN frameworks with no preprocessing\noverheads and support general GNN algorithms. We introduce the Coalesced Row\nCaching method to process columns in parallel and ensure coalesced access to\nsparse matrix data. We also present the Coarse-grained Warp Merging to reduce\nredundant data loading among GPU warps. Experiments on a real-world graph\ndataset show that GE-SpMM achieves up to 1.41X speedup over Nvidia cuSPARSE and\nup to 1.81X over GraphBLAST. We also embed GE-SpMM in GNN frameworks and get up\nto 3.67X speedup over popular GNN models like GCN and GraphSAGE.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 03:01:28 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Huang", "Guyue", ""], ["Dai", "Guohao", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""]]}, {"id": "2007.03220", "submitter": "Wenhao Lyu", "authors": "Wenhao Lyu, Youyou Lu, Jiwu Shu, Wei Zhao", "title": "Sapphire: Automatic Configuration Recommendation for Distributed Storage\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern distributed storage systems come with aplethora of configurable\nparameters that controlmodule behavior and affect system performance. Default\nsettings provided by developers are often suboptimal for specific user cases.\nTuning parameters can provide significant performance gains but is a difficult\ntask requiring profound experience and expertise, due to the immense number of\nconfigurable parameters, complex inner dependencies and non-linearsystem\nbehaviors. To overcome these difficulties, we propose an automatic\nsimulation-based approach, Sapphire, to recommend optimal configurations by\nleveraging machine learning and black-box optimization techniques. We evaluate\nSapphire on Ceph. Results show that Sapphire significantly boosts Ceph\nperformance to 2.2x compared to the default configuration.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 06:17:07 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Lyu", "Wenhao", ""], ["Lu", "Youyou", ""], ["Shu", "Jiwu", ""], ["Zhao", "Wei", ""]]}, {"id": "2007.03273", "submitter": "Saurav Prakash", "authors": "Saurav Prakash, Sagar Dhakal, Mustafa Akdeniz, A. Salman Avestimehr,\n  Nageen Himayat", "title": "Coded Computing for Federated Learning at the Edge", "comments": "Work accepted for presentation at the International Workshop on\n  Federated Learning for User Privacy and Data Confidentiality, in Conjunction\n  with ICML 2020 (FL-ICML'20). This work was part of Saurav Prakash's\n  internship projects at Intel. arXiv admin note: text overlap with\n  arXiv:2011.06223", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an exciting new paradigm that enables training a\nglobal model from data generated locally at the client nodes, without moving\nclient data to a centralized server. Performance of FL in a multi-access edge\ncomputing (MEC) network suffers from slow convergence due to heterogeneity and\nstochastic fluctuations in compute power and communication link qualities\nacross clients. A recent work, Coded Federated Learning (CFL), proposes to\nmitigate stragglers and speed up training for linear regression tasks by\nassigning redundant computations at the MEC server. Coding redundancy in CFL is\ncomputed by exploiting statistical properties of compute and communication\ndelays. We develop CodedFedL that addresses the difficult task of extending CFL\nto distributed non-linear regression and classification problems with\nmultioutput labels. The key innovation of our work is to exploit distributed\nkernel embedding using random Fourier features that transforms the training\ntask into distributed linear regression. We provide an analytical solution for\nload allocation, and demonstrate significant performance gains for CodedFedL\nthrough experiments over benchmark datasets using practical network parameters.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 08:20:47 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 10:36:58 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 20:09:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Prakash", "Saurav", ""], ["Dhakal", "Sagar", ""], ["Akdeniz", "Mustafa", ""], ["Avestimehr", "A. Salman", ""], ["Himayat", "Nageen", ""]]}, {"id": "2007.03291", "submitter": "Fabian Reiter", "authors": "Javier Esparza, Fabian Reiter", "title": "A Classification of Weak Asynchronous Models of Distributed Computing", "comments": "16 pages (+ 19 pages of appendices), 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conduct a systematic study of asynchronous models of distributed computing\nconsisting of identical finite-state devices that cooperate in a network to\ndecide if the network satisfies a given graph-theoretical property. Models\ndiscussed in the literature differ in the detection capabilities of the agents\nresiding at the nodes of the network (detecting the set of states of their\nneighbors, or counting the number of neighbors in each state), the notion of\nacceptance (acceptance by halting in a particular configuration, or by stable\nconsensus), the notion of step (synchronous move, interleaving, or arbitrary\ntiming), and the fairness assumptions (non-starving, or stochastic-like). We\nstudy the expressive power of the combinations of these features, and show that\nthe initially twenty possible combinations fit into seven equivalence classes.\nThe classification is the consequence of several equi-expressivity results with\na clear interpretation. In particular, we show that acceptance by halting\nconfiguration only has non-trivial expressive power if it is combined with\ncounting, and that synchronous and interleaving models have the same power as\nthose in which an arbitrary set of nodes can move at the same time. We also\nidentify simple graph properties that distinguish the expressive power of the\nseven classes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:12:42 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Esparza", "Javier", ""], ["Reiter", "Fabian", ""]]}, {"id": "2007.03298", "submitter": "Weiyan Wang", "authors": "Weiyan Wang, Cengguang Zhang, Liu Yang, Jiacheng Xia, Kai Chen, Kun\n  Tan", "title": "Divide-and-Shuffle Synchronization for Distributed Machine Learning", "comments": "15 pages, 5 figures, NeurIPS 2020 under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed Machine Learning suffers from the bottleneck of synchronization\nto all-reduce workers' updates. Previous works mainly consider better network\ntopology, gradient compression, or stale updates to speed up communication and\nrelieve the bottleneck. However, all these works ignore the importance of\nreducing the scale of synchronized elements and inevitable serial executed\noperators. To address the problem, our work proposes the Divide-and-Shuffle\nSynchronization(DS-Sync), which divides workers into several parallel groups\nand shuffles group members. DS-Sync only synchronizes the workers in the same\ngroup so that the scale of a group is much smaller. The shuffle of workers\nmaintains the algorithm's convergence speed, which is interpreted in theory.\nComprehensive experiments also show the significant improvements in the latest\nand popular models like Bert, WideResnet, and DeepFM on challenging datasets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:29:01 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Wang", "Weiyan", ""], ["Zhang", "Cengguang", ""], ["Yang", "Liu", ""], ["Xia", "Jiacheng", ""], ["Chen", "Kai", ""], ["Tan", "Kun", ""]]}, {"id": "2007.03451", "submitter": "Abhinav Bhatele", "authors": "Ian J. Costello, Abhinav Bhatele", "title": "Analytics of Longitudinal System Monitoring Data for Performance\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, several HPC facilities have started continuous monitoring of\ntheir systems and jobs to collect performance-related data for understanding\nperformance and operational efficiency. Such data can be used to optimize the\nperformance of individual jobs and the overall system by creating data-driven\nmodels that can predict the performance of pending jobs. In this paper, we\nmodel the performance of representative control jobs using longitudinal\nsystem-wide monitoring data to explore the causes of performance variability.\nUsing machine learning, we are able to predict the performance of unseen jobs\nbefore they are executed based on the current system state. We analyze these\nprediction models in great detail to identify the features that are dominant\npredictors of performance. We demonstrate that such models can be\napplication-agnostic and can be used for predicting performance of applications\nthat are not included in training.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 13:57:59 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Costello", "Ian J.", ""], ["Bhatele", "Abhinav", ""]]}, {"id": "2007.03490", "submitter": "Brian Bockelman", "authors": "Brian Bockelman, Andrea Ceccanti, Fabrizio Furano, Paul Millar, Dmitry\n  Litvintsev, Alessandra Forti", "title": "Third-party transfers in WLCG using HTTP", "comments": "7 pages, 3 figures, to appear in the proceedings of CHEP 2020", "journal-ref": null, "doi": "10.1051/epjconf/202024504031", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its earliest days, the Worldwide LHC Computational Grid (WLCG) has\nrelied on GridFTP to transfer data between sites. The announcement that Globus\nis dropping support of its open source Globus Toolkit (GT), which forms the\nbasis for several FTP client and servers, has created an opportunity to\nreevaluate the use of FTP. HTTP-TPC, an extension to HTTP compatible with\nWebDAV, has arisen as a strong contender for an alternative approach.\n  In this paper, we describe the HTTP-TPC protocol itself, along with the\ncurrent status of its support in different implementations, and the\ninteroperability testing done within the WLCG DOMA working group's TPC\nactivity. This protocol also provides the first real use-case for token-based\nauthorisation for this community. We will demonstrate the benefits of such\nauthorisation by showing how it allows HTTP-TPC to support new technologies\n(such as OAuth, OpenID Connect, Macaroons and SciTokens) without changing the\nprotocol. We will also discuss the next steps for HTTP-TPC and the plans to use\nthe protocol for WLCG transfers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:25:42 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Bockelman", "Brian", ""], ["Ceccanti", "Andrea", ""], ["Furano", "Fabrizio", ""], ["Millar", "Paul", ""], ["Litvintsev", "Dmitry", ""], ["Forti", "Alessandra", ""]]}, {"id": "2007.03505", "submitter": "Gabriele D'Angelo", "authors": "Mirko Zichichi, Stefano Ferretti, Gabriele D'Angelo", "title": "On the Efficiency of Decentralized File Storage for Personal Information\n  Management Systems", "comments": "To appear in the Proceedings of the 25th IEEE Symposium on Computers\n  and Communications (ISCC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.IR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an architecture, based on Distributed Ledger Technologies\n(DLTs) and Decentralized File Storage (DFS) systems, to support the use of\nPersonal Information Management Systems (PIMS). DLT and DFS are used to manage\ndata sensed by mobile users equipped with devices with sensing capability. DLTs\nguarantee the immutability, traceability and verifiability of references to\npersonal data, that are stored in DFS. In fact, the inclusion of data digests\nin the DLT makes it possible to obtain an unalterable reference and a\ntamper-proof log, while remaining compliant with the regulations on personal\ndata, i.e. GDPR. We provide an experimental evaluation on the feasibility of\nthe use of DFS. Three different scenarios have been studied: i) a proprietary\nIPFS approach with a dedicated node interfacing with the data producers, ii) a\npublic IPFS service and iii) Sia Skynet. Results show that through proper\nconfiguration of the system infrastructure, it is viable to build a\ndecentralized Personal Data Storage (PDS).\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:41:18 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Zichichi", "Mirko", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2007.03520", "submitter": "Huawei Huang", "authors": "Huawei Huang, Wei Kong, Sicong Zhou, Zibin Zheng, Song Guo", "title": "A Survey of State-of-the-Art on Blockchains: Theories, Modelings, and\n  Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To draw a roadmap of current research activities of the blockchain community,\nwe first conduct a brief overview of state-of-the-art blockchain surveys\npublished in the recent 5 years. We found that those surveys are basically\nstudying the blockchain-based applications, such as blockchain-assisted\nInternet of Things (IoT), business applications, security-enabled solutions,\nand many other applications in diverse fields. However, we think that a\ncomprehensive survey towards the essentials of blockchains by exploiting the\nstate-of-the-art theoretical modelings, analytic models, and useful experiment\ntools is still missing. To fill this gap, we perform a thorough survey by\nidentifying and classifying the most recent high-quality research outputs that\nare closely related to the theoretical findings and essential mechanisms of\nblockchain systems and networks. Several promising open issues are also\nsummarized finally for future research directions. We wish this survey can\nserve as a useful guideline for researchers, engineers, and educators about the\ncutting-edge development of blockchains in the perspectives of theories,\nmodelings, and tools.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 14:50:32 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 02:47:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huang", "Huawei", ""], ["Kong", "Wei", ""], ["Zhou", "Sicong", ""], ["Zheng", "Zibin", ""], ["Guo", "Song", ""]]}, {"id": "2007.03576", "submitter": "Mirko Myllykoski", "authors": "Mirko Myllykoski", "title": "A Task-based Multi-shift QR/QZ Algorithm with Aggressive Early Deflation", "comments": "34 pages, 19 figures, 8 tables. Minor corrections to text and\n  figures. Submitted to TOMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The QR algorithm is one of the three phases in the process of computing the\neigenvalues and the eigenvectors of a dense nonsymmetric matrix. This paper\ndescribes a task-based QR algorithm for reducing an upper Hessenberg matrix to\nreal Schur form. The task-based algorithm also supports generalized eigenvalue\nproblems (QZ algorithm) but this paper focuses more on the standard case. The\ntask-based algorithm inherits previous algorithmic improvements, such as\ntightly-coupled multi-shifts and Aggressive Early Deflation (AED), and also\nincorporates several new ideas that significantly improve the performance. This\nincludes the elimination of several synchronization points, the dynamic merging\nof previously separate computational steps, the shorting and the prioritization\nof the critical path, and the introduction of an experimental GPU support. The\ntask-based implementation is demonstrated to be significantly faster than\nmulti-threaded LAPACK and ScaLAPACK in both single-node and multi-node\nconfigurations on two different machines based on Intel and AMD CPUs. The\nimplementation is built on top of the StarPU runtime system and is part of an\nopen-source StarNEig library.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:53:50 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 12:07:51 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Myllykoski", "Mirko", ""]]}, {"id": "2007.03602", "submitter": "Brian Bockelman", "authors": "Brian Bockelman, Andrea Ceccanti, Ian Collier, Linda Cornwall, Thomas\n  Dack, Jaroslav Guenther, Mario Lassnig, Maarten Litmaath, Paul Millar, Mischa\n  Sall\\'e, Hannah Short, Jeny Teheran, Romain Wartel", "title": "WLCG Authorisation from X.509 to Tokens", "comments": "8 pages, 3 figures, to appear in the proceedings of CHEP 2019", "journal-ref": null, "doi": "10.1051/epjconf/202024503001", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The WLCG Authorisation Working Group was formed in July 2017 with the\nobjective to understand and meet the needs of a future-looking Authentication\nand Authorisation Infrastructure (AAI) for WLCG experiments. Much has changed\nsince the early 2000s when X.509 certificates presented the most suitable\nchoice for authorisation within the grid; progress in token based authorisation\nand identity federation has provided an interesting alternative with notable\nadvantages in usability and compatibility with external (commercial) partners.\nThe need for interoperability in this new model is paramount as infrastructures\nand research communities become increasingly interdependent. Over the past two\nyears, the working group has made significant steps towards identifying a\nsystem to meet the technical needs highlighted by the community during staged\nrequirements gathering activities. Enhancement work has been possible thanks to\nexternally funded projects, allowing existing AAI solutions to be adapted to\nour needs. A cornerstone of the infrastructure is the reliance on a common\ntoken schema in line with evolving standards and best practices, allowing for\nmaximum compatibility and easy cooperation with peer infrastructures and\nservices. We present the work of the group and an analysis of the anticipated\nchanges in authorisation model by moving from X.509 to token based\nauthorisation. A concrete example of token integration in Rucio is presented.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:39:30 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Bockelman", "Brian", ""], ["Ceccanti", "Andrea", ""], ["Collier", "Ian", ""], ["Cornwall", "Linda", ""], ["Dack", "Thomas", ""], ["Guenther", "Jaroslav", ""], ["Lassnig", "Mario", ""], ["Litmaath", "Maarten", ""], ["Millar", "Paul", ""], ["Sall\u00e9", "Mischa", ""], ["Short", "Hannah", ""], ["Teheran", "Jeny", ""], ["Wartel", "Romain", ""]]}, {"id": "2007.03731", "submitter": "Ran Ben Basat", "authors": "Ran Ben Basat, Sivaramakrishnan Ramanathan, Yuliang Li, Gianni\n  Antichi, Minlan Yu, Michael Mitzenmacher", "title": "PINT: Probabilistic In-band Network Telemetry", "comments": "To appear in ACM SIGCOMM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commodity network devices support adding in-band telemetry measurements into\ndata packets, enabling a wide range of applications, including network\ntroubleshooting, congestion control, and path tracing. However, including such\ninformation on packets adds significant overhead that impacts both flow\ncompletion times and application-level performance.\n  We introduce PINT, an in-band telemetry framework that bounds the amount of\ninformation added to each packet. PINT encodes the requested data on multiple\npackets, allowing per-packet overhead limits that can be as low as one bit. We\nanalyze PINT and prove performance bounds, including cases when multiple\nqueries are running simultaneously. PINT is implemented in P4 and can be\ndeployed on network devices. Using real topologies and traffic characteristics,\nwe show that PINT concurrently enables applications such as congestion control,\npath tracing, and computing tail latencies, using only sixteen bits per packet,\nwith performance comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:43:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Basat", "Ran Ben", ""], ["Ramanathan", "Sivaramakrishnan", ""], ["Li", "Yuliang", ""], ["Antichi", "Gianni", ""], ["Yu", "Minlan", ""], ["Mitzenmacher", "Michael", ""]]}, {"id": "2007.03776", "submitter": "Maciej Besta", "authors": "Maciej Besta, Jens Domke, Marcel Schneider, Marek Konieczny, Salvatore\n  Di Girolamo, Timo Schneider, Ankit Singla, Torsten Hoefler", "title": "High-Performance Routing with Multipathing and Path Diversity in\n  Ethernet and HPC Networks", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems (TPDS), 2021", "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent line of research into topology design focuses on lowering network\ndiameter. Many low-diameter topologies such as Slim Fly or Jellyfish that\nsubstantially reduce cost, power consumption, and latency have been proposed. A\nkey challenge in realizing the benefits of these topologies is routing. On one\nhand, these networks provide shorter path lengths than established topologies\nsuch as Clos or torus, leading to performance improvements. On the other hand,\nthe number of shortest paths between each pair of endpoints is much smaller\nthan in Clos, but there is a large number of non-minimal paths between router\npairs. This hampers or even makes it impossible to use established multipath\nrouting schemes such as ECMP. In this work, to facilitate high-performance\nrouting in modern networks, we analyze existing routing protocols and\narchitectures, focusing on how well they exploit the diversity of minimal and\nnon-minimal paths. We first develop a taxonomy of different forms of support\nfor multipathing and overall path diversity. Then, we analyze how existing\nrouting schemes support this diversity. Among others, we consider multipathing\nwith both shortest and non-shortest paths, support for disjoint paths, or\nenabling adaptivity. To address the ongoing convergence of HPC and \"Big Data\"\ndomains, we consider routing protocols developed for both HPC systems and for\ndata centers as well as general clusters. Thus, we cover architectures and\nprotocols based on Ethernet, InfiniBand, and other HPC networks such as\nMyrinet. Our review will foster developing future high-performance multipathing\nrouting protocols in supercomputers and data centers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 20:16:54 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 21:44:48 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 00:08:46 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Besta", "Maciej", ""], ["Domke", "Jens", ""], ["Schneider", "Marcel", ""], ["Konieczny", "Marek", ""], ["Di Girolamo", "Salvatore", ""], ["Schneider", "Timo", ""], ["Singla", "Ankit", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2007.03797", "submitter": "Lingyang Chu", "authors": "Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu,\n  Jian Pei, Yong Zhang", "title": "Personalized Cross-Silo Federated Learning on Non-IID Data", "comments": "Accepted by AAAI 2021. The API of this work is available at Huawei\n  Cloud (https://t.ly/nGN9), free registration is required before use", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-IID data present a tough challenge for federated learning. In this paper,\nwe explore a novel idea of facilitating pairwise collaborations between clients\nwith similar data. We propose FedAMP, a new method employing federated\nattentive message passing to facilitate similar clients to collaborate more. We\nestablish the convergence of FedAMP for both convex and non-convex models, and\npropose a heuristic method to further improve the performance of FedAMP when\nclients adopt deep neural networks as personalized models. Our extensive\nexperiments on benchmark data sets demonstrate the superior performance of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 21:38:36 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 22:32:43 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 17:25:31 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 20:45:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huang", "Yutao", ""], ["Chu", "Lingyang", ""], ["Zhou", "Zirui", ""], ["Wang", "Lanjun", ""], ["Liu", "Jiangchuan", ""], ["Pei", "Jian", ""], ["Zhang", "Yong", ""]]}, {"id": "2007.03812", "submitter": "Daniel Vial", "authors": "Daniel Vial, Sanjay Shakkottai, R. Srikant", "title": "Robust Multi-Agent Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown that agents facing independent instances of a\nstochastic $K$-armed bandit can collaborate to decrease regret. However, these\nworks assume that each agent always recommends their individual best-arm\nestimates to other agents, which is unrealistic in envisioned applications\n(machine faults in distributed computing or spam in social recommendation\nsystems). Hence, we generalize the setting to include $n$ honest and $m$\nmalicious agents who recommend best-arm estimates and arbitrary arms,\nrespectively. We first show that even with a single malicious agent, existing\ncollaboration-based algorithms fail to improve regret guarantees over a\nsingle-agent baseline. We propose a scheme where honest agents learn who is\nmalicious and dynamically reduce communication with (i.e., \"block\") them. We\nshow that collaboration indeed decreases regret for this algorithm, assuming\n$m$ is small compared to $K$ but without assumptions on malicious agents'\nbehavior, thus ensuring that our algorithm is robust against any malicious\nrecommendation strategy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:27:30 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 22:31:09 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Vial", "Daniel", ""], ["Shakkottai", "Sanjay", ""], ["Srikant", "R.", ""]]}, {"id": "2007.03915", "submitter": "Zhiguo Wan", "authors": "Yan Zhou, Zhiguo Wan, Zhangshuang Guan", "title": "Open-Pub: A Transparent yet Privacy-Preserving Academic Publication\n  System based on Blockchain", "comments": "Protoytpe implemented and detailed expeirmental performance results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.DL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Academic publications of latest research results are crucial to advance the\ndevelopment of all disciplines. However, there are several severe disadvantages\nin current academic publication systems. The first is the misconduct during the\npublication process due to the opaque paper review process. An anonymous\nreviewer may give biased comments to a paper without being noticed because the\ncomments are seldom published for evaluation. Second, access to research papers\nis restricted to only subscribers, and even the authors cannot access their own\npapers. To address the above problems, we propose Open-Pub, a decentralized,\ntransparent yet privacy-preserving academic publication scheme using the\nblockchain technology. In Open-Pub, we first design a threshold identity-based\ngroup signature (TIBGS) that protects identities of signers using verifiable\nsecret sharing. Then we develop a strong double-blind mechanism to protect the\nidentities of authors and reviewers. With this strong double-blind mechanism,\nauthors can choose to submit papers anonymously, and validators distribute\npapers anonymously to reviewers on the blockchain according to their research\ninterests. This process is publicly recorded and traceable on the blockchain so\nas to realize transparent peer preview. To evaluate its efficiency, we\nimplement Open-Pub based on Ethereum and conduct comprehensive experiments to\nevaluate its performance, including computation cost and processing delay. The\nexperiment results show that Open-Pub is highly efficient in computation and\nprocessing anonymous transactions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 06:38:56 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:22:02 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Zhou", "Yan", ""], ["Wan", "Zhiguo", ""], ["Guan", "Zhangshuang", ""]]}, {"id": "2007.03970", "submitter": "Matthias Langer", "authors": "Matthias Langer, Zhen He, Wenny Rahayu, and Yanbo Xue", "title": "Distributed Training of Deep Learning Models: A Taxonomic Perspective", "comments": null, "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, 2020,\n  Volume: 31, Issue: 12, Pages: 2802-2818", "doi": "10.1109/TPDS.2020.3003307", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed deep learning systems (DDLS) train deep neural network models by\nutilizing the distributed resources of a cluster. Developers of DDLS are\nrequired to make many decisions to process their particular workloads in their\nchosen environment efficiently. The advent of GPU-based deep learning, the\never-increasing size of datasets and deep neural network models, in combination\nwith the bandwidth constraints that exist in cluster environments require\ndevelopers of DDLS to be innovative in order to train high quality models\nquickly. Comparing DDLS side-by-side is difficult due to their extensive\nfeature lists and architectural deviations. We aim to shine some light on the\nfundamental principles that are at work when training deep neural networks in a\ncluster of independent machines by analyzing the general properties associated\nwith training deep learning models and how such workloads can be distributed in\na cluster to achieve collaborative model training. Thereby we provide an\noverview of the different techniques that are used by contemporary DDLS and\ndiscuss their influence and implications on the training process. To\nconceptualize and compare DDLS, we group different techniques into categories,\nthus establishing a taxonomy of distributed deep learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:56:58 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Langer", "Matthias", ""], ["He", "Zhen", ""], ["Rahayu", "Wenny", ""], ["Xue", "Yanbo", ""]]}, {"id": "2007.03972", "submitter": "Nitish Mital", "authors": "Nitish Mital, Cong Ling, Deniz Gunduz", "title": "Secure Distributed Matrix Computation with Discrete Fourier Transform", "comments": "Under journal review, 13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of secure distributed matrix computation (SDMC),\nwhere a \\textit{user} can query a function of data matrices generated at\ndistributed \\textit{source} nodes. We assume the availability of $N$ honest but\ncurious computation servers, which are connected to the sources, the user, and\neach other through orthogonal and reliable communication links. Our goal is to\nminimize the amount of data that must be transmitted from the sources to the\nservers, called the \\textit{upload cost}, while guaranteeing that no $T$\ncolluding servers can learn any information about the source matrices, and the\nuser cannot learn any information beyond the computation result. We first focus\non secure distributed matrix multiplication (SDMM), considering two matrices,\nand propose a novel polynomial coding scheme using the properties of finite\nfield discrete Fourier transform, which achieves an upload cost significantly\nlower than the existing results in the literature. We then generalize the\nproposed scheme to include straggler mitigation, as well as to the\nmultiplication of multiple matrices while keeping the input matrices, the\nintermediate computation results, as well as the final result secure against\nany $T$ colluding servers. We also consider a special case, called computation\nwith own data, where the data matrices used for computation belong to the user.\nIn this case, we drop the security requirement against the user, and show that\nthe proposed scheme achieves the minimal upload cost. We then propose methods\nfor performing other common matrix computations securely on distributed\nservers, including changing the parameters of secret sharing, matrix transpose,\nmatrix exponentiation, solving a linear system, and matrix inversion, which are\nthen used to show how arbitrary matrix polynomials can be computed securely on\ndistributed servers using the proposed procedure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 09:03:59 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Mital", "Nitish", ""], ["Ling", "Cong", ""], ["Gunduz", "Deniz", ""]]}, {"id": "2007.04066", "submitter": "Markus Levonyak", "authors": "Carlos Pachajoa, Christina Pacher, Markus Levonyak, Wilfried N.\n  Gansterer", "title": "Algorithm-Based Checkpoint-Recovery for the Conjugate Gradient Method", "comments": "11 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computers reach exascale and beyond, the incidence of faults will\nincrease. Solutions to this problem are an active research topic. We focus on\nstrategies to make the preconditioned conjugate gradient (PCG) solver resilient\nagainst node failures, specifically, the exact state reconstruction (ESR)\nmethod, which exploits redundancies in PCG.\n  Reducing the frequency at which redundant information is stored lessens the\nruntime overhead. However, after the node failure, the solver must restart from\nthe last iteration for which redundant information was stored, which increases\nrecovery overhead. This formulation highlights the method's similarities to\ncheckpoint-restart (CR). Thus, this method, which we call ESR with periodic\nstorage (ESRP), can be considered a form of algorithm-based checkpoint-restart.\nThe state is stored implicitly, by exploiting redundancy inherent to the\nalgorithm, rather than explicitly as in CR. We also minimize the amount of data\nto be stored and retrieved compared to CR, but additional computation is\nrequired to reconstruct the solver's state. In this paper, we describe the\nnecessary modifications to ESR to convert it into ESRP, and perform an\nexperimental evaluation.\n  We compare ESRP experimentally with previously-existing ESR and\napplication-level in-memory CR. Our results confirm that the overhead for ESR\nis reduced significantly, both in the failure-free case, and if node failures\nare introduced. In the former case, the overhead of ESRP is usually lower than\nthat of CR. However, CR is faster if node failures happen. We claim that these\ndifferences can be alleviated by the implementation of more appropriate\npreconditioners.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:34:11 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Pachajoa", "Carlos", ""], ["Pacher", "Christina", ""], ["Levonyak", "Markus", ""], ["Gansterer", "Wilfried N.", ""]]}, {"id": "2007.04069", "submitter": "Shiqing Fan", "authors": "Siyu Wang, Yi Rong, Shiqing Fan, Zhen Zheng, LanSong Diao, Guoping\n  Long, Jun Yang, Xiaoyong Liu, Wei Lin", "title": "Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for\n  DNN Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed growth in the computational requirements for\ntraining deep neural networks. Current approaches (e.g., data/model\nparallelism, pipeline parallelism) parallelize training tasks onto multiple\ndevices. However, these approaches always rely on specific deep learning\nframeworks and requires elaborate manual design, which make it difficult to\nmaintain and share between different type of models. In this paper, we propose\nAuto-MAP, a framework for exploring distributed execution plans for DNN\nworkloads, which can automatically discovering fast parallelization strategies\nthrough reinforcement learning on IR level of deep learning models. Efficient\nexploration remains a major challenge for reinforcement learning. We leverage\nDQN with task-specific pruning strategies to help efficiently explore the\nsearch space including optimized strategies. Our evaluation shows that Auto-MAP\ncan find the optimal solution in two hours, while achieving better throughput\non several NLP and convolution models.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 12:38:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wang", "Siyu", ""], ["Rong", "Yi", ""], ["Fan", "Shiqing", ""], ["Zheng", "Zhen", ""], ["Diao", "LanSong", ""], ["Long", "Guoping", ""], ["Yang", "Jun", ""], ["Liu", "Xiaoyong", ""], ["Lin", "Wei", ""]]}, {"id": "2007.04193", "submitter": "Shreya Ghosh Ms.", "authors": "Shreya Ghosh and Soumya Ghosh", "title": "Mobility driven Cloud-Fog-Edge Framework for Location-aware Services: A\n  Comprehensive Review", "comments": "17 pages, 4 figures, Submitted as a book-chapter in Mobile Edge\n  Computing, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the pervasiveness of IoT devices, smart-phones and improvement of\nlocation-tracking technologies huge volume of heterogeneous geo-tagged\n(location specific) data is generated which facilitates several location-aware\nservices. The analytics with this spatio-temporal (having location and time\ndimensions) datasets provide varied important services such as, smart\ntransportation, emergency services (health-care, national defence or urban\nplanning). While cloud paradigm is suitable for the capability of storage and\ncomputation, the major bottleneck is network connectivity loss. In\ntime-critical application, where real-time response is required for emergency\nservice-provisioning, such connectivity issues increases the latency and thus\naffects the overall quality of system (QoS). To overcome the issue, fog/ edge\ntopology has emerged, where partial computation is carried out in the edge of\nthe network to reduce the delay in communication. Such fog/ edge based system\ncomplements the cloud technology and extends the features of the system. This\nchapter discusses cloud-fog-edge based hierarchical collaborative framework,\nwhere several components are deployed to improve the QoS. On the other side.\nmobility is another critical factor to enhance the efficacy of such\nlocation-aware service provisioning. Therefore, this chapter discusses the\nconcerns and challenges associated with mobility-driven cloud-fog-edge based\nframework to provide several location-aware services to the end-users\nefficiently.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:55:50 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Ghosh", "Shreya", ""], ["Ghosh", "Soumya", ""]]}, {"id": "2007.04207", "submitter": "Altan Cakir", "authors": "Altan Cakir, Yousef Alkhanafseh, Esra Karabiyik, Erhan Kurubas, Rabia\n  Burcu Bunyak, Cenk Anil Bahcevan", "title": "Cloud Based Big Data DNS Analytics at Turknet", "comments": "To be published at Advances in Intelligent Systems and Computing\n  Series by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Domain Name System (DNS) is a hierarchical distributed naming system for\ncomputers, services, or any resource connected to the Internet. A DNS resolves\nqueries for URLs into IP addresses for the purpose of locating computer\nservices and devices worldwide. As of now, analytical applications with a vast\namount of DNS data are a challenging problem. Clustering the features of domain\ntraffic from a DNS data has given necessity to the need for more sophisticated\nanalytics platforms and tools because of the sensitivity of the data\ncharacterization. In this study, a cloud based big data application, based on\nApache Spark, on DNS data is proposed, as well as a periodic trend pattern\nbased on traffic to partition numerous domain names and region into separate\ngroups by the characteristics of their query traffic time series. Preliminary\nexperimental results on a Turknet DNS data in daily operations are discussed\nwith business intelligence applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 15:48:21 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Cakir", "Altan", ""], ["Alkhanafseh", "Yousef", ""], ["Karabiyik", "Esra", ""], ["Kurubas", "Erhan", ""], ["Bunyak", "Rabia Burcu", ""], ["Bahcevan", "Cenk Anil", ""]]}, {"id": "2007.04377", "submitter": "Joshua Daymude", "authors": "Joshua J. Daymude and Andr\\'ea W. Richa and Jamison W. Weber", "title": "Bio-Inspired Energy Distribution for Programmable Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems of active programmable matter, individual modules require a\nconstant supply of energy to participate in the system's collective behavior.\nThese systems are often powered by an external energy source accessible by at\nleast one module and rely on module-to-module power transfer to distribute\nenergy throughout the system. While much effort has gone into addressing\nchallenging aspects of power management in programmable matter hardware,\nalgorithmic theory for programmable matter has largely ignored the impact of\nenergy usage and distribution on algorithm feasibility and efficiency. In this\nwork, we present an algorithm for energy distribution in the amoebot model that\nis loosely inspired by the growth behavior of Bacillus subtilis bacterial\nbiofilms. These bacteria use chemical signaling to communicate their metabolic\nstates and regulate nutrient consumption throughout the biofilm, ensuring that\nall bacteria receive the nutrients they need. Our algorithm similarly uses\ncommunication to inhibit energy usage when there are starving modules, enabling\nall modules to receive sufficient energy to meet their demands. As a supporting\nbut independent result, we extend the amoebot model's well-established spanning\nforest primitive so that it self-stabilizes in the presence of crash failures.\nWe conclude by showing how this self-stabilizing primitive can be leveraged to\ncompose our energy distribution algorithm with existing amoebot model\nalgorithms, effectively generalizing previous work to also consider energy\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:06:33 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 18:28:37 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Daymude", "Joshua J.", ""], ["Richa", "Andr\u00e9a W.", ""], ["Weber", "Jamison W.", ""]]}, {"id": "2007.04457", "submitter": "Jieyang Chen", "authors": "Jieyang Chen, Lipeng Wan, Xin Liang, Ben Whitney, Qing Liu, David\n  Pugmire, Nicholas Thompson, Matthew Wolf, Todd Munson, Ian Foster, Scott\n  Klasky", "title": "Accelerating Multigrid-based Hierarchical Scientific Data Refactoring on\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth in scientific data and a widening gap between computational\nspeed and I/O bandwidth make it increasingly infeasible to store and share all\ndata produced by scientific simulations. Instead, we need methods for reducing\ndata volumes: ideally, methods that can scale data volumes adaptively so as to\nenable negotiation of performance and fidelity tradeoffs in different\nsituations. Multigrid-based hierarchical data representations hold promise as a\nsolution to this problem, allowing for flexible conversion between different\nfidelities so that, for example, data can be created at high fidelity and then\ntransferred or stored at lower fidelity via logically simple and mathematically\nsound operations. However, the effective use of such representations has been\nhindered until now by the relatively high costs of creating, accessing,\nreducing, and otherwise operating on such representations. We describe here\nhighly optimized data refactoring kernels for GPU accelerators that enable\nefficient creation and manipulation of data in multigrid-based hierarchical\nforms. We demonstrate that our optimized design can achieve up to 250 TB/s\naggregated data refactoring throughput -- 83% of theoretical peak -- on 1024\nnodes of the Summit supercomputer. We showcase our optimized design by applying\nit to a large-scale scientific visualization workflow and the MGARD lossy\ncompression software.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 22:24:36 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:05:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Jieyang", ""], ["Wan", "Lipeng", ""], ["Liang", "Xin", ""], ["Whitney", "Ben", ""], ["Liu", "Qing", ""], ["Pugmire", "David", ""], ["Thompson", "Nicholas", ""], ["Wolf", "Matthew", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""], ["Klasky", "Scott", ""]]}, {"id": "2007.04681", "submitter": "Lorenzo Federici Mr.", "authors": "Lorenzo Federici, Boris Benedikter, Alessandro Zavoli", "title": "EOS: a Parallel, Self-Adaptive, Multi-Population Evolutionary Algorithm\n  for Constrained Global Optimization", "comments": "2020 IEEE Congress on Evolutionary Computation (IEEE World Congress\n  on Computational Intelligence), Glasgow, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the main characteristics of the evolutionary optimization\ncode named EOS, Evolutionary Optimization at Sapienza, and its successful\napplication to challenging, real-world space trajectory optimization problems.\nEOS is a global optimization algorithm for constrained and unconstrained\nproblems of real-valued variables. It implements a number of improvements to\nthe well-known Differential Evolution (DE) algorithm, namely, a self-adaptation\nof the control parameters, an epidemic mechanism, a clustering technique, an\n$\\varepsilon$-constrained method to deal with nonlinear constraints, and a\nsynchronous island-model to handle multiple populations in parallel. The\nresults reported prove that EOSis capable of achieving increased performance\ncompared to state-of-the-art single-population self-adaptive DE algorithms when\napplied to high-dimensional or highly-constrained space trajectory optimization\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:19:22 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 10:27:23 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Federici", "Lorenzo", ""], ["Benedikter", "Boris", ""], ["Zavoli", "Alessandro", ""]]}, {"id": "2007.04766", "submitter": "Adrien Luxey", "authors": "Daniel Bosk (KTH), Y\\'erom-David Bromberg (WIDE, IRISA), Sonja\n  Buchegger (KTH), Adrien Luxey (WIDE, IRISA), Fran\\c{c}ois Ta\\\"iani (WIDE,\n  IRISA)", "title": "Spores: Stateless Predictive Onion Routing for E-Squads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass surveillance of the population by state agencies and corporate parties\nis now a well-known fact. Journalists and whistle-blowers still lack means to\ncircumvent global spying for the sake of their investigations. With Spores, we\npropose a way for journalists and their sources to plan a posteriori file\nexchanges when they physically meet. We leverage on the multiplication of\npersonal devices per capita to provide a lightweight, robust and fully\nanonymous decentralised file transfer protocol between users. Spores hinges on\nour novel concept of e-squads: one's personal devices, rendered intelligent by\ngossip communication protocols, can provide private and dependable services to\ntheir user. People's e-squads are federated into a novel onion routing network,\nable to withstand the inherent unreliability of personal appliances while\nproviding reliable routing. Spores' performances are competitive, and its\nprivacy properties of the communication outperform state of the art onion\nrouting strategies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:24:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Bosk", "Daniel", "", "KTH"], ["Bromberg", "Y\u00e9rom-David", "", "WIDE, IRISA"], ["Buchegger", "Sonja", "", "KTH"], ["Luxey", "Adrien", "", "WIDE, IRISA"], ["Ta\u00efani", "Fran\u00e7ois", "", "WIDE,\n  IRISA"]]}, {"id": "2007.04847", "submitter": "Vikas Jaiman", "authors": "Vikas Jaiman and Visara Urovi", "title": "A Consent Model for Blockchain-based Distributed Data Sharing Platforms", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.3014565", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern healthcare systems, being able to share electronic health records\nis crucial for providing quality care and for enabling a larger spectrum of\nhealth services. Health data sharing is dependent on obtaining individual\nconsent which, in turn, is hindered by a lack of resources. To this extend,\nblockchain-based platforms facilitate data sharing by inherently creating a\ntrusted distributed network of users. These users are enabled to share their\ndata without depending on the time and resources of specific players (such as\nthe health services). In blockchain-based platforms, data governance mechanisms\nbecome very important due to the need to specify and monitor data sharing and\ndata use conditions. In this paper, we present a blockchain-based data sharing\nconsent model for access control over individual health data. We use smart\ncontracts to dynamically represent the individual consent over health data and\nto enable data requesters to search and access them. The dynamic consent model\nextends upon two ontologies: the Data Use Ontology (DUO) which models the\nindividual consent of users and the Automatable Discovery and Access Matrix\n(ADA-M) which describes queries from data requesters. We deploy the model on\nEthereum blockchain and evaluate different data sharing scenarios. The\ncontribution of this paper is to create an individual consent model for health\ndata sharing platforms. Such a model guarantees that individual consent is\nrespected and that there is accountability for all the participants in the data\nsharing platform. The evaluation of our solution indicates that such a data\nsharing model provides a flexible approach to decide how the data is used by\ndata requesters. Our experimental evaluation shows that the proposed model is\nefficient and adapts to personalized access control policies in data sharing.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:51:41 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Jaiman", "Vikas", ""], ["Urovi", "Visara", ""]]}, {"id": "2007.04868", "submitter": "Filippo Mantovani", "authors": "Filippo Mantovani, Marta Garcia-Gasulla, Jos\\'e Gracia, Esteban\n  Stafford, Fabio Banchelli, Marc Josep-Fabrego, Joel Criado-Ledesma, Mathias\n  Nachtmann", "title": "Performance and energy consumption of HPC workloads on a cluster based\n  on Arm ThunderX2 CPU", "comments": null, "journal-ref": "Future Generation Computer Systems, 2020", "doi": "10.1016/j.future.2020.06.033", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the performance and energy consumption of an\nArm-based high-performance computing (HPC) system developed within the European\nproject Mont-Blanc 3. This system, called Dibona, has been integrated by\nATOS/Bull, and it is powered by the latest Marvell's CPU, ThunderX2. This CPU\nis the same one that powers the Astra supercomputer, the first Arm-based\nsupercomputer entering the Top500 in November 2018. We study from\nmicro-benchmarks up to large production codes. We include an interdisciplinary\nevaluation of three scientific applications (a finite-element fluid dynamics\ncode, a smoothed particle hydrodynamics code, and a lattice Boltzmann code) and\nthe Graph 500 benchmark, focusing on parallel and energy efficiency as well as\nstudying their scalability up to thousands of Armv8 cores. For comparison, we\nrun the same tests on state-of-the-art x86 nodes included in Dibona and the\nTier-0 supercomputer MareNostrum4. Our experiments show that the ThunderX2 has\na 25% lower performance on average, mainly due to its small vector unit yet\nsomewhat compensated by its 30% wider links between the CPU and the main\nmemory. We found that the software ecosystem of the Armv8 architecture is\ncomparable to the one available for Intel. Our results also show that ThunderX2\ndelivers similar or better energy-to-solution and scalability, proving that\nArm-based chips are legitimate contenders in the market of next-generation HPC\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:12:51 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 05:44:13 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Mantovani", "Filippo", ""], ["Garcia-Gasulla", "Marta", ""], ["Gracia", "Jos\u00e9", ""], ["Stafford", "Esteban", ""], ["Banchelli", "Fabio", ""], ["Josep-Fabrego", "Marc", ""], ["Criado-Ledesma", "Joel", ""], ["Nachtmann", "Mathias", ""]]}, {"id": "2007.04939", "submitter": "Cristian Ramon-Cortes", "authors": "Cristian Ramon-Cortes, Francesc Lordan, Jorge Ejarque, Rosa M. Badia", "title": "A Programming Model for Hybrid Workflows: combining Task-based Workflows\n  and Dataflows all-in-one", "comments": "Accepted in Future Generation Computer Systems (FGCS). Licensed under\n  CC-BY-NC-ND", "journal-ref": null, "doi": "10.1016/j.future.2020.07.007", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tries to reduce the effort of learning, deploying, and integrating\nseveral frameworks for the development of e-Science applications that combine\nsimulations with High-Performance Data Analytics (HPDA). We propose a way to\nextend task-based management systems to support continuous input and output\ndata to enable the combination of task-based workflows and dataflows (Hybrid\nWorkflows from now on) using a single programming model. Hence, developers can\nbuild complex Data Science workflows with different approaches depending on the\nrequirements. To illustrate the capabilities of Hybrid Workflows, we have built\na Distributed Stream Library and a fully functional prototype extending COMPSs,\na mature, general-purpose, task-based, parallel programming model. The library\ncan be easily integrated with existing task-based frameworks to provide support\nfor dataflows. Also, it provides a homogeneous, generic, and simple\nrepresentation of object and file streams in both Java and Python; enabling\ncomplex workflows to handle any data type without dealing directly with the\nstreaming back-end.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:09:50 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ramon-Cortes", "Cristian", ""], ["Lordan", "Francesc", ""], ["Ejarque", "Jorge", ""], ["Badia", "Rosa M.", ""]]}, {"id": "2007.05084", "submitter": "Kartik Sreenivasan", "authors": "Hongyi Wang, Kartik Sreenivasan, Shashank Rajput, Harit Vishwakarma,\n  Saurabh Agarwal, Jy-yong Sohn, Kangwook Lee, Dimitris Papailiopoulos", "title": "Attack of the Tails: Yes, You Really Can Backdoor Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its decentralized nature, Federated Learning (FL) lends itself to\nadversarial attacks in the form of backdoors during training. The goal of a\nbackdoor is to corrupt the performance of the trained model on specific\nsub-tasks (e.g., by classifying green cars as frogs). A range of FL backdoor\nattacks have been introduced in the literature, but also methods to defend\nagainst them, and it is currently an open question whether FL systems can be\ntailored to be robust against backdoors. In this work, we provide evidence to\nthe contrary. We first establish that, in the general case, robustness to\nbackdoors implies model robustness to adversarial examples, a major open\nproblem in itself. Furthermore, detecting the presence of a backdoor in a FL\nmodel is unlikely assuming first order oracles or polynomial time. We couple\nour theoretical results with a new family of backdoor attacks, which we refer\nto as edge-case backdoors. An edge-case backdoor forces a model to misclassify\non seemingly easy inputs that are however unlikely to be part of the training,\nor test data, i.e., they live on the tail of the input distribution. We explain\nhow these edge-case backdoors can lead to unsavory failures and may have\nserious repercussions on fairness, and exhibit that with careful tuning at the\nside of the adversary, one can insert them across a range of machine learning\ntasks (e.g., image classification, OCR, text prediction, sentiment analysis).\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:50:54 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Wang", "Hongyi", ""], ["Sreenivasan", "Kartik", ""], ["Rajput", "Shashank", ""], ["Vishwakarma", "Harit", ""], ["Agarwal", "Saurabh", ""], ["Sohn", "Jy-yong", ""], ["Lee", "Kangwook", ""], ["Papailiopoulos", "Dimitris", ""]]}, {"id": "2007.05111", "submitter": "Narges Dastanpour", "authors": "Mahsa Teymourzadeh, Roshanak Vahed, Soulmaz Alibeygi, and Narges\n  Dastanpour", "title": "Security in Wireless Sensor Networks: Issues and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A wireless sensor network (WSN) has important applications such as remote\nenvironmental monitoring and target tracking. In addition, Wireless Sensor\nnetworks is an emerging technology and have great potential to be employed in\ncritical situations like battlefields and commercial applications such as\nbuilding, traffic surveillance, habitat monitoring and smart homes and many\nmore scenarios. One of the major challenges wireless sensor networks face today\nis security. This has been enabled by the availability for a kind of possible\nattacks; the innate power and recall limit of sensor nodes earn customary\nsecurity solutions unfeasible. These sensors are equipped with wireless\ninterfaces with which they can communicate with one pther to form a network. In\nthis paper we present a survey of security issues in WSNs, address the state of\nthe art in research\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 23:58:59 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Teymourzadeh", "Mahsa", ""], ["Vahed", "Roshanak", ""], ["Alibeygi", "Soulmaz", ""], ["Dastanpour", "Narges", ""]]}, {"id": "2007.05212", "submitter": "Immanuel Kunz", "authors": "Immanuel Kunz, Valentina Casola, Angelika Schneider, Christian Banse\n  and Julian Sch\\\"utte", "title": "Towards Tracking Data Flows in Cloud Architectures", "comments": "11 pages, 5 figures, 2020 IEEE 13th International Conference on Cloud\n  Computing (CLOUD)", "journal-ref": null, "doi": "10.1109/CLOUD49709.2020.00066", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cloud services become central in an increasing number of applications,\nthey process and store more personal and business-critical data. At the same\ntime, privacy and compliance regulations such as GDPR, the EU ePrivacy\nregulation, PCI, and the upcoming EU Cybersecurity Act raise the bar for secure\nprocessing and traceability of critical data. Especially the demand to provide\ninformation about existing data records of an individual and the ability to\ndelete them on demand is central in privacy regulations. Common to these\nrequirements is that cloud providers must be able to track data as it flows\nacross the different services to ensure that it never moves outside of the\nlegitimate realm, and it is known at all times where a specific copy of a\nrecord that belongs to a specific individual or business process is located.\nHowever, current cloud architectures do neither provide the means to\nholistically track data flows across different services nor to enforce policies\non data flows. In this paper, we point out the deficits in the data flow\ntracking functionalities of major cloud providers by means of a set of\npractical experiments. We then generalize from these experiments introducing a\ngeneric architecture that aims at solving the problem of cloud-wide data flow\ntracking and show how it can be built in a Kubernetes-based prototype\nimplementation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 07:31:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kunz", "Immanuel", ""], ["Casola", "Valentina", ""], ["Schneider", "Angelika", ""], ["Banse", "Christian", ""], ["Sch\u00fctte", "Julian", ""]]}, {"id": "2007.05261", "submitter": "Evangelos Pournaras", "authors": "Jovan Nikolic, Nursultan Jubatyrov, Evangelos Pournaras", "title": "Self-healing Dilemmas in Distributed Systems: Fault Correction vs. Fault\n  Tolerance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MA cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale decentralized systems of autonomous agents interacting via\nasynchronous communication often experience the following self-healing dilemma:\nfault detection inherits network uncertainties making a remote faulty process\nindistinguishable from a slow process. In the case of a slow process without\nfault, fault correction is undesirable as it can trigger new faults that could\nbe prevented with fault tolerance that is a more proactive system maintenance.\nBut in the case of an actual faulty process, fault tolerance alone without\neventually correcting persistent faults can make systems underperforming.\nMeasuring, understanding and resolving such self-healing dilemmas is a timely\nchallenge and critical requirement given the rise of distributed ledgers, edge\ncomputing, the Internet of Things in several energy, transport and health\napplications. This paper contributes a novel and general-purpose modeling of\nfault scenarios during system runtime. They are used to accurately measure and\npredict inconsistencies generated by the undesirable outcomes of fault\ncorrection and fault tolerance as the means to improve self-healing of\nlarge-scale decentralized systems at the design phase. A rigorous experimental\nmethodology is designed that evaluates 696 experimental settings of different\nfault scales, fault profiles and fault detection thresholds in a prototyped\ndecentralized network of 3000 nodes. Almost 9 million measurements of\ninconsistencies were collected in a network, where each node monitors the\nhealth status of another node, while both can defect. The prediction\nperformance of the modeled fault scenarios is validated in a challenging\napplication scenario of decentralized and dynamic in-network data aggregation\nusing real-world data from a Smart Grid pilot project. Findings confirm the\norigin of inconsistencies at design phase.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 09:10:00 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:50:13 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 16:34:40 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nikolic", "Jovan", ""], ["Jubatyrov", "Nursultan", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2007.05316", "submitter": "Keren Censor-Hillel", "authors": "Keren Censor-Hillel and Fran\\c{c}ois Le Gall and Dean Leitersdorf", "title": "On Distributed Listing of Cliques", "comments": "To appear in PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show an $\\tilde{O}(n^{p/(p+2)})$-round algorithm in the \\congest model for\n\\emph{listing} of $K_p$ (a clique with $p$ nodes), for all $p =4, p\\geq 6$. For\n$p = 5$, we show an $\\tilde{O}(n^{3/4})$-round algorithm.\n  For $p=4$ and $p=5$, our results improve upon the previous state-of-the-art\nof $O(n^{5/6+o(1)})$ and $O(n^{21/22+o(1)})$, respectively, by Eden et al.\n[DISC 2019]. For all $p\\geq 6$, ours is the first sub-linear round algorithm\nfor $K_p$ listing.\n  We leverage the recent expander decomposition algorithm of Chang et al. [SODA\n2019] to create clusters with a good mixing time. Three key novelties in our\nalgorithm are: (1) we carefully iterate our listing process with coupled values\nof min-degree within the clusters and arboricity outside the clusters, (2) all\nthe listing is done within the cluster, which necessitates new techniques for\nbringing into the cluster the information about \\emph{all} edges that can\npotentially form $K_p$ instances with the cluster edges, and (3) within each\ncluster we use a sparsity-aware listing algorithm, which is faster than a\ngeneral listing algorithm and which we can allow the cluster to use since we\nmake sure to sparsify the graph as the iterations proceed.\n  As a byproduct of our algorithm, we show an \\emph{optimal} sparsity-aware\nalgorithm for $K_p$ listing, which runs in $\\tilde{\\Theta}(1 + m/n^{1 + 2/p})$\nrounds in the \\clique model. Previously, Pandurangan et al. [SPAA 2018], Chang\net al. [SODA 2019], and Censor-Hillel et al. [TCS 2020] showed sparsity-aware\nalgorithms for the case of $p = 3$, yet ours is the first such sparsity aware\nalgorithm for $p \\geq 4$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:27:32 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Censor-Hillel", "Keren", ""], ["Gall", "Fran\u00e7ois Le", ""], ["Leitersdorf", "Dean", ""]]}, {"id": "2007.05373", "submitter": "Joris Dugueperoux", "authors": "Joris Dugu\\'ep\\'eroux (DRUID), Tristan Allard (DRUID)", "title": "From Task Tuning to Task Assignment in Privacy-Preserving Crowdsourcing\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized worker profiles of crowdsourcing platforms may contain a large\namount of identifying and possibly sensitive personal information (e.g.,\npersonal preferences, skills, available slots, available devices) raising\nstrong privacy concerns. This led to the design of privacy-preserving\ncrowdsourcing platforms, that aim at enabling efficient crowd-sourcing\nprocesses while providing strong privacy guarantees even when the platform is\nnot fully trusted. In this paper, we propose two contributions. First, we\npropose the PKD algorithm with the goal of supporting a large variety of\naggregate usages of worker profiles within a privacy-preserving crowdsourcing\nplatform. The PKD algorithm combines together homomorphic encryption and\ndifferential privacy for computing (perturbed) partitions of the\nmulti-dimensional space of skills of the actual population of workers and a\n(perturbed) COUNT of workers per partition. Second, we propose to benefit from\nrecent progresses in Private Information Retrieval techniques in order to\ndesign a solution to task assignment that is both private and affordable. We\nperform an in-depth study of the problem of using PIR techniques for proposing\ntasks to workers, show that it is NP-Hard, and come up with the PKD PIR Packing\nheuristic that groups tasks together according to the partitioning output by\nthe PKD algorithm. In a nutshell, we design the PKD algorithm and the PKD PIR\nPacking heuristic, we prove formally their security against honest-but-curious\nworkers and/or platform, we analyze their complexities, and we demonstrate\ntheir quality and affordability in real-life scenarios through an extensive\nexperimental evaluation performed over both synthetic and realistic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:21:18 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dugu\u00e9p\u00e9roux", "Joris", "", "DRUID"], ["Allard", "Tristan", "", "DRUID"]]}, {"id": "2007.05505", "submitter": "Chetan Bansal", "authors": "Manish Shetty, Chetan Bansal, Sumit Kumar, Nikitha Rao, Nachiappan\n  Nagappan, Thomas Zimmermann", "title": "Neural Knowledge Extraction From Cloud Service Incidents", "comments": "To be published in the proceedings of ICSE 2021 - Software\n  Engineering in Practice Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, two paradigm shifts have reshaped the software industry -\nthe move from boxed products to services and the widespread adoption of cloud\ncomputing. This has had a huge impact on the software development life cycle\nand the DevOps processes. Particularly, incident management has become critical\nfor developing and operating large-scale services. Incidents are created to\nensure timely communication of service issues and, also, their resolution.\nPrior work on incident management has been heavily focused on the challenges\nwith incident triaging and de-duplication. In this work, we address the\nfundamental problem of structured knowledge extraction from service incidents.\nWe have built SoftNER, a framework for unsupervised knowledge extraction from\nservice incidents. We frame the knowledge extraction problem as a Named-entity\nRecognition task for extracting factual information. SoftNER leverages\nstructural patterns like key,value pairs and tables for bootstrapping the\ntraining data. Further, we build a novel multi-task learning based BiLSTM-CRF\nmodel which leverages not just the semantic context but also the data-types for\nnamed-entity extraction. We have deployed SoftNER at Microsoft, a major cloud\nservice provider and have evaluated it on more than 2 months of cloud\nincidents. We show that the unsupervised machine learning based approach has a\nhigh precision of 0.96. Our multi-task learning based deep learning model also\noutperforms the state of the art NER models. Lastly, using the knowledge\nextracted by SoftNER we are able to build significantly more accurate models\nfor important downstream tasks like incident triaging.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:33:07 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 06:47:23 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 07:07:03 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 21:56:16 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Shetty", "Manish", ""], ["Bansal", "Chetan", ""], ["Kumar", "Sumit", ""], ["Rao", "Nikitha", ""], ["Nagappan", "Nachiappan", ""], ["Zimmermann", "Thomas", ""]]}, {"id": "2007.05540", "submitter": "Ryan Levy", "authors": "Ryan Levy, Edgar Solomonik, Bryan K. Clark", "title": "Distributed-Memory DMRG via Sparse and Dense Parallel Tensor\n  Contractions", "comments": null, "journal-ref": "SC20: International Conference for High Performance Computing,\n  Networking, Storage and Analysis (SC), (2020) 319-332", "doi": "10.5555/3433701.3433732 10.1109/SC41405.2020.00028", "report-no": null, "categories": "cs.DC cond-mat.str-el physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Density Matrix Renormalization Group (DMRG) algorithm is a powerful tool\nfor solving eigenvalue problems to model quantum systems. DMRG relies on tensor\ncontractions and dense linear algebra to compute properties of condensed matter\nphysics systems. However, its efficient parallel implementation is challenging\ndue to limited concurrency, large memory footprint, and tensor sparsity. We\nmitigate these problems by implementing two new parallel approaches that handle\nblock sparsity arising in DMRG, via Cyclops, a distributed memory tensor\ncontraction library. We benchmark their performance on two physical systems\nusing the Blue Waters and Stampede2 supercomputers. Our DMRG performance is\nimproved by up to 5.9X in runtime and 99X in processing rate over ITensor, at\nroughly comparable computational resource use. This enables higher accuracy\ncalculations via larger tensors for quantum state approximation. We demonstrate\nthat despite having limited concurrency, DMRG is weakly scalable with the use\nof efficient parallel tensor contraction mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:00:03 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Levy", "Ryan", ""], ["Solomonik", "Edgar", ""], ["Clark", "Bryan K.", ""]]}, {"id": "2007.05553", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko A. Heikkil\\\"a, Antti Koskela, Kana Shimizu, Samuel Kaski, Antti\n  Honkela", "title": "Differentially private cross-silo federated learning", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict privacy is of paramount importance in distributed machine learning.\nFederated learning, with the main idea of communicating only what is needed for\nlearning, has been recently introduced as a general approach for distributed\nlearning to enhance learning and improve security. However, federated learning\nby itself does not guarantee any privacy for data subjects. To quantify and\ncontrol how much privacy is compromised in the worst-case, we can use\ndifferential privacy.\n  In this paper we combine additively homomorphic secure summation protocols\nwith differential privacy in the so-called cross-silo federated learning\nsetting. The goal is to learn complex models like neural networks while\nguaranteeing strict privacy for the individual data subjects. We demonstrate\nthat our proposed solutions give prediction accuracy that is comparable to the\nnon-distributed setting, and are fast enough to enable learning models with\nmillions of parameters in a reasonable time.\n  To enable learning under strict privacy guarantees that need privacy\namplification by subsampling, we present a general algorithm for oblivious\ndistributed subsampling. However, we also argue that when malicious parties are\npresent, a simple approach using distributed Poisson subsampling gives better\nprivacy.\n  Finally, we show that by leveraging random projections we can further\nscale-up our approach to larger models while suffering only a modest\nperformance loss.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:15:10 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Heikkil\u00e4", "Mikko A.", ""], ["Koskela", "Antti", ""], ["Shimizu", "Kana", ""], ["Kaski", "Samuel", ""], ["Honkela", "Antti", ""]]}, {"id": "2007.05788", "submitter": "Charles Garrocho", "authors": "Charles Tim Batista Garrocho, C\\'elio Marcio Soares Ferreira, Carlos\n  Frederico Marcelo da Cunha Cavalcanti, Ricardo Augusto Rabelo Oliveira", "title": "Blockchain-Based Process Control and Monitoring Architecture for\n  Vertical Integration of Industry 4.0", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Industrial Internet of Things is a new milestone that will require new\nindustry paradigms and investments. In this context, cyber-physical systems are\nconsidered the bridge to the fourth revolution. Centralized approaches and\nobservance of real-time constraints are two important challenges that must be\novercome for the advancement of Industry 4.0. To solve these problems, a\nblockchain-based vertical integration architecture of the process automation\nsystems is proposed in which it performs the control and monitoring of\nindustrial processes. Proof of concept experiments reveal the feasibility and\nperformance of the proposal.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 14:51:56 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 12:04:12 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Garrocho", "Charles Tim Batista", ""], ["Ferreira", "C\u00e9lio Marcio Soares", ""], ["Cavalcanti", "Carlos Frederico Marcelo da Cunha", ""], ["Oliveira", "Ricardo Augusto Rabelo", ""]]}, {"id": "2007.05832", "submitter": "Vikram Sreekanti", "authors": "Vikram Sreekanti, Harikaran Subbaraj, Chenggang Wu, Joseph E.\n  Gonzalez, Joseph M. Hellerstein", "title": "Optimizing Prediction Serving on Low-Latency Serverless Dataflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction serving systems are designed to provide large volumes of\nlow-latency inferences machine learning models. These systems mix data\nprocessing and computationally intensive model inference and benefit from\nmultiple heterogeneous processors and distributed computing resources. In this\npaper, we argue that a familiar dataflow API is well-suited to this\nlatency-sensitive task, and amenable to optimization even with unmodified\nblack-box ML models. We present the design of Cloudflow, a system that provides\nthis API and realizes it on an autoscaling serverless backend. Cloudflow\ntransparently implements performance-critical optimizations including operator\nfusion and competitive execution. Our evaluation shows that Cloudflow's\noptimizations yield significant performance improvements on synthetic workloads\nand that Cloudflow outperforms state-of-the-art prediction serving systems by\nas much as 2x on real-world prediction pipelines, meeting latency goals of\ndemanding applications like real-time video analysis.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 19:02:33 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Sreekanti", "Vikram", ""], ["Subbaraj", "Harikaran", ""], ["Wu", "Chenggang", ""], ["Gonzalez", "Joseph E.", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "2007.05948", "submitter": "Filipe Correia", "authors": "Tiago Matias (1), Filipe F. Correia (1,2), Jonas Fritzsch (4,5),\n  Justus Bogner (5,4), Hugo S. Ferreira (1,2), and Andr\\'e Restivo (1,3) ((1)\n  Faculty of Engineering, University of Porto, Portugal, (2) INESC TEC, FEUP\n  Campus, Portugal, (3) LIACC, FEUP Campus, Portugal, (4) Institute of Software\n  Technology, University of Stuttgart, Germany, (5) University of Applied\n  Sciences Reutlingen, Germany)", "title": "Determining Microservice Boundaries: A Case Study Using Static and\n  Dynamic Software Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of approaches have been proposed to identify service boundaries when\ndecomposing a monolith to microservices. However, only a few use systematic\nmethods and have been demonstrated with replicable empirical studies.\n  We describe a systematic approach for refactoring systems to microservice\narchitectures that uses static analysis to determine the system's structure and\ndynamic analysis to understand its actual behavior. A prototype of a tool was\nbuilt using this approach (MonoBreaker) and was used to conduct a case study on\na real-world software project.\n  The goal was to assess the feasibility and benefits of a systematic approach\nto decomposition that combines static and dynamic analysis. The three study\nparticipants regarded as positive the decomposition proposed by our tool, and\nconsidered that it showed improvements over approaches that rely only on static\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 09:52:13 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Matias", "Tiago", ""], ["Correia", "Filipe F.", ""], ["Fritzsch", "Jonas", ""], ["Bogner", "Justus", ""], ["Ferreira", "Hugo S.", ""], ["Restivo", "Andr\u00e9", ""]]}, {"id": "2007.06000", "submitter": "Xueying Wang", "authors": "Xueying Wang, Guangli Li, Xiao Dong, Jiansong Li, Lei Liu, and\n  Xiaobing Feng", "title": "Accelerating Deep Learning Inference with Cross-Layer Data Reuse on GPUs", "comments": "15 pages, 8 figures, to be published in Euro-Par 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating the deep learning inference is very important for real-time\napplications. In this paper, we propose a novel method to fuse the layers of\nconvolutional neural networks (CNNs) on Graphics Processing Units (GPUs), which\napplies data reuse analysis and access optimization in different levels of the\nmemory hierarchy. To achieve the balance between computation and memory access,\nwe explore the fusion opportunities in the CNN computation graph and propose\nthree fusion modes of convolutional neural networks: straight, merge and split.\nThen, an approach for generating efficient fused code is designed, which goes\ndeeper in multi-level memory usage for cross-layer data reuse. The\neffectiveness of our method is evaluated with the network layers from\nstate-of-the-art CNNs on two different GPU platforms, NVIDIA TITAN Xp and Tesla\nP4. The experiments show that the average speedup is 2.02x on representative\nstructures of CNNs, and 1.57x on end-to-end inference of SqueezeNet.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 14:31:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 14:15:06 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Xueying", ""], ["Li", "Guangli", ""], ["Dong", "Xiao", ""], ["Li", "Jiansong", ""], ["Liu", "Lei", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2007.06048", "submitter": "Mauricio Araya", "authors": "Jie Meng and Andreas Atle and Henri Calandra and Mauricio Araya-Polo", "title": "Minimod: A Finite Difference solver for Seismic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a benchmark application for seismic modeling using\nfinite difference method, which is namedMiniMod, a mini application for seismic\nmodeling. The purpose is to provide a benchmark suite that is, on one hand easy\nto build and adapt to the state of the art in programming models and changing\nhigh performance hardware landscape. On the other hand, the intention is to\nhave a proxy application to actual production geophysical exploration workloads\nfor Oil & Gas exploration, and other geosciences applications based on the wave\nequation. From top to bottom, we describe the design concepts, algorithms, code\nstructure of the application, and present the benchmark results on different\ncurrent computer architectures.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 17:44:58 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Meng", "Jie", ""], ["Atle", "Andreas", ""], ["Calandra", "Henri", ""], ["Araya-Polo", "Mauricio", ""]]}, {"id": "2007.06081", "submitter": "Yuejiao Sun", "authors": "Tianyi Chen, Xiao Jin, Yuejiao Sun, and Wotao Yin", "title": "VAFL: a Method of Vertical Asynchronous Federated Learning", "comments": "FL-ICML'20: Proc. of ICML Workshop on Federated Learning for User\n  Privacy and Data Confidentiality, July 2020", "journal-ref": "Proc. of ICML Workshop on Federated Learning for User Privacy and\n  Data Confidentiality, July 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horizontal Federated learning (FL) handles multi-client data that share the\nsame set of features, and vertical FL trains a better predictor that combine\nall the features from different clients. This paper targets solving vertical FL\nin an asynchronous fashion, and develops a simple FL method. The new method\nallows each client to run stochastic gradient algorithms without coordination\nwith other clients, so it is suitable for intermittent connectivity of clients.\nThis method further uses a new technique of perturbed local embedding to ensure\ndata privacy and improve communication efficiency. Theoretically, we present\nthe convergence rate and privacy level of our method for strongly convex,\nnonconvex and even nonsmooth objectives separately. Empirically, we apply our\nmethod to FL on various image and healthcare datasets. The results compare\nfavorably to centralized and synchronous FL methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:09:25 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chen", "Tianyi", ""], ["Jin", "Xiao", ""], ["Sun", "Yuejiao", ""], ["Yin", "Wotao", ""]]}, {"id": "2007.06134", "submitter": "Peng Jiang", "authors": "Peng Jiang, Gagan Agrawal", "title": "Adaptive Periodic Averaging: A Practical Approach to Reducing\n  Communication in Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic Gradient Descent (SGD) is the key learning algorithm for many\nmachine learning tasks. Because of its computational costs, there is a growing\ninterest in accelerating SGD on HPC resources like GPU clusters. However, the\nperformance of parallel SGD is still bottlenecked by the high communication\ncosts even with a fast connection among the machines. A simple approach to\nalleviating this problem, used in many existing efforts, is to perform\ncommunication every few iterations, using a constant averaging period. In this\npaper, we show that the optimal averaging period in terms of convergence and\ncommunication cost is not a constant, but instead varies over the course of the\nexecution. Specifically, we observe that reducing the variance of model\nparameters among the computing nodes is critical to the convergence of periodic\nparameter averaging SGD. Given a fixed communication budget, we show that it is\nmore beneficial to synchronize more frequently in early iterations to reduce\nthe initial large variance and synchronize less frequently in the later phase\nof the training process. We propose a practical algorithm, named ADaptive\nPeriodic parameter averaging SGD (ADPSGD), to achieve a smaller overall\nvariance of model parameters, and thus better convergence compared with the\nConstant Periodic parameter averaging SGD (CPSGD). We evaluate our method with\nseveral image classification benchmarks and show that our ADPSGD indeed\nachieves smaller training losses and higher test accuracies with smaller\ncommunication compared with CPSGD. Compared with gradient-quantization SGD, we\nshow that our algorithm achieves faster convergence with only half of the\ncommunication. Compared with full-communication SGD, our ADPSGD achieves 1:14x\nto 1:27x speedups with a 100Gbps connection among computing nodes, and the\nspeedups increase to 1:46x ~ 1:95x with a 10Gbps connection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 00:04:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 15:45:04 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Jiang", "Peng", ""], ["Agrawal", "Gagan", ""]]}, {"id": "2007.06225", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rihawi,\n  Yu Wang, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin\n  Steinegger, Debsindhu Bhowmik, Burkhard Rost", "title": "ProtTrans: Towards Cracking the Language of Life's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "comments": "17 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational biology and bioinformatics provide vast data gold-mines from\nprotein sequences, ideal for Language Models taken from NLP. These LMs reach\nfor new prediction frontiers at low inference costs. Here, we trained two\nauto-regressive models (Transformer-XL, XLNet) and four auto-encoder models\n(BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393\nbillion amino acids. The LMs were trained on the Summit supercomputer using\n5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that\nthe raw protein LM-embeddings from unlabeled data captured some biophysical\nfeatures of protein sequences. We validated the advantage of using the\nembeddings as exclusive input for several subsequent tasks. The first was a\nper-residue prediction of protein secondary structure (3-state accuracy\nQ3=81%-87%); the second were per-protein predictions of protein sub-cellular\nlocalization (ten-state accuracy: Q10=81%) and membrane vs. water-soluble\n(2-state accuracy Q2=91%). For the per-residue predictions the transfer of the\nmost informative embeddings (ProtT5) for the first time outperformed the\nstate-of-the-art without using evolutionary information thereby bypassing\nexpensive database searches. Taken together, the results implied that protein\nLMs learned some of the grammar of the language of life. To facilitate future\nwork, we released our models at https://github.com/agemagician/ProtTrans.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:54:20 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 21:18:05 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 20:18:22 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Heinzinger", "Michael", ""], ["Dallago", "Christian", ""], ["Rihawi", "Ghalia", ""], ["Wang", "Yu", ""], ["Jones", "Llion", ""], ["Gibbs", "Tom", ""], ["Feher", "Tamas", ""], ["Angerer", "Christoph", ""], ["Steinegger", "Martin", ""], ["Bhowmik", "Debsindhu", ""], ["Rost", "Burkhard", ""]]}, {"id": "2007.06248", "submitter": "Balasubramanian A.R", "authors": "A. R. Balasubramanian, Javier Esparza and Marijana Lazic", "title": "Complexity of Verification and Synthesis of Threshold Automata", "comments": "Accepted at ATVA20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Threshold automata are a formalism for modeling and analyzing fault-tolerant\ndistributed algorithms, recently introduced by Konnov, Veith, and Widder,\ndescribing protocols executed by a fixed but arbitrary number of processes. We\nconduct the first systematic study of the complexity of verification and\nsynthesis problems for threshold automata. We prove that the coverability,\nreachability, safety, and liveness problems are NP-complete, and that the\nbounded synthesis problem is $\\Sigma_p^2$ complete. A key to our results is a\nnovel characterization of the reachability relation of a threshold automaton as\nan existential Presburger formula. The characterization also leads to novel\nverification and synthesis algorithms. We report on an implementation, and\nprovide experimental results.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:53:18 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Balasubramanian", "A. R.", ""], ["Esparza", "Javier", ""], ["Lazic", "Marijana", ""]]}, {"id": "2007.06354", "submitter": "Sasikanth Avancha", "authors": "Sasikanth Avancha, Vasimuddin Md, Sanchit Misra, Ramanarayan Mohanty", "title": "Deep Graph Library Optimizations for Intel(R) x86 Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Deep Graph Library (DGL) was designed as a tool to enable structure\nlearning from graphs, by supporting a core abstraction for graphs, including\nthe popular Graph Neural Networks (GNN). DGL contains implementations of all\ncore graph operations for both the CPU and GPU. In this paper, we focus\nspecifically on CPU implementations and present performance analysis,\noptimizations and results across a set of GNN applications using the latest\nversion of DGL(0.4.3). Across 7 applications, we achieve speed-ups ranging\nfrom1 1.5x-13x over the baseline CPU implementations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 12:57:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Avancha", "Sasikanth", ""], ["Md", "Vasimuddin", ""], ["Misra", "Sanchit", ""], ["Mohanty", "Ramanarayan", ""]]}, {"id": "2007.06483", "submitter": "Alptekin Temizel", "authors": "Kadir Cenk Alpay, Kadir Berkay Aydemir, Alptekin Temizel", "title": "Accelerating Translational Image Registration for HDR Images on GPU", "comments": "Submitted for Consideration for Publication in High Performance\n  Computing Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Dynamic Range (HDR) images are generated using multiple exposures of a\nscene. When a hand-held camera is used to capture a static scene, these images\nneed to be aligned by globally shifting each image in both dimensions. For a\nfast and robust alignment, the shift amount is commonly calculated using Median\nThreshold Bitmaps (MTB) and creating an image pyramid. In this study, we\noptimize these computations using a parallel processing approach utilizing GPU.\nExperimental evaluation shows that the proposed implementation achieves a\nspeed-up of up to 6.24 times over the baseline multi-threaded CPU\nimplementation on the alignment of one image pair. The source code is available\nat https://github.com/kadircenk/WardMTBCuda\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:34:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Alpay", "Kadir Cenk", ""], ["Aydemir", "Kadir Berkay", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.06775", "submitter": "Jayashree Mohan", "authors": "Jayashree Mohan, Amar Phanishayee, Ashish Raniwala, Vijay Chidambaram", "title": "Analyzing and Mitigating Data Stalls in DNN Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training Deep Neural Networks (DNNs) is resource-intensive and\ntime-consuming. While prior research has explored many different ways of\nreducing DNN training time, the impact of input data pipeline, i.e., fetching\nraw data items from storage and performing data pre-processing in memory, has\nbeen relatively unexplored. This paper makes the following contributions: (1)\nWe present the first comprehensive analysis of how the input data pipeline\naffects the training time of widely-used computer vision and audio Deep Neural\nNetworks (DNNs), that typically involve complex data preprocessing. We analyze\nnine different models across three tasks and four datasets while varying\nfactors such as the amount of memory, number of CPU threads, storage device,\nGPU generation etc on servers that are a part of a large production cluster at\nMicrosoft. We find that in many cases, DNN training time is dominated by data\nstall time: time spent waiting for data to be fetched and preprocessed. (2) We\nbuild a tool, DS-Analyzer to precisely measure data stalls using a differential\ntechnique, and perform predictive what-if analysis on data stalls. (3) Finally,\nbased on the insights from our analysis, we design and implement three simple\nbut effective techniques in a data-loading library, CoorDL, to mitigate data\nstalls. Our experiments on a range of DNN tasks, models, datasets, and hardware\nconfigs show that when PyTorch uses CoorDL instead of the state-of-the-art DALI\ndata loading library, DNN training time is reduced significantly (by as much as\n5x on a single server).\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 02:16:56 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 16:20:47 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 18:35:27 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Mohan", "Jayashree", ""], ["Phanishayee", "Amar", ""], ["Raniwala", "Ashish", ""], ["Chidambaram", "Vijay", ""]]}, {"id": "2007.06892", "submitter": "Jose Gracia", "authors": "Huan Zhou and Jose Gracia and Ralf Schneider", "title": "MPI Collectives for Multi-core Clusters: Optimized Performance of the\n  Hybrid MPI+MPI Parallel Codes", "comments": "10 pages. Accepted for publication in ICPP Workshops 2019", "journal-ref": "ICPP Workshops 2019: 18:1-18:10", "doi": "10.1145/3339186.3339199", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of multi-/many-core processors in clusters advocates hybrid\nparallel programming, which combines Message Passing Interface (MPI) for\ninter-node parallelism with a shared memory model for on-node parallelism.\nCompared to the traditional hybrid approach of MPI plus OpenMP, a new, but\npromising hybrid approach of MPI plus MPI-3 shared-memory extensions (MPI+MPI)\nis gaining attraction. We describe an algorithmic approach for collective\noperations (with allgather and broadcast as concrete examples) in the context\nof hybrid MPI+MPI, so as to minimize memory consumption and memory copies. With\nthis approach, only one memory copy is maintained and shared by on-node\nprocesses. This allows the removal of unnecessary on-node copies of replicated\ndata that are required between MPI processes when the collectives are invoked\nin the context of pure MPI. We compare our approach of collectives for hybrid\nMPI+MPI and the traditional one for pure MPI, and also have a discussion on the\nsynchronization that is required to guarantee data integrity. The performance\nof our approach has been validated on a Cray XC40 system (Cray MPI) and NEC\ncluster (OpenMPI), showing that it achieves comparable or better performance\nfor allgather operations. We have further validated our approach with a\nstandard computational kernel, namely distributed matrix multiplication, and a\nBayesian Probabilistic Matrix Factorization code.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 08:03:27 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhou", "Huan", ""], ["Gracia", "Jose", ""], ["Schneider", "Ralf", ""]]}, {"id": "2007.07336", "submitter": "Andrew Kirby", "authors": "Andrew C. Kirby, Siddharth Samsi, Michael Jones, Albert Reuther,\n  Jeremy Kepner, Vijay Gadepally", "title": "Layer-Parallel Training with GPU Concurrency of Deep Residual Neural\n  Networks via Nonlinear Multigrid", "comments": "7 pages, 6 figures, 27 citations. Accepted to 2020 IEEE High\n  Performance Extreme Computing Conference - Outstanding Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multigrid Full Approximation Storage algorithm for solving Deep Residual\nNetworks is developed to enable neural network parallelized layer-wise training\nand concurrent computational kernel execution on GPUs. This work demonstrates a\n10.2x speedup over traditional layer-wise model parallelism techniques using\nthe same number of compute units.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 20:15:36 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 18:34:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kirby", "Andrew C.", ""], ["Samsi", "Siddharth", ""], ["Jones", "Michael", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""]]}, {"id": "2007.07366", "submitter": "Clive Cox", "authors": "Clive Cox, Dan Sun, Ellis Tarn, Animesh Singh, Rakesh Kelkar, David\n  Goodwin", "title": "Serverless inferencing on Kubernetes", "comments": "4 pages, 1 figure, presented at workshop on \"Challenges in Deploying\n  and Monitoring Machine Learning System\" at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisations are increasingly putting machine learning models into\nproduction at scale. The increasing popularity of serverless scale-to-zero\nparadigms presents an opportunity for deploying machine learning models to help\nmitigate infrastructure costs when many models may not be in continuous use. We\nwill discuss the KFServing project which builds on the KNative serverless\nparadigm to provide a serverless machine learning inference solution that\nallows a consistent and simple interface for data scientists to deploy their\nmodels. We will show how it solves the challenges of autoscaling GPU based\ninference and discuss some of the lessons learnt from using it in production.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 21:23:59 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 07:18:25 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Cox", "Clive", ""], ["Sun", "Dan", ""], ["Tarn", "Ellis", ""], ["Singh", "Animesh", ""], ["Kelkar", "Rakesh", ""], ["Goodwin", "David", ""]]}, {"id": "2007.07481", "submitter": "Jianyu Wang", "authors": "Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, H. Vincent Poor", "title": "Tackling the Objective Inconsistency Problem in Heterogeneous Federated\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In federated optimization, heterogeneity in the clients' local datasets and\ncomputation speeds results in large variations in the number of local updates\nperformed by each client in each communication round. Naive weighted\naggregation of such models causes objective inconsistency, that is, the global\nmodel converges to a stationary point of a mismatched objective function which\ncan be arbitrarily different from the true objective. This paper provides a\ngeneral framework to analyze the convergence of federated heterogeneous\noptimization algorithms. It subsumes previously proposed methods such as FedAvg\nand FedProx and provides the first principled understanding of the solution\nbias and the convergence slowdown due to objective inconsistency. Using\ninsights from this analysis, we propose FedNova, a normalized averaging method\nthat eliminates objective inconsistency while preserving fast error\nconvergence.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:01:23 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Wang", "Jianyu", ""], ["Liu", "Qinghua", ""], ["Liang", "Hao", ""], ["Joshi", "Gauri", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2007.07610", "submitter": "Lo\\\"ic Lannelongue", "authors": "Lo\\\"ic Lannelongue, Jason Grealey and Michael Inouye", "title": "Green Algorithms: Quantifying the carbon footprint of computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate change is profoundly affecting nearly all aspects of life on earth,\nincluding human societies, economies and health. Various human activities are\nresponsible for significant greenhouse gas emissions, including data centres\nand other sources of large-scale computation. Although many important\nscientific milestones have been achieved thanks to the development of\nhigh-performance computing, the resultant environmental impact has been\nunderappreciated. In this paper, we present a methodological framework to\nestimate the carbon footprint of any computational task in a standardised and\nreliable way, based on the processing time, type of computing cores, memory\navailable and the efficiency and location of the computing facility. Metrics to\ninterpret and contextualise greenhouse gas emissions are defined, including the\nequivalent distance travelled by car or plane as well as the number of\ntree-months necessary for carbon sequestration. We develop a freely available\nonline tool, Green Algorithms (www.green-algorithms.org), which enables a user\nto estimate and report the carbon footprint of their computation. The Green\nAlgorithms tool easily integrates with computational processes as it requires\nminimal information and does not interfere with existing code, while also\naccounting for a broad range of CPUs, GPUs, cloud computing, local servers and\ndesktop computers. Finally, by applying Green Algorithms, we quantify the\ngreenhouse gas emissions of algorithms used for particle physics simulations,\nweather forecasts and natural language processing. Taken together, this study\ndevelops a simple generalisable framework and freely available tool to quantify\nthe carbon footprint of nearly any computation. Combined with a series of\nrecommendations to minimise unnecessary CO2 emissions, we hope to raise\nawareness and facilitate greener computation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:05:33 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 07:59:58 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 11:24:24 GMT"}, {"version": "v4", "created": "Wed, 14 Oct 2020 10:08:52 GMT"}, {"version": "v5", "created": "Thu, 17 Dec 2020 14:30:09 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lannelongue", "Lo\u00efc", ""], ["Grealey", "Jason", ""], ["Inouye", "Michael", ""]]}, {"id": "2007.07638", "submitter": "Philipp J. Meyer", "authors": "Javier Esparza, Martin Helfrich, Stefan Jaax, Philipp J. Meyer", "title": "Peregrine 2.0: Explaining Correctness of Population Protocols through\n  Stage Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new version of Peregrine, the tool for the analysis and\nparameterized verification of population protocols introduced in [Blondin et\nal., CAV'2018]. Population protocols are a model of computation, intensely\nstudied by the distributed computing community, in which mobile anonymous\nagents interact stochastically to perform a task.\n  Peregrine 2.0 features a novel verification engine based on the construction\nof stage graphs. Stage graphs are proof certificates, introduced in [Blondin et\nal., CAV'2020], that are typically succinct and can be independently checked.\nMoreover, unlike the techniques of Peregrine 1.0, the stage graph methodology\ncan verify protocols whose executions never terminate, a class including recent\nfast majority protocols. Peregrine 2.0 also features a novel proof\nvisualization component that allows the user to interactively explore the stage\ngraph generated for a given protocol.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:53:31 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Esparza", "Javier", ""], ["Helfrich", "Martin", ""], ["Jaax", "Stefan", ""], ["Meyer", "Philipp J.", ""]]}, {"id": "2007.07642", "submitter": "Xiaodong Qi", "authors": "Xiaodong Qi and Yin Yang and Zhao Zhang and Cheqing Jin and Aoying\n  Zhou", "title": "LinSBFT: Linear-Communication One-Step BFT Protocol for Public\n  Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents LinSBFT, a Byzantine Fault Tolerance (BFT) protocol with\nthe capacity of processing over 2000 smart contract transactions per second in\nproduction. LinSBFT applies to a permissionless, public blockchain system, in\nwhich there is no public-key infrastructure, based on the classic PBFT with 4\nimprovements: (\\romannumeral1) LinSBFT achieves $O(n)$ worst-case communication\nvolume, in contract to PBFT's $O(n^4)$; (\\romannumeral2) LinSBFT rotates the\nleader of protocol randomly to reduce the risk of denial-of-service attacks on\nleader; and (\\romannumeral3) each run of LinSBFT finalizes one block, which is\nrobust against participants that are honest in one run of the protocol, and\ndishonest in another, and the set of participants is dynamic, which is update\nperiodically. (\\romannumeral4) LinSBFT helps the delayed nodes to catch up via\na synchronization mechanism to promise the liveness. Further, in the ordinary\ncase, LinSBFT involves only a single round of voting instead of two in PBFT,\nwhich reduces both communication overhead and confirmation time, and employs\nthe \\emph{proof-of-stake} scheme to reward all participants. Extensive\nexperiments using data obtained from the Ethereum demonstrate that LinSBFT\nconsistently and significantly outperforms existing in-production BFT protocols\nfor blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 11:58:19 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Qi", "Xiaodong", ""], ["Yang", "Yin", ""], ["Zhang", "Zhao", ""], ["Jin", "Cheqing", ""], ["Zhou", "Aoying", ""]]}, {"id": "2007.07753", "submitter": "Peter Hillmann", "authors": "Sandro Passarelli, Cem G\\\"undogan, Lars Stiemert, Matthias Schopp,\n  Peter Hillmann", "title": "NERD: Neural Network for Edict of Risky Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.DC cs.IR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber incidents can have a wide range of cause from a simple connection loss\nto an insistent attack. Once a potential cyber security incidents and system\nfailures have been identified, deciding how to proceed is often complex.\nEspecially, if the real cause is not directly in detail determinable.\nTherefore, we developed the concept of a Cyber Incident Handling Support\nSystem. The developed system is enriched with information by multiple sources\nsuch as intrusion detection systems and monitoring tools. It uses over twenty\nkey attributes like sync-package ratio to identify potential security incidents\nand to classify the data into different priority categories. Afterwards, the\nsystem uses artificial intelligence to support the further decision-making\nprocess and to generate corresponding reports to brief the Board of Directors.\nOriginating from this information, appropriate and detailed suggestions are\nmade regarding the causes and troubleshooting measures. Feedback from users\nregarding the problem solutions are included into future decision-making by\nusing labelled flow data as input for the learning process. The prototype shows\nthat the decision making can be sustainably improved and the Cyber Incident\nHandling process becomes much more effective.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 14:24:48 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Passarelli", "Sandro", ""], ["G\u00fcndogan", "Cem", ""], ["Stiemert", "Lars", ""], ["Schopp", "Matthias", ""], ["Hillmann", "Peter", ""]]}, {"id": "2007.07807", "submitter": "Spyridon Mastorakis", "authors": "Abderrahmen Mtibaa, Spyridon Mastorakis", "title": "NDNTP: A Named Data Networking Time Protocol", "comments": "This paper has been accepted for publication by the IEEE Network\n  Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Named Data Networking (NDN) architectural features, including multicast data\ndelivery, stateful forwarding, and in-network data caching, have shown promise\nfor applications such as video streaming and file sharing. However,\ncollaborative applications, requiring a multi-producer participation introduce\nnew NDN design challenges. In this paper, we highlight these challenges in the\ncontext of the Network Time Protocol (NTP) and one of its most widely-used\ndeployments for NTP server discovery, the NTP pool project. We discuss the\ndesign requirements for the support of NTP and NTP pool and present general\ndirections for the design of a time synchronization protocol over NDN, coined\nNamed Data Networking Time Protocol (NDNTP).\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 16:32:36 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Mtibaa", "Abderrahmen", ""], ["Mastorakis", "Spyridon", ""]]}, {"id": "2007.07977", "submitter": "Joshua Booth", "authors": "Joshua Dennis Booth and Phillip Lane", "title": "An Adaptive Self-Scheduling Loop Scheduler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many shared-memory parallel irregular applications, such as sparse linear\nalgebra and graph algorithms, depend on efficient loop scheduling (LS) in a\nfork-join manner despite that the work per loop iteration can greatly vary\ndepending on the application and the input. Because of its importance, many\ndifferent methods, e.g., workload-aware self-scheduling, and parameters, e.g.,\nchunk size, have been explored to achieve reasonable performance that requires\nexpert prior knowledge about the application and input. This work proposes a\nnew LS method that requires little to no expert knowledge to achieve speedups\nclose to those of tuned LS methods by self-managing chunk size based on a\nheuristic of workload variance and using work-stealing. This method, named\n\\ichunk, is implemented into libgomp for testing. It is evaluated against\nOpenMP's guided, dynamic, and taskloop methods and is evaluated against BinLPT\nand generic work-stealing on an array of applications that includes: a\nsynthetic benchmark, breadth-first search, K-Means, the molecular dynamics code\nLavaMD, and sparse matrix-vector multiplication. On 28 thread Intel system,\n\\ichunk is the only method to always be one of the top three LS methods. On\naverage across all applications, \\ichunk is within 5.4% of the best method and\nis even able to outperform other LS methods for breadth-first search and\nK-Means.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:20:20 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 21:18:40 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Booth", "Joshua Dennis", ""], ["Lane", "Phillip", ""]]}, {"id": "2007.08001", "submitter": "Xianfu Chen", "authors": "Xianfu Chen and Celimuge Wu and Zhi Liu and Ning Zhang and Yusheng Ji", "title": "Computation Offloading in Beyond 5G Networks: A Distributed Learning\n  Framework and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facing the trend of merging wireless communications and multi-access edge\ncomputing (MEC), this article studies computation offloading in the beyond\nfifth-generation networks. To address the technical challenges originating from\nthe uncertainties and the sharing of limited resource in an MEC system, we\nformulate the computation offloading problem as a multi-agent Markov decision\nprocess, for which a distributed learning framework is proposed. We present a\ncase study on resource orchestration in computation offloading to showcase the\npotentials of an online distributed reinforcement learning algorithm developed\nunder the proposed framework. Experimental results demonstrate that our\nlearning algorithm outperforms the benchmark resource orchestration algorithms.\nFurthermore, we outline the research directions worth in-depth investigation to\nminimize the time cost, which is one of the main practical issues that prevent\nthe implementation of the proposed distributed learning framework.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 21:25:15 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Xianfu", ""], ["Wu", "Celimuge", ""], ["Liu", "Zhi", ""], ["Zhang", "Ning", ""], ["Ji", "Yusheng", ""]]}, {"id": "2007.08061", "submitter": "Kanak Mahadik", "authors": "Kanak Mahadik, Qingyun Wu, Shuai Li, and Amit Sabne", "title": "Fast Distributed Bandits for Online Recommendation Systems", "comments": "13 pages, Appeared at ACM International Conference on Supercomputing\n  2020 (ICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandit algorithms are commonly used in recommender systems, where\ncontent popularity can change rapidly. These algorithms continuously learn\nlatent mappings between users and items, based on contexts associated with them\nboth. Recent recommendation algorithms that learn clustering or social\nstructures between users have exhibited higher recommendation accuracy.\nHowever, as the number of users and items in the environment increases, the\ntime required to generate recommendations deteriorates significantly. As a\nresult, these cannot be deployed in practice. The state-of-the-art distributed\nbandit algorithm - DCCB - relies on a peer-to-peer net-work to share\ninformation among distributed workers. However, this approach does not scale\nwell with the increasing number of users. Furthermore, it suffers from slow\ndiscovery of clusters, resulting in accuracy degradation. To address the above\nissues, this paper proposes a novel distributed bandit-based algorithm called\nDistCLUB. This algorithm lazily creates clusters in a distributed manner, and\ndramatically reduces the network data sharing requirement, achieving high\nscalability. Additionally, DistCLUB finds clusters much faster, achieving\nbetter accuracy than the state-of-the-art algorithm. Evaluation over both\nreal-world benchmarks and synthetic datasets shows that DistCLUB is on average\n8.87x faster than DCCB, and achieves 14.5% higher normalized prediction\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:18:08 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mahadik", "Kanak", ""], ["Wu", "Qingyun", ""], ["Li", "Shuai", ""], ["Sabne", "Amit", ""]]}, {"id": "2007.08066", "submitter": "Ryunosuke Nagayama", "authors": "Ryunosuke Nagayama, Ryohei Banno, Kazuyuki Shudo", "title": "Trail: A Blockchain Architecture for Light Nodes", "comments": "Proc. 25th IEEE Symposium on Computers and Communications (IEEE ISCC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bitcoin and Ethereum, nodes require large storage capacity to maintain all\nthe blockchain data, such as transactions, UTXOs, and account states. As of May\n2020, the storage size of the Bitcoin blockchain has expanded to 270 GB, and it\nwill continue to increase. This storage requirement is a major hurdle to\nbecoming a block proposer or validator. Although many studies have attempted to\nreduce the storage size, in the proposed methods, a node cannot keep all blocks\nor cannot generate a block. We propose an architecture called Trail that allows\nnodes to hold all blocks in a small storage and to generate and validate blocks\nand transactions. Trail does not depend on a consensus algorithm or fork choice\nrule. In this architecture, a client who issues transactions has the data to\nprove its own balances and can generate a transaction containing the proof of\nbalances. The nodes in Trail do not store transactions, UTXOs and account\nbalances: they keep only blocks. The blocksize is approximately 8 KB, which is\n100 times smaller than that of Bitcoin. Further, the block size is constant\nregardless of the number of accounts and the number of transactions. Compared\nto traditional blockchains, clients who issue transactions must store\nadditional data. However, we show that proper data archiving can keep the\naccount device storage size small. Trail allows more users to be block\nproposers and validators and improves the decentralization of the blockchain.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 01:38:33 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Nagayama", "Ryunosuke", ""], ["Banno", "Ryohei", ""], ["Shudo", "Kazuyuki", ""]]}, {"id": "2007.08077", "submitter": "Ali HeydariGorji", "authors": "Ali HeydariGorji, Siavash Rezaei, Mahdi Torabzadehkashi, Hossein\n  Bobarshad, Vladimir Alves, Pai H. Chou", "title": "HyperTune: Dynamic Hyperparameter Tuning For Efficient Distribution of\n  DNN Training Over Heterogeneous Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training is a novel approach to accelerate Deep Neural Networks\n(DNN) training, but common training libraries fall short of addressing the\ndistributed cases with heterogeneous processors or the cases where the\nprocessing nodes get interrupted by other workloads. This paper describes\ndistributed training of DNN on computational storage devices (CSD), which are\nNAND flash-based, high capacity data storage with internal processing engines.\nA CSD-based distributed architecture incorporates the advantages of federated\nlearning in terms of performance scalability, resiliency, and data privacy by\neliminating the unnecessary data movement between the storage device and the\nhost processor. The paper also describes Stannis, a DNN training framework that\nimproves on the shortcomings of existing distributed training frameworks by\ndynamically tuning the training hyperparameters in heterogeneous systems to\nmaintain the maximum overall processing speed in term of processed images per\nsecond and energy efficiency. Experimental results on image classification\ntraining benchmarks show up to 3.1x improvement in performance and 2.45x\nreduction in energy consumption when using Stannis plus CSD compare to the\ngeneric systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:12:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["HeydariGorji", "Ali", ""], ["Rezaei", "Siavash", ""], ["Torabzadehkashi", "Mahdi", ""], ["Bobarshad", "Hossein", ""], ["Alves", "Vladimir", ""], ["Chou", "Pai H.", ""]]}, {"id": "2007.08082", "submitter": "Yasuhiro Fujita", "authors": "Yasuhiro Fujita, Kota Uenishi, Avinash Ummadisingu, Prabhat Nagarajan,\n  Shimpei Masuda, and Mario Ynocente Castro", "title": "Distributed Reinforcement Learning of Targeted Grasping with Active\n  Vision for Mobile Manipulators", "comments": "Accepted at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing personal robots that can perform a diverse range of manipulation\ntasks in unstructured environments necessitates solving several challenges for\nrobotic grasping systems. We take a step towards this broader goal by\npresenting the first RL-based system, to our knowledge, for a mobile\nmanipulator that can (a) achieve targeted grasping generalizing to unseen\ntarget objects, (b) learn complex grasping strategies for cluttered scenes with\noccluded objects, and (c) perform active vision through its movable wrist\ncamera to better locate objects. The system is informed of the desired target\nobject in the form of a single, arbitrary-pose RGB image of that object,\nenabling the system to generalize to unseen objects without retraining. To\nachieve such a system, we combine several advances in deep reinforcement\nlearning and present a large-scale distributed training system using\nsynchronous SGD that seamlessly scales to multi-node, multi-GPU infrastructure\nto make rapid prototyping easier. We train and evaluate our system in a\nsimulated environment, identify key components for improving performance,\nanalyze its behaviors, and transfer to a real-world setup.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 02:47:48 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 08:59:39 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Fujita", "Yasuhiro", ""], ["Uenishi", "Kota", ""], ["Ummadisingu", "Avinash", ""], ["Nagarajan", "Prabhat", ""], ["Masuda", "Shimpei", ""], ["Castro", "Mario Ynocente", ""]]}, {"id": "2007.08084", "submitter": "Pedro Montealegre", "authors": "Laurent Feuilloley, Pierre Fraigniaud, Pedro Montealegre, Ivan\n  Rapaport, \\'Eric R\\'emila and Ioan Todinca", "title": "Local Certification of Graphs with Bounded Genus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naor, Parter, and Yogev [SODA 2020] recently designed a compiler for\nautomatically translating standard centralized interactive protocols to\ndistributed interactive protocols, as introduced by Kol, Oshman, and Saxena\n[PODC 2018]. In particular, by using this compiler, every linear-time algorithm\nfor deciding the membership to some fixed graph class can be translated into a\n$\\mathsf{dMAM}(O(\\log n))$ protocol for this class, that is, a distributed\ninteractive protocol with $O(\\log n)$-bit proof size in $n$-node graphs, and\nthree interactions between the (centralizer) computationally-unbounded but\nnon-trustable prover Merlin, and the (decentralized) randomized\ncomputationally-limited verifier Arthur. As a corollary, there is a\n$\\mathsf{dMAM}(O(\\log n))$ protocol for the class of planar graphs, as well as\nfor the class of graphs with bounded genus.\n  We show that there exists a distributed interactive protocol for the class of\ngraphs with bounded genus performing just a single interaction, from the prover\nto the verifier, yet preserving proof size of $O(\\log n)$ bits. This result\nalso holds for the class of graphs with bounded demi-genus, that is, graphs\nthat can be embedded on a non-orientable surface of bounded genus. The\ninteractive protocols described in this paper are actually proof-labeling\nschemes, i.e., a subclass of interactive protocols, previously introduced by\nKorman, Kutten, and Peleg [PODC 2005]. In particular, these schemes do not\nrequire any randomization from the verifier, and the proofs may often be\ncomputed a priori, at low cost, by the nodes themselves. Our results thus\nextend the recent proof-labeling scheme for planar graphs by Feuilloley et al.\n[PODC 2020], to graphs of bounded genus, and to graphs of bounded demigenus.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 03:01:35 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Fraigniaud", "Pierre", ""], ["Montealegre", "Pedro", ""], ["Rapaport", "Ivan", ""], ["R\u00e9mila", "\u00c9ric", ""], ["Todinca", "Ioan", ""]]}, {"id": "2007.08152", "submitter": "Rob van Glabbeek", "authors": "Rob van Glabbeek, Vincent Gramoli and Pierre Tholoniat", "title": "Feasibility of Cross-Chain Payment with Success Guarantees", "comments": "This is a summary of the work reported in arXiv:1912.04513", "journal-ref": "Proc. 32nd ACM Symposium on Parallelism in Algorithms and\n  Architectures, SPAA'20, July 2020, pp. 579-581", "doi": "10.1145/3350755.3400264", "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of cross-chain payment whereby customers of different\nescrows---implemented by a bank or a blockchain smart contract---successfully\ntransfer digital assets without trusting each other. Prior to this work,\ncross-chain payment problems did not require this success, or any form of\nprogress. We demonstrate that it is possible to solve this problem when\nassuming synchrony, in the sense that each message is guaranteed to arrive\nwithin a known amount of time, but impossible to solve without assuming\nsynchrony. Yet, we solve a weaker variant of this problem, where success is\nconditional on the patience of the participants, without assuming synchrony,\nand in the presence of Byzantine failures. We also discuss the relation with\nthe recently defined cross-chain deals.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:22:20 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["van Glabbeek", "Rob", ""], ["Gramoli", "Vincent", ""], ["Tholoniat", "Pierre", ""]]}, {"id": "2007.08187", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Daniele Varacca (LACL)", "title": "Processes, Systems \\& Tests: Defining Contextual Equivalences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, we would like to offer and defend a new template to\nstudy equivalences between programs -- in the particular framework of process\nalgebras for concurrent computation.We believe that our layered model of\ndevelopment will clarify the distinction that is too often left implicit\nbetween the tasks and duties of the programmer and of the tester. It will also\nenlighten pre-existing issues that have been running across process algebras as\ndiverse as the calculus of communicating systems, the \\(\\pi\\)-calculus -- also\nin its distributed version -- or mobile ambients.Our distinction starts by\nsubdividing the notion of process itself in three conceptually separated\nentities, that we call \\emph{Processes}, \\emph{Systems} and \\emph{Tests}.While\nthe role of what can be observed and the subtleties in the definitions of\ncongruences have been intensively studied, the fact that \\emph{not every\nprocess can be tested}, and that \\emph{the tester should have access to a\ndifferent set of tools than the programmer} is curiously left out, or at least\nnot often formally discussed.We argue that this blind spot comes from the\nunder-specification of contexts -- environments in which comparisons takes\nplace -- that play multiple distinct roles but supposedly always \\enquote{stay\nthe same}.We illustrate our statement with a simple Java example, the\n\\enquote{usual} concurrent languages, but also back it up with\n\\(\\lambda\\)-calculus and existing implementations of concurrent languages as\nwell.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 08:54:46 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 12:57:02 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 14:16:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LACL"], ["Varacca", "Daniele", "", "LACL"]]}, {"id": "2007.08217", "submitter": "Jion Hirose", "authors": "Jion Hirose and Junya Nakamura and Fukuhito Ooshita and Michiko Inoue", "title": "Gathering with a strong team in weakly Byzantine environments", "comments": "20 pages, 1 figure, 2 tables, 5 pseudo-codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the gathering problem requiring a team of mobile agents to gather at\na single node in arbitrary networks. The team consists of $k$ agents with\nunique identifiers (IDs), and $f$ of them are weakly Byzantine agents, which\nbehave arbitrarily except falsifying their identifiers. The agents move in\nsynchronous rounds and cannot leave any information on nodes. If the number of\nnodes $n$ is given to agents, the existing fastest algorithm tolerates any\nnumber of weakly Byzantine agents and achieves gathering with simultaneous\ntermination in $O(n^4\\cdot|\\Lambda_{good}|\\cdot X(n))$ rounds, where\n$|\\Lambda_{good}|$ is the length of the maximum ID of non-Byzantine agents and\n$X(n)$ is the number of rounds required to explore any network composed of $n$\nnodes. In this paper, we ask the question of whether we can reduce the time\ncomplexity if we have a strong team, i.e., a team with a few Byzantine agents,\nbecause not so many agents are subject to faults in practice. We give a\npositive answer to this question by proposing two algorithms in the case where\nat least $4f^2+9f+4$ agents exist. Both the algorithms take the upper bound $N$\nof $n$ as input. The first algorithm achieves gathering with non-simultaneous\ntermination in $O((f+|\\Lambda_{good}|)\\cdot X(N))$ rounds. The second algorithm\nachieves gathering with simultaneous termination in $O((f+|\\Lambda_{all}|)\\cdot\nX(N))$ rounds, where $|\\Lambda_{all}|$ is the length of the maximum ID of all\nagents. The second algorithm significantly reduces the time complexity compared\nto the existing one if $n$ is given to agents and\n$|\\Lambda_{all}|=O(|\\Lambda_{good}|)$ holds.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:43:47 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 09:21:02 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hirose", "Jion", ""], ["Nakamura", "Junya", ""], ["Ooshita", "Fukuhito", ""], ["Inoue", "Michiko", ""]]}, {"id": "2007.08253", "submitter": "V\\'aclav Rozho\\v{n}", "authors": "Mohsen Ghaffari, Christoph Grunau, V\\'aclav Rozho\\v{n}", "title": "Improved Deterministic Network Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network decomposition is a central tool in distributed graph algorithms. We\npresent two improvements on the state of the art for network decomposition,\nwhich thus lead to improvements in the (deterministic and randomized)\ncomplexity of several well-studied graph problems.\n  - We provide a deterministic distributed network decomposition algorithm with\n$O(\\log^5 n)$ round complexity, using $O(\\log n)$-bit messages. This improves\non the $O(\\log^7 n)$-round algorithm of Rozho\\v{n} and Ghaffari [STOC'20],\nwhich used large messages, and their $O(\\log^8 n)$-round algorithm with $O(\\log\nn)$-bit messages. This directly leads to similar improvements for a wide range\nof deterministic and randomized distributed algorithms, whose solution relies\non network decomposition, including the general distributed derandomization of\nGhaffari, Kuhn, and Harris [FOCS'18].\n  - One drawback of the algorithm of Rozho\\v{n} and Ghaffari, in the\n$\\mathsf{CONGEST}$ model, was its dependence on the length of the identifiers.\nBecause of this, for instance, the algorithm could not be used in the\nshattering framework in the $\\mathsf{CONGEST}$ model. Thus, the state of the\nart randomized complexity of several problems in this model remained with an\nadditive $2^{O(\\sqrt{\\log\\log n})}$ term, which was a clear leftover of the\nolder network decomposition complexity [Panconesi and Srinivasan STOC'92]. We\npresent a modified version that remedies this, constructing a decomposition\nwhose quality does not depend on the identifiers, and thus improves the\nrandomized round complexity for various problems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 10:57:29 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Grunau", "Christoph", ""], ["Rozho\u0148", "V\u00e1clav", ""]]}, {"id": "2007.08552", "submitter": "Diego Montezanti", "authors": "Diego Montezanti, Enzo Rucci, Armando De Giusti, Marcelo Naiouf,\n  Dolores Rexachs, Emilio Luque", "title": "Soft Errors Detection and Automatic Recovery based on Replication\n  combined with different Levels of Checkpointing", "comments": "26 pages, 3 figures (1 and 2 with subfigures), 4 tables, sent to\n  review to Future Generation Computer Systems", "journal-ref": "FGCS Volume 113, December 2020, Pages 240-254, ISSN 0167-739X", "doi": "10.1016/j.future.2020.07.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling faults is a growing concern in HPC. In future exascale systems, it\nis projected that silent undetected errors will occur several times a day,\nincreasing the occurrence of corrupted results. In this article, we propose\nSEDAR, which is a methodology that improves system reliability against\ntransient faults when running parallel message-passing applications. Our\napproach, based on process replication for detection, combined with different\nlevels of checkpointing for automatic recovery, has the goal of helping users\nof scientific applications to obtain executions with correct results. SEDAR is\nstructured in three levels: (1) only detection and safe-stop with notification;\n(2) recovery based on multiple system-level checkpoints; and (3) recovery based\non a single valid user-level checkpoint. As each of these variants supplies a\nparticular coverage but involves limitations and implementation costs, SEDAR\ncan be adapted to the needs of the system. In this work, a description of the\nmethodology is presented and the temporal behavior of employing each SEDAR\nstrategy is mathematically described, both in the absence and presence of\nfaults. A model that considers all the fault scenarios on a test application is\nintroduced to show the validity of the detection and recovery mechanisms. An\noverhead evaluation of each variant is performed with applications involving\ndifferent communication patterns; this is also used to extract guidelines about\nwhen it is beneficial to employ each SEDAR protection level. As a result, we\nshow its efficacy and viability to tolerate transient faults in target HPC\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:17:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 23:01:33 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Montezanti", "Diego", ""], ["Rucci", "Enzo", ""], ["De Giusti", "Armando", ""], ["Naiouf", "Marcelo", ""], ["Rexachs", "Dolores", ""], ["Luque", "Emilio", ""]]}, {"id": "2007.08563", "submitter": "Bingbing Li", "authors": "Bingbing Li, Santosh Pandey, Haowen Fang, Yanjun Lyv, Ji Li, Jieyang\n  Chen, Mimi Xie, Lipeng Wan, Hang Liu, Caiwen Ding", "title": "FTRANS: Energy-Efficient Acceleration of Transformers using FPGA", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In natural language processing (NLP), the \"Transformer\" architecture was\nproposed as the first transduction model replying entirely on self-attention\nmechanisms without using sequence-aligned recurrent neural networks (RNNs) or\nconvolution, and it achieved significant improvements for sequence to sequence\ntasks. The introduced intensive computation and storage of these pre-trained\nlanguage representations has impeded their popularity into computation and\nmemory-constrained devices. The field-programmable gate array (FPGA) is widely\nused to accelerate deep learning algorithms for its high parallelism and low\nlatency. However, the trained models are still too large to accommodate to an\nFPGA fabric. In this paper, we propose an efficient acceleration framework,\nFtrans, for transformer-based large scale language representations. Our\nframework includes enhanced block-circulant matrix (BCM)-based weight\nrepresentation to enable model compression on large-scale language\nrepresentations at the algorithm level with few accuracy degradation, and an\nacceleration design at the architecture level. Experimental results show that\nour proposed framework significantly reduces the model size of NLP models by up\nto 16 times. Our FPGA design achieves 27.07x and 81x improvement in performance\nand energy efficiency compared to CPU, and up to 8.80x improvement in energy\nefficiency compared to GPU.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 18:58:31 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Bingbing", ""], ["Pandey", "Santosh", ""], ["Fang", "Haowen", ""], ["Lyv", "Yanjun", ""], ["Li", "Ji", ""], ["Chen", "Jieyang", ""], ["Xie", "Mimi", ""], ["Wan", "Lipeng", ""], ["Liu", "Hang", ""], ["Ding", "Caiwen", ""]]}, {"id": "2007.08596", "submitter": "Truc Nguyen", "authors": "Lan N. Nguyen, Truc D. T. Nguyen, Thang N. Dinh, My T. Thai", "title": "OptChain: Optimal Transactions Placement for Scalable Blockchain\n  Sharding", "comments": null, "journal-ref": null, "doi": "10.1109/ICDCS.2019.00059", "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in blockchain sharding protocols is that more than 95%\ntransactions are cross-shard. Not only those cross-shard transactions degrade\nthe system throughput but also double the confirmation time, and exhaust an\nalready scarce network bandwidth. Are cross-shard transactions imminent for\nsharding schemes? In this paper, we propose a new sharding paradigm, called\nOptChain, in which cross-shard transactions are minimized, resulting in almost\ntwice faster confirmation time and throughput. By treating transactions as a\nstream of nodes in an online graph, OptChain utilizes a lightweight and\non-the-fly transaction placement method to group both related and soon-related\ntransactions into the same shards. At the same time, OptChain maintains a\ntemporal balance among shards to guarantee the high parallelism. Our\ncomprehensive and large-scale simulation using Oversim P2P library confirms a\nsignificant boost in performance with up to 10 folds reduction in cross-shard\ntransactions, more than twice reduction in confirmation time, and 50% increase\nin throughput. When combined with Omniledger sharding protocol, OptChain\ndelivers a 6000 transactions per second throughput with 10.5s confirmation\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 19:54:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nguyen", "Lan N.", ""], ["Nguyen", "Truc D. T.", ""], ["Dinh", "Thang N.", ""], ["Thai", "My T.", ""]]}, {"id": "2007.08695", "submitter": "Oussama Smimite", "authors": "Oussama Smimite and Karim Afdel", "title": "Containers Placement and Migration on Cloud System", "comments": "10 pages , 16 figures", "journal-ref": "International Journal of Computer Applications 176(35):9-18, July\n  2020", "doi": "10.5120/ijca2020920493", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many businesses are using cloud computing to obtain an entire IT\ninfrastructure remotely while delegating its management to a third party. The\nprovider of this architecture ensures the operation and maintenance of the\nservices while offering management capabilities via web consoles.These\nproviders offer solutions based on bare metal or virtualization platforms\n(mainly virtual machines). Recently, a new type of virtualization-based on\ncontainerization technology has emerged. Containers can be deployed on bare\nmetal servers or in virtual machines. The migration of virtual machines (VMs)\nand Containers in Dynamic Resource Management (DRM) is a crucial factor in\nminimizing the operating costs of data centers by reducing their energy\nconsumption and subsequently limiting their impact on climate change.In this\narticle, live migration for both types of virtualization will be studied. for\nthat, container placement and migration algorithms are proposed, which takes\ninto account the QoS requirements of different users in order to minimize\nenergy consumption. Thus, a dynamic approach is suggested based on a threshold\nof RAM usage for host and virtual machines in the data center to avoid\nunnecessary power consumption. In this paper, the proposed work is compared\nwith VM/Container placement and migration methods, the results of the\nexperiment indicate that using container migration instead of VMs demonstrates\na reduction in power consumption, and also reduces the migration time which\nimpacts QoS and reduces SLA violation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 23:59:30 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Smimite", "Oussama", ""], ["Afdel", "Karim", ""]]}, {"id": "2007.08725", "submitter": "Shilong Wang", "authors": "Shilong Wang (1), Hang Liu (2), Anil Gaihre (2), Hengyong Yu (1) ((1)\n  University of Massachusetts Lowell, (2) Stevens Institute of Technology)", "title": "EZLDA: Efficient and Scalable LDA on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LDA is a statistical approach for topic modeling with a wide range of\napplications. However, there exist very few attempts to accelerate LDA on GPUs\nwhich come with exceptional computing and memory throughput capabilities. To\nthis end, we introduce EZLDA which achieves efficient and scalable LDA training\non GPUs with the following three contributions: First, EZLDA introduces\nthree-branch sampling method which takes advantage of the convergence\nheterogeneity of various tokens to reduce the redundant sampling task. Second,\nto enable sparsity-aware format for both D and W on GPUs with fast sampling and\nupdating, we introduce hybrid format for W along with corresponding token\npartition to T and inverted index designs. Third, we design a hierarchical\nworkload balancing solution to address the extremely skewed workload imbalance\nproblem on GPU and scaleEZLDA across multiple GPUs. Taken together, EZLDA\nachieves superior performance over the state-of-the-art attempts with lower\nmemory consumption.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 02:40:03 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Wang", "Shilong", ""], ["Liu", "Hang", ""], ["Gaihre", "Anil", ""], ["Yu", "Hengyong", ""]]}, {"id": "2007.08803", "submitter": "Mahdi Soleymani", "authors": "Mahdi Soleymani, Hessam Mahdavifar, A. Salman Avestimehr", "title": "Privacy-Preserving Distributed Learning in the Analog Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the critical problem of distributed learning over data while\nkeeping it private from the computational servers. The state-of-the-art\napproaches to this problem rely on quantizing the data into a finite field, so\nthat the cryptographic approaches for secure multiparty computing can then be\nemployed. These approaches, however, can result in substantial accuracy losses\ndue to fixed-point representation of the data and computation overflows. To\naddress these critical issues, we propose a novel algorithm to solve the\nproblem when data is in the analog domain, e.g., the field of real/complex\nnumbers. We characterize the privacy of the data from both\ninformation-theoretic and cryptographic perspectives, while establishing a\nconnection between the two notions in the analog domain. More specifically, the\nwell-known connection between the distinguishing security (DS) and the mutual\ninformation security (MIS) metrics is extended from the discrete domain to the\ncontinues domain. This is then utilized to bound the amount of information\nabout the data leaked to the servers in our protocol, in terms of the DS\nmetric, using well-known results on the capacity of single-input\nmultiple-output (SIMO) channel with correlated noise. It is shown how the\nproposed framework can be adopted to do computation tasks when data is\nrepresented using floating-point numbers. We then show that this leads to a\nfundamental trade-off between the privacy level of data and accuracy of the\nresult. As an application, we also show how to train a machine learning model\nwhile keeping the data as well as the trained model private. Then numerical\nresults are shown for experiments on the MNIST dataset. Furthermore,\nexperimental advantages are shown comparing to fixed-point implementations over\nfinite fields.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 07:56:39 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Soleymani", "Mahdi", ""], ["Mahdavifar", "Hessam", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2007.09072", "submitter": "Xu Chen", "authors": "Xin Tang and Xu Chen and Liekang Zeng and Shuai Yu and Lin Chen", "title": "Joint Multi-User DNN Partitioning and Computational Resource Allocation\n  for Collaborative Edge Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Edge Computing (MEC) has emerged as a promising supporting\narchitecture providing a variety of resources to the network edge, thus acting\nas an enabler for edge intelligence services empowering massive mobile and\nInternet of Things (IoT) devices with AI capability. With the assistance of\nedge servers, user equipments (UEs) are able to run deep neural network (DNN)\nbased AI applications, which are generally resource-hungry and\ncompute-intensive, such that an individual UE can hardly afford by itself in\nreal time. However the resources in each individual edge server are typically\nlimited. Therefore, any resource optimization involving edge servers is by\nnature a resource-constrained optimization problem and needs to be tackled in\nsuch realistic context. Motivated by this observation, we investigate the\noptimization problem of DNN partitioning (an emerging DNN offloading scheme) in\na realistic multi-user resource-constrained condition that rarely considered in\nprevious works. Despite the extremely large solution space, we reveal several\nproperties of this specific optimization problem of joint multi-UE DNN\npartitioning and computational resource allocation. We propose an algorithm\ncalled Iterative Alternating Optimization (IAO) that can achieve the optimal\nsolution in polynomial time. In addition, we present rigorous theoretic\nanalysis of our algorithm in terms of time complexity and performance under\nrealistic estimation error. Moreover, we build a prototype that implements our\nframework and conduct extensive experiments using realistic DNN models, whose\nresults demonstrate its effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 09:40:13 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tang", "Xin", ""], ["Chen", "Xu", ""], ["Zeng", "Liekang", ""], ["Yu", "Shuai", ""], ["Chen", "Lin", ""]]}, {"id": "2007.09227", "submitter": "Yuxiao Hu", "authors": "Sean Wang, Yuxiao Hu, Jason Wu", "title": "KubeEdge.AI: AI Platform for Edge Devices", "comments": "https://www.embedded-ai.org", "journal-ref": "Embedded AI Summit 2019, Shenzhen, china", "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for smartness in embedded systems has been mounting up drastically\nin the past few years. Embedded system today must address the fundamental\nchallenges introduced by cloud computing and artificial intelligence. KubeEdge\n[1] is an edge computing framework build on top of Kubernetes [2]. It provides\ncompute resource management, deployment, runtime and operation capabilities on\ngeo-located edge computing resources, from the cloud, which is a natural fit\nfor embedded systems. Here we propose KubeEdge.AI, an edge AI framework on top\nof KubeEdge. It provides a set of key modules and interfaces: a data handling\nand processing engine, a concise AI runtime, a decision engine, and a\ndistributed data query interface. KubeEdge.AI will help reduce the burdens for\ndeveloping specific edge/embedded AI systems and promote edge-cloud\ncoordination and synergy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 23:36:23 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Sean", ""], ["Hu", "Yuxiao", ""], ["Wu", "Jason", ""]]}, {"id": "2007.09361", "submitter": "Anish Krishnakumar", "authors": "Anish Krishnakumar, Samet E. Arda, A. Alper Goksoy, Sumit K. Mandal,\n  Umit Y. Ogras, Anderson L. Sartor, Radu Marculescu", "title": "Runtime Task Scheduling using Imitation Learning for Heterogeneous\n  Many-Core Systems", "comments": "14 pages, 12 figures, 8 tables. Accepted for publication in Embedded\n  Systems Week CODES+ISSS 2020 (Special Issue in IEEE TCAD)", "journal-ref": null, "doi": "10.1109/TCAD.2020.3012861", "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific systems-on-chip, a class of heterogeneous many-core systems,\nare recognized as a key approach to narrow down the performance and\nenergy-efficiency gap between custom hardware accelerators and programmable\nprocessors. Reaching the full potential of these architectures depends\ncritically on optimally scheduling the applications to available resources at\nruntime. Existing optimization-based techniques cannot achieve this objective\nat runtime due to the combinatorial nature of the task scheduling problem. As\nthe main theoretical contribution, this paper poses scheduling as a\nclassification problem and proposes a hierarchical imitation learning\n(IL)-based scheduler that learns from an Oracle to maximize the performance of\nmultiple domain-specific applications. Extensive evaluations with six streaming\napplications from wireless communications and radar domains show that the\nproposed IL-based scheduler approximates an offline Oracle policy with more\nthan 99% accuracy for performance- and energy-based optimization objectives.\nFurthermore, it achieves almost identical performance to the Oracle with a low\nruntime overhead and successfully adapts to new applications, many-core system\nconfigurations, and runtime variations in application characteristics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 08:20:21 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 23:37:23 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Krishnakumar", "Anish", ""], ["Arda", "Samet E.", ""], ["Goksoy", "A. Alper", ""], ["Mandal", "Sumit K.", ""], ["Ogras", "Umit Y.", ""], ["Sartor", "Anderson L.", ""], ["Marculescu", "Radu", ""]]}, {"id": "2007.09370", "submitter": "Lingjuan Lyu", "authors": "Lingjuan Lyu, Yitong Li, Karthik Nandakumar, Jiangshan Yu, Xingjun Ma", "title": "How to Democratise and Protect AI: Fair and Differentially Private\n  Decentralised Deep Learning", "comments": "Accepted for publication in TDSC", "journal-ref": null, "doi": "10.1109/TDSC.2020.3006287", "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper firstly considers the research problem of fairness in\ncollaborative deep learning, while ensuring privacy. A novel reputation system\nis proposed through digital tokens and local credibility to ensure fairness, in\ncombination with differential privacy to guarantee privacy. In particular, we\nbuild a fair and differentially private decentralised deep learning framework\ncalled FDPDDL, which enables parties to derive more accurate local models in a\nfair and private manner by using our developed two-stage scheme: during the\ninitialisation stage, artificial samples generated by Differentially Private\nGenerative Adversarial Network (DPGAN) are used to mutually benchmark the local\ncredibility of each party and generate initial tokens; during the update stage,\nDifferentially Private SGD (DPSGD) is used to facilitate collaborative\nprivacy-preserving deep learning, and local credibility and tokens of each\nparty are updated according to the quality and quantity of individually\nreleased gradients. Experimental results on benchmark datasets under three\nrealistic settings demonstrate that FDPDDL achieves high fairness, yields\ncomparable accuracy to the centralised and distributed frameworks, and delivers\nbetter accuracy than the standalone framework.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 09:06:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lyu", "Lingjuan", ""], ["Li", "Yitong", ""], ["Nandakumar", "Karthik", ""], ["Yu", "Jiangshan", ""], ["Ma", "Xingjun", ""]]}, {"id": "2007.09436", "submitter": "Konstantinos Kallas", "authors": "Nikos Vasilakis (MIT), Konstantinos Kallas (University of\n  Pennsylvania), Konstantinos Mamouras (Rice University), Achilleas\n  Benetopoulos (Unaffiliated), Lazar Cvetkovi\\'c (University of Belgrade)", "title": "PaSh: Light-touch Data-Parallel Shell Processing", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": "10.1145/3447786.3456228", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents {\\scshape PaSh}, a system for parallelizing POSIX shell\nscripts. Given a script, {\\scshape PaSh} converts it to a dataflow graph,\nperforms a series of semantics-preserving program transformations that expose\nparallelism, and then converts the dataflow graph back into a script -- one\nthat adds POSIX constructs to explicitly guide parallelism coupled with\n{\\scshape PaSh}-provided {\\scshape Unix}-aware runtime primitives for\naddressing performance- and correctness-related issues. A lightweight\nannotation language allows command developers to express key parallelizability\nproperties about their commands. An accompanying parallelizability study of\nPOSIX and GNU commands -- two large and commonly used groups -- guides the\nannotation language and optimized aggregator library that {\\scshape PaSh} uses.\nFinally, {\\scshape PaSh}'s {\\scshape PaSh}'s extensive evaluation over 44\nunmodified {\\scshape Unix} scripts shows significant speedups\n($0.89$--$61.1\\times$, avg: $6.7\\times$) stemming from the combination of its\nprogram transformations and runtime primitives.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 14:14:11 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:24:41 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 02:04:55 GMT"}, {"version": "v4", "created": "Sat, 3 Apr 2021 16:02:11 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vasilakis", "Nikos", "", "MIT"], ["Kallas", "Konstantinos", "", "University of\n  Pennsylvania"], ["Mamouras", "Konstantinos", "", "Rice University"], ["Benetopoulos", "Achilleas", "", "Unaffiliated"], ["Cvetkovi\u0107", "Lazar", "", "University of Belgrade"]]}, {"id": "2007.09468", "submitter": "Michael Whittaker", "authors": "Michael Whittaker, Neil Giridharan, Adriana Szekeres, Joseph M.\n  Hellerstein, Heidi Howard, Faisal Nawab, Ion Stoica", "title": "Matchmaker Paxos: A Reconfigurable Consensus Protocol [Technical Report]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State machine replication protocols, like MultiPaxos and Raft, are at the\nheart of nearly every strongly consistent distributed database. To tolerate\nmachine failures, these protocols must replace failed machines with live\nmachines, a process known as reconfiguration. Reconfiguration has become\nincreasingly important over time as the need for frequent reconfiguration has\ngrown. Despite this, reconfiguration has largely been neglected in the\nliterature. In this paper, we present Matchmaker Paxos and Matchmaker\nMultiPaxos, a reconfigurable consensus and state machine replication protocol\nrespectively. Our protocols can perform a reconfiguration with little to no\nimpact on the latency or throughput of command processing; they can perform a\nreconfiguration in one round trip (theoretically) and a few milliseconds\n(empirically); they provide a number of theoretical insights; and they present\na framework that can be generalized to other replication protocols in a way\nthat previous reconfiguration techniques can not. We provide proofs of\ncorrectness for the protocols and optimizations, and present empirical results\nfrom an open source implementation.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 16:31:31 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 03:25:07 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Whittaker", "Michael", ""], ["Giridharan", "Neil", ""], ["Szekeres", "Adriana", ""], ["Hellerstein", "Joseph M.", ""], ["Howard", "Heidi", ""], ["Nawab", "Faisal", ""], ["Stoica", "Ion", ""]]}, {"id": "2007.09589", "submitter": "Supun Kamburugamuve", "authors": "Chathura Widanage, Niranda Perera, Vibhatha Abeykoon, Supun\n  Kamburugamuve, Thejaka Amila Kanewala, Hasara Maithree, Pulasthi\n  Wickramasinghe, Ahmet Uyar, Gurhan Gunduz, and Geoffrey Fox", "title": "High Performance Data Engineering Everywhere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amazing advances being made in the fields of machine and deep learning\nare a highlight of the Big Data era for both enterprise and research\ncommunities. Modern applications require resources beyond a single node's\nability to provide. However this is just a small part of the issues facing the\noverall data processing environment, which must also support a raft of data\nengineering for pre- and post-data processing, communication, and system\nintegration. An important requirement of data analytics tools is to be able to\neasily integrate with existing frameworks in a multitude of languages, thereby\nincreasing user productivity and efficiency. All this demands an efficient and\nhighly distributed integrated approach for data processing, yet many of today's\npopular data analytics tools are unable to satisfy all these requirements at\nthe same time.\n  In this paper we present Cylon, an open-source high performance distributed\ndata processing library that can be seamlessly integrated with existing Big\nData and AI/ML frameworks. It is developed with a flexible C++ core on top of a\ncompact data structure and exposes language bindings to C++, Java, and Python.\nWe discuss Cylon's architecture in detail, and reveal how it can be imported as\na library to existing applications or operate as a standalone framework.\nInitial experiments show that Cylon enhances popular tools such as Apache Spark\nand Dask with major performance improvements for key operations and better\ncomponent linkages. Finally, we show how its design enables Cylon to be used\ncross-platform with minimum overhead, which includes popular AI tools such as\nPyTorch, Tensorflow, and Jupyter notebooks.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 04:28:42 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Widanage", "Chathura", ""], ["Perera", "Niranda", ""], ["Abeykoon", "Vibhatha", ""], ["Kamburugamuve", "Supun", ""], ["Kanewala", "Thejaka Amila", ""], ["Maithree", "Hasara", ""], ["Wickramasinghe", "Pulasthi", ""], ["Uyar", "Ahmet", ""], ["Gunduz", "Gurhan", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2007.09625", "submitter": "Dingwen Tao", "authors": "Jiannan Tian, Sheng Di, Kai Zhao, Cody Rivera, Megan Hickman Fulp,\n  Robert Underwood, Sian Jin, Xin Liang, Jon Calhoun, Dingwen Tao, Franck\n  Cappello", "title": "cuSZ: An Efficient GPU-Based Error-Bounded Lossy Compression Framework\n  for Scientific Data", "comments": "13 pages, 8 figures, 9 tables, published in PACT '20", "journal-ref": null, "doi": "10.1145/3410463.3414624", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-bounded lossy compression is a state-of-the-art data reduction\ntechnique for HPC applications because it not only significantly reduces\nstorage overhead but also can retain high fidelity for postanalysis. Because\nsupercomputers and HPC applications are becoming heterogeneous using\naccelerator-based architectures, in particular GPUs, several development teams\nhave recently released GPU versions of their lossy compressors. However,\nexisting state-of-the-art GPU-based lossy compressors suffer from either low\ncompression and decompression throughput or low compression quality. In this\npaper, we present an optimized GPU version, cuSZ, for one of the best\nerror-bounded lossy compressors-SZ. To the best of our knowledge, cuSZ is the\nfirst error-bounded lossy compressor on GPUs for scientific data. Our\ncontributions are fourfold. (1) We propose a dual-quantization scheme to\nentirely remove the data dependency in the prediction step of SZ such that this\nstep can be performed very efficiently on GPUs. (2) We develop an efficient\ncustomized Huffman coding for the SZ compressor on GPUs. (3) We implement cuSZ\nusing CUDA and optimize its performance by improving the utilization of GPU\nmemory bandwidth. (4) We evaluate our cuSZ on five real-world HPC application\ndatasets from the Scientific Data Reduction Benchmarks and compare it with\nother state-of-the-art methods on both CPUs and GPUs. Experiments show that our\ncuSZ improves SZ's compression throughput by up to 370.1x and 13.1x,\nrespectively, over the production version running on single and multiple CPU\ncores, respectively, while getting the same quality of reconstructed data. It\nalso improves the compression ratio by up to 3.48x on the tested data compared\nwith another state-of-the-art GPU supported lossy compressor.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 08:54:39 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 05:11:48 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 17:45:36 GMT"}, {"version": "v4", "created": "Thu, 10 Sep 2020 15:26:22 GMT"}, {"version": "v5", "created": "Fri, 11 Sep 2020 03:30:37 GMT"}, {"version": "v6", "created": "Mon, 21 Sep 2020 14:38:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Tian", "Jiannan", ""], ["Di", "Sheng", ""], ["Zhao", "Kai", ""], ["Rivera", "Cody", ""], ["Fulp", "Megan Hickman", ""], ["Underwood", "Robert", ""], ["Jin", "Sian", ""], ["Liang", "Xin", ""], ["Calhoun", "Jon", ""], ["Tao", "Dingwen", ""], ["Cappello", "Franck", ""]]}, {"id": "2007.09712", "submitter": "Yi Liu", "authors": "Yi Liu, Sahil Garg, Jiangtian Nie, Yang Zhang, Zehui Xiong, Jiawen\n  Kang, M. Shamim Hossain", "title": "Deep Anomaly Detection for Time-series Data in Industrial IoT: A\n  Communication-Efficient On-device Federated Learning Approach", "comments": "IEEE Internet of Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2020.3011726", "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since edge device failures (i.e., anomalies) seriously affect the production\nof industrial products in Industrial IoT (IIoT), accurately and timely\ndetecting anomalies is becoming increasingly important. Furthermore, data\ncollected by the edge device may contain the user's private data, which is\nchallenging the current detection approaches as user privacy is calling for the\npublic concern in recent years. With this focus, this paper proposes a new\ncommunication-efficient on-device federated learning (FL)-based deep anomaly\ndetection framework for sensing time-series data in IIoT. Specifically, we\nfirst introduce a FL framework to enable decentralized edge devices to\ncollaboratively train an anomaly detection model, which can improve its\ngeneralization ability. Second, we propose an Attention Mechanism-based\nConvolutional Neural Network-Long Short Term Memory (AMCNN-LSTM) model to\naccurately detect anomalies. The AMCNN-LSTM model uses attention\nmechanism-based CNN units to capture important fine-grained features, thereby\npreventing memory loss and gradient dispersion problems. Furthermore, this\nmodel retains the advantages of LSTM unit in predicting time series data.\nThird, to adapt the proposed framework to the timeliness of industrial anomaly\ndetection, we propose a gradient compression mechanism based on Top-\\textit{k}\nselection to improve communication efficiency. Extensive experiment studies on\nfour real-world datasets demonstrate that the proposed framework can accurately\nand timely detect anomalies and also reduce the communication overhead by 50\\%\ncompared to the federated learning framework that does not use a gradient\ncompression scheme.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 16:47:26 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Liu", "Yi", ""], ["Garg", "Sahil", ""], ["Nie", "Jiangtian", ""], ["Zhang", "Yang", ""], ["Xiong", "Zehui", ""], ["Kang", "Jiawen", ""], ["Hossain", "M. Shamim", ""]]}, {"id": "2007.09733", "submitter": "Jo\\~ao Louren\\c{c}o", "authors": "Tiago M. Vale, Jo\\~ao Leit\\~ao, Nuno Pregui\\c{c}a, Rodrigo Rodrigues,\n  Ricardo J. Dias, Jo\\~ao M. Louren\\c{c}o", "title": "Lazy State Determination: More concurrency for contending linearizable\n  transactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concurrency control algorithms in transactional systems limits\nconcurrency to provide strong semantics, which leads to poor performance under\nhigh contention. As a consequence, many transactional systems eschew strong\nsemantics to achieve acceptable performance. We show that by leveraging\nsemantic information associated with the transactional programs to increase\nconcurrency, it is possible to significantly improve performance while\nmaintaining linearizability. To this end, we introduce the lazy state\ndetermination API to easily expose the semantics of application transactions to\nthe database, and propose new optimistic and pessimistic concurrency control\nalgorithms that leverage this information to safely increase concurrency in the\npresence of contention. Our evaluation shows that our approach can achieve up\nto 5x more throughput with 1.5c less latency than standard techniques in the\npopular TPC-C benchmark.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 18:02:05 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Vale", "Tiago M.", ""], ["Leit\u00e3o", "Jo\u00e3o", ""], ["Pregui\u00e7a", "Nuno", ""], ["Rodrigues", "Rodrigo", ""], ["Dias", "Ricardo J.", ""], ["Louren\u00e7o", "Jo\u00e3o M.", ""]]}, {"id": "2007.09816", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Varsha Dani, Thomas P. Hayes, Seth Pettie", "title": "The Energy Complexity of BFS in Radio Networks", "comments": "To appear in PODC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a model of energy complexity in Radio Networks in which\ntransmitting or listening on the channel costs one unit of energy and\ncomputation is free. This simplified model captures key aspects of\nbattery-powered sensors: that battery life is most influenced by transceiver\nusage, and that at low transmission powers, the actual cost of transmitting and\nlistening are very similar.\n  The energy complexity of tasks in single-hop networks is well understood.\nRecent work of Chang et al. considered energy complexity in multi-hop networks\nand showed that $\\mathsf{Broadcast}$ admits an energy-efficient protocol, by\nwhich we mean each of the $n$ nodes in the network spends\n$O(\\text{polylog}(n))$ energy. This work left open the strange possibility that\nall natural problems in multi-hop networks might admit such an energy-efficient\nsolution.\n  In this paper we prove that the landscape of energy complexity is rich enough\nto support a multitude of problem complexities. Whereas $\\mathsf{Broadcast}$\ncan be solved by an energy-efficient protocol, exact computation of\n$\\mathsf{Diameter}$ cannot, requiring $\\Omega(n)$ energy. Our main result is\nthat $\\mathsf{Breadth First Search}$ has sub-polynomial energy complexity at\nmost $2^{O(\\sqrt{\\log n\\log\\log n})}=n^{o(1)}$; whether it admits an efficient\n$O(\\text{polylog}(n))$-energy protocol is an open problem.\n  Our main algorithm involves recursively solving a generalized BFS problem on\na cluster graph introduced by Miller, Peng, and Xu. In this application, we\nmake crucial use of a close relationship between distances in this cluster\ngraph, and distances in the original network. This relationship is new and may\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 23:26:59 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Dani", "Varsha", ""], ["Hayes", "Thomas P.", ""], ["Pettie", "Seth", ""]]}, {"id": "2007.09985", "submitter": "Amani Abusafia", "authors": "Amani Abusafia and Athman Bouguettaya and Sajib Mistry", "title": "Incentive-Based Selection and Composition of IoT Energy Services", "comments": "8 pages, 12 Figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel incentive-based framework for composing energy service\nrequests. An incentive model is designed that considers the context of the\nproviders and consumers to determine rewards for sharing wireless energy. We\npropose a novel priority scheduling approach to compose energy service requests\nthat maximizes the reward of the provider. A set of exhaustive experiments with\na dataset and collected IoT users' behavior is conducted to evaluate the\nproposed approach. Experimental results prove the efficiency of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:18:13 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Abusafia", "Amani", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2007.10054", "submitter": "William Saunders", "authors": "William Robert Saunders, James Grant, Eike Hermann M\\\"uller", "title": "Parallel Performance of ARM ThunderX2 for Atomistic Simulation\n  Algorithms", "comments": "10 pages, 3 figures, 1 tables; submitted to EAHPC-2020 (Embracing\n  Arm: a journey of porting and optimization to the latest Arm-based processors\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Atomistic simulation drives scientific advances in modern material science\nand accounts for a significant proportion of wall time on High Performance\nComputing facilities. It is important that algorithms are efficient and\nimplementations are performant in a continuously diversifying hardware\nlandscape. Furthermore, they have to be portable to make best use of the\navailable computing resource.\n  In this paper we assess the parallel performance of some key algorithms\nimplemented in a performance portable framework developed by us. We consider\nMolecular Dynamics with short range interactions, the Fast Multipole Method and\nKinetic Monte Carlo. To assess the performance of emerging architectures, we\ncompare the Marvell ThunderX2 (ARM) architecture to traditional x86_64 hardware\nmade available through the Azure cloud computing service.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 12:47:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Saunders", "William Robert", ""], ["Grant", "James", ""], ["M\u00fcller", "Eike Hermann", ""]]}, {"id": "2007.10095", "submitter": "Simona Rombo", "authors": "Mario Randazzo, Simona E. Rombo", "title": "A Big Data Approach for Sequences Indexing on the Cloud via Burrows\n  Wheeler Transform", "comments": "Accepted at HELPLINE@ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexing sequence data is important in the context of Precision Medicine,\nwhere large amounts of ``omics'' data have to be daily collected and analyzed\nin order to categorize patients and identify the most effective therapies. Here\nwe propose an algorithm for the computation of Burrows Wheeler transform\nrelying on Big Data technologies, i.e., Apache Spark and Hadoop. Our approach\nis the first that distributes the index computation and not only the input\ndataset, allowing to fully benefit of the available cloud resources.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 13:39:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Randazzo", "Mario", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2007.10219", "submitter": "Diego Cirilo Do Nascimento", "authors": "Reinaldo Agostinho de Souza Filho, Diego V. Cirilo do Nascimento,\n  Samuel Xavier-de-Souza", "title": "An OpenMP translator for the GAP8 MPSoC", "comments": "Presented at the 13th International Workshop on Programmability and\n  Architectures for Heterogeneous Multicores, 2020 (arXiv:2005.07619)", "journal-ref": null, "doi": null, "report-no": "Report-no: MULTIPROG/2020/6", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the barriers to the adoption of parallel computing is the inherent\ncomplexity of its programming. The Open Multi-Processing (OpenMP) Application\nProgramming Interface (API) facilitates such implementations, providing high\nabstraction level directives. On another front, new architectures aimed at low\nenergy consumption have been developed, such as the Greenwaves Technologies\nGAP8, a Multi-Processor System-on-Chip (MPSoC) based on the Parallel Ultra Low\nPower (PULP) Platform. The GAP8 has an 8-core cluster and a Fabric\nController(FC) master core. Parallel programming with GAP8 is very promising on\nthe efficiency side, but its recent development and lack of a robust OS to\nhandle threads and core scheduling complicate a simple implementation of the\nOpenMP APIs. This project implements a source to source translator that\ninterprets a limited set of OpenMP directives, and is capable of generating\nparallel microcontroller code manipulating the cores directly. The preliminary\nresults obtained in this work shows a reduction of the code size, if compared\nwith the base implementation, proving the efficiency of the project to ease the\nprogramming of the GAP8. Further work is need in order to implement more OpenMP\ndirectives.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:02:01 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Filho", "Reinaldo Agostinho de Souza", ""], ["Nascimento", "Diego V. Cirilo do", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "2007.10290", "submitter": "Benjamin Allen", "authors": "Benjamin S. Allen, Matthew A. Ezell, Paul Peltz, Doug Jacobsen, Eric\n  Roman, Cory Lueninghoener, and J. Lowell Wofford", "title": "Modernizing the HPC System Software Stack", "comments": "Presented at the SC (SC20), Atlanta, GA: Zenodo (2020)", "journal-ref": null, "doi": "10.5281/zenodo.4324415", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the 1990s, HPC centers at national laboratories, universities, and\nother large sites designed distributed system architectures and software stacks\nthat enabled extreme-scale computing. By the 2010s, these centers were eclipsed\nby the scale of web-scale and cloud computing architectures, and today even\nupcoming exascale HPC systems are magnitudes of scale smaller than those of\ndatacenters employed by large web companies. Meanwhile, the HPC community has\nallowed system software designs to stagnate, relying on incremental changes to\ntried-and-true designs to move between generations of systems. We contend that\na modern system software stack that focuses on manageability, scalability,\nsecurity, and modern methods will benefit the entire HPC community. In this\npaper, we break down the logical parts of a typical HPC system software stack,\nlook at more modern ways to meet their needs, and make recommendations of\nfuture work that would help the community move in that direction.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 17:18:04 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Allen", "Benjamin S.", ""], ["Ezell", "Matthew A.", ""], ["Peltz", "Paul", ""], ["Jacobsen", "Doug", ""], ["Roman", "Eric", ""], ["Lueninghoener", "Cory", ""], ["Wofford", "J. Lowell", ""]]}, {"id": "2007.10312", "submitter": "Martin Uhrin", "authors": "Martin Uhrin and Sebastiaan P. Huber and Jusong Yu and Nicola Marzari\n  and Giovanni Pizzi", "title": "Workflows in AiiDA: Engineering a high-throughput, event-based engine\n  for robust and modular computational workflows", "comments": null, "journal-ref": "Computational Materials Science 187, 110086 (2021)", "doi": "10.1016/j.commatsci.2020.110086", "report-no": null, "categories": "cs.DC cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Over the last two decades, the field of computational science has seen a\ndramatic shift towards incorporating high-throughput computation and big-data\nanalysis as fundamental pillars of the scientific discovery process. This has\nnecessitated the development of tools and techniques to deal with the\ngeneration, storage and processing of large amounts of data. In this work we\npresent an in-depth look at the workflow engine powering AiiDA, a widely\nadopted, highly flexible and database-backed informatics infrastructure with an\nemphasis on data reproducibility. We detail many of the design choices that\nwere made which were informed by several important goals: the ability to scale\nfrom running on individual laptops up to high-performance supercomputers,\nmanaging jobs with runtimes spanning from fractions of a second to weeks and\nscaling up to thousands of jobs concurrently, and all this while maximising\nrobustness. In short, AiiDA aims to be a Swiss army knife for high-throughput\ncomputational science. As well as the architecture, we outline important API\ndesign choices made to give workflow writers a great deal of liberty whilst\nguiding them towards writing robust and modular workflows, ultimately enabling\nthem to encode their scientific knowledge to the benefit of the wider\nscientific community.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 09:14:58 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 08:47:15 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Uhrin", "Martin", ""], ["Huber", "Sebastiaan P.", ""], ["Yu", "Jusong", ""], ["Marzari", "Nicola", ""], ["Pizzi", "Giovanni", ""]]}, {"id": "2007.10359", "submitter": "Jeffrey Krupa", "authors": "Jeffrey Krupa, Kelvin Lin, Maria Acosta Flechas, Jack Dinsmore, Javier\n  Duarte, Philip Harris, Scott Hauck, Burt Holzman, Shih-Chieh Hsu, Thomas\n  Klijnsma, Mia Liu, Kevin Pedro, Dylan Rankin, Natchanon Suaysom, Matt Trahms,\n  Nhan Tran", "title": "GPU coprocessors as a service for deep learning inference in high energy\n  physics", "comments": "26 pages, 7 figures, 2 tables", "journal-ref": "Mach. Learn.: Sci. Technol. 2 (2021) 035005", "doi": "10.1088/2632-2153/abec21", "report-no": "FERMILAB-PUB-20-338-E-SCD", "categories": "physics.comp-ph cs.DC hep-ex physics.data-an physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the next decade, the demands for computing in large scientific experiments\nare expected to grow tremendously. During the same time period, CPU performance\nincreases will be limited. At the CERN Large Hadron Collider (LHC), these two\nissues will confront one another as the collider is upgraded for high\nluminosity running. Alternative processors such as graphics processing units\n(GPUs) can resolve this confrontation provided that algorithms can be\nsufficiently accelerated. In many cases, algorithmic speedups are found to be\nlargest through the adoption of deep learning algorithms. We present a\ncomprehensive exploration of the use of GPU-based hardware acceleration for\ndeep learning inference within the data reconstruction workflow of high energy\nphysics. We present several realistic examples and discuss a strategy for the\nseamless integration of coprocessors so that the LHC can maintain, if not\nexceed, its current performance throughout its running.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:00:04 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 17:06:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Krupa", "Jeffrey", ""], ["Lin", "Kelvin", ""], ["Flechas", "Maria Acosta", ""], ["Dinsmore", "Jack", ""], ["Duarte", "Javier", ""], ["Harris", "Philip", ""], ["Hauck", "Scott", ""], ["Holzman", "Burt", ""], ["Hsu", "Shih-Chieh", ""], ["Klijnsma", "Thomas", ""], ["Liu", "Mia", ""], ["Pedro", "Kevin", ""], ["Rankin", "Dylan", ""], ["Suaysom", "Natchanon", ""], ["Trahms", "Matt", ""], ["Tran", "Nhan", ""]]}, {"id": "2007.10459", "submitter": "Suchita Pati", "authors": "Suchita Pati, Shaizeen Aga, Matthew D. Sinclair, and Nuwan Jayasena", "title": "SeqPoint: Identifying Representative Iterations of Sequence-based Neural\n  Networks", "comments": "To appear in IEEE International Symposium on Performance Analysis of\n  Systems and Software (ISPASS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of deep neural networks (DNNs) continues to rise, making them a\ncrucial application class for hardware optimizations. However, detailed\nprofiling and characterization of DNN training remains difficult as these\napplications often run for hours to days on real hardware. Prior works exploit\nthe iterative nature of DNNs to profile a few training iterations. While such a\nstrategy is sound for networks like convolutional neural networks (CNNs), where\nthe nature of the computation is largely input independent, we observe in this\nwork that this approach is sub-optimal for sequence-based neural networks\n(SQNNs) such as recurrent neural networks (RNNs). The amount and nature of\ncomputations in SQNNs can vary for each input, resulting in heterogeneity\nacross iterations. Thus, arbitrarily selecting a few iterations is insufficient\nto accurately summarize the behavior of the entire training run. To tackle this\nchallenge, we carefully study the factors that impact SQNN training iterations\nand identify input sequence length as the key determining factor for variations\nacross iterations. We then use this observation to characterize all iterations\nof an SQNN training run (requiring no profiling or simulation of the\napplication) and select representative iterations, which we term SeqPoints. We\nanalyze two state-of-the-art SQNNs, DeepSpeech2 and Google's Neural Machine\nTranslation (GNMT), and show that SeqPoints can represent their entire training\nruns accurately, resulting in geomean errors of only 0.11% and 0.53%,\nrespectively, when projecting overall runtime and 0.13% and 1.50% when\nprojecting speedups due to architectural changes. This high accuracy is\nachieved while reducing the time needed for profiling by 345x and 214x for the\ntwo networks compared to full training runs. As a result, SeqPoint can enable\nanalysis of SQNN training runs in mere minutes instead of hours or days.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 20:39:33 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Pati", "Suchita", ""], ["Aga", "Shaizeen", ""], ["Sinclair", "Matthew D.", ""], ["Jayasena", "Nuwan", ""]]}, {"id": "2007.10491", "submitter": "Jieyang Chen", "authors": "Jieyang Chen, Qiang Guan, Li-Ta Lo, Patricia Grubel, and Tim Randles", "title": "BeeSwarm: Enabling Scalability Tests in Continuous Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing is one of the most important steps in software development. It\nensures the quality of software. Continuous Integration (CI) is a widely used\ntesting system that can report software quality to the developer in a timely\nmanner during the development progress. Performance, especially scalability, is\nanother key factor for High Performance Computing (HPC) applications. Though\nthere are many applications and tools to profile the performance of HPC\napplications, none of them are integrated into the continuous integration. On\nthe other hand, no current continuous integration tools provide easy-to-use\nscalability test capabilities. In this work, we propose BeeSwarm, a scalability\ntest system that can be easily applied to the current CI test environment\nenabling scalability test capability for HPC developers. As a showcase,\nBeeSwarm is integrated into Travis CI and GitLab CI to execute the scalability\ntest workflow on Chameleon cloud.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:35:49 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chen", "Jieyang", ""], ["Guan", "Qiang", ""], ["Lo", "Li-Ta", ""], ["Grubel", "Patricia", ""], ["Randles", "Tim", ""]]}, {"id": "2007.10498", "submitter": "Fatemeh Rouzbeh", "authors": "Fatemeh Rouzbeh, Ananth Grama, Paul Griffin, Mohammad Adibuzzaman", "title": "Collaborative Cloud Computing Framework for Health Data with Open Source\n  Technologies", "comments": "This paper is accepted in ACM-BCB 2020", "journal-ref": null, "doi": "10.1145/3388440.3412460", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of sensor technologies and advancements in data collection\nmethods have enabled the accumulation of very large amounts of data.\nIncreasingly, these datasets are considered for scientific research. However,\nthe design of the system architecture to achieve high performance in terms of\nparallelization, query processing time, aggregation of heterogeneous data types\n(e.g., time series, images, structured data, among others), and difficulty in\nreproducing scientific research remain a major challenge. This is specifically\ntrue for health sciences research, where the systems must be i) easy to use\nwith the flexibility to manipulate data at the most granular level, ii)\nagnostic of programming language kernel, iii) scalable, and iv) compliant with\nthe HIPAA privacy law. In this paper, we review the existing literature for\nsuch big data systems for scientific research in health sciences and identify\nthe gaps of the current system landscape. We propose a novel architecture for\nsoftware-hardware-data ecosystem using open source technologies such as Apache\nHadoop, Kubernetes and JupyterHub in a distributed environment. We also\nevaluate the system using a large clinical data set of 69M patients.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:50:35 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Rouzbeh", "Fatemeh", ""], ["Grama", "Ananth", ""], ["Griffin", "Paul", ""], ["Adibuzzaman", "Mohammad", ""]]}, {"id": "2007.10528", "submitter": "Chuka Oham", "authors": "Chuka Oham, Regio Michelin, Salil S. Kanhere, Raja Jurdak and Sanjay\n  Jha", "title": "B-FERL: Blockchain based Framework for Securing Smart Vehicles", "comments": "11 Pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of connecting technologies in smart vehicles and the incremental\nautomation of its functionalities promise significant benefits, including a\nsignificant decline in congestion and road fatalities. However, increasing\nautomation and connectedness broadens the attack surface and heightens the\nlikelihood of a malicious entity successfully executing an attack. In this\npaper, we propose a Blockchain based Framework for sEcuring smaRt vehicLes\n(B-FERL). B-FERL uses permissioned blockchain technology to tailor information\naccess to restricted entities in the connected vehicle ecosystem. It also uses\na challenge-response data exchange between the vehicles and roadside units to\nmonitor the internal state of the vehicle to identify cases of in-vehicle\nnetwork compromise. In order to enable authentic and valid communication in the\nvehicular network, only vehicles with a verifiable record in the blockchain can\nexchange messages. Through qualitative arguments, we show that B-FERL is\nresilient to identified attacks. Also, quantitative evaluations in an emulated\nscenario show that B-FERL ensures a suitable response time and required storage\nsize compatible with realistic scenarios. Finally, we demonstrate how B-FERL\nachieves various important functions relevant to the automotive ecosystem such\nas trust management, vehicular forensics and secure vehicular networks.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 23:28:24 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Oham", "Chuka", ""], ["Michelin", "Regio", ""], ["Kanhere", "Salil S.", ""], ["Jurdak", "Raja", ""], ["Jha", "Sanjay", ""]]}, {"id": "2007.10541", "submitter": "Alejandro Ranchal-Pedrosa", "authors": "Alejandro Ranchal-Pedrosa, Vincent Gramoli", "title": "Blockchain Is Dead, Long Live Blockchain! Accountable State Machine\n  Replication for Longlasting Blockchain", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-standing impossibility of reaching agreement restricts the lifespan\nof blockchains. In fact, the consensus on a block to be appended to any\nblockchain succeeds either with some probability or at the condition that two\nthirds of the $n$ replicas are not Byzantine. In the former case, the\nprobability that the blockchain fails grows exponentially with the number of\nnewly appended blocks. In the latter case, the blockchain fails as soon as a\ncoalition bribes $f=n/3$ replicas. As a result, one may wonder whether\nblockchains are doomed to fail.\n  In this paper, we answer this question in the negative by proposing the first\nLonglasting Blockchain system, \\emph{LLB}. LLB builds upon the observation that\nblockchains are rarely subject to benign faults. As opposed to probabilistic\nblockchains, LLB solves consensus deterministically when $f<n/3$. As opposed to\nByzantine fault tolerant blockchains, it resolves a series of disagreements by\nreducing eventually the number of deceitful replicas from $n/3\\leq f<2n/3$ to\n$f'<n'/3$ among a new set of $n'$ replicas. To demonstrate its effectiveness,\nwe implement two coalition attacks and a zero loss payment application that\nforces replicas that misbehave to reimburse conflicting transactions. Finally,\nLLB outperforms the raw state machine replication at the heart of Facebook's\nLibra and achieves performance comparable to a scalable blockchain that cannot\ntolerate $n/3$ failures.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 00:57:25 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 23:42:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ranchal-Pedrosa", "Alejandro", ""], ["Gramoli", "Vincent", ""]]}, {"id": "2007.10560", "submitter": "Zhaoxiong Yang", "authors": "Zhaoxiong Yang, Shuihai Hu, Kai Chen", "title": "FPGA-Based Hardware Accelerator of Homomorphic Encryption for Efficient\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing awareness of privacy protection and data fragmentation\nproblem, federated learning has been emerging as a new paradigm of machine\nlearning. Federated learning tends to utilize various privacy preserving\nmechanisms to protect the transferred intermediate data, among which\nhomomorphic encryption strikes a balance between security and ease of\nutilization. However, the complicated operations and large operands impose\nsignificant overhead on federated learning. Maintaining accuracy and security\nmore efficiently has been a key problem of federated learning. In this work, we\ninvestigate a hardware solution, and design an FPGA-based homomorphic\nencryption framework, aiming to accelerate the training phase in federated\nlearning. The root complexity lies in searching for a compact architecture for\nthe core operation of homomorphic encryption, to suit the requirement of\nfederated learning about high encryption throughput and flexibility of\nconfiguration. Our framework implements the representative Paillier homomorphic\ncryptosystem with high level synthesis for flexibility and portability, with\ncareful optimization on the modular multiplication operation in terms of\nprocessing clock cycle, resource usage and clock frequency. Our accelerator\nachieves a near-optimal execution clock cycle, with a better DSP-efficiency\nthan existing designs, and reduces the encryption time by up to 71% during\ntraining process of various federated learning models.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 01:59:58 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Yang", "Zhaoxiong", ""], ["Hu", "Shuihai", ""], ["Chen", "Kai", ""]]}, {"id": "2007.10571", "submitter": "Daniel Richins", "authors": "Daniel Richins, Dharmisha Doshi, Matthew Blackmore, Aswathy\n  Thulaseedharan Nair, Neha Pathapati, Ankit Patel, Brainard Daguman, Daniel\n  Dobrijalowski, Ramesh Illikkal, Kevin Long, David Zimmerman, Vijay Janapa\n  Reddi", "title": "AI Tax: The Hidden Cost of AI Data Center Applications", "comments": "32 pages. 16 figures. Submitted to ACM \"Transactions on Computer\n  Systems.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence and machine learning are experiencing widespread\nadoption in industry and academia. This has been driven by rapid advances in\nthe applications and accuracy of AI through increasingly complex algorithms and\nmodels; this, in turn, has spurred research into specialized hardware AI\naccelerators. Given the rapid pace of advances, it is easy to forget that they\nare often developed and evaluated in a vacuum without considering the full\napplication environment. This paper emphasizes the need for a holistic,\nend-to-end analysis of AI workloads and reveals the \"AI tax.\" We deploy and\ncharacterize Face Recognition in an edge data center. The application is an\nAI-centric edge video analytics application built using popular open source\ninfrastructure and ML tools. Despite using state-of-the-art AI and ML\nalgorithms, the application relies heavily on pre-and post-processing code. As\nAI-centric applications benefit from the acceleration promised by accelerators,\nwe find they impose stresses on the hardware and software infrastructure:\nstorage and network bandwidth become major bottlenecks with increasing AI\nacceleration. By specializing for AI applications, we show that a purpose-built\nedge data center can be designed for the stresses of accelerated AI at 15%\nlower TCO than one derived from homogeneous servers and infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 02:42:26 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Richins", "Daniel", ""], ["Doshi", "Dharmisha", ""], ["Blackmore", "Matthew", ""], ["Nair", "Aswathy Thulaseedharan", ""], ["Pathapati", "Neha", ""], ["Patel", "Ankit", ""], ["Daguman", "Brainard", ""], ["Dobrijalowski", "Daniel", ""], ["Illikkal", "Ramesh", ""], ["Long", "Kevin", ""], ["Zimmerman", "David", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "2007.10581", "submitter": "Jungyeon Baek", "authors": "Jungyeon Baek and Georges Kaddoum", "title": "Heterogeneous Task Offloading and Resource Allocations via Deep\n  Recurrent Reinforcement Learning in Partial Observable Multi-Fog Networks", "comments": null, "journal-ref": null, "doi": "10.1109/JIOT.2020.3009540", "report-no": null, "categories": "cs.DC cs.MA eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As wireless services and applications become more sophisticated and require\nfaster and higher-capacity networks, there is a need for an efficient\nmanagement of the execution of increasingly complex tasks based on the\nrequirements of each application. In this regard, fog computing enables the\nintegration of virtualized servers into networks and brings cloud services\ncloser to end devices. In contrast to the cloud server, the computing capacity\nof fog nodes is limited and thus a single fog node might not be capable of\ncomputing-intensive tasks. In this context, task offloading can be particularly\nuseful at the fog nodes by selecting the suitable nodes and proper resource\nmanagement while guaranteeing the Quality-of-Service (QoS) requirements of the\nusers. This paper studies the design of a joint task offloading and resource\nallocation control for heterogeneous service tasks in multi-fog nodes systems.\nThis problem is formulated as a partially observable stochastic game, in which\neach fog node cooperates to maximize the aggregated local rewards while the\nnodes only have access to local observations. To deal with partial\nobservability, we apply a deep recurrent Q-network (DRQN) approach to\napproximate the optimal value functions. The solution is then compared to a\ndeep Q-network (DQN) and deep convolutional Q-network (DCQN) approach to\nevaluate the performance of different neural networks. Moreover, to guarantee\nthe convergence and accuracy of the neural network, an adjusted\nexploration-exploitation method is adopted. Provided numerical results show\nthat the proposed algorithm can achieve a higher average success rate and lower\naverage overflow than baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 03:41:28 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Baek", "Jungyeon", ""], ["Kaddoum", "Georges", ""]]}, {"id": "2007.10752", "submitter": "Alptekin Temizel", "authors": "Kaan Furkan Alt{\\i}nok, Af\\c{s}in Peker, Alptekin Temizel", "title": "Bit-level Parallelization of 3DES Encryption on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triple DES (3DES) is a standard fundamental encryption algorithm, used in\nseveral electronic payment applications and web browsers. In this paper, we\npropose a parallel implementation of 3DES on GPU. Since 3DES encrypts data with\n64-bit blocks, our approach considers each 64-bit block a kernel block and\nassign a separate thread to process each bit. Algorithm's permutation\noperations, XOR operations, and S-box operations are done in parallel within\nthese kernel blocks. The implementation benefits from the use of constant and\nshared memory types to optimize memory access. The results show an average\n10.70x speed-up against the baseline multi-threaded CPU implementation. The\nimplementation is publicly available at\nhttps://github.com/kaanfurkan35/3DES_GPU\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 12:28:48 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Alt\u0131nok", "Kaan Furkan", ""], ["Peker", "Af\u015fin", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.10958", "submitter": "Jeremie Decouchant", "authors": "David Kozhaya, Jeremie Decouchant, Vincent Rahli, Paulo\n  Esteves-Verissimo", "title": "PISTIS: An Event-Triggered Real-Time Byzantine-Resilient Protocol Suite", "comments": "This is an extended version of a journal article, which appears in\n  the IEEE Transactions on Parallel and Distributed Systems", "journal-ref": null, "doi": "10.1109/TPDS.2021.3056718", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The accelerated digitalisation of society along with technological evolution\nhave extended the geographical span of cyber-physical systems. Two main threats\nhave made the reliable and real-time control of these systems challenging: (i)\nuncertainty in the communication infrastructure induced by scale, and\nheterogeneity of the environment and devices; and (ii) targeted attacks\nmaliciously worsening the impact of the above-mentioned communication\nuncertainties, disrupting the correctness of real-time applications. This paper\naddresses those challenges by showing how to build distributed protocols that\nprovide both real-time with practical performance, and scalability in the\npresence of network faults and attacks, in probabilistic synchronous\nenvironments. We provide a suite of real-time Byzantine protocols, which we\nprove correct, starting from a reliable broadcast protocol, called PISTIS, up\nto atomic broadcast and consensus. This suite simplifies the construction of\npowerful distributed and decentralized monitoring and control applications,\nincluding state-machine replication. Extensive empirical simulations showcase\nPISTIS's robustness, latency, and scalability. For example, PISTIS can\nwithstand message loss (and delay) rates up to 50% in systems with 49 nodes and\nprovides bounded delivery latencies in the order of a few milliseconds.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:18:57 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 15:51:33 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Kozhaya", "David", ""], ["Decouchant", "Jeremie", ""], ["Rahli", "Vincent", ""], ["Esteves-Verissimo", "Paulo", ""]]}, {"id": "2007.10987", "submitter": "Heiko Ludwig", "authors": "Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar,\n  Shashank Rajamoni, Yuya Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu\n  Sinn, Mark Purcell, Ambrish Rawat, Tran Minh, Naoise Holohan, Supriyo\n  Chakraborty, Shalisha Whitherspoon, Dean Steuer, Laura Wynter, Hifaz Hassan,\n  Sean Laguna, Mikhail Yurochkin, Mayank Agarwal, Ebube Chuba, Annie Abay", "title": "IBM Federated Learning: an Enterprise Framework White Paper V0.1", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is an approach to conduct machine learning without\ncentralizing training data in a single place, for reasons of privacy,\nconfidentiality or data volume. However, solving federated machine learning\nproblems raises issues above and beyond those of centralized machine learning.\nThese issues include setting up communication infrastructure between parties,\ncoordinating the learning process, integrating party results, understanding the\ncharacteristics of the training data sets of different participating parties,\nhandling data heterogeneity, and operating with the absence of a verification\ndata set.\n  IBM Federated Learning provides infrastructure and coordination for federated\nlearning. Data scientists can design and run federated learning jobs based on\nexisting, centralized machine learning models and can provide high-level\ninstructions on how to run the federation. The framework applies to both Deep\nNeural Networks as well as ``traditional'' approaches for the most common\nmachine learning libraries. {\\proj} enables data scientists to expand their\nscope from centralized to federated machine learning, minimizing the learning\ncurve at the outset while also providing the flexibility to deploy to different\ncompute environments and design custom fusion algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 05:32:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Ludwig", "Heiko", ""], ["Baracaldo", "Nathalie", ""], ["Thomas", "Gegi", ""], ["Zhou", "Yi", ""], ["Anwar", "Ali", ""], ["Rajamoni", "Shashank", ""], ["Ong", "Yuya", ""], ["Radhakrishnan", "Jayaram", ""], ["Verma", "Ashish", ""], ["Sinn", "Mathieu", ""], ["Purcell", "Mark", ""], ["Rawat", "Ambrish", ""], ["Minh", "Tran", ""], ["Holohan", "Naoise", ""], ["Chakraborty", "Supriyo", ""], ["Whitherspoon", "Shalisha", ""], ["Steuer", "Dean", ""], ["Wynter", "Laura", ""], ["Hassan", "Hifaz", ""], ["Laguna", "Sean", ""], ["Yurochkin", "Mikhail", ""], ["Agarwal", "Mayank", ""], ["Chuba", "Ebube", ""], ["Abay", "Annie", ""]]}, {"id": "2007.11089", "submitter": "Jos\\'e Cano", "authors": "Martina Lofqvist, Jos\\'e Cano", "title": "Accelerating Deep Learning Applications in Space", "comments": "Published as a workshop paper at SmallSat 2020 - The 34th Annual\n  Small Satellite Conference. 19 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computing at the edge offers intriguing possibilities for the development of\nautonomy and artificial intelligence. The advancements in autonomous\ntechnologies and the resurgence of computer vision have led to a rise in demand\nfor fast and reliable deep learning applications. In recent years, the industry\nhas introduced devices with impressive processing power to perform various\nobject detection tasks. However, with real-time detection, devices are\nconstrained in memory, computational capacity, and power, which may compromise\nthe overall performance. This could be solved either by optimizing the object\ndetector or modifying the images. In this paper, we investigate the performance\nof CNN-based object detectors on constrained devices when applying different\nimage compression techniques. We examine the capabilities of a NVIDIA Jetson\nNano; a low-power, high-performance computer, with an integrated GPU, small\nenough to fit on-board a CubeSat. We take a closer look at the Single Shot\nMultiBox Detector (SSD) and Region-based Fully Convolutional Network (R-FCN)\nthat are pre-trained on DOTA - a Large Scale Dataset for Object Detection in\nAerial Images. The performance is measured in terms of inference time, memory\nconsumption, and accuracy. By applying image compression techniques, we are\nable to optimize performance. The two techniques applied, lossless compression\nand image scaling, improves speed and memory consumption with no or little\nchange in accuracy. The image scaling technique achieves a 100% runnable\ndataset and we suggest combining both techniques in order to optimize the\nspeed/memory/accuracy trade-off.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:06:30 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Lofqvist", "Martina", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2007.11112", "submitter": "Christos Kozyrakis", "authors": "Michael Cafarella and David DeWitt and Vijay Gadepally and Jeremy\n  Kepner and Christos Kozyrakis and Tim Kraska and Michael Stonebraker and\n  Matei Zaharia", "title": "DBOS: A Proposal for a Data-Centric Operating System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OS cs.AR cs.DB cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current operating systems are complex systems that were designed before\ntoday's computing environments. This makes it difficult for them to meet the\nscalability, heterogeneity, availability, and security challenges in current\ncloud and parallel computing environments. To address these problems, we\npropose a radically new OS design based on data-centric architecture: all\noperating system state should be represented uniformly as database tables, and\noperations on this state should be made via queries from otherwise stateless\ntasks. This design makes it easy to scale and evolve the OS without\nwhole-system refactoring, inspect and debug system state, upgrade components\nwithout downtime, manage decisions using machine learning, and implement\nsophisticated security features. We discuss how a database OS (DBOS) can\nimprove the programmability and performance of many of today's most important\napplications and propose a plan for the development of a DBOS proof of concept.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:01:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Cafarella", "Michael", ""], ["DeWitt", "David", ""], ["Gadepally", "Vijay", ""], ["Kepner", "Jeremy", ""], ["Kozyrakis", "Christos", ""], ["Kraska", "Tim", ""], ["Stonebraker", "Michael", ""], ["Zaharia", "Matei", ""]]}, {"id": "2007.11115", "submitter": "Jinhyun So", "authors": "Jinhyun So, Basak Guler, A. Salman Avestimehr", "title": "Byzantine-Resilient Secure Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure federated learning is a privacy-preserving framework to improve\nmachine learning models by training over large volumes of data collected by\nmobile users. This is achieved through an iterative process where, at each\niteration, users update a global model using their local datasets. Each user\nthen masks its local model via random keys, and the masked models are\naggregated at a central server to compute the global model for the next\niteration. As the local models are protected by random masks, the server cannot\nobserve their true values. This presents a major challenge for the resilience\nof the model against adversarial (Byzantine) users, who can manipulate the\nglobal model by modifying their local models or datasets. Towards addressing\nthis challenge, this paper presents the first single-server Byzantine-resilient\nsecure aggregation framework (BREA) for secure federated learning. BREA is\nbased on an integrated stochastic quantization, verifiable outlier detection,\nand secure model aggregation approach to guarantee Byzantine-resilience,\nprivacy, and convergence simultaneously. We provide theoretical convergence and\nprivacy guarantees and characterize the fundamental trade-offs in terms of the\nnetwork size, user dropouts, and privacy protection. Our experiments\ndemonstrate convergence in the presence of Byzantine users, and comparable\naccuracy to conventional federated learning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:15:11 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2021 21:57:10 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["So", "Jinhyun", ""], ["Guler", "Basak", ""], ["Avestimehr", "A. Salman", ""]]}, {"id": "2007.11116", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Rong-Rong Chen, Mingyue Ji", "title": "A New Combinatorial Coded Design for Heterogeneous Distributed Computing", "comments": "30 pages, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coded Distributed Computing (CDC) introduced by Li et al. in 2015 offers an\nefficient approach to trade computing power to reduce the communication load in\ngeneral distributed computing frameworks such as MapReduce and Spark. In\nparticular, increasing the computation load in the Map phase by a factor of r\ncan create coded multicasting opportunities to reduce the communication load in\nthe Shuffle phase by the same factor. However, the CDC scheme is designed for\nthe homogeneous settings, where the storage, computation load and communication\nload on the computing nodes are the same. In addition, it requires an\nexponentially large number of input files (data batches), reduce functions and\nmulticasting groups relative to the number of nodes to achieve the promised\ngain. We address the CDC limitations by proposing a novel CDC approach based on\na combinatorial design, which accommodates heterogeneous networks where nodes\nhave varying storage and computing capabilities. In addition, the proposed\napproach requires an exponentially less number of input files compared to the\noriginal CDC scheme proposed by Li et al. Meanwhile, the resulting\ncomputation-communication trade-off maintains the multiplicative gain compared\nto conventional uncoded unicast and asymptotically achieves the optimal\nperformance proposed by Li et al.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:16:40 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2007.11354", "submitter": "Sin Kit Lo", "authors": "Sin Kit Lo, Qinghua Lu, Chen Wang, Hye-Young Paik, Liming Zhu", "title": "A Systematic Literature Review on Federated Machine Learning: From A\n  Software Engineering Perspective", "comments": "Published on ACM Computing Survey. Latest version available here:\n  https://dl.acm.org/doi/10.1145/3450288", "journal-ref": "ACM.CSUR.54.95 (2021) 1-39", "doi": "10.1145/3450288", "report-no": null, "categories": "cs.SE cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is an emerging machine learning paradigm where clients\ntrain models locally and formulate a global model based on the local model\nupdates. To identify the state-of-the-art in federated learning and explore how\nto develop federated learning systems, we perform a systematic literature\nreview from a software engineering perspective, based on 231 primary studies.\nOur data synthesis covers the lifecycle of federated learning system\ndevelopment that includes background understanding, requirement analysis,\narchitecture design, implementation, and evaluation. We highlight and summarise\nthe findings from the results, and identify future trends to encourage\nresearchers to advance their current work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:59:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 05:28:26 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 00:16:29 GMT"}, {"version": "v4", "created": "Mon, 27 Jul 2020 05:10:07 GMT"}, {"version": "v5", "created": "Sat, 24 Oct 2020 04:27:44 GMT"}, {"version": "v6", "created": "Tue, 27 Oct 2020 04:14:32 GMT"}, {"version": "v7", "created": "Fri, 4 Dec 2020 01:23:24 GMT"}, {"version": "v8", "created": "Wed, 26 May 2021 00:27:22 GMT"}, {"version": "v9", "created": "Fri, 28 May 2021 04:54:59 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lo", "Sin Kit", ""], ["Lu", "Qinghua", ""], ["Wang", "Chen", ""], ["Paik", "Hye-Young", ""], ["Zhu", "Liming", ""]]}, {"id": "2007.11360", "submitter": "Linyan Mei", "authors": "Linyan Mei, Pouya Houshmand, Vikram Jain, Sebastian Giraldo, Marian\n  Verhelst", "title": "ZigZag: A Memory-Centric Rapid DNN Accelerator Design Space Exploration\n  Framework", "comments": "14 pages, 20 figures. Source code is available at\n  https://github.com/ZigZag-Project/zigzag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building efficient embedded deep learning systems requires a tight co-design\nbetween DNN algorithms, memory hierarchy, and dataflow. However, owing to the\nlarge degrees of freedom in the design space, finding an optimal solution\nthrough the implementation of individual design points becomes infeasible.\nRecently, several estimation frameworks for fast design space exploration (DSE)\nhave emerged, yet they either suffer from long runtimes or a limited\nexploration space. This work introduces ZigZag, a memory-centric rapid DNN\naccelerator DSE framework which extends the DSE with uneven mapping\nopportunities, in which operands at shared memory levels are no longer bound to\nuse the same memory levels for each loop index. For this, ZigZag uses a\nmemory-centric nested-for-loop format as a uniform representation to integrate\nalgorithm, accelerator, and algorithm-to-accelerator mapping, and consists of\nthree key components: 1) a latency-enhanced analytical Hardware Cost Estimator,\n2) a Temporal Mapping Generator that supports even/uneven scheduling on any\ntype of memory hierarchy, and 3) an Architecture Generator that explores the\nwhole memory hierarchy design space. Benchmarking experiments against existing\nframeworks, together with three case studies at different design abstraction\nlevels show the strength of ZigZag. Up to 33% more energy-efficient solutions\nare found by introducing ZigZag's uneven scheduling opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 12:17:48 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 18:44:10 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 13:59:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Mei", "Linyan", ""], ["Houshmand", "Pouya", ""], ["Jain", "Vikram", ""], ["Giraldo", "Sebastian", ""], ["Verhelst", "Marian", ""]]}, {"id": "2007.11496", "submitter": "Jose Gracia", "authors": "Huan Zhou, Jose Gracia, Naweiluo Zhou, Ralf Schneider", "title": "Collectives in hybrid MPI+MPI code: design, practice and performance", "comments": "14 pages. Accepted for publication in Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of hybrid scheme combining the message passing programming models for\ninter-node parallelism and the shared memory programming models for node-level\nparallelism is widely spread. Existing extensive practices on hybrid Message\nPassing Interface (MPI) plus Open Multi-Processing (OpenMP) programming account\nfor its popularity. Nevertheless, strong programming efforts are required to\ngain performance benefits from the MPI+OpenMP code. An emerging hybrid method\nthat combines MPI and the MPI shared memory model (MPI+MPI) is promising.\nHowever, writing an efficient hybrid MPI+MPI program -- especially when the\ncollective communication operations are involved -- is not to be taken for\ngranted.\n  In this paper, we propose a new design method to implement hybrid MPI+MPI\ncontext-based collective communication operations. Our method avoids on-node\nmemory replications (on-node communication overheads) that are required by\nsemantics in pure MPI. We also offer wrapper primitives hiding all the design\ndetails from users, which comes with practices on how to structure hybrid\nMPI+MPI code with these primitives. The micro-benchmarks show that our\ncollectives are comparable or superior to those in pure MPI context. We have\nfurther validated the effectiveness of the hybrid MPI+MPI model (which uses our\nwrapper primitives) in three computational kernels, by comparison to the pure\nMPI and hybrid MPI+OpenMP models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 15:34:13 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhou", "Huan", ""], ["Gracia", "Jose", ""], ["Zhou", "Naweiluo", ""], ["Schneider", "Ralf", ""]]}, {"id": "2007.11651", "submitter": "Tin Vu", "authors": "Tin Vu, Ahmed Eldawy", "title": "R*-Grove: Balanced Spatial Partitioning for Large-scale Datasets", "comments": "29 pages, to be published in Frontiers in Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of big spatial data urged the research community to develop\nseveral big spatial data systems. Regardless of their architecture, one of the\nfundamental requirements of all these systems is to spatially partition the\ndata efficiently across machines. The core challenges of big spatial\npartitioning are building high spatial quality partitions while simultaneously\ntaking advantages of distributed processing models by providing load balanced\npartitions. Previous works on big spatial partitioning are to reuse existing\nindex search trees as-is, e.g., the R-tree family, STR, Kd-tree, and Quad-tree,\nby building a temporary tree for a sample of the input and use its leaf nodes\nas partition boundaries. However, we show in this paper that none of those\ntechniques has addressed the mentioned challenges completely. This paper\nproposes a novel partitioning method, termed R*-Grove, which can partition very\nlarge spatial datasets into high quality partitions with excellent load balance\nand block utilization. This appealing property allows R*-Grove to outperform\nexisting techniques in spatial query processing. R*-Grove can be easily\nintegrated into any big data platforms such as Apache Spark or Apache Hadoop.\nOur experiments show that R*-Grove outperforms the existing partitioning\ntechniques for big spatial data systems. With all the proposed work publicly\navailable as open source, we envision that R*-Grove will be adopted by the\ncommunity to better serve big spatial data research.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 20:08:41 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Vu", "Tin", ""], ["Eldawy", "Ahmed", ""]]}, {"id": "2007.11705", "submitter": "Sheik Mohammad Mostakim Fattah", "authors": "Sheik Mohammad Mostakim Fattah and Athman Bouguettaya", "title": "Event-based Detection of Changes in IaaS Performance Signatures", "comments": "8 pages, Accepted and to appear in 2020 International Conference on\n  Services Computing (IEEE SCC 2020). Content may change prior to final\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel ECA approach to manage changes in IaaS performance\nsignatures. The proposed approach relies on the detection of anomalous\nperformance behavior in the context of IaaS performance signatures. A novel\nanomaly-based event detection technique is proposed. It utilizes the experience\nof free trial users to detect potential changes in IaaS performance signatures.\nA signature change detection technique is proposed using the cumulative sum\ncontrol chart analysis. Additionally, a self-adjustment method is introduced to\nimprove the accuracy of the proposed approach. A set of experiments based on\nreal-world datasets are conducted to show the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 22:27:31 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 01:18:30 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fattah", "Sheik Mohammad Mostakim", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2007.11831", "submitter": "Qing Ye", "authors": "Qing Ye, Yuhao Zhou, Mingjia Shi, Yanan Sun, Jiancheng Lv", "title": "DBS: Dynamic Batch Size For Distributed Deep Neural Network Training", "comments": "NIPS2020 under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous strategies with data parallelism, such as the Synchronous\nStochasticGradient Descent (S-SGD) and the model averaging methods, are widely\nutilizedin distributed training of Deep Neural Networks (DNNs), largely owing\nto itseasy implementation yet promising performance. Particularly, each worker\nofthe cluster hosts a copy of the DNN and an evenly divided share of the\ndatasetwith the fixed mini-batch size, to keep the training of DNNs\nconvergence. In thestrategies, the workers with different computational\ncapability, need to wait foreach other because of the synchronization and\ndelays in network transmission,which will inevitably result in the\nhigh-performance workers wasting computation.Consequently, the utilization of\nthe cluster is relatively low. To alleviate thisissue, we propose the Dynamic\nBatch Size (DBS) strategy for the distributedtraining of DNNs. Specifically,\nthe performance of each worker is evaluatedfirst based on the fact in the\nprevious epoch, and then the batch size and datasetpartition are dynamically\nadjusted in consideration of the current performanceof the worker, thereby\nimproving the utilization of the cluster. To verify theeffectiveness of the\nproposed strategy, extensive experiments have been conducted,and the\nexperimental results indicate that the proposed strategy can fully utilizethe\nperformance of the cluster, reduce the training time, and have good\nrobustnesswith disturbance by irrelevant tasks. Furthermore, rigorous\ntheoretical analysis hasalso been provided to prove the convergence of the\nproposed strategy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 07:31:55 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ye", "Qing", ""], ["Zhou", "Yuhao", ""], ["Shi", "Mingjia", ""], ["Sun", "Yanan", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2007.12069", "submitter": "Quan Wang", "authors": "Quan Wang, Ignacio Lopez Moreno", "title": "Version Control of Speaker Recognition Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.DC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses one of the most challenging practical engineering\nproblems in speaker recognition systems - the version control of models and\nuser profiles. A typical speaker recognition system consists of two stages: the\nenrollment stage, where a profile is generated from user-provided enrollment\naudio; and the runtime stage, where the voice identity of the runtime audio is\ncompared against the stored profiles. As technology advances, the speaker\nrecognition system needs to be updated for better performance. However, if the\nstored user profiles are not updated accordingly, version mismatch will result\nin meaningless recognition results. In this paper, we describe different\nversion control strategies for different types of speaker recognition systems,\naccording to how they are deployed in the production environment.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:28:58 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 17:18:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Quan", ""], ["Moreno", "Ignacio Lopez", ""]]}, {"id": "2007.12174", "submitter": "Freark van der Berg", "authors": "Freark I. van der Berg", "title": "Recursive Variable-Length State Compression for Multi-Core Software\n  Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-performance multi-core software typically uses concurrent data\nstructures. Tests for such data structures have significantly smaller state\nspaces than the entire software, making it feasible to model check them.\nHowever, dynamic memory allocations on the heap complicate the use of standard\nfixed-length state vectors. In this paper, we introduce dtree, a concurrent\ncompression tree data structure that compactly stores variable-length states\nwhile allowing partial state reconstruction and incremental updates without\nconcretising states. It supports describing a state as a tree, allowing direct\nmodeling of the heap. We implemented dtree in DMC, our multi-core model\nchecker. We show that its performance approaches that of state-of-the-art model\ncheckers for fixed-length states. For models with variable-length states, dtree\nis up to 2.9 times faster.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:19:22 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:32:57 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["van der Berg", "Freark I.", ""]]}, {"id": "2007.12268", "submitter": "Eisa Alanazi", "authors": "Mohsin Kamal, Abdulah Aljohani, Eisa Alanazi", "title": "IoT meets COVID-19: Status, Challenges, and Opportunities", "comments": "17 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the global pandemic of COVID-19, there is an urgent need to utilize\nexisting technologies to their full potential. Internet of Things (IoT) is\nregarded as one of the most trending technologies with a great potential in\nfighting against the coronavirus outbreak. The IoT comprises of a scarce\nnetwork in which the IoT devices sense the environment and send useful data on\nthe Internet. In this paper, we examine the current status of IoT applications\nrelated to COVID-19, identify their deployment and operational challenges, and\nsuggest possible opportunities to further contain the pandemic. Furthermore, we\nperform analysis for implementing IoT in which internal and external factors\nare discussed.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 21:43:08 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kamal", "Mohsin", ""], ["Aljohani", "Abdulah", ""], ["Alanazi", "Eisa", ""]]}, {"id": "2007.12323", "submitter": "Huacheng Yu", "authors": "Huacheng Yu", "title": "Tight Distributed Sketching Lower Bound for Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the distributed sketching complexity of connectivity.\nIn distributed graph sketching, an $n$-node graph $G$ is distributed to $n$\nplayers such that each player sees the neighborhood of one vertex. The players\nthen simultaneously send one message to the referee, who must compute some\nfunction of $G$ with high probability. For connectivity, the referee must\noutput whether $G$ is connected. The goal is to minimize the message lengths.\nSuch sketching schemes are equivalent to one-round protocols in the broadcast\ncongested clique model.\n  We prove that the expected average message length must be at least\n$\\Omega(\\log^3 n)$ bits, if the error probability is at most $1/4$. It matches\nthe upper bound obtained by the AGM sketch [AGM12], which even allows the\nreferee to output a spanning forest of $G$ with probability\n$1-1/\\mathrm{poly}\\, n$. Our lower bound strengthens the previous\n$\\Omega(\\log^3 n)$ lower bound for spanning forest computation [NY19]. Hence,\nit implies that connectivity, a decision problem, is as hard as its \"search\"\nversion in this model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 02:41:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Yu", "Huacheng", ""]]}, {"id": "2007.12359", "submitter": "Shegufta Ahsan", "authors": "Shegufta Bakht Ahsan, Rui Yang, Shadi A. Noghabi, Indranil Gupta", "title": "Home, SafeHome: Smart Home Reliability with Visibility and Atomicity", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3447786.3456261", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart environments (homes, factories, hospitals, buildings) contain an\nincreasing number of IoT devices, making them complex to manage. Today, in\nsmart homes where users or triggers initiate routines (i.e., a sequence of\ncommands), concurrent routines and device failures can cause incongruent\noutcomes. We describe SafeHome, a system that provides notions of atomicity and\nserial equivalence for smart homes. Due to the human-facing nature of smart\nhomes, SafeHome offers a spectrum of {\\it visibility models} which trade off\nbetween responsiveness vs. incongruence of the smart home state. We implemented\nSafeHome and performed workload-driven experiments. We find that a weak\nvisibility model, called {\\it eventual visibility}, is almost as fast as\ntoday's status quo (up to 23\\% slower) and yet guarantees serially-equivalent\nend states.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:43:14 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ahsan", "Shegufta Bakht", ""], ["Yang", "Rui", ""], ["Noghabi", "Shadi A.", ""], ["Gupta", "Indranil", ""]]}, {"id": "2007.12487", "submitter": "Dipankar Chaki", "authors": "Dipankar Chaki, Athman Bouguettaya", "title": "Fine-grained Conflict Detection of IoT Services", "comments": "9 pages, 6 figures, 4 tables. This is an accepted paper and it is\n  going to appear in the Proceedings of the 2020 IEEE International Conference\n  on Services Computing (IEEE SCC 2020) affiliated with the 2020 IEEE World\n  Congress on Services (IEEE SERVICES 2020), Beijing, China. arXiv admin note:\n  text overlap with arXiv:2004.12702", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to detect conflicts among IoT services in a\nmulti-resident smart home. A fine-grained conflict model is proposed\nconsidering the functional and non-functional properties of IoT services. The\nproposed conflict model is designed using the concept of entropy and\ninformation gain from information theory. We use a novel algorithm based on\ntemporal proximity to detect conflicts. Experimental results on real-world\ndatasets show the efficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:15:57 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Chaki", "Dipankar", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2007.12557", "submitter": "Ziyao Liu", "authors": "Ziyao Liu, Ivan Tjuawinata, Chaoping Xing, Kwok-Yan Lam", "title": "MPC-enabled Privacy-Preserving Neural Network Training against Malicious\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of secure multiparty computation (MPC) in machine learning,\nespecially privacy-preserving neural network training, has attracted tremendous\nattention from the research community in recent years. MPC enables several data\nowners to jointly train a neural network while preserving the data privacy of\neach participant. However, most of the previous works focus on semi-honest\nthreat model that cannot withstand fraudulent messages sent by malicious\nparticipants. In this paper, we propose an approach for constructing efficient\n$n$-party protocols for secure neural network training that can provide\nsecurity for all honest participants even when a majority of the parties are\nmalicious. Compared to the other designs that provide semi-honest security in a\ndishonest majority setting, our actively secure neural network training incurs\naffordable efficiency overheads of around 2X and 2.7X in LAN and WAN settings,\nrespectively. Besides, we propose a scheme to allow additive shares defined\nover an integer ring $\\mathbb{Z}_N$ to be securely converted to additive shares\nover a finite field $\\mathbb{Z}_Q$, which may be of independent interest. Such\nconversion scheme is essential in securely and correctly converting shared\nBeaver triples defined over an integer ring generated in the preprocessing\nphase to triples defined over a field to be used in the calculation in the\nonline phase.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 15:03:51 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 07:11:55 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2021 05:51:53 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Liu", "Ziyao", ""], ["Tjuawinata", "Ivan", ""], ["Xing", "Chaoping", ""], ["Lam", "Kwok-Yan", ""]]}, {"id": "2007.12637", "submitter": "Manuel Bravo", "authors": "Manuel Bravo and Zsolt Istv\\'an and Man-Kit Sit", "title": "Towards Improving the Performance of BFT Consensus For Future\n  Permissioned Blockchains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permissioned Blockchains are increasingly considered in enterprise use-cases,\nmany of which do not require geo-distribution, or even disallow it due to\nlegislation. Examples include country-wide networks, such as Alastria, or those\ndeployed using cloud-based platforms such as IBM Blockchain Platform. We expect\nthese blockchains to eventually run in environments with high bandwidth and low\nlatency modern networks, as well as, advanced programmable hardware\naccelerators in servers.\n  Even though there is renewed interest in BFT consensus algorithms with\nvarious proposals targeting Permissioned Blockchains, related work does not\noptimize for fast networks and does not incorporate hardware accelerators -- we\nmake the case that doing so will pay off in the long run. To this end, we\nre-implemented the seminal PBFT algorithm in a way that allows us to measure\ndifferent configurations of the protocol. Through this we explore the benefits\nof various common optimization strategies and show that the protocol is\nunlikely to saturate more than 10Gbps networks without relying on specialized\nhardware-based offloading. We discuss two concrete ways in which the cost of\nconsensus in Permissioned Blockchains could be reduced in high speed networking\nenvironments, namely, offloading to SmartNICs and implementing the protocol on\nstandalone FPGAs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:44:47 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Bravo", "Manuel", ""], ["Istv\u00e1n", "Zsolt", ""], ["Sit", "Man-Kit", ""]]}, {"id": "2007.12648", "submitter": "Kostas Kolomvatsos", "authors": "Kostas Kolomvatsos", "title": "An Intelligent Scheme for Uncertainty Management of Data Synopses\n  Management in Pervasive Computing Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive computing applications deal with the incorporation of intelligent\ncomponents around end users to facilitate their activities. Such applications\ncan be provided upon the vast infrastructures of Internet of Things (IoT) and\nEdge Computing (EC). IoT devices collect ambient data transferring them towards\nthe EC and Cloud for further processing. EC nodes could become the hosts of\ndistributed datasets where various processing activities take place. The future\nof EC involves numerous nodes interacting with the IoT devices and themselves\nin a cooperative manner to realize the desired processing. A critical issue for\nconcluding this cooperative approach is the exchange of data synopses to have\nEC nodes informed about the data present in their peers. Such knowledge will be\nuseful for decision making related to the execution of processing activities.\nIn this paper, we propose n uncertainty driven model for the exchange of data\nsynopses. We argue that EC nodes should delay the exchange of synopses\nespecially when no significant differences with historical values are present.\nOur mechanism adopts a Fuzzy Logic (FL) system to decide when there is a\nsignificant difference with the previous reported synopses to decide the\nexchange of the new one. Our scheme is capable of alleviating the network from\nnumerous messages retrieved even for low fluctuations in synopses. We\nanalytically describe our model and evaluate it through a large set of\nexperiments. Our experimental evaluation targets to detect the efficiency of\nthe approach based on the elimination of unnecessary messages while keeping\nimmediately informed peer nodes for significant statistical changes in the\ndistributed datasets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:58:51 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kolomvatsos", "Kostas", ""]]}, {"id": "2007.12856", "submitter": "Yosuke Oyama", "authors": "Yosuke Oyama, Naoya Maruyama, Nikoli Dryden, Erin McCarthy, Peter\n  Harrington, Jan Balewski, Satoshi Matsuoka, Peter Nugent, Brian Van Essen", "title": "The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs\n  with Hybrid Parallelism", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "LLNL-JRNL-812691", "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present scalable hybrid-parallel algorithms for training large-scale 3D\nconvolutional neural networks. Deep learning-based emerging scientific\nworkflows often require model training with large, high-dimensional samples,\nwhich can make training much more costly and even infeasible due to excessive\nmemory usage. We solve these challenges by extensively applying hybrid\nparallelism throughout the end-to-end training pipeline, including both\ncomputations and I/O. Our hybrid-parallel algorithm extends the standard data\nparallelism with spatial parallelism, which partitions a single sample in the\nspatial domain, realizing strong scaling beyond the mini-batch dimension with a\nlarger aggregated memory capacity. We evaluate our proposed training algorithms\nwith two challenging 3D CNNs, CosmoFlow and 3D U-Net. Our comprehensive\nperformance studies show that good weak and strong scaling can be achieved for\nboth networks using up 2K GPUs. More importantly, we enable training of\nCosmoFlow with much larger samples than previously possible, realizing an\norder-of-magnitude improvement in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 05:06:06 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Oyama", "Yosuke", ""], ["Maruyama", "Naoya", ""], ["Dryden", "Nikoli", ""], ["McCarthy", "Erin", ""], ["Harrington", "Peter", ""], ["Balewski", "Jan", ""], ["Matsuoka", "Satoshi", ""], ["Nugent", "Peter", ""], ["Van Essen", "Brian", ""]]}, {"id": "2007.12860", "submitter": "Kostas Kolomvatsos", "authors": "Panagiotis Fountas, Kostas Kolomvatsos", "title": "A Data Imputation Model based on an Ensemble Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge Computing (EC) offers an infrastructure that acts as the mediator\nbetween the Cloud and the Internet of Things (IoT). The goal is to reduce the\nlatency that we enjoy when relying on Cloud. IoT devices interact with their\nenvironment to collect data relaying them towards the Cloud through the EC.\nVarious services can be provided at the EC for the immediate management of the\ncollected data. One significant task is the management of missing values. In\nthis paper, we propose an ensemble based approach for data imputation that\ntakes into consideration the spatio-temporal aspect of the collected data and\nthe reporting devices. We propose to rely on the group of IoT devices that\nresemble to the device reporting missing data and enhance its data imputation\nprocess. We continuously reason on the correlation of the reported streams and\nefficiently combine the available data. Our aim is to `aggregate' the local\nview on the appropriate replacement with the `opinion' of the group. We adopt\nwidely known similarity techniques and a statistical modelling methodology to\ndeliver the final outcome. We provide the description of our model and evaluate\nit through a high number of simulations adopting various experimental\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 05:39:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Fountas", "Panagiotis", ""], ["Kolomvatsos", "Kostas", ""]]}, {"id": "2007.12908", "submitter": "Alena Kopanicakova", "authors": "Patrick Zulian and Alena Kopani\\v{c}\\'akov\\'a and Maria Giuseppina\n  Chiara Nestola and Andreas Fink and Nur Aiman Fadel and Joost Vandevondele\n  and Rolf Krause", "title": "Large scale simulation of pressure induced phase-field fracture\n  propagation using Utopia", "comments": "CCF Trans. HPC (2021)", "journal-ref": null, "doi": "10.1007/s42514-021-00069-6", "report-no": null, "categories": "physics.comp-ph cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear phase field models are increasingly used for the simulation of\nfracture propagation models. The numerical simulation of fracture networks of\nrealistic size requires the efficient parallel solution of large coupled\nnon-linear systems. Although in principle efficient iterative multi-level\nmethods for these types of problems are available, they are not widely used in\npractice due to the complexity of their parallel implementation.\n  Here, we present Utopia, which is an open-source C++ library for parallel\nnon-linear multilevel solution strategies. Utopia provides the advantages of\nhigh-level programming interfaces while at the same time a framework to access\nlow-level data-structures without breaking code encapsulation. Complex\nnumerical procedures can be expressed with few lines of code, and evaluated by\ndifferent implementations, libraries, or computing hardware. In this paper, we\ninvestigate the parallel performance of our implementation of the recursive\nmultilevel trust-region (RMTR) method based on the Utopia library. RMTR is a\nglobally convergent multilevel solution strategy designed to solve non-convex\nconstrained minimization problems. In particular, we solve pressure-induced\nphase-field fracture propagation in large and complex fracture networks.\nSolving such problems is deemed challenging even for a few fractures, however,\nhere we are considering networks of realistic size with up to 1000 fractures.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 10:50:38 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Zulian", "Patrick", ""], ["Kopani\u010d\u00e1kov\u00e1", "Alena", ""], ["Nestola", "Maria Giuseppina Chiara", ""], ["Fink", "Andreas", ""], ["Fadel", "Nur Aiman", ""], ["Vandevondele", "Joost", ""], ["Krause", "Rolf", ""]]}, {"id": "2007.13030", "submitter": "Vidhya Tekken Valapil", "authors": "Vidhya Tekken Valapil, Sandeep Kulkarni, Eric Torng, and Gabe Appleton", "title": "Efficient Two-Layered Monitor for Partially Synchronous Distributed\n  Systems (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring distributed systems to ensure their correctness is a challenging\nand expensive but essential problem. It is challenging because while execution\nof a distributed system creates a partial order among events, the monitor will\ntypically observe only one serialization of that partial order. This means that\neven if the observed serialization is consistent with the system\nspecifications, the monitor cannot assume that the system is correct because\nsome other unobserved serialization can be inconsistent with the system\nspecifications. Existing solutions that guarantee identification of all such\nunobserved violations require some combination of lots of time and large\nclocks, e.g. O(n) sized Vector Clocks.\n  We present a new, efficient two-layered monitoring approach that overcomes\nboth the time and space limitations of earlier monitors. The first layer is\nimprecise but efficient and the second layer is precise but (relatively)\ninefficient. We show that the combination of these two layers reduces the cost\nof monitoring by 85-95%. Furthermore, the two-layered monitor permits the use\nof O(1) sized Hybrid Logical Clocks.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 23:45:31 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Valapil", "Vidhya Tekken", ""], ["Kulkarni", "Sandeep", ""], ["Torng", "Eric", ""], ["Appleton", "Gabe", ""]]}, {"id": "2007.13055", "submitter": "Zijing Gu", "authors": "Zijing Gu", "title": "Optimizing Block-Sparse Matrix Multiplications on CUDA with TVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implemented and optimized matrix multiplications between dense and\nblock-sparse matrices on CUDA. We leveraged TVM, a deep learning compiler, to\nexplore the schedule space of the operation and generate efficient CUDA code.\nWith the automatic parameter tuning in TVM, our cross-thread reduction based\nimplementation achieved competitive or better performance compared with other\nstate-of-the-art frameworks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 04:50:51 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gu", "Zijing", ""]]}, {"id": "2007.13137", "submitter": "Hung Nguyen", "authors": "Hung T. Nguyen, Vikash Sehwag, Seyyedali Hosseinalipour, Christopher\n  G. Brinton, Mung Chiang, H. Vincent Poor", "title": "Fast-Convergent Federated Learning", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Federated learning has emerged recently as a promising solution for\ndistributing machine learning tasks through modern networks of mobile devices.\nRecent studies have obtained lower bounds on the expected decrease in model\nloss that is achieved through each round of federated learning. However,\nconvergence generally requires a large number of communication rounds, which\ninduces delay in model training and is costly in terms of network resources. In\nthis paper, we propose a fast-convergent federated learning algorithm, called\nFOLB, which performs intelligent sampling of devices in each round of model\ntraining to optimize the expected convergence speed. We first theoretically\ncharacterize a lower bound on improvement that can be obtained in each round if\ndevices are selected according to the expected improvement their local models\nwill provide to the current global model. Then, we show that FOLB obtains this\nbound through uniform sampling by weighting device updates according to their\ngradient information. FOLB is able to handle both communication and computation\nheterogeneity of devices by adapting the aggregations according to estimates of\ndevice's capabilities of contributing to the updates. We evaluate FOLB in\ncomparison with existing federated learning algorithms and experimentally show\nits improvement in trained model accuracy, convergence speed, and/or model\nstability across various machine learning tasks and datasets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:37:51 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 11:21:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Nguyen", "Hung T.", ""], ["Sehwag", "Vikash", ""], ["Hosseinalipour", "Seyyedali", ""], ["Brinton", "Christopher G.", ""], ["Chiang", "Mung", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2007.13175", "submitter": "Atsuki Momose", "authors": "Atsuki Momose, Ling Ren", "title": "Optimal Communication Complexity of Authenticated Byzantine Agreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Byzantine Agreement (BA) is one of the most fundamental problems in\ndistributed computing, and its communication complexity is an important\nefficiency metric. It is well known that quadratic communication is necessary\nfor BA in the worst case due to a lower bound by Dolev and Reischuk. This lower\nbound has been shown to be tight for the unauthenticated setting with $f < n/3$\nby Berman et al. but a considerable gap remains for the authenticated setting\nwith $n/3 \\le f < n/2$.\n  This paper provides two results towards closing this gap. Both protocols have\na quadratic communication complexity and have different trade-offs in\nresilience and assumptions. The first protocol achieves the optimal resilience\nof $f < n/2$ but requires a trusted setup for threshold signature. The second\nprotocol achieves near optimal resilience $f \\le (1/2 - \\varepsilon)n$ in the\nstandard PKI model.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 16:45:50 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 06:25:09 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 12:02:23 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 12:59:10 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Momose", "Atsuki", ""], ["Ren", "Ling", ""]]}, {"id": "2007.13200", "submitter": "Yahya Hassanzadeh Nazarabadi", "authors": "Yahya Hassanzadeh-Nazarabadi, Ali Utkan \\c{S}ahin, \\\"Oznur \\\"Ozkasap,\n  Alptekin K\\\"up\\c{c}\\\"u", "title": "SkipSim: Scalable Skip Graph Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SkipSim is an offline Skip Graph simulator that enables Skip Graph-based\nalgorithms including blockchains and P2P cloud storage to be simulated while\npreserving their scalability and decentralized nature. To the best of our\nknowledge, it is the first Skip Graph simulator that provides several features\nfor experimentation on Skip Graph-based overlay networks. In this demo paper,\nwe present SkipSim features, its architecture, as well as a sample blockchain\ndemo scenario.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 19:21:40 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Hassanzadeh-Nazarabadi", "Yahya", ""], ["\u015eahin", "Ali Utkan", ""], ["\u00d6zkasap", "\u00d6znur", ""], ["K\u00fcp\u00e7\u00fc", "Alptekin", ""]]}, {"id": "2007.13203", "submitter": "Yahya Hassanzadeh Nazarabadi", "authors": "Yahya Hassanzadeh-Nazarabadi, Nazir Nayal, Shadi Sameh Hamdan, \\\"Oznur\n  \\\"Ozkasap, Alptekin K\\\"up\\c{c}\\\"u", "title": "A containerized proof-of-concept implementation of LightChain system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LightChain is the first Distributed Hash Table (DHT)-based blockchain with a\nlogarithmic asymptotic message and memory complexity. In this demo paper, we\npresent the software architecture of our open-source implementation of\nLightChain, as well as a novel deployment scenario of the entire LightChain\nsystem on a single machine aiming at results reproducibility.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 19:36:39 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Hassanzadeh-Nazarabadi", "Yahya", ""], ["Nayal", "Nazir", ""], ["Hamdan", "Shadi Sameh", ""], ["\u00d6zkasap", "\u00d6znur", ""], ["K\u00fcp\u00e7\u00fc", "Alptekin", ""]]}, {"id": "2007.13212", "submitter": "Sanaz Taheri Boshrooyeh", "authors": "Sanaz Taheri-Boshrooyeh, Ali Utkan \\c{S}ahin, Yahya\n  Hassanzadeh-Nazarabadi, \\\"Oznur \\\"Ozkasap", "title": "Demo: A Proof-of-Concept Implementation of Guard Secure Routing Protocol", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skip Graphs belong to the family of Distributed Hash Table (DHT) structures\nthat are utilized as routing overlays in various peer-to-peer applications\nincluding blockchains, cloud storage, and social networks. In a Skip Graph\noverlay, any misbehavior of peers during the routing of a query compromises the\nsystem functionality. Guard is the first authenticated search mechanism for\nSkip Graphs, enables reliable search operation in a fully decentralized manner.\nIn this demo paper, we present a proof-of-concept implementation of Guard on\nSkip Graph nodes as well as a deployment demo scenario.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 20:28:30 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Taheri-Boshrooyeh", "Sanaz", ""], ["\u015eahin", "Ali Utkan", ""], ["Hassanzadeh-Nazarabadi", "Yahya", ""], ["\u00d6zkasap", "\u00d6znur", ""]]}, {"id": "2007.13221", "submitter": "Cong Xie", "authors": "Cong Xie, Shuai Zheng, Oluwasanmi Koyejo, Indranil Gupta, Mu Li,\n  Haibin Lin", "title": "CSER: Communication-efficient SGD with Error Reset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of Distributed Stochastic Gradient Descent (SGD) is today\nlimited by communication bottlenecks. We propose a novel SGD variant:\nCommunication-efficient SGD with Error Reset, or CSER. The key idea in CSER is\nfirst a new technique called \"error reset\" that adapts arbitrary compressors\nfor SGD, producing bifurcated local models with periodic reset of resulting\nlocal residual errors. Second we introduce partial synchronization for both the\ngradients and the models, leveraging advantages from them. We prove the\nconvergence of CSER for smooth non-convex problems. Empirical results show that\nwhen combined with highly aggressive compressors, the CSER algorithms\naccelerate the distributed training by nearly 10x for CIFAR-100, and by 4.5x\nfor ImageNet.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 21:23:31 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 20:28:58 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 00:06:28 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Xie", "Cong", ""], ["Zheng", "Shuai", ""], ["Koyejo", "Oluwasanmi", ""], ["Gupta", "Indranil", ""], ["Li", "Mu", ""], ["Lin", "Haibin", ""]]}, {"id": "2007.13289", "submitter": "Haitao Xiang", "authors": "Haitao Xiang, Zhijie Ren, Ziheng Zhou, Ning Wang, Hanqing Jin", "title": "AlphaBlock: An Evaluation Framework for Blockchain Consensus Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus protocols play a pivotal role to balance security and efficiency in\nblockchain systems. In this paper, we propose an evaluation framework for\nblockchain consensus protocols termed as AlphaBlock. In this framework, we\ncompare the overall performance of Byzantine Fault Tolerant (BFT) consensus and\nNakamoto Consensus (NC). BFT consensus is reached by multiple rounds of quorum\nvotes from the supermajority, while NC is reached by accumulating credibility\nwith the implicit voting from appending blocks. AlphaBlock incorporates the key\nconcepts of Hotstu BFT (HBFT) and Proof-of-authority (PoA) as the case study of\nBFT and NC. Using this framework, we compare the throughput and latency of HBFT\nand PoA with practical network and blockchain configurations. Our results show\nthat the performance of HBFT dominates PoA in most scenarios due to the absence\nof forks in HBFT. Moreover, we find out a set of optimal configurations in\nAlphaBlock, which sheds a light for improving the performance of blockchain\nconsensus algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 03:05:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Xiang", "Haitao", ""], ["Ren", "Zhijie", ""], ["Zhou", "Ziheng", ""], ["Wang", "Ning", ""], ["Jin", "Hanqing", ""]]}, {"id": "2007.13460", "submitter": "Martin Nischwitz", "authors": "Martin Nischwitz, Marko Esche, Florian Tschorsch", "title": "Bernoulli Meets PBFT: Modeling BFT Protocols in the Presence of Dynamic\n  Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The publication of the pivotal state machine replication protocol PBFT laid\nthe foundation for a large body of BFT protocols. While many successors to PBFT\nhave been developed, there is no general technique to compare these protocols\nunder realistic network conditions such as unreliable links. In this paper, we\nintroduce a probabilistic model for evaluating BFT protocols in the presence of\ndynamic link and crash failures. Based on modeling techniques from\ncommunication theory, the network state of replicas is captured and used to\nderive the success probability of the protocol execution. To this end, we\nexamine the influence of link and crash failure rates as well as the number of\nreplicas. The model is derived from the communication pattern, making it\nimplementation-independent and facilitating an adaptation to other BFT\nprotocols. The model is validated with a simulation of PBFT and BFT-SMaRt.\nFurther, a comparison in protocol behavior of PBFT, Zyzzyva and SBFT is\nperformed and critical failure thresholds are identified.e thresholds are\nidentified.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 12:03:54 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:06:23 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Nischwitz", "Martin", ""], ["Esche", "Marko", ""], ["Tschorsch", "Florian", ""]]}, {"id": "2007.13552", "submitter": "Markus G\\\"otz", "authors": "Markus G\\\"otz, Daniel Coquelin, Charlotte Debus, Kai Krajsek, Claudia\n  Comito, Philipp Knechtges, Bj\\\"orn Hagemeier, Michael Tarnawa, Simon\n  Hanselmann, Martin Siggel, Achim Basermann, Achim Streit", "title": "HeAT -- a Distributed and GPU-accelerated Tensor Framework for Data\n  Analytics", "comments": "10 pages, 8 figures, 5 listings, 1 table", "journal-ref": null, "doi": "10.1109/BigData50022.2020.9378050", "report-no": null, "categories": "cs.DC cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To cope with the rapid growth in available data, the efficiency of data\nanalysis and machine learning libraries has recently received increased\nattention. Although great advancements have been made in traditional\narray-based computations, most are limited by the resources available on a\nsingle computation node. Consequently, novel approaches must be made to exploit\ndistributed resources, e.g. distributed memory architectures. To this end, we\nintroduce HeAT, an array-based numerical programming framework for large-scale\nparallel processing with an easy-to-use NumPy-like API. HeAT utilizes PyTorch\nas a node-local eager execution engine and distributes the workload on\narbitrarily large high-performance computing systems via MPI. It provides both\nlow-level array computations, as well as assorted higher-level algorithms. With\nHeAT, it is possible for a NumPy user to take full advantage of their available\nresources, significantly lowering the barrier to distributed data analysis.\nWhen compared to similar frameworks, HeAT achieves speedups of up to two orders\nof magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:33:17 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 08:12:25 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["G\u00f6tz", "Markus", ""], ["Coquelin", "Daniel", ""], ["Debus", "Charlotte", ""], ["Krajsek", "Kai", ""], ["Comito", "Claudia", ""], ["Knechtges", "Philipp", ""], ["Hagemeier", "Bj\u00f6rn", ""], ["Tarnawa", "Michael", ""], ["Hanselmann", "Simon", ""], ["Siggel", "Martin", ""], ["Basermann", "Achim", ""], ["Streit", "Achim", ""]]}, {"id": "2007.13594", "submitter": "Silvia Butti", "authors": "Silvia Butti, Victor Dalmau", "title": "The Complexity of the Distributed Constraint Satisfaction Problem", "comments": "Full version of a STACS'21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the Distributed Constraint Satisfaction Problem\n(DCSP) on a synchronous, anonymous network from a theoretical standpoint. In\nthis setting, variables and constraints are controlled by agents which\ncommunicate with each other by sending messages through fixed communication\nchannels. Our results endorse the well-known fact from classical CSPs that the\ncomplexity of fixed-template computational problems depends on the template's\ninvariance under certain operations. Specifically, we show that DCSP($\\Gamma$)\nis polynomial-time tractable if and only if $\\Gamma$ is invariant under\nsymmetric polymorphisms of all arities. Otherwise, there are no algorithms that\nsolve DCSP($\\Gamma$) in finite time. We also show that the same condition holds\nfor the search variant of DCSP. Collaterally, our results unveil a feature of\nthe processes' neighbourhood in a distributed network, its iterated degree,\nwhich plays a major role in the analysis. We explore this notion establishing a\ntight connection with the basic linear programming relaxation of a CSP.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 14:23:26 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 13:16:37 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Butti", "Silvia", ""], ["Dalmau", "Victor", ""]]}, {"id": "2007.13631", "submitter": "Leonardo Ravaglia", "authors": "Leonardo Ravaglia, Manuele Rusci, Alessandro Capotondi, Francesco\n  Conti, Lorenzo Pellegrini, Vincenzo Lomonaco, Davide Maltoni, Luca Benini", "title": "Memory-Latency-Accuracy Trade-offs for Continual Learning on a RISC-V\n  Extreme-Edge Node", "comments": "6 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI-powered edge devices currently lack the ability to adapt their embedded\ninference models to the ever-changing environment. To tackle this issue,\nContinual Learning (CL) strategies aim at incrementally improving the decision\ncapabilities based on newly acquired data. In this work, after quantifying\nmemory and computational requirements of CL algorithms, we define a novel HW/SW\nextreme-edge platform featuring a low power RISC-V octa-core cluster tailored\nfor on-demand incremental learning over locally sensed data. The presented\nmulti-core HW/SW architecture achieves a peak performance of 2.21 and 1.70\nMAC/cycle, respectively, when running forward and backward steps of the\ngradient descent. We report the trade-off between memory footprint, latency,\nand accuracy for learning a new class with Latent Replay CL when targeting an\nimage classification task on the CORe50 dataset. For a CL setting that retrains\nall the layers, taking 5h to learn a new class and achieving up to 77.3% of\nprecision, a more efficient solution retrains only part of the network,\nreaching an accuracy of 72.5% with a memory requirement of 300 MB and a\ncomputation latency of 1.5 hours. On the other side, retraining only the last\nlayer results in the fastest (867 ms) and less memory hungry (20 MB) solution\nbut scoring 58% on the CORe50 dataset. Thanks to the parallelism of the\nlow-power cluster engine, our HW/SW platform results 25x faster than typical\nMCU device, on which CL is still impractical, and demonstrates an 11x gain in\nterms of energy consumption with respect to mobile-class solutions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:44:10 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ravaglia", "Leonardo", ""], ["Rusci", "Manuele", ""], ["Capotondi", "Alessandro", ""], ["Conti", "Francesco", ""], ["Pellegrini", "Lorenzo", ""], ["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""], ["Benini", "Luca", ""]]}, {"id": "2007.13647", "submitter": "Pranav Singh Kumar", "authors": "Pranav Kumar Singh, Roshan Singh, and Sukumar Nandi", "title": "V-CARE: A Blockchain Based Framework for Secure Vehicle Health Record\n  System", "comments": "The work is published in IEEE COMSOC MMTC Communications - Frontiers\n  Vol. 15, No. 4, July 2020, Page 12-17", "journal-ref": null, "doi": null, "report-no": "https://mmc.committees.comsoc.org/files/2020/07/MMTC_Communication_Frontier_July_2020.pdf", "categories": "cs.DC cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the biggest challenges associated with connected and autonomous\nvehicles (CAVs) is to maintain and make use of vehicles health records (VHR).\nVHR can facilitate different entities to offer various services in a proactive,\ntransparent, secure, reliable and in an efficient manner. The state-of-the-art\nsolutions for maintaining the VHR are centralized in nature, mainly owned by\nmanufacturer and authorized in-vehicle device developers. Owners, drivers, and\nother key service providers have limited accessibility and control to the VHR.\nWe need to change the strategy from single or limited party access to\nmulti-party access to VHR in an secured manner so that all stakeholders of\nintelligent transportation system (ITS) can be benefited from this. Any\nunauthorized attempt to alter the data should also be prevented. Blockchain is\none such potential candidate, which can facilitate the sharing of such data\namong different participating organizations and individuals. For example,\nowners, manufacturers, trusted third parties, road authorities, insurance\ncompanies, charging stations, and car selling ventures can access VHR stored on\nthe blockchain in a permissioned, secured, and with a higher level of\nconfidence. In this paper, a blockchain-based decentralized secure system for\nV-CARE is proposed to manage records in an interoperable framework that leads\nto improved ITS services in terms of safety, availability, reliability,\nefficiency, and maintenance. Insurance based on pay-how-you-drive (PHYD), and\nsale and purchase of used vehicles can also be made more transparent and\nreliable without compromising the confidentiality and security of sensitive\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:37:18 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Singh", "Pranav Kumar", ""], ["Singh", "Roshan", ""], ["Nandi", "Sukumar", ""]]}, {"id": "2007.13648", "submitter": "Perry Gibson", "authors": "Perry Gibson, Jos\\'e Cano", "title": "Orpheus: A New Deep Learning Framework for Easy Deployment and\n  Evaluation of Edge Inference", "comments": "To be published as a poster in 2020 IEEE International Symposium on\n  Performance Analysis of Systems and Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV cs.LG cs.PF stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Optimising deep learning inference across edge devices and optimisation\ntargets such as inference time, memory footprint and power consumption is a key\nchallenge due to the ubiquity of neural networks. Today, production deep\nlearning frameworks provide useful abstractions to aid machine learning\nengineers and systems researchers. However, in exchange they can suffer from\ncompatibility challenges (especially on constrained platforms), inaccessible\ncode complexity, or design choices that otherwise limit research from a systems\nperspective. This paper presents Orpheus, a new deep learning framework for\neasy prototyping, deployment and evaluation of inference optimisations. Orpheus\nfeatures a small codebase, minimal dependencies, and a simple process for\nintegrating other third party systems. We present some preliminary evaluation\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 14:54:40 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 20:58:35 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gibson", "Perry", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2007.13673", "submitter": "Francesco Palini", "authors": "Umberto Ferraro Petrillo, Francesco Palini, Giuseppe Cattaneo,\n  Raffaele Giancarlo", "title": "FASTA/Q Data Compressors for MapReduce-Hadoop Genomics:Space and Time\n  Savings Made Easy -- Version 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Storage of genomic data is a major cost for the Life Sciences,\neffectively addressed mostly via specialized data compression methods. For the\nsame reasons of abundance in data production, the use of Big Data technologies\nis seen as the future for genomic data storage and processing, with\nMapReduce-Hadoop as leaders. Somewhat surprisingly, none of the specialized\nFASTA/Q compressors is available within Hadoop. Indeed, their deployment there\nis not exactly immediate. Such a State of the Art is problematic. Results: We\nprovide major advances in two different directions. Methodologically, we\npropose two general methods, with the corresponding software, that make very\neasy to deploy a specialized FASTA/Q compressor within MapReduce-Hadoop for\nprocessing files stored on the distributed Hadoop File System, with very little\nknowledge of Hadoop. Practically, we provide evidence that the deployment of\nthose specialized compressors within Hadoop, not available so far, results in\nmajor cost savings, i.e., on large plant genomes, 30% less HDFS data blocks\n(one block=128MB), speed-up of at least x1.5 in I/O time and comparable or\nreduced network communication time with respect to the use of generic\ncompressors available in Hadoop. Finally, we observe that these results hold\nalso for the Apache Spark framework, when used to process FASTA/Q files stored\non the Hadoop File System.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:40:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Petrillo", "Umberto Ferraro", ""], ["Palini", "Francesco", ""], ["Cattaneo", "Giuseppe", ""], ["Giancarlo", "Raffaele", ""]]}, {"id": "2007.14135", "submitter": "Alptekin Temizel", "authors": "Hamza Eray, Alptekin Temizel", "title": "Performance Analysis of Noise Subspace-based Narrowband\n  Direction-of-Arrival (DOA) Estimation Algorithms on CPU and GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance computing of array signal processing problems is a critical\ntask as real-time system performance is required for many applications. Noise\nsubspace-based Direction-of-Arrival (DOA) estimation algorithms are popular in\nthe literature since they provide higher angular resolution and higher\nrobustness. In this study, we investigate various optimization strategies for\nhigh-performance DOA estimation on GPU and comparatively analyze alternative\nimplementations (MATLAB, C/C++ and CUDA). Experiments show that up to 3.1x\nspeedup can be achieved on GPU compared to the baseline multi-threaded CPU\nimplementation. The source code is publicly available at the following link:\nhttps://github.com/erayhamza/NssDOACuda\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 11:40:49 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Eray", "Hamza", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.14152", "submitter": "Carl Pearson", "authors": "Mert Hidayetoglu, Carl Pearson, Vikram Sharma Mailthody, Eiman\n  Ebrahimi, Jinjun Xiong, Rakesh Nagi, Wen-Mei Hwu", "title": "At-Scale Sparse Deep Neural Network Inference with Efficient GPU\n  Implementation", "comments": "7 pages", "journal-ref": "High Performance Extreme Computing (2020)", "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GPU performance optimization and scaling results for\ninference models of the Sparse Deep Neural Network Challenge 2020. Demands for\nnetwork quality have increased rapidly, pushing the size and thus the memory\nrequirements of many neural networks beyond the capacity of available\naccelerators. Sparse deep neural networks (SpDNN) have shown promise for\nreining in the memory footprint of large neural networks. However, there is\nroom for improvement in implementing SpDNN operations on GPUs. This work\npresents optimized sparse matrix multiplication kernels fused with the ReLU\nfunction. The optimized kernels reuse input feature maps from the shared memory\nand sparse weights from registers. For multi-GPU parallelism, our SpDNN\nimplementation duplicates weights and statically partition the feature maps\nacross GPUs. Results for the challenge benchmarks show that the proposed kernel\ndesign and multi-GPU parallelization achieve up to 180 tera-edges per second\ninference throughput. These results are up to 4.3x faster for a single GPU and\nan order of magnitude faster at full scale than those of the champion of the\n2019 Sparse Deep Neural Network Graph Challenge for the same generation of\nNVIDIA V100 GPUs. Using the same implementation, we also show single-GPU\nthroughput on NVIDIA A100 is 2.37$\\times$ faster than V100.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 12:09:43 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 23:30:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Hidayetoglu", "Mert", ""], ["Pearson", "Carl", ""], ["Mailthody", "Vikram Sharma", ""], ["Ebrahimi", "Eiman", ""], ["Xiong", "Jinjun", ""], ["Nagi", "Rakesh", ""], ["Hwu", "Wen-Mei", ""]]}, {"id": "2007.14178", "submitter": "Alptekin Temizel", "authors": "Mete Can Kaya, Alperen \\.Inci, Alptekin Temizel", "title": "Optimization of XNOR Convolution for Binary Convolutional Neural\n  Networks on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary convolutional networks have lower computational load and lower memory\nfoot-print compared to their full-precision counterparts. So, they are a\nfeasible alternative for the deployment of computer vision applications on\nlimited capacity embedded devices. Once trained on less resource-constrained\ncomputational environments, they can be deployed for real-time inference on\nsuch devices. In this study, we propose an implementation of binary\nconvolutional network inference on GPU by focusing on optimization of XNOR\nconvolution. Experimental results show that using GPU can provide a speed-up of\nup to $42.61\\times$ with a kernel size of $3\\times3$. The implementation is\npublicly available at\nhttps://github.com/metcan/Binary-Convolutional-Neural-Network-Inference-on-GPU\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:01:17 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Kaya", "Mete Can", ""], ["\u0130nci", "Alperen", ""], ["Temizel", "Alptekin", ""]]}, {"id": "2007.14215", "submitter": "Muhammad Asim", "authors": "Muhammad Asim, Yong Wang, Kezhi Wang, and Pei-Qiu Huang", "title": "A Review on Computational Intelligence Techniques in Cloud and Edge\n  Computing", "comments": "Accepted by IEEE Transactions on Emerging Topics in Computational\n  Intelligence", "journal-ref": null, "doi": "10.1109/TETCI.2020.3007905", "report-no": null, "categories": "cs.DC cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing (CC) is a centralized computing paradigm that accumulates\nresources centrally and provides these resources to users through Internet.\nAlthough CC holds a large number of resources, it may not be acceptable by\nreal-time mobile applications, as it is usually far away from users\ngeographically. On the other hand, edge computing (EC), which distributes\nresources to the network edge, enjoys increasing popularity in the applications\nwith low-latency and high-reliability requirements. EC provides resources in a\ndecentralized manner, which can respond to users' requirements faster than the\nnormal CC, but with limited computing capacities. As both CC and EC are\nresource-sensitive, several big issues arise, such as how to conduct job\nscheduling, resource allocation, and task offloading, which significantly\ninfluence the performance of the whole system. To tackle these issues, many\noptimization problems have been formulated. These optimization problems usually\nhave complex properties, such as non-convexity and NP-hardness, which may not\nbe addressed by the traditional convex optimization-based solutions.\nComputational intelligence (CI), consisting of a set of nature-inspired\ncomputational approaches, recently exhibits great potential in addressing these\noptimization problems in CC and EC. This paper provides an overview of research\nproblems in CC and EC and recent progresses in addressing them with the help of\nCI techniques. Informative discussions and future research trends are also\npresented, with the aim of offering insights to the readers and motivating new\nresearch directions.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 09:29:21 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Asim", "Muhammad", ""], ["Wang", "Yong", ""], ["Wang", "Kezhi", ""], ["Huang", "Pei-Qiu", ""]]}, {"id": "2007.14218", "submitter": "Duong Nguyen", "authors": "Duong Nguyen, Sandeep S. Kulkarni", "title": "Technical Report: Benefits of Stabilization versus Rollback in\n  Self-Stabilizing Graph-Based Applications on Eventually Consistent Key-Value\n  Stores", "comments": "arXiv admin note: text overlap with arXiv:1910.08248", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we evaluate and compare the performance of two approaches,\nnamely self-stabilization and rollback, to handling consistency violating\nfaults (\\cvf) that occur when a self-stabilizing distributed graph-based\nprogram is executed on an eventually consistent key-value store. Consistency\nviolating faults are caused by reading wrong values due to weaker level of\nconsistency provided by the key-value store. One way to deal with these faults\nis to utilize rollback whereas another way is to rely on the property of\nself-stabilization that is expected to provide recovery from arbitrary states.\nWe evaluate both these approaches in different case studies --planar graph\ncoloring, arbitrary graph coloring, and maximal matching-- as well as for\ndifferent problem dimensions such as input data characteristics, workload\npartition, and network latency. We also consider the effect of executing\nnon-stabilizing algorithm with rollback with a similar stabilizing algorithm\nthat does not utilize rollback.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 00:40:33 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Nguyen", "Duong", ""], ["Kulkarni", "Sandeep S.", ""]]}, {"id": "2007.14236", "submitter": "Bruno Golosio", "authors": "Bruno Golosio, Gianmarco Tiddia, Chiara De Luca, Elena Pastorelli,\n  Francesco Simula, Pier Stanislao Paolucci", "title": "Fast simulations of highly-connected spiking cortical models using GPUs", "comments": null, "journal-ref": "Front. Comput. Neurosci. 15:627620 2021", "doi": "10.3389/fncom.2021.627620", "report-no": null, "categories": "q-bio.NC cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade there has been a growing interest in the development of\nparallel hardware systems for simulating large-scale networks of spiking\nneurons. Compared to other highly-parallel systems, GPU-accelerated solutions\nhave the advantage of a relatively low cost and a great versatility, thanks\nalso to the possibility of using the CUDA-C/C++ programming languages.\nNeuronGPU is a GPU library for large-scale simulations of spiking neural\nnetwork models, written in the C++ and CUDA-C++ programming languages, based on\na novel spike-delivery algorithm. This library includes simple LIF\n(leaky-integrate-and-fire) neuron models as well as several multisynapse AdEx\n(adaptive-exponential-integrate-and-fire) neuron models with current or\nconductance based synapses, user definable models and different devices. The\nnumerical solution of the differential equations of the dynamics of the AdEx\nmodels is performed through a parallel implementation, written in CUDA-C++, of\nthe fifth-order Runge-Kutta method with adaptive step-size control. In this\nwork we evaluate the performance of this library on the simulation of a\ncortical microcircuit model, based on LIF neurons and current-based synapses,\nand on a balanced network of excitatory and inhibitory neurons, using AdEx\nneurons and conductance-based synapses. On these models, we will show that the\nproposed library achieves state-of-the-art performance in terms of simulation\ntime per second of biological activity. In particular, using a single NVIDIA\nGeForce RTX 2080 Ti GPU board, the full-scale cortical-microcircuit model,\nwhich includes about 77,000 neurons and $3 \\cdot 10^8$ connections, can be\nsimulated at a speed very close to real time, while the simulation time of a\nbalanced network of 1,000,000 AdEx neurons with 1,000 connections per neuron\nwas about 70 s per second of biological activity.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:58:50 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 15:43:02 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 17:13:33 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Golosio", "Bruno", ""], ["Tiddia", "Gianmarco", ""], ["De Luca", "Chiara", ""], ["Pastorelli", "Elena", ""], ["Simula", "Francesco", ""], ["Paolucci", "Pier Stanislao", ""]]}, {"id": "2007.14241", "submitter": "Andrea Borghesi", "authors": "Alessio Netti, Zeynep Kiziltan, Ozalp Babaoglu, Alina Sirbu, Andrea\n  Bartolini, Andrea Borghesi", "title": "A Machine Learning Approach to Online Fault Classification in HPC\n  Systems", "comments": "arXiv admin note: text overlap with arXiv:1807.10056,\n  arXiv:1810.11208", "journal-ref": "Future Generation Computer Systems, Volume 110, September 2020,\n  Pages 1009-1022", "doi": "10.1016/j.future.2019.11.029", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As High-Performance Computing (HPC) systems strive towards the exascale goal,\nfailure rates both at the hardware and software levels will increase\nsignificantly. Thus, detecting and classifying faults in HPC systems as they\noccur and initiating corrective actions before they can transform into failures\nbecomes essential for continued operation. Central to this objective is fault\ninjection, which is the deliberate triggering of faults in a system so as to\nobserve their behavior in a controlled environment. In this paper, we propose a\nfault classification method for HPC systems based on machine learning. The\nnovelty of our approach rests with the fact that it can be operated on streamed\ndata in an online manner, thus opening the possibility to devise and enact\ncontrol actions on the target system in real-time. We introduce a high-level,\neasy-to-use fault injection tool called FINJ, with a focus on the management of\ncomplex experiments. In order to train and evaluate our machine learning\nclassifiers, we inject faults to an in-house experimental HPC system using\nFINJ, and generate a fault dataset which we describe extensively. Both FINJ and\nthe dataset are publicly available to facilitate resiliency research in the HPC\nsystems field. Experimental results demonstrate that our approach allows almost\nperfect classification accuracy to be reached for different fault types with\nlow computational overhead and minimal delay.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 15:36:56 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Netti", "Alessio", ""], ["Kiziltan", "Zeynep", ""], ["Babaoglu", "Ozalp", ""], ["Sirbu", "Alina", ""], ["Bartolini", "Andrea", ""], ["Borghesi", "Andrea", ""]]}, {"id": "2007.14307", "submitter": "Yuval Gil", "authors": "Yuval Emek and Yuval Gil", "title": "Twenty-Two New Approximate Proof Labeling Schemes (Full Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduced by Korman, Kutten, and Peleg (Distributed Computing 2005), a\n\\emph{proof labeling scheme (PLS)} is a system dedicated to verifying that a\ngiven configuration graph satisfies a certain property.\n  It is composed of a centralized \\emph{prover}, whose role is to generate a\nproof for yes-instances in the form of an assignment of labels to the nodes,\nand a distributed \\emph{verifier}, whose role is to verify the validity of the\nproof by local means and accept it if and only if the property is satisfied.\n  To overcome lower bounds on the label size of PLSs for certain graph\nproperties, Censor-Hillel, Paz, and Perry (SIROCCO 2017) introduced the notion\nof an \\emph{approximate proof labeling scheme (APLS)} that allows the verifier\nto accept also some no-instances as long as they are not \"too far\" from\nsatisfying the property.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:20:34 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 06:35:50 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 12:56:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Emek", "Yuval", ""], ["Gil", "Yuval", ""]]}, {"id": "2007.14330", "submitter": "Kostas Kolomvatsos", "authors": "T. Koukaras, K. Kolomvatsos", "title": "An Ensemble Scheme for Proactive Data Allocation in Distributed Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the Internet of Things (IoT) gives the opportunity to numerous\ndevices to interact with their environment, collect and process data. Data are\ntransferred, in an upwards mode, to the Cloud through the Edge Computing (EC)\ninfrastructure. A high number of EC nodes become the hosts of distributed\ndatasets where various processing activities can be realized in close distance\nwith end users. This approach can limit the latency in the provision of\nresponses. In this paper, we focus on a model that proactively decides where\nthe collected data should be stored in order to maximize the accuracy of\ndatasets present at the EC infrastructure. We consider that the accuracy is\ndefined by the solidity of datasets exposed as the statistical resemblance of\ndata. We argue upon the similarity of the incoming data with the available\ndatasets and select the most appropriate of them to store the new information.\nFor alleviating processing nodes from the burden of a continuous, complicated\nstatistical processing, we propose the use of synopses as the subject of the\nsimilarity process. The incoming data are matched against the available\nsynopses based on an ensemble scheme, then, we select the appropriate host to\nstore them and perform the update of the corresponding synopsis. We provide the\ndescription of the problem and the formulation of our solution. Our\nexperimental evaluation targets to reveal the performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:56:44 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Koukaras", "T.", ""], ["Kolomvatsos", "K.", ""]]}, {"id": "2007.14371", "submitter": "Augusto Vega", "authors": "Augusto Vega and Aporva Amarnath and John-David Wellman and Hiwot\n  Kassa and Subhankar Pal and Hubertus Franke and Alper Buyuktosunoglu and\n  Ronald Dreslinski and Pradip Bose", "title": "STOMP: A Tool for Evaluation of Scheduling Policies in Heterogeneous\n  Multi-Processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of heterogeneous chip multiprocessors in recent years has\nreached unprecedented levels. Traditional homogeneous platforms have shown\nfundamental limitations when it comes to enabling high-performance\nyet-ultra-low-power computing, in particular in application domains with\nreal-time execution deadlines or criticality constraints. By combining the\nright set of general purpose cores and hardware accelerators together, along\nwith proper chip interconnects and memory technology, heterogeneous chip\nmultiprocessors have become an effective high-performance and low-power\ncomputing alternative.\n  One of the challenges of heterogeneous architectures relates to efficient\nscheduling of application tasks (processes, threads) across the variety of\noptions in the chip. As a result, it is key to provide tools to enable\nearly-stage prototyping and evaluation of new scheduling policies for\nheterogeneous platforms. In this paper, we present STOMP (Scheduling Techniques\nOptimization in heterogeneous Multi-Processors), a simulator for fast\nimplementation and evaluation of task scheduling policies in\nmulti-core/multi-processor systems with a convenient interface for \"plugging\"\nin new scheduling policies in a simple manner. Thorough validation of STOMP\nexhibits small relative errors when compared against closed-formed equivalent\nmodels during steady-state analysis.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:29:38 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Vega", "Augusto", ""], ["Amarnath", "Aporva", ""], ["Wellman", "John-David", ""], ["Kassa", "Hiwot", ""], ["Pal", "Subhankar", ""], ["Franke", "Hubertus", ""], ["Buyuktosunoglu", "Alper", ""], ["Dreslinski", "Ronald", ""], ["Bose", "Pradip", ""]]}, {"id": "2007.14374", "submitter": "Wentai Wu", "authors": "Wentai Wu, Ligang He, Weiwei Lin, Rui Mao", "title": "Accelerating Federated Learning over Reliability-Agnostic Clients in\n  Mobile Edge Computing Systems", "comments": "14 pages, 7 figures, with Appendix", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems. Vol.32,\n  no.7, pp.1539-1551 (2020)", "doi": "10.1109/TPDS.2020.3040867", "report-no": null, "categories": "cs.DC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mobile Edge Computing (MEC), which incorporates the Cloud, edge nodes and end\ndevices, has shown great potential in bringing data processing closer to the\ndata sources. Meanwhile, Federated learning (FL) has emerged as a promising\nprivacy-preserving approach to facilitating AI applications. However, it\nremains a big challenge to optimize the efficiency and effectiveness of FL when\nit is integrated with the MEC architecture. Moreover, the unreliable nature\n(e.g., stragglers and intermittent drop-out) of end devices significantly slows\ndown the FL process and affects the global model's quality Xin such\ncircumstances. In this paper, a multi-layer federated learning protocol called\nHybridFL is designed for the MEC architecture. HybridFL adopts two levels (the\nedge level and the cloud level) of model aggregation enacting different\naggregation strategies. Moreover, in order to mitigate stragglers and end\ndevice drop-out, we introduce regional slack factors into the stage of client\nselection performed at the edge nodes using a probabilistic approach without\nidentifying or probing the state of end devices (whose reliability is\nagnostic). We demonstrate the effectiveness of our method in modulating the\nproportion of clients selected and present the convergence analysis for our\nprotocol. We have conducted extensive experiments with machine learning tasks\nin different scales of MEC system. The results show that HybridFL improves the\nFL training process significantly in terms of shortening the federated round\nlength, speeding up the global model's convergence (by up to 12X) and reducing\nend device energy consumption (by up to 58%).\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:35:39 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 09:05:07 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 10:01:09 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Wentai", ""], ["He", "Ligang", ""], ["Lin", "Weiwei", ""], ["Mao", "Rui", ""]]}, {"id": "2007.14644", "submitter": "Gabriele D'Angelo", "authors": "Luca Serena, Stefano Ferretti, Gabriele D'Angelo", "title": "DiLeNA: Distributed Ledger Network Analyzer", "comments": "Proceeding of the 3rd Workshop on Cryptocurrencies and Blockchains\n  for Distributed Systems (CryBlock 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the Distributed Ledger Network Analyzer (DiLeNA), a new\nsoftware tool for the analysis of the transactions network recorded in\nDistributed Ledger Technologies (DLTs). The set of transactions in a DLT forms\na complex network. Studying its characteristics and peculiarities is of\nparamount importance, in order to understand how users interact in the\ndistributed ledger system. The tool design and implementation is introduced and\nsome results are provided. In particular, the Bitcoin and Ethereum blockchains,\ni.e. the most famous and used DLTs at the time of writing, have been analyzed\nand compared.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 07:41:26 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Serena", "Luca", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2007.14681", "submitter": "Isabella Ziccardi", "authors": "Luca Becchetti, Andrea Clementi, Francesco Pasquale, Luca Trevisan,\n  Isabella Ziccardi", "title": "Expansion and Flooding in Dynamic Random Networks with Node Churn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study expansion and information diffusion in dynamic networks, that is in\nnetworks in which nodes and edges are continuously created and destroyed. We\nconsider information diffusion by {\\em flooding}, the process by which, once a\nnode is informed, it broadcasts its information to all its neighbors.\n  We study models in which the network is {\\em sparse}, meaning that it has\n$\\mathcal{O}(n)$ edges, where $n$ is the number of nodes, and in which edges\nare created randomly, rather than according to a carefully designed distributed\nalgorithm. In our models, when a node is \"born\", it connects to\n$d=\\mathcal{O}(1)$ random other nodes. An edge remains alive as long as both\nits endpoints do.\n  If no further edge creation takes place, we show that, although the network\nwill have $\\Omega_d(n)$ isolated nodes, it is possible, with large constant\nprobability, to inform a $1-exp(-\\Omega(d))$ fraction of nodes in\n$\\mathcal{O}(\\log n)$ time. Furthermore, the graph exhibits, at any given time,\na \"large-set expansion\" property.\n  We also consider models with {\\em edge regeneration}, in which if an edge\n$(v,w)$ chosen by $v$ at birth goes down because of the death of $w$, the edge\nis replaced by a fresh random edge $(v,z)$. In models with edge regeneration,\nwe prove that the network is, with high probability, a vertex expander at any\ngiven time, and flooding takes $\\mathcal{O}(\\log n)$ time.\n  The above results hold both for a simple but artificial streaming model of\nnode churn, in which at each time step one node is born and the oldest node\ndies, and in a more realistic continuous-time model in which the time between\nbirths is Poisson and the lifetime of each node follows an exponential\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:55:28 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Becchetti", "Luca", ""], ["Clementi", "Andrea", ""], ["Pasquale", "Francesco", ""], ["Trevisan", "Luca", ""], ["Ziccardi", "Isabella", ""]]}, {"id": "2007.14898", "submitter": "Yi-Jun Chang", "authors": "Yi-Jun Chang, Thatchaphol Saranurak", "title": "Deterministic Distributed Expander Decomposition and Routing with\n  Applications in Distributed Derandomization", "comments": "To appear in FOCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a recent exciting line of work in distributed graph algorithms in\nthe $\\mathsf{CONGEST}$ model that exploit expanders. All these algorithms so\nfar are based on two tools: expander decomposition and expander routing. An\n$(\\epsilon,\\phi)$-expander decomposition removes $\\epsilon$-fraction of the\nedges so that the remaining connected components have conductance at least\n$\\phi$, i.e., they are $\\phi$-expanders, and expander routing allows each\nvertex $v$ in a $\\phi$-expander to very quickly exchange $\\text{deg}(v)$\nmessages with any other vertices, not just its local neighbors.\n  In this paper, we give the first efficient deterministic distributed\nalgorithms for both tools. We show that an $(\\epsilon,\\phi)$-expander\ndecomposition can be deterministically computed in $\\text{poly}(\\epsilon^{-1})\nn^{o(1)}$ rounds for $\\phi = \\text{poly}(\\epsilon) n^{-o(1)}$, and that\nexpander routing can be performed deterministically in\n$\\text{poly}(\\phi^{-1})n^{o(1)}$ rounds. Both results match previous bounds of\nrandomized algorithms by [Chang and Saranurak, PODC 2019] and [Ghaffari, Kuhn,\nand Su, PODC 2017] up to subpolynomial factors.\n  Consequently, we derandomize existing distributed algorithms that exploit\nexpanders. We show that a minimum spanning tree on $n^{o(1)}$-expanders can be\nconstructed deterministically in $n^{o(1)}$ rounds, and triangle detection and\nenumeration on general graphs can be solved deterministically in $O(n^{0.58})$\nand $n^{2/3 + o(1)}$ rounds, respectively.\n  We also give the first polylogarithmic-round randomized algorithm for\nconstructing an $(\\epsilon,\\phi)$-expander decomposition in\n$\\text{poly}(\\epsilon^{-1}, \\log n)$ rounds for $\\phi = 1 /\n\\text{poly}(\\epsilon^{-1}, \\log n)$. The previous algorithm by [Chang and\nSaranurak, PODC 2019] needs $n^{\\Omega(1)}$ rounds for any $\\phi\\ge\n1/\\text{poly}\\log n$.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:15:06 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chang", "Yi-Jun", ""], ["Saranurak", "Thatchaphol", ""]]}, {"id": "2007.14990", "submitter": "Saptaparni Kumar", "authors": "Yingjian Wu, Haochen Pan, Saptaparni Kumar, Lewis Tseng", "title": "Reliable Broadcast in Practical Networks: Algorithm and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable broadcast is an important primitive to ensure that a source node can\nreliably disseminate a message to all the non-faulty nodes in an asynchronous\nand failure-prone networked system. Byzantine Reliable Broadcast protocols were\nfirst proposed by Bracha in 1987, and have been widely used in fault-tolerant\nsystems and protocols. Several recent protocols have improved the round and bit\ncomplexity of these algorithms. Motivated by the constraints in practical\nnetworks, we revisit the problem. In particular, we use cryptographic hash\nfunctions and erasure coding to reduce communication and computation complexity\nand simplify the protocol design. We also identify the fundamental trade-offs\nof Byzantine Reliable Broadcast protocols with respect to resilience (number of\nnodes), local computation, round complexity, and bit complexity. Finally, we\nalso design and implement a general testing framework for similar communication\nprotocols. We evaluate our protocols using our framework. The results\ndemonstrate that our protocols have superior performance in practical networks.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:59:29 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wu", "Yingjian", ""], ["Pan", "Haochen", ""], ["Kumar", "Saptaparni", ""], ["Tseng", "Lewis", ""]]}, {"id": "2007.15152", "submitter": "Vanderson Martins do Rosario", "authors": "Ot\\'avio O. Napoli, Vanderson Martins do Rosario, Jo\\~ao Paulo\n  Navarro, Pedro M\\'ario Cruz e Silva, Edson Borin", "title": "Accelerating Multi-attribute Unsupervised Seismic Facies Analysis With\n  RAPIDS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of seismic facies is done by clustering seismic data samples\nbased on their attributes. Year after year, 3D datasets used by exploration\ngeophysics increase in size, complexity, and number of attributes, requiring a\ncontinuous rise in the classification performance. In this work, we explore the\nuse of Graphics Processing Units (GPUs) to perform the classification of\nseismic surveys using the well-established Machine Learning (ML) method\nk-means. We show that the high-performance distributed implementation of the\nk-means algorithm available at the RAPIDS library can be used to classify\nfacies in large seismic datasets much faster than a classical parallel CPU\nimplementation (up to 258-fold faster in NVIDIA V100 GPUs), especially for\nlarge seismic blocks. We tested the algorithm with different real seismic\nvolumes, including Netherlands, Parihaka, and Kahu (from 12GB to 66GB).\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 23:41:33 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 01:32:28 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Napoli", "Ot\u00e1vio O.", ""], ["Rosario", "Vanderson Martins do", ""], ["Navarro", "Jo\u00e3o Paulo", ""], ["Silva", "Pedro M\u00e1rio Cruz e", ""], ["Borin", "Edson", ""]]}, {"id": "2007.15251", "submitter": "Yannic Maus", "authors": "Yannic Maus and Tigran Tonoyan", "title": "Local Conflict Coloring Revisited: Linial for Lists", "comments": "to appear at DISC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linial's famous color reduction algorithm reduces a given $m$-coloring of a\ngraph with maximum degree $\\Delta$ to a $O(\\Delta^2\\log m)$-coloring, in a\nsingle round in the LOCAL model. We show a similar result when nodes are\nrestricted to choose their color from a list of allowed colors: given an\n$m$-coloring in a directed graph of maximum outdegree $\\beta$, if every node\nhas a list of size $\\Omega(\\beta^2 (\\log \\beta+\\log\\log m + \\log \\log\n|\\mathcal{C}|))$ from a color space $\\mathcal{C}$ then they can select a color\nin two rounds in the LOCAL model. Moreover, the communication of a node\nessentially consists of sending its list to the neighbors. This is obtained as\npart of a framework that also contains Linial's color reduction (with an\nalternative proof) as a special case. Our result also leads to a defective list\ncoloring algorithm. As a corollary, we improve the state-of-the-art truly local\n$(deg+1)$-list coloring algorithm from Barenboim et al. [PODC'18] by slightly\nreducing the runtime to $O(\\sqrt{\\Delta\\log\\Delta})+\\log^* n$ and significantly\nreducing the message size (from huge to roughly $\\Delta$). Our techniques are\ninspired by the local conflict coloring framework of Fraigniaud et al.\n[FOCS'16].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:18:23 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Maus", "Yannic", ""], ["Tonoyan", "Tigran", ""]]}, {"id": "2007.15260", "submitter": "Gabriele D'Angelo", "authors": "Luca Serena, Gabriele D'Angelo, Stefano Ferretti", "title": "Implications of Dissemination Strategies on the Security of Distributed\n  Ledgers", "comments": "Proceedings of the 3rd Workshop on Cryptocurrencies and Blockchains\n  for Distributed Systems (CryBlock 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a simulation study on security attacks over Distributed\nLedger Technologies (DLTs). We specifically focus on attacks at the underlying\npeer-to-peer layer of these systems, that is in charge of disseminating\nmessages containing data and transaction to be spread among all participants.\nIn particular, we consider the Sybil attack, according to which a malicious\nnode creates many Sybils that drop messages coming from a specific attacked\nnode, or even all messages from honest nodes. Our study shows that the\nselection of the specific dissemination protocol, as well as the amount of\nconnections each peer has, have an influence on the resistance to this attack.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 06:52:04 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Serena", "Luca", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""]]}, {"id": "2007.15306", "submitter": "Emilio Cruciani", "authors": "Emilio Cruciani, Hlafo Alfie Mimun, Matteo Quattropani, Sara Rizzo", "title": "Phase Transition of the k-Majority Dynamics in Biased Communication\n  Models", "comments": "Preliminary versions published in DISC 2020 (Brief Announcement) and\n  ICDCN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a graph where each of the $n$ nodes is either in state $\\mathcal{R}$\nor $\\mathcal{B}$. Herein, we analyze the synchronous $k$-Majority dynamics,\nwhere in each discrete-time round nodes simultaneously sample $k$ neighbors\nuniformly at random with replacement and adopt the majority state among the\nnodes in the sample (breaking ties uniformly at random).\n  Differently from previous work, we study the robustness of the $k$-Majority\nin maintaining a majority, that we consider $\\mathcal{R}$ w.l.o.g., when the\ndynamics is subject to two forms of adversarial noise, or bias, toward state\n$\\mathcal{B}$. We consider an external agent that wants to subvert the initial\nmajority and, in each round, either tries to alter the communication between\neach pair of nodes transmitting state $\\mathcal{B}$ (first form of bias), or\ntries to corrupt each node directly making it update to $\\mathcal{B}$ (second\nform of bias), with a probability of success $p$.\n  Our results show a phase transition in both forms of bias and on the same\ncritical value. By considering initial configurations in which each node has\nprobability $q \\in (\\frac{1}{2},1]$ of being in state $\\mathcal{R}$, we prove\nthat for every $k\\geq3$ there exists a critical value $p_{k,q}^*$ such that,\nwith high probability: if $p>p_{k,q}^*$, the external agent is able to subvert\nthe initial majority within a constant number of rounds; if $p<p_{k,q}^*$, the\nexternal agent needs at least a superpolynomial number of rounds to subvert the\ninitial majority.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 08:30:42 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:25:14 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cruciani", "Emilio", ""], ["Mimun", "Hlafo Alfie", ""], ["Quattropani", "Matteo", ""], ["Rizzo", "Sara", ""]]}, {"id": "2007.15338", "submitter": "Andrey Chupakhin", "authors": "A. Chupakhin, A. Kolosov, R. Smeliansky, V. Antonenko, G. Ishelev", "title": "New approach to MPI program execution time prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of MPI programs execution time prediction on a certain set of\ncomputer installations is considered. This problem emerges with orchestration\nand provisioning a virtual infrastructure in a cloud computing environment over\na heterogeneous network of computer installations: supercomputers or clusters\nof servers (e.g. mini data centers). One of the key criteria for the\neffectiveness of the cloud computing environment is the time staying by the\nprogram inside the environment. This time consists of the waiting time in the\nqueue and the execution time on the selected physical computer installation, to\nwhich the computational resource of the virtual infrastructure is dynamically\nmapped. One of the components of this problem is the estimation of the MPI\nprograms execution time on a certain set of computer installations. This is\nnecessary to determine a proper choice of order and place for program\nexecution. The article proposes two new approaches to the program execution\ntime prediction problem. The first one is based on computer installations\ngrouping based on the Pearson correlation coefficient. The second one is based\non vector representations of computer installations and MPI programs, so-called\nembeddings. The embedding technique is actively used in recommendation systems,\nsuch as for goods (Amazon), for articles (Arxiv.org), for videos (YouTube,\nNetflix). The article shows how the embeddings technique helps to predict the\nexecution time of a MPI program on a certain set of computer installations.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 09:35:08 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chupakhin", "A.", ""], ["Kolosov", "A.", ""], ["Smeliansky", "R.", ""], ["Antonenko", "V.", ""], ["Ishelev", "G.", ""]]}, {"id": "2007.15826", "submitter": "Yaser Mansouri", "authors": "Yaser Mansouri and M. Ali Babar", "title": "The Impact of Distance on Performance and Scalability of Distributed\n  Database Systems in Hybrid Clouds", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing need for managing big data has led the emergence of advanced\ndatabase management systems. There has been increased efforts aimed at\nevaluating the performance and scalability of NoSQL and Relational databases\nhosted by either private or public cloud datacenters. However, there has been\nlittle work on evaluating the performance and scalability of these databases in\nhybrid clouds, where the distance between private and public cloud datacenters\ncan be one of the key factors that can affect their performance. Hence, in this\npaper, we present a detailed evaluation of throughput, scalability, and VMs\nsize vs. VMs number for six modern databases in a hybrid cloud, consisting of a\nprivate cloud in Adelaide and Azure based datacenter in Sydney, Mumbai, and\nVirginia regions. Based on results, as the distance between private and public\nclouds increases, the throughput performance of most databases reduces. Second,\nMongoDB obtains the best throughput performance, followed by MySQL C luster,\nwhilst Cassandra exposes the most fluctuation in through performance. Third,\nvertical scalability improves the throughput of databases more than the\nhorizontal scalability. Forth, exploiting bigger VMs rather than more VMs with\nless cores can increase throughput performance for Cassandra, Riak, and Redis.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:36:58 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Mansouri", "Yaser", ""], ["Babar", "M. Ali", ""]]}, {"id": "2007.15859", "submitter": "Yongbin Gu", "authors": "Pengcheng Li, Yongbin Gu", "title": "Learning Forward Reuse Distance", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caching techniques are widely used in the era of cloud computing from\napplications, such as Web caches to infrastructures, Memcached and memory\ncaches in computer architectures. Prediction of cached data can greatly help\nimprove cache management and performance. The recent advancement of deep\nlearning techniques enables the design of novel intelligent cache replacement\npolicies. In this work, we propose a learning-aided approach to predict future\ndata accesses. We find that a powerful LSTM-based recurrent neural network\nmodel can provide high prediction accuracy based on only a cache trace as\ninput. The high accuracy results from a carefully crafted locality-driven\nfeature design. Inspired by the high prediction accuracy, we propose a pseudo\nOPT policy and evaluate it upon 13 real-world storage workloads from Microsoft\nResearch. Results demonstrate that the new cache policy improves state-of-art\npractical policies by up to 19.2% and incurs only 2.3% higher miss ratio than\nOPT on average.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 05:57:50 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Li", "Pengcheng", ""], ["Gu", "Yongbin", ""]]}, {"id": "2007.16135", "submitter": "Garrett Wright", "authors": "Garrett Wright", "title": "Improved Time Warp Edit Distance -- A Parallel Dynamic Program in Linear\n  Memory", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC cs.LG cs.MS math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edit Distance is a classic family of dynamic programming problems, among\nwhich Time Warp Edit Distance refines the problem with the notion of a metric\nand temporal elasticity. A novel Improved Time Warp Edit Distance algorithm\nthat is both massively parallelizable and requiring only linear storage is\npresented. This method uses the procession of a three diagonal band to cover\nthe original dynamic program space. Every element of the diagonal update can be\ncomputed in parallel. The core method is a feature of the TWED Longest Common\nSubsequence data dependence and is applicable to dynamic programs that share\nsimilar band subproblem structure. The algorithm has been implemented as a CUDA\nC library with Python bindings. Speedups for challenging problems are\nphenomenal.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 15:31:05 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Wright", "Garrett", ""]]}]