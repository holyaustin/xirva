[{"id": "1603.00131", "submitter": "Kamala Sundararaman", "authors": "B. Kamala, B. Priya, J. M. Nandhini", "title": "Platform Autonomous Custom Scalable Service using Service Oriented Cloud\n  Computing Architecture", "comments": "IJERA", "journal-ref": "International Journal of Engineering Research and Applications\n  (IJERA) ISSN: 2248-9622,Vol. 2, Issue 2,Mar-Apr 2012, pp.1467-1471", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global economic recession and the shrinking budget of IT projects have\nled to the need of development of integrated information systems at a lower\ncost. Today, the emerging phenomenon of cloud computing aims at transforming\nthe traditional way of computing by providing both software applications and\nhardware resources as a service. With the rapid evolution of Information\nCommunication Technology (ICT) governments, organizations and businesses are\nlooking for solutions to improve their services and integrate their IT\ninfrastructures. In recent years advanced technologies such as SOA and Cloud\ncomputing have been evolved to address integration problems. The Clouds\nenormous capacity with comparable low cost makes it an ideal platform for SOA\ndeployment. This paper deals with the combined approach of Cloud and Service\nOriented Architecture along with a Case Study and a review.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 04:10:24 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Kamala", "B.", ""], ["Priya", "B.", ""], ["Nandhini", "J. M.", ""]]}, {"id": "1603.00149", "submitter": "Sleiman Mhanna Mr.", "authors": "Sleiman Mhanna, Archie Chapman, and Gregor Verbic", "title": "A Fast Distributed Algorithm for Large-Scale Demand Response Aggregation", "comments": "Accepted in IEEE Transactions on Smart Grid", "journal-ref": null, "doi": "10.1109/TSG.2016.2536740", "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge to implementing residential demand response is that of\naligning the objectives of many households, each of which aims to minimize its\npayments and maximize its comfort level, while balancing this with the\nobjectives of an aggregator that aims to minimize the cost of electricity\npurchased in a pooled wholesale market. This paper presents a fast distributed\nalgorithm for aggregating a large number of households with a mixture of\ndiscrete and continuous energy levels. A distinctive feature of the method in\nthis paper is that the nonconvex DR problem is decomposed in terms of\nhouseholds as opposed to devices, which allows incorporating more intricate\ncouplings between energy storage devices, appliances and distributed energy\nresources. The proposed method is a fast distributed algorithm applied to the\ndouble smoothed dual function of the adopted DR model. The method is tested on\nsystems with up to 2560 households, each with 10 devices on average. The\nproposed algorithm is designed to terminate in 60 iterations irrespective of\nsystem size, which can be ideal for an on-line version of this problem.\nMoreover, numerical results show that with minimal parameter tuning, the\nalgorithm exhibits a very similar convergence behavior throughout the studied\nsystems and converges to near-optimal solutions, which corroborates its\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 05:38:20 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Mhanna", "Sleiman", ""], ["Chapman", "Archie", ""], ["Verbic", "Gregor", ""]]}, {"id": "1603.00307", "submitter": "Christopher M. Poskitt", "authors": "Claudio Corrodi, Alexander Heu{\\ss}ner, Christopher M. Poskitt", "title": "A Graph-Based Semantics Workbench for Concurrent Asynchronous Programs", "comments": "Accepted for publication in the proceedings of FASE 2016 (to appear)", "journal-ref": "Proc. International Conference on Fundamental Approaches to\n  Software Engineering (FASE 2016), volume 9633 of LNCS, pages 31-48. Springer,\n  2016", "doi": "10.1007/978-3-662-49665-7_3", "report-no": null, "categories": "cs.SE cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of novel programming languages and libraries have been proposed that\noffer simpler-to-use models of concurrency than threads. It is challenging,\nhowever, to devise execution models that successfully realise their\nabstractions without forfeiting performance or introducing unintended\nbehaviours. This is exemplified by SCOOP---a concurrent object-oriented\nmessage-passing language---which has seen multiple semantics proposed and\nimplemented over its evolution. We propose a \"semantics workbench\" with fully\nand semi-automatic tools for SCOOP, that can be used to analyse and compare\nprograms with respect to different execution models. We demonstrate its use in\nchecking the consistency of semantics by applying it to a set of representative\nprograms, and highlighting a deadlock-related discrepancy between the principal\nexecution models of the language. Our workbench is based on a modular and\nparameterisable graph transformation semantics implemented in the GROOVE tool.\nWe discuss how graph transformations are leveraged to atomically model\nintricate language abstractions, and how the visual yet algebraic nature of the\nmodel can be used to ascertain soundness.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 15:10:21 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Corrodi", "Claudio", ""], ["Heu\u00dfner", "Alexander", ""], ["Poskitt", "Christopher M.", ""]]}, {"id": "1603.00747", "submitter": "Donghyuk Lee", "authors": "Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk\n  Lee, Chris Wilkerson, Konrad Lai, Onur Mutlu", "title": "RowHammer: Reliability Analysis and Security Implications", "comments": "This is the summary of the paper titled \"Flipping Bits in Memory\n  Without Accessing Them: An Experimental Study of DRAM Disturbance Errors\"\n  which appeared in ISCA in June 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As process technology scales down to smaller dimensions, DRAM chips become\nmore vulnerable to disturbance, a phenomenon in which different DRAM cells\ninterfere with each other's operation. For the first time in academic\nliterature, our ISCA paper exposes the existence of disturbance errors in\ncommodity DRAM chips that are sold and used today. We show that repeatedly\nreading from the same address could corrupt data in nearby addresses. More\nspecifically: When a DRAM row is opened (i.e., activated) and closed (i.e.,\nprecharged) repeatedly (i.e., hammered), it can induce disturbance errors in\nadjacent DRAM rows. This failure mode is popularly called RowHammer. We tested\n129 DRAM modules manufactured within the past six years (2008-2014) and found\n110 of them to exhibit RowHammer disturbance errors, the earliest of which\ndates back to 2010. In particular, all modules from the past two years\n(2012-2013) were vulnerable, which implies that the errors are a recent\nphenomenon affecting more advanced generations of process technology.\nImportantly, disturbance errors pose an easily-exploitable security threat\nsince they are a breach of memory protection, wherein accesses to one page\n(mapped to one row) modifies the data stored in another page (mapped to an\nadjacent row).\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 17:19:04 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kim", "Yoongu", ""], ["Daly", "Ross", ""], ["Kim", "Jeremie", ""], ["Fallin", "Chris", ""], ["Lee", "Ji Hye", ""], ["Lee", "Donghyuk", ""], ["Wilkerson", "Chris", ""], ["Lai", "Konrad", ""], ["Mutlu", "Onur", ""]]}, {"id": "1603.01112", "submitter": "Reem Elkhouly", "authors": "Reem Elkhouly, Keiji Kimura, Ahmed El-Mahdy", "title": "If-Conversion Optimization using Neuro Evolution of Augmenting\n  Topologies", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 6 pages, LaTeX, 2 PDF figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control-flow dependence is an intrinsic limiting factor for pro- gram\nacceleration. With the availability of instruction-level par- allel\narchitectures, if-conversion optimization has, therefore, be- come pivotal for\nextracting parallelism from serial programs. While many if-conversion\noptimization heuristics have been proposed in the literature, most of them\nconsider rigid criteria regardless of the underlying hardware and input\nprograms. In this paper, we propose a novel if-conversion scheme that preforms\nan efficient if-conversion transformation using a machine learning technique\n(NEAT). This method enables if-conversion customization overall branches within\na program unlike the literature that considered in- dividual branches. Our\ntechnique also provides flexibility required when compiling for heterogeneous\nsystems. The efficacy of our approach is shown by experiments and reported\nresults which il- lustrate that the programs can be accelerated on the same\narchi- tecture and without modifying the original code. Our technique applies\nfor general purpose programming languages (e.g. C/C++) and is transparent for\nthe programmer. We implemented our tech- nique in LLVM 3.6.1 compilation\ninfrastructure and experimented on the kernels of SPEC-CPU2006 v1.1 benchmarks\nsuite running on a multicore system of Intel(R) Xeon(R) 3.50GHz processors. Our\nfindings show a performance gain up to 8.6% over the stan- dard optimized code\n(LLVM -O2 with if-conversion included), in- dicating the need for If-conversion\ncompilation optimization that can adapt to the unique characteristics of every\nindividual branch.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 14:41:12 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Elkhouly", "Reem", ""], ["Kimura", "Keiji", ""], ["El-Mahdy", "Ahmed", ""]]}, {"id": "1603.01352", "submitter": "Hao Wu", "authors": "Hao Wu, Fangfei Liu, Ruby B. Lee", "title": "Cloud Server Benchmarks for Performance Evaluation of New Hardware\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adding new hardware features to a cloud computing server requires testing\nboth the functionalities and the performance of the new hardware mechanisms.\nHowever, commonly used cloud computing server workloads are not\nwell-represented by the SPEC integer and floating-point benchmark and Parsec\nsuites typically used by the computer architecture community. Existing cloud\nbenchmark suites for scale-out or scale-up computing are not representative of\nthe most common cloud usage, and are very difficult to run on a cycle-accurate\nsimulator that can accurately model new hardware, like gem5. In this paper, we\npresent PALMScloud, a suite of cloud computing benchmarks for performance\nevaluation of cloud servers, that is ready to run on the gem5 cycle-accurate\nsimulator. We demonstrate how our cloud computing benchmarks are used in\nevaluating the cache performance of a new secure cache called Newcache as a\ncase study. We hope that these cloud benchmarks, ready to run on a dual-machine\ngem5 simulator or on real machines, can be useful to other researchers\ninterested in improving hardware micro-architecture and cloud server\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 05:49:12 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Wu", "Hao", ""], ["Liu", "Fangfei", ""], ["Lee", "Ruby B.", ""]]}, {"id": "1603.01384", "submitter": "Srivatsan Ravi Mr", "authors": "Vincent Gramoli, Petr Kuznetsov, Srivatsan Ravi", "title": "In the Search of Optimal Concurrency", "comments": "Extended version of results in arXiv:1203.4751", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing a concurrent data structure typically begins with defining its\nsequential specification. However, when used \\emph{as is}, a nontrivial\nsequential data structure, such as a linked list, a search tree, or a hash\ntable, may expose incorrect behavior: lost updates, inconsistent responses,\netc. To ensure correctness, portions of the sequential code operating on the\nshared data must be \"protected\" from data races using synchronization\nprimitives and, thus, certain schedules of the steps of concurrent operations\nmust be rejected. But can we ensure that we do not \"overuse\" synchronization,\ni.e., that we reject a concurrent schedule only if it violates correctness?\n  In this paper, we treat this question formally by introducing the notion of a\n\\emph{concurrency-optimal} implementation. A program's concurrency is defined\nhere as its ability to accept concurrent schedules, i.e., interleavings of\nsteps of its sequential implementation. An implementation is\nconcurrency-optimal if it accepts all interleavings that do not violate the\nprogram's correctness. We explore the concurrency properties of \\emph{search}\ndata structures which can be represented in the form of directed acyclic graphs\nexporting insert, delete and search operations. We prove, for the first time,\nthat \\emph{pessimistic} e.g., based on conservative locking) and\n\\emph{optimistic serializable} e.g., based on serializable transactional\nmemory) implementations of search data-structures are incomparable in terms of\nconcurrency. Specifically, there exist simple interleavings of sequential code\nthat cannot be accepted by \\emph{any} pessimistic (and \\emph{resp.},\nserializable optimistic) implementation, but accepted by a serializable\noptimistic one (and \\emph{resp.}, pessimistic). Thus, neither of these two\nimplementation classes is concurrency-optimal.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 09:06:31 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Gramoli", "Vincent", ""], ["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1603.01407", "submitter": "Karim Djemame", "authors": "Karim Djemame and Django Armstrong and Richard Kavanagh and\n  Jean-Christophe Deprez and Ana Juan Ferrer and David Garcia Perez and Rosa\n  Badia and Raul Sirvent and Jorge Ejarque and Yiannis Georgiou", "title": "TANGO: Transparent heterogeneous hardware Architecture deployment for\n  eNergy Gain in Operation", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 7 pages, LaTeX, 3 PNG figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with the issue of how software systems actually use\nHeterogeneous Parallel Architectures (HPAs), with the goal of optimizing power\nconsumption on these resources. It argues the need for novel methods and tools\nto support software developers aiming to optimise power consumption resulting\nfrom designing, developing, deploying and running software on HPAs, while\nmaintaining other quality aspects of software to adequate and agreed levels. To\ndo so, a reference architecture to support energy efficiency at application\nconstruction, deployment, and operation is discussed, as well as its\nimplementation and evaluation plans.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:13:56 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Djemame", "Karim", ""], ["Armstrong", "Django", ""], ["Kavanagh", "Richard", ""], ["Deprez", "Jean-Christophe", ""], ["Ferrer", "Ana Juan", ""], ["Perez", "David Garcia", ""], ["Badia", "Rosa", ""], ["Sirvent", "Raul", ""], ["Ejarque", "Jorge", ""], ["Georgiou", "Yiannis", ""]]}, {"id": "1603.01412", "submitter": "Brijesh Dongol", "authors": "Brijesh Dongol and Lindsay Groves", "title": "Contextual trace refinement for concurrent objects: Safety and progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correctness of concurrent objects is defined in terms of safety properties\nsuch as linearizability, sequential consistency, and quiescent consistency, and\nprogress properties such as wait-, lock-, and obstruction-freedom. These\nproperties, however, only refer to the behaviours of the object in isolation,\nwhich does not tell us what guarantees these correctness conditions on\nconcurrent objects provide to their client programs. This paper investigates\nthe links between safety and progress properties of concurrent objects and a\nform of trace refinement for client programs, called contextual trace\nrefinement. In particular, we show that linearizability together with a minimal\nnotion of progress are sufficient properties of concurrent objects to ensure\ncontextual trace refinement, but sequential consistency and quiescent\nconsistency are both too weak. Our reasoning is carried out in the action\nsystems framework with procedure calls, which we extend to cope with non-atomic\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 10:20:23 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Dongol", "Brijesh", ""], ["Groves", "Lindsay", ""]]}, {"id": "1603.01486", "submitter": "David Harris", "authors": "David G. Harris, Johannes Schneider, Hsin-Hao Su", "title": "Distributed $(\\Delta+1)$-Coloring in Sublogarithmic Rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new randomized distributed algorithm for $(\\Delta+1)$-coloring in\nthe LOCAL model, running in $O(\\sqrt{\\log \\Delta})+ 2^{O(\\sqrt{\\log \\log n})}$\nrounds in a graph of maximum degree~$\\Delta$. This implies that the\n$(\\Delta+1)$-coloring problem is easier than the maximal independent set\nproblem and the maximal matching problem, due to their lower bounds of $\\Omega\n\\left( \\min \\left( \\sqrt{\\frac{\\log n}{\\log \\log n}}, \\frac{\\log \\Delta}{\\log\n\\log \\Delta} \\right) \\right)$ by Kuhn, Moscibroda, and Wattenhofer [PODC'04].\nOur algorithm also extends to list-coloring where the palette of each node\ncontains $\\Delta+1$ colors. We extend the set of distributed symmetry-breaking\ntechniques by performing a decomposition of graphs into dense and sparse parts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 14:58:44 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 12:36:06 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 14:26:31 GMT"}, {"version": "v4", "created": "Wed, 17 Jan 2018 21:33:13 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Harris", "David G.", ""], ["Schneider", "Johannes", ""], ["Su", "Hsin-Hao", ""]]}, {"id": "1603.01529", "submitter": "Paulo S\\'ergio Almeida", "authors": "Paulo S\\'ergio Almeida, Ali Shoker, Carlos Baquero", "title": "Delta State Replicated Data Types", "comments": "arXiv admin note: substantial text overlap with arXiv:1410.2803", "journal-ref": "Journal of Parallel and Distributed Computing, Volume 111, January\n  2018, Pages 162-173", "doi": "10.1016/j.jpdc.2017.08.003", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CRDTs are distributed data types that make eventual consistency of a\ndistributed object possible and non ad-hoc. Specifically, state-based CRDTs\nensure convergence through disseminating the entire state, that may be large,\nand merging it to other replicas; whereas operation-based CRDTs disseminate\noperations (i.e., small states) assuming an exactly-once reliable dissemination\nlayer. We introduce Delta State Conflict-Free Replicated Data Types\n($\\delta$-CRDTs) that can achieve the best of both worlds: small messages with\nan incremental nature, as in operation-based CRDTs, disseminated over\nunreliable communication channels, as in traditional state-based CRDTs. This is\nachieved by defining delta mutators to return a delta-state, typically with a\nmuch smaller size than the full state, that to be joined with both local and\nremote states. We introduce the $\\delta$-CRDT framework, and we explain it\nthrough establishing a correspondence to current state-based CRDTs. In\naddition, we present an anti-entropy algorithm for eventual convergence, and\nanother one that ensures causal consistency. Finally, we introduce several\n$\\delta$-CRDT specifications of both well-known replicated datatypes and novel\ndatatypes, including a generic map composition.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:38:40 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Almeida", "Paulo S\u00e9rgio", ""], ["Shoker", "Ali", ""], ["Baquero", "Carlos", ""]]}, {"id": "1603.01536", "submitter": "Kamran Idrees", "authors": "Kamran Idrees, Tobias Fuchs, Colin W. Glass", "title": "Effective use of the PGAS Paradigm: Driving Transformations and\n  Self-Adaptive Behavior in DASH-Applications", "comments": "10 pages, 8 figures, Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) Workshop, held in conjunction with the\n  International Symposium on Code Generation and Optimization (CGO) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DASH is a library of distributed data structures and algorithms designed for\nrunning the applications on modern HPC architectures, composed of hierarchical\nnetwork interconnections and stratified memory. DASH implements a PGAS\n(partitioned global address space) model in the form of C++ templates, built on\ntop of DART -- a run-time system with an abstracted tier above existing\none-sided communication libraries.\n  In order to facilitate the application development process for exploiting the\nhierarchical organization of HPC machines, DART allows to reorder the placement\nof the computational units. In this paper we present an automatic, hierarchical\nunits mapping technique (using a similar approach to the Hilbert curve\ntransformation) to reorder the placement of DART units on the Cray XC40 machine\nHazel Hen at HLRS. To evaluate the performance of new units mapping which takes\ninto the account the topology of allocated compute nodes, we perform latency\nbenchmark for a 3D stencil code. The technique of units mapping is generic and\ncan be be adopted in other DART communication substrates and on other hardware\nplatforms.\n  Furthermore, high--level features of DASH are presented, enabling more\ncomplex automatic transformations and optimizations in the future.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 17:03:33 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Idrees", "Kamran", ""], ["Fuchs", "Tobias", ""], ["Glass", "Colin W.", ""]]}, {"id": "1603.01581", "submitter": "Philipp Geiger", "authors": "Philipp Geiger, Lucian Carata, Bernhard Schoelkopf", "title": "Causal inference for data-driven debugging and decision making in cloud\n  computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing involves complex technical and economical systems and\ninteractions. This brings about various challenges, two of which are: (1)\ndebugging and control to optimize the performance of computing systems, with\nthe help of sandbox experiments, and (2) privacy-preserving prediction of the\ncost of ``spot'' resources for decision making of cloud clients. In this paper,\nwe formalize debugging by counterfactual probabilities and control by\npost-(soft-)interventional probabilities. We prove that counterfactuals can\napproximately be calculated from a ``stochastic'' graphical causal model (while\nthey are originally defined only for ``deterministic'' functional causal\nmodels), and based on this sketch a data-driven approach to address problem\n(1). To address problem (2), we formalize bidding by post-(soft-)interventional\nprobabilities and present a simple mathematical result on approximate\nintegration of ``incomplete'' conditional probability distributions. We show\nhow this can be used by cloud clients to trade off privacy against\npredictability of the outcome of their bidding actions in a toy scenario. We\nreport experiments on simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:28:13 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 12:09:23 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 09:14:54 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 15:50:17 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2019 13:45:27 GMT"}, {"version": "v6", "created": "Mon, 10 Jun 2019 11:53:17 GMT"}, {"version": "v7", "created": "Sat, 25 Jan 2020 07:37:15 GMT"}, {"version": "v8", "created": "Tue, 10 Mar 2020 09:58:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Geiger", "Philipp", ""], ["Carata", "Lucian", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1603.01708", "submitter": "Huawei Huang", "authors": "Huawei Huang, Song Guo, Weifa Liang, Keqiu Li and Baoliu Ye", "title": "Technique Report: Near-Optimal Routing Protection for SDN Networks Using\n  Distributed Markov Approximation", "comments": "This paper has been withdrawn by the author due to a crucial\n  presentation error in Algorithm 1", "journal-ref": "IEEE Journal on Selected Areas in Communications ( Volume: 34,\n  Issue: 11, Nov. 2016 )", "doi": "10.1109/JSAC.2016.2615184", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking (SDN) brings numbers of advantages along with\nmany challenges. One particular concern is on the control-plane resilience,\nwhile the existing protection approaches proposed for SDN networks mainly focus\non data-plane. In order to achieve the carrier-grade recovery from link\nfailures, we adopt the dedicated protection scheme towards finding optimal\nprotection routing for control-plane traffic. To this end, we study a weighted\ncost minimization problem, in which the traffic load balancing and flow table\nrule placement are jointly considered when selecting protection paths for\ncontroller-switch sessions. Because this problem is known as NP-hard, we\npropose a Markov approximation based combinatorial optimization approach for\nrouting protection in SDN control-plane, which produces near-optimal solution\nin a distributed fashion. We then extend our solution to an on-line case that\ncan handle the single-link failure one at a time. The induced performance\nfluctuation is also analyzed with theoretical derivation. Extensive\nexperimental results show that our proposed algorithm has fast convergence and\nhigh efficiency in resource utilization.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 10:26:01 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 04:21:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Huang", "Huawei", ""], ["Guo", "Song", ""], ["Liang", "Weifa", ""], ["Li", "Keqiu", ""], ["Ye", "Baoliu", ""]]}, {"id": "1603.01876", "submitter": "Jeremy Kepner", "authors": "Patrick Dreher, Chansup Byun, Chris Hill, Vijay Gadepally, Bradley\n  Kuszmaul, Jeremy Kepner", "title": "PageRank Pipeline Benchmark: Proposal for a Holistic System Benchmark\n  for Big-Data Platforms", "comments": "9 pages, 7 figures, to appear in IPDPS 2016 Graph Algorithms Building\n  Blocks (GABB) workshop", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.89", "report-no": null, "categories": "cs.PF astro-ph.IM cs.DC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of big data systems has created a need for benchmarks to measure and\ncompare the capabilities of these systems. Big data benchmarks present unique\nscalability challenges. The supercomputing community has wrestled with these\nchallenges for decades and developed methodologies for creating rigorous\nscalable benchmarks (e.g., HPC Challenge). The proposed PageRank pipeline\nbenchmark employs supercomputing benchmarking methodologies to create a\nscalable benchmark that is reflective of many real-world big data processing\nsystems. The PageRank pipeline benchmark builds on existing prior scalable\nbenchmarks (Graph500, Sort, and PageRank) to create a holistic benchmark with\nmultiple integrated kernels that can be run together or independently. Each\nkernel is well defined mathematically and can be implemented in any programming\nenvironment. The linear algebraic nature of PageRank makes it well suited to\nbeing implemented using the GraphBLAS standard. The computations are simple\nenough that performance predictions can be made based on simple computing\nhardware models. The surrounding kernels provide the context for each kernel\nthat allows rigorous definition of both the input and the output for each\nkernel. Furthermore, since the proposed PageRank pipeline benchmark is scalable\nin both problem size and hardware, it can be used to measure and quantitatively\ncompare a wide range of present day and future systems. Serial implementations\nin C++, Python, Python with Pandas, Matlab, Octave, and Julia have been\nimplemented and their single threaded performance has been measured.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 20:51:47 GMT"}, {"version": "v2", "created": "Sat, 4 Jun 2016 01:42:05 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Dreher", "Patrick", ""], ["Byun", "Chansup", ""], ["Hill", "Chris", ""], ["Gadepally", "Vijay", ""], ["Kuszmaul", "Bradley", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1603.02188", "submitter": "Frederik Mallmann-Trenn", "authors": "Petra Berenbrink, Tom Friedetzky, Peter Kling, Frederik\n  Mallmann-Trenn, Lars Nagel, and Chris Wastell", "title": "Self-stabilizing Balls & Bins in Batches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in distributed computing is the distribution of\nrequests to a set of uniform servers without a centralized controller.\nClassically, such problems are modeled as static balls into bins processes,\nwhere $m$ balls (tasks) are to be distributed to $n$ bins (servers). In a\nseminal work, Azar et al. proposed the sequential strategy \\greedy{d} for\n$n=m$. When thrown, a ball queries the load of $d$ random bins and is allocated\nto a least loaded of these. Azar et al. showed that $d=2$ yields an exponential\nimprovement compared to $d=1$. Berenbrink et al. extended this to $m\\gg n$,\nshowing that the maximal load difference is independent of $m$ for $d=2$ (in\ncontrast to $d=1$).\n  We propose a new variant of an \\emph{infinite} balls into bins process. Each\nround an expected number of $\\lambda n$ new balls arrive and are distributed\n(in parallel) to the bins. Each non-empty bin deletes one of its balls. This\nsetting models a set of servers processing incoming requests, where clients can\nquery a server's current load but receive no information about parallel\nrequests. We study the \\greedy{d} distribution scheme in this setting and show\na strong self-stabilizing property: For \\emph{any} arrival rate\n$\\lambda=\\lambda(n)<1$, the system load is time-invariant. Moreover, for\n\\emph{any} (even super-exponential) round $t$, the maximum system load is\n(w.h.p.) $O(\\frac{1}{1-\\lambda}\\cdot\\log\\frac{n}{1-\\lambda})$ for $d=1$ and\n$O(\\log\\frac{n}{1-\\lambda})$ for $d=2$. In particular, \\greedy{2} has an\nexponentially smaller system load for high arrival rates.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 18:19:13 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Berenbrink", "Petra", ""], ["Friedetzky", "Tom", ""], ["Kling", "Peter", ""], ["Mallmann-Trenn", "Frederik", ""], ["Nagel", "Lars", ""], ["Wastell", "Chris", ""]]}, {"id": "1603.02226", "submitter": "Jose Gracia", "authors": "Huan Zhou and Kamran Idrees and Jos\\'e Gracia", "title": "Leveraging MPI-3 Shared-Memory Extensions for Efficient PGAS Runtime\n  Systems", "comments": "12 papers, accepted for publication in EuroPar 2015", "journal-ref": null, "doi": "10.1007/978-3-662-48096-0_29", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relaxed semantics and rich functionality of one-sided communication\nprimitives of MPI-3 makes MPI an attractive candidate for the implementation of\nPGAS models. However, the performance of such implementation suffers from the\nfact, that current MPI RMA implementations typically have a large overhead when\nsource and target of a communication request share a common, local physical\nmemory. In this paper, we present an optimized PGAS-like runtime system which\nuses the new MPI-3 shared-memory extensions to serve intra-node communication\nrequests and MPI-3 one-sided communication primitives to serve inter-node\ncommunication requests. The performance of our runtime system is evaluated on a\nCray XC40 system through low-level communication benchmarks, a random-access\nbenchmark and a stencil kernel. The results of the experiments demonstrate that\nthe performance of our hybrid runtime system matches the performance of\nlow-level RMA libraries for intra-node transfers, and that of MPI-3 for\ninter-node transfers.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 19:45:07 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Zhou", "Huan", ""], ["Idrees", "Kamran", ""], ["Gracia", "Jos\u00e9", ""]]}, {"id": "1603.02297", "submitter": "Paul Springer", "authors": "Paul Springer and Jeff R. Hammond and Paolo Bientinesi", "title": "TTC: A high-performance Compiler for Tensor Transpositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TTC, an open-source parallel compiler for multidimensional tensor\ntranspositions. In order to generate high-performance C++ code, TTC explores a\nnumber of optimizations, including software prefetching, blocking,\nloop-reordering, and explicit vectorization. To evaluate the performance of\nmultidimensional transpositions across a range of possible use-cases, we also\nrelease a benchmark covering arbitrary transpositions of up to six dimensions.\nPerformance results show that the routines generated by TTC achieve close to\npeak memory bandwidth on both the Intel Haswell and the AMD Steamroller\narchitectures, and yield significant performance gains over modern compilers.\nBy implementing a set of pruning heuristics, TTC allows users to limit the\nnumber of potential solutions; this option is especially useful when dealing\nwith high-dimensional tensors, as the search space might become prohibitively\nlarge. Experiments indicate that when only 100 potential solutions are\nconsidered, the resulting performance is about 99% of that achieved with\nexhaustive search.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 21:13:00 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Springer", "Paul", ""], ["Hammond", "Jeff R.", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1603.02339", "submitter": "Charles Siegel", "authors": "Abhinav Vishnu, Charles Siegel and Jeffrey Daily", "title": "Distributed TensorFlow with MPI", "comments": "6 pages; fixed significant typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning and Data Mining (MLDM) algorithms are becoming increasingly\nimportant in analyzing large volume of data generated by simulations,\nexperiments and mobile devices. With increasing data volume, distributed memory\nsystems (such as tightly connected supercomputers or cloud computing systems)\nare becoming important in designing in-memory and massively parallel MLDM\nalgorithms. Yet, the majority of open source MLDM software is limited to\nsequential execution with a few supporting multi-core/many-core execution.\n  In this paper, we extend recently proposed Google TensorFlow for execution on\nlarge scale clusters using Message Passing Interface (MPI). Our approach\nrequires minimal changes to the TensorFlow runtime -- making the proposed\nimplementation generic and readily usable to increasingly large users of\nTensorFlow. We evaluate our implementation using an InfiniBand cluster and\nseveral well knowndatasets. Our evaluation indicates the efficiency of our\nproposed implementation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 23:16:44 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 17:27:34 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Vishnu", "Abhinav", ""], ["Siegel", "Charles", ""], ["Daily", "Jeffrey", ""]]}, {"id": "1603.02526", "submitter": "Nate Derbinsky", "authors": "Ning Hao and AmirReza Oghbaee and Mohammad Rostami and Nate Derbinsky\n  and Jos\\'e Bento", "title": "Testing fine-grained parallelism for the ADMM on a factor-graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an ongoing effort to develop tools that apply distributed\ncomputational resources to tackle large problems or reduce the time to solve\nthem. In this context, the Alternating Direction Method of Multipliers (ADMM)\narises as a method that can exploit distributed resources like the dual ascent\nmethod and has the robustness and improved convergence of the augmented\nLagrangian method. Traditional approaches to accelerate the ADMM using multiple\ncores are problem-specific and often require multi-core programming. By\ncontrast, we propose a problem-independent scheme of accelerating the ADMM that\ndoes not require the user to write any parallel code. We show that this scheme,\nan interpretation of the ADMM as a message-passing algorithm on a factor-graph,\ncan automatically exploit fine-grained parallelism both in GPUs and\nshared-memory multi-core computers and achieves significant speedup in such\ndiverse application domains as combinatorial optimization, machine learning,\nand optimal control. Specifically, we obtain 10-18x speedup using a GPU, and\n5-9x using multiple CPU cores, over a serial, optimized C-version of the ADMM,\nwhich is similar to the typical speedup reported for existing GPU-accelerated\nlibraries, including cuFFT (19x), cuBLAS (17x), and cuRAND (8x).\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 14:13:38 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Hao", "Ning", ""], ["Oghbaee", "AmirReza", ""], ["Rostami", "Mohammad", ""], ["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""]]}, {"id": "1603.02580", "submitter": "Jeremy Morse", "authors": "Jeremy Morse, Steve Kerrison, Kerstin Eder", "title": "On the limitations of analysing worst-case dynamic energy of processing", "comments": null, "journal-ref": null, "doi": "10.1145/3173042", "report-no": null, "categories": "cs.CC cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines dynamic energy consumption caused by data during software\nexecution on deeply embedded microprocessors, which can be significant on some\ndevices. In worst-case energy consumption analysis, energy models are used to\nfind the most costly execution path. Taking each instruction's worst case\nenergy produces a safe but overly pessimistic upper bound. Algorithms for safe\nand tight bounds would be desirable. We show that finding exact worst-case\nenergy is NP-hard, and that tight bounds cannot be approximated with guaranteed\nsafety. We conclude that any energy model targeting tightness must either\nsacrifice safety or accept overapproximation proportional to data-dependent\nenergy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 19:22:13 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 12:55:19 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 11:06:30 GMT"}, {"version": "v4", "created": "Wed, 21 Feb 2018 16:10:19 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Morse", "Jeremy", ""], ["Kerrison", "Steve", ""], ["Eder", "Kerstin", ""]]}, {"id": "1603.02655", "submitter": "Varun Nagpal", "authors": "Varun Nagpal", "title": "Study and evaluation of an Irregular Graph Algorithm on Multicore and\n  GPU Processor Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": "1115-1213Nagpal", "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One area of Computing applications which poses significant challenge of\nperformance scalability on Chip Multiprocessors(CMP's) are Irregular\napplications. Such applications have very little computation and unpredictable\nmemory access patterns making them memory-bound in contrast to compute-bound\napplications. Since the gap between processor and memory performance continues\nto exist, difficulty to hide and decrease this gap is one of the important\nfactors which results in poor performance of these applications on CMP's.\n  The goal of this thesis is to overcome many such challenges posed during\nperformance acceleration of an irregular graph algorithm called Triad Census.\nWe accelerated the Triad Census algorithm on two significantly different Chip\nMultiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads per\nsocket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads\nper core).\n  The experimental results obtained on Intel Multicore Xeon system shows\nperformance speedups (w.r.t baseline sequential) of maximum 56x , average 33x\nand minimum 8.3x for real world graph data sets. On NVIDIA Tesla C1060 GPGPU,\nwe were able to match almost equally the Multicore results - 58.4x maximum,\n32.8x average and 4.2x minimum speedups w.r.t baseline sequential. In terms of\nraw performance, for the graph data set called Patents network, our results on\nIntel Xeon Multicore(16 hw threads) were 1.27x times faster than previous\nresults on Cray XMT(16 hw threads) while results achieved on GPGPU were\ncomparatively slower(0.72x). To the best of our knowledge, this algorithm has\nonly been accelerated on supercomputer class computer named Cray XMT and no\nwork exists that demonstrates performance evaluation and comparison of this\nalgorithm on relatively lower-cost Multicore and GPGPU based platforms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 20:07:31 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Nagpal", "Varun", ""]]}, {"id": "1603.02886", "submitter": "Farah Hariri Dr.", "authors": "F. Hariri, T.M. Tran, A. Jocksch, E. Lanti, J. Progsch, P. Messmer, S.\n  Brunner, G. Gheller, L. Villard", "title": "A portable platform for accelerated PIC codes and its application to\n  GPUs using OpenACC", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2016.05.008", "report-no": null, "categories": "physics.comp-ph cs.DC physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a portable platform, called PIC_ENGINE, for accelerating\nParticle-In-Cell (PIC) codes on heterogeneous many-core architectures such as\nGraphic Processing Units (GPUs). The aim of this development is efficient\nsimulations on future exascale systems by allowing different parallelization\nstrategies depending on the application problem and the specific architecture.\nTo this end, this platform contains the basic steps of the PIC algorithm and\nhas been designed as a test bed for different algorithmic options and data\nstructures. Among the architectures that this engine can explore, particular\nattention is given here to systems equipped with GPUs. The study demonstrates\nthat our portable PIC implementation based on the OpenACC programming model can\nachieve performance closely matching theoretical predictions. Using the Cray\nXC30 system, Piz Daint, at the Swiss National Supercomputing Centre (CSCS), we\nshow that PIC_ENGINE running on an NVIDIA Kepler K20X GPU can outperform the\none on an Intel Sandybridge 8-core CPU by a factor of 3.4.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 13:55:56 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Hariri", "F.", ""], ["Tran", "T. M.", ""], ["Jocksch", "A.", ""], ["Lanti", "E.", ""], ["Progsch", "J.", ""], ["Messmer", "P.", ""], ["Brunner", "S.", ""], ["Gheller", "G.", ""], ["Villard", "L.", ""]]}, {"id": "1603.02955", "submitter": "Roberto Morabito", "authors": "Roberto Morabito", "title": "A Performance Evaluation of Container Technologies on Internet of Things\n  Devices", "comments": null, "journal-ref": null, "doi": "10.1109/INFCOMW.2016.7562228", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of virtualization technologies in different contexts - such as Cloud\nEnvironments, Internet of Things (IoT), Software Defined Networking (SDN) - has\nrapidly increased during the last years. Among these technologies,\ncontainer-based solutions own characteristics for deploying distributed and\nlightweight applications. This paper presents a performance evaluation of\ncontainer technologies on constrained devices, in this case, on Raspberry Pi.\nThe study shows that, overall, the overhead added by containers is negligible.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 16:40:39 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Morabito", "Roberto", ""]]}, {"id": "1603.02981", "submitter": "Cameron Musco", "authors": "Cameron Musco, Hsin-Hao Su, Nancy Lynch", "title": "Ant-Inspired Density Estimation via Random Walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many ant species employ distributed population density estimation in\napplications ranging from quorum sensing [Pra05], to task allocation [Gor99],\nto appraisal of enemy colony strength [Ada90]. It has been shown that ants\nestimate density by tracking encounter rates -- the higher the population\ndensity, the more often the ants bump into each other [Pra05,GPT93].\n  We study distributed density estimation from a theoretical perspective. We\nprove that a group of anonymous agents randomly walking on a grid are able to\nestimate their density within a small multiplicative error in few steps by\nmeasuring their rates of encounter with other agents. Despite dependencies\ninherent in the fact that nearby agents may collide repeatedly (and, worse,\ncannot recognize when this happens), our bound nearly matches what would be\nrequired to estimate density by independently sampling grid locations.\n  From a biological perspective, our work helps shed light on how ants and\nother social insects can obtain relatively accurate density estimates via\nencounter rates. From a technical perspective, our analysis provides new tools\nfor understanding complex dependencies in the collision probabilities of\nmultiple random walks. We bound the strength of these dependencies using\n$local\\ mixing\\ properties$ of the underlying graph. Our results extend beyond\nthe grid to more general graphs and we discuss applications to size estimation\nfor social networks and density estimation for robot swarms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 18:00:41 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 15:56:40 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Musco", "Cameron", ""], ["Su", "Hsin-Hao", ""], ["Lynch", "Nancy", ""]]}, {"id": "1603.03301", "submitter": "Hunter Monroe", "authors": "Daniel Monroe", "title": "New Lower Bounds for van der Waerden Numbers Using Distributed Computing", "comments": "8 pages, 1 figure. This version reflects new results and reader\n  comments", "journal-ref": "Journal of Combinatorial Mathematics and Combinatorial Computing,\n  2021", "doi": null, "report-no": null, "categories": "math.CO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides new lower bounds for van der Waerden numbers. The number\n$W(k,r)$ is defined to be the smallest integer $n$ for which any $r$-coloring\nof the integers $0 \\ldots, n-1$ admits monochromatic arithmetic progression of\nlength $k$; its existence is implied by van der Waerden's Theorem. We exhibit\n$r$-colorings of $0\\ldots n-1$ that do not contain monochromatic arithmetic\nprogressions of length $k$ to prove that $W(k, r)>n$. These colorings are\nconstructed using existing techniques. Rabung's method, given a prime $p$ and a\nprimitive root $\\rho$, applies a color given by the discrete logarithm base\n$\\rho$ mod $r$ and concatenates $k-1$ copies. We also used Herwig et al's\nCyclic Zipper Method, which doubles or quadruples the length of a coloring,\nwith the faster check of Rabung and Lotts. We were able to check larger primes\nthan previous results, employing around 2 teraflops of computing power for 12\nmonths through distributed computing by over 500 volunteers. This allowed us to\ncheck all primes through 950 million, compared to 10 million by Rabung and\nLotts. Our lower bounds appear to grow roughly exponentially in $k$. Given that\nthese constructions produce tight lower bounds for known van der Waerden\nnumbers, this data suggests that exact van der Waerden Numbers grow\nexponentially in $k$ with ratio $r$ asymptotically, which is a new conjecture,\naccording to Graham.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 03:29:25 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 03:37:49 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2016 22:18:34 GMT"}, {"version": "v4", "created": "Mon, 6 Nov 2017 16:24:39 GMT"}, {"version": "v5", "created": "Sun, 19 Nov 2017 15:53:47 GMT"}, {"version": "v6", "created": "Wed, 22 May 2019 22:03:05 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Monroe", "Daniel", ""]]}, {"id": "1603.03404", "submitter": "Tianwei Zhang", "authors": "Tianwei Zhang, Yinqian Zhang, Ruby B. Lee", "title": "Memory DoS Attacks in Multi-tenant Clouds: Severity and Mitigation", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cloud computing, network Denial of Service (DoS) attacks are well studied\nand defenses have been implemented, but severe DoS attacks on a victim's\nworking memory by a single hostile VM are not well understood. Memory DoS\nattacks are Denial of Service (or Degradation of Service) attacks caused by\ncontention for hardware memory resources on a cloud server. Despite the strong\nmemory isolation techniques for virtual machines (VMs) enforced by the software\nvirtualization layer in cloud servers, the underlying hardware memory layers\nare still shared by the VMs and can be exploited by a clever attacker in a\nhostile VM co-located on the same server as the victim VM, denying the victim\nthe working memory he needs. We first show quantitatively the severity of\ncontention on different memory resources. We then show that a malicious cloud\ncustomer can mount low-cost attacks to cause severe performance degradation for\na Hadoop distributed application, and 38X delay in response time for an\nE-commerce website in the Amazon EC2 cloud.\n  Then, we design an effective, new defense against these memory DoS attacks,\nusing a statistical metric to detect their existence and execution throttling\nto mitigate the attack damage. We achieve this by a novel re-purposing of\nexisting hardware performance counters and duty cycle modulation for security,\nrather than for improving performance or power consumption. We implement a full\nprototype on the OpenStack cloud system. Our evaluations show that this defense\nsystem can effectively defeat memory DoS attacks with negligible performance\noverhead.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 20:16:52 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 04:46:07 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 16:43:59 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Zhang", "Tianwei", ""], ["Zhang", "Yinqian", ""], ["Lee", "Ruby B.", ""]]}, {"id": "1603.03488", "submitter": "Salvador Tamarit", "authors": "Salvador Tamarit, Julio Mari\\~no, Guillermo Vigueras, Manuel Carro", "title": "Proceedings of the First Workshop on Program Transformation for\n  Programmability in Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of PROHA 2016, the first workshop on\nProgram Transformation for Programmability in Heterogeneous Architectures, held\non March 12, 2016 in Barcelona, Spain, as an affiliated workshop of CGO 2016,\nthe 14th International Symposium on Code Generation and Optimization.\nDeveloping and maintaining high-performance applications and libraries for\nheterogeneous architectures while preserving its semantics and with a\nreasonable efficiency is a time-consuming task which is often only possible for\nexperts. It often requires manually adapting sequential, platform-agnostic code\nto different infrastructures, and keeping the changes in all of these\ninfrastructures in sync. These program modification tasks are costly and\nerror-prone. Tools to assist in and, if possible, automate such transformations\nare of course of great interest. However, such tools may need significant\nreasoning and knowledge processing capabilities, including, for example, being\nable to process machine-understandable descriptions of the semantics of a piece\nof code is expected to do; to perform program transformations inside a context\nin which they are applicable; to use strategies to identify the sequence of\ntransformations leading to the best resulting code; and others.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:44:33 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Tamarit", "Salvador", ""], ["Mari\u00f1o", "Julio", ""], ["Vigueras", "Guillermo", ""], ["Carro", "Manuel", ""]]}, {"id": "1603.03502", "submitter": "Mohammad Rahman", "authors": "Mohammad Tanvir Rahman, Hien Nguyen, Jaspal Subhlok, Gopal Pandurangan", "title": "Checkpointing to minimize completion time for Inter-dependent Parallel\n  Processes on Volunteer Grids", "comments": "This paper has 10 pages, 4 figures. The paper was accepted in ccgrid\n  2016 as a short paper; We are keeping the longer version here", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volunteer computing is being used successfully for large scale scientific\ncomputations. This research is in the context of Volpex, a programming\nframework that supports communicating parallel processes in a volunteer\nenvironment. Redundancy and checkpointing are combined to ensure consistent\nforward progress with Volpex in this unique execution environment characterized\nby heterogeneous failure prone nodes and interdependent replicated processes.\nAn important parameter for optimizing performance with Volpex is the frequency\nof checkpointing. The paper presents a mathematical model to minimize the\ncompletion time for inter-dependent parallel processes running in a volunteer\nenvironment by finding a suitable checkpoint interval. Validation is performed\nwith a sample real world application running on a pool of distributed volunteer\nnodes. The results indicate that the performance with our predicted checkpoint\ninterval is fairly close to the best performance obtained empirically by\nvarying the checkpoint interval.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 02:26:11 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Rahman", "Mohammad Tanvir", ""], ["Nguyen", "Hien", ""], ["Subhlok", "Jaspal", ""], ["Pandurangan", "Gopal", ""]]}, {"id": "1603.03820", "submitter": "Wei Tan", "authors": "Wei Tan, Liangliang Cao, Liana Fong", "title": "Faster and Cheaper: Parallelizing Large-Scale Matrix Factorization on\n  GPUs", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix factorization (MF) is employed by many popular algorithms, e.g.,\ncollaborative filtering. The emerging GPU technology, with massively multicore\nand high intra-chip memory bandwidth but limited memory capacity, presents an\nopportunity for accelerating MF much further when appropriately exploiting the\nGPU architectural characteristics.\n  This paper presents cuMF, a CUDA-based matrix factorization library that\nimplements memory-optimized alternate least square (ALS) method to solve very\nlarge-scale MF. CuMF uses a variety set of techniques to maximize the\nperformance on either single or multiple GPUs. These techniques include smart\naccess of sparse data leveraging GPU memory hierarchy, using data parallelism\nin conjunction with model parallelism, minimizing the communication overhead\nbetween computing units, and utilizing a novel topology-aware parallel\nreduction scheme.\n  With only a single machine with four Nvidia GPU cards, cuMF can be 6-10 times\nas fast, and 33-100 times as cost-efficient, compared with the state-of-art\ndistributed CPU solutions. Moreover, this cuMF can solve the largest matrix\nfactorization problem ever reported yet in current literature, while\nmaintaining impressively good performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 23:27:37 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Tan", "Wei", ""], ["Cao", "Liangliang", ""], ["Fong", "Liana", ""]]}, {"id": "1603.03849", "submitter": "Haifeng Li", "authors": "Jie Chen, Jian Peng, Min Deng, Chao Tao, Haifeng Li", "title": "Queue Theory based Response Time Analyses for Geo-Information Processing\n  Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical characteristics of remote sensing applications are concurrent tasks,\nsuch as those found in disaster rapid response. The existing composition\napproach to geographical information processing service chain, searches for an\noptimisation solution and is what can be deemed a \"selfish\" way. This way leads\nto problems of conflict amongst concurrent tasks and decreases the performance\nof all service chains. In this study, a non-cooperative game-based mathematical\nmodel to analyse the competitive relationships between tasks, is proposed. A\nbest response function is used, to assure each task maintains utility\noptimisation by considering composition strategies of other tasks and\nquantifying conflicts between tasks. Based on this, an iterative algorithm that\nconverges to Nash equilibrium is presented, the aim being to provide good\nconvergence and maximise the utilisation of all tasks under concurrent task\nconditions. Theoretical analyses and experiments showed that the newly proposed\nmethod, when compared to existing service composition methods, has better\npractical utility in all tasks.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 02:32:35 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Chen", "Jie", ""], ["Peng", "Jian", ""], ["Deng", "Min", ""], ["Tao", "Chao", ""], ["Li", "Haifeng", ""]]}, {"id": "1603.03888", "submitter": "Kamran Idrees", "authors": "Kamran Idrees, Christoph Niethammer, Aniello Esposito, Colin W. Glass", "title": "Performance Evaluation of Unified Parallel C for Molecular Dynamics", "comments": "6 pages, 6 figures, Proceedings of the 7th International Conference\n  on PGAS Programming Models 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partitioned Global Address Space (PGAS) integrates the concepts of shared\nmemory programming and the control of data distribution and locality provided\nby message passing into a single parallel programming model. The purpose of\nallying distributed data with shared memory is to cultivate a locality-aware\nshared memory paradigm. PGAS is comprised of a single shared address space,\nwhich is partitioned among threads. Each thread has a portion of the shared\naddress space in local memory and therefore it can exploit data locality by\nmainly doing computation on local data. Unified Parallel C (UPC) is a parallel\nextension of ISO C and an implementation of the PGAS model. In this paper, we\nevaluate the performance of UPC based on a real-world scenario from Molecular\nDynamics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 10:41:15 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Idrees", "Kamran", ""], ["Niethammer", "Christoph", ""], ["Esposito", "Aniello", ""], ["Glass", "Colin W.", ""]]}, {"id": "1603.03971", "submitter": "Sri Raj Paul", "authors": "Sri Raj Paul, John Mellor-Crummey, Mauricio Araya-Polo, Detlef Hohl", "title": "Performance Analysis and Optimization of a Hybrid Distributed Reverse\n  Time Migration Application", "comments": "2 page extended abstract presented at The International Conference\n  for High Performance Computing, Networking, Storage and Analysis (SC) 2015\n  for ACM Student Research Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications to process seismic data employ scalable parallel systems to\nproduce timely results. To fully exploit emerging processor architectures,\napplication will need to employ threaded parallelism within a node and message\npassing across nodes. Today, MPI+OpenMP is the preferred programming model for\nthis task. However, tuning hybrid programs for clusters is difficult.\nPerformance tools can help users identify bottlenecks and uncover opportunities\nfor improvement. This poster describes our experiences of applying Rice\nUniversity's HPCToolkit and hardware performance counters to gain insight into\nan MPI+OpenMP code that performs Reverse Time Migration (RTM) on a cluster of\nmulticore processors. The tools provided us with insights into the\neffectiveness of the domain decomposition strategy, the use of threaded\nparallelism, and functional unit utilization in individual cores. By applying\ninsights obtained from the tools, we were able to improve the performance of\nthe RTM code by roughly 30 percent.\n", "versions": [{"version": "v1", "created": "Sat, 12 Mar 2016 22:38:53 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Paul", "Sri Raj", ""], ["Mellor-Crummey", "John", ""], ["Araya-Polo", "Mauricio", ""], ["Hohl", "Detlef", ""]]}, {"id": "1603.04002", "submitter": "Richard Nock", "authors": "Giorgio Patrini, Richard Nock, Stephen Hardy, Tiberio Caetano", "title": "Fast Learning from Distributed Datasets without Entity Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following data fusion scenario: two datasets/peers contain the\nsame real-world entities described using partially shared features, e.g.\nbanking and insurance company records of the same customer base. Our goal is to\nlearn a classifier in the cross product space of the two domains, in the hard\ncase in which no shared ID is available -- e.g. due to anonymization.\nTraditionally, the problem is approached by first addressing entity matching\nand subsequently learning the classifier in a standard manner. We present an\nend-to-end solution which bypasses matching entities, based on the recently\nintroduced concept of Rademacher observations (rados). Informally, we replace\nthe minimisation of a loss over examples, which requires to solve entity\nresolution, by the equivalent minimisation of a (different) loss over rados.\nAmong others, key properties we show are (i) a potentially huge subset of these\nrados does not require to perform entity matching, and (ii) the algorithm that\nprovably minimizes the rado loss over these rados has time and space\ncomplexities smaller than the algorithm minimizing the equivalent example loss.\nLast, we relax a key assumption of the model, that the data is vertically\npartitioned among peers --- in this case, we would not even know the existence\nof a solution to entity resolution. In this more general setting, experiments\nvalidate the possibility of significantly beating even the optimal peer in\nhindsight.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 06:03:39 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Patrini", "Giorgio", ""], ["Nock", "Richard", ""], ["Hardy", "Stephen", ""], ["Caetano", "Tiberio", ""]]}, {"id": "1603.04067", "submitter": "Maryam Helmi", "authors": "Maryam Helmi, Lisa Higham, Philipp Woelfel", "title": "Space Bounds for Adaptive Renaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the space complexity of implementing long-lived and one-shot\nadaptive renaming from multi-reader multi-writer registers, in an asynchronous\ndistributed system with $n$ processes. As a result of an $f$-adaptive renaming\nalgorithm each participating process gets a distinct name in the range\n$\\{1,\\dots,f(k)\\}$ provided $k$ processes participate.\n  Let $f: \\{1,\\dots,n\\} \\rightarrow \\mathbb{N}$ be a non-decreasing function\nsatisfying $f(1) \\leq n-1$ and let $d = \\max\\{x ~|~ f(x) \\leq n-1\\}$. We show\nthat any non-deterministic solo-terminating long-lived $f$-adaptive renaming\nobject requires $d + 1$ registers. This implies a lower bound of $n-c$\nregisters for long-lived $(k+c)$-adaptive renaming, which we observe is tight.\n  We also prove a lower bound of $\\lfloor \\frac{2(n - c)}{c+2} \\rfloor$\nregisters for implementing any non-deterministic solo-terminating one-shot\n$(k+c)$-adaptive renaming. We provide two one-shot renaming algorithms: a\nwait-free algorithm and an obstruction-free algorithm. Each algorithm employs a\nparameter to depict the tradeoff between space and adaptivity. When these\nparameters are chosen appropriately, this results in a wait-free one-shot\n$(\\frac{3k^2}{2})$-adaptive renaming algorithm from $\\lceil \\sqrt{n} \\rceil +\n1$ registers, and an obstruction-free one-shot $f$-adaptive renaming algorithm\nfrom only $\\min\\{n, x ~|~ f(x) \\geq 2n\\} + 1$ registers.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 19:28:20 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 00:38:04 GMT"}, {"version": "v3", "created": "Sat, 19 Mar 2016 02:11:10 GMT"}, {"version": "v4", "created": "Tue, 22 Mar 2016 06:29:03 GMT"}, {"version": "v5", "created": "Tue, 29 Mar 2016 03:01:18 GMT"}, {"version": "v6", "created": "Sun, 15 May 2016 23:41:37 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Helmi", "Maryam", ""], ["Higham", "Lisa", ""], ["Woelfel", "Philipp", ""]]}, {"id": "1603.04199", "submitter": "Matthieu Perrin", "authors": "Matthieu Perrin (LINA), Achour Mostefaoui (LINA), Claude Jard (LINA)", "title": "Causal Consistency: Beyond Memory", "comments": "21st ACM SIGPLAN Symposium on Principles and Practice of Parallel\n  Programming, Mar 2016, Barcelone, Spain", "journal-ref": null, "doi": "10.1145/2851141.2851170", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed systems where strong consistency is costly when not\nimpossible, causal consistency provides a valuable abstraction to represent\nprogram executions as partial orders. In addition to the sequential program\norder of each computing entity, causal order also contains the semantic links\nbetween the events that affect the shared objects -- messages emission and\nreception in a communication channel , reads and writes on a shared register.\nUsual approaches based on semantic links are very difficult to adapt to other\ndata types such as queues or counters because they require a specific analysis\nof causal dependencies for each data type. This paper presents a new approach\nto define causal consistency for any abstract data type based on sequential\nspecifications. It explores, formalizes and studies the differences between\nthree variations of causal consistency and highlights them in the light of\nPRAM, eventual consistency and sequential consistency: weak causal consistency,\nthat captures the notion of causality preservation when focusing on convergence\n; causal convergence that mixes weak causal consistency and convergence; and\ncausal consistency, that coincides with causal memory when applied to shared\nmemory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 10:43:01 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Perrin", "Matthieu", "", "LINA"], ["Mostefaoui", "Achour", "", "LINA"], ["Jard", "Claude", "", "LINA"]]}, {"id": "1603.04228", "submitter": "Juan Jos\\'e Berm\\'udez", "authors": "Juanjo Berm\\'udez", "title": "A practical multi-party computation algorithm for a secure distributed\n  online voting system", "comments": "7 pages, 6 tables, patent pending (WO 2015193524 A1)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online voting architecture based on partitioning the election\nin small clusters of voters and using a new Multi-party Computation algorithm\nfor obtaining voting results from the clusters. This new algorithm has some\npractical advantages over other previously known algorithms and isn't bound to\nany specific cryptographic concept; so it can be adapted to future\ncryptographic exigencies. Compared with other online voting technologies, we\nsee that this new architecture is less vulnerable to hacker attacks and attacks\nfrom dishonest authorities, given that no sensitive information is stored in\nany public server and there is no need for any trustee to safeguard the\nlegality of the election process. Even in case of an attack succeeding, the\nrisks associated with the overall election are far lower than with any other\nvoting system. This architecture can also be combined with any other voting\nsystem, inheriting advantages from both systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 12:10:47 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Berm\u00fadez", "Juanjo", ""]]}, {"id": "1603.04234", "submitter": "Arnaud Labourel", "authors": "Julian Anaya (DII), J\\'er\\'emie Chalopin (LIF), Jurek Czyzowicz (DII),\n  Arnaud Labourel (LBI2M), Andrzej Pelc (DII), Yann Vax\\`es (LIF)", "title": "Convergecast and Broadcast by Power-Aware Mobile Agents", "comments": null, "journal-ref": null, "doi": "10.1007/s00453-014-9939-8", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of identical, mobile agents is deployed in a weighted network. Each\nagent has a battery -- a power source allowing it to move along network edges.\nAn agent uses its battery proportionally to the distance traveled. We consider\ntwo tasks : convergecast, in which at the beginning, each agent has some\ninitial piece of information, and information of all agents has to be collected\nby some agent; and broadcast in which information of one specified agent has to\nbe made available to all other agents. In both tasks, the agents exchange the\ncurrently possessed information when they meet. The objective of this paper is\nto investigate what is the minimal value of power, initially available to all\nagents, so that convergecast or broadcast can be achieved. We study this\nquestion in the centralized and the distributed settings. In the centralized\nsetting, there is a central monitor that schedules the moves of all agents. In\nthe distributed setting every agent has to perform an algorithm being unaware\nof the network. In the centralized setting, we give a linear-time algorithm to\ncompute the optimal battery power and the strategy using it, both for\nconvergecast and for broadcast, when agents are on the line. We also show that\nfinding the optimal battery power for convergecast or for broadcast is NP-hard\nfor the class of trees. On the other hand, we give a polynomial algorithm that\nfinds a 2-approximation for convergecast and a 4-approximation for broadcast,\nfor arbitrary graphs. In the distributed setting, we give a 2-competitive\nalgorithm for convergecast in trees and a 4-competitive algorithm for broadcast\nin trees. The competitive ratio of 2 is proved to be the best for the problem\nof convergecast, even if we only consider line networks. Indeed, we show that\nthere is no (2 -- $\\epsilon$)-competitive algorithm for convergecast or for\nbroadcast in the class of lines, for any $\\epsilon$ \\textgreater{} 0.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 12:23:10 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Anaya", "Julian", "", "DII"], ["Chalopin", "J\u00e9r\u00e9mie", "", "LIF"], ["Czyzowicz", "Jurek", "", "DII"], ["Labourel", "Arnaud", "", "LBI2M"], ["Pelc", "Andrzej", "", "DII"], ["Vax\u00e8s", "Yann", "", "LIF"]]}, {"id": "1603.04394", "submitter": "Evgenia Christoforou", "authors": "Evgenia Christoforou, Antonio Fern\\'andez Anta, Chryssis Georgiou and\n  Miguel A. Mosteiro", "title": "Internet Computing: Using Reputation to Select Workers from a Pool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment and execution of tasks over the Internet is an inexpensive\nsolution in contrast with supercomputers. We consider an Internet-based\nMaster-Worker task computing approach, such as SETI@home. A master process\nsends tasks, across the Internet, to worker processors. Workers execute, and\nreport back a result. Unfortunately, the disadvantage of this approach is the\nunreliable nature of the worker processes. Through different studies, workers\nhave been categorized as either malicious (always report an incorrect result),\naltruistic (always report a correct result), or rational (report whatever\nresult maximizes their benefit). We develop a reputation-based mechanism that\nguarantees that, eventually, the master will always be receiving the correct\ntask result. We model the behavior of the rational workers through\nreinforcement learning, and we present three different reputation types to\nchoose, for each computational round, the most reputable from a pool of\nworkers. As workers are not always available, we enhance our reputation scheme\nto select the most responsive workers. We prove sufficient conditions for\neventual correctness under the different reputation types. Our analysis is\ncomplemented by simulations exploring various scenarios. Our simulation results\nexpose interesting trade-offs among the different reputation types, workers\navailability, and cost.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 19:07:53 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 09:09:13 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Christoforou", "Evgenia", ""], ["Anta", "Antonio Fern\u00e1ndez", ""], ["Georgiou", "Chryssis", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "1603.04467", "submitter": "Ian Goodfellow", "authors": "Mart\\'in Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng\n  Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,\n  Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,\n  Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh\n  Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\n  Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar,\n  Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol\n  Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang\n  Zheng", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed\n  Systems", "comments": "Version 2 updates only the metadata, to correct the formatting of\n  Mart\\'in Abadi's name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow is an interface for expressing machine learning algorithms, and an\nimplementation for executing such algorithms. A computation expressed using\nTensorFlow can be executed with little or no change on a wide variety of\nheterogeneous systems, ranging from mobile devices such as phones and tablets\nup to large-scale distributed systems of hundreds of machines and thousands of\ncomputational devices such as GPU cards. The system is flexible and can be used\nto express a wide variety of algorithms, including training and inference\nalgorithms for deep neural network models, and it has been used for conducting\nresearch and for deploying machine learning systems into production across more\nthan a dozen areas of computer science and other fields, including speech\nrecognition, computer vision, robotics, information retrieval, natural language\nprocessing, geographic information extraction, and computational drug\ndiscovery. This paper describes the TensorFlow interface and an implementation\nof that interface that we have built at Google. The TensorFlow API and a\nreference implementation were released as an open-source package under the\nApache 2.0 license in November, 2015 and are available at www.tensorflow.org.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 20:50:20 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 16:57:12 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Abadi", "Mart\u00edn", ""], ["Agarwal", "Ashish", ""], ["Barham", "Paul", ""], ["Brevdo", "Eugene", ""], ["Chen", "Zhifeng", ""], ["Citro", "Craig", ""], ["Corrado", "Greg S.", ""], ["Davis", "Andy", ""], ["Dean", "Jeffrey", ""], ["Devin", "Matthieu", ""], ["Ghemawat", "Sanjay", ""], ["Goodfellow", "Ian", ""], ["Harp", "Andrew", ""], ["Irving", "Geoffrey", ""], ["Isard", "Michael", ""], ["Jia", "Yangqing", ""], ["Jozefowicz", "Rafal", ""], ["Kaiser", "Lukasz", ""], ["Kudlur", "Manjunath", ""], ["Levenberg", "Josh", ""], ["Mane", "Dan", ""], ["Monga", "Rajat", ""], ["Moore", "Sherry", ""], ["Murray", "Derek", ""], ["Olah", "Chris", ""], ["Schuster", "Mike", ""], ["Shlens", "Jonathon", ""], ["Steiner", "Benoit", ""], ["Sutskever", "Ilya", ""], ["Talwar", "Kunal", ""], ["Tucker", "Paul", ""], ["Vanhoucke", "Vincent", ""], ["Vasudevan", "Vijay", ""], ["Viegas", "Fernanda", ""], ["Vinyals", "Oriol", ""], ["Warden", "Pete", ""], ["Wattenberg", "Martin", ""], ["Wicke", "Martin", ""], ["Yu", "Yuan", ""], ["Zheng", "Xiaoqiang", ""]]}, {"id": "1603.04891", "submitter": "Robert Kl\\\"ofkorn", "authors": "Martin Alk\\\"amper and Robert Kl\\\"ofkorn", "title": "Distributed Newest Vertex Bisection", "comments": "18 pages, 13 pictures, 4 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed adaptive conforming refinement requires multiple iterations of\nthe serial refinement algorithm and global communication as the refinement can\nbe propagated over several processor boundaries. We show bounds on the maximum\nnumber of iterations. The algorithm is implemented within the software package\nDUNE-ALUGrid.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 21:36:44 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Alk\u00e4mper", "Martin", ""], ["Kl\u00f6fkorn", "Robert", ""]]}, {"id": "1603.05163", "submitter": "Yan Wang", "authors": "Yan Wang and Xunrui Yin and Dongsheng Wei and Xin Wang and Yucheng He", "title": "Accelerating Data Regeneration for Distributed Storage Systems with\n  Heterogeneous Link Capacities", "comments": "submitted to Trans. IT in Feb. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems provide large-scale reliable data storage\nservices by spreading redundancy across a large group of storage nodes. In such\na large system, node failures take place on a regular basis. When a storage\nnode breaks down, a replacement node is expected to regenerate the redundant\ndata as soon as possible in order to maintain the same level of redundancy.\nPrevious results have been mainly focused on the minimization of network\ntraffic in regeneration. However, in practical networks, where link capacities\nvary in a wide range, minimizing network traffic does not always yield the\nminimum regeneration time. In this paper, we investigate two approaches to the\nproblem of minimizing regeneration time in networks with heterogeneous link\ncapacities. The first approach is to download different amounts of repair data\nfrom the helping nodes according to the link capacities. The second approach\ngeneralizes the conventional star-structured regeneration topology to\ntree-structured topologies so that we can utilize the links between helping\nnodes with bypassing low-capacity links. Simulation results show that the\nflexible tree-structured regeneration scheme that combines the advantages of\nboth approaches can achieve a substantial reduction in the regeneration time.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 16:11:35 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Wang", "Yan", ""], ["Yin", "Xunrui", ""], ["Wei", "Dongsheng", ""], ["Wang", "Xin", ""], ["He", "Yucheng", ""]]}, {"id": "1603.05299", "submitter": "Junfeng Wang", "authors": "Junfeng Wang, Mark S. Miesch, Chunlei Liang", "title": "Convection in Oblate Solar-Type Stars", "comments": null, "journal-ref": null, "doi": "10.3847/0004-637X/830/1/45", "report-no": null, "categories": "astro-ph.SR cs.DC physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first global 3D simulations of thermal convection in the\noblate envelopes of rapidly-rotating solar-type stars. This has been achieved\nby exploiting the capabilities of the new Compressible High-ORder Unstructured\nSpectral difference (CHORUS) code. We consider rotation rates up to 85\\% of the\ncritical (breakup) rotation rate, which yields an equatorial radius that is up\nto 17\\% larger than the polar radius. This substantial oblateness enhances the\ndisparity between polar and equatorial modes of convection. We find that the\nconvection redistributes the heat flux emitted from the outer surface, leading\nto an enhancement of the heat flux in the polar and equatorial regions. This\nfinding implies that lower-mass stars with convective envelopes may not have\ndarker equators as predicted by classical gravity darkening arguments. The\nvigorous high-latitude convection also establishes elongated axisymmetric\ncirculation cells and zonal jets in the polar regions. Though the overall\namplitude of the surface differential rotation, $\\Delta \\Omega$, is insensitive\nto the oblateness, the oblateness does limit the fractional kinetic energy\ncontained in the differential rotation to no more than 61\\%. Furthermore, we\nargue that this level of differential rotation is not enough to have a\nsignificant impact on the oblateness of the star.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 22:20:14 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 22:02:32 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Wang", "Junfeng", ""], ["Miesch", "Mark S.", ""], ["Liang", "Chunlei", ""]]}, {"id": "1603.05459", "submitter": "Miguel Mosteiro", "authors": "Maitri Chakraborty and Alessia Milani and Miguel A. Mosteiro", "title": "Counting in Practical Anonymous Dynamic Networks is Polynomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anonymous Dynamic Networks is a harsh computational environment due to\nchanging topology and lack of identifiers. Computing the size of the network, a\nproblem known as Counting, is particularly challenging because messages\nreceived cannot be tagged to a specific sender. Previous works on Counting in\nAnonymous Dynamic Networks do not provide enough guarantees to be used in\npractice. Indeed, they either compute only an upper bound on the network size\nthat may be as bad as exponential, or guarantee only double-exponential running\ntime, or do not terminate, or guarantee only eventual termination without\nrunning-time guarantees. Faster experimental protocols do not guarantee the\ncorrect count. Recently, we presented the first Counting protocol that computes\nthe exact count with exponential running-time guarantees. The protocol requires\nthe presence of one leader node and knowledge of any upper bound Delta on the\nmaximum number of neighbors that any node will ever have. In the present work,\nwe complement the latter theoretical study evaluating the performance of such\nprotocol in practice. We tested a variety of network topologies that may appear\nin practice, including extremal cases such as trees, paths, and continuously\nchanging topologies. We also tested networks that temporarily are not\nconnected. Our simulations showed that the protocol is polynomial for all the\ninputs tested, paving the way to use it in practical applications where\ntopology changes are predictable. The simulations also provided insight on the\nimpact of topology changes on information dissemination. To the best of our\nknowledge, this is the first experimental study that shows the possibility of\ncomputing the exact count in polynomial time in a variety of Anonymous Dynamic\nNetworks that are worse than expected in practice.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 12:51:50 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Chakraborty", "Maitri", ""], ["Milani", "Alessia", ""], ["Mosteiro", "Miguel A.", ""]]}, {"id": "1603.05487", "submitter": "Piotr Ar{\\l}ukowicz", "authors": "Janusz Kowalik (1), Piotr Ar{\\l}ukowicz (1), Erika Parsons (2) ((1)\n  University of Gda\\'nsk, (2) University of Washington)", "title": "Speeding Up Computers", "comments": "This is not very technical article but a rather short overview of\n  quite new technology introduced by Intel. Intended audience are scientists\n  and engineers using parallel computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are two distinct approaches to speeding up large parallel computers.\nThe older method is the General Purpose Graphics Processing Units (GPGPU). The\nnewer is the Many Integrated Core (MIC) technology . Here we attempt to focus\non the MIC technology and point out differences between the two approaches to\naccelerating supercomputers. This is a user perspective.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 13:52:33 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Kowalik", "Janusz", ""], ["Ar\u0142ukowicz", "Piotr", ""], ["Parsons", "Erika", ""]]}, {"id": "1603.05544", "submitter": "Linnan Wang", "authors": "Linnan Wang, Yi Yang, Martin Renqiang Min, Srimat Chakradhar", "title": "Accelerating Deep Neural Network Training with Inconsistent Stochastic\n  Gradient Descent", "comments": "The patent of ISGD belongs to NEC Labs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SGD is the widely adopted method to train CNN. Conceptually it approximates\nthe population with a randomly sampled batch; then it evenly trains batches by\nconducting a gradient update on every batch in an epoch. In this paper, we\ndemonstrate Sampling Bias, Intrinsic Image Difference and Fixed Cycle Pseudo\nRandom Sampling differentiate batches in training, which then affect learning\nspeeds on them. Because of this, the unbiased treatment of batches involved in\nSGD creates improper load balancing. To address this issue, we present\nInconsistent Stochastic Gradient Descent (ISGD) to dynamically vary training\neffort according to learning statuses on batches. Specifically ISGD leverages\ntechniques in Statistical Process Control to identify a undertrained batch.\nOnce a batch is undertrained, ISGD solves a new subproblem, a chasing logic\nplus a conservative constraint, to accelerate the training on the batch while\navoid drastic parameter changes. Extensive experiments on a variety of datasets\ndemonstrate ISGD converges faster than SGD. In training AlexNet, ISGD is\n21.05\\% faster than SGD to reach 56\\% top1 accuracy under the exactly same\nexperiment setup. We also extend ISGD to work on multiGPU or heterogeneous\ndistributed system based on data parallelism, enabling the batch size to be the\nkey to scalability. Then we present the study of ISGD batch size to the\nlearning rate, parallelism, synchronization cost, system saturation and\nscalability. We conclude the optimal ISGD batch size is machine dependent.\nVarious experiments on a multiGPU system validate our claim. In particular,\nISGD trains AlexNet to 56.3% top1 and 80.1% top5 accuracy in 11.5 hours with 4\nNVIDIA TITAN X at the batch size of 1536.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 15:49:48 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 05:35:22 GMT"}, {"version": "v3", "created": "Tue, 28 Mar 2017 13:56:03 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Wang", "Linnan", ""], ["Yang", "Yi", ""], ["Min", "Martin Renqiang", ""], ["Chakradhar", "Srimat", ""]]}, {"id": "1603.05627", "submitter": "Grey Ballard", "authors": "Grey Ballard, Alex Druinsky, Nicholas Knight, Oded Schwartz", "title": "Hypergraph Partitioning for Sparse Matrix-Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fine-grained hypergraph model for sparse matrix-matrix\nmultiplication (SpGEMM), a key computational kernel in scientific computing and\ndata analysis whose performance is often communication bound. This model\ncorrectly describes both the interprocessor communication volume along a\ncritical path in a parallel computation and also the volume of data moving\nthrough the memory hierarchy in a sequential computation. We show that\nidentifying a communication-optimal algorithm for particular input matrices is\nequivalent to solving a hypergraph partitioning problem. Our approach is\nsparsity dependent, meaning that we seek the best algorithm for the given input\nmatrices.\n  In addition to our (3D) fine-grained model, we also propose coarse-grained 1D\nand 2D models that correspond to simpler SpGEMM algorithms. We explore the\nrelations between our models theoretically, and we study their performance\nexperimentally in the context of three applications that use SpGEMM as a key\ncomputation. For each application, we find that at least one coarse-grained\nmodel is as communication efficient as the fine-grained model. We also observe\nthat different applications have affinities for different algorithms.\n  Our results demonstrate that hypergraphs are an accurate model for reasoning\nabout the communication costs of SpGEMM as well as a practical tool for\nexploring the SpGEMM algorithm design space.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 19:23:59 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Ballard", "Grey", ""], ["Druinsky", "Alex", ""], ["Knight", "Nicholas", ""], ["Schwartz", "Oded", ""]]}, {"id": "1603.05746", "submitter": "Yong Zhan", "authors": "Yong Zhan, Mahdi Ghamkhari, Du Xu, Shaolei Ren and Hamed Mohsenian-Rad", "title": "Extending Demand Response to Tenants in Cloud Data Centers via\n  Non-intrusive Workload Flexibility Pricing", "comments": null, "journal-ref": null, "doi": "10.1109/TSG.2016.2628886", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participating in demand response programs is a promising tool for reducing\nenergy costs in data centers by modulating energy consumption. Towards this\nend, data centers can employ a rich set of resource management knobs, such as\nworkload shifting and dynamic server provisioning. Nonetheless, these knobs may\nnot be readily available in a cloud data center (CDC) that serves cloud\ntenants/users, because workloads in CDCs are managed by tenants themselves who\nare typically charged based on a usage-based or flat-rate pricing and often\nhave no incentive to cooperate with the CDC operator for demand response and\ncost saving. Towards breaking such \"split incentive\" hurdle, a few recent\nstudies have tried market-based mechanisms, such as dynamic pricing, inside\nCDCs. However, such mechanisms often rely on complex designs that are hard to\nimplement and difficult to cope with by tenants. To address this limitation, we\npropose a novel incentive mechanism that is not dynamic, i.e., it keeps pricing\nfor cloud resources unchanged for a long period. While it charges tenants based\non a Usage-based Pricing (UP) as used by today's major cloud operators, it\nrewards tenants proportionally based on the time length that tenants set as\ndeadlines for completing their workloads. This new mechanism is called\nUsage-based Pricing with Monetary Reward (UPMR). We demonstrate the\neffectiveness of UPMR both analytically and empirically. We show that UPMR can\nreduce the CDC operator's energy cost by 12.9% while increasing its profit by\n4.9%, compared to the state-of-the-art approaches used by today's CDC operators\nto charge their tenants.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 01:53:10 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Zhan", "Yong", ""], ["Ghamkhari", "Mahdi", ""], ["Xu", "Du", ""], ["Ren", "Shaolei", ""], ["Mohsenian-Rad", "Hamed", ""]]}, {"id": "1603.05752", "submitter": "Yong Zhan", "authors": "Yong Zhan, Mahdi Ghamkhari, Hossein Akhavan-Hejazi, Du Xu and Hamed\n  Mohsenian-Rad", "title": "Optimal Response to Burstable Billing under Demand Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Burstable billing is widely adopted in practice, e.g., by colocation data\ncenter providers, to charge for their users, e.g., data centers, for data\ntransferring. However, there is still a lack of research on what the best way\nis for a user to manage its workload in response to burstable billing. To\novercome this shortcoming, we propose a novel method to optimally respond to\nburstable billing under demand uncertainty. First, we develop a tractable\nmathematical expression to calculate the 95th percentile usage of a user, who\nis charged by provider via burstable billing for bandwidth usage. This model is\nthen used to formulate a new bandwidth allocation problem to maximize the\nuser's surplus, i.e., its net utility minus cost. Additionally, we examine\ndifferent non-convex solution methods for the formulated stochastic\noptimization problem. We also extend our design to the case where a user can\nreceive service from multiple providers, who all employ burstable billing.\nUsing real-world workload traces, we show that our proposed method can reduce\nuser's bandwidth cost by 26% and increase its total surplus by 23%, compared to\nthe current practice of allocating bandwidth on-demand.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 02:23:59 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Zhan", "Yong", ""], ["Ghamkhari", "Mahdi", ""], ["Akhavan-Hejazi", "Hossein", ""], ["Xu", "Du", ""], ["Mohsenian-Rad", "Hamed", ""]]}, {"id": "1603.05922", "submitter": "Erick Elejalde", "authors": "Erick Elejalde and Jose Fuentes-Sep\\'ulveda and Leo Ferres", "title": "Empirical Evaluation of a Thread-Safe Dynamic Range Min-Max Tree using\n  HTM", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Succinct trees, such as wavelet trees and those based on, for instance, range\nMin-Max trees (RMMTs), are a family of practical data structures that store\ninformation close to their information-theoretic space lower bound. These\nstructures are often static; meaning that once they are built, nodes cannot be\nadded, deleted or modified. This read-only property simplifies concurrency.\nHowever, newer versions of these data structures allow for a fair degree of\ndynamism. Parallel programming using Hardware Transactional Memory(HTM), has\nbeen available in mainstream microprocessors since a few years ago. One\nlimitation of HTM is still on the size of each transaction. This is why HTM's\nuse, for the moment, is limited to operations that involve few memory addresses\nthat need to be updated atomically, or where the level of concurrency is low.\nWe provide the first available implementation of a concurrent, dynamic RMMT\nbased on HTM, and we compare empirically how well HTM performs compared to a\nnaive implementation using locks. We have shown that because of the formal\nproperties of RMMTs, HTM is a good fit for adding concurrency to otherwise slow\nlock-based alternatives. We have also shown that HTM performs better than locks\nwhen the number of write operations increase, making it a practical structure\nto use in several write-intensive contexts. This is, as far as we know, the\nonly practical implementation of RMMTs thoroughly tested using HTM.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 16:55:16 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Elejalde", "Erick", ""], ["Fuentes-Sep\u00falveda", "Jose", ""], ["Ferres", "Leo", ""]]}, {"id": "1603.05939", "submitter": "Elli Zavou", "authors": "Elli Zavou, Antonio Fern\\'andez Anta", "title": "Online Distributed Scheduling on a Fault-prone Parallel System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a parallel system of $m$ identical machines prone to\nunpredictable crashes and restarts, trying to cope with the continuous arrival\nof tasks to be executed. Tasks have different computational requirements (i.e.,\nprocessing time or size). The flow of tasks, their size, and the crash and\nrestart of the machines are assumed to be controlled by an adversary. Then, we\nfocus on the study of online distributed algorithms for the efficient\nscheduling of the tasks. We use competitive analysis, considering as efficiency\nmetric the completed-load, i.e., the aggregated size of the completed tasks. We\nfirst present optimal completed-load competitiveness algorithms when the number\nof different task sizes that can be injected by the adversary is bounded. (It\nis known that, if it is not bounded, competitiveness is not achievable.) We\nfirst consider only two different task sizes, and then proceed to $k$ different\nones, showing in both cases that the optimal completed-load competitiveness can\nbe achieved. Then, we consider the possibility of having some form of resource\naugmentation, allowing the scheduling algorithm to run with a speedup $s \\geq\n1$. In this case, we show that the competitiveness of all work-conserving\nscheduling algorithms can be increased by using a large enough speedup.\n", "versions": [{"version": "v1", "created": "Fri, 18 Mar 2016 17:51:53 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Zavou", "Elli", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "1603.06241", "submitter": "Guillaume Maquart", "authors": "T. Maquart and G. Maquart", "title": "Homemade assembly and parameterization of a High Performance Cluster\n  using PelicanHPC with Flops testing and controlled temperature thanks to MCUs\n  Arduino project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows a lower cost realization of a compute cluster using Debian\ndistribution such as PelicanHPC. We will explain parameterization and network\nconfiguration for master and compute slave nodes. Performance testing will take\nplace using flops.f file given by MPI. The results will be compared between\ndifferents clusters. We will explain quickly how the temperature is controlled\nby a microcontroller unit.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 17:16:56 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Maquart", "T.", ""], ["Maquart", "G.", ""]]}, {"id": "1603.06346", "submitter": "Yatish Turakhia", "authors": "Yatish Turakhia, Guangshuo Liu, Siddharth Garg, Diana Marculescu", "title": "Thread Progress Equalization: Dynamically Adaptive Power and Performance\n  Optimization of Multi-threaded Applications", "comments": null, "journal-ref": null, "doi": "10.1109/TC.2016.2608951", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically adaptive multi-core architectures have been proposed as an\neffective solution to optimize performance for peak power constrained\nprocessors. In processors, the micro-architectural parameters or\nvoltage/frequency of each core to be changed at run-time, thus providing a\nrange of power/performance operating points for each core. In this paper, we\npropose Thread Progress Equalization (TPEq), a run-time mechanism for power\nconstrained performance maximization of multithreaded applications running on\ndynamically adaptive multicore processors. Compared to existing approaches,\nTPEq (i) identifies and addresses two primary sources of inter-thread\nheterogeneity in multithreaded applications, (ii) determines the optimal core\nconfigurations in polynomial time with respect to the number of cores and\nconfigurations, and (iii) requires no modifications in the user-level source\ncode. Our experimental evaluations demonstrate that TPEq outperforms\nstate-of-the-art run-time power/performance optimization techniques proposed in\nliterature for dynamically adaptive multicores by up to 23%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 07:53:07 GMT"}], "update_date": "2016-09-22", "authors_parsed": [["Turakhia", "Yatish", ""], ["Liu", "Guangshuo", ""], ["Garg", "Siddharth", ""], ["Marculescu", "Diana", ""]]}, {"id": "1603.06809", "submitter": "Jose Gracia", "authors": "Huan Zhou, Vladimir Marjanovic, Christoph Niethammer, Jos\\'e Gracia", "title": "A Bandwidth-saving Optimization for MPI Broadcast Collective Operation", "comments": "8 pages, accepted for publication in ICPP workshops 2015", "journal-ref": null, "doi": "10.1109/ICPPW.2015.20", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency and scalability of MPI collective operations, in particular\nthe broadcast operation, plays an integral part in high performance computing\napplications. MPICH, as one of the contemporary widely-used MPI software\nstacks, implements the broadcast operation based on point-to-point operation.\nDepending on the parameters, such as message size and process count, the\nlibrary chooses to use different algorithms, as for instance binomial\ndissemination, recursive-doubling exchange or ring all-to-all broadcast\n(allgather). However, the existing broadcast design in latest release of MPICH\ndoes not provide good performance for large messages (\\textit{lmsg}) or medium\nmessages with non-power-of-two process counts (\\textit{mmsg-npof2}) due to the\ninner suboptimal ring allgather algorithm. In this paper, based on the native\nbroadcast design in MPICH, we propose a tuned broadcast approach with\nbandwidth-saving in mind catering to the case of \\textit{lmsg} and\n\\textit{mmsg-npof2}. Several comparisons of the native and tuned broadcast\ndesigns are made for different data sizes and program sizes on Cray XC40\ncluster. The results show that the performance of the tuned broadcast design\ncan get improved by a range from 2\\% to 54\\% for \\textit{lmsg} and\n\\textit{mmsg-npof2} in terms of user-level testing.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 14:41:28 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Zhou", "Huan", ""], ["Marjanovic", "Vladimir", ""], ["Niethammer", "Christoph", ""], ["Gracia", "Jos\u00e9", ""]]}, {"id": "1603.07064", "submitter": "Saman Sarraf", "authors": "Saman Sarraf, Mehdi Ostadhashem", "title": "Big Data Spark Solution for Functional Magnetic Resonance Imaging", "comments": "4 pages, IEEE EMBS 2016 ORLANDO", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Big Data applications have rapidly expanded into different\nindustries. Healthcare is also one the industries willing to use big data\nplatforms so that some big data analytics tools have been adopted in this field\nto some extent. Medical imaging which is a pillar in diagnostic healthcare\ndeals with high volume of data collection and processing. A huge amount of 3D\nand 4D images are acquired in different forms and resolutions using a variety\nof medical imaging modalities. Preprocessing and analyzing imaging data is\ncurrently a long process and cost and time consuming. However, not many big\ndata platforms have been provided or redesigned for medical imaging purposes\nbecause of some restrictions such as data format. In this paper, we designed,\ndeveloped and successfully tested a new pipeline for medical imaging data\n(especially functional magnetic resonance imaging - fMRI) using Big Data Spark\n/ PySpark platform on a single node which allows us to read and load imaging\ndata, convert them to Resilient Distributed Datasets in order manipulate and\nperform in-memory data processing in parallel and convert final results to\nimaging format while the pipeline provides an option to store the results in\nother formats such as data frame. Using this new solution and pipeline, we\nrepeated our previous works in which we extracted brain networks from fMRI data\nusing template matching and sum of squared differences (SSD) method. The final\nresults revealed our Spark (PySpark) based solution improved the performance\n(in terms of processing time) around 4 times on a single compared to the\nprevious work developed in Python.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 03:42:44 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Sarraf", "Saman", ""], ["Ostadhashem", "Mehdi", ""]]}, {"id": "1603.07195", "submitter": "Mark Eisen", "authors": "Mark Eisen, Aryan Mokhtari, Alejandro Ribeiro", "title": "A Decentralized Quasi-Newton Method for Dual Formulations of Consensus\n  Optimization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers consensus optimization problems where each node of a\nnetwork has access to a different summand of an aggregate cost function. Nodes\ntry to minimize the aggregate cost function, while they exchange information\nonly with their neighbors. We modify the dual decomposition method to\nincorporate a curvature correction inspired by the\nBroyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method. The resulting dual\nD-BFGS method is a fully decentralized algorithm in which nodes approximate\ncurvature information of themselves and their neighbors through the\nsatisfaction of a secant condition. Dual D-BFGS is of interest in consensus\noptimization problems that are not well conditioned, making first order\ndecentralized methods ineffective, and in which second order information is not\nreadily available, making decentralized second order methods infeasible.\nAsynchronous implementation is discussed and convergence of D-BFGS is\nestablished formally for both synchronous and asynchronous implementations.\nPerformance advantages relative to alternative decentralized algorithms are\nshown numerically.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 14:24:39 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Eisen", "Mark", ""], ["Mokhtari", "Aryan", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1603.07322", "submitter": "Yin Sun", "authors": "Yin Sun, C. Emre Koksal, and Ness B. Shroff", "title": "On Delay-Optimal Scheduling in Queueing Systems with Replications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.IT cs.NI math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern computer systems, jobs are divided into short tasks and executed in\nparallel. Empirical observations in practical systems suggest that the task\nservice times are highly random and the job service time is bottlenecked by the\nslowest straggling task. One common solution for straggler mitigation is to\nreplicate a task on multiple servers and wait for one replica of the task to\nfinish early. The delay performance of replications depends heavily on the\nscheduling decisions of when to replicate, which servers to replicate on, and\nwhich job to serve first. So far, little is understood on how to optimize these\nscheduling decisions for minimizing the delay to complete the jobs. In this\npaper, we present a comprehensive study on delay-optimal scheduling of\nreplications in both centralized and distributed multi-server systems.\nLow-complexity scheduling policies are designed and are proven to be\ndelay-optimal or near delay-optimal in stochastic ordering among all causal and\nnon-preemptive policies. These theoretical results are established for general\nsystem settings and delay metrics that allow for arbitrary arrival processes,\narbitrary job sizes, arbitrary due times, and heterogeneous servers with data\nlocality constraints. Novel sample-path tools are developed to prove these\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 19:55:10 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 17:25:40 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 06:35:32 GMT"}, {"version": "v4", "created": "Thu, 12 May 2016 06:17:50 GMT"}, {"version": "v5", "created": "Thu, 26 Jan 2017 05:21:33 GMT"}, {"version": "v6", "created": "Sat, 28 Jan 2017 19:44:11 GMT"}, {"version": "v7", "created": "Thu, 2 Feb 2017 21:10:07 GMT"}, {"version": "v8", "created": "Mon, 6 Feb 2017 21:38:14 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Sun", "Yin", ""], ["Koksal", "C. Emre", ""], ["Shroff", "Ness B.", ""]]}, {"id": "1603.07351", "submitter": "Christian Cachin", "authors": "Christian Cachin, Simon Schubert, Marko Vukoli\\'c", "title": "Non-determinism in Byzantine Fault-Tolerant Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service replication distributes an application over many processes for\ntolerating faults, attacks, and misbehavior among a subset of the processes.\nThe established state-machine replication paradigm inherently requires the\napplication to be deterministic. This paper distinguishes three models for\ndealing with non-determinism in replicated services, where some processes are\nsubject to faults and arbitrary behavior (so-called Byzantine faults): first, a\nmodular approach that does not require any changes to the potentially\nnon-deterministic application (and neither access to its internal data);\nsecond, a master-slave approach, in which ties are broken by a leader and the\nother processes validate the choices of the leader; and finally, a treatment of\napplications that use cryptography and secret keys. Cryptographic operations\nand secrets must be treated specially because they require strong randomness to\nsatisfy their goals.\n  The paper also introduces two new protocols. The first uses the modular\napproach for filtering out non-de\\-ter\\-min\\-istic operations in an\napplication. It ensures that all correct processes produce the same outputs and\nthat their internal states do not diverge. The second protocol implements\ncryptographically secure randomness generation with a verifiable random\nfunction and is appropriate for certain security models. All protocols are\ndescribed in a generic way and do not assume a particular implementation of the\nunderlying consensus primitive.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:42:55 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 17:08:40 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Cachin", "Christian", ""], ["Schubert", "Simon", ""], ["Vukoli\u0107", "Marko", ""]]}, {"id": "1603.07357", "submitter": "Blesson Varghese", "authors": "Blesson Varghese, Lawan Thamsuhang Subba, Long Thai, Adam Barker", "title": "DocLite: A Docker-Based Lightweight Cloud Benchmarking Tool", "comments": "16th IEEE/ACM International Symposium on Cluster, Cloud and Grid\n  Computing (CCGrid), 2016, Cartagena, Colombia. arXiv admin note: substantial\n  text overlap with arXiv:1601.03872", "journal-ref": null, "doi": "10.1109/CCGrid.2016.14", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing benchmarking methods are time consuming processes as they typically\nbenchmark the entire Virtual Machine (VM) in order to generate accurate\nperformance data, making them less suitable for real-time analytics. The\nresearch in this paper is aimed to surmount the above challenge by presenting\nDocLite - Docker Container-based Lightweight benchmarking tool. DocLite\nexplores lightweight cloud benchmarking methods for rapidly executing\nbenchmarks in near real-time. DocLite is built on the Docker container\ntechnology, which allows a user-defined memory size and number of CPU cores of\nthe VM to be benchmarked. The tool incorporates two benchmarking methods - the\nfirst referred to as the native method employs containers to benchmark a small\nportion of the VM and generate performance ranks, and the second uses historic\nbenchmark data along with the native method as a hybrid to generate VM ranks.\nThe proposed methods are evaluated on three use-cases and are observed to be up\nto 91 times faster than benchmarking the entire VM. In both methods, small\ncontainers provide the same quality of rankings as a large container. The\nnative method generates ranks with over 90% and 86% accuracy for sequential and\nparallel execution of an application compared against benchmarking the whole\nVM. The hybrid method did not improve the quality of the rankings\nsignificantly.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 20:55:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Varghese", "Blesson", ""], ["Subba", "Lawan Thamsuhang", ""], ["Thai", "Long", ""], ["Barker", "Adam", ""]]}, {"id": "1603.07400", "submitter": "Raqibul Hasan", "authors": "Raqibul Hasan, and Tarek Taha", "title": "A Reconfigurable Low Power High Throughput Architecture for Deep Network\n  Training", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General purpose computing systems are used for a large variety of\napplications. Extensive supports for flexibility in these systems limit their\nenergy efficiencies. Neural networks, including deep networks, are widely used\nfor signal processing and pattern recognition applications. In this paper we\npropose a multicore architecture for deep neural network based processing.\nMemristor crossbars are utilized to provide low power high throughput execution\nof neural networks. The system has both training and recognition (evaluation of\nnew input) capabilities. The proposed system could be used for classification,\ndimensionality reduction, feature extraction, and anomaly detection\napplications. The system level area and power benefits of the specialized\narchitecture is compared with the NVIDIA Telsa K20 GPGPU. Our experimental\nevaluations show that the proposed architecture can provide up to five orders\nof magnitude more energy efficiency over GPGPUs for deep neural network\nprocessing.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 00:52:22 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 01:26:31 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Hasan", "Raqibul", ""], ["Taha", "Tarek", ""]]}, {"id": "1603.07846", "submitter": "Wei Wang", "authors": "Wei Wang, Gang Chen, Haibo Chen, Tien Tuan Anh Dinh, Jinyang Gao, Beng\n  Chin Ooi, Kian-Lee Tan and Sheng Wang", "title": "Deep Learning At Scale and At Ease", "comments": "submitted to TOMM (under review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning techniques have enjoyed success in various multimedia\napplications, such as image classification and multi-modal data analysis. Large\ndeep learning models are developed for learning rich representations of complex\ndata. There are two challenges to overcome before deep learning can be widely\nadopted in multimedia and other applications. One is usability, namely the\nimplementation of different models and training algorithms must be done by\nnon-experts without much effort especially when the model is large and complex.\nThe other is scalability, that is the deep learning system must be able to\nprovision for a huge demand of computing resources for training large models\nwith massive datasets. To address these two challenges, in this paper, we\ndesign a distributed deep learning platform called SINGA which has an intuitive\nprogramming model based on the common layer abstraction of deep learning\nmodels. Good scalability is achieved through flexible distributed training\narchitecture and specific optimization techniques. SINGA runs on GPUs as well\nas on CPUs, and we show that it outperforms many other state-of-the-art deep\nlearning systems. Our experience with developing and training deep learning\nmodels for real-life multimedia applications in SINGA shows that the platform\nis both usable and scalable.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 08:46:02 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Wang", "Wei", ""], ["Chen", "Gang", ""], ["Chen", "Haibo", ""], ["Dinh", "Tien Tuan Anh", ""], ["Gao", "Jinyang", ""], ["Ooi", "Beng Chin", ""], ["Tan", "Kian-Lee", ""], ["Wang", "Sheng", ""]]}, {"id": "1603.07899", "submitter": "Konrad Siek", "authors": "Jan Baranowski, Pawe{\\l} Kobyli\\'nski, Konrad Siek, Pawe{\\l} T.\n  Wojciechowski", "title": "Helenos: A Realistic Benchmark for Distributed Transactional Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional Memory (TM) is an approach to concurrency control that aims to\nmake writing parallel programs both effective and simple. The approach is\nstarted in non-distributed multiprocessor systems, but is gaining popularity in\ndistributed systems to synchronize tasks at large scales. Efficiency and\nscalability are often the key issues in TM research, so performance benchmarks\nare an important part of it. However, while standard TM benchmarks like the\nSTAMP suite and STMBench7 are available and widely accepted, they do not\ntranslate well into distributed systems. Hence, the set of benchmarks usable\nwith distributed TM systems is very limited, and must be padded with\nmicrobenchmarks, whose simplicity and artificial nature often makes them\nuninformative or misleading. Therefore, this paper introduces Helenos, a\nrealistic, complex, and comprehensive distributed TM benchmark based on the\nproblem of the Facebook inbox, an application of the Cassandra distributed\nstore.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 13:14:23 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2016 12:00:49 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Baranowski", "Jan", ""], ["Kobyli\u0144ski", "Pawe\u0142", ""], ["Siek", "Konrad", ""], ["Wojciechowski", "Pawe\u0142 T.", ""]]}, {"id": "1603.07936", "submitter": "Subhajit Sidhanta Subhajit Sidhanta", "authors": "Subhajit Sidhanta, Wojciech Golab, and Supratik Mukhopadhyay", "title": "OptEx: A Deadline-Aware Cost Optimization Model for Spark", "comments": "10 pages, IEEE CCGrid 2016", "journal-ref": null, "doi": "10.1109/CCGrid.2016.10", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OptEx, a closed-form model of job execution on Apache Spark, a\npopular parallel processing engine. To the best of our knowledge, OptEx is the\nfirst work that analytically models job completion time on Spark. The model can\nbe used to estimate the completion time of a given Spark job on a cloud, with\nrespect to the size of the input dataset, the number of iterations, the number\nof nodes comprising the underlying cluster. Experimental results demonstrate\nthat OptEx yields a mean relative error of 6% in estimating the job completion\ntime. Furthermore, the model can be applied for estimating the cost optimal\ncluster composition for running a given Spark job on a cloud under a completion\ndeadline specified in the SLO (i.e., Service Level Objective). We show\nexperimentally that OptEx is able to correctly estimate the cost optimal\ncluster composition for running a given Spark job under an SLO deadline with an\naccuracy of 98%.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 15:28:56 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sidhanta", "Subhajit", ""], ["Golab", "Wojciech", ""], ["Mukhopadhyay", "Supratik", ""]]}, {"id": "1603.07938", "submitter": "Subhajit Sidhanta Subhajit Sidhanta", "authors": "Subhajit Sidhanta, Wojciech Golab, Supratik Mukhopadhyay, and Saikat\n  Basu", "title": "OptCon: An Adaptable SLA-Aware Consistency Tuning Framework for\n  Quorum-based Stores", "comments": "10 pages, IEEE CCGrid 2016", "journal-ref": null, "doi": "10.1109/CCGrid.2016.9", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users of distributed datastores that employ quorum-based replication are\nburdened with the choice of a suitable client-centric consistency setting for\neach storage operation. The above matching choice is difficult to reason about\nas it requires deliberating about the tradeoff between the latency and\nstaleness, i.e., how stale (old) the result is. The latency and staleness for a\ngiven operation depend on the client-centric consistency setting applied, as\nwell as dynamic parameters such as the current workload and network\ncondition.We present OptCon, a novel machine learning-based predictive\nframework, that can automate the choice of client-centric consistency setting\nunder user-specified latency and staleness thresholds given in the service\nlevel agreement (SLA). Under a given SLA, OptCon predicts a client-centric\nconsistency setting that is matching, i.e., it is weak enough to satisfy the\nlatency threshold, while being strong enough to satisfy the staleness\nthreshold. While manually tuned consistency settings remain fixed unless\nexplicitly reconfigured, OptCon tunes consistency settings on a per-operation\nbasis with respect to changing workload and network state. Using decision tree\nlearning, OptCon yields 0.14 cross validation error in predicting matching\nconsistency settings under latency and staleness thresholds given in the SLA.\nWe demonstrate experimentally that OptCon is at least as effective as any\nmanually chosen consistency settings in adapting to the SLA thresholds for\ndifferent use cases. We also demonstrate that OptCon adapts to variations in\nworkload, whereas a given manually chosen fixed consistency setting satisfies\nthe SLA only for a characteristic workload.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 15:29:04 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sidhanta", "Subhajit", ""], ["Golab", "Wojciech", ""], ["Mukhopadhyay", "Supratik", ""], ["Basu", "Saikat", ""]]}, {"id": "1603.07991", "submitter": "Joshua Daymude", "authors": "Sarah Cannon, Joshua J. Daymude, Dana Randall, Andr\\'ea W. Richa", "title": "A Markov Chain Algorithm for Compression in Self-Organizing Particle\n  Systems", "comments": null, "journal-ref": "PODC '16: Proceedings of the 2016 ACM Symposium on Principles of\n  Distributed Computing, pp. 279-288", "doi": "10.1145/2933057.2933107", "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems of programmable matter, we are given a collection of simple\ncomputation elements (or particles) with limited (constant-size) memory. We are\ninterested in when they can self-organize to solve system-wide problems of\nmovement, configuration and coordination. Here, we initiate a stochastic\napproach to developing robust distributed algorithms for programmable matter\nsystems using Markov chains. We are able to leverage the wealth of prior work\nin Markov chains and related areas to design and rigorously analyze our\ndistributed algorithms and show that they have several desirable properties.\n  We study the compression problem, in which a particle system must gather as\ntightly together as possible, as in a sphere or its equivalent in the presence\nof some underlying geometry. More specifically, we seek fully distributed,\nlocal, and asynchronous algorithms that lead the system to converge to a\nconfiguration with small boundary. We present a Markov chain-based algorithm\nthat solves the compression problem under the geometric amoebot model, for\nparticle systems that begin in a connected configuration. The algorithm takes\nas input a bias parameter $\\lambda$, where $\\lambda > 1$ corresponds to\nparticles favoring having more neighbors. We show that for all $\\lambda >\n2+\\sqrt{2}$, there is a constant $\\alpha > 1$ such that eventually with all but\nexponentially small probability the particles are $\\alpha$-compressed, meaning\nthe perimeter of the system configuration is at most $\\alpha \\cdot p_{min}$,\nwhere $p_{min}$ is the minimum possible perimeter of the particle system.\nSurprisingly, the same algorithm can also be used for expansion when $0 <\n\\lambda < 2.17$, and we prove similar results about expansion for values of\n$\\lambda$ in this range. This is counterintuitive as it shows that particles\npreferring to be next to each other ($\\lambda > 1$) is not sufficient to\nguarantee compression.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 19:36:24 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 14:30:25 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 14:07:03 GMT"}, {"version": "v4", "created": "Tue, 26 Feb 2019 00:13:06 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Cannon", "Sarah", ""], ["Daymude", "Joshua J.", ""], ["Randall", "Dana", ""], ["Richa", "Andr\u00e9a W.", ""]]}, {"id": "1603.08014", "submitter": "Jun Guo", "authors": "Jun Guo and Hamid Jafarkhani", "title": "Sensor Deployment with Limited Communication Range in Homogeneous and\n  Heterogeneous Wireless Sensor Networks", "comments": "28 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the heterogeneous wireless sensor networks (WSNs) and propose the\nnecessary condition of the optimal sensor deployment. Similar to that in\nhomogeneous WSNs, the necessary condition implies that every sensor node\nlocation should coincide with the centroid of its own optimal sensing region.\nMoreover, we discuss the dynamic sensor deployment in both homogeneous and\nheterogeneous WSNs with limited communication range for the sensor nodes. The\npurpose of sensor deployment is to improve sensing performance, reflected by\ndistortion and coverage. We model the sensor deployment problem as a source\ncoding problem with distortion reflecting sensing accuracy. Traditionally,\ncoverage is the area covered by the sensor nodes. However, when the\ncommunication range is limited, a WSN may be divided into several disconnected\nsub-graphs. Under such a scenario, neither the conventional distortion nor the\ncoverage represents the sensing performance as the collected data in\ndisconnected sub-graphs cannot be communicated with the access point. By\ndefining an appropriate distortion measure, we propose a Restrained Lloyd (RL)\nalgorithm and a Deterministic Annealing (DA) algorithm to optimize sensor\ndeployment in both homogeneous and heterogeneous WSNs. Our simulation results\nshow that both DA and RL algorithms outperform the existing Lloyd algorithm\nwhen communication range is limited.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 20:06:33 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Guo", "Jun", ""], ["Jafarkhani", "Hamid", ""]]}, {"id": "1603.08102", "submitter": "Mansaf Alam Dr", "authors": "Shweta Malhotra, Mohammad Najmud Doja, Bashir Alam, Mansaf Alam", "title": "GENMR: Generalized Query Processing through Map Reduce In Cloud Database\n  Management System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data, Cloud computing, Cloud Database Management techniques, Data Science\nand many more are the fantasizing words which are the future of IT industry.\nFor all the new techniques one common thing is that they deal with Data, not\njust Data but the Big Data. Users store their various kinds of data on cloud\nrepositories. Cloud Database Management System deals with such large sets of\ndata. For processing such gigantic amount of data, traditional approaches are\nnot suitable because these approaches are not able to handle such size of data.\nTo handle these, various solutions have been developed such as Hadoop, Map\nReduce Programming codes, HIVE, PIG etc. Map Reduce codes provides both\nscalability and reliability. But till date, users are habitual of SQL, Oracle\nkind of codes for dealing with data and they are not aware of Map Reduce codes.\nIn this paper, a generalized model GENMR has been implemented, which takes\nqueries written in various RDBMS forms like SQL, ORACLE, DB2, MYSQL and convert\ninto Map Reduce codes. A comparison has been done to evaluate the performance\nof GENMR with latest techniques like HIVE and PIG and it has been concluded\nthat GENMR shows much better performance as compare to both the techniques. We\nalso introduce an optimization technique for mapper placement problems to\nenhance the effect of parallelism which improves the performance of such\nAmalgam approach.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 11:43:41 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Malhotra", "Shweta", ""], ["Doja", "Mohammad Najmud", ""], ["Alam", "Bashir", ""], ["Alam", "Mansaf", ""]]}, {"id": "1603.08297", "submitter": "Evgeny Nikulchev", "authors": "M. Aubakirov, E. Nikulchev", "title": "Development of System Architecture for E-Government Cloud Platforms", "comments": "6 pages", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications 7 (2016) 253-258", "doi": "10.14569/IJACSA.2016.070235", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Requirements and criteria for selection of cloud platform and platform\nvisualization are stated by which optimal cloud products will be chosen for the\nRepublic of Kazakhstan e-Government considering quality-price ratio, and also\nthe framework of information and communication architecture will be introduced.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 20:06:37 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Aubakirov", "M.", ""], ["Nikulchev", "E.", ""]]}, {"id": "1603.08390", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Qi Guo, H. V. Jagadish, Lubo\\v{s} Kr\\v{c}\\'al, Siyuan\n  Liu, Wenhao Luan, Anthony K. H. Tung, Yueji Yang, Yuxin Zheng", "title": "A Generic Inverted Index Framework for Similarity Search on the GPU -\n  Technical Report", "comments": "18 pages, technical report for the ICDE 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel generic inverted index framework on the GPU (called\nGENIE), aiming to reduce the programming complexity of the GPU for parallel\nsimilarity search of different data types. Not every data type and similarity\nmeasure are supported by GENIE, but many popular ones are. We present the\nsystem design of GENIE, and demonstrate similarity search with GENIE on several\ndata types along with a theoretical analysis of search results. A new concept\nof locality sensitive hashing (LSH) named $\\tau$-ANN search, and a novel data\nstructure c-PQ on the GPU are also proposed for achieving this purpose.\nExtensive experiments on different real-life datasets demonstrate the\nefficiency and effectiveness of our framework. The implemented system has been\nreleased as open source.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:44:34 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 06:05:25 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 08:49:16 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Zhou", "Jingbo", ""], ["Guo", "Qi", ""], ["Jagadish", "H. V.", ""], ["Kr\u010d\u00e1l", "Lubo\u0161", ""], ["Liu", "Siyuan", ""], ["Luan", "Wenhao", ""], ["Tung", "Anthony K. H.", ""], ["Yang", "Yueji", ""], ["Zheng", "Yuxin", ""]]}, {"id": "1603.08393", "submitter": "Dimitris Sakavalas", "authors": "Sushanta Karmakar, Paraschos Koutris, Aris Pagourtzis, Dimitris\n  Sakavalas", "title": "$k$-shot Broadcasting in Ad Hoc Radio Networks", "comments": "21 pages, 2 figures, preliminary version presented in CATS 2011:\n  161-168", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributed broadcasting protocols with few transmissions (`shots')\nin radio networks where the topology is unknown. In particular, we examine the\ncase in which a bound $k$ is given and a node may transmit at most $k$ times\nduring the broadcasting protocol. Initially, we focus on oblivious algorithms\nfor $k$-shot broadcasting, that is, algorithms where each node decides whether\nto transmit or not with no consideration of the transmission history. Our main\ncontributions are (a) a lower bound of $\\Omega(n^2/k)$ on the broadcasting time\nof any oblivious $k$-shot broadcasting algorithm and (b) an oblivious\nbroadcasting protocol that achieves a matching upper bound, namely $O(n^2/k)$,\nfor every $k \\le \\sqrt{n}$ and an upper bound of $O(n^{3/2})$ for every $k >\n\\sqrt{n}$. We also study the general case of adaptive broadcasting protocols\nwhere nodes decide whether to transmit based on all the available information,\nnamely the transmission history known by each. We prove a lower bound of\n$\\Omega\\left(n^{\\frac{1+k}{k}}\\right)$ on the broadcasting time of any protocol\nby introducing the \\emph{transmission tree} construction which generalizes\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:55:10 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Karmakar", "Sushanta", ""], ["Koutris", "Paraschos", ""], ["Pagourtzis", "Aris", ""], ["Sakavalas", "Dimitris", ""]]}, {"id": "1603.08619", "submitter": "Jianbin Fang", "authors": "Zhaokui Li, Jianbin Fang, Tao Tang, Xuhao Chen, Cheng Chen and Canqun\n  Yang", "title": "Evaluating the Performance Impact of Multiple Streams on the MIC-based\n  Heterogeneous Platform", "comments": "accepted by the 2016 IPDPS workshop (LSPP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using \\textit{multiple streams} can improve the overall system performance by\nmitigating the data transfer overhead on heterogeneous systems. Prior work\nfocuses a lot on GPUs but little is known about the performance impact on\n(Intel Xeon) Phi. In this work, we apply multiple streams into six real-world\napplications on Phi. We then systematically evaluate the performance benefits\nof using multiple streams. The evaluation work is performed at two levels: the\nmicrobenchmarking level and the real-world application level. Our experimental\nresults at the microbenchmark level show that data transfers and kernel\nexecution can be overlapped on Phi, while data transfers in both directions are\nperformed in a serial manner. At the real-world application level, we show that\nboth overlappable and non-overlappable applications can benefit from using\nmultiple streams (with an performance improvement of up to 24\\%). We also\nquantify how task granularity and resource granularity impact the overall\nperformance. Finally, we present a set of heuristics to reduce the search space\nwhen determining a proper task granularity and resource granularity. To\nconclude, our evaluation work provides lots of insights for runtime and\narchitecture designers when using multiple streams on Phi.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 03:01:06 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Li", "Zhaokui", ""], ["Fang", "Jianbin", ""], ["Tang", "Tao", ""], ["Chen", "Xuhao", ""], ["Chen", "Cheng", ""], ["Yang", "Canqun", ""]]}, {"id": "1603.08634", "submitter": "EPTCS", "authors": "Luke Chircop, Christian Colombo, Gordon J. Pace", "title": "Device-Centric Monitoring for Mobile Device Management", "comments": "In Proceedings FESCA 2016, arXiv:1603.08371", "journal-ref": "EPTCS 205, 2016, pp. 31-44", "doi": "10.4204/EPTCS.205.3", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of computing devices has led to an increased need to ensure not\nonly that the applications deployed on them are correct with respect to their\nspecifications, but also that the devices are used in an appropriate manner,\nespecially in situations where the device is provided by a party other than the\nactual user. Much work which has been done on runtime verification for mobile\ndevices and operating systems is mostly application-centric, resulting in\nglobal, device-centric properties (e.g. the user may not send more than 100\nmessages per day across all applications) being difficult or impossible to\nverify. In this paper we present a device-centric approach to runtime verify\nthe device behaviour against a device policy with the different applications\nacting as independent components contributing to the overall behaviour of the\ndevice. We also present an implementation for Android devices, and evaluate it\non a number of device-centric policies, reporting the empirical results\nobtained.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:34:14 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Chircop", "Luke", ""], ["Colombo", "Christian", ""], ["Pace", "Gordon J.", ""]]}, {"id": "1603.08743", "submitter": "Dmitry N. Kozlov", "authors": "Dmitry N. Kozlov", "title": "All binomial identities are orderable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main result of this paper is to show that all binomial identities are\norderable. This is a natural statement in the combinatorial theory of finite\nsets, which can also be applied in distributed computing to derive new strong\nbounds on the round complexity of the weak symmetry breaking task.\n  Furthermore, we introduce the notion of a fundamental binomial identity and\nfind an infinite family of values, other than the prime powers, for which no\nfundamental binomial identity can exist.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 12:33:44 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 14:43:55 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 09:33:10 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Kozlov", "Dmitry N.", ""]]}, {"id": "1603.08767", "submitter": "Daniel Pop", "authors": "Daniel Pop", "title": "Machine Learning and Cloud Computing: Survey of Distributed and SaaS\n  Solutions", "comments": "This manuscript was originally published as IEAT Technical Report at\n  https://www.ieat.ro/technical-reports in 2012", "journal-ref": null, "doi": null, "report-no": "IEAT-TR-2012-1", "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Applying popular machine learning algorithms to large amounts of data raised\nnew challenges for the ML practitioners. Traditional ML libraries does not\nsupport well processing of huge datasets, so that new approaches were needed.\nParallelization using modern parallel computing frameworks, such as MapReduce,\nCUDA, or Dryad gained in popularity and acceptance, resulting in new ML\nlibraries developed on top of these frameworks. We will briefly introduce the\nmost prominent industrial and academic outcomes, such as Apache Mahout,\nGraphLab or Jubatus.\n  We will investigate how cloud computing paradigm impacted the field of ML.\nFirst direction is of popular statistics tools and libraries (R system, Python)\ndeployed in the cloud. A second line of products is augmenting existing tools\nwith plugins that allow users to create a Hadoop cluster in the cloud and run\njobs on it. Next on the list are libraries of distributed implementations for\nML algorithms, and on-premise deployments of complex systems for data analytics\nand data mining. Last approach on the radar of this survey is ML as\nSoftware-as-a-Service, several BigData start-ups (and large companies as well)\nalready opening their solutions to the market.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 13:29:35 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Pop", "Daniel", ""]]}, {"id": "1603.09035", "submitter": "Ignacio Cano", "authors": "Ignacio Cano, Markus Weimer, Dhruv Mahajan, Carlo Curino and Giovanni\n  Matteo Fumarola", "title": "Towards Geo-Distributed Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latency to end-users and regulatory requirements push large companies to\nbuild data centers all around the world. The resulting data is \"born\"\ngeographically distributed. On the other hand, many machine learning\napplications require a global view of such data in order to achieve the best\nresults. These types of applications form a new class of learning problems,\nwhich we call Geo-Distributed Machine Learning (GDML). Such applications need\nto cope with: 1) scarce and expensive cross-data center bandwidth, and 2)\ngrowing privacy concerns that are pushing for stricter data sovereignty\nregulations. Current solutions to learning from geo-distributed data sources\nrevolve around the idea of first centralizing the data in one data center, and\nthen training locally. As machine learning algorithms are\ncommunication-intensive, the cost of centralizing the data is thought to be\noffset by the lower cost of intra-data center communication during training. In\nthis work, we show that the current centralized practice can be far from\noptimal, and propose a system for doing geo-distributed training. Furthermore,\nwe argue that the geo-distributed approach is structurally more amenable to\ndealing with regulatory constraints, as raw data never leaves the source data\ncenter. Our empirical evaluation on three real datasets confirms the general\nvalidity of our approach, and shows that GDML is not only possible but also\nadvisable in many scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 04:05:29 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Cano", "Ignacio", ""], ["Weimer", "Markus", ""], ["Mahajan", "Dhruv", ""], ["Curino", "Carlo", ""], ["Fumarola", "Giovanni Matteo", ""]]}, {"id": "1603.09176", "submitter": "Ramzi Mahmoudi", "authors": "Ramzi Mahmoudi (LIGM), Mohamed Akil (LIGM)", "title": "Enhanced computation method of topological smoothing on shared memory\n  parallel machines", "comments": null, "journal-ref": "EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING, 2011", "doi": "10.1007/s10851-005-4891-5", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To prepare images for better segmentation, we need preprocessing\napplications, such as smoothing, to reduce noise. In this paper, we present an\nenhanced computation method for smoothing 2D object in binary case. Unlike\nexisting approaches, proposed method provides a parallel computation and better\nmemory management, while preserving the topology (number of connected\ncomponents) of the original image by using homotopic transformations defined in\nthe framework of digital topology. We introduce an adapted parallelization\nstrategy called split, distribute and merge (SDM) strategy which allows\nefficient parallelization of a large class of topological operators. To achieve\na good speedup and better memory allocation, we cared about task scheduling and\nmanaging. Distributed work during smoothing process is done by a variable\nnumber of threads. Tests on 2D grayscale image (512*512), using shared memory\nparallel machine (SMPM) with 8 CPU cores (2 Xeon E5405 running at frequency of\n2 GHz), showed an enhancement of 5.2 with cache success rate of 70%.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:21:03 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Mahmoudi", "Ramzi", "", "LIGM"], ["Akil", "Mohamed", "", "LIGM"]]}, {"id": "1603.09180", "submitter": "Ramzi Mahmoudi", "authors": "Ramzi Mahmoudi (LIGM), Mohamed Akil (LIGM), Petr Matas (LIGM)", "title": "Parallel image thinning through topological operators on shared memory\n  parallel machines", "comments": null, "journal-ref": null, "doi": "10.1109/ACSSC.2009.5469946", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a concurrent implementation of a powerful\ntopological thinning operator. This operator is able to act directly over\ngrayscale images without modifying their topology. We introduce an adapted\nparallelization methodology which combines split, distribute and merge (SDM)\nstrategy and mixed parallelism techniques (data and thread parallelism). The\nintroduced strategy allows efficient parallelization of a large class of\ntopological operators including, mainly, {\\lambda}-leveling, skeletonization\nand crest restoring algorithms. To achieve a good speedup, we cared about\ncoordination of threads. Distributed work during thinning process is done by a\nvariable number of threads. Tests on 2D grayscale image (512*512), using shared\nmemory parallel machine (SMPM) with 8 CPU cores (2x Xeon E5405 running at\nfrequency of 2 GHz), showed an enhancement of 6.2 with a maximum achieved\ncadency of 125 images/s using 8 threads.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:28:04 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Mahmoudi", "Ramzi", "", "LIGM"], ["Akil", "Mohamed", "", "LIGM"], ["Matas", "Petr", "", "LIGM"]]}, {"id": "1603.09337", "submitter": "Ramzi Mahmoudi", "authors": "Ramzi Mahmoudi (LIGM), Mohamed Akil (LIGM)", "title": "Real-time topological image smoothing on shared memory parallel machines", "comments": "arXiv admin note: substantial text overlap with arXiv:1603.09176", "journal-ref": null, "doi": "10.1117/12.872275", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing filter is the method of choice for image preprocessing and pattern\nrecognition. We present a new concurrent method for smoothing 2D object in\nbinary case. Proposed method provides a parallel computation while preserving\nthe topology by using homotopic transformations. We introduce an adapted\nparallelization strategy called split, distribute and merge (SDM) strategy\nwhich allows efficient parallelization of a large class of topological\noperators including, mainly, smoothing, skeletonization, and watershed\nalgorithms. To achieve a good speedup, we cared about task scheduling.\nDistributed work during smoothing process is done by a variable number of\nthreads. Tests on 2D binary image (512*512), using shared memory parallel\nmachine (SMPM) with 8 CPU cores (2 Xeon E5405 running at frequency of 2 GHz),\nshowed an enhancement of 5.2 thus a cadency of 32 images per second is\nachieved.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 13:26:10 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Mahmoudi", "Ramzi", "", "LIGM"], ["Akil", "Mohamed", "", "LIGM"]]}, {"id": "1603.09642", "submitter": "Andrey Demichev", "authors": "Stanislav P. Polyakov, Andrey P. Demichev and Alexander P. Kryukov", "title": "Web Toolkit for Scientific Research: State of the Art and the Prospect\n  for Development", "comments": "10 pages; 4th International Young Scientists Conference on\n  Computational Science (YSC 2015)", "journal-ref": "Procedia computer science, Volume 66, 2015, Pages 429 - 438", "doi": "10.1016/j.procs.2015.11.049", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper examines the current trends in designing of systems for convenient\nand secure remote job submission to various computer resources, including\nsupercomputers, computer clusters, cloud resources, data storages and\ndatabases, and grid infrastructures by authorized users, as well as remote job\nmonitoring and obtaining the results. Currently, high-perfomance computing and\nstorage resources are capable of solving independently the majority of\npractical problems in the field of science and technology. Therefore, the focus\nin the development of a new generation of middleware shifts from the global\ngrid systems to building convenient and efficient web platforms for remote\naccess to individual computing resources. The paper examines the general\nprinciples of the construction and briefly describes some of the specific\nimplementations of the web platforms.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 15:36:25 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Polyakov", "Stanislav P.", ""], ["Demichev", "Andrey P.", ""], ["Kryukov", "Alexander P.", ""]]}, {"id": "1603.09679", "submitter": "Colin Barrett", "authors": "Colin Barrett and Christos Kotselidis and Mikel Luj\\'an", "title": "Towards co-designed optimizations in parallel frameworks: A MapReduce\n  case study", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The explosion of Big Data was followed by the proliferation of numerous\ncomplex parallel software stacks whose aim is to tackle the challenges of data\ndeluge. A drawback of a such multi-layered hierarchical deployment is the\ninability to maintain and delegate vital semantic information between layers in\nthe stack. Software abstractions increase the semantic distance between an\napplication and its generated code. However, parallel software frameworks\ncontain inherent semantic information that general purpose compilers are not\ndesigned to exploit.\n  This paper presents a case study demonstrating how the specific semantic\ninformation of the MapReduce paradigm can be exploited on multicore\narchitectures. MR4J has been implemented in Java and evaluated against\nhand-optimized C and C++ equivalents. The initial observed results led to the\ndesign of a semantically aware optimizer that runs automatically without\nrequiring modification to application code.\n  The optimizer is able to speedup the execution time of MR4J by up to 2.0x.\nThe introduced optimization not only improves the performance of the generated\ncode, during the map phase, but also reduces the pressure on the garbage\ncollector. This demonstrates how semantic information can be harnessed without\nsacrificing sound software engineering practices when using parallel software\nframeworks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 16:59:36 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Barrett", "Colin", ""], ["Kotselidis", "Christos", ""], ["Luj\u00e1n", "Mikel", ""]]}]