[{"id": "2107.00051", "submitter": "Lichao Sun", "authors": "Wanning Pan, Lichao Sun", "title": "Global Knowledge Distillation in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation has caught a lot of attention in Federated Learning\n(FL) recently. It has the advantage for FL to train on heterogeneous clients\nwhich have different data size and data structure. However, data samples across\nall devices are usually not independent and identically distributed\n(non-i.i.d), posing additional challenges to the convergence and speed of\nfederated learning. As FL randomly asks the clients to join the training\nprocess and each client only learns from local non-i.i.d data, which makes\nlearning processing even slower. In order to solve this problem, an intuitive\nidea is using the global model to guide local training. In this paper, we\npropose a novel global knowledge distillation method, named FedGKD, which\nlearns the knowledge from past global models to tackle down the local bias\ntraining problem. By learning from global knowledge and consistent with current\nlocal models, FedGKD learns a global knowledge model in FL. To demonstrate the\neffectiveness of the proposed method, we conduct extensive experiments on\nvarious CV datasets (CIFAR-10/100) and settings (non-i.i.d data). The\nevaluation results show that FedGKD outperforms previous state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:14:24 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Pan", "Wanning", ""], ["Sun", "Lichao", ""]]}, {"id": "2107.00059", "submitter": "Mohammad Javad Shayegan", "authors": "Mohammad Javad Shayegan and Kiarash Shamsi", "title": "A Fair Method for Distributing Collective Assets in the Stellar\n  Blockchain Financial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The financial industry is a pioneer in Blockchain technology. One of the most\npopular platforms in Token-based banking is the flexible Stellar platform. This\nplatform is open-source, and today, its wide range of features makes it\npossible for many countries and companies to use it in cryptocurrency and\nToken-based modern banking. This network charges a fee for each transaction. As\nwell, a percentage of the net amount is generated as the inflation rate of the\nnetwork due to the increased number of tokens. These fees and inflationary\namounts are aggregated into a general account and ultimately distributed among\nmembers of the network on a collective vote basis. In this mechanism, network\nusers select an account as the destination for which they wish to transfer\nassets using their user interface, which is generally a wallet. This account\ncould be the account of charities that need this help. It is then determined\nthe target distribution network based on the voting results of all members. One\nof the challenges in this network is the purposeful and fair distribution of\nthese funds between accounts. In this paper, the first step is a complete\ninfrastructure of a Stellar financial network that will consist of three\nnetwork-based segments of the core network, off-chain server, and wallet\ninterface. In the second step, a context-aware recommendation system will be\nexplored and implemented as a solution for the purposeful management of payroll\naccount selection. The results of this study concerning the importance of the\npurposeful division of collective assets and showing a context-aware\nrecommendation system as a solution to improve the process of stellar users'\nparticipation in the voting process by effectively helping them in choosing an\neligible destination\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:50:30 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Shayegan", "Mohammad Javad", ""], ["Shamsi", "Kiarash", ""]]}, {"id": "2107.00075", "submitter": "Erik Boman", "authors": "Ian Bogle and Erik G Boman and Karen D Devine and Sivasankaran\n  Rajamanickam and George M Slota", "title": "Parallel Graph Coloring Algorithms for Distributed GPU Environments", "comments": "Submitted to Parallel Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph coloring is often used in parallelizing scientific computations that\nrun in distributed and multi-GPU environments; it identifies sets of\nindependent data that can be updated in parallel. Many algorithms exist for\ngraph coloring on a single GPU or in distributed memory, but to the best of our\nknowledge, hybrid MPI+GPU algorithms have been unexplored until this work. We\npresent several MPI+GPU coloring approaches based on the distributed coloring\nalgorithms of Gebremedhin et al. and the shared-memory algorithms of Deveci et\nal. . The on-node parallel coloring uses implementations in KokkosKernels,\n  which provide parallelization for both multicore CPUs and GPUs. We further\nextend our approaches to compute distance-2 and partial distance-2 colorings,\ngiving the first known distributed, multi-GPU algorithm for these problems. In\naddition, we propose a novel heuristic to reduce communication for recoloring\nin distributed graph coloring. Our experiments show that our approaches operate\nefficiently on inputs too large to fit on a single GPU and scale up to graphs\nwith 76.7 billion edges running on 128 GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:55:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bogle", "Ian", ""], ["Boman", "Erik G", ""], ["Devine", "Karen D", ""], ["Rajamanickam", "Sivasankaran", ""], ["Slota", "George M", ""]]}, {"id": "2107.00164", "submitter": "Seung-Seob Lee", "authors": "Seung-seob Lee, Yanpeng Yu, Yupeng Tang, Anurag Khandelwal, Lin Zhong,\n  and Abhishek Bhattacharjee", "title": "MIND: In-Network Memory Management for Disaggregated Data Centers", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory-compute disaggregation promises transparent elasticity, high\nutilization and balanced usage for resources in data centers by physically\nseparating memory and compute into network-attached resource \"blades\". However,\nexisting designs achieve performance at the cost of resource elasticity,\nrestricting memory sharing to a single compute blade to avoid costly memory\ncoherence traffic over the network.\n  In this work, we show that emerging programmable network switches can enable\nan efficient shared memory abstraction for disaggregated architectures by\nplacing memory management logic in the network fabric. We find that\ncentralizing memory management in the network permits bandwidth and\nlatency-efficient realization of in-network cache coherence protocols, while\nprogrammable switch ASICs support other memory management logic at line-rate.\nWe realize these insights into MIND, an in-network memory management unit for\nrack-scale memory disaggregation. MIND enables transparent resource elasticity\nwhile matching the performance of prior memory disaggregation proposals for\nreal-world workloads.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 01:14:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lee", "Seung-seob", ""], ["Yu", "Yanpeng", ""], ["Tang", "Yupeng", ""], ["Khandelwal", "Anurag", ""], ["Zhong", "Lin", ""], ["Bhattacharjee", "Abhishek", ""]]}, {"id": "2107.00179", "submitter": "Hongji Wei", "authors": "T. Tony Cai and Hongji Wei", "title": "Distributed Nonparametric Function Estimation: Optimal Rate of\n  Convergence and Cost of Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DC cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed minimax estimation and distributed adaptive estimation under\ncommunication constraints for Gaussian sequence model and white noise model are\nstudied. The minimax rate of convergence for distributed estimation over a\ngiven Besov class, which serves as a benchmark for the cost of adaptation, is\nestablished. We then quantify the exact communication cost for adaptation and\nconstruct an optimally adaptive procedure for distributed estimation over a\nrange of Besov classes. The results demonstrate significant differences between\nnonparametric function estimation in the distributed setting and the\nconventional centralized setting. For global estimation, adaptation in general\ncannot be achieved for free in the distributed setting. The new technical tools\nto obtain the exact characterization for the cost of adaptation can be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:16:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Cai", "T. Tony", ""], ["Wei", "Hongji", ""]]}, {"id": "2107.00185", "submitter": "Soheil Saraji", "authors": "Soheil Saraji, Mike Borowczak", "title": "A Blockchain-based Carbon Credit Ecosystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Climate change and global warming are the significant challenges of the new\ncentury. A viable solution to mitigate greenhouse gas emissions is via a\nglobally incentivized market mechanism proposed in the Kyoto protocol. In this\nview, the carbon dioxide (or other greenhouse gases) emission is considered a\ncommodity, forming a carbon trading system. There have been attempts in\ndeveloping this idea in the past decade with limited success. The main\nchallenges of current systems are fragmented implementations, lack of\ntransparency leading to over-crediting and double-spending, and substantial\ntransaction costs that transfer wealth to brokers and agents. We aim to create\na Carbon Credit Ecosystem using smart contracts that operate in conjunction\nwith blockchain technology in order to bring more transparency, accessibility,\nliquidity, and standardization to carbon markets. This ecosystem includes a\ntokenization mechanism to securely digitize carbon credits with clear minting\nand burning protocols, a transparent mechanism for distribution of tokens, a\nfree automated market maker for trading the carbon tokens, and mechanisms to\nengage all stakeholders, including the energy industry, project verifiers,\nliquidity providers, NGOs, concerned citizens, and governments. This approach\ncould be used in a variety of other credit/trading systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:29:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Saraji", "Soheil", ""], ["Borowczak", "Mike", ""]]}, {"id": "2107.00187", "submitter": "Marco Netto", "authors": "Renato L. F. Cunha, Lucas V. Real, Renan Souza, Bruno Silva, Marco A.\n  S. Netto", "title": "Context-aware Execution Migration Tool for Data Science Jupyter\n  Notebooks on Hybrid Clouds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive computing notebooks, such as Jupyter notebooks, have become a\npopular tool for developing and improving data-driven models. Such notebooks\ntend to be executed either in the user's own machine or in a cloud environment,\nhaving drawbacks and benefits in both approaches. This paper presents a\nsolution developed as a Jupyter extension that automatically selects which\ncells, as well as in which scenarios, such cells should be migrated to a more\nsuitable platform for execution. We describe how we reduce the execution state\nof the notebook to decrease migration time and we explore the knowledge of user\ninteractivity patterns with the notebook to determine which blocks of cells\nshould be migrated. Using notebooks from Earth science (remote sensing), image\nrecognition, and hand written digit identification (machine learning), our\nexperiments show notebook state reductions of up to 55x and migration decisions\nleading to performance gains of up to 3.25x when the user interactivity with\nthe notebook is taken into consideration.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 02:33:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Cunha", "Renato L. F.", ""], ["Real", "Lucas V.", ""], ["Souza", "Renan", ""], ["Silva", "Bruno", ""], ["Netto", "Marco A. S.", ""]]}, {"id": "2107.00233", "submitter": "Tehrim Yoon", "authors": "Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang", "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 06:14:51 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yoon", "Tehrim", ""], ["Shin", "Sumin", ""], ["Hwang", "Sung Ju", ""], ["Yang", "Eunho", ""]]}, {"id": "2107.00271", "submitter": "Heike Wehrheim", "authors": "Heike Wehrheim", "title": "On the (Non-)Applicability of a Small Model Theorem to Model Checking\n  STMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software Transactional Memory (STM) algorithms provide programmers with a\nsynchronisation mechanism for concurrent access to shared variables. Basically,\nprogrammers can specify transactions (reading from and writing to shared state)\nwhich execute \"seemingly\" atomic. This property is captured in a correctness\ncriterion called opacity. For model checking opacity of an STM algorithm, we --\nin principle -- need to check opacity for all possible combinations of\ntransactions writing to and reading from potentially unboundedly many\nvariables.\n  To still apply automatic model checking techniques to opacity checking, a so\ncalled small model theorem has been proven which states that model checking on\ntwo variables and two transactions is sufficient for correctness verification\nof STMs. In this paper, we take a fresh look at this small model theorem and\ninvestigate its applicability to opacity checking of STM algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 07:44:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Wehrheim", "Heike", ""]]}, {"id": "2107.00416", "submitter": "Christian G\\\"ottel", "authors": "Christian G\\\"ottel, Konstantinos Parasyris, Osman Unsal, Pascal\n  Felber, Marcelo Pasin, Valerio Schiavoni", "title": "Scrooge Attack: Undervolting ARM Processors for Profit", "comments": "European Commission Project: LEGaTO - Low Energy Toolset for\n  Heterogeneous Computing (EC-H2020-780681)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest ARM processors are approaching the computational power of x86\narchitectures while consuming much less energy. Consequently, supply follows\ndemand with Amazon EC2, Equinix Metal and Microsoft Azure offering ARM-based\ninstances, while Oracle Cloud Infrastructure is about to add such support. We\nexpect this trend to continue, with an increasing number of cloud providers\noffering ARM-based cloud instances.\n  ARM processors are more energy-efficient leading to substantial electricity\nsavings for cloud providers. However, a malicious cloud provider could\nintentionally reduce the CPU voltage to further lower its costs. Running\napplications malfunction when the undervolting goes below critical thresholds.\nBy avoiding critical voltage regions, a cloud provider can run undervolted\ninstances in a stealthy manner.\n  This practical experience report describes a novel attack scenario: an attack\nlaunched by the cloud provider against its users to aggressively reduce the\nprocessor voltage for saving energy to the last penny. We call it the Scrooge\nAttack and show how it could be executed using ARM-based computing instances.\nWe mimic ARM-based cloud instances by deploying our own ARM-based devices using\ndifferent generations of Raspberry Pi. Using realistic and synthetic workloads,\nwe demonstrate to which degree of aggressiveness the attack is relevant. The\nattack is unnoticeable by our detection method up to an offset of -50mV. We\nshow that the attack may even remain completely stealthy for certain workloads.\nFinally, we propose a set of client-based detection methods that can identify\nundervolted instances. We support experimental reproducibility and provide\ninstructions to reproduce our results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:58:23 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 06:41:58 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["G\u00f6ttel", "Christian", ""], ["Parasyris", "Konstantinos", ""], ["Unsal", "Osman", ""], ["Felber", "Pascal", ""], ["Pasin", "Marcelo", ""], ["Schiavoni", "Valerio", ""]]}, {"id": "2107.00417", "submitter": "Daniel S. Katz", "authors": "Joe Stubbs, Suresh Marru, Daniel Mejia, Daniel S. Katz, Kyle Chard,\n  Maytal Dahan, Marlon Pierce, Michael Zentner", "title": "Toward Interoperable Cyberinfrastructure: Common Descriptions for\n  Computational Resources and Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3311790.3400848", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The user-facing components of the Cyberinfrastructure (CI) ecosystem, science\ngateways and scientific workflow systems, share a common need of interfacing\nwith physical resources (storage systems and execution environments) to manage\ndata and execute codes (applications). However, there is no uniform,\nplatform-independent way to describe either the resources or the applications.\nTo address this, we propose uniform semantics for describing resources and\napplications that will be relevant to a diverse set of stakeholders. We sketch\na solution to the problem of a common description and catalog of resources: we\ndescribe an approach to implementing a resource registry for use by the\ncommunity and discuss potential approaches to some long-term challenges. We\nconclude by looking ahead to the application description language.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:00:05 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Stubbs", "Joe", ""], ["Marru", "Suresh", ""], ["Mejia", "Daniel", ""], ["Katz", "Daniel S.", ""], ["Chard", "Kyle", ""], ["Dahan", "Maytal", ""], ["Pierce", "Marlon", ""], ["Zentner", "Michael", ""]]}, {"id": "2107.00431", "submitter": "Guilherme Ramos", "authors": "Guilherme Ramos and Daniel Silvestre and Carlos Silvestre", "title": "A Discrete-time Reputation-based Resilient Consensus Algorithm for\n  Synchronous or Asynchronous Communications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.MA cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of a set of agents achieving resilient consensus in the\npresence of attacked agents. We present a discrete-time reputation-based\nconsensus algorithm for synchronous and asynchronous networks by developing a\nlocal strategy where, at each time, each agent assigns a reputation (between\nzero and one) to each neighbor. The reputation is then used to weigh the\nneighbors' values in the update of its state. Under mild assumptions, we show\nthat: (i) the proposed method converges exponentially to the consensus of the\nregular agents; (ii) if a regular agent identifies a neighbor as an attacked\nnode, then it is indeed an attacked node; (iii) if the consensus value of the\nnormal nodes differs from that of any of the attacked nodes' values, then the\nreputation that a regular agent assigns to the attacked neighbors goes to zero.\nFurther, we extend our method to achieve resilience in the scenarios where\nthere are noisy nodes, dynamic networks and stochastic node selection. Finally,\nwe illustrate our algorithm with several examples, and we delineate some\nattacking scenarios that can be dealt by the current proposal but not by the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 13:26:57 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ramos", "Guilherme", ""], ["Silvestre", "Daniel", ""], ["Silvestre", "Carlos", ""]]}, {"id": "2107.00555", "submitter": "Alexandros Nikolaos Ziogas", "authors": "Alexandros Nikolaos Ziogas, Timo Schneider, Tal Ben-Nun, Alexandru\n  Calotoiu, Tiziano De Matteis, Johannes de Fine Licht, Luca Lavarini, and\n  Torsten Hoefler", "title": "Productivity, Portability, Performance: Data-Centric Python", "comments": null, "journal-ref": null, "doi": "10.1145/1122445.1122456", "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python has become the de facto language for scientific computing. Programming\nin Python is highly productive, mainly due to its rich science-oriented\nsoftware ecosystem built around the NumPy module. As a result, the demand for\nPython support in High Performance Computing (HPC) has skyrocketed. However,\nthe Python language itself does not necessarily offer high performance. In this\nwork, we present a workflow that retains Python's high productivity while\nachieving portable performance across different architectures. The workflow's\nkey features are HPC-oriented language extensions and a set of automatic\noptimizations powered by a data-centric intermediate representation. We show\nperformance results and scaling across CPU, GPU, FPGA, and the Piz Daint\nsupercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over\nprevious-best solutions, first-ever Xilinx and Intel FPGA results of annotated\nPython, and up to 93.16% scaling efficiency on 512 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:51:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Ben-Nun", "Tal", ""], ["Calotoiu", "Alexandru", ""], ["De Matteis", "Tiziano", ""], ["Licht", "Johannes de Fine", ""], ["Lavarini", "Luca", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2107.00881", "submitter": "Geet Shingi", "authors": "Geet Shingi, Harsh Saglani, Preeti Jain", "title": "Segmented Federated Learning for Adaptive Intrusion Detection System", "comments": "Accepted at the Workshop on Artificial Intelligence for Social Good\n  (AI4SG) at the 30th International Joint Conference on Artificial Intelligence\n  (IJCAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cyberattacks are a major issues and it causes organizations great financial,\nand reputation harm. However, due to various factors, the current network\nintrusion detection systems (NIDS) seem to be insufficent. Predominant NIDS\nidentifies Cyberattacks through a handcrafted dataset of rules. Although the\nrecent applications of machine learning and deep learning have alleviated the\nenormous effort in NIDS, the security of network data has always been a prime\nconcern. However, to encounter the security problem and enable sharing among\norganizations, Federated Learning (FL) scheme is employed. Although the current\nFL systems have been successful, a network's data distribution does not always\nfit into a single global model as in FL. Thus, in such cases, having a single\nglobal model in FL is no feasible. In this paper, we propose a\nSegmented-Federated Learning (Segmented-FL) learning scheme for a more\nefficient NIDS. The Segmented-FL approach employs periodic local model\nevaluation based on which the segmentation occurs. We aim to bring similar\nnetwork environments to the same group. Further, the Segmented-FL system is\ncoupled with a weighted aggregation of local model parameters based on the\nnumber of data samples a worker possesses to further augment the performance.\nThe improved performance by our system as compared to the FL and centralized\nsystems on standard dataset further validates our system and makes a strong\ncase for extending our technique across various tasks. The solution finds its\napplication in organizations that want to collaboratively learn on diverse\nnetwork environments and protect the privacy of individual datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 07:47:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Shingi", "Geet", ""], ["Saglani", "Harsh", ""], ["Jain", "Preeti", ""]]}, {"id": "2107.00961", "submitter": "Anastasios Kyrillidis", "authors": "Chen Dun, Cameron R. Wolfe, Christopher M. Jermaine, Anastasios\n  Kyrillidis", "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:48:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dun", "Chen", ""], ["Wolfe", "Cameron R.", ""], ["Jermaine", "Christopher M.", ""], ["Kyrillidis", "Anastasios", ""]]}, {"id": "2107.01056", "submitter": "Alexandre Bonvin", "authors": "Rodrigo Vargas Honorato, Panagiotis I. Koukos, Brian\n  Jim\\'enez-Garc\\'ia, Andrei Tsaregorodtsev, Marco Verlato, Andrea Giachetti,\n  Antonio Rosato and Alexandre M.J.J. Bonvin", "title": "Structural biology in the clouds: The WeNMR-EOSC Ecosystem", "comments": "14 pages, 3 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structural biology aims at characterizing the structural and dynamic\nproperties of biological macromolecules at atomic details. Gaining insight into\nthree dimensional structures of biomolecules and their interactions is critical\nfor understanding the vast majority of cellular processes, with direct\napplications in health and food sciences. Since 2010, the WeNMR project\n(www.wenmr.eu) has implemented numerous web-based services to facilitate the\nuse of advanced computational tools by researchers in the field, using the high\nthroughput computing infrastructure provided by EGI. These services have been\nfurther developed in subsequent initiatives under H2020 projects and are now\noperating as Thematic Services in the European Open Science Cloud (EOSC) portal\n(www.eosc-portal.eu), sending >12 millions of jobs and using around 4000\nCPU-years per year. Here we review 10 years of successful e-infrastructure\nsolutions serving a large worldwide community of over 23,000 users to date,\nproviding them with user-friendly, web-based solutions that run complex\nworkflows in structural biology. The current set of active WeNMR portals are\ndescribed, together with the complex backend machinery that allows distributed\ncomputing resources to be harvested efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:04:31 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Honorato", "Rodrigo Vargas", ""], ["Koukos", "Panagiotis I.", ""], ["Jim\u00e9nez-Garc\u00eda", "Brian", ""], ["Tsaregorodtsev", "Andrei", ""], ["Verlato", "Marco", ""], ["Giachetti", "Andrea", ""], ["Rosato", "Antonio", ""], ["Bonvin", "Alexandre M. J. J.", ""]]}, {"id": "2107.01104", "submitter": "Cristian Constantin Lalescu", "authors": "Cristian C. Lalescu, B\\'erenger Bramas, Markus Rampp, Michael Wilczek", "title": "An Efficient Particle Tracking Algorithm for Large-Scale Parallel\n  Pseudo-Spectral Simulations of Turbulence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle tracking in large-scale numerical simulations of turbulent flows\npresents one of the major bottlenecks in parallel performance and scaling\nefficiency. Here, we describe a particle tracking algorithm for large-scale\nparallel pseudo-spectral simulations of turbulence which scales well up to\nbillions of tracer particles on modern high-performance computing\narchitectures. We summarize the standard parallel methods used to solve the\nfluid equations in our hybrid MPI/OpenMP implementation. As the main focus, we\ndescribe the implementation of the particle tracking algorithm and document its\ncomputational performance. To address the extensive inter-process communication\nrequired by particle tracking, we introduce a task-based approach to overlap\npoint-to-point communications with computations, thereby enabling improved\nresource utilization. We characterize the computational cost as a function of\nthe number of particles tracked and compare it with the flow field computation,\nshowing that the cost of particle tracking is very small for typical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:35:40 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Lalescu", "Cristian C.", ""], ["Bramas", "B\u00e9renger", ""], ["Rampp", "Markus", ""], ["Wilczek", "Michael", ""]]}, {"id": "2107.01119", "submitter": "Yuhao Lu", "authors": "Yuhao Lu, Zhenqing Liu, Dejun Jiang, Liuying Ma, Jin Xiong", "title": "A Micro-Service based Approach for Constructing Distributed Storage\n  System", "comments": "3 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for constructing distributed storage system\nbased on micro-service architecture. By building storage functionalities using\nmicro services, we can provide new capabilities to a distributed storage system\nin a flexible way. We take erasure coding and compression as two case studies\nto show how to build a micro-service based distributed storage system. We also\nshow that by building erasure coding and compression as micro-services, the\ndistributed storage system still achieves reasonable performance compared to\nthe monolithic one.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:05:49 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Lu", "Yuhao", ""], ["Liu", "Zhenqing", ""], ["Jiang", "Dejun", ""], ["Ma", "Liuying", ""], ["Xiong", "Jin", ""]]}, {"id": "2107.01142", "submitter": "Liangkai Liu", "authors": "Liangkai Liu, Shaoshan Liu, and Weisong Shi", "title": "4C: A Computation, Communication, and Control Co-Design Framework for\n  CAVs", "comments": "7 pages, 4 figures, accepted by IEEE Wireless Communication Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Connected and autonomous vehicles (CAVs) are promising due to their potential\nsafety and efficiency benefits and have attracted massive investment and\ninterest from government agencies, industry, and academia. With more computing\nand communication resources are available, both vehicles and edge servers are\nequipped with a set of camera-based vision sensors, also known as Visual IoT\n(V-IoT) techniques, for sensing and perception. Tremendous efforts have been\nmade for achieving programmable communication, computation, and control.\nHowever, they are conducted mainly in the silo mode, limiting the\nresponsiveness and efficiency of handling challenging scenarios in the real\nworld. To improve the end-to-end performance, we envision that future CAVs\nrequire the co-design of communication, computation, and control. This paper\npresents our vision of the end-to-end design principle for CAVs, called 4C,\nwhich extends the V-IoT system by providing a unified communication,\ncomputation, and control co-design framework. With programmable communications,\nfine-grained heterogeneous computation, and efficient vehicle controls in 4C,\nCAVs can handle critical scenarios and achieve energy-efficient autonomous\ndriving. Finally, we present several challenges to achieving the vision of the\n4C framework.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:36:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Liu", "Liangkai", ""], ["Liu", "Shaoshan", ""], ["Shi", "Weisong", ""]]}, {"id": "2107.01405", "submitter": "Bing Lin", "authors": "Bing Lin, Chaowei Lin, Xing Chen, Neal N. Xiong, and Qiang Shen", "title": "A Fuzzy Scheduling Strategy for Workflow Decision Making in Uncertain\n  Edge-Cloud Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workflow decision making is critical to performing many practical workflow\napplications. Scheduling in edge-cloud environments can address the high\ncomplexity of workflow applications, while decreasing the data transmission\ndelay between the cloud and end devices. However, due to the heterogeneous\nresources in edge-cloud environments and the complicated data dependencies\nbetween the tasks in a workflow, significant challenges for workflow scheduling\nremain, including the selection of an optimal tasks-servers solution from the\npossible numerous combinations. Existing studies are mainly done subject to\nrigorous conditions without fluctuations, ignoring the fact that workflow\nscheduling is typically present in uncertain environments. In this study, we\nfocus on reducing the execution cost of workflow applications mainly caused by\ntask computation and data transmission, while satisfying the workflow deadline\nin uncertain edge-cloud environments. The Triangular Fuzzy Numbers (TFNs) are\nadopted to represent the task processing time and data transferring time. A\ncost-driven fuzzy scheduling strategy based on an Adaptive Discrete Particle\nSwarm Optimization (ADPSO) algorithm is proposed, which employs the operators\nof Genetic Algorithm (GA). This strategy introduces the randomly two-point\ncrossover operator, neighborhood mutation operator, and adaptive multipoint\nmutation operator of GA to effectively avoid converging on local optima. The\nexperimental results show that our strategy can effectively reduce the workflow\nexecution cost in uncertain edge-cloud environments, compared with other\nbenchmark solutions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 10:54:58 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 08:39:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lin", "Bing", ""], ["Lin", "Chaowei", ""], ["Chen", "Xing", ""], ["Xiong", "Neal N.", ""], ["Shen", "Qiang", ""]]}, {"id": "2107.01499", "submitter": "Shaoduo Gan", "authors": "Shaoduo Gan, Xiangru Lian, Rui Wang, Jianbin Chang, Chengjun Liu,\n  Hongmei Shi, Shengzhuo Zhang, Xianghong Li, Tengxu Sun, Jiawei Jiang, Binhang\n  Yuan, Sen Yang, Ji Liu, Ce Zhang", "title": "BAGUA: Scaling up Distributed Learning with System Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\ncommunication framework whose design goal is to provide a system abstraction\nthat is both flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by the new system design, BAGUA has\na great ability to implement and extend various state-of-the-art distributed\nlearning algorithms. In a production cluster with up to 16 machines (128 GPUs),\nBAGUA can outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training\ntime by a significant margin (up to 1.95 times) across a diverse range of\ntasks. Moreover, we conduct a rigorous tradeoff exploration showing that\ndifferent algorithms and system relaxations achieve the best performance over\ndifferent network conditions.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jul 2021 21:27:45 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 08:18:02 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 14:20:19 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gan", "Shaoduo", ""], ["Lian", "Xiangru", ""], ["Wang", "Rui", ""], ["Chang", "Jianbin", ""], ["Liu", "Chengjun", ""], ["Shi", "Hongmei", ""], ["Zhang", "Shengzhuo", ""], ["Li", "Xianghong", ""], ["Sun", "Tengxu", ""], ["Jiang", "Jiawei", ""], ["Yuan", "Binhang", ""], ["Yang", "Sen", ""], ["Liu", "Ji", ""], ["Zhang", "Ce", ""]]}, {"id": "2107.01542", "submitter": "Gershom Bazerman", "authors": "Gershom Bazerman", "title": "The Semantics of Package Management via Event Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an approach to the semantics of package management which relates\nit to general event structures, well-known mathematical objects used in the\nsemantics of concurrent, nondeterministic systems. In this approach, the data\nof a package repository is treated as a declarative specification of a\nnondeterministic, concurrent program. We introduce a process calculus\ncorresponding to this data, and investigate its operational and categorical\nsemantics. Our hope is this lays the basis for further formal study of package\nmanagement in which the weight of existing tools can be brought to bear.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:14:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bazerman", "Gershom", ""]]}, {"id": "2107.01600", "submitter": "Oliver Stengele", "authors": "Oliver Stengele, Markus Raiber, J\\\"orn M\\\"uller-Quade, Hannes\n  Hartenstein", "title": "ETHTID: Deployable Threshold Information Disclosure on Ethereum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Threshold Information Disclosure (TID) problem on Ethereum: An\narbitrary number of users commit to the scheduled disclosure of their\nindividual messages recorded on the Ethereum blockchain if and only if all such\nmessages are disclosed. Before a disclosure, only the original sender of each\nmessage should know its contents. To accomplish this, we task a small council\nwith executing a distributed generation and threshold sharing of an asymmetric\nkey pair. The public key can be used to encrypt messages which only become\nreadable once the threshold-shared decryption key is reconstructed at a\npredefined point in time and recorded on-chain. With blockchains like Ethereum,\nit is possible to coordinate such procedures and attach economic stakes to the\nactions of participating individuals. In this paper, we present ETHTID, an\nEthereum smart contract application to coordinate Threshold Information\nDisclosure. We base our implementation on ETHDKG [1], a smart contract\napplication for distributed key generation and threshold sharing, and adapt it\nto fit our differing use case as well as add functionality to oversee a\nscheduled reconstruction of the decryption key. For our main cost saving\noptimisation, we show that the security of the underlying cryptographic scheme\nis maintained. We evaluate how the execution costs depend on the size of the\ncouncil and the threshold and show that the presented protocol is deployable on\nEthereum with a council of more than 200 members with gas savings of 20-40%\ncompared to ETHDKG.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 12:09:03 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Stengele", "Oliver", ""], ["Raiber", "Markus", ""], ["M\u00fcller-Quade", "J\u00f6rn", ""], ["Hartenstein", "Hannes", ""]]}, {"id": "2107.01707", "submitter": "Rasheed el-Bouri", "authors": "Rasheed el-Bouri, Tingting Zhu, David A. Clifton", "title": "Towards Scheduling Federated Deep Learning using Meta-Gradients for\n  Inter-Hospital Learning", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the abundance and ease of access of personal data today, individual\nprivacy has become of paramount importance, particularly in the healthcare\ndomain. In this work, we aim to utilise patient data extracted from multiple\nhospital data centres to train a machine learning model without sacrificing\npatient privacy. We develop a scheduling algorithm in conjunction with a\nstudent-teacher algorithm that is deployed in a federated manner. This allows a\ncentral model to learn from batches of data at each federal node. The teacher\nacts between data centres to update the main task (student) algorithm using the\ndata that is stored in the various data centres. We show that the scheduler,\ntrained using meta-gradients, can effectively organise training and as a result\ntrain a machine learning model on a diverse dataset without needing explicit\naccess to the patient data. We achieve state-of-the-art performance and show\nhow our method overcomes some of the problems faced in the federated learning\nsuch as node poisoning. We further show how the scheduler can be used as a\nmechanism for transfer learning, allowing different teachers to work together\nin training a student for state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 18:45:58 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["el-Bouri", "Rasheed", ""], ["Zhu", "Tingting", ""], ["Clifton", "David A.", ""]]}, {"id": "2107.01735", "submitter": "Thomas Robertazzi", "authors": "Abdulaziz M. Alqarni and Thomas G. Robertazzi", "title": "Cloud Versus Local Processing in Distributed Networks", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A method for evaluating the relative performance of local, cloud and combined\nprocessing of divisible (i.e. partitionable) data loads is presented. It is\nshown how to do this in the context of Amdahl's law. A single level (star)\nnetwork operating under each of three fundamental scheduling policies is used\nas an example. Applications include mobile computing, cloud computing and\nsignature searching.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 21:11:26 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Alqarni", "Abdulaziz M.", ""], ["Robertazzi", "Thomas G.", ""]]}, {"id": "2107.01739", "submitter": "J. Gregory Pauloski", "authors": "J. Gregory Pauloski, Qi Huang, Lei Huang, Shivaram Venkataraman, Kyle\n  Chard, Ian Foster, Zhao Zhang", "title": "KAISA: An Adaptive Second-order Optimizer Framework for Deep Neural\n  Networks", "comments": "To be published in the proceedings of the International Conference\n  for High Performance Computing, Networking, Storage and Analysis (SC21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to\nconverge faster in deep neural network (DNN) training than stochastic gradient\ndescent (SGD); however, K-FAC's larger memory footprint hinders its\napplicability to large models. We present KAISA, a K-FAC-enabled, Adaptable,\nImproved, and ScAlable second-order optimizer framework that adapts the memory\nfootprint, communication, and computation given specific models and hardware to\nachieve maximized performance and enhanced scalability. We quantify the\ntradeoffs between memory and communication cost and evaluate KAISA on large\nmodels, including ResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA\nA100 GPUs. Compared to the original optimizers, KAISA converges 18.1-36.3%\nfaster across applications with the same global batch size. Under a fixed\nmemory budget, KAISA converges 32.5% and 41.6% faster in ResNet-50 and\nBERT-Large, respectively. KAISA can balance memory and communication to achieve\nscaling efficiency equal to or better than the baseline optimizers.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 21:34:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pauloski", "J. Gregory", ""], ["Huang", "Qi", ""], ["Huang", "Lei", ""], ["Venkataraman", "Shivaram", ""], ["Chard", "Kyle", ""], ["Foster", "Ian", ""], ["Zhang", "Zhao", ""]]}, {"id": "2107.01895", "submitter": "Yipeng Zhou", "authors": "Yipeng Zhou and Xuezheng Liu and Yao Fu and Di Wu and Chao Li and Shui\n  Yu", "title": "Optimizing the Numbers of Queries and Replies in Federated Learning with\n  Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) empowers distributed clients to collaboratively train\na shared machine learning model through exchanging parameter information.\nDespite the fact that FL can protect clients' raw data, malicious users can\nstill crack original data with disclosed parameters. To amend this flaw,\ndifferential privacy (DP) is incorporated into FL clients to disturb original\nparameters, which however can significantly impair the accuracy of the trained\nmodel. In this work, we study a crucial question which has been vastly\noverlooked by existing works: what are the optimal numbers of queries and\nreplies in FL with DP so that the final model accuracy is maximized. In FL, the\nparameter server (PS) needs to query participating clients for multiple global\niterations to complete training. Each client responds a query from the PS by\nconducting a local iteration. Our work investigates how many times the PS\nshould query clients and how many times each client should reply the PS. We\ninvestigate two most extensively used DP mechanisms (i.e., the Laplace\nmechanism and Gaussian mechanisms). Through conducting convergence rate\nanalysis, we can determine the optimal numbers of queries and replies in FL\nwith DP so that the final model accuracy can be maximized. Finally, extensive\nexperiments are conducted with publicly available datasets: MNIST and FEMNIST,\nto verify our analysis and the results demonstrate that properly setting the\nnumbers of queries and replies can significantly improve the final model\naccuracy in FL with DP.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:42:56 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Zhou", "Yipeng", ""], ["Liu", "Xuezheng", ""], ["Fu", "Yao", ""], ["Wu", "Di", ""], ["Li", "Chao", ""], ["Yu", "Shui", ""]]}, {"id": "2107.02342", "submitter": "Shashikant Ilager Mr", "authors": "Shashikant Ilager and Rajkumar Buyya", "title": "Energy and Thermal-aware Resource Management of Cloud Data Centres: A\n  Taxonomy and Future Directions", "comments": "Submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the existing resource management approaches in Cloud\nData Centres for energy and thermal efficiency. It identifies the need for\nintegrated computing and cooling systems management and learning-based\nsolutions in resource management systems. A taxonomy on energy and thermal\nefficient resource management in data centres is proposed based on an in-depth\nanalysis of the literature. Furthermore, a detailed survey on existing\napproaches is conducted according to the taxonomy and recent advancements\nincluding machine learning-based resource management approaches and cooling\nmanagement technologies are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 01:49:21 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ilager", "Shashikant", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2107.02426", "submitter": "Raphael Bleuse", "authors": "Eric Rutten (CTRL-A ), Sophie Cerf (CTRL-A ), Rapha\\\"el Bleuse (CTRL-A\n  ), Valentin Reis (ANL), Swann Perarnau (ANL)", "title": "Sustaining Performance While Reducing Energy Consumption: A Control\n  Theory Approach", "comments": "The datasets and code generated and analyzed during the current\n  studyare available in the Figshare repository:\n  https://doi.org/10.6084/m9.figshare.14754468[5]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production high-performance computing systems continue to grow in complexity\nand size. As applications struggle to make use of increasingly heterogeneous\ncompute nodes, maintaining high efficiency (performance per watt) for the whole\nplatform becomes a challenge. Alongside the growing complexity of scientific\nworkloads, this extreme heterogeneity is also an opportunity: as applications\ndynamically undergo variations in workload, due to phases or data/compute\nmovement between devices, one can dynamically adjust power across compute\nelements to save energy without impacting performance. With an aim toward an\nautonomous and dynamic power management strategy for current and future HPC\narchitectures, this paper explores the use of control theory for the design of\na dynamic power regulation method. Structured as a feedback loop, our\napproach-which is novel in computing resource management-consists of\nperiodically monitoring application progress and choosing at runtime a suitable\npower cap for processors. Thanks to a preliminary offline identification\nprocess, we derive a model of the dynamics of the system and a\nproportional-integral (PI) controller. We evaluate our approach on top of an\nexisting resource management framework, the Argo Node Resource Manager,\ndeployed on several clusters of Grid'5000, using a standard memory-bound HPC\nbenchmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 06:59:58 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Rutten", "Eric", "", "CTRL-A"], ["Cerf", "Sophie", "", "CTRL-A"], ["Bleuse", "Rapha\u00ebl", "", "CTRL-A"], ["Reis", "Valentin", "", "ANL"], ["Perarnau", "Swann", "", "ANL"]]}, {"id": "2107.02466", "submitter": "Zimu Zheng Dr.", "authors": "Zimu Zheng, Qiong Chen, Chuang Hu, Dan Wang, Fangming Liu", "title": "On-edge Multi-task Transfer Learning: Model and Practice with\n  Data-driven Task Allocation", "comments": "15 pages, published in IEEE TRANSACTIONS ON Parallel and Distributed\n  Systems, VOL. 31, NO. 6, JUNE 2020", "journal-ref": "in IEEE Transactions on Parallel and Distributed Systems, vol. 31,\n  no. 6, pp. 1357-1371, 1 June 2020", "doi": "10.1109/TPDS.2019.2962435", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On edge devices, data scarcity occurs as a common problem where transfer\nlearning serves as a widely-suggested remedy. Nevertheless, transfer learning\nimposes a heavy computation burden to resource-constrained edge devices.\nExisting task allocation works usually assume all submitted tasks are equally\nimportant, leading to inefficient resource allocation at a task level when\ndirectly applied in Multi-task Transfer Learning (MTL). To address these\nissues, we first reveal that it is crucial to measure the impact of tasks on\noverall decision performance improvement and quantify \\emph{task importance}.\nWe then show that task allocation with task importance for MTL (TATIM) is a\nvariant of the NP-complete Knapsack problem, where the complicated computation\nto solve this problem needs to be conducted repeatedly under varying contexts.\nTo solve TATIM with high computational efficiency, we propose a Data-driven\nCooperative Task Allocation (DCTA) approach. Finally, we evaluate the\nperformance of DCTA by not only a trace-driven simulation, but also a new\ncomprehensive real-world AIOps case study that bridges model and practice via a\nnew architecture and main components design within the AIOps system. Extensive\nexperiments show that our DCTA reduces 3.24 times of processing time, and saves\n48.4\\% energy consumption compared with the state-of-the-art when solving\nTATIM.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:24:25 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zheng", "Zimu", ""], ["Chen", "Qiong", ""], ["Hu", "Chuang", ""], ["Wang", "Dan", ""], ["Liu", "Fangming", ""]]}, {"id": "2107.02539", "submitter": "Maria Predari", "authors": "Maria Predari, Charilaos Tzovas, Christian Schulz and Henning\n  Meyerhenke", "title": "An MPI-based Algorithm for Mapping Complex Networks onto Hierarchical\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Processing massive application graphs on distributed memory systems requires\nto map the graphs onto the system's processing elements (PEs). This task\nbecomes all the more important when PEs have non-uniform communication costs or\nthe input is highly irregular. Typically, mapping is addressed using\npartitioning, in a two-step approach or an integrated one. Parallel\npartitioning tools do exist; yet, corresponding mapping algorithms or their\npublic implementations all have major sequential parts or other severe scaling\nlimitations. In this paper, we propose a parallel algorithm that maps graphs\nonto the PEs of a hierarchical system. Our solution integrates partitioning and\nmapping; it models the system hierarchy in a concise way as an implicit labeled\ntree. The vertices of the application graph are labeled as well, and these\nvertex labels induce the mapping. The mapping optimization follows the basic\nidea of parallel label propagation, but we tailor the gain computations of\nlabel changes to quickly account for the induced communication costs. Our\nMPI-based code is the first public implementation of a parallel graph mapping\nalgorithm; to this end, we extend the partitioning library ParHIP. To evaluate\nour algorithm's implementation, we perform comparative experiments with complex\nnetworks in the million- and billion-scale range. In general our mapping tool\nshows good scalability on up to a few thousand PEs. Compared to other MPI-based\ncompetitors, our algorithm achieves the best speed to quality trade-off and our\nquality results are even better than non-parallel mapping tools.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:10:36 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Predari", "Maria", ""], ["Tzovas", "Charilaos", ""], ["Schulz", "Christian", ""], ["Meyerhenke", "Henning", ""]]}, {"id": "2107.02755", "submitter": "Van-Dinh Nguyen", "authors": "Van-Dinh Nguyen, Symeon Chatzinotas, Bjorn Ottersten, and Trung Q.\n  Duong", "title": "FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems", "comments": "30 pages, 12 figures. This work has been submitted to the IEEE for\n  possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) is capable of performing large distributed machine\nlearning tasks across multiple edge users by periodically aggregating trained\nlocal parameters. To address key challenges of enabling FL over a wireless\nfog-cloud system (e.g., non-i.i.d. data, users' heterogeneity), we first\npropose an efficient FL algorithm (called FedFog) to perform the local\naggregation of gradient parameters at fog servers and global training update at\nthe cloud. Next, we employ FedFog in wireless fog-cloud systems by\ninvestigating a novel network-aware FL optimization problem that strikes the\nbalance between the global loss and completion time. An iterative algorithm is\nthen developed to obtain a precise measurement of the system performance, which\nhelps design an efficient stopping criteria to output an appropriate number of\nglobal rounds. To mitigate the straggler effect, we propose a flexible user\naggregation strategy that trains fast users first to obtain a certain level of\naccuracy before allowing slow users to join the global training updates.\nExtensive numerical results using several real-world FL tasks are provided to\nverify the theoretical convergence of FedFog. We also show that the proposed\nco-design of FL and communication is essential to substantially improve\nresource utilization while achieving comparable accuracy of the learning model.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 08:03:15 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nguyen", "Van-Dinh", ""], ["Chatzinotas", "Symeon", ""], ["Ottersten", "Bjorn", ""], ["Duong", "Trung Q.", ""]]}, {"id": "2107.02769", "submitter": "Kaustav Bose", "authors": "Archak Das, Kaustav Bose and Buddhadeb Sau", "title": "Exploring a Dynamic Ring without Landmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a group of autonomous mobile computational entities, called agents,\narbitrarily placed at some nodes of a dynamic but always connected ring. The\nagents neither have any knowledge about the size of the ring nor have a common\nnotion of orientation. We consider the \\textsc{Exploration} problem where the\nagents have to collaboratively to explore the graph and terminate, with the\nrequirement that each node has to be visited by at least one agent. It has been\nshown by Di Luna et al. [Distrib. Comput. 2020] that the problem is solvable by\ntwo anonymous agents if there is a single observably different node in the ring\ncalled landmark node. The problem is unsolvable by any number of anonymous\nagents in absence of a landmark node. We consider the problem with\nnon-anonymous agents (agents with distinct identifiers) in a ring with no\nlandmark node. The assumption of agents with distinct identifiers is strictly\nweaker than having a landmark node as the problem is unsolvable by two agents\nwith distinct identifiers in absence of a landmark node. This setting has been\nrecently studied by Mandal et al. [ALGOSENSORS 2020]. There it is shown that\nthe problem is solvable in this setting by three agents assuming that they have\nedge crossing detection capability. Edge crossing detection capability is a\nstrong assumption which enables two agents moving in opposite directions\nthrough an edge in the same round to detect each other and also exchange\ninformation. In this paper we give an algorithm that solves the problem with\nthree agents without the edge crossing detection capability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:28:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Das", "Archak", ""], ["Bose", "Kaustav", ""], ["Sau", "Buddhadeb", ""]]}, {"id": "2107.02841", "submitter": "Daniel S. Katz", "authors": "Justin M. Wozniak, Timothy G. Armstrong, Ketan C. Maheshwari, Daniel\n  S. Katz, Michael Wilde, Ian T. Foster", "title": "Toward Interlanguage Parallel Scripting for Distributed-Memory\n  Scientific Computing", "comments": "2015 IEEE International Conference on Cluster Computing", "journal-ref": null, "doi": "10.1109/CLUSTER.2015.74", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scripting languages such as Python and R have been widely adopted as tools\nfor the productive development of scientific software because of the power and\nexpressiveness of the languages and available libraries. However, deploying\nscripted applications on large-scale parallel computer systems such as the IBM\nBlue Gene/Q or Cray XE6 is a challenge because of issues including operating\nsystem limitations, interoperability challenges, parallel filesystem overheads\ndue to the small file system accesses common in scripted approaches, and other\nissues. We present here a new approach to these problems in which the Swift\nscripting system is used to integrate high-level scripts written in Python, R,\nand Tcl, with native code developed in C, C++, and Fortran, by linking Swift to\nthe library interfaces to the script interpreters. In this approach, Swift\nhandles data management, movement, and marshaling among distributed-memory\nprocesses without direct user manipulation of low-level communication libraries\nsuch as MPI. We present a technique to efficiently launch scripted applications\non large-scale supercomputers using a hierarchical programming model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:50:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wozniak", "Justin M.", ""], ["Armstrong", "Timothy G.", ""], ["Maheshwari", "Ketan C.", ""], ["Katz", "Daniel S.", ""], ["Wilde", "Michael", ""], ["Foster", "Ian T.", ""]]}, {"id": "2107.02943", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Choiru Za'in, Edwin Lughofer, Eric Pardede, Dwi A.\n  P. Rahayu", "title": "Scalable Teacher Forcing Network for Semi-Supervised Large Scale Data\n  Streams", "comments": "This paper has been accepted for publication in Information Sciences", "journal-ref": "Information Sciences, 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The large-scale data stream problem refers to high-speed information flow\nwhich cannot be processed in scalable manner under a traditional computing\nplatform. This problem also imposes expensive labelling cost making the\ndeployment of fully supervised algorithms unfeasible. On the other hand, the\nproblem of semi-supervised large-scale data streams is little explored in the\nliterature because most works are designed in the traditional single-node\ncomputing environments while also being fully supervised approaches. This paper\noffers Weakly Supervised Scalable Teacher Forcing Network (WeScatterNet) to\ncope with the scarcity of labelled samples and the large-scale data streams\nsimultaneously. WeScatterNet is crafted under distributed computing platform of\nApache Spark with a data-free model fusion strategy for model compression after\nparallel computing stage. It features an open network structure to address the\nglobal and local drift problems while integrating a data augmentation,\nannotation and auto-correction ($DA^3$) method for handling partially labelled\ndata streams. The performance of WeScatterNet is numerically evaluated in the\nsix large-scale data stream problems with only $25\\%$ label proportions. It\nshows highly competitive performance even if compared with fully supervised\nlearners with $100\\%$ label proportions.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:37:40 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Za'in", "Choiru", ""], ["Lughofer", "Edwin", ""], ["Pardede", "Eric", ""], ["Rahayu", "Dwi A. P.", ""]]}, {"id": "2107.02944", "submitter": "Sachithra Lokuge", "authors": "Chandima Wickramatunga, Darshana Sedera and Sachithra Lokuge", "title": "Do Small Firms Implement Enterprise Systems Differently? The Case of\n  E-Silk Route Ventures", "comments": "Pacific Asia Conference on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost effectiveness, ease of learning, connectedness and in-depth\nanalytical capabilities provided through cloud computing technologies, have\nprovided small firms the opportunity to implement enterprise systems (ES),\nwhich was reserved only for the much resourceful firms. However, it is evident\nthat small firms are still struggling to attain purported benefits of ES and\nstill find it difficult to manage complexities of ES implementations. Further,\nlimited research has been conducted that discusses ES implementation in small\nfirms. This research is an attempt to further the understanding of\nES-implementation of small firms and re-evaluate the applicability of\nfundamental critical success factors to small firms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:25:59 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wickramatunga", "Chandima", ""], ["Sedera", "Darshana", ""], ["Lokuge", "Sachithra", ""]]}, {"id": "2107.02945", "submitter": "Sachithra Lokuge", "authors": "Chandima Wickramatunga and Sachithra Lokuge", "title": "The Implementation of an ES in a Small Firm: The case of Silk\n  Cooperation", "comments": "Pacific Asia Conference on Information Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of cloud computing has provided opportunities for small\nbusinesses to implement enterprise systems (ES) in their organizations and\nthereby improve their business processes. While there have been many studies\nfocusing on ES implementation among medium-large sized firms, the factors that\ninfluence the implementations of ES in such firms are different to that of\nsmall firms. This teaching case discusses an implementation of a cloud\nenterprise resource planning (ERP) system in a small firm in the Asian region.\nThe case illustrates factors that enabled successful implementation of a cloud\nERP system in a small firm and the lessons learnt through this successful\nendeavor. The case study and the teaching notes are suitable for any\nundergraduate or postgraduate cohort, following a course in management\ninformation systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 09:23:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wickramatunga", "Chandima", ""], ["Lokuge", "Sachithra", ""]]}, {"id": "2107.03078", "submitter": "Mohit Garg", "authors": "Mohit Garg, Cian Johnston and M\\'elanie Bouroche", "title": "Can Connected Autonomous Vehicles really improve mixed traffic\n  efficiency in realistic scenarios?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SP eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connected autonomous vehicles (CAVs) can supplement the information from\ntheir own sensors with information from surrounding CAVs for decision making\nand control. This has the potential to improve traffic efficiency. CAVs face\nadditional challenges in their driving, however, when they interact with\nhuman-driven vehicles (HDVs) in mixed-traffic environments due to the\nuncertainty in human's driving behavior e.g. larger reaction times, perception\nerrors, etc. While a lot of research has investigated the impact of CAVs on\ntraffic safety and efficiency at different penetration rates, all have assumed\neither perfect communication or very simple scenarios with imperfect\ncommunication. In practice, the presence of communication delays and packet\nlosses means that CAVs might receive only partial information from surrounding\nvehicles, and this can have detrimental effects on their performance. This\npaper investigates the impact of CAVs on traffic efficiency in realistic\ncommunication and road network scenarios (i.e. imperfect communication and\nlarge-scale road network). We analyze the effect of unreliable communication\nlinks on CAVs operation in mixed traffic with various penetration rates and\nevaluate traffic performance in congested traffic scenarios on a large-scale\nroad network (the M50 motorway, in Ireland). Results show that CAVs can\nsignificantly improve traffic efficiency in congested traffic scenarios at high\npenetration rates. The scale of the improvement depends on communication\nreliability, with a packet drop rate of 70% leading to an increase in traffic\ncongestion by 28.7% and 11.88% at 40% and 70% penetration rates respectively\ncompared to perfect communication.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:52:15 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 18:46:23 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Garg", "Mohit", ""], ["Johnston", "Cian", ""], ["Bouroche", "M\u00e9lanie", ""]]}, {"id": "2107.03248", "submitter": "Venkatesh Venkataramanan", "authors": "Venkatesh Venkataramanan, Sridevi Kaza, and Anuradha M. Annaswamy", "title": "DER Forecast using Privacy Preserving Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.DC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing penetration of Distributed Energy Resources (DERs) in grid\nedge including renewable generation, flexible loads, and storage, accurate\nprediction of distributed generation and consumption at the consumer level\nbecomes important. However, DER prediction based on the transmission of\ncustomer level data, either repeatedly or in large amounts, is not feasible due\nto privacy concerns. In this paper, a distributed machine learning approach,\nFederated Learning, is proposed to carry out DER forecasting using a network of\nIoT nodes, each of which transmits a model of the consumption and generation\npatterns without revealing consumer data. We consider a simulation study which\nincludes 1000 DERs, and show that our method leads to an accurate prediction of\npreserve consumer privacy, while still leading to an accurate forecast. We also\nevaluate grid-specific performance metrics such as load swings and load\ncurtailment and show that our FL algorithm leads to satisfactory performance.\nSimulations are also performed on the Pecan street dataset to demonstrate the\nvalidity of the proposed approach on real data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 14:25:43 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Venkataramanan", "Venkatesh", ""], ["Kaza", "Sridevi", ""], ["Annaswamy", "Anuradha M.", ""]]}, {"id": "2107.03341", "submitter": "Simona Rombo", "authors": "Ylenia Galluzzo, Raffaele Giancarlo, Mario Randazzo, Simona E. Rombo", "title": "Burrows Wheeler Transform on a Large Scale: Algorithms Implemented in\n  Apache Spark", "comments": "11 pages, 2 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2007.10095", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of Next Generation Sequencing (NGS) technologies, large\namounts of \"omics\" data are daily collected and need to be processed. Indexing\nand compressing large sequences datasets are some of the most important tasks\nin this context. Here we propose algorithms for the computation of Burrows\nWheeler transform relying on Big Data technologies, i.e., Apache Spark and\nHadoop. Our algorithms are the first ones that distribute the index computation\nand not only the input dataset, allowing to fully benefit of the available\ncloud resources.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:34:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Galluzzo", "Ylenia", ""], ["Giancarlo", "Raffaele", ""], ["Randazzo", "Mario", ""], ["Rombo", "Simona E.", ""]]}, {"id": "2107.03367", "submitter": "Seyed Moeen Nehzati", "authors": "MohammadAmin Fazli, Seyed Moeen Nehzati, MohammadAmin Salarkia", "title": "Building Stable Off-chain Payment Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Payment channel is a protocol which allows cryptocurrency users to route\nmultiple transactions through network without committing them to the main\nblockchain network (mainnet). This ability makes them the most prominent\nsolution to blockchains' scalability problem. Each modification of payment\nchannels requires a transaction on the mainnet and therefore, big transaction\nfees. In this paper, we assume that a set of payment transactions are given\n(batch or online) and we study the problem of scheduling modificiations on\npayment channels to route all of the transactions with minimum modification\ncost.\n  We investigate two cost models for aforementioned problem: the step cost\nfunction in which every channel modification has a constant cost and the linear\ncost function in which modification costs are proportional to the amount of\nchange. For the step cost function model, we prove impossibility results for\nboth batch and online case. Moreover, some heuristic methods for the batch case\nare presented and compared. For the linear cost we propose a polynomial time\nalgorithm using linear programming for the batch case.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 17:25:01 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fazli", "MohammadAmin", ""], ["Nehzati", "Seyed Moeen", ""], ["Salarkia", "MohammadAmin", ""]]}, {"id": "2107.03467", "submitter": "In Kee Kim", "authors": "Jianwei Hao, Ting Jiang, Wei Wang, In Kee Kim", "title": "An Empirical Analysis of VM Startup Times in Public IaaS Clouds: An\n  Extended Report", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VM startup time is an essential factor in designing elastic cloud\napplications. For example, a cloud application with autoscaling can reduce\nunder- and over-provisioning of VM instances with a precise estimation of VM\nstartup time, and in turn, it is likely to guarantee the application's\nperformance and improve the cost efficiency. However, VM startup time has been\nlittle studied, and available measurement results performed previously did not\nconsider various configurations of VMs for modern cloud applications. In this\nwork, we perform comprehensive measurements and analysis of VM startup time\nfrom two major cloud providers, namely Amazon Web Services (AWS) and Google\nCloud Platform (GCP). With three months of measurements, we collected more than\n300,000 data points from each provider by applying a set of configurations,\nincluding 11+ VM types, four different data center locations, four VM image\nsizes, two OS types, and two purchase models (e.g., spot/preemptible VMs vs.\non-demand VMs). With extensive analysis, we found that VM startup time can vary\nsignificantly because of several important factors, such as VM image sizes,\ndata center locations, VM types, and OS types. Moreover, by comparing with\nprevious measurement results, we confirm that cloud providers (specifically\nAWS) made significant improvements for the VM startup times and currently have\nmuch quicker VM startup times than in the past.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 20:20:49 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hao", "Jianwei", ""], ["Jiang", "Ting", ""], ["Wang", "Wei", ""], ["Kim", "In Kee", ""]]}, {"id": "2107.03492", "submitter": "Eleftherios Kosmas", "authors": "Panagiota Fatourou, Nikolaos D. Kallimanis, Eleftherios Kosmas", "title": "Persistent Software Combining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the performance power of software combining in designing persistent\nalgorithms and data structures. We present Bcomb, a new blocking\nhighly-efficient combining protocol, and built upon it to get PBcomb, a\npersistent version of it that performs a small number of persistence\ninstructions and exhibits low synchronization cost. We built fundamental\nrecoverable data structures, such as stacks and queues based on PBcomb, as well\nas on PWFcomb, a wait-free universal construction we present. Our experiments\nshow that PBcomb and PWFcomb outperform by far state-of-the-art recoverable\nuniversal constructions and transactional memory systems, many of which ensure\nweaker consistency properties than our algorithms. We built recoverable queues\nand stacks, based on PBcomb and PWFcomb, and present experiments to show that\nthey have much better performance than previous recoverable implementations of\nstacks and queues. We build the first recoverable implementation of a\nconcurrent heap and present experiments to show that it has good performance\nwhen the size of the heap is not very large.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:40:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 13:33:30 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Fatourou", "Panagiota", ""], ["Kallimanis", "Nikolaos D.", ""], ["Kosmas", "Eleftherios", ""]]}, {"id": "2107.03653", "submitter": "Nikhil Pratap Ghanathe", "authors": "Nikhil Pratap Ghanathe, Vivek Seshadri, Rahul Sharma, Steve Wilton,\n  Aayan Kumar", "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications", "comments": "Accepted at The International Conference on Field-Programmable Logic\n  and Applications (FPL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:38:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ghanathe", "Nikhil Pratap", ""], ["Seshadri", "Vivek", ""], ["Sharma", "Rahul", ""], ["Wilton", "Steve", ""], ["Kumar", "Aayan", ""]]}, {"id": "2107.03774", "submitter": "Jos\\'e Cano", "authors": "Martina Lofqvist, Jos\\'e Cano", "title": "Optimizing Data Processing in Space for Object Detection in Satellite\n  Imagery", "comments": "Published as a workshop paper at SmallSat 2021 - The 35th Annual\n  Small Satellite Conference. 9 pages, 10 figures. arXiv admin note: text\n  overlap with arXiv:2007.11089", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:37:24 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lofqvist", "Martina", ""], ["Cano", "Jos\u00e9", ""]]}, {"id": "2107.03882", "submitter": "Dimuthu Wannipurage", "authors": "Dimuthu Wannipurage, Isuru Ranawaka, Eroma Abeysinghe, Marcus\n  Christie, Suresh Marru and Marlon Pierce", "title": "A Multi-Protocol, Secure, and Dynamic Data Storage Integration\n  Frameworkfor Multi-tenanted Science Gateway Middleware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Science gateways are user-centric, end-to-end cyberinfrastructure for\nmanaging scientific data and executions of computational software on\ndistributed resources. In order to simplify the creation and management of\nscience gateways, we have pursued a multi-tenanted, platform-as-a-service\napproach that allows multiple gateway front-ends (portals) to be integrated\nwith a consolidated middleware that manages the movement of data and the\nexecution of workflows on multiple back-end scientific computing resources. An\nimportant challenge for this approach is to provide an end-to-end data movement\nand management solution that allows gateway users to integrate their own data\nstores with the gateway platform. These user-provided data stores may include\ncommercial cloud-based object store systems, third-party data stores accessed\nthrough APIs such as REST endpoints, and users' own local storage resources. In\nthis paper, we present a solution design and implementation based on the\nintegration of a managed file transfer (MFT) service (Airavata MFT) into the\nplatform.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:59:46 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Wannipurage", "Dimuthu", ""], ["Ranawaka", "Isuru", ""], ["Abeysinghe", "Eroma", ""], ["Christie", "Marcus", ""], ["Marru", "Suresh", ""], ["Pierce", "Marlon", ""]]}, {"id": "2107.03947", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Frank W\\\"urthwein, Thomas DeFanti and John Graham", "title": "HTCondor data movement at 100 Gbps", "comments": "2 pages, 2 figures, to be published in proceedings of eScience 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  HTCondor is a major workload management system used in distributed high\nthroughput computing (dHTC) environments, e.g., the Open Science Grid. One of\nthe distinguishing features of HTCondor is the native support for data\nmovement, allowing it to operate without a shared filesystem. Coupling data\nhandling and compute scheduling is both convenient for users and allows for\nsignificant infrastructure flexibility but does introduce some limitations. The\ndefault HTCondor data transfer mechanism routes both the input and output data\nthrough the submission node, making it a potential bottleneck. In this document\nwe show that by using a node equipped with a 100 Gbps network interface (NIC)\nHTCondor can serve data at up to 90 Gbps, which is sufficient for most current\nuse cases, as it would saturate the border network links of most research\nuniversities at the time of writing.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:18:20 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Sfiligoi", "Igor", ""], ["W\u00fcrthwein", "Frank", ""], ["DeFanti", "Thomas", ""], ["Graham", "John", ""]]}, {"id": "2107.03963", "submitter": "Igor Sfiligoi", "authors": "Igor Sfiligoi, Shava Smallen, Frank W\\\"urthwein, Nicole Wolter, David\n  Schultz and Benedikt Riedel", "title": "Expanding IceCube GPU computing into the Clouds", "comments": "2 pages, 2 figures, to be published in proceedings of eScience 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The IceCube collaboration relies on GPU compute for many of its needs,\nincluding ray tracing simulation and machine learning activities. GPUs are\nhowever still a relatively scarce commodity in the scientific resource provider\ncommunity, so we expanded the available resource pool with GPUs provisioned\nfrom the commercial Cloud providers. The provisioned resources were fully\nintegrated into the normal IceCube workload management system through the Open\nScience Grid (OSG) infrastructure and used CloudBank for budget management. The\nresult was an approximate doubling of GPU wall hours used by IceCube over a\nperiod of 2 weeks, adding over 3.1 fp32 EFLOP hours for a price tag of about\n$58k. This paper describes the setup used and the operational experience.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:42:27 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Sfiligoi", "Igor", ""], ["Smallen", "Shava", ""], ["W\u00fcrthwein", "Frank", ""], ["Wolter", "Nicole", ""], ["Schultz", "David", ""], ["Riedel", "Benedikt", ""]]}, {"id": "2107.04092", "submitter": "Dennis Bautembach", "authors": "Dennis Bautembach, Iason Oikonomidis, Antonis Argyros", "title": "Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared\n  Atomics", "comments": "Submitted to IEEE-HPEC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two novel optimizations that accelerate clock-based spiking neural\nnetwork (SNN) simulators. The first one targets spike timing dependent\nplasticity (STDP). It combines lazy- with event-driven plasticity and\nefficiently facilitates the computation of pre- and post-synaptic spikes using\nbitfields and integer intrinsics. It offers higher bandwidth than event-driven\nplasticity alone and achieves a 1.5x-2x speedup over our closest competitor.\nThe second optimization targets spike delivery. We partition our graph\nrepresentation in a way that bounds the number of neurons that need be updated\nat any given time which allows us to perform said update in shared memory\ninstead of global memory. This is 2x-2.5x faster than our closest competitor.\nBoth optimizations represent the final evolutionary stages of years of\niteration on STDP and spike delivery inside \"Spice\" (/spaIk/), our state of the\nart SNN simulator. The proposed optimizations are not exclusive to our graph\nrepresentation or pipeline but are applicable to a multitude of simulator\ndesigns. We evaluate our performance on three well-established models and\ncompare ourselves against three other state of the art simulators.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 20:13:54 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Bautembach", "Dennis", ""], ["Oikonomidis", "Iason", ""], ["Argyros", "Antonis", ""]]}, {"id": "2107.04117", "submitter": "Evangelos Pournaras", "authors": "Evangelos Pournaras, Atif Nabi Ghulam, Renato Kunz, Regula H\\\"anggli", "title": "Crowd Sensing and Living Lab Outdoor Experimentation Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor `living lab' experimentation using pervasive computing provides new\nopportunities: higher realism, external validity and large-scale\nsocio-spatio-temporal observations. However, experimentation `in the wild' is\nhighly complex and costly. Noise, biases, privacy concerns to comply with\nstandards of ethical review boards, remote moderation, control of experimental\nconditions and equipment perplex the collection of high-quality data for causal\ninference. This article introduces Smart Agora, a novel open-source software\nplatform for rigorous systematic outdoor experimentation. Without writing a\nsingle line of code, highly complex experimental scenarios are visually\ndesigned and automatically deployed to smart phones. Novel geolocated survey\nand sensor data are collected subject of participants verifying desired\nexperimental conditions, for instance. their presence at certain urban spots.\nThis new approach drastically improves the quality and purposefulness of crowd\nsensing, tailored to conditions that confirm/reject hypotheses. The features\nthat support this innovative functionality and the broad spectrum of its\napplicability are demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:49:32 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Pournaras", "Evangelos", ""], ["Ghulam", "Atif Nabi", ""], ["Kunz", "Renato", ""], ["H\u00e4nggli", "Regula", ""]]}, {"id": "2107.04129", "submitter": "Bo Liu", "authors": "Bo Liu, Chaowei Tan, Jiazhou Wang, Tao Zeng, Huasong Shan, Houpu Yao,\n  Huang Heng, Peng Dai, Liefeng Bo, Yanqing Chen", "title": "Fedlearn-Algo: A flexible open-source privacy-preserving machine\n  learning platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present Fedlearn-Algo, an open-source privacy preserving\nmachine learning platform. We use this platform to demonstrate our research and\ndevelopment results on privacy preserving machine learning algorithms. As the\nfirst batch of novel FL algorithm examples, we release vertical federated\nkernel binary classification model and vertical federated random forest model.\nThey have been tested to be more efficient than existing vertical federated\nlearning models in our practice. Besides the novel FL algorithm examples, we\nalso release a machine communication module. The uniform data transfer\ninterface supports transfering widely used data formats between machines. We\nwill maintain this platform by adding more functional modules and algorithm\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:59:56 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Liu", "Bo", ""], ["Tan", "Chaowei", ""], ["Wang", "Jiazhou", ""], ["Zeng", "Tao", ""], ["Shan", "Huasong", ""], ["Yao", "Houpu", ""], ["Heng", "Huang", ""], ["Dai", "Peng", ""], ["Bo", "Liefeng", ""], ["Chen", "Yanqing", ""]]}, {"id": "2107.04172", "submitter": "Isuru Ranawaka Mr", "authors": "Isuru Ranawaka, Samitha Liyanage, Dannon Baker, Alexandru Mahmoud,\n  Juleen Graham, Terry Fleury, Dimuthu Wannipurage, Yu Ma, Enis Afgan, Jim\n  Basney, Suresh Marru, Marlon Pierce", "title": "Experiences with Integrating Custos SecurityServices", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science gateways are user-facing cyberinfrastruc-ture that provide\nresearchers and educators with Web-basedaccess to scientific software,\ncomputing, and data resources.Managing user identities, accounts, and\npermissions are essentialtasks for science gateways, and gateways likewise must\nman-age secure connections between their middleware and remoteresources. The\nCustos project is an effort to build open sourcesoftware that can be operated\nas a multi-tenanted service thatprovides reliable implementations of common\nscience gatewaycybersecurity needs, including federated authentication,\niden-tity management, group and authorization management, andresource\ncredential management. Custos aims further to provideintegrated solutions\nthrough these capabilities, delivering end-to-end support for several science\ngateway usage scenarios. Thispaper examines four deployment scenarios using\nCustos andassociated extensions beyond previously described work. Thefirst\ncapability illustrated by these scenarios is the need forCustos to provide\nhierarchical tenant management that allowsmultiple gateway deployments to be\nfederated together andalso to support consolidated, hosted science gateway\nplatformservices. The second capability illustrated by these scenarios is\ntheneed to support service accounts that can support non-browserapplications\nand agent applications that can act on behalf ofusers on edge resources. We\nillustrate how the latter can be builtusing Web security standards combined\nwith Custos permissionmanagement mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 01:46:55 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Ranawaka", "Isuru", ""], ["Liyanage", "Samitha", ""], ["Baker", "Dannon", ""], ["Mahmoud", "Alexandru", ""], ["Graham", "Juleen", ""], ["Fleury", "Terry", ""], ["Wannipurage", "Dimuthu", ""], ["Ma", "Yu", ""], ["Afgan", "Enis", ""], ["Basney", "Jim", ""], ["Marru", "Suresh", ""], ["Pierce", "Marlon", ""]]}, {"id": "2107.04271", "submitter": "Blesson Varghese", "authors": "Di Wu and Rehmat Ullah and Paul Harvey and Peter Kilpatrick and Ivor\n  Spence and Blesson Varghese", "title": "FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Applying Federated Learning (FL) on Internet-of-Things devices is\nnecessitated by the large volumes of data they produce and growing concerns of\ndata privacy. However, there are three challenges that need to be addressed to\nmake FL efficient: (i) execute on devices with limited computational\ncapabilities, (ii) account for stragglers due to computational heterogeneity of\ndevices, and (iii) adapt to the changing network bandwidths. This paper\npresents FedAdapt, an adaptive offloading FL framework to mitigate the\naforementioned challenges. FedAdapt accelerates local training in\ncomputationally constrained devices by leveraging layer offloading of deep\nneural networks (DNNs) to servers. Further, FedAdapt adopts reinforcement\nlearning-based optimization and clustering to adaptively identify which layers\nof the DNN should be offloaded for each individual device on to a server to\ntackle the challenges of computational heterogeneity and changing network\nbandwidth. Experimental studies are carried out on a lab-based testbed\ncomprising five IoT devices. By offloading a DNN from the device to the server\nFedAdapt reduces the training time of a typical IoT device by over half\ncompared to classic FL. The training time of extreme stragglers and the overall\ntraining time can be reduced by up to 57%. Furthermore, with changing network\nbandwidth, FedAdapt is demonstrated to reduce the training time by up to 40%\nwhen compared to classic FL, without sacrificing accuracy. FedAdapt can be\ndownloaded from https://github.com/qub-blesson/FedAdapt.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 07:29:55 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wu", "Di", ""], ["Ullah", "Rehmat", ""], ["Harvey", "Paul", ""], ["Kilpatrick", "Peter", ""], ["Spence", "Ivor", ""], ["Varghese", "Blesson", ""]]}, {"id": "2107.04409", "submitter": "Raphael Cohen", "authors": "Raphael Y. Cohen, Aaron D. Sodickson", "title": "An Orchestration Platform that Puts Radiologists in the Driver's Seat of\n  AI Innovation: A Methodological Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.DC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current AI-driven research in radiology requires resources and expertise that\nare often inaccessible to small and resource-limited labs. The clinicians who\nare able to participate in AI research are frequently well-funded,\nwell-staffed, and either have significant experience with AI and computing, or\nhave access to colleagues or facilities that do. Current imaging data is\nclinician-oriented and is not easily amenable to machine learning initiatives,\nresulting in inefficient, time consuming, and costly efforts that rely upon a\ncrew of data engineers and machine learning scientists, and all too often\npreclude radiologists from driving AI research and innovation. We present the\nsystem and methodology we have developed to address infrastructure and platform\nneeds, while reducing the staffing and resource barriers to entry. We emphasize\na data-first and modular approach that streamlines the AI development and\ndeployment process while providing efficient and familiar interfaces for\nradiologists, such that they can be the drivers of new AI innovations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:32:14 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Cohen", "Raphael Y.", ""], ["Sodickson", "Aaron D.", ""]]}, {"id": "2107.04648", "submitter": "Marwan Dhuheir", "authors": "Marwan Dhuheir, Emna Baccour, Aiman Erbad, Sinan Sabeeh, Mounir Hamdi", "title": "Efficient Real-Time Image Recognition Using Collaborative Swarm of UAVs\n  and Convolutional Networks", "comments": "conference paper accepted and presented at 17th Int. Wireless\n  Communications & Mobile Computing Conference - IWCMC 2021, Harbin, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned Aerial Vehicles (UAVs) have recently attracted significant attention\ndue to their outstanding ability to be used in different sectors and serve in\ndifficult and dangerous areas. Moreover, the advancements in computer vision\nand artificial intelligence have increased the use of UAVs in various\napplications and solutions, such as forest fires detection and borders\nmonitoring. However, using deep neural networks (DNNs) with UAVs introduces\nseveral challenges of processing deeper networks and complex models, which\nrestricts their on-board computation. In this work, we present a strategy\naiming at distributing inference requests to a swarm of resource-constrained\nUAVs that classifies captured images on-board and finds the minimum\ndecision-making latency. We formulate the model as an optimization problem that\nminimizes the latency between acquiring images and making the final decisions.\nThe formulated optimization solution is an NP-hard problem. Hence it is not\nadequate for online resource allocation. Therefore, we introduce an online\nheuristic solution, namely DistInference, to find the layers placement strategy\nthat gives the best latency among the available UAVs. The proposed approach is\ngeneral enough to be used for different low decision-latency applications as\nwell as for all CNN types organized into the pipeline of layers (e.g., VGG) or\nbased on residual blocks (e.g., ResNet).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:47:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dhuheir", "Marwan", ""], ["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Sabeeh", "Sinan", ""], ["Hamdi", "Mounir", ""]]}, {"id": "2107.04748", "submitter": "Jiaming Cheng Mr", "authors": "Jiaming Cheng, Duong Tung Nguyen and Vijay K. Bhargava", "title": "Resilient Edge Service Placement and Workload Allocation under\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an optimal service placement and workload allocation\nproblem for a service provider (SP), who can procure resources from numerous\nedge nodes to serve its users.The SP aims to improve the user experience while\nminimizing its cost, considering various system uncertainties. To tackle this\nchallenging problem, we propose a novel resilience-aware edge service placement\nand workload allocation model that jointly captures the uncertainties of\nresource demand and node failures. The first-stage decisions include the\noptimal service placement and resource procurement, while the optimal workload\nreallocation is determined in the second stage after the uncertainties are\ndisclosed. The salient feature of the proposed model is that it produces a\nplacement and procurement solution that is robust against any possible\nrealization of the uncertainties. By leveraging the column-and-constraint\ngeneration method, we introduce two iterative algorithms that can converge to\nan exact optimal solution within a finite number of iterations. We further\nsuggest an affine decision rule approximation approach for solving large-scale\nproblem instances in a reasonable time. Extensive numerical results are shown\nto demonstrate the advantages of the proposed model and solutions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 03:39:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Cheng", "Jiaming", ""], ["Nguyen", "Duong Tung", ""], ["Bhargava", "Vijay K.", ""]]}, {"id": "2107.04885", "submitter": "Debasish Pattanayak", "authors": "Sai Vamshi Samala, Subhajit Pramanick, Debasish Pattanayak, and Partha\n  Sarathi Mandal", "title": "Filling MIS Vertices by Myopic Luminous Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the problem of finding a maximal independent set (MIS) (named as\n\\emph{MIS Filling problem}) of an arbitrary connected graph having $n$ vertices\nwith luminous myopic mobile robots. The robots enter the graph one after\nanother from a particular vertex called the \\emph{Door} and disperse along the\nedges of the graph without collision to occupy vertices such that the set of\nvertices occupied by the robots is a maximal independent set. We assume the\nrobots have knowledge only about the maximum degree of the graph, denoted by\n$\\Delta$.\n  In this paper, we explore two versions of the problem: the solution to the\nfirst version, named as \\emph{MIS Filling with Single Door}, works under an\nasynchronous scheduler using robots with 3 hops of visibility range, $\\Delta +\n6$ number of colors and $O(\\log \\Delta)$ bits of persistent storage. The time\ncomplexity is measured in terms of epochs and it can be solved in $O(n^2)$\nepochs. An epoch is the smallest time interval in which each participating\nrobot gets activated and executes the algorithm at least once. For the second\nversion with $k~ ( > 1)$ \\textit{Doors}, named as \\emph{MIS Filling with\nMultiple Doors}, the solution works under a semi-synchronous scheduler using\nrobots with 5 hops of visibility range, $\\Delta + k + 6$ number of colors and\n$O(\\log (\\Delta + k))$ bits of persistent storage. The problem with multiple\nDoors can be solved in $O(n^2)$ epochs.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 18:12:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Samala", "Sai Vamshi", ""], ["Pramanick", "Subhajit", ""], ["Pattanayak", "Debasish", ""], ["Mandal", "Partha Sarathi", ""]]}, {"id": "2107.04904", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov, Qiben Yan and Qingyang Wang", "title": "Blockumulus: A Scalable Framework for Smart Contracts on the Cloud", "comments": "41st IEEE International Conference on Distributed Computing Systems\n  (ICDCS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public blockchains have spurred the growing popularity of decentralized\ntransactions and smart contracts, but they exhibit limitations on the\ntransaction throughput, storage, and computation. To avoid transaction\ngridlock, public blockchains impose large fees and per-block resource limits,\nmaking it difficult to accommodate the ever-growing transaction demand.\nPrevious research endeavors to improve the scalability of blockchain through\nvarious technologies, such as side-chaining, sharding, secured off-chain\ncomputation, communication network optimizations, and efficient consensus\nprotocols. However, these approaches have not attained a widespread adoption\ndue to their inability in delivering a cloud-like performance, in terms of the\nscalability in transaction throughput, storage, and compute capacity. In this\nwork, we determine that the major obstacle to public blockchain scalability is\ntheir underlying unstructured P2P networks. We further show that a centralized\nnetwork can support the deployment of decentralized smart contracts. We propose\na novel approach for achieving scalable decentralization: instead of trying to\nmake blockchain scalable, we deliver decentralization to already scalable cloud\nby using an Ethereum smart contract. We introduce Blockumulus, a framework that\ncan deploy decentralized cloud smart contract environments using a novel\ntechnique called overlay consensus. Through experiments, we demonstrate that\nBlockumulus is scalable in all three dimensions: computation, data storage, and\ntransaction throughput. Besides eliminating the current code execution and\nstorage restrictions, Blockumulus delivers a transaction latency between 2 and\n5 seconds under normal load. Moreover, the stress test of our prototype reveals\nthe ability to execute 20,000 simultaneous transactions under 26 seconds, which\nis on par with the average throughput of worldwide credit card transactions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 20:18:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Yan", "Qiben", ""], ["Wang", "Qingyang", ""]]}, {"id": "2107.04947", "submitter": "Jianyu Niu", "authors": "Jianyu Niu, Fangyu Gai, Mohammad M. Jalalzai, and Chen Feng", "title": "On the Performance of Pipelined HotStuff", "comments": "IEEE International Conference on Computer Communications (INFOCOM'\n  21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  HotStuff is a state-of-the-art Byzantine fault-tolerant consensus protocol.\nIt can be pipelined to build large-scale blockchains. One of its variants\ncalled LibraBFT is adopted in Facebook's Libra blockchain. Although it is well\nknown that pipelined HotStuff is secure against up to $1/3$ of Byzantine nodes,\nits performance in terms of throughput and delay is still under-explored. In\nthis paper, we develop a multi-metric evaluation framework to quantitatively\nanalyze pipelined \\mbox{HotStuff's performance} with respect to its chain\ngrowth rate, chain quality, and latency. We then propose two attack strategies\nand evaluate their effects on the performance of pipelined HotStuff. Our\nanalysis shows that the chain growth rate (resp, chain quality) of pipelined\nHotStuff under our attacks can drop to as low as 4/9 (resp, 12/17) of that\nwithout attacks when $1/3$ nodes are Byzantine. As another application, we use\nour framework to evaluate certain engineering optimizations adopted by\nLibraBFT. We find that these optimizations make the system more vulnerable to\nour attacks than the original pipelined HotStuff. Finally, we provide two\ncountermeasures to thwart these attacks. We hope that our studies can shed\nlight on the rigorous understanding of the state-of-the-art pipelined HotStuff\nprotocol as well as its variants.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 02:40:38 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Niu", "Jianyu", ""], ["Gai", "Fangyu", ""], ["Jalalzai", "Mohammad M.", ""], ["Feng", "Chen", ""]]}, {"id": "2107.05252", "submitter": "Jiacheng Liang", "authors": "Jiacheng Liang, Wensi Jiang and Songze Li", "title": "OmniLytics: A Blockchain-based Secure Data Market for Decentralized\n  Machine Learning", "comments": "12 pages,5 figures, accepted by International Workshop on Federated\n  Learning for User Privacy and Data Confidentiality in Conjunction with ICML\n  2021(http://federated-learning.org/fl-icml-2021/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose OmniLytics, a blockchain-based secure data trading marketplace for\nmachine learning applications. Utilizing OmniLytics, many distributed data\nowners can contribute their private data to collectively train a ML model\nrequested by some model owners, and get compensated for data contribution.\nOmniLytics enables such model training while simultaneously providing 1) model\nsecurity against curious data owners; 2) data security against curious model\nand data owners; 3) resilience to malicious data owners who provide faulty\nresults to poison model training; and 4) resilience to malicious model owner\nwho intents to evade the payment. OmniLytics is implemented as a smart contract\non the Ethereum blockchain to guarantee the atomicity of payment. In\nOmniLytics, a model owner publishes encrypted initial model on the contract,\nover which the participating data owners compute gradients using their private\ndata, and securely aggregate the gradients through the contract. Finally, the\ncontract reimburses the data owners, and the model owner decrypts the\naggregated model update. We implement a working prototype of OmniLytics on\nEthereum, and perform extensive experiments to measure its gas cost and\nexecution time under various parameter combinations, demonstrating its high\ncomputation and cost efficiency and strong practicality.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 08:28:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Liang", "Jiacheng", ""], ["Jiang", "Wensi", ""], ["Li", "Songze", ""]]}, {"id": "2107.05395", "submitter": "Enda Carroll", "authors": "Enda Carroll, Andrew Gloster, Miguel D. Bustamante, Lennon \\'O'\n  N\\'araigh", "title": "A Batched GPU Methodology for Numerical Solutions of Partial\n  Differential Equations", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.04539", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a methodology for data accesses when solving batches\nof Tridiagonal and Pentadiagonal matrices that all share the same\nleft-hand-side (LHS) matrix. The intended application is to the numerical\nsolution of Partial Differential Equations via the finite-difference method,\nalthough the methodology is applicable more broadly. By only storing one copy\nof this matrix, a significant reduction in storage overheads is obtained,\ntogether with a corresponding decrease in compute time. Taken together, these\ntwo performance enhancements lead to an overall more efficient implementation\nover the current state of the art algorithms cuThomasBatch and cuPentBatch,\nallowing for a greater number of systems to be solved on a single GPU. We\ndemonstrate the methodology in the case of the Diffusion Equation,\nHyperdiffusion Equation, and the Cahn--Hilliard Equation, all in one spatial\ndimension. In this last example, we demonstrate how the method can be used to\nperform $2^{20}$ independent simulations of phase separation in one dimension.\nIn this way, we build up a robust statistical description of the coarsening\nphenomenon which is the defining behavior of phase separation. We anticipate\nthat the method will be of further use in other similar contexts requiring\nstatistical simulation of physical systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:41:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Carroll", "Enda", ""], ["Gloster", "Andrew", ""], ["Bustamante", "Miguel D.", ""], ["N\u00e1raigh", "Lennon \u00d3'", ""]]}, {"id": "2107.05473", "submitter": "Hung-Wei Tseng", "authors": "Kuan-Chieh Hsu and Hung-Wei Tseng", "title": "GPTPU: Accelerating Applications using Edge Tensor Processing Units", "comments": "This paper is a pre-print of a paper in the 2021 SC, the\n  International Conference for High Performance Computing, Networking, Storage\n  and Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network (NN) accelerators have been integrated into a wide-spectrum of\ncomputer systems to accommodate the rapidly growing demands for artificial\nintelligence (AI) and machine learning (ML) applications. NN accelerators share\nthe idea of providing native hardware support for operations on\nmultidimensional tensor data. Therefore, NN accelerators are theoretically\ntensor processors that can improve system performance for any problem that uses\ntensors as inputs/outputs. Unfortunately, commercially available NN\naccelerators only expose computation capabilities through AI/ML-specific\ninterfaces. Furthermore, NN accelerators reveal very few hardware design\ndetails, so applications cannot easily leverage the tensor operations NN\naccelerators provide.\n  This paper introduces General-Purpose Computing on Edge Tensor Processing\nUnits (GPTPU), an open-source, open-architecture framework that allows the\ndeveloper and research communities to discover opportunities that NN\naccelerators enable for applications. GPTPU includes a powerful programming\ninterface with efficient runtime system-level support -- similar to that of\nCUDA/OpenCL in GPGPU computing -- to bridge the gap between application demands\nand mismatched hardware/software interfaces.\n  We built GPTPU machine uses Edge Tensor Processing Units (Edge TPUs), which\nare widely available and representative of many commercial NN accelerators. We\nidentified several novel use cases and revisited the algorithms. By leveraging\nthe underlying Edge TPUs to perform tensor-algorithm-based compute kernels, our\nresults reveal that GPTPU can achieve a 2.46x speedup over high-end CPUs and\nreduce energy consumption by 40%.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:03:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 06:04:21 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Hsu", "Kuan-Chieh", ""], ["Tseng", "Hung-Wei", ""]]}, {"id": "2107.05516", "submitter": "Sri Raj Paul", "authors": "Sri Raj Paul, Akihiro Hayashi, Kun Chen, Vivek Sarkar", "title": "A Scalable Actor-based Programming System for PGAS Runtimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PGAS runtimes are well suited to irregular applications due to their support\nfor short, one-sided messages. However, there are two major sources of overhead\nin PGAS runtimes that prevent them from achieving acceptable performance on\nlarge scale parallel systems. First, despite the availability of APIs that\nsupport non-blocking operations for important special cases, many PGAS\noperations on remote locations are synchronous by default, which can lead to\nlong-latency stalls. Second, efficient inter-node communication requires\ncareful aggregation and management of short messages. Experiments has shown\nthat the performance of PGAS programs can be improved by more than 20$\\times$\nthrough the use of specialized lower-level libraries, such as Conveyors, but\nwith a significant impact on programming productivity.\n  The actor model has been gaining popularity in many modern programming\nlanguages such as Scala or Rust and also within the cloud computing community.\nIn this paper, we introduce a new programming system for PGAS runtimes, in\nwhich all remote operations are asynchronous by default through the use of an\nactor-based programming system. In this approach, the programmer does not need\nto worry about complexities related to message aggregation and termination\ndetection. Thus, our approach offers a desirable point in the\nproductivity-performance spectrum, with scalable performance that approaches\nthat of lower-level aggregation libraries but with higher productivity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 15:45:01 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Paul", "Sri Raj", ""], ["Hayashi", "Akihiro", ""], ["Chen", "Kun", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2107.05561", "submitter": "Sudeep Pasricha", "authors": "Vipin K. Kukkala, Sooryaa V. Thiruloga, Sudeep Pasricha", "title": "LATTE: LSTM Self-Attention based Anomaly Detection in Embedded\n  Automotive Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern vehicles can be thought of as complex distributed embedded systems\nthat run a variety of automotive applications with real-time constraints.\nRecent advances in the automotive industry towards greater autonomy are driving\nvehicles to be increasingly connected with various external systems (e.g.,\nroadside beacons, other vehicles), which makes emerging vehicles highly\nvulnerable to cyber-attacks. Additionally, the increased complexity of\nautomotive applications and the in-vehicle networks results in poor attack\nvisibility, which makes detecting such attacks particularly challenging in\nautomotive systems. In this work, we present a novel anomaly detection\nframework called LATTE to detect cyber-attacks in Controller Area Network (CAN)\nbased networks within automotive platforms. Our proposed LATTE framework uses a\nstacked Long Short Term Memory (LSTM) predictor network with novel attention\nmechanisms to learn the normal operating behavior at design time. Subsequently,\na novel detection scheme (also trained at design time) is used to detect\nvarious cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE\nframework under different automotive attack scenarios and present a detailed\ncomparison with the best-known prior works in this area, to demonstrate the\npotential of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:32:47 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kukkala", "Vipin K.", ""], ["Thiruloga", "Sooryaa V.", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2107.05791", "submitter": "Hunza Zainab", "authors": "Hunza Zainab, Giorgio Audrito, Soura Dasgupta and Jacob Beal", "title": "Monotonic Filtering for Distributed Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed data collection is a fundamental task in open systems. In such\nnetworks, data is aggregated across a network to produce a single aggregated\nresult at a source device. Though self-stabilizing, algorithms performing data\ncollection can produce large overestimates in the transient phase. For example,\nin [1] we demonstrated that in a line graph, a switch of sources after initial\nstabilization may produce overestimates that are quadratic in the network\ndiameter. We also proposed monotonic filtering as a strategy for removing such\nlarge overestimates. Monotonic filtering prevents the transfer of data from\ndevice A to device B unless the distance estimate at A is more than that at B\nat the previous iteration. For a line graph, [1] shows that monotonic filtering\nprevents quadratic overestimates. This paper analyzes monotonic filtering for\nan arbitrary graph topology, showing that for an N device network, the largest\noverestimate after switching sources is at most 2N.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:32:36 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zainab", "Hunza", ""], ["Audrito", "Giorgio", ""], ["Dasgupta", "Soura", ""], ["Beal", "Jacob", ""]]}, {"id": "2107.05793", "submitter": "S M Ferdous", "authors": "S M Ferdous, Alex Pothen, Arif Khan, Ajay Panyala, Mahantesh\n  Halappanavar", "title": "A Parallel Approximation Algorithm for Maximizing Submodular\n  $b$-Matching", "comments": "10 pages, accepted for SIAM ACDA 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We design new serial and parallel approximation algorithms for computing a\nmaximum weight $b$-matching in an edge-weighted graph with a submodular\nobjective function. This problem is NP-hard; the new algorithms have\napproximation ratio $1/3$, and are relaxations of the Greedy algorithm that\nrely only on local information in the graph, making them parallelizable. We\nhave designed and implemented Local Lazy Greedy algorithms for both serial and\nparallel computers. We have applied the approximate submodular $b$-matching\nalgorithm to assign tasks to processors in the computation of Fock matrices in\nquantum chemistry on parallel computers. The assignment seeks to reduce the run\ntime by balancing the computational load on the processors and bounding the\nnumber of messages that each processor sends. We show that the new assignment\nof tasks to processors provides a four fold speedup over the currently used\nassignment in the NWChemEx software on $8000$ processors on the Summit\nsupercomputer at Oak Ridge National Lab.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 00:35:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ferdous", "S M", ""], ["Pothen", "Alex", ""], ["Khan", "Arif", ""], ["Panyala", "Ajay", ""], ["Halappanavar", "Mahantesh", ""]]}, {"id": "2107.05951", "submitter": "Aleksandr Beznosikov", "authors": "Ivan Stepanov, Artyom Voronov, Aleksandr Beznosikov, Alexander\n  Gasnikov", "title": "One-Point Gradient-Free Methods for Composite Optimization with\n  Applications to Distributed Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is devoted to solving the composite optimization problem with the\nmixture oracle: for the smooth part of the problem, we have access to the\ngradient, and for the non-smooth part, only to the one-point zero-order oracle.\nWe present a method based on the sliding algorithm. Our method allows to\nseparate the oracle complexities and compute the gradient for one of the\nfunction as rarely as possible. The paper also examines the applicability of\nthis method to the problems of distributed optimization and federated learning.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:51:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Stepanov", "Ivan", ""], ["Voronov", "Artyom", ""], ["Beznosikov", "Aleksandr", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2107.06005", "submitter": "Niloofar Gholipour", "authors": "Niloofar Gholipour, Ehsan Arianyan, Rajkumar Buyya", "title": "Recent Advances in Energy Efficient Resource Management Techniques in\n  Cloud Computing Environments", "comments": "29 pages", "journal-ref": "New Frontiers in IoT and Cloud Computing for Post-COVID-19, 2021", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays cloud computing adoption as a form of hosted application and\nservices is widespread due to decreasing costs of hardware, software, and\nmaintenance. Cloud enables access to a shared pool of virtual resources hosted\nin large energy-hungry data centers for diverse information and communication\nservices with dynamic workloads. The huge energy consumption of cloud data\ncenters results in high electricity bills as well as emission of a large amount\nof carbon dioxide gas. Needless to say, efficient resource management in cloud\nenvironments has become one of the most important priorities of cloud providers\nand consequently has increased the interest of researchers to propose novel\nenergy saving solutions. This chapter presents a scientific and taxonomic\nsurvey of recent energy efficient cloud resource management' solutions in cloud\nenvironments. The main objective of this study is to propose a novel complete\ntaxonomy for energy-efficient cloud resource management solutions, review\nrecent research advancements in this area, classify the existing techniques\nbased on our proposed taxonomy, and open up new research directions. Besides,\nit reviews and surveys the literature in the range of 2015 through 2021 in the\nsubject of energy-efficient cloud resource management techniques and maps them\nto its proposed taxonomy, which unveils novel research directions and\nfacilitates the conduction of future researches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:55:06 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gholipour", "Niloofar", ""], ["Arianyan", "Ehsan", ""], ["Buyya", "Rajkumar", ""]]}, {"id": "2107.06108", "submitter": "Franz Poeschel", "authors": "Franz Poeschel and Juncheng E and William F. Godoy and Norbert\n  Podhorszki and Scott Klasky and Greg Eisenhauer and Philip E. Davis and\n  Lipeng Wan and Ana Gainaru and Junmin Gu and Fabian Koller and Ren\\'e Widera\n  and Michael Bussmann and Axel Huebl", "title": "Transitioning from file-based HPC workflows to streaming data pipelines\n  with openPMD and ADIOS2", "comments": "18 pages, 9 figures, SMC2021, supplementary material at\n  https://zenodo.org/record/4906276", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to create a transition path from file-based IO to\nstreaming-based workflows for scientific applications in an HPC environment. By\nusing the openPMP-api, traditional workflows limited by filesystem bottlenecks\ncan be overcome and flexibly extended for in situ analysis. The openPMD-api is\na library for the description of scientific data according to the Open Standard\nfor Particle-Mesh Data (openPMD). Its approach towards recent challenges posed\nby hardware heterogeneity lies in the decoupling of data description in domain\nsciences, such as plasma physics simulations, from concrete implementations in\nhardware and IO. The streaming backend is provided by the ADIOS2 framework,\ndeveloped at Oak Ridge National Laboratory. This paper surveys two\nopenPMD-based loosely coupled setups to demonstrate flexible applicability and\nto evaluate performance. In loose coupling, as opposed to tight coupling, two\n(or more) applications are executed separately, e.g. in individual MPI\ncontexts, yet cooperate by exchanging data. This way, a streaming-based\nworkflow allows for standalone codes instead of tightly-coupled plugins, using\na unified streaming-aware API and leveraging high-speed communication\ninfrastructure available in modern compute clusters for massive data exchange.\nWe determine new challenges in resource allocation and in the need of\nstrategies for a flexible data distribution, demonstrating their influence on\nefficiency and scaling on the Summit compute system. The presented setups show\nthe potential for a more flexible use of compute resources brought by streaming\nIO as well as the ability to increase throughput by avoiding filesystem\nbottlenecks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:59:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Poeschel", "Franz", ""], ["E", "Juncheng", ""], ["Godoy", "William F.", ""], ["Podhorszki", "Norbert", ""], ["Klasky", "Scott", ""], ["Eisenhauer", "Greg", ""], ["Davis", "Philip E.", ""], ["Wan", "Lipeng", ""], ["Gainaru", "Ana", ""], ["Gu", "Junmin", ""], ["Koller", "Fabian", ""], ["Widera", "Ren\u00e9", ""], ["Bussmann", "Michael", ""], ["Huebl", "Axel", ""]]}, {"id": "2107.06433", "submitter": "Jesmin Jahan Tithi", "authors": "Jesmin Jahan Tithi and Fabrizio Petrini", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its\n  Performance on PIUMA and Xeon CPU", "comments": "11 Pages. arXiv admin note: substantial text overlap with\n  arXiv:2005.06727", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an optimization\nproblem that costs $O (V^3 \\log(V)) $ where $V$ is the number of unique words\nin the document. Fortunately, WMD can be framed as an Earth Mover's Distance\n(EMD) for which the algorithmic complexity can be reduced to $O(V^2)$ by adding\nan entropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by computing the WMD of a single query document against multiple\ntarget documents at once, for example by finding whether a given tweet is\nsimilar to any other tweets of a given day.\n  In this paper, we first present a shared-memory parallel Sinkhorn-Knopp\nalgorithm to compute the WMD of one document against many other documents by\nadopting the $ O(V^2)$ EMD algorithm. We then algorithmically transform the\noriginal $O(V^2)$ dense compute-heavy version into an equivalent sparse one\nwhich is mapped onto the new Intel Programmable Integrated Unified Memory\nArchitecture (PIUMA) system. The WMD parallel implementation achieves 67x\nspeedup on 96 cores across 4 NUMA sockets of an Intel Cascade Lake system. We\nalso show that PIUMA cores are around 1.2-2.6x faster than Xeon cores on\nSinkhorn-WMD and also provide better strong scaling.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 00:29:18 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tithi", "Jesmin Jahan", ""], ["Petrini", "Fabrizio", ""]]}, {"id": "2107.06469", "submitter": "Kabir Nagrecha", "authors": "Kabir Nagrecha", "title": "Model-Parallel Model Selection for Deep Learning Systems", "comments": "2 pages, 3 figures. 1st place winner of ACM SIGMOD '21 Student\n  Research Competition. Appeared in ACM SIGMOD/PODS '21 Proceedings", "journal-ref": null, "doi": "10.1145/3448016.3450571", "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep learning becomes more expensive, both in terms of time and compute,\ninefficiencies in machine learning (ML) training prevent practical usage of\nstate-of-the-art models for most users. The newest model architectures are\nsimply too large to be fit onto a single processor. To address the issue, many\nML practitioners have turned to model parallelism as a method of distributing\nthe computational requirements across several devices. Unfortunately, the\nsequential nature of neural networks causes very low efficiency and device\nutilization in model parallel training jobs. We propose a new form of \"shard\nparallelism\" combining task and model parallelism, then package it into a\nframework we name Hydra. Hydra recasts the problem of model parallelism in the\nmulti-model context to produce a fine-grained parallel workload of independent\nmodel shards, rather than independent models. This new parallel design promises\ndramatic speedups relative to the traditional model parallelism paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 03:20:37 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nagrecha", "Kabir", ""]]}, {"id": "2107.06533", "submitter": "Shaohuai Shi", "authors": "Shaohuai Shi, Lin Zhang, Bo Li", "title": "Accelerating Distributed K-FAC with Smart Parallelism of Computing and\n  Communication Tasks", "comments": "11 pages. Accepted to IEEE ICDCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training with synchronous stochastic gradient descent (SGD) on\nGPU clusters has been widely used to accelerate the training process of deep\nmodels. However, SGD only utilizes the first-order gradient in model parameter\nupdates, which may take days or weeks. Recent studies have successfully\nexploited approximate second-order information to speed up the training\nprocess, in which the Kronecker-Factored Approximate Curvature (KFAC) emerges\nas one of the most efficient approximation algorithms for training deep models.\nYet, when leveraging GPU clusters to train models with distributed KFAC\n(D-KFAC), it incurs extensive computation as well as introduces extra\ncommunications during each iteration. In this work, we propose D-KFAC\n(SPD-KFAC) with smart parallelism of computing and communication tasks to\nreduce the iteration time. Specifically, 1) we first characterize the\nperformance bottlenecks of D-KFAC, 2) we design and implement a pipelining\nmechanism for Kronecker factors computation and communication with dynamic\ntensor fusion, and 3) we develop a load balancing placement for inverting\nmultiple matrices on GPU clusters. We conduct real-world experiments on a\n64-GPU cluster with 100Gb/s InfiniBand interconnect. Experimental results show\nthat our proposed SPD-KFAC training scheme can achieve 10%-35% improvement over\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:01:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Shi", "Shaohuai", ""], ["Zhang", "Lin", ""], ["Li", "Bo", ""]]}, {"id": "2107.06548", "submitter": "Alaa Awad Abdellatif", "authors": "Alaa Awad Abdellatif, Naram Mhaisen, Amr Mohamed, Aiman Erbad, Mohsen\n  Guizani, Zaher Dawy, Wassim Nasreddine", "title": "Communication-Efficient Hierarchical Federated Learning for IoT\n  Heterogeneous Systems with Imbalanced Data", "comments": "A version of this work has been submitted in Transactions on Network\n  Science and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.MA cs.NI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) is a distributed learning methodology that allows\nmultiple nodes to cooperatively train a deep learning model, without the need\nto share their local data. It is a promising solution for telemonitoring\nsystems that demand intensive data collection, for detection, classification,\nand prediction of future events, from different locations while maintaining a\nstrict privacy constraint. Due to privacy concerns and critical communication\nbottlenecks, it can become impractical to send the FL updated models to a\ncentralized server. Thus, this paper studies the potential of hierarchical FL\nin IoT heterogeneous systems and propose an optimized solution for user\nassignment and resource allocation on multiple edge nodes. In particular, this\nwork focuses on a generic class of machine learning models that are trained\nusing gradient-descent-based schemes while considering the practical\nconstraints of non-uniformly distributed data across different users. We\nevaluate the proposed system using two real-world datasets, and we show that it\noutperforms state-of-the-art FL solutions. In particular, our numerical results\nhighlight the effectiveness of our approach and its ability to provide 4-6%\nincrease in the classification accuracy, with respect to hierarchical FL\nschemes that consider distance-based user assignment. Furthermore, the proposed\napproach could significantly accelerate FL training and reduce communication\noverhead by providing 75-85% reduction in the communication rounds between edge\nnodes and the centralized server, for the same model accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 08:32:39 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Abdellatif", "Alaa Awad", ""], ["Mhaisen", "Naram", ""], ["Mohamed", "Amr", ""], ["Erbad", "Aiman", ""], ["Guizani", "Mohsen", ""], ["Dawy", "Zaher", ""], ["Nasreddine", "Wassim", ""]]}, {"id": "2107.06580", "submitter": "David Roschewitz", "authors": "David Roschewitz, Mary-Anne Hartley, Luca Corinzia, Martin Jaggi", "title": "IFedAvg: Interpretable Data-Interoperability for Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the ever-growing demand for privacy-oriented machine learning has\nmotivated researchers to develop federated and decentralized learning\ntechniques, allowing individual clients to train models collaboratively without\ndisclosing their private datasets. However, widespread adoption has been\nlimited in domains relying on high levels of user trust, where assessment of\ndata compatibility is essential. In this work, we define and address low\ninteroperability induced by underlying client data inconsistencies in federated\nlearning for tabular data. The proposed method, iFedAvg, builds on federated\naveraging adding local element-wise affine layers to allow for a personalized\nand granular understanding of the collaborative learning process. Thus,\nenabling the detection of outlier datasets in the federation and also learning\nthe compensation for local data distribution shifts without sharing any\noriginal data. We evaluate iFedAvg using several public benchmarks and a\npreviously unstudied collection of real-world datasets from the 2014 - 2016\nWest African Ebola epidemic, jointly forming the largest such dataset in the\nworld. In all evaluations, iFedAvg achieves competitive average performance\nwith negligible overhead. It additionally shows substantial improvement on\noutlier clients, highlighting increased robustness to individual dataset\nshifts. Most importantly, our method provides valuable client-specific insights\nat a fine-grained level to guide interoperable federated learning.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:54:00 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Roschewitz", "David", ""], ["Hartley", "Mary-Anne", ""], ["Corinzia", "Luca", ""], ["Jaggi", "Martin", ""]]}, {"id": "2107.06640", "submitter": "Lukas Krenz", "authors": "Lukas Krenz, Carsten Uphoff, Thomas Ulrich, Alice-Agnes Gabriel,\n  Lauren S. Abrahams, Eric M. Dunham, Michael Bader", "title": "3D Acoustic-Elastic Coupling with Gravity: The Dynamics of the 2018\n  Palu, Sulawesi Earthquake and Tsunami", "comments": "13 pages, 6 figures; Accepted at the International Conference for\n  High Performance Computing, Networking, Storage and Analysis 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a highly scalable 3D fully-coupled Earth & ocean model of\nearthquake rupture and tsunami generation and perform the first fully coupled\nsimulation of an actual earthquake-tsunami event and a 3D benchmark problem of\ntsunami generation by a mega-thrust dynamic earthquake rupture. Multi-petascale\nsimulations, with excellent performance demonstrated on three different\nplatforms, allow high-resolution forward modeling. Our largest mesh has\n$\\approx$261 billion degrees of freedom, resolving at least 15 Hz of the\nacoustic wave field. We self-consistently model seismic, acoustic and surface\ngravity wave propagation in elastic (Earth) and acoustic (ocean) materials\nsourced by physics-based non-linear earthquake dynamic rupture, thereby gaining\ninsight into the tsunami generation process without relying on approximations\nthat have previously been applied to permit solution of this challenging\nproblem. Complicated geometries, including high-resolution bathymetry,\ncoastlines and segmented earthquake faults are discretized by adaptive\nunstructured tetrahedral meshes. This leads inevitably to large differences in\nelement sizes and wave speeds which can be mitigated by ADER local\ntime-stepping and a Discontinuous Galerkin discretisation yielding high-order\naccuracy in time and space.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 11:09:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Krenz", "Lukas", ""], ["Uphoff", "Carsten", ""], ["Ulrich", "Thomas", ""], ["Gabriel", "Alice-Agnes", ""], ["Abrahams", "Lauren S.", ""], ["Dunham", "Eric M.", ""], ["Bader", "Michael", ""]]}, {"id": "2107.06676", "submitter": "Steven W. D. Chien", "authors": "Martin Svedin, Artur Podobas, Steven W. D. Chien, Stefano Markidis", "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with\n  StreamBrain", "comments": "Submitted to The 2nd Workshop on Artificial Intelligence and Machine\n  Learning for Scientific Applications (AI4S 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 13:08:19 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Svedin", "Martin", ""], ["Podobas", "Artur", ""], ["Chien", "Steven W. D.", ""], ["Markidis", "Stefano", ""]]}, {"id": "2107.06724", "submitter": "Matthias Reisser", "authors": "Matthias Reisser, Christos Louizos, Efstratios Gavves, Max Welling", "title": "Federated Mixture of Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning (FL) has emerged as the predominant approach for\ncollaborative training of neural network models across multiple users, without\nthe need to gather the data at a central location. One of the important\nchallenges in this setting is data heterogeneity, i.e. different users have\ndifferent data characteristics. For this reason, training and using a single\nglobal model might be suboptimal when considering the performance of each of\nthe individual user's data. In this work, we tackle this problem via Federated\nMixture of Experts, FedMix, a framework that allows us to train an ensemble of\nspecialized models. FedMix adaptively selects and trains a user-specific\nselection of the ensemble members. We show that users with similar data\ncharacteristics select the same members and therefore share statistical\nstrength while mitigating the effect of non-i.i.d data. Empirically, we show\nthrough an extensive experimental evaluation that FedMix improves performance\ncompared to using a single global model across a variety of different sources\nof non-i.i.d.-ness.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 14:15:24 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Reisser", "Matthias", ""], ["Louizos", "Christos", ""], ["Gavves", "Efstratios", ""], ["Welling", "Max", ""]]}, {"id": "2107.06771", "submitter": "Mirko Zichichi", "authors": "Luca Serena, Mirko Zichichi, Gabriele D'Angelo, Stefano Ferretti", "title": "Simulation of Dissemination Strategies on Temporal Networks", "comments": "To appear in the Proceedings of the 2021 Annual Modeling and\n  Simulation Conference (ANNSIM 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed environments, such as distributed ledgers technologies and\nother peer-to-peer architectures, communication represents a crucial topic. The\nability to efficiently disseminate contents is strongly influenced by the type\nof system architecture, the protocol used to spread such contents over the\nnetwork and the actual dynamicity of the communication links (i.e. static vs.\ntemporal nets). In particular, the dissemination strategies either focus on\nachieving an optimal coverage, minimizing the network traffic or providing\nassurances on anonymity (that is a fundamental requirement of many\ncryptocurrencies). In this work, the behaviour of multiple dissemination\nprotocols is discussed and studied through simulation. The performance\nevaluation has been carried out on temporal networks with the help of\nLUNES-temporal, a discrete event simulator that allows to test algorithms\nrunning on a distributed environment. The experiments show that some gossip\nprotocols allow to either save a considerable number of messages or to provide\nbetter anonymity guarantees, at the cost of a little lower coverage achieved\nand/or a little increase of the delivery time.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:32:04 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Serena", "Luca", ""], ["Zichichi", "Mirko", ""], ["D'Angelo", "Gabriele", ""], ["Ferretti", "Stefano", ""]]}, {"id": "2107.06790", "submitter": "Mirko Zichichi", "authors": "Mirko Zichichi, Luca Serena, Stefano Ferretti, Gabriele D'Angelo", "title": "Governing Decentralized Complex Queries Through a DAO", "comments": "To appear in the ACM International Conference on Information\n  Technology for Social Good (GoodIT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a new generation of P2P systems capable of addressing data\nintegrity and authenticity has emerged for the development of new applications\nfor a \"more\" decentralized Internet, i.e., Distributed Ledger Technologies\n(DLT) and Decentralized File Systems (DFS). However, these technologies still\nhave some unanswered issues, mostly related to data lookup and discovery. In\nthis paper, first, we propose a Distributed Hash Table (DHT) system that\nefficiently manages decentralized keyword-based queries executed on data stored\nin DFS. Through a hypercube logical layout, queries are efficiently routed\namong the network, where each node is responsible for a specific keywords set\nand the related contents. Second, we provide a framework for the governance of\nthe above network, based on a Decentralized Autonomous Organization (DAO)\nimplementation. We show how the use of smart contracts enables organizational\ndecision making and rewards for nodes that have actively contributed to the\nDHT. Finally, we provide experimental validation of an implementation of our\nproposal, where the execution of the same protocol for different logical nodes\nof the hypercube allows us to evaluate the efficiency of communication within\nthe network.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:46:01 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Zichichi", "Mirko", ""], ["Serena", "Luca", ""], ["Ferretti", "Stefano", ""], ["D'Angelo", "Gabriele", ""]]}, {"id": "2107.06835", "submitter": "Ripon Patgiri", "authors": "Sabuzima Nayak, Ripon Patgiri, Lilapati Waikhom, Arif Ahmed", "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,\n  Future Directions, and Applications", "comments": "Submitted to Elsevier for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge technology aims to bring Cloud resources (specifically, the compute,\nstorage, and network) to the closed proximity of the Edge devices, i.e., smart\ndevices where the data are produced and consumed. Embedding computing and\napplication in Edge devices lead to emerging of two new concepts in Edge\ntechnology, namely, Edge computing and Edge analytics. Edge analytics uses some\ntechniques or algorithms to analyze the data generated by the Edge devices.\nWith the emerging of Edge analytics, the Edge devices have become a complete\nset. Currently, Edge analytics is unable to provide full support for the\nexecution of the analytic techniques. The Edge devices cannot execute advanced\nand sophisticated analytic algorithms following various constraints such as\nlimited power supply, small memory size, limited resources, etc. This article\naims to provide a detailed discussion on Edge analytics. A clear explanation to\ndistinguish between the three concepts of Edge technology, namely, Edge\ndevices, Edge computing, and Edge analytics, along with their issues.\nFurthermore, the article discusses the implementation of Edge analytics to\nsolve many problems in various areas such as retail, agriculture, industry, and\nhealthcare. In addition, the research papers of the state-of-the-art edge\nanalytics are rigorously reviewed in this article to explore the existing\nissues, emerging challenges, research opportunities and their directions, and\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 21:48:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Nayak", "Sabuzima", ""], ["Patgiri", "Ripon", ""], ["Waikhom", "Lilapati", ""], ["Ahmed", "Arif", ""]]}, {"id": "2107.06836", "submitter": "Xinxin Liu", "authors": "Xinxin Liu, Yu Hua, Rong Bai", "title": "Consistent RDMA-Friendly Hashing on Remote Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalescing RDMA and Persistent Memory (PM) delivers high end-to-end\nperformance for networked storage systems, which requires rethinking the design\nof efficient hash structures. In general, existing hashing schemes separately\noptimize RDMA and PM, thus partially addressing the problems of RDMA Access\nAmplification and High-Overhead PM Consistency. In order to address these\nproblems, we propose a continuity hashing, which is a \"one-stone-two-birds\"\ndesign to optimize both RDMA and PM. The continuity hashing leverages a\nfine-grained contiguous shared region, called SBuckets, to provide standby\npositions for the neighbouring two buckets in case of hash collisions. In the\ncontinuity hashing, remote read only needs a single RDMA read to directly fetch\nthe home bucket and the neighbouring SBuckets, which contain all the positions\nof maintaining a key-value item, thus alleviating RDMA access amplification.\nContinuity hashing further leverages indicators that can be atomically modified\nto support log-free PM consistency for all the write operations. Evaluation\nresults demonstrate that compared with state-of-the-art schemes, continuity\nhashing achieves high throughput (i.e., 1.45X -- 2.43X improvement), low\nlatency (about 1.7X speedup) and the smallest number of PM writes with various\nworkloads, while has acceptable load factors of about 70%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 16:51:07 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Liu", "Xinxin", ""], ["Hua", "Yu", ""], ["Bai", "Rong", ""]]}, {"id": "2107.06877", "submitter": "Vasileios Tsouvalas", "authors": "Vasileios Tsouvalas, Aaqib Saeed, Tanir Ozcelebi", "title": "Federated Self-Training for Semi-Supervised Audio Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is a distributed machine learning paradigm dealing with\ndecentralized and personal datasets. Since data reside on devices like\nsmartphones and virtual assistants, labeling is entrusted to the clients, or\nlabels are extracted in an automated way. Specifically, in the case of audio\ndata, acquiring semantic annotations can be prohibitively expensive and\ntime-consuming. As a result, an abundance of audio data remains unlabeled and\nunexploited on users' devices. Most existing federated learning approaches\nfocus on supervised learning without harnessing the unlabeled data. In this\nwork, we study the problem of semi-supervised learning of audio models via\nself-training in conjunction with federated learning. We propose FedSTAR to\nexploit large-scale on-device unlabeled data to improve the generalization of\naudio recognition models. We further demonstrate that self-supervised\npre-trained models can accelerate the training of on-device models,\nsignificantly improving convergence to within fewer training rounds. We conduct\nexperiments on diverse public audio classification datasets and investigate the\nperformance of our models under varying percentages of labeled and unlabeled\ndata. Notably, we show that with as little as 3% labeled data available,\nFedSTAR on average can improve the recognition rate by 13.28% compared to the\nfully supervised federated model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:40:10 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Tsouvalas", "Vasileios", ""], ["Saeed", "Aaqib", ""], ["Ozcelebi", "Tanir", ""]]}, {"id": "2107.06922", "submitter": "Yacov Manevich", "authors": "Artem Barger, Yacov Manevich, Hagar Meir, Yoav Tock", "title": "A Byzantine Fault-Tolerant Consensus Library for Hyperledger Fabric", "comments": "IEEE International Conference on Blockchain and Cryptocurrency,\n  https://icbc2021.ieee-icbc.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperledger Fabric is an enterprise grade permissioned distributed ledger\nplatform that offers modularity for a broad set of industry use cases. One\nmodular component is a pluggable ordering service that establishes consensus on\nthe order of transactions and batches them into blocks. However, as of the time\nof this writing, there is no production grade Byzantine Fault-Tolerant (BFT)\nordering service for Fabric, with the latest version (v2.1) supporting only\nCrash Fault-Tolerance (CFT). In our work, we address crucial aspects of BFT\nintegration into Fabric that were left unsolved in all prior works, making them\nunfit for production use. In this work we describe the design and\nimplementation of a BFT ordering service for Fabric, employing a new BFT\nconsensus library. The new library, based on the BFT-Smart protocol and written\nin Go, is tailored to the blockchain use-case, yet is general enough to cater\nto a wide variety of other uses. We evaluate the new BFT ordering service by\ncomparing it with the currently supported Raft-based CFT ordering service in\nHyperledger Fabric.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:14:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Barger", "Artem", ""], ["Manevich", "Yacov", ""], ["Meir", "Hagar", ""], ["Tock", "Yoav", ""]]}, {"id": "2107.06925", "submitter": "Shigang Li", "authors": "Shigang Li, Torsten Hoefler", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with\n  Bidirectional Pipelines", "comments": "The paper was accepted by the 2021 International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC'21), in Best\n  Paper Finalist", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large deep learning models at scale is very challenging. This paper\nproposes Chimera, a novel pipeline parallelism scheme which combines\nbidirectional pipelines for efficiently training large-scale models. Chimera is\na synchronous approach and therefore no loss of accuracy, which is more\nconvergence-friendly than asynchronous approaches. Compared with the latest\nsynchronous pipeline approach, Chimera reduces the number of bubbles by up to\n50%; benefiting from the sophisticated scheduling of bidirectional pipelines,\nChimera has a more balanced activation memory consumption. Evaluations are\nconducted on Transformer based language models. For a GPT-2 model with 1.3\nbillion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,\nChimera improves the training throughput by 1.16x-2.34x over the\nstate-of-the-art synchronous and asynchronous pipeline approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 18:16:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Li", "Shigang", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2107.06999", "submitter": "Valentina Janev", "authors": "Valentina Janev, Du\\v{s}an Popadi\\'c, Dea Puji\\'c, Maria Esther Vidal,\n  Kemele Endris", "title": "Reuse of Semantic Models for Emerging Smart Grids Applications", "comments": "Paper presented at the ICIST Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the energy domain grows at unprecedented rates. Despite the great\npotential that IoT platforms and other big data-driven technologies have\nbrought in the energy sector, data exchange and data integration are still not\nwholly achieved. As a result, fragmented applications are developed against\nenergy data silos, and data exchange is limited to few applications. Therefore,\nthis paper identifies semantic models that can be reused for building\ninteroperable energy management services and applications. The ambition is to\ninnovate the Institute Mihajlo Pupin proprietary SCADA system and to enable\nintegration of the Institute Mihajlo Pupin services and applications in the\nEuropean Union (EU) Energy Data Space. The selection of reusable models has\nbeen done based on a set of scenarios related to electricity balancing\nservices, predictive maintenance services, and services for the residential,\ncommercial and industrial sectors.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:18:09 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Janev", "Valentina", ""], ["Popadi\u0107", "Du\u0161an", ""], ["Puji\u0107", "Dea", ""], ["Vidal", "Maria Esther", ""], ["Endris", "Kemele", ""]]}, {"id": "2107.07063", "submitter": "I Wayan Budi  Sentana", "authors": "I Wayan Budi Sentana, Muhammad Ikram, Mohamed Ali Kaafar", "title": "BlockJack: Towards Improved Prevention of IP Prefix Hijacking Attacks in\n  Inter-Domain Routing Via Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose BlockJack, a system based on a distributed and tamper-proof\nconsortium Blockchain that aims at blocking IP prefix hijacking in the Border\nGateway Protocol (BGP). In essence, BlockJack provides synchronization among\nBlockChain and BGP network through interfaces ensuring operational independence\nand this approach preserving the legacy system and accommodates the impact of a\nrace condition if the Blockchain process exceeds the BGP update interval.\nBlockJack is also resilient to dynamic routing path changes during the\noccurrence of the IP prefix hijacking in the routing tables. We implement\nBlockJack using Hyperledger Fabric Blockchain and Quagga software package and\nwe perform initial sets of experiments to evaluate its efficacy. We evaluate\nthe performance and resilience of BlockJack in various attack scenarios\nincluding single path attacks, multiple path attacks, and attacks from random\nsources in the random network topology. The Evaluation results show that\nBlockJack is able to handle multiple attacks caused by AS paths changes during\na BGP prefix hijacking. In experiment settings with 50 random routers,\nBlockJack takes on average 0.08 seconds (with a standard deviation of 0.04\nseconds) to block BGP prefix hijacking attacks. The test result showing that\nBlockJack conservative approach feasible to handle the IP Prefix hijacking in\nthe Border Gateway Protocol.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 01:11:56 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Sentana", "I Wayan Budi", ""], ["Ikram", "Muhammad", ""], ["Kaafar", "Mohamed Ali", ""]]}, {"id": "2107.07104", "submitter": "Benjamin Maier", "authors": "Benjamin Maier", "title": "Scalable Biophysical Simulations of the Neuromuscular System", "comments": "PhD thesis, 530 pages, 208 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.NA math.NA physics.bio-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human neuromuscular system consisting of skeletal muscles and neural\ncircuits is a complex system that is not yet fully understood. Surface\nelectromyography (EMG) can be used to study muscle behavior from the outside.\nComputer simulations with detailed biophysical models provide a non-invasive\ntool to interpret EMG signals and gain new insights into the system. The\nnumerical solution of such multi-scale models imposes high computational work\nloads, which restricts their application to short simulation time spans or\ncoarse resolutions. We tackled this challenge by providing scalable software\nemploying instruction-level and task-level parallelism, suitable numerical\nmethods and efficient data handling. We implemented a comprehensive,\nstate-of-the-art, multi-scale multi-physics model framework that can simulate\nsurface EMG signals and muscle contraction as a result of neuromuscular\nstimulation.\n  This work describes the model framework and its numerical discretization,\ndevelops new algorithms for mesh generation and parallelization, covers the use\nand implementation of our software OpenDiHu, and evaluates its computational\nperformance in numerous use cases. We obtain a speedup of several hundred\ncompared to a baseline solver from the literature and demonstrate, that our\ndistributed-memory parallelization and the use of High Performance Computing\nresources enables us to simulate muscular surface EMG of the biceps brachii\nmuscle with realistic muscle fiber counts of several hundred thousands. We find\nthat certain model effects are only visible with such high resolution. In\nconclusion, our software contributes to more realistic simulations of the\nneuromuscular system and provides a tool for applied researchers to complement\nin vivo experiments with in-silico studies. It can serve as a building block to\nset up comprehensive models for more organs in the musculoskeletal system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 19:12:46 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Maier", "Benjamin", ""]]}, {"id": "2107.07108", "submitter": "Lipeng Wan", "authors": "Lipeng Wan, Axel Huebl, Junmin Gu, Franz Poeschel, Ana Gainaru, Ruonan\n  Wang, Jieyang Chen, Xin Liang, Dmitry Ganyushin, Todd Munson, Ian Foster,\n  Jean-Luc Vay, Norbert Podhorszki, Kesheng Wu, Scott Klasky", "title": "Improving I/O Performance for Exascale Applications through Online Data\n  Layout Reorganization", "comments": "12 pages, 15 figures, accepted by IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applications being developed within the U.S. Exascale Computing Project\n(ECP) to run on imminent Exascale computers will generate scientific results\nwith unprecedented fidelity and record turn-around time. Many of these codes\nare based on particle-mesh methods and use advanced algorithms, especially\ndynamic load-balancing and mesh-refinement, to achieve high performance on\nExascale machines. Yet, as such algorithms improve parallel application\nefficiency, they raise new challenges for I/O logic due to their irregular and\ndynamic data distributions. Thus, while the enormous data rates of Exascale\nsimulations already challenge existing file system write strategies, the need\nfor efficient read and processing of generated data introduces additional\nconstraints on the data layout strategies that can be used when writing data to\nsecondary storage. We review these I/O challenges and introduce two online data\nlayout reorganization approaches for achieving good tradeoffs between read and\nwrite performance. We demonstrate the benefits of using these two approaches\nfor the ECP particle-in-cell simulation WarpX, which serves as a motif for a\nlarge class of important Exascale applications. We show that by understanding\napplication I/O patterns and carefully designing data layouts we can increase\nread performance by more than 80%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 04:17:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wan", "Lipeng", ""], ["Huebl", "Axel", ""], ["Gu", "Junmin", ""], ["Poeschel", "Franz", ""], ["Gainaru", "Ana", ""], ["Wang", "Ruonan", ""], ["Chen", "Jieyang", ""], ["Liang", "Xin", ""], ["Ganyushin", "Dmitry", ""], ["Munson", "Todd", ""], ["Foster", "Ian", ""], ["Vay", "Jean-Luc", ""], ["Podhorszki", "Norbert", ""], ["Wu", "Kesheng", ""], ["Klasky", "Scott", ""]]}, {"id": "2107.07157", "submitter": "Jens Domke", "authors": "Jens Domke", "title": "A64FX -- Your Compiler You Must Decide!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current number one of the TOP500 list, Supercomputer Fugaku, has\ndemonstrated that CPU-only HPC systems aren't dead and CPUs can be used for\nmore than just being the host controller for a discrete accelerators. While the\nspecifications of the chip and overall system architecture, and benchmarks\nsubmitted to various lists, like TOP500 and Green500, etc., are clearly\nhighlighting the potential, the proliferation of Arm into the HPC business is\nrather recent and hence the software stack might not be fully matured and\ntuned, yet. We test three state-of-the-art compiler suite against a broad set\nof benchmarks. Our measurements show that orders of magnitudes in performance\ncan be gained by deviating from the recommended usage model of the A64FX\ncompute nodes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:05:36 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Domke", "Jens", ""]]}, {"id": "2107.07171", "submitter": "Ye Yuan", "authors": "Ye Yuan, Ruijuan Chen, Chuan Sun, Maolin Wang, Feng Hua, Xinlei Yi,\n  Tao Yang and Jun Liu", "title": "DeFed: A Principled Decentralized and Privacy-Preserving Federated\n  Learning Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables a large number of clients to participate in\nlearning a shared model while maintaining the training data stored in each\nclient, which protects data privacy and security. Till now, federated learning\nframeworks are built in a centralized way, in which a central client is needed\nfor collecting and distributing information from every other client. This not\nonly leads to high communication pressure at the central client, but also\nrenders the central client highly vulnerable to failure and attack. Here we\npropose a principled decentralized federated learning algorithm (DeFed), which\nremoves the central client in the classical Federated Averaging (FedAvg)\nsetting and only relies information transmission between clients and their\nlocal neighbors. The proposed DeFed algorithm is proven to reach the global\nminimum with a convergence rate of $O(1/T)$ when the loss function is smooth\nand strongly convex, where $T$ is the number of iterations in gradient descent.\nFinally, the proposed algorithm has been applied to a number of toy examples to\ndemonstrate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 07:39:19 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yuan", "Ye", ""], ["Chen", "Ruijuan", ""], ["Sun", "Chuan", ""], ["Wang", "Maolin", ""], ["Hua", "Feng", ""], ["Yi", "Xinlei", ""], ["Yang", "Tao", ""], ["Liu", "Jun", ""]]}, {"id": "2107.07195", "submitter": "Patrizio Dazzi Ph.D.", "authors": "Emanuele Carlini, Patrizio Dazzi, Luca Ferrucci, Matteo Mordacchini", "title": "Efficient Resources Distribution for an Ephemeral Cloud/Edge continuum", "comments": "arXiv admin note: substantial text overlap with arXiv:1610.07371", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the idea and the concepts behind the vision of an\nEphemeral Cloud/Edge Continuum, a cloud/edge computing landscape that enables\nthe exploitation of a widely distributed, dynamic, and context-aware set of\nresources. The Ephemeral Continuum answer to the need of combining a plethora\nof heterogeneous devices, which nowadays are pervasively embedding anthropic\nenvironments, with both federations of cloud providers and the resources\nlocated at the Edge. The aim of the Ephemeral Continuum is to realise a\ncontext-aware and personalised federation of computational, data and network\nresources, able to manage their heterogeneity in a highly distributed\ndeployment.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 08:45:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Carlini", "Emanuele", ""], ["Dazzi", "Patrizio", ""], ["Ferrucci", "Luca", ""], ["Mordacchini", "Matteo", ""]]}, {"id": "2107.07208", "submitter": "Christian Lienen", "authors": "Christian Lienen, Marco Platzner", "title": "Design of Distributed Reconfigurable Robotics Systems with ReconROS", "comments": "Paper is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotics applications process large amounts of data in real-time and require\ncompute platforms that provide high performance and energy-efficiency. FPGAs\nare well-suited for many of these applications, but there is a reluctance in\nthe robotics community to use hardware acceleration due to increased design\ncomplexity and a lack of consistent programming models across the\nsoftware/hardware boundary. In this paper we present ReconROS, a framework that\nintegrates the widely-used robot operating system (ROS) with ReconOS, which\nfeatures multithreaded programming of hardware and software threads for\nreconfigurable computers. This unique combination gives ROS2 developers the\nflexibility to transparently accelerate parts of their robotics applications in\nhardware. We elaborate on the architecture and the design flow for ReconROS and\nreport on a set of experiments that underline the feasibility and flexibility\nof our approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 09:34:18 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lienen", "Christian", ""], ["Platzner", "Marco", ""]]}, {"id": "2107.07341", "submitter": "Rutwik Shah", "authors": "Rutwik Shah, Bruno Astuto, Tyler Gleason, Will Fletcher, Justin\n  Banaga, Kevin Sweetwood, Allen Ye, Rina Patel, Kevin McGill, Thomas Link,\n  Jason Crane, Valentina Pedoia, Sharmila Majumdar", "title": "Leveraging wisdom of the crowds to improve consensus among radiologists\n  by real time, blinded collaborations on a digital swarm platform", "comments": "24 pages, 2 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.DC cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 06:52:06 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Shah", "Rutwik", ""], ["Astuto", "Bruno", ""], ["Gleason", "Tyler", ""], ["Fletcher", "Will", ""], ["Banaga", "Justin", ""], ["Sweetwood", "Kevin", ""], ["Ye", "Allen", ""], ["Patel", "Rina", ""], ["McGill", "Kevin", ""], ["Link", "Thomas", ""], ["Crane", "Jason", ""], ["Pedoia", "Valentina", ""], ["Majumdar", "Sharmila", ""]]}, {"id": "2107.07365", "submitter": "Kristan Temme", "authors": "Pawel Wocjan and Kristan Temme", "title": "Szegedy Walk Unitaries for Quantum Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.other cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Szegedy developed a generic method for quantizing classical algorithms based\non random walks [Proceedings of FOCS, 2004, pp. 32-41]. A major contribution of\nhis work was the construction of a walk unitary for any reversible random walk.\nSuch unitary posses two crucial properties: its eigenvector with eigenphase $0$\nis a quantum sample of the limiting distribution of the random walk and its\neigenphase gap is quadratically larger than the spectral gap of the random\nwalk. It was an open question if it is possible to generalize Szegedy's\nquantization method for stochastic maps to quantum maps. We answer this in the\naffirmative by presenting an explicit construction of a Szegedy walk unitary\nfor detailed balanced Lindbladians -- generators of quantum Markov semigroups\n-- and detailed balanced quantum channels. We prove that our Szegedy walk\nunitary has a purification of the fixed point of the Lindbladian as eigenvector\nwith eigenphase $0$ and that its eigenphase gap is quadratically larger than\nthe spectral gap of the Lindbladian. To construct the walk unitary we leverage\na canonical form for detailed balanced Lindbladians showing that they are\nstructurally related to Davies generators. We also explain how the quantization\nmethod for Lindbladians can be applied to quantum channels. We give an\nefficient quantum algorithm for quantizing Davies generators that describe many\nimportant open-system dynamics, for instance, the relaxation of a quantum\nsystem coupled to a bath. Our algorithm extends known techniques for simulating\nquantum systems on a quantum computer.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:44:35 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wocjan", "Pawel", ""], ["Temme", "Kristan", ""]]}, {"id": "2107.07406", "submitter": "Omar Otoniel Flores-Cortez", "authors": "Otoniel Flores-Cortez, Ronny Cortez, Bruno Gonz\\'alez", "title": "Design and Implementation of an IoT Based LPG and CO Gases Monitoring\n  System", "comments": "David C. Wyld et al. (Eds): BIoT, DKMP, CCSEA, EMSA - 2021 pp. 31-39,\n  2021. CS & IT - CSCP 2021", "journal-ref": null, "doi": "10.5121/csit.2021.110803", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays use of liquefied petroleum gas (LPG) has increased. LPG is an\nasphyxiating, volatile and highly flammable gas. In a LPG leak situation,\npotential health accidents are increased either by inhalation or by combustion\nof the gas. On the other hand, carbon monoxide (CO) is a toxic gas that comes\nmainly from combustion in car engines. Breathing CO-polluted air can cause\ndizziness, fainting, breathing problems, and sometimes death. To prevent health\naccidents, including explosions, in open or closed environments, remote and\nreal-time monitoring of the concentration levels of CO and LPG gases has become\na necessity. The aim of this work is to demonstrate the use of Internet of\nThings (IoT) techniques to design and build a telemetry system to monitor in\nreal-time the concentration of GLP and CO gases in the surrounding air. To\nimplement this work, as central hardware there is a microcontroller, CO and PLG\nsensors on the electronic station. Besides, Amazon Web Services (AWS) was used\nas an IoT platform and data storage in the cloud. The main result was a\ntelematics system to monitor in real time the concentrations of both GLP and CO\ngases, whose data is accessible from any device with internet access through a\nwebsite. Field tests have been successful and have shown that the proposed\nsystem is an efficient and low-cost option.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:01:55 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Flores-Cortez", "Otoniel", ""], ["Cortez", "Ronny", ""], ["Gonz\u00e1lez", "Bruno", ""]]}, {"id": "2107.07442", "submitter": "Weitao Wang", "authors": "Weitao Wang, Sushovan Das, Xinyu Crystal Wu, Zhuang Wang, Ang Chen, T.\n  S. Eugene Ng", "title": "MXDAG: A Hybrid Abstraction for Cluster Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed applications, such as database queries and distributed training,\nconsist of both compute and network tasks. DAG-based abstraction primarily\ntargets compute tasks and has no explicit network-level scheduling. In\ncontrast, Coflow abstraction collectively schedules network flows among compute\ntasks but lacks the end-to-end view of the application DAG. Because of the\ndependencies and interactions between these two types of tasks, it is\nsub-optimal to only consider one of them. We argue that co-scheduling of both\ncompute and network tasks can help applications towards the globally optimal\nend-to-end performance. However, none of the existing abstractions can provide\nfine-grained information for co-scheduling. We propose MXDAG, an abstraction to\ntreat both compute and network tasks explicitly. It can capture the\ndependencies and interactions of both compute and network tasks leading to\nimproved application performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 16:40:23 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Wang", "Weitao", ""], ["Das", "Sushovan", ""], ["Wu", "Xinyu Crystal", ""], ["Wang", "Zhuang", ""], ["Chen", "Ang", ""], ["Ng", "T. S. Eugene", ""]]}, {"id": "2107.07647", "submitter": "Ian Colbert", "authors": "Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das", "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image\n  Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 23:49:37 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 05:34:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Colbert", "Ian", ""], ["Kreutz-Delgado", "Ken", ""], ["Das", "Srinjoy", ""]]}, {"id": "2107.07703", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "Estimation from Partially Sampled Distributed Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is often a necessary evil to reduce the processing and storage costs\nof distributed tracing. In this work, we describe a scalable and adaptive\nsampling approach that can preserve events of interest better than the widely\nused head-based sampling approach. Sampling rates can be chosen individually\nand independently for every span, allowing to take span attributes and local\nresource constraints into account. The resulting traces are often only\npartially and not completely sampled which complicates statistical analysis. To\nexploit the given information, an unbiased estimation algorithm is presented.\nEven though it does not need to know whether the traces are complete, it\nreduces the estimation error in many cases compared to considering only\ncomplete traces.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:41:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "2107.07809", "submitter": "Michael Lukin", "authors": "K. I. Mihajlenko, M. A. Lukin, A. S. Stankevich", "title": "A method for decompilation of AMD GCN kernels to OpenCL", "comments": "10 pages, 5 figures", "journal-ref": "Information and Control Systems, 2021, no. 2, pp. 33-42", "doi": "10.31799/1684-8853-2021-2-33-42", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Decompilers are useful tools for software analysis and support\nin the absence of source code. They are available for many hardware\narchitectures and programming languages. However, none of the existing\ndecompilers support modern AMD GPU architectures such as AMD GCN and RDNA.\nPurpose: We aim at developing the first assembly decompiler tool for a modern\nAMD GPU architecture that generates code in the OpenCL language, which is\nwidely used for programming GPGPUs. Results: We developed the algorithms for\nthe following operations: preprocessing assembly code, searching data accesses,\nextracting system values, decompiling arithmetic operations and recovering data\ntypes. We also developed templates for decompilation of branching operations.\nPractical relevance: We implemented the presented algorithms in Python as a\ntool called OpenCLDecompiler, which supports a large subset of AMD GCN\ninstructions. This tool automatically converts disassembled GPGPU code into the\nequivalent OpenCL code, which reduces the effort required to analyze assembly\ncode.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:32:54 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mihajlenko", "K. I.", ""], ["Lukin", "M. A.", ""], ["Stankevich", "A. S.", ""]]}, {"id": "2107.07866", "submitter": "Genshen Chu", "authors": "Genshen Chu, Yang Li, Runchu Zhao, Shuai Ren, Wen Yang, Xinfu He,\n  Chungjun Hu, Jue Wang", "title": "MD Simulation of Hundred-Billion-Metal-Atom Cascade Collision on Sunway\n  Taihulight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiation damage to the steel material of reactor pressure vessels is a major\nthreat to the nuclear reactor safety. It is caused by the metal atom cascade\ncollision, initialized when the atoms are struck by a high-energy neutron. The\npaper presents MISA-MD, a new implementation of molecular dynamics, to simulate\nsuch cascade collision with EAM potential. MISA-MD realizes (1) a hash-based\ndata structure to efficiently store an atom and find its neighbors, and (2)\nseveral acceleration and optimization strategies based on SW26010 processor of\nSunway Taihulight supercomputer, including an efficient potential table storage\nand interpolation method, a coloring method to avoid write conflicts, and\ndouble-buffer and data reuse strategies. The experimental results demonstrated\nthat MISA-MD has good accuracy and scalability, and obtains a parallel\nefficiency of over 79% in an 655-billion-atom system. Compared with a\nstate-of-the-art MD program LAMMPS, MISA-MD requires less memory usage and\nachieves better computational performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 12:59:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chu", "Genshen", ""], ["Li", "Yang", ""], ["Zhao", "Runchu", ""], ["Ren", "Shuai", ""], ["Yang", "Wen", ""], ["He", "Xinfu", ""], ["Hu", "Chungjun", ""], ["Wang", "Jue", ""]]}, {"id": "2107.07972", "submitter": "Ege Erdogan", "authors": "Ege Erdogan, Can Arda Aydin, Oznur Ozkasap, Waris Gill", "title": "Demo -- Zelig: Customizable Blockchain Simulator", "comments": "40th International Symposium on Reliable Distributed Systems (SRDS\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As blockchain-based systems see wider adoption, it becomes increasingly\ncritical to ensure their reliability, security, and efficiency. Running\nsimulations is an effective method of gaining insights on the existing systems\nand analyzing potential improvements. However, many of the existing blockchain\nsimulators have various shortcomings that yield them insufficient for a wide\nrange of scenarios. In this demo paper, we present Zelig: our blockchain\nsimulator designed with the main goals of customizability and extensibility. To\nthe best of our knowledge, Zelig is the only blockchain simulator that enables\nsimulating custom network topologies without modifying the simulator code. We\nexplain our simulator design, validate via experimental analysis against the\nreal-world Bitcoin network, and highlight potential use cases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:36:46 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Erdogan", "Ege", ""], ["Aydin", "Can Arda", ""], ["Ozkasap", "Oznur", ""], ["Gill", "Waris", ""]]}, {"id": "2107.08147", "submitter": "Young Geun Kim", "authors": "Young Geun Kim and Carole-Jean Wu", "title": "AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables a cluster of decentralized mobile devices at the\nedge to collaboratively train a shared machine learning model, while keeping\nall the raw training samples on device. This decentralized training approach is\ndemonstrated as a practical solution to mitigate the risk of privacy leakage.\nHowever, enabling efficient FL deployment at the edge is challenging because of\nnon-IID training data distribution, wide system heterogeneity and\nstochastic-varying runtime effects in the field. This paper jointly optimizes\ntime-to-convergence and energy efficiency of state-of-the-art FL use cases by\ntaking into account the stochastic nature of edge execution. We propose AutoFL\nby tailor-designing a reinforcement learning algorithm that learns and\ndetermines which K participant devices and per-device execution targets for\neach FL model aggregation round in the presence of stochastic runtime variance,\nsystem and data heterogeneity. By considering the unique characteristics of FL\nedge deployment judiciously, AutoFL achieves 3.6 times faster model convergence\ntime and 4.7 and 5.2 times higher energy efficiency for local clients and\nglobally over the cluster of K participants, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 23:41:26 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kim", "Young Geun", ""], ["Wu", "Carole-Jean", ""]]}, {"id": "2107.08267", "submitter": "Wenzheng Xu", "authors": "Wenzheng Xu, Yueying Sun, Rui Zou, Weifa Liang, Qiufen Xia, Feng Shan,\n  Tian Wang, Xiaohua Jia, and Zheng Li", "title": "Throughput Maximization of UAV Networks", "comments": "14 pages, this paper was submitted to the journal of IEEE/ACM\n  Transactions on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we study the deployment of multiple unmanned aerial vehicles\n(UAVs) to form a temporal UAV network for the provisioning of emergent\ncommunications to affected people in a disaster zone, where each UAV is\nequipped with a lightweight base station device and thus can act as an aerial\nbase station for users. Unlike most existing studies that assumed that a UAV\ncan serve all users in its communication range, we observe that both\ncomputation and communication capabilities of a single lightweight UAV are very\nlimited, due to various constraints on its size, weight, and power supply.\nThus, a single UAV can only provide communication services to a limited number\nof users. We study a novel problem of deploying $K$ UAVs in the top of a\ndisaster area such that the sum of the data rates of users served by the UAVs\nis maximized, subject to that (i) the number of users served by each UAV is no\ngreater than its service capacity; and (ii) the communication network induced\nby the $K$ UAVs is connected. We then propose a $\\frac{1-1/e}{\\lfloor \\sqrt{K}\n\\rfloor}$-approximation algorithm for the problem, improving the current best\nresult of the problem by five times (the best approximation ratio so far is\n$\\frac{1-1/e}{5( \\sqrt{K} +1)}$), where $e$ is the base of the natural\nlogarithm. We finally evaluate the algorithm performance via simulation\nexperiments. Experimental results show that the proposed algorithm is very\npromising. Especially, the solution delivered by the proposed algorithm is up\nto 12% better than those by existing algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 16:05:26 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Xu", "Wenzheng", ""], ["Sun", "Yueying", ""], ["Zou", "Rui", ""], ["Liang", "Weifa", ""], ["Xia", "Qiufen", ""], ["Shan", "Feng", ""], ["Wang", "Tian", ""], ["Jia", "Xiaohua", ""], ["Li", "Zheng", ""]]}, {"id": "2107.08334", "submitter": "Behnam Dezfouli", "authors": "Jesse Chen and Behnam Dezfouli", "title": "Predictable Bandwidth Slicing with Open vSwitch", "comments": null, "journal-ref": null, "doi": null, "report-no": "SIOTLAB-SPRING-2021-SDN", "categories": "cs.NI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software switching, a.k.a virtual switching, plays a vital role in network\nvirtualization and network function virtualization, enhances configurability,\nand reduces deployment and operational costs. Software switching also\nfacilitates the development of edge and fog computing networks by allowing the\nuse of commodity hardware for both data processing and packet switching.\nDespite these benefits, characterizing and ensuring deterministic performance\nwith software switches is harder, compared to physical switching appliances. In\nparticular, achieving deterministic performance is essential to adopt software\nswitching in mission-critical applications, especially those deployed in edge\nand fog computing architectures. In this paper, we study the impact of switch\nconfigurations on bandwidth slicing and predictable packet latency. We\ndemonstrate that latency and predictability are dependent on the implementation\nof the bandwidth slicing mechanism and that the packet schedulers used in OVS\nKernel-Path and OVS-DPDK each focus on different aspects of switching\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 00:59:55 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Jesse", ""], ["Dezfouli", "Behnam", ""]]}, {"id": "2107.08348", "submitter": "Dipankar Chaki", "authors": "Dipankar Chaki and Athman Bouguettaya", "title": "Adaptive Priority-based Conflict Resolution of IoT Services", "comments": "6 pages, 7 figures. Accepted paper and to appear in the Proceedings\n  of the 2021 IEEE International Conference on Web Services (IEEE ICWS 2021)\n  affiliated with the 2021 IEEE World Congress on Services (IEEE SERVICES 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel conflict resolution framework for IoT services in\nmulti-resident smart homes. An adaptive priority model is developed considering\nthe residents' contextual factors (e.g., age, illness, impairment). The\nproposed priority model is designed using the concept of the analytic hierarchy\nprocess. A set of experiments on real-world datasets are conducted to show the\nefficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 02:41:26 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chaki", "Dipankar", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2107.08386", "submitter": "Tarannum Nisha", "authors": "Tarannum Nisha, Duong Tung Nguyen, Vijay K. Bhargava", "title": "A Bilevel Programming Framework for Joint Edge Resource Management and\n  Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging edge computing paradigm promises to provide low latency and\nubiquitous computation to numerous mobile and Internet of Things (IoT) devices\nat the network edge. How to efficiently allocate geographically distributed\nheterogeneous edge resources to a variety of services is a challenging task.\nWhile this problem has been studied extensively in recent years, most of the\nprevious work has largely ignored the preferences of the services when making\nedge resource allocation decisions. To this end, this paper introduces a novel\nbilevel optimization model, which explicitly takes the service preferences into\nconsideration, to study the interaction between an EC platform and multiple\nservices. The platform manages a set of edge nodes (ENs) and acts as the leader\nwhile the services are the followers. Given the service placement and resource\npricing decisions of the leader, each service decides how to optimally divide\nits workload to different ENs. The proposed framework not only maximizes the\nprofit of the platform but also minimizes the cost of every service. When there\nis a single EN, we derive a simple analytic solution for the underlying\nproblem. For the general case with multiple ENs and multiple services, we\npresent a Karush Kuhn Tucker based solution and a duality based solution,\ncombining with a series of linearizations, to solve the bilevel problem.\nExtensive numerical results are shown to illustrate the efficacy of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 08:29:43 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Nisha", "Tarannum", ""], ["Nguyen", "Duong Tung", ""], ["Bhargava", "Vijay K.", ""]]}, {"id": "2107.08402", "submitter": "Farnaz Tahmasebian", "authors": "Farnaz Tahmasebian, Jian Lou, and Li Xiong", "title": "RobustFed: A Truth Inference Approach for Robust Federated Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning is a prominent framework that enables clients (e.g.,\nmobile devices or organizations) to train a collaboratively global model under\na central server's orchestration while keeping local training datasets'\nprivacy. However, the aggregation step in federated learning is vulnerable to\nadversarial attacks as the central server cannot manage clients' behavior.\nTherefore, the global model's performance and convergence of the training\nprocess will be affected under such attacks.To mitigate this vulnerability\nissue, we propose a novel robust aggregation algorithm inspired by the truth\ninference methods in crowdsourcing via incorporating the worker's reliability\ninto aggregation. We evaluate our solution on three real-world datasets with a\nvariety of machine learning models. Experimental results show that our solution\nensures robust federated learning and is resilient to various types of attacks,\nincluding noisy data attacks, Byzantine attacks, and label flipping attacks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 09:34:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Tahmasebian", "Farnaz", ""], ["Lou", "Jian", ""], ["Xiong", "Li", ""]]}, {"id": "2107.08450", "submitter": "Babar Shahzaad", "authors": "Babar Shahzaad and Athman Bouguettaya and Sajib Mistry", "title": "Robust Composition of Drone Delivery Services under Uncertainty", "comments": "6 pages, 3 figures. This is an accepted paper and it is going to\n  appear in the Proceedings of the 2021 IEEE International Conference on Web\n  Services (IEEE ICWS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel robust composition framework for drone delivery services\nconsidering changes in the wind patterns in urban areas. The proposed framework\nincorporates the dynamic arrival of drone services at the recharging stations.\nWe propose a Probabilistic Forward Search (PFS) algorithm to select and compose\nthe best drone delivery services under uncertainty. A set of experiments with a\nreal drone dataset is conducted to illustrate the effectiveness and efficiency\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:45:05 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Shahzaad", "Babar", ""], ["Bouguettaya", "Athman", ""], ["Mistry", "Sajib", ""]]}, {"id": "2107.08490", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov and Qiben Yan", "title": "System-Wide Security for Offline Payment Terminals", "comments": "17th EAI International Conference on Security and Privacy in\n  Communication Networks (SecureComm 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most self-service payment terminals require network connectivity for\nprocessing electronic payments. The necessity to maintain network connectivity\nincreases costs, introduces cybersecurity risks, and significantly limits the\nnumber of places where the terminals can be installed. Leading payment service\nproviders have proposed offline payment solutions that rely on algorithmically\ngenerated payment tokens. Existing payment token solutions, however, require\ncomplex mechanisms for authentication, transaction management, and most\nimportantly, security risk management. In this paper, we present VolgaPay, a\nblockchain-based system that allows merchants to deploy secure offline payment\nterminal infrastructure that does not require collection and storage of any\nsensitive data. We design a novel payment protocol which mitigates security\nthreats for all the participants of VolgaPay, such that the maximum loss from\ngaining full access to any component by an adversary incurs only a limited\nscope of harm. We achieve significant enhancements in security, operation\nefficiency, and cost reduction via a combination of polynomial multi-hash chain\nmicropayment channels and blockchain grafting for off-chain channel state\ntransition. We implement the VolgaPay payment system, and with thorough\nevaluation and security analysis, we demonstrate that VolgaPay is capable of\ndelivering a fast, secure, and cost-efficient solution for offline payment\nterminals.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 16:50:57 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Yan", "Qiben", ""]]}, {"id": "2107.08496", "submitter": "Mingyue Ji", "authors": "Nicholas Woolsey, Joerg Kliewer, Rong-Rong Chen, Mingyue Ji", "title": "A Practical Algorithm Design and Evaluation for Heterogeneous Elastic\n  Computing with Stragglers", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Our extensive real measurements over Amazon EC2 show that the virtual\ninstances often have different computing speeds even if they share the same\nconfigurations. This motivates us to study heterogeneous Coded Storage Elastic\nComputing (CSEC) systems where machines, with different computing speeds, join\nand leave the network arbitrarily over different computing steps. In CSEC\nsystems, a Maximum Distance Separable (MDS) code is used for coded storage such\nthat the file placement does not have to be redefined with each elastic event.\nComputation assignment algorithms are used to minimize the computation time\ngiven computation speeds of different machines. While previous studies of\nheterogeneous CSEC do not include stragglers-the slow machines during the\ncomputation, we develop a new framework in heterogeneous CSEC that introduces\nstraggler tolerance. Based on this framework, we design a novel algorithm using\nour previously proposed approach for heterogeneous CSEC such that the system\ncan handle any subset of stragglers of a specified size while minimizing the\ncomputation time. Furthermore, we establish a trade-off in computation time and\nstraggler tolerance. Another major limitation of existing CSEC designs is the\nlack of practical evaluations using real applications. In this paper, we\nevaluate the performance of our designs on Amazon EC2 for applications of the\npower iteration and linear regression. Evaluation results show that the\nproposed heterogeneous CSEC algorithms outperform the state-of-the-art designs\nby more than 30%.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 17:13:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Woolsey", "Nicholas", ""], ["Kliewer", "Joerg", ""], ["Chen", "Rong-Rong", ""], ["Ji", "Mingyue", ""]]}, {"id": "2107.08538", "submitter": "Chris Porter", "authors": "Chao Chen, Chris Porter, Santosh Pande", "title": "Effective GPU Sharing Under Compiler Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computing platforms tend to deploy multiple GPUs (2, 4, or more) on a\nsingle node to boost system performance, with each GPU having a large capacity\nof global memory and streaming multiprocessors (SMs). GPUs are an expensive\nresource, and boosting utilization of GPUs without causing performance\ndegradation of individual workloads is an important and challenging problem.\nAlthough services like MPS support simultaneous execution of multiple\nco-operative kernels on a single device, they do not solve the above problem\nfor uncooperative kernels, MPS being oblivious to the resource needs of each\nkernel.\n  We propose a fully automated compiler-assisted scheduling framework. The\ncompiler constructs GPU tasks by identifying kernel launches and their related\nGPU operations (e.g. memory allocations). For each GPU task, a probe is\ninstrumented in the host-side code right before its launch point. At runtime,\nthe probe conveys the information about the task's resource requirements (e.g.\nmemory and compute cores) to a scheduler, such that the scheduler can place the\ntask on an appropriate device based on the task's resource requirements and\ndevices' load in a memory-safe, resource-aware manner. To demonstrate its\nadvantages, we prototyped a throughput-oriented scheduler based on the\nframework, and evaluated it with the Rodinia benchmark suite and the Darknet\nneural network framework on NVIDIA GPUs. The results show that the proposed\nsolution outperforms existing state-of-the-art solutions by leveraging its\nknowledge about applications' multiple resource requirements, which include\nmemory as well as SMs. It improves throughput by up to 2.5x for Rodinia\nbenchmarks, and up to 2.7x for Darknet neural networks. In addition, it\nimproves job turnaround time by up to 4.9x, and limits individual kernel\nperformance degradation to at most 2.5%.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 21:17:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Chen", "Chao", ""], ["Porter", "Chris", ""], ["Pande", "Santosh", ""]]}, {"id": "2107.08628", "submitter": "Soohyun Park", "authors": "Yoo Jeong Ha, Minjae Yoo, Soohyun Park, Soyi Jung, and Joongheon Kim", "title": "Secure Aerial Surveillance using Split Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal monitoring devices such as cyclist helmet cameras to record\naccidents or dash cams to catch collisions have proliferated, with more\ncompanies producing smaller and compact recording gadgets. As these devices are\nbecoming a part of citizens' everyday arsenal, concerns over the residents'\nprivacy are progressing. Therefore, this paper presents SASSL, a secure aerial\nsurveillance drone using split learning to classify whether there is a presence\nof a fire on the streets. This innovative split learning method transfers CCTV\nfootage captured with a drone to a nearby server to run a deep neural network\nto detect a fire's presence in real-time without exposing the original data. We\ndevise a scenario where surveillance UAVs roam around the suburb, recording any\nunnatural behavior. The UAV can process the recordings through its on-mobile\ndeep neural network system or transfer the information to a server. Due to the\nresource limitations of mobile UAVs, the UAV does not have the capacity to run\nan entire deep neural network on its own. This is where the split learning\nmethod comes in handy. The UAV runs the deep neural network only up to the\nfirst hidden layer and sends only the feature map to the cloud server, where\nthe rest of the deep neural network is processed. By ensuring that the learning\nprocess is divided between the UAV and the server, the privacy of raw data is\nsecured while the UAV does not overexert its minimal resources.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 05:58:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ha", "Yoo Jeong", ""], ["Yoo", "Minjae", ""], ["Park", "Soohyun", ""], ["Jung", "Soyi", ""], ["Kim", "Joongheon", ""]]}, {"id": "2107.08681", "submitter": "Jinke Ren", "authors": "Jinke Ren, Chonghe Liu, Guanding Yu, Dongning Guo", "title": "A New Distributed Method for Training Generative Adversarial Networks", "comments": "Submitted to IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are emerging machine learning models\nfor generating synthesized data similar to real data by jointly training a\ngenerator and a discriminator. In many applications, data and computational\nresources are distributed over many devices, so centralized computation with\nall data in one location is infeasible due to privacy and/or communication\nconstraints. This paper proposes a new framework for training GANs in a\ndistributed fashion: Each device computes a local discriminator using local\ndata; a single server aggregates their results and computes a global GAN.\nSpecifically, in each iteration, the server sends the global GAN to the\ndevices, which then update their local discriminators; the devices send their\nresults to the server, which then computes their average as the global\ndiscriminator and updates the global generator accordingly. Two different\nupdate schedules are designed with different levels of parallelism between the\ndevices and the server. Numerical results obtained using three popular datasets\ndemonstrate that the proposed framework can outperform a state-of-the-art\nframework in terms of convergence speed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:38:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ren", "Jinke", ""], ["Liu", "Chonghe", ""], ["Yu", "Guanding", ""], ["Guo", "Dongning", ""]]}, {"id": "2107.08716", "submitter": "Gagandeep Singh", "authors": "Gagandeep Singh, Dionysios Diamantopoulos, Juan G\\'omez-Luna,\n  Christoph Hagleitner, Sander Stuijk, Henk Corporaal, Onur Mutlu", "title": "NERO: Accelerating Weather Prediction using Near-Memory Reconfigurable\n  Fabric", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.08241,\n  arXiv:2106.06433", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing climate change calls for fast and accurate weather and climate\nmodeling. However, when solving large-scale weather prediction simulations,\nstate-of-the-art CPU and GPU implementations suffer from limited performance\nand high energy consumption. These implementations are dominated by complex\nirregular memory access patterns and low arithmetic intensity that pose\nfundamental challenges to acceleration. To overcome these challenges, we\npropose and evaluate the use of near-memory acceleration using a reconfigurable\nfabric with high-bandwidth memory (HBM). We focus on compound stencils that are\nfundamental kernels in weather prediction models. By using high-level synthesis\ntechniques, we develop NERO, an FPGA+HBM-based accelerator connected through\nIBM OCAPI (Open Coherent Accelerator Processor Interface) to an IBM POWER9 host\nsystem. Our experimental results show that NERO outperforms a 16-core POWER9\nsystem by 5.3x and 12.7x when running two different compound stencil kernels.\nNERO reduces the energy consumption by 12x and 35x for the same two kernels\nover the POWER9 system with an energy efficiency of 1.61 GFLOPS/Watt and 21.01\nGFLOPS/Watt. We conclude that employing near-memory acceleration solutions for\nweather prediction modeling is promising as a means to achieve both high\nperformance and high energy efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:41:19 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Singh", "Gagandeep", ""], ["Diamantopoulos", "Dionysios", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Hagleitner", "Christoph", ""], ["Stuijk", "Sander", ""], ["Corporaal", "Henk", ""], ["Mutlu", "Onur", ""]]}, {"id": "2107.08809", "submitter": "Guoqiang Zhang", "authors": "Guoqiang Zhang and Kenta Niwa and W. Bastiaan Kleijn", "title": "Revisiting the Primal-Dual Method of Multipliers for Optimisation over\n  Centralised Networks", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primal-dual method of multipliers (PDMM) was originally designed for\nsolving a decomposable optimisation problem over a general network. In this\npaper, we revisit PDMM for optimisation over a centralized network. We first\nnote that the recently proposed method FedSplit [1] implements PDMM for a\ncentralized network. In [1], Inexact FedSplit (i.e., gradient based FedSplit)\nwas also studied both empirically and theoretically. We identify the cause for\nthe poor reported performance of Inexact FedSplit, which is due to the improper\ninitialisation in the gradient operations at the client side. To fix the issue\nof Inexact FedSplit, we propose two versions of Inexact PDMM, which are\nreferred to as gradient-based PDMM (GPDMM) and accelerated GPDMM (AGPDMM),\nrespectively. AGPDMM accelerates GPDMM at the cost of transmitting two times\nthe number of parameters from the server to each client per iteration compared\nto GPDMM. We provide a new convergence bound for GPDMM for a class of convex\noptimisation problems. Our new bounds are tighter than those derived for\nInexact FedSplit. We also investigate the update expressions of AGPDMM and\nSCAFFOLD to find their similarities. It is found that when the number K of\ngradient steps at the client side per iteration is K=1, both AGPDMM and\nSCAFFOLD reduce to vanilla gradient descent with proper parameter setup.\nExperimental results indicate that AGPDMM converges faster than SCAFFOLD when\nK>1 while GPDMM converges slightly worse than SCAFFOLD.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 12:44:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Zhang", "Guoqiang", ""], ["Niwa", "Kenta", ""], ["Kleijn", "W. Bastiaan", ""]]}, {"id": "2107.08873", "submitter": "Guang Yang", "authors": "Guang Yang, Ke Mu, Chunhe Song, Zhijia Yang, and Tierui Gong", "title": "RingFed: Reducing Communication Costs in Federated Learning on Non-IID\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated learning is a widely used distributed deep learning framework that\nprotects the privacy of each client by exchanging model parameters rather than\nraw data. However, federated learning suffers from high communication costs, as\na considerable number of model parameters need to be transmitted many times\nduring the training process, making the approach inefficient, especially when\nthe communication network bandwidth is limited. This article proposes RingFed,\na novel framework to reduce communication overhead during the training process\nof federated learning. Rather than transmitting parameters between the center\nserver and each client, as in original federated learning, in the proposed\nRingFed, the updated parameters are transmitted between each client in turn,\nand only the final result is transmitted to the central server, thereby\nreducing the communication overhead substantially. After several local updates,\nclients first send their parameters to another proximal client, not to the\ncenter server directly, to preaggregate. Experiments on two different public\ndatasets show that RingFed has fast convergence, high model accuracy, and low\ncommunication cost.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:43:10 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yang", "Guang", ""], ["Mu", "Ke", ""], ["Song", "Chunhe", ""], ["Yang", "Zhijia", ""], ["Gong", "Tierui", ""]]}, {"id": "2107.09308", "submitter": "Xiang He", "authors": "Xiang He, Zhiying Tu, Markus Wagner, Xiaofei Xu, and Zhongjie Wang", "title": "Online Deployment Algorithms for Microservice Systems with Complex\n  Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud and edge computing have been widely adopted in many application\nscenarios. With the increasing demand of fast iteration and complexity of\nbusiness logic, it is challenging to achieve rapid development and continuous\ndelivery in such highly distributed cloud and edge computing environment. At\npresent, microservice-based architecture has been the dominant deployment\nstyle, and a microservice system has to evolve agilely to offer stable Quality\nof Service (QoS) in the situation where user requirement changes frequently.\nMany research have been conducted to optimally re-deploy microservices to adapt\nto changing requirements. Nevertheless, complex dependencies between\nmicroservices and the existence of multiple instances of one single\nmicroservice in a microservice system have not been fully considered in\nexisting works. This paper defines SPPMS, the Service Placement Problem in\nMicroservice Systems that feature complex dependencies and multiple instances,\nas a Fractional Polynomial Problem (FPP) . Considering the high computation\ncomplexity of FPP, it is then transformed into a Quadratic Sum-of-Ratios\nFractional Problem (QSRFP) which is further solved by the proposed greedy-based\nalgorithms. Experiments demonstrate that our models and algorithms outperform\nexisting approaches in both quality and computation speed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:49:45 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["He", "Xiang", ""], ["Tu", "Zhiying", ""], ["Wagner", "Markus", ""], ["Xu", "Xiaofei", ""], ["Wang", "Zhongjie", ""]]}, {"id": "2107.09309", "submitter": "Mohanad Odema", "authors": "Mohanad Odema, Nafiul Rashid, Berken Utku Demirel, Mohammad Abdullah\n  Al Faruque", "title": "LENS: Layer Distribution Enabled Neural Architecture Search in\n  Edge-Cloud Hierarchies", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-Cloud hierarchical systems employing intelligence through Deep Neural\nNetworks (DNNs) endure the dilemma of workload distribution within them.\nPrevious solutions proposed to distribute workloads at runtime according to the\nstate of the surroundings, like the wireless conditions. However, such\nconditions are usually overlooked at design time. This paper addresses this\nissue for DNN architectural design by presenting a novel methodology, LENS,\nwhich administers multi-objective Neural Architecture Search (NAS) for\ntwo-tiered systems, where the performance objectives are refashioned to\nconsider the wireless communication parameters. From our experimental search\nspace, we demonstrate that LENS improves upon the traditional solution's Pareto\nset by 76.47% and 75% with respect to the energy and latency metrics,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 07:53:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Odema", "Mohanad", ""], ["Rashid", "Nafiul", ""], ["Demirel", "Berken Utku", ""], ["Faruque", "Mohammad Abdullah Al", ""]]}, {"id": "2107.09461", "submitter": "Zhize Li", "authors": "Zhize Li, Peter Richt\\'arik", "title": "CANITA: Faster Rates for Distributed Convex Optimization with\n  Communication Compression", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the high communication cost in distributed and federated learning,\nmethods relying on compressed communication are becoming increasingly popular.\nBesides, the best theoretically and practically performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of communications (faster convergence), e.g., Nesterov's accelerated\ngradient descent (Nesterov, 2004) and Adam (Kingma and Ba, 2014). In order to\ncombine the benefits of communication compression and convergence acceleration,\nwe propose a \\emph{compressed and accelerated} gradient method for distributed\noptimization, which we call CANITA. Our CANITA achieves the \\emph{first\naccelerated rate}\n$O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} +\n\\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon\nthe state-of-the-art non-accelerated rate\n$O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} +\n\\frac{\\omega^2+n}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA (Khaled et al.,\n2020b) for distributed general convex problems, where $\\epsilon$ is the target\nerror, $L$ is the smooth parameter of the objective, $n$ is the number of\nmachines/devices, and $\\omega$ is the compression parameter (larger $\\omega$\nmeans more compression can be applied, and no compression implies $\\omega=0$).\nOur results show that as long as the number of devices $n$ is large (often true\nin distributed/federated learning), or the compression $\\omega$ is not very\nhigh, CANITA achieves the faster convergence rate\n$O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication\nrounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs.\n$O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result,\nCANITA enjoys the advantages of both compression (compressed communication in\neach round) and acceleration (much fewer communication rounds).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:01:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Li", "Zhize", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2107.09558", "submitter": "Hieu Tran", "authors": "Hieu Tran, Son Nguyen, I-Ling Yen, Farokh Bastani", "title": "Into Summarization Techniques for IoT Data Discovery Routing", "comments": "10 pages, 8 figures", "journal-ref": "IEEE International Conference on Cloud Computing 2021 (IEEE CLOUD\n  2021)", "doi": null, "report-no": null, "categories": "cs.NI cs.DB cs.DC cs.IR cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the IoT data discovery problem in very large and\ngrowing scale networks. Specifically, we investigate in depth the routing table\nsummarization techniques to support effective and space-efficient IoT data\ndiscovery routing. Novel summarization algorithms, including alphabetical\nbased, hash based, and meaning based summarization and their corresponding\ncoding schemes are proposed. The issue of potentially misleading routing due to\nsummarization is also investigated. Subsequently, we analyze the strategy of\nwhen to summarize in order to balance the tradeoff between the routing table\ncompression rate and the chance of causing misleading routing. For experimental\nstudy, we have collected 100K IoT data streams from various IoT databases as\nthe input dataset. Experimental results show that our summarization solution\ncan reduce the routing table size by 20 to 30 folds with 2-5% increase in\nlatency when compared with similar peer-to-peer discovery routing algorithms\nwithout summarization. Also, our approach outperforms DHT based approaches by 2\nto 6 folds in terms of latency and traffic.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:22:16 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 14:52:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tran", "Hieu", ""], ["Nguyen", "Son", ""], ["Yen", "I-Ling", ""], ["Bastani", "Farokh", ""]]}, {"id": "2107.09657", "submitter": "Mingyue Ji", "authors": "Mingyue Ji, Xiang Zhang, Kai Wan", "title": "A New Design Framework for Heterogeneous Uncoded Storage Elastic\n  Computing", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Elasticity is one important feature in modern cloud computing systems and can\nresult in computation failure or significantly increase computing time. Such\nelasticity means that virtual machines over the cloud can be preempted under a\nshort notice (e.g., hours or minutes) if a high-priority job appears; on the\nother hand, new virtual machines may become available over time to compensate\nthe computing resources. Coded Storage Elastic Computing (CSEC) introduced by\nYang et al. in 2018 is an effective and efficient approach to overcome the\nelasticity and it costs relatively less storage and computation load. However,\none of the limitations of the CSEC is that it may only be applied to certain\ntypes of computations (e.g., linear) and may be challenging to be applied to\nmore involved computations because the coded data storage and approximation are\noften needed. Hence, it may be preferred to use uncoded storage by directly\ncopying data into the virtual machines. In addition, based on our own\nmeasurement, virtual machines on Amazon EC2 clusters often have heterogeneous\ncomputation speed even if they have exactly the same configurations (e.g., CPU,\nRAM, I/O cost). In this paper, we introduce a new optimization framework on\nUncoded Storage Elastic Computing (USEC) systems with heterogeneous computing\nspeed to minimize the overall computation time. Under this framework, we\npropose optimal solutions of USEC systems with or without straggler tolerance\nusing different storage placements. Our proposed algorithms are evaluated using\npower iteration applications on Amazon EC2.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 17:44:24 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 05:05:52 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ji", "Mingyue", ""], ["Zhang", "Xiang", ""], ["Wan", "Kai", ""]]}, {"id": "2107.09707", "submitter": "David Lajeunesse", "authors": "David Lajeunesse and Hugo D. Scolnik", "title": "A Cooperative Optimal Mining Model for Bitcoin", "comments": "8 pages, 2 figures, Accepted to 2021 3rd Conference on Blockchain\n  Research & Applications for Innovative Networks and Services (BRAINS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze Bitcoin mining from the perspective of a game and propose an\noptimal mining model that maximizes profits of pools and miners. The model is a\ntwo-stage Stackelberg game in which each stage forms a sub-game. In stage I,\npools are the leaders who assign a computing power to be consumed by miners. In\nstage II, miners decide of their power consumption and distribution. They find\nthemselves in a social dilemma in which they must choose between mining in\nsolo, therefore prioritizing their individual preferences, and participating in\na pool for the collective interest. The model relies on a pool protocol based\non a simulated game in which the miners compete for the reward won by the pool.\nThe solutions for the stage I sub-game and the simulated protocol game are\nunique and stable Nash equilibriums while the stage II sub-game leads to a\nstable cooperative equilibrium only when miners choose their strategies\naccording to certain criteria. We conclude that the cooperative optimal mining\nmodel has the potential to favor Bitcoin decentralization and stability.\nMainly, the social dilemma faced by miners together with the balance of\nincentives ensure a certain distribution of the network computing power between\npools and solo miners, while equilibriums in the game solutions provide\nstability to the system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:20:20 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Lajeunesse", "David", ""], ["Scolnik", "Hugo D.", ""]]}, {"id": "2107.09834", "submitter": "Caleb Ju", "authors": "Caleb Ju, Yifan Zhang, Edgar Solomonik", "title": "Communication Lower Bounds for Nested Bilinear Algorithms", "comments": "32 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop lower bounds on communication in the memory hierarchy or between\nprocessors for nested bilinear algorithms, such as Strassen's algorithm for\nmatrix multiplication. We build on a previous framework that establishes\ncommunication lower bounds by use of the rank expansion, or the minimum rank of\nany fixed size subset of columns of a matrix, for each of the three matrices\nencoding the bilinear algorithm. This framework provides lower bounds for any\nway of computing a bilinear algorithm, which encompasses a larger space of\nalgorithms than by fixing a particular dependency graph. Nested bilinear\nalgorithms include fast recursive algorithms for convolution, matrix\nmultiplication, and contraction of tensors with symmetry. Two bilinear\nalgorithms can be nested by taking Kronecker products between their encoding\nmatrices. Our main result is a lower bound on the rank expansion of a matrix\nconstructed by a Kronecker product derived from lower bounds on the rank\nexpansion of the Kronecker product's operands. To prove this bound, we map a\nsubset of columns from a submatrix to a 2D grid, collapse them into a dense\ngrid, expand the grid, and use the size of the expanded grid to bound the\nnumber of linearly independent columns of the submatrix. We apply the rank\nexpansion lower bounds to obtain novel communication lower bounds for nested\nToom-Cook convolution, Strassen's algorithm, and fast algorithms for partially\nsymmetric contractions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 02:01:57 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Ju", "Caleb", ""], ["Zhang", "Yifan", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2107.09886", "submitter": "Quang Minh Nguyen", "authors": "Minh Quang Nguyen, Dumitrel Loghin, Tien Tuan Anh Dinh", "title": "Understanding the Scalability of Hyperledger Fabric", "comments": "10 pages, BCDL 2019 in conjunction with ACM VLDB. Los Angeles, USA,\n  26-30 Aug 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth of blockchain systems leads to increasing interest in\nunderstanding and comparing blockchain performance at scale. In this paper, we\nfocus on analyzing the performance of Hyperledger Fabric v1.1 - one of the most\npopular permissioned blockchain systems. Prior works have analyzed Hyperledger\nFabric v0.6 in depth, but newer versions of the system undergo significant\nchanges that warrant new analysis. Existing works on benchmarking the system\nare limited in their scope: some consider only small networks, others consider\nscalability of only parts of the system instead of the whole. We perform a\ncomprehensive performance analysis of Hyperledger Fabric v1.1 at scale. We\nextend an existing benchmarking tool to conduct experiments over many servers\nwhile scaling all important components of the system. Our results demonstrate\nthat Fabric v1.1's scalability bottlenecks lie in the communication overhead\nbetween the execution and ordering phase. Furthermore, we show that scaling the\nKafka cluster that is used for the ordering phase does not affect the overall\nthroughput.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 05:57:31 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Nguyen", "Minh Quang", ""], ["Loghin", "Dumitrel", ""], ["Dinh", "Tien Tuan Anh", ""]]}, {"id": "2107.10008", "submitter": "Mohak Chadha", "authors": "Mohak Chadha, Anshul Jindal, Michael Gerndt", "title": "Architecture-Specific Performance Optimization of Compute-Intensive FaaS\n  Functions", "comments": "Extended version IEEE CLOUD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FaaS allows an application to be decomposed into functions that are executed\non a FaaS platform. The FaaS platform is responsible for the resource\nprovisioning of the functions. Recently, there is a growing trend towards the\nexecution of compute-intensive FaaS functions that run for several seconds.\nHowever, due to the billing policies followed by commercial FaaS offerings, the\nexecution of these functions can incur significantly higher costs. Moreover,\ndue to the abstraction of underlying processor architectures on which the\nfunctions are executed, the performance optimization of these functions is\nchallenging. As a result, most FaaS functions use pre-compiled libraries\ngeneric to x86-64 leading to performance degradation. In this paper, we examine\nthe underlying processor architectures for Google Cloud Functions (GCF) and\ndetermine their prevalence across the 19 available GCF regions. We modify,\nadapt, and optimize three compute-intensive FaaS workloads written in Python\nusing Numba, a JIT compiler based on LLVM, and present results wrt performance,\nmemory consumption, and costs on GCF. Results from our experiments show that\nthe optimization of FaaS functions can improve performance by 12.8x (geometric\nmean) and save costs by 73.4% on average for the three functions. Our results\nshow that optimization of the FaaS functions for the specific architecture is\nvery important. We achieved a maximum speedup of 1.79x by tuning the function\nespecially for the instruction set of the underlying processor architecture.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:11:19 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Chadha", "Mohak", ""], ["Jindal", "Anshul", ""], ["Gerndt", "Michael", ""]]}, {"id": "2107.10018", "submitter": "Victor A. Melent'ev", "authors": "V.A. Melent'ev", "title": "Formal method of synthesis of optimal topologies of computing systems\n  based on projective description of graphs", "comments": "In Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deterministic method for synthesizing the interconnect topologies optimized\nfor the required properties is proposed. The method is based on the original\ndescription of graphs by projections, on establishing the bijective\ncorrespondence of the required properties and the projection properties of the\ninitial graph, on postulating the corresponding restrictions of modified\nprojections and on iteratively applying these restrictions to them either until\nthe projection system is solved and the projections of the desired graph are\nobtained, or until its incompatibility with the given initial conditions is\nrevealed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 11:28:06 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 03:25:18 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Melent'ev", "V. A.", ""]]}, {"id": "2107.10448", "submitter": "Weiqi Li", "authors": "Weiqi Li, Zhen Chen, Zhiying Wang, Syed A. Jafar, Hamid Jafarkhani", "title": "Flexible Distributed Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed matrix multiplication problem with an unknown number of\nstragglers is considered, where the goal is to efficiently and flexibly obtain\nthe product of two massive matrices by distributing the computation across N\nservers. There are up to N - R stragglers but the exact number is not known a\npriori. Motivated by reducing the computation load of each server, a flexible\nsolution is proposed to fully utilize the computation capability of available\nservers. The computing task for each server is separated into several subtasks,\nconstructed based on Entangled Polynomial codes by Yu et al. The final results\ncan be obtained from either a larger number of servers with a smaller amount of\ncomputation completed per server or a smaller number of servers with a larger\namount of computation completed per server. The required finite field size of\nthe proposed solution is less than 2N. Moreover, the optimal design parameters\nsuch as the partitioning of the input matrices is discussed. Our constructions\ncan also be generalized to other settings such as batch distributed matrix\nmultiplication and secure distributed matrix multiplication.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:12:41 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Li", "Weiqi", ""], ["Chen", "Zhen", ""], ["Wang", "Zhiying", ""], ["Jafar", "Syed A.", ""], ["Jafarkhani", "Hamid", ""]]}, {"id": "2107.10467", "submitter": "Qing Zhang", "authors": "Qing Zhang, Xueping Gong, Huizhong Li, Hao Wu, Jiheng Zhang", "title": "Improving Blockchain Consistency by Assigning Weights to Random Blocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains based on the celebrated Nakamoto consensus protocol have shown\npromise in several applications, including cryptocurrencies. However, these\nblockchains have inherent scalability limits caused by the protocol's consensus\nproperties. In particular, the consistency property demonstrates a tight\ntrade-off between block production speed and the system's security in terms of\nresisting adversarial attacks. This paper proposes a novel method, Ironclad,\nthat improves blockchain consistency by assigning different weights to randomly\nselected blocks. We analyze the fundamental properties of our method and show\nthat the combination of our method with Nakamoto consensus protocols can lead\nto significant improvement in consistency. A direct result is that\nNakamoto+Ironclad can enable a much faster ($10\\sim 50$ times with normal\nparameter settings) block production rate than Nakamoto protocol under the same\nsecurity guarantee with the same proportion of malicious mining power.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 05:56:57 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhang", "Qing", ""], ["Gong", "Xueping", ""], ["Li", "Huizhong", ""], ["Wu", "Hao", ""], ["Zhang", "Jiheng", ""]]}, {"id": "2107.10558", "submitter": "Kostas Kolomvatsos", "authors": "Kostas Kolomvatsos, Christos Anagnostopoulos", "title": "A Proactive Management Scheme for Data Synopses at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of the infrastructure provided by the Internet of Things\n(IoT) with numerous processing nodes present at the Edge Computing (EC)\necosystem opens up new pathways to support intelligent applications. Such\napplications can be provided upon humongous volumes of data collected by IoT\ndevices being transferred to the edge nodes through the network. Various\nprocessing activities can be performed on the discussed data and multiple\ncollaborative opportunities between EC nodes can facilitate the execution of\nthe desired tasks. In order to support an effective interaction between edge\nnodes, the knowledge about the geographically distributed data should be\nshared. Obviously, the migration of large amounts of data will harm the\nstability of the network stability and its performance. In this paper, we\nrecommend the exchange of data synopses than real data between EC nodes to\nprovide them with the necessary knowledge about peer nodes owning similar data.\nThis knowledge can be valuable when considering decisions such as data/service\nmigration and tasks offloading. We describe an continuous reasoning model that\nbuilds a temporal similarity map of the available datasets to get nodes\nunderstanding the evolution of data in their peers. We support the proposed\ndecision making mechanism through an intelligent similarity extraction scheme\nbased on an unsupervised machine learning model, and, at the same time, combine\nit with a statistical measure that represents the trend of the so-called\ndiscrepancy quantum. Our model can reveal the differences in the exchanged\nsynopses and provide a datasets similarity map which becomes the appropriate\nknowledge base to support the desired processing activities. We present the\nproblem under consideration and suggest a solution for that, while, at the same\ntime, we reveal its advantages and disadvantages through a large number of\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:22:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kolomvatsos", "Kostas", ""], ["Anagnostopoulos", "Christos", ""]]}, {"id": "2107.10706", "submitter": "Aleksandr Beznosikov", "authors": "Aleksandr Beznosikov, Gesualdo Scutari, Alexander Rogozin, Alexander\n  Gasnikov", "title": "Distributed Saddle-Point Problems Under Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study solution methods for (strongly-)convex-(strongly)-concave\nSaddle-Point Problems (SPPs) over networks of two type - master/workers (thus\ncentralized) architectures and meshed (thus decentralized) networks. The local\nfunctions at each node are assumed to be similar, due to statistical data\nsimilarity or otherwise. We establish lower complexity bounds for a fairly\ngeneral class of algorithms solving the SPP. We show that a given suboptimality\n$\\epsilon>0$ is achieved over master/workers networks in\n$\\Omega\\big(\\Delta\\cdot \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of\ncommunications, where $\\delta>0$ measures the degree of similarity of the local\nfunctions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the\ndiameter of the network. The lower communication complexity bound over meshed\nnetworks reads $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot {\\delta}/{\\mu}\\cdot\\log\n(1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip\nmatrix used for the communication between neighbouring nodes. We then propose\nalgorithms matching the lower bounds over either types of networks (up to\nlog-factors). We assess the effectiveness of the proposed algorithms on a\nrobust logistic regression problem.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 14:25:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Beznosikov", "Aleksandr", ""], ["Scutari", "Gesualdo", ""], ["Rogozin", "Alexander", ""], ["Gasnikov", "Alexander", ""]]}, {"id": "2107.10881", "submitter": "Cosimo Sguanci", "authors": "Cosimo Sguanci, Roberto Spatafora, Andrea Mario Vergani", "title": "Layer 2 Blockchain Scaling: a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Blockchain technology is affected by massive limitations in scalability with\nconsequent repercussions on performance. This discussion aims at analyzing the\nstate of the art of current available Layer II solutions to overcome these\nlimitations, both focusing on theoretical and practical aspects and\nhighlighting the main differences among the examined frameworks. The structure\nof the work is based on three major sections. In particular, the first one is\nan introductory part about the technology, the scalability issue and Layer II\nas a solution. The second section represents the core of the discussion and\nconsists of three different subsections, each with a detailed examination of\nthe respective solution (Lightning Network, Plasma, Rollups); the analysis of\neach solution is based on how it affects five key aspects of blockchain\ntechnology and Layer II: scalability, security, decentralization, privacy, fees\nand micropayments (the last two are analyzed together given their high\ncorrelation). Finally, the third section includes a tabular summary, followed\nby a detailed description of a use-case specifically thought for a practical\nevaluation of the presented frameworks. The results of the work met\nexpectations: all solutions effectively contribute to increasing scalability. A\ncrucial clarification is that none of the three dominates the others in all\npossible fields of application, and the consequences in adopting each, are\ndifferent. Therefore, the choice depends on the application context, and a\ntrade-off must be found between the aspects previously mentioned.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 18:40:14 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Sguanci", "Cosimo", ""], ["Spatafora", "Roberto", ""], ["Vergani", "Andrea Mario", ""]]}, {"id": "2107.10976", "submitter": "Muhammad Asad", "authors": "Muhammad Asad, Ahmed Moustafa, and Takayuki Ito", "title": "Federated Learning Versus Classical Machine Learning: A Convergence\n  Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past few decades, machine learning has revolutionized data processing\nfor large scale applications. Simultaneously, increasing privacy threats in\ntrending applications led to the redesign of classical data training models. In\nparticular, classical machine learning involves centralized data training,\nwhere the data is gathered, and the entire training process executes at the\ncentral server. Despite significant convergence, this training involves several\nprivacy threats on participants' data when shared with the central cloud\nserver. To this end, federated learning has achieved significant importance\nover distributed data training. In particular, the federated learning allows\nparticipants to collaboratively train the local models on local data without\nrevealing their sensitive information to the central cloud server. In this\npaper, we perform a convergence comparison between classical machine learning\nand federated learning on two publicly available datasets, namely,\nlogistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.\nThe simulation results demonstrate that federated learning achieves higher\nconvergence within limited communication rounds while maintaining participants'\nanonymity. We hope that this research will show the benefits and help federated\nlearning to be implemented widely.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:14:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Asad", "Muhammad", ""], ["Moustafa", "Ahmed", ""], ["Ito", "Takayuki", ""]]}, {"id": "2107.10979", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov, Hanqing Guo, and Qiben Yan", "title": "Rectifying Administrated ERC20 Tokens", "comments": "23rd International Conference on Information and Communications\n  Security (ICICS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The developers of Ethereum smart contracts often implement administrating\npatterns, such as censoring certain users, creating or destroying balances on\ndemand, destroying smart contracts, or injecting arbitrary code. These routines\nturn an ERC20 token into an administrated token - the type of Ethereum smart\ncontract that we scrutinize in this research. We discover that many smart\ncontracts are administrated, and the owners of these tokens carry lesser social\nand legal responsibilities compared to the traditional centralized actors that\nthose tokens intend to disrupt. This entails two major problems: a) the owners\nof the tokens have the ability to quickly steal all the funds and disappear\nfrom the market; and b) if the private key of the owner's account is stolen,\nall the assets might immediately turn into the property of the attacker. We\ndevelop a pattern recognition framework based on 9 syntactic features\ncharacterizing administrated ERC20 tokens, which we use to analyze existing\nsmart contracts deployed on Ethereum Mainnet. Our analysis of 84,062 unique\nEthereum smart contracts reveals that nearly 58% of them are administrated\nERC20 tokens, which accounts for almost 90% of all ERC20 tokens deployed on\nEthereum. To protect users from the frivolousness of unregulated token owners\nwithout depriving the ability of these owners to properly manage their tokens,\nwe introduce SafelyAdministrated - a library that enforces a responsible\nownership and management of ERC20 tokens. The library introduces three\nmechanisms: deferred maintenance, board of trustees and safe pause. We\nimplement and test SafelyAdministrated in the form of Solidity abstract\ncontract, which is ready to be used by the next generation of safely\nadministrated ERC20 tokens.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 18:40:34 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Guo", "Hanqing", ""], ["Yan", "Qiben", ""]]}, {"id": "2107.10987", "submitter": "Patrick Diehl", "authors": "Patrick Diehl and Gregor Dai{\\ss} and Dominic Marcello and Kevin Huck\n  and Sagiv Shiber and Hartmut Kaiser and Juhan Frank and Dirk Pfl\\\"uger", "title": "Octo-Tiger's New Hydro Module and Performance Using HPX+CUDA on ORNL's\n  Summit", "comments": "Accepted to IEEE Cluster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Octo-Tiger is a code for modeling three-dimensional self-gravitating\nastrophysical fluids. It was particularly designed for the study of dynamical\nmass transfer between interacting binary stars. Octo-Tiger is parallelized for\ndistributed systems using the asynchronous many-task runtime system, the C++\nstandard library for parallelism and concurrency (HPX) and utilizes CUDA for\nits gravity solver. Recently, we have remodeled Octo-Tiger's hydro solver to\nuse a three-dimensional reconstruction scheme. In addition, we have ported the\nhydro solver to GPU using CUDA kernels. We present scaling results for the new\nhydro kernels on ORNL's Summit machine using a Sedov-Taylor blast wave problem.\nWe also compare Octo-Tiger's new hydro scheme with its old hydro scheme, using\na rotating star as a test problem.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 01:37:58 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 16:45:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Diehl", "Patrick", ""], ["Dai\u00df", "Gregor", ""], ["Marcello", "Dominic", ""], ["Huck", "Kevin", ""], ["Shiber", "Sagiv", ""], ["Kaiser", "Hartmut", ""], ["Frank", "Juhan", ""], ["Pfl\u00fcger", "Dirk", ""]]}, {"id": "2107.11144", "submitter": "Christian Berger", "authors": "Christian Berger, Hans P. Reiser, Alysson Bessani", "title": "Making Reads in BFT State Machine Replication Fast, Linearizable, and\n  Live", "comments": "12 pages, to appear in the 40th IEEE Symposium on Reliable\n  Distributed Systems (SRDS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical Byzantine Fault Tolerance (PBFT) is a seminal state machine\nreplication protocol that achieves a performance comparable to non-replicated\nsystems in realistic environments. A reason for such high performance is the\nset of optimizations introduced in the protocol. One of these optimizations is\nread-only requests, a particular type of client request which avoids running\nthe three-step agreement protocol and allows replicas to respond directly, thus\nreducing the latency of reads from five to two communication steps. Given\nPBFT's broad influence, its design and optimizations influenced many BFT\nprotocols and systems that followed, e.g., BFT-SMaRt. We show, for the first\ntime, that the read-only request optimization introduced in PBFT more than 20\nyears ago can violate its liveness. Notably, the problem affects not only the\noptimized read-only operations but also standard, totally-ordered operations.\nWe show this weakness by presenting an attack in which a malicious leader\nblocks correct clients and present two solutions for patching the protocol,\nmaking read-only operations fast and correct. The two solutions were\nimplemented on BFT-SMaRt and evaluated in different scenarios, showing their\neffectiveness in preventing the identified attack.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 11:36:10 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Berger", "Christian", ""], ["Reiser", "Hans P.", ""], ["Bessani", "Alysson", ""]]}, {"id": "2107.11331", "submitter": "Orestis Alpos", "authors": "Orestis Alpos, Christian Cachin, Luca Zanolini", "title": "How to Trust Strangers: Composition of Byzantine Quorum Systems", "comments": "To appear in the proceedings of SRDS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trust is the basis of any distributed, fault-tolerant, or secure system. A\ntrust assumption specifies the failures that a system, such as a blockchain\nnetwork, can tolerate and determines the conditions under which it operates\ncorrectly. In systems subject to Byzantine faults, the trust assumption is\nusually specified through sets of processes that may fail together. Trust has\ntraditionally been symmetric, such that all processes in the system adhere to\nthe same, global assumption about potential faults. Recently, asymmetric trust\nmodels have also been considered, especially in the context of blockchains,\nwhere every participant is free to choose who to trust.\n  In both cases, it is an open question how to compose trust assumptions.\nConsider two or more systems, run by different and possibly disjoint sets of\nparticipants, with different assumptions about faults: how can they work\ntogether? This work answers this question for the first time and offers\ncomposition rules for symmetric and for asymmetric quorum systems. These rules\nare static and do not require interaction or agreement on the new trust\nassumption among the participants. Moreover, they ensure that if the original\nsystems allow for running a particular protocol (guaranteeing consistency and\navailability), then so will the joint system. At the same time, the composed\nsystem tolerates as many faults as possible, subject to the underlying\nconsistency and availability properties.\n  Reaching consensus with asymmetric trust in the model of personal Byzantine\nquorum systems (Losa et al., DISC 2019) was shown to be impossible, if the\ntrust assumptions of the processes diverge from each other. With asymmetric\nquorum systems, and by applying our composition rule, we show how consensus is\nactually possible, even with the combination of disjoint sets of processes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:19:06 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Alpos", "Orestis", ""], ["Cachin", "Christian", ""], ["Zanolini", "Luca", ""]]}, {"id": "2107.11415", "submitter": "Chung-Hsuan Hu", "authors": "Chung-Hsuan Hu, Zheng Chen, Erik G. Larsson", "title": "Device Scheduling and Update Aggregation Policies for Asynchronous\n  Federated Learning", "comments": "5 pages, 4 figures, accepted in 22nd IEEE international workshop on\n  signal processing advances in wireless communications (SPAWC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC cs.IT eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is a newly emerged decentralized machine learning\n(ML) framework that combines on-device local training with server-based model\nsynchronization to train a centralized ML model over distributed nodes. In this\npaper, we propose an asynchronous FL framework with periodic aggregation to\neliminate the straggler issue in FL systems. For the proposed model, we\ninvestigate several device scheduling and update aggregation policies and\ncompare their performances when the devices have heterogeneous computation\ncapabilities and training data distributions. From the simulation results, we\nconclude that the scheduling and aggregation design for asynchronous FL can be\nrather different from the synchronous case. For example, a norm-based\nsignificance-aware scheduling policy might not be efficient in an asynchronous\nFL setting, and an appropriate \"age-aware\" weighting design for the model\naggregation can greatly improve the learning performance of such systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:57:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Hu", "Chung-Hsuan", ""], ["Chen", "Zheng", ""], ["Larsson", "Erik G.", ""]]}, {"id": "2107.11417", "submitter": "Biswadip Maity", "authors": "Tiago M\\\"uck, Bryan Donyanavard, Biswadip Maity, Kasra Moazzemi, Nikil\n  Dutt", "title": "MARS: Middleware for Adaptive Reflective Computer Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-adaptive approaches for runtime resource management of manycore\ncomputing platforms often require a runtime model of the system that represents\nthe software organization or the architecture of the target platform. The\nincreasing heterogeneity in a platform's resource types and the interactions\nbetween resources pose challenges for coordinated model-based decision making\nin the face of dynamic workloads. Self-awareness properties address these\nchallenges for emerging heterogeneous manycore processing (HMP) platforms\nthrough reflective resource managers. However, with HMP computing platform\narchitectures evolving rapidly, porting the self-aware decision logic across\ndifferent hardware platforms is challenging, requiring resource managers to\nupdate their models and platform-specific interfaces. We propose MARS\n(Middleware for Adaptive and Reflective Systems), a cross-layer and\nmulti-platform framework that allows users to easily create resource managers\nby composing system models and resource management policies in a flexible and\ncoordinated manner. MARS consists of a generic user-level sensing/actuation\ninterface that allows for portable policy design, and a reflective system model\nused to coordinate multiple policies. We demonstrate MARS' interaction across\nmultiple layers of the system stack through a dynamic voltage and frequency\nscaling (DVFS) policy example which can run on any Linux-based HMP computing\nplatform.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:58:52 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["M\u00fcck", "Tiago", ""], ["Donyanavard", "Bryan", ""], ["Maity", "Biswadip", ""], ["Moazzemi", "Kasra", ""], ["Dutt", "Nikil", ""]]}, {"id": "2107.11513", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Yibo Xu, Yonggui Yan, Jie Chen", "title": "Distributed stochastic inertial methods with delayed derivatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient methods (SGMs) are predominant approaches for solving\nstochastic optimization. On smooth nonconvex problems, a few acceleration\ntechniques have been applied to improve the convergence rate of SGMs. However,\nlittle exploration has been made on applying a certain acceleration technique\nto a stochastic subgradient method (SsGM) for nonsmooth nonconvex problems. In\naddition, few efforts have been made to analyze an (accelerated) SsGM with\ndelayed derivatives. The information delay naturally happens in a distributed\nsystem, where computing workers do not coordinate with each other.\n  In this paper, we propose an inertial proximal SsGM for solving nonsmooth\nnonconvex stochastic optimization problems. The proposed method can have\nguaranteed convergence even with delayed derivative information in a\ndistributed environment. Convergence rate results are established to three\nclasses of nonconvex problems: weakly-convex nonsmooth problems with a convex\nregularizer, composite nonconvex problems with a nonsmooth convex regularizer,\nand smooth nonconvex problems. For each problem class, the convergence rate is\n$O(1/K^{\\frac{1}{2}})$ in the expected value of the gradient norm square, for\n$K$ iterations. In a distributed environment, the convergence rate of the\nproposed method will be slowed down by the information delay. Nevertheless, the\nslow-down effect will decay with the number of iterations for the latter two\nproblem classes. We test the proposed method on three applications. The\nnumerical results clearly demonstrate the advantages of using the\ninertial-based acceleration. Furthermore, we observe higher parallelization\nspeed-up in asynchronous updates over the synchronous counterpart, though the\nformer uses delayed derivatives.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 02:33:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Yangyang", ""], ["Xu", "Yibo", ""], ["Yan", "Yonggui", ""], ["Chen", "Jie", ""]]}, {"id": "2107.11536", "submitter": "Bingbing Rao", "authors": "Bingbing Rao, Zixia Liu, Hong Zhang, Siyang Lu, Liqiang Wang", "title": "SODA: A Semantics-Aware Optimization Framework for Data-Intensive\n  Applications Using Hybrid Program Analysis", "comments": "2021 IEEE International Conference on Cloud Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the era of data explosion, a growing number of data-intensive computing\nframeworks, such as Apache Hadoop and Spark, have been proposed to handle the\nmassive volume of unstructured data in parallel. Since programming models\nprovided by these frameworks allow users to specify complex and diversified\nuser-defined functions (UDFs) with predefined operations, the grand challenge\nof tuning up entire system performance arises if programmers do not fully\nunderstand the semantics of code, data, and runtime systems. In this paper, we\ndesign a holistic semantics-aware optimization for data-intensive applications\nusing hybrid program analysis} (SODA) to assist programmers to tune performance\nissues. SODA is a two-phase framework: the offline phase is a static analysis\nthat analyzes code and performance profiling data from the online phase of\nprior executions to generate a parameterized and instrumented application; the\nonline phase is a dynamic analysis that keeps track of the application's\nexecution and collects runtime information of data and system. Extensive\nexperimental results on four real-world Spark applications show that SODA can\ngain up to 60%, 10%, 8%, faster than its original implementation, with the\nthree proposed optimization strategies, i.e., cache management, operation\nreordering, and element pruning, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:33:05 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Rao", "Bingbing", ""], ["Liu", "Zixia", ""], ["Zhang", "Hong", ""], ["Lu", "Siyang", ""], ["Wang", "Liqiang", ""]]}, {"id": "2107.11540", "submitter": "Bingbing Rao", "authors": "Bingbing Rao, Liqiang Wang", "title": "A Survey of Semantics-Aware Performance Optimization for Data-Intensive\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are living in the era of Big Data and witnessing the explosion of data.\nGiven that the limitation of CPU and I/O in a single computer, the mainstream\napproach to scalability is to distribute computations among a large number of\nprocessing nodes in a cluster or cloud. This paradigm gives rise to the term of\ndata-intensive computing, which denotes a data parallel approach to process\nmassive volume of data. Through the efforts of different disciplines, several\npromising programming models and a few platforms have been proposed for\ndata-intensive computing, such as MapReduce, Hadoop, Apache Spark and Dyrad.\nEven though a large body of research work has being proposed to improve overall\nperformance of these platforms, there is still a gap between the actual\nperformance demand and the capability of current commodity systems. This paper\nis aimed to provide a comprehensive understanding about current semantics-aware\napproaches to improve the performance of data-intensive computing. We first\nintroduce common characteristics and paradigm shifts in the evolution of\ndata-intensive computing, as well as contemporary programming models and\ntechnologies. We then propose four kinds of performance defects and survey the\nstate-of-the-art semantics-aware techniques. Finally, we discuss the research\nchallenges and opportunities in the field of semantics-aware performance\noptimization for data-intensive computing.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 05:46:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Rao", "Bingbing", ""], ["Wang", "Liqiang", ""]]}, {"id": "2107.11541", "submitter": "Guillermo Oyarzun", "authors": "Guillermo Oyarzun, Dani Mira, Guillaume Houzeaux", "title": "Performance assessment of CUDA and OpenACC in large scale combustion\n  simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  GPUs have climbed up to the top of supercomputer systems making life harder\nto many legacy scientific codes. Nowadays, many recipes are being used in such\ncode's portability, without any clarity of which is the best option. We present\na comparative analysis of the two most common approaches, CUDA and OpenACC,\ninto the multi-physics CFD code Alya. Our focus is the combustion problems\nwhich are one of the most computing demanding CFD simulations. The most\ncomputing-intensive parts of the code were analyzed in detail. New data\nstructures for the matrix assembly step have been created to facilitate a SIMD\nexecution that benefits vectorization in the CPU and stream processing in the\nGPU. As a result, the CPU code has improved its performance by up to 25%. In\nGPU execution, CUDA has proven to be up to 2 times faster than OpenACC for the\nassembly of the matrix. On the contrary, similar performance has been obtained\nin the kernels related to vector operations used in the linear solver, where\nthere is minimal memory reuse.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 06:16:08 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Oyarzun", "Guillermo", ""], ["Mira", "Dani", ""], ["Houzeaux", "Guillaume", ""]]}, {"id": "2107.11588", "submitter": "Guangxu Zhu", "authors": "Maojun Zhang, Guangxu Zhu, Shuai Wang, Jiamo Jiang, Caijun Zhong,\n  Shuguang Cui", "title": "Accelerating Federated Edge Learning via Optimized Probabilistic Device\n  Scheduling", "comments": "In Proc. IEEE SPAWC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC cs.IT cs.LG eess.SP math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popular federated edge learning (FEEL) framework allows\nprivacy-preserving collaborative model training via frequent learning-updates\nexchange between edge devices and server. Due to the constrained bandwidth,\nonly a subset of devices can upload their updates at each communication round.\nThis has led to an active research area in FEEL studying the optimal device\nscheduling policy for minimizing communication time. However, owing to the\ndifficulty in quantifying the exact communication time, prior work in this area\ncan only tackle the problem partially by considering either the communication\nrounds or per-round latency, while the total communication time is determined\nby both metrics. To close this gap, we make the first attempt in this paper to\nformulate and solve the communication time minimization problem. We first\nderive a tight bound to approximate the communication time through\ncross-disciplinary effort involving both learning theory for convergence\nanalysis and communication theory for per-round latency analysis. Building on\nthe analytical result, an optimized probabilistic scheduling policy is derived\nin closed-form by solving the approximate communication time minimization\nproblem. It is found that the optimized policy gradually turns its priority\nfrom suppressing the remaining communication rounds to reducing per-round\nlatency as the training process evolves. The effectiveness of the proposed\nscheme is demonstrated via a use case on collaborative 3D objective detection\nin autonomous driving.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 11:39:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zhang", "Maojun", ""], ["Zhu", "Guangxu", ""], ["Wang", "Shuai", ""], ["Jiang", "Jiamo", ""], ["Zhong", "Caijun", ""], ["Cui", "Shuguang", ""]]}, {"id": "2107.11592", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Mohammad Sadoghi", "title": "Blockchain Transaction Processing", "comments": null, "journal-ref": "Encyclopedia of Big Data Technologies 2019", "doi": "10.1007/978-3-319-77525-8_333", "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A blockchain is a linked list of immutable tamper-proof blocks, which is\nstored at each participating node. Each block records a set of transactions and\nthe associated metadata. Blockchain transactions act on the identical ledger\ndata stored at each node. Blockchain was first perceived by Satoshi Nakamoto,\nas a peer-to-peer money exchange system. Nakamoto referred to the transactional\ntokens exchanged among clients in his system as Bitcoins.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 12:20:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gupta", "Suyash", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2107.11728", "submitter": "Fengjiao Li", "authors": "Fengjiao Li, Jia Liu, and Bo Ji", "title": "Federated Learning with Fair Worker Selection: A Multi-Round Submodular\n  Maximization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 05:17:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Fengjiao", ""], ["Liu", "Jia", ""], ["Ji", "Bo", ""]]}, {"id": "2107.11832", "submitter": "Laurens Versluis", "authors": "Laurens Versluis, Mehmet Cetin, Caspar Greeven, Kristian Laursen,\n  Damian Podareanu, Valeriu Codreanu, Alexandru Uta, Alexandru Iosup", "title": "A Holistic Analysis of Datacenter Operations: Resource Usage, Energy,\n  and Workload Characterization -- Extended Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving datacenter operations is vital for the digital society. We posit\nthat doing so requires our community to shift, from operational aspects taken\nin isolation to holistic analysis of datacenter resources, energy, and\nworkloads. In turn, this shift will require new analysis methods, and\nopen-access, FAIR datasets with fine temporal and spatial granularity. We\nleverage in this work one of the (rare) public datasets providing fine-grained\ninformation on datacenter operations. Using it, we show strong evidence that\nfine-grained information reveals new operational aspects. We then propose a\nmethod for holistic analysis of datacenter operations, providing statistical\ncharacterization of node, energy, and workload aspects. We demonstrate the\nbenefits of our holistic analysis method by applying it to the operations of a\ndatacenter infrastructure with over 300 nodes. Our analysis reveals both\ngeneric and ML-specific aspects, and further details how the operational\nbehavior of the datacenter changed during the 2020 COVID-19 pandemic. We make\nover 30 main observations, providing holistic insight into the long-term\noperation of a large-scale, public scientific infrastructure. We suggest such\nobservations can help immediately with performance engineering tasks such as\npredicting future datacenter load, and also long-term with the design of\ndatacenter infrastructure.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 15:52:29 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Versluis", "Laurens", ""], ["Cetin", "Mehmet", ""], ["Greeven", "Caspar", ""], ["Laursen", "Kristian", ""], ["Podareanu", "Damian", ""], ["Codreanu", "Valeriu", ""], ["Uta", "Alexandru", ""], ["Iosup", "Alexandru", ""]]}, {"id": "2107.11912", "submitter": "Enzo Rucci", "authors": "Manuel Costanzo and Enzo Rucci and Marcelo Naiouf and Armando De\n  Giusti", "title": "Performance vs Programming Effort between Rust and C on Multicore\n  Architectures: Case Study in N-Body", "comments": "This article was submitted to 2021 XLVI Latin American Computing\n  Conference (CLEI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Historically, Fortran and C have been the default programming languages in\nHigh-Performance Computing (HPC). In both, programmers have primitives and\nfunctions available that allow manipulating system memory and interacting\ndirectly with the underlying hardware, resulting in efficient code in both\nresponse times and resource use. On the other hand, it is a real challenge to\ngenerate code that is maintainable and scalable over time in these types of\nlanguages. In 2010, Rust emerged as a new programming language designed for\nconcurrent and secure applications, which adopts features of procedural,\nobject-oriented and functional languages. Among its design principles, Rust is\naimed at matching C in terms of efficiency, but with increased code security\nand productivity. This paper presents a comparative study between C and Rust in\nterms of performance and programming effort, selecting as a case study the\nsimulation of N computational bodies (N-Body), a popular problem in the HPC\ncommunity. Based on the experimental work, it was possible to establish that\nRust is a language that reduces programming effort while maintaining acceptable\nperformance levels, meaning that it is a possible alternative to C for HPC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 00:09:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Costanzo", "Manuel", ""], ["Rucci", "Enzo", ""], ["Naiouf", "Marcelo", ""], ["De Giusti", "Armando", ""]]}, {"id": "2107.12016", "submitter": "Nan Gao", "authors": "Dongwei Li, Shuliang Wang, Nan Gao, Qiang He, and Yun Yang", "title": "Cost-effective Land Cover Classification for Remote Sensing Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land cover maps are of vital importance to various fields such as land use\npolicy development, ecosystem services, urban planning and agriculture\nmonitoring, which are mainly generated from remote sensing image classification\ntechniques. Traditional land cover classification usually needs tremendous\ncomputational resources, which often becomes a huge burden to the remote\nsensing community. Undoubtedly cloud computing is one of the best choices for\nland cover classification, however, if not managed properly, the computation\ncost on the cloud could be surprisingly high. Recently, cutting the unnecessary\ncomputation long tail has become a promising solution for saving the cost in\nthe cloud. For land cover classification, it is generally not necessary to\nachieve the best accuracy and 85% can be regarded as a reliable land cover\nclassification. Therefore, in this paper, we propose a framework for\ncost-effective remote sensing classification. Given the desired accuracy, the\nclustering algorithm can stop early for cost-saving whilst achieving sufficient\naccuracy for land cover image classification. Experimental results show that\nachieving 85%-99.9% accuracy needs only 27.34%-60.83% of the total cloud\ncomputation cost for achieving a 100% accuracy. To put it into perspective, for\nthe US land cover classification example, the proposed approach can save over\n$721,580.46 for the government in each single-use when the desired accuracy is\n90%.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 08:13:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Dongwei", ""], ["Wang", "Shuliang", ""], ["Gao", "Nan", ""], ["He", "Qiang", ""], ["Yang", "Yun", ""]]}, {"id": "2107.12048", "submitter": "Wei Liu", "authors": "Wei Liu, Li Chen, and Wenyi Zhang", "title": "Decentralized Federated Learning: Balancing Communication and Computing\n  Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized federated learning (DFL) is a powerful framework of distributed\nmachine learning and decentralized stochastic gradient descent (SGD) is a\ndriving engine for DFL. The performance of decentralized SGD is jointly\ninfluenced by communication-efficiency and convergence rate. In this paper, we\npropose a general decentralized federated learning framework to strike a\nbalance between communication-efficiency and convergence performance. The\nproposed framework performs both multiple local updates and multiple inter-node\ncommunications periodically, unifying traditional decentralized SGD methods. We\nestablish strong convergence guarantees for the proposed DFL algorithm without\nthe assumption of convex objective function. The balance of communication and\ncomputation rounds is essential to optimize decentralized federated learning\nunder constrained communication and computation resources. For further\nimproving communication-efficiency of DFL, compressed communication is applied\nto DFL, named DFL with compressed communication (C-DFL). The proposed C-DFL\nexhibits linear convergence for strongly convex objectives. Experiment results\nbased on MNIST and CIFAR-10 datasets illustrate the superiority of DFL over\ntraditional decentralized SGD methods and show that C-DFL further enhances\ncommunication-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:09:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Wei", ""], ["Chen", "Li", ""], ["Zhang", "Wenyi", ""]]}, {"id": "2107.12053", "submitter": "Tomohiro Harada", "authors": "Tomohiro Harada", "title": "A Frequency-based Parent Selection for Reducing the Effect of Evaluation\n  Time Bias in Asynchronous Parallel Multi-objective Evolutionary Algorithms", "comments": "18 pages, submitted to Neural Computing and Applications and under\n  review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new parent selection method for reducing the effect of\nevaluation time bias in asynchronous parallel evolutionary algorithms (APEAs).\nAPEAs have the advantage of increasing computational efficiency even when the\nevaluation times of solutions differ. However, APEAs have a problem that their\nsearch direction is biased toward the search region with a short evaluation\ntime. The proposed parent selection method considers the search frequency of\nsolutions to reduce such an adverse influence of APEAs while maintaining their\ncomputational efficiency. We conduct experiments on toy problems that reproduce\nthe evaluation time bias on multi-objective optimization problems to\ninvestigate the effectiveness of the proposed method. The experiments use\nNSGA-III, a well-known multi-objective evolutionary algorithm. In the\nexperiments, we compare the proposed method with the synchronous and\nasynchronous methods. The experimental results reveal that the proposed method\ncan reduce the effect of the evaluation time bias while reducing the computing\ntime of the parallel NSGA-III.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:20:55 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Harada", "Tomohiro", ""]]}, {"id": "2107.12065", "submitter": "Zhuoqing Song", "authors": "Zhuoqing Song, Lei Shi, Shi Pu, Ming Yan", "title": "Provably Accelerated Decentralized Gradient Method Over Unbalanced\n  Directed Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.SY eess.SP eess.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the decentralized optimization problem in which a\nnetwork of $n$ agents, each possessing a smooth and convex objective function,\nwish to collaboratively minimize the average of all the objective functions\nthrough peer-to-peer communication in a directed graph. To solve the problem,\nwe propose two accelerated Push-DIGing methods termed APD and APD-SC for\nminimizing non-strongly convex objective functions and strongly convex ones,\nrespectively. We show that APD and APD-SC respectively converge at the rates\n$O\\left(\\frac{1}{k^2}\\right)$ and $O\\left(\\left(1 -\nC\\sqrt{\\frac{\\mu}{L}}\\right)^k\\right)$ up to constant factors depending only on\nthe mixing matrix. To the best of our knowledge, APD and APD-SC are the first\ndecentralized methods to achieve provable acceleration over unbalanced directed\ngraphs. Numerical experiments demonstrate the effectiveness of both methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:42:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Song", "Zhuoqing", ""], ["Shi", "Lei", ""], ["Pu", "Shi", ""], ["Yan", "Ming", ""]]}, {"id": "2107.12147", "submitter": "Somali Chaterji", "authors": "Pranjal Jain, Shreyas Goenka, Saurabh Bagchi, Biplab Banerjee, Somali\n  Chaterji", "title": "Federated Action Recognition on Heterogeneous Embedded Devices", "comments": "13 pages, 12 figures", "journal-ref": "IEEE Transactions on Artificial Intelligence 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning allows a large number of devices to jointly learn a model\nwithout sharing data. In this work, we enable clients with limited computing\npower to perform action recognition, a computationally heavy task. We first\nperform model compression at the central server through knowledge distillation\non a large dataset. This allows the model to learn complex features and serves\nas an initialization for model fine-tuning. The fine-tuning is required because\nthe limited data present in smaller datasets is not adequate for action\nrecognition models to learn complex spatio-temporal features. Because the\nclients present are often heterogeneous in their computing resources, we use an\nasynchronous federated optimization and we further show a convergence bound. We\ncompare our approach to two baseline approaches: fine-tuning at the central\nserver (no clients) and fine-tuning using (heterogeneous) clients using\nsynchronous federated averaging. We empirically show on a testbed of\nheterogeneous embedded devices that we can perform action recognition with\ncomparable accuracy to the two baselines above, while our asynchronous learning\nstrategy reduces the training time by 40%, relative to synchronous learning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 02:33:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jain", "Pranjal", ""], ["Goenka", "Shreyas", ""], ["Bagchi", "Saurabh", ""], ["Banerjee", "Biplab", ""], ["Chaterji", "Somali", ""]]}, {"id": "2107.12148", "submitter": "Rakhmatulin Ildar", "authors": "R. Ildar", "title": "Increasing FPS for single board computers and embedded computers in 2021\n  (Jetson nano and YOVOv4-tiny). Practice and review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This manuscript provides a review of methods for increasing the frame per\nsecond of single-board computers. The main emphasis is on the Jetson family of\nsingle-board computers from Nvidia Company, due to the possibility of using a\ngraphical interface for calculations. But taking into account the popular\nlow-cost segment of single-board computers as RaspberryPI family, BananaPI,\nOrangePI, etc., we also provided an overview of methods for increasing the\nframe per second without using a Graphics Processing Unit. We considered\nframeworks, software development kit, and various libraries that can be used in\nthe process of increasing the frame per second in single-board computers.\nFinally, we tested the presented methods for the YOLOv4-tiny model with a\ncustom dataset on the Jetson nano and presented the results in the table.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 08:40:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ildar", "R.", ""]]}, {"id": "2107.12211", "submitter": "Yann Fraboni", "authors": "Yann Fraboni, Richard Vidal, Laetitia Kameni, Marco Lorenzi", "title": "On The Impact of Client Sampling on Federated Learning Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While clients' sampling is a central operation of current state-of-the-art\nfederated learning (FL) approaches, the impact of this procedure on the\nconvergence and speed of FL remains to date under-investigated. In this work we\nintroduce a novel decomposition theorem for the convergence of FL, allowing to\nclearly quantify the impact of client sampling on the global model update.\nContrarily to previous convergence analyses, our theorem provides the exact\ndecomposition of a given convergence step, thus enabling accurate\nconsiderations about the role of client sampling and heterogeneity. First, we\nprovide a theoretical ground for previously reported results on the\nrelationship between FL convergence and the variance of the aggregation\nweights. Second, we prove for the first time that the quality of FL convergence\nis also impacted by the resulting covariance between aggregation weights.\nThird, we establish that the sum of the aggregation weights is another source\nof slow-down and should be equal to 1 to improve FL convergence speed. Our\ntheory is general, and is here applied to Multinomial Distribution (MD) and\nUniform sampling, the two default client sampling in FL, and demonstrated\nthrough a series of experiments in non-iid and unbalanced scenarios. Our\nresults suggest that MD sampling should be used as default sampling scheme, due\nto the resilience to the changes in data ratio during the learning process,\nwhile Uniform sampling is superior only in the special case when clients have\nthe same amount of data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 13:36:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Fraboni", "Yann", ""], ["Vidal", "Richard", ""], ["Kameni", "Laetitia", ""], ["Lorenzi", "Marco", ""]]}, {"id": "2107.12330", "submitter": "Trevor Steil", "authors": "Trevor Steil, Tahsin Reza, Keita Iwabuchi, Benjamin W. Priest,\n  Geoffrey Sanders, and Roger Pearce", "title": "TriPoll: Computing Surveys of Triangles in Massive-Scale Temporal Graphs\n  with Metadata", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-822890", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the higher-order interactions within network data is a key\nobjective of network science. Surveys of metadata triangles (or patterned\n3-cycles in metadata-enriched graphs) are often of interest in this pursuit. In\nthis work, we develop TriPoll, a prototype distributed HPC system capable of\nsurveying triangles in massive graphs containing metadata on their edges and\nvertices. We contrast our approach with much of the prior effort on triangle\nanalysis, which often focuses on simple triangle counting, usually in simple\ngraphs with no metadata. We assess the scalability of TriPoll when surveying\ntriangles involving metadata on real and synthetic graphs with up to hundreds\nof billions of edges.We utilize communication-reducing optimizations to\ndemonstrate a triangle counting task on a 224 billion edge web graph in\napproximately half of the time of competing approaches, while additionally\nsupporting metadata-aware capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:11:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Steil", "Trevor", ""], ["Reza", "Tahsin", ""], ["Iwabuchi", "Keita", ""], ["Priest", "Benjamin W.", ""], ["Sanders", "Geoffrey", ""], ["Pearce", "Roger", ""]]}, {"id": "2107.12332", "submitter": "Vitaly Aksenov", "authors": "Vitaly Aksenov", "title": "Overview of Bachelors Theses 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we review Bachelors Theses done under the supervision of Vitaly\nAksenov at ITMO University. This overview contains the short description of six\ntheses: \"Development of a Streaming Algorithm for the Decomposition of Graph\nMetrics to Tree Metrics\" by Oleg Fafurin, \"Development of Memory-friendly\nConcurrent Data Structures\" by Roman Smirnov, \"Theoretical Analysis of the\nPerformance of Concurrent Data Structures\" by Daniil Bolotov, \"Parallel Batched\nInterpolation Search Tree\" by Alena Martsenyuk, \"Parallel Batched\nSelf-adjusting Data Structures\" by Vitalii Krasnov, and \"Parallel Batched\nPersistent Binary Search Trees\" by Ildar Zinatulin.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 17:15:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aksenov", "Vitaly", ""]]}, {"id": "2107.12371", "submitter": "Guillermo Oyarzun", "authors": "Guillermo Oyarzun, Daniel Peyrolon, Carlos Alvarez, Xavier Martorell", "title": "An FPGA cached sparse matrix vector product (SpMV) for unstructured\n  computational fluid dynamics simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Field Programmable Gate Arrays generate algorithmic specific architectures\nthat improve the code's FLOP per watt ratio. Such devices are re-gaining\ninterest due to the rise of new tools that facilitate their programming, such\nas OmpSs. The computational fluid dynamics community is always investigating\nnew architectures that can improve its algorithm's performance. Commonly, those\nalgorithms have a low arithmetic intensity and only reach a small percentage of\nthe peak performance. The sparse matrix-vector multiplication is one of the\nmost time-consuming operations on unstructured simulations. The matrix's\nsparsity pattern determines the indirect memory accesses of the multiplying\nvector. This data path is hard to predict, making traditional implementations\nfail. In this work, we present an FPGA architecture that maximizes the vector's\nre-usability by introducing a cache-like architecture. The cache is implemented\nas a circular list that maintains the BRAM vector components while needed.\nFollowing this strategy, up to 16 times of acceleration is obtained compared to\na naive implementation of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 06:09:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Oyarzun", "Guillermo", ""], ["Peyrolon", "Daniel", ""], ["Alvarez", "Carlos", ""], ["Martorell", "Xavier", ""]]}, {"id": "2107.12486", "submitter": "In Kee Kim", "authors": "Piyush Subedi, Jianwei Hao, In Kee Kim, Lakshmish Ramaswamy", "title": "AI Multi-Tenancy on Edge: Concurrent Deep Learning Model Executions and\n  Dynamic Model Placements on Edge Devices", "comments": "12 pages, To appear in 2021 IEEE International Conference on Cloud\n  Computing, September, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications are widely adopting the edge computing paradigm\ndue to its low latency and better privacy protection. With notable success in\nAI and deep learning (DL), edge devices and AI accelerators play a crucial role\nin deploying DL inference services at the edge of the Internet. While prior\nworks quantified various edge devices' efficiency, most studies focused on the\nperformance of edge devices with single DL tasks. Therefore, there is an urgent\nneed to investigate AI multi-tenancy on edge devices, required by many advanced\nDL applications for edge computing.\n  This work investigates two techniques - concurrent model executions and\ndynamic model placements - for AI multi-tenancy on edge devices. With image\nclassification as an example scenario, we empirically evaluate AI multi-tenancy\non various edge devices, AI accelerators, and DL frameworks to identify its\nbenefits and limitations. Our results show that multi-tenancy significantly\nimproves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI\nmulti-tenancy techniques also open up new opportunities for flexible deployment\nof multiple DL services on edge devices and AI accelerators.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:19:03 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Subedi", "Piyush", ""], ["Hao", "Jianwei", ""], ["Kim", "In Kee", ""], ["Ramaswamy", "Lakshmish", ""]]}, {"id": "2107.12490", "submitter": "Yi Zhou", "authors": "Kamala Varma, Yi Zhou, Nathalie Baracaldo, Ali Anwar", "title": "LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating\n  Byzantine Attacks in Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning has arisen as a mechanism to allow multiple participants\nto collaboratively train a model without sharing their data. In these settings,\nparticipants (workers) may not trust each other fully; for instance, a set of\ncompetitors may collaboratively train a machine learning model to detect fraud.\nThe workers provide local gradients that a central server uses to update a\nglobal model. This global model can be corrupted when Byzantine workers send\nmalicious gradients, which necessitates robust methods for aggregating\ngradients that mitigate the adverse effects of Byzantine inputs. Existing\nrobust aggregation algorithms are often computationally expensive and only\neffective under strict assumptions. In this paper, we introduce LayerwisE\nGradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,\nscalable and generalizable. Informed by a study of layer-specific responses of\ngradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing\nscheme that is novel in its treatment of gradients based on layer-specific\nrobustness. We show that LEGATO is more computationally efficient than multiple\nstate-of-the-art techniques and more generally robust across a variety of\nattack settings in practice. We also demonstrate LEGATO's benefits for gradient\ndescent convergence in the absence of an attack.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 21:34:45 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Varma", "Kamala", ""], ["Zhou", "Yi", ""], ["Baracaldo", "Nathalie", ""], ["Anwar", "Ali", ""]]}, {"id": "2107.12519", "submitter": "Abdallah Lakhdari", "authors": "Abdallah Lakhdari and Athman Bouguettaya", "title": "Proactive Composition of Mobile IoT Energy Services", "comments": "paper accepted in IEEE ICWS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel proactive composition framework of wireless energy\nservices in a crowdsourced IoT environment. We define a new model for energy\nservices and requests that includes providers' and consumers' mobility patterns\nand energy usage behavior. The proposed composition approach leverages the\nmobility and energy usage behavior to generate energy services and requests\nproactively. Preliminary experimental results demonstrate the effectiveness of\ngenerating proactive energy requests and composing proactive services.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 23:48:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Lakhdari", "Abdallah", ""], ["Bouguettaya", "Athman", ""]]}, {"id": "2107.12563", "submitter": "Yanzhao Wu", "authors": "Yanzhao Wu, Ling Liu, Ramana Kompella", "title": "Parallel Detection for Efficient Video Analytics at the Edge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Network (DNN) trained object detectors are widely deployed in\nmany mission-critical systems for real time video analytics at the edge, such\nas autonomous driving and video surveillance. A common performance requirement\nin these mission-critical edge services is the near real-time latency of online\nobject detection on edge devices. However, even with well-trained DNN object\ndetectors, the online detection quality at edge may deteriorate for a number of\nreasons, such as limited capacity to run DNN object detection models on\nheterogeneous edge devices, and detection quality degradation due to random\nframe dropping when the detection processing rate is significantly slower than\nthe incoming video frame rate. This paper addresses these problems by\nexploiting multi-model multi-device detection parallelism for fast object\ndetection in edge systems with heterogeneous edge devices. First, we analyze\nthe performance bottleneck of running a well-trained DNN model at edge for real\ntime online object detection. We use the offline detection as a reference\nmodel, and examine the root cause by analyzing the mismatch among the incoming\nvideo streaming rate, video processing rate for object detection, and output\nrate for real time detection visualization of video streaming. Second, we study\nperformance optimizations by exploiting multi-model detection parallelism. We\nshow that the model-parallel detection approach can effectively speed up the\nFPS detection processing rate, minimizing the FPS disparity with the incoming\nvideo frame rate on heterogeneous edge devices. We evaluate the proposed\napproach using SSD300 and YOLOv3 on benchmark videos of different video stream\nrates. The results show that exploiting multi-model detection parallelism can\nspeed up the online object detection processing rate and deliver near real-time\nobject detection performance for efficient video analytics at edge.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:50:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wu", "Yanzhao", ""], ["Liu", "Ling", ""], ["Kompella", "Ramana", ""]]}, {"id": "2107.12581", "submitter": "Shuqin Gao", "authors": "Shuqin Gao, Costas Courcoubetis and Lingjie Duan", "title": "Average-Case Analysis of Greedy Matching for D2D Resource Sharing", "comments": "12 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the proximity of many wireless users and their diversity in consuming\nlocal resources (e.g., data-plans, computation and even energy resources),\ndevice-to-device (D2D) resource sharing is a promising approach towards\nrealizing a sharing economy. In the resulting networked economy, $n$ users\nsegment themselves into sellers and buyers that need to be efficiently matched\nlocally. This paper adopts an easy-to-implement greedy matching algorithm with\ndistributed fashion and only sub-linear $O(\\log n)$ parallel complexity, which\noffers a great advantage compared to the optimal but computational-expensive\ncentralized matching. But is it efficient compared to the optimal matching?\nExtensive simulations indicate that in a large number of practical cases the\naverage loss is no more than $10\\%$, a far better result than the $50\\%$ loss\nbound in the worst case. However, there is no rigorous average-case analysis in\nthe literature to back up such encouraging findings, which is a fundamental\nstep towards supporting the practical use of greedy matching in D2D sharing.\nThis paper is the first to present the rigorous average analysis of certain\nrepresentative classes of graphs with random parameters, by proposing a new\nasymptotic methodology. For typical 2D grids with random matching weights we\nrigorously prove that our greedy algorithm performs better than $84.9\\%$ of the\noptimal, while for typical Erdos-Renyi random graphs we prove a lower bound of\n$79\\%$ when the graph is neither dense nor sparse. Finally, we use realistic\ndata to show that our random graph models approximate well D2D sharing networks\nencountered in practice.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:03:00 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Gao", "Shuqin", ""], ["Courcoubetis", "Costas", ""], ["Duan", "Lingjie", ""]]}, {"id": "2107.12603", "submitter": "Ming Liu Dr", "authors": "Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang", "title": "Federated Learning Meets Natural Language Processing: A Survey", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 05:07:48 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Liu", "Ming", ""], ["Ho", "Stella", ""], ["Wang", "Mengqi", ""], ["Gao", "Longxiang", ""], ["Jin", "Yuan", ""], ["Zhang", "He", ""]]}, {"id": "2107.12720", "submitter": "Bin Guo", "authors": "Bin Guo, Emil Sekerinski", "title": "Efficient Parallel Graph Trimming by Arc-Consistency", "comments": "42 pages, 9 figures, The Journal of Supercomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large data graph, trimming techniques can reduce the search space by\nremoving vertices without outgoing edges. One application is to speed up the\nparallel decomposition of graphs into strongly connected components (SCC\ndecomposition), which is a fundamental step for analyzing graphs. We observe\nthat graph trimming is essentially a kind of arc-consistency problem, and AC-3,\nAC-4, and AC-6 are the most relevant arc-consistency algorithms for application\nto graph trimming. The existing parallel graph trimming methods require\nworst-case $\\mathcal O(nm)$ time and worst-case $\\mathcal O(n)$ space for\ngraphs with $n$ vertices and $m$ edges. We call these parallel AC-3-based as\nthey are much like the AC-3 algorithm. In this work, we propose AC-4-based and\nAC-6-based trimming methods. That is, AC-4-based trimming has an improved\nworst-case time of $\\mathcal O(n+m)$ but requires worst-case space of $\\mathcal\nO(n+m)$; compared with AC-4-based trimming, AC-6-based has the same worst-case\ntime of $\\mathcal O(n+m)$ but an improved worst-case space of $\\mathcal O(n)$.\nWe parallelize the AC-4-based and AC-6-based algorithms to be suitable for\nshared-memory multi-core machines. The algorithms are designed to minimize\nsynchronization overhead. For these algorithms, we also prove the correctness\nand analyze time complexities with the work-depth model. In experiments, we\ncompare these three parallel trimming algorithms over a variety of real and\nsynthetic graphs. Specifically, for the maximum number of traversed edges per\nworker by using 16 workers, AC-3-based traverses up to 58.3 and 36.5 times more\nedges than AC-6-based trimming and AC-4-based trimming, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:47:43 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Guo", "Bin", ""], ["Sekerinski", "Emil", ""]]}, {"id": "2107.12740", "submitter": "Xin Du", "authors": "Yujie Wamg, Xin Du, Xuzhao Chen, Zhihui Lu", "title": "Edge service resource allocation strategy based on intelligent\n  prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence is one of the important technologies for industrial\napplications, but it requires a lot of computing resources and sensor data to\nsupport it. With the development of edge computing and the Internet of Things,\nartificial intelligence are playing an increasingly important role in the field\nof edge services. Therefore, how to make intelligent algorithms provide better\nservices and the development of the Internet of Things has become an\nincreasingly important topic. This paper focuses on the application of edge\nservice distribution strategy, and proposes an edge service distribution\nstrategy based on intelligent prediction, which reduces the bandwidth\nconsumption of edge service providers and minimizes the cost of edge service\nproviders. In addition, this article uses the real data provided by the Wangsu\nTechnology Company and an improved long and short term memory prediction method\nto dynamically change the bandwidth, and achieves better optimization of\nresources allocation comparing with actual industrial applications.The\nsimulation results show that our intelligent prediction can achieve good\nresults, and the mechanism can achieve higher resource utilization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:35:40 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wamg", "Yujie", ""], ["Du", "Xin", ""], ["Chen", "Xuzhao", ""], ["Lu", "Zhihui", ""]]}, {"id": "2107.12788", "submitter": "Rafal Kapelko", "authors": "Roy Friedman, Rafa{\\l} Kapelko, Karol Marchwicki", "title": "On the data persistency of replicated erasure codes in distributed\n  storage systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the fundamental problem of data persistency for a general\nfamily of redundancy schemes in distributed storage systems, called replicated\nerasure codes. Namely, we analyze two strategies of replicated erasure codes\ndistribution: random and symmetric. For both strategies we derive closed\nanalytical and asymptotic formulas for expected data persistency despite nodes\nfailure.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:00:21 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Friedman", "Roy", ""], ["Kapelko", "Rafa\u0142", ""], ["Marchwicki", "Karol", ""]]}, {"id": "2107.12807", "submitter": "Supun Kamburugamuve", "authors": "Supun Kamburugamuve, Chathura Widanage, Niranda Perera, Vibhatha\n  Abeykoon, Ahmet Uyar, Thejaka Amila Kanewala, Gregor von Laszewski, and\n  Geoffrey Fox", "title": "HPTMT: Operator-Based Architecture for ScalableHigh-Performance\n  Data-Intensive Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-intensive applications impact many domains, and their steadily\nincreasing size and complexity demands high-performance, highly usable\nenvironments. We integrate a set of ideas developed in various data science and\ndata engineering frameworks. They employ a set of operators on specific data\nabstractions that include vectors, matrices, tensors, graphs, and tables. Our\nkey concepts are inspired from systems like MPI, HPF (High-Performance\nFortran), NumPy, Pandas, Spark, Modin, PyTorch, TensorFlow, RAPIDS(NVIDIA), and\nOneAPI (Intel). Further, it is crucial to support different languages in\neveryday use in the Big Data arena, including Python, R, C++, and Java. We note\nthe importance of Apache Arrow and Parquet for enabling language agnostic high\nperformance and interoperability. In this paper, we propose High-Performance\nTensors, Matrices and Tables (HPTMT), an operator-based architecture for\ndata-intensive applications, and identify the fundamental principles needed for\nperformance and usability success. We illustrate these principles by a\ndiscussion of examples using our software environments, Cylon and Twister2 that\nembody HPTMT.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 13:28:34 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kamburugamuve", "Supun", ""], ["Widanage", "Chathura", ""], ["Perera", "Niranda", ""], ["Abeykoon", "Vibhatha", ""], ["Uyar", "Ahmet", ""], ["Kanewala", "Thejaka Amila", ""], ["von Laszewski", "Gregor", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2107.12867", "submitter": "Wenqiang Li", "authors": "Wenqiang Li, Le Guan, Jingqiang Lin, Jiameng Shi, Fengjun Li", "title": "From Library Portability to Para-rehosting: Natively Executing\n  Microcontroller Software on Commodity Hardware", "comments": "18 pages, 4 figures, Network and Distributed Systems Security (NDSS)\n  Symposium 2021", "journal-ref": null, "doi": "10.14722/ndss.2021.24308", "report-no": null, "categories": "cs.PL cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding bugs in microcontroller (MCU) firmware is challenging, even for\ndevice manufacturers who own the source code. The MCU runs different\ninstruction sets than x86 and exposes a very different development environment.\nThis invalidates many existing sophisticated software testing tools on x86. To\nmaintain a unified developing and testing environment, a straightforward way is\nto re-compile the source code into the native executable for a commodity\nmachine (called rehosting). However, ad-hoc re-hosting is a daunting and\ntedious task and subject to many issues (library-dependence, kernel-dependence\nand hardware-dependence). In this work, we systematically explore the\nportability problem of MCU software and propose pararehosting to ease the\nporting process. Specifically, we abstract and implement a portable MCU (PMCU)\nusing the POSIX interface. It models common functions of the MCU cores. For\nperipheral specific logic, we propose HAL-based peripheral function\nreplacement, in which high-level hardware functions are replaced with an\nequivalent backend driver on the host. These backend drivers are invoked by\nwell-designed para-APIs and can be reused across many MCU OSs. We categorize\ncommon HAL functions into four types and implement templates for quick backend\ndevelopment. Using the proposed approach, we have successfully rehosted nine\nMCU OSs including the widely deployed Amazon FreeRTOS, ARM Mbed OS, Zephyr and\nLiteOS. To demonstrate the superiority of our approach in terms of security\ntesting, we used off-the-shelf dynamic analysis tools (AFL and ASAN) against\nthe rehosted programs and discovered 28 previously-unknown bugs, among which 5\nwere confirmed by CVE and the other 19 were confirmed by vendors at the time of\nwriting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 16:54:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Wenqiang", ""], ["Guan", "Le", ""], ["Lin", "Jingqiang", ""], ["Shi", "Jiameng", ""], ["Li", "Fengjun", ""]]}, {"id": "2107.12958", "submitter": "Tingting Tang", "authors": "Tingting Tang, Ramy E. Ali, Hanieh Hashemi, Tynan Gangwani, Salman\n  Avestimehr and Murali Annavaram", "title": "Verifiable Coded Computing: Towards Fast, Secure and Private Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stragglers, Byzantine workers, and data privacy are the main bottlenecks in\ndistributed cloud computing. Several prior works proposed coded computing\nstrategies to jointly address all three challenges. They require either a large\nnumber of workers, a significant communication cost or a significant\ncomputational complexity to tolerate malicious workers. Much of the overhead in\nprior schemes comes from the fact that they tightly couple coding for all three\nproblems into a single framework. In this work, we propose Verifiable Coded\nComputing (VCC) framework that decouples Byzantine node detection challenge\nfrom the straggler tolerance. VCC leverages coded computing just for handling\nstragglers and privacy, and then uses an orthogonal approach of verifiable\ncomputing to tackle Byzantine nodes. Furthermore, VCC dynamically adapts its\ncoding scheme to tradeoff straggler tolerance with Byzantine protection and\nvice-versa. We evaluate VCC on compute intensive distributed logistic\nregression application. Our experiments show that VCC speeds up the\nconventional uncoded implementation of distributed logistic regression by\n$3.2\\times-6.9\\times$, and also improves the test accuracy by up to $12.6\\%$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:23:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Tang", "Tingting", ""], ["Ali", "Ramy E.", ""], ["Hashemi", "Hanieh", ""], ["Gangwani", "Tynan", ""], ["Avestimehr", "Salman", ""], ["Annavaram", "Murali", ""]]}, {"id": "2107.12981", "submitter": "Akihiro Fujihara Dr.", "authors": "Takaaki Yanagihara and Akihiro Fujihara", "title": "Cross-Referencing Method for Scalable Public Blockchain", "comments": "(29 pages, 18 figures, Internet of Things 15 (2021) 100419)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We previously proposed a cross-referencing method for enabling multiple\npeer-to-peer network domains to manage their own public blockchains and\nperiodically exchanging the state of the latest fixed block in the blockchain\nwith hysteresis signatures among all the domains via an upper network layer. In\nthis study, we evaluated the effectiveness of our method from three theoretical\nviewpoints: decentralization, scalability, and tamper resistance. We show that\nthe performance of the entire system can be improved because transactions and\nblocks are distributed only inside the domain. We argue that the transaction\nprocessing capacity will increase to 56,000 transactions per second, which is\nas much as that of a VISA credit card system. The capacity is also evaluated by\nmultiplying the number of domains by the average reduction in\ntransaction-processing time due to the increase in block size and reduction in\nthe block-generation-time interval by domain partition. For tamper resistance,\neach domain has evidence of the hysteresis signatures of the other domains in\nthe blockchain. We introduce two types of tamper-resistance-improvement ratios\nas evaluation measures of tamper resistance for a blockchain and theoretically\nexplain how tamper resistance is improved using our cross-referencing method.\nWith our method, tamper resistance improves as the number of domains increases.\nThe proposed system of 1,000 domains are 3-10 times more tamper-resistant than\nthat of 100 domains, and the capacity is 10 times higher. We conclude that our\nmethod enables a more scalable and tamper-resistant public blockchain balanced\nwith decentralization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:50:37 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Yanagihara", "Takaaki", ""], ["Fujihara", "Akihiro", ""]]}, {"id": "2107.13047", "submitter": "Suyash Gupta", "authors": "Sajjad Rahnama, Suyash Gupta, Rohan Sogani, Dhruv Krishnan, Mohammad\n  Sadoghi", "title": "RingBFT: Resilient Consensus over Sharded Ring Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The recent surge in federated data-management applications has brought forth\nconcerns about the security of underlying data and the consistency of replicas\nin the presence of malicious attacks. A prominent solution in this direction is\nto employ a permissioned blockchain framework that is modeled around\ntraditional Byzantine Fault-Tolerant (BFT) consensus protocols. Any federated\napplication expects its data to be globally scattered to achieve faster access.\nBut, prior works have shown that traditional BFT protocols are slow and this\nled to the rise of sharded-replicated blockchains. Existing BFT protocols for\nthese sharded blockchains are efficient if client transactions require access\nto a single-shard, but face performance degradation if there is a cross-shard\ntransaction that requires access to multiple shards. However, cross-shard\ntransactions are common, and to resolve this dilemma, we present RingBFT, a\nnovel meta-BFT protocol for sharded blockchains. RingBFT requires shards to\nadhere to the ring order, and follow the principle of process, forward, and\nre-transmit while ensuring the communication between shards is linear. Our\nevaluation of RingBFT against state-of-the-art sharding BFT protocols\nillustrates that RingBFT achieves up to 25x higher throughput, easily scales to\nnearly 500 globally distributed nodes, and achieves a peak throughput of 1.2\nmillion txns/s.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:15:23 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Rahnama", "Sajjad", ""], ["Gupta", "Suyash", ""], ["Sogani", "Rohan", ""], ["Krishnan", "Dhruv", ""], ["Sadoghi", "Mohammad", ""]]}, {"id": "2107.13057", "submitter": "James Aimone", "authors": "J. Darby Smith, Aaron J. Hill, Leah E. Reeder, Brian C. Franke,\n  Richard B. Lehoucq, Ojas Parekh, William Severa, James B. Aimone", "title": "Neuromorphic scaling advantages for energy-efficient random walk\n  computation", "comments": "Paper, figures, supplement", "journal-ref": null, "doi": null, "report-no": "SAND2021-9085 O", "categories": "cs.NE cs.DC cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing stands to be radically improved by neuromorphic computing (NMC)\napproaches inspired by the brain's incredible efficiency and capabilities. Most\nNMC research, which aims to replicate the brain's computational structure and\narchitecture in man-made hardware, has focused on artificial intelligence;\nhowever, less explored is whether this brain-inspired hardware can provide\nvalue beyond cognitive tasks. We demonstrate that high-degree parallelism and\nconfigurability of spiking neuromorphic architectures makes them well-suited to\nimplement random walks via discrete time Markov chains. Such random walks are\nuseful in Monte Carlo methods, which represent a fundamental computational tool\nfor solving a wide range of numerical computing tasks. Additionally, we show\nhow the mathematical basis for a probabilistic solution involving a class of\nstochastic differential equations can leverage those simulations to provide\nsolutions for a range of broadly applicable computational tasks. Despite being\nin an early development stage, we find that NMC platforms, at a sufficient\nscale, can drastically reduce the energy demands of high-performance computing\n(HPC) platforms.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 19:44:33 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Smith", "J. Darby", ""], ["Hill", "Aaron J.", ""], ["Reeder", "Leah E.", ""], ["Franke", "Brian C.", ""], ["Lehoucq", "Richard B.", ""], ["Parekh", "Ojas", ""], ["Severa", "William", ""], ["Aimone", "James B.", ""]]}, {"id": "2107.13212", "submitter": "Michal Zasadzinski", "authors": "Michal Zasadzinski, Michael Theodoulou, Markus Thurner and Kshitij\n  Ranganath", "title": "The Trip to The Enterprise Gourmet Data Product Marketplace through a\n  Self-service Data Platform", "comments": "Peer reviewed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data Analytics provides core business reporting needs in many software\ncompanies, acts as a source of truth for key information, and enables building\nadvanced solutions, e.g., predictive models, machine learning, real-time\nrecommendations, to grow the business.\n  A self-service, multi-tenant, API-first, and scalable data platform is the\nfoundational requirement in creating an enterprise data marketplace, which\nenables the creation, publishing, and exchange of data products. Such a\nmarketplace enables the exploration and discovery of data products, further\nproviding high-level data governance and oversight on marketplace contents. In\nthis paper, we describe our way to the gourmet data product marketplace. We\ncover the design principles, the implementation details, technology choices,\nand the journey to build an enterprise data platform that meets the above\ncharacteristics. The platform consists of ingestion, streaming, storage,\ntransformation, schema generation, fail-safe, data sharing, access management,\nPII data automatic identification, self-service storage optimization\nrecommendations, and CI/CD integration.\n  We then show how the platform enables and operates the data marketplace,\nfacilitating the exchange of stable data products across users and tenants. We\nmotivate and show how we run scalable decentralized data governance. All of\nthis is built and run for Cimpress Technology (CT), which operates the Mass\nCustomization Platform for Cimpress and its businesses. The CT data platform\nserves 1000s of users from different platform participants, with data sourced\nfrom heterogeneous sources. Data is ingested at a rate of well over 1000\nindividual messages per second and serves more than 100k analytical queries\ndaily.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 07:52:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zasadzinski", "Michal", ""], ["Theodoulou", "Michael", ""], ["Thurner", "Markus", ""], ["Ranganath", "Kshitij", ""]]}, {"id": "2107.13309", "submitter": "Chhaya Trehan", "authors": "Michael Elkin, Chhaya Trehan", "title": "$(1+\\epsilon)$-Approximate Shortest Paths in Dynamic Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing approximate shortest paths in the dynamic streaming setting is a\nfundamental challenge that has been intensively studied during the last decade.\nCurrently existing solutions for this problem either build a sparse\nmultiplicative spanner of the input graph and compute shortest paths in the\nspanner offline, or compute an exact single source BFS tree.\n  Solutions of the first type are doomed to incur a stretch-space tradeoff of\n$2\\kappa-1$ versus $n^{1+1/\\kappa}$, for an integer parameter $\\kappa$. (In\nfact, existing solutions also incur an extra factor of $1+\\epsilon$ in the\nstretch for weighted graphs, and an additional factor of $\\log^{O(1)}n$ in the\nspace.) The only existing solution of the second type uses $n^{1/2 -\nO(1/\\kappa)}$ passes over the stream (for space $O(n^{1+1/\\kappa})$), and\napplies only to unweighted graphs.\n  In this paper we show that $(1+\\epsilon)$-approximate single-source shortest\npaths can be computed in this setting with $\\tilde{O}(n^{1+1/\\kappa})$ space\nusing just \\emph{constantly} many passes in unweighted graphs, and\npolylogarithmically many passes in weighted graphs (assuming $\\epsilon$ and\n$\\kappa$ are constant). Moreover, in fact, the same result applies for\nmulti-source shortest paths, as long as the number of sources is\n$O(n^{1/\\kappa})$.\n  We achieve these results by devising efficient dynamic streaming\nconstructions of $(1 + \\epsilon, \\beta)$-spanners and hopsets. We believe that\nthese constructions are of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 12:05:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Elkin", "Michael", ""], ["Trehan", "Chhaya", ""]]}, {"id": "2107.13317", "submitter": "Jonathan Will", "authors": "Jonathan Will and Lauritz Thamsen and Dominik Scheinert and Jonathan\n  Bader and Odej Kao", "title": "C3O: Collaborative Cluster Configuration Optimization for Distributed\n  Data Processing in Public Clouds", "comments": "10 pages, 5 figures, IEEE IC2E 2021. arXiv admin note: text overlap\n  with arXiv:2011.07965", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed dataflow systems enable data-parallel processing of large\ndatasets on clusters. Public cloud providers offer a large variety and quantity\nof resources that can be used for such clusters. Yet, selecting appropriate\ncloud resources for dataflow jobs - that neither lead to bottlenecks nor to low\nresource utilization - is often challenging, even for expert users such as data\nengineers.\n  We present C3O, a collaborative system for optimizing data processing cluster\nconfigurations in public clouds based on shared historical runtime data. The\nshared data is utilized for predicting the runtimes of data processing jobs on\ndifferent possible cluster configurations, using specialized regression models.\nThese models take the diverse execution contexts of different users into\naccount and exhibit mean absolute errors below 3% in our experimental\nevaluation with 930 unique Spark jobs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 12:29:05 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Will", "Jonathan", ""], ["Thamsen", "Lauritz", ""], ["Scheinert", "Dominik", ""], ["Bader", "Jonathan", ""], ["Kao", "Odej", ""]]}, {"id": "2107.13320", "submitter": "Simon Eismann", "authors": "Simon Eismann, Diego Elias Costa, Lizhi Liao, Cor-Paul Bezemer, Weiyi\n  Shang, Andr\\'e van Hoorn, Samuel Kounev", "title": "A Case Study on the Stability of Performance Tests for Serverless\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. While in serverless computing, application resource management and\noperational concerns are generally delegated to the cloud provider, ensuring\nthat serverless applications meet their performance requirements is still a\nresponsibility of the developers. Performance testing is a commonly used\nperformance assessment practice; however, it traditionally requires visibility\nof the resource environment.\n  Objective. In this study, we investigate whether performance tests of\nserverless applications are stable, that is, if their results are reproducible,\nand what implications the serverless paradigm has for performance tests.\n  Method. We conduct a case study where we collect two datasets of performance\ntest results: (a) repetitions of performance tests for varying memory size and\nload intensities and (b) three repetitions of the same performance test every\nday for ten months.\n  Results. We find that performance tests of serverless applications are\ncomparatively stable if conducted on the same day. However, we also observe\nshort-term performance variations and frequent long-term performance changes.\n  Conclusion. Performance tests for serverless applications can be stable;\nhowever, the serverless model impacts the planning, execution, and analysis of\nperformance tests.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 12:32:00 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Eismann", "Simon", ""], ["Costa", "Diego Elias", ""], ["Liao", "Lizhi", ""], ["Bezemer", "Cor-Paul", ""], ["Shang", "Weiyi", ""], ["van Hoorn", "Andr\u00e9", ""], ["Kounev", "Samuel", ""]]}, {"id": "2107.13500", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Accelerating advection for atmospheric modelling on Xilinx and Intel\n  FPGAs", "comments": "Preprint of article in the IEEE Cluster FPGA for HPC Workshop 2021\n  (HPC FPGA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable architectures, such as FPGAs, enable the execution of code at\nthe electronics level, avoiding the assumptions imposed by the general purpose\nblack-box micro-architectures of CPUs and GPUs. Such tailored execution can\nresult in increased performance and power efficiency, and as the HPC community\nmoves towards exascale an important question is the role such hardware\ntechnologies can play in future supercomputers.\n  In this paper we explore the porting of the PW advection kernel, an important\ncode component used in a variety of atmospheric simulations and accounting for\naround 40\\% of the runtime of the popular Met Office NERC Cloud model (MONC).\nBuilding upon previous work which ported this kernel to an older generation of\nXilinx FPGA, we target latest generation Xilinx Alveo U280 and Intel Stratix 10\nFPGAs. Exploring the development of a dataflow design which is performance\nportable between vendors, we then describe implementation differences between\nthe tool chains and compare kernel performance between FPGA hardware. This is\nfollowed by a more general performance comparison, scaling up the number of\nkernels on the Xilinx Alveo and Intel Stratix 10, against a 24 core Xeon\nPlatinum Cascade Lake CPU and NVIDIA Tesla V100 GPU. When overlapping the\ntransfer of data to and from the boards with compute, the FPGA solutions\nconsiderably outperform the CPU and, whilst falling short of the GPU in terms\nof performance, demonstrate power usage benefits, with the Alveo being\nespecially power efficient. The result of this work is a comparison and set of\ndesign techniques that apply both to this specific atmospheric advection kernel\non Xilinx and Intel FPGAs, and that are also of interest more widely when\nlooking to accelerate HPC codes on a variety of reconfigurable architectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:14:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2107.13502", "submitter": "Deepika Saxena", "authors": "Deepika Saxena, Ishu Gupta, Jitendra Kumar, Ashutosh Kumar Singh, and\n  Xiaoqing Wen", "title": "A Secure and Multi-objective Virtual Machine Placement Framework for\n  Cloud Data Centre", "comments": "This article has been accepted for inclusion in a future issue of\n  IEEE Systems Journal (2021)", "journal-ref": null, "doi": "10.1109/JSYST.2021.3092521", "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To facilitate cost-effective and elastic computing benefits to the cloud\nusers, the energy-efficient and secure allocation of virtual machines (VMs)\nplays a significant role at the data centre. The inefficient VM Placement (VMP)\nand sharing of common physical machines among multiple users leads to resource\nwastage, excessive power consumption, increased inter-communication cost and\nsecurity breaches. To address the aforementioned challenges, a novel secure and\nmulti-objective virtual machine placement (SM-VMP) framework is proposed with\nan efficient VM migration. The proposed framework ensures an energy-efficient\ndistribution of physical resources among VMs that emphasizes secure and timely\nexecution of user application by reducing inter-communication delay. The VMP is\ncarried out by applying the proposed Whale Optimization Genetic Algorithm\n(WOGA), inspired by whale evolutionary optimization and non-dominated sorting\nbased genetic algorithms. The performance evaluation for static and dynamic VMP\nand comparison with recent state-of-the-arts observed a notable reduction in\nshared servers, inter-communication cost, power consumption and execution time\nup to 28.81%, 25.7%, 35.9% and 82.21%, respectively and increased resource\nutilization up to 30.21%.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:15:04 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Saxena", "Deepika", ""], ["Gupta", "Ishu", ""], ["Kumar", "Jitendra", ""], ["Singh", "Ashutosh Kumar", ""], ["Wen", "Xiaoqing", ""]]}, {"id": "2107.13520", "submitter": "Bhavesh Lakhotia", "authors": "Bhavesh Lakhotia", "title": "Exponentiation Using Laplace Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article derives an equation for exponentiation that can be used for\ncalculating exponents using a parallel computing architecture.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 08:37:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lakhotia", "Bhavesh", ""]]}, {"id": "2107.13694", "submitter": "Gaoxiong Zeng", "authors": "Ge Chen and Gaoxiong Zeng and Li Chen", "title": "P4COM: In-Network Computation with Programmable Switches", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, switches only provide forwarding services and have no credits\non computation in distributed computing frameworks. The emerging programmable\nswitches make in-network computing (INC) possible, i.e., offloading some\ncomputation to the switch data plane. While some proposals have attempted to\noffload computation onto special hardwares (e.g., NetFPGA), many practical\nissues have not been addressed. Therefore, we propose P4COM - a user-friendly,\nmemory-efficient, and fault-tolerant framework realizing in-network computation\n(e.g., MapReduce) with programmable switches.\n  P4COM consists of three modules. First, P4COM automatically translates\napplication logic to switch data plane programs with a lightweight interpreter.\nSecond, P4COM adopts a memory management policy to efficiently utilize the\nlimited switch on-chip memory. Third, P4COM provides a cutting-payload\nmechanism to handle packet losses. We have built a P4COM prototype with a\nBarefoot Tofino switch and multiple commodity servers. Through a combination of\ntestbed experiments and large-scale simulations, we show that P4COM is able to\nachieve line-rate processing at 10Gbps links, and can increase the data\nshuffling throughput by 2-5 times for the MapReduce-style applications.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 01:12:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Ge", ""], ["Zeng", "Gaoxiong", ""], ["Chen", "Li", ""]]}, {"id": "2107.13814", "submitter": "Haodi Ping", "authors": "Haodi Ping, Yongcai Wang, Deying Li", "title": "DCG: Distributed Conjugate Gradient for Efficient Linear Equations\n  Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms to solve linear equations in multi-agent networks have\nattracted great research attention and many iteration-based distributed\nalgorithms have been developed. The convergence speed is a key factor to be\nconsidered for distributed algorithms, and it is shown dependent on the\nspectral radius of the iteration matrix. However, the iteration matrix is\ndetermined by the network structure and is hardly pre-tuned, making the\niterative-based distributed algorithms may converge very slowly when the\nspectral radius is close to 1. In contrast, in centralized optimization, the\nConjugate Gradient (CG) is a widely adopted idea to speed up the convergence of\nthe centralized solvers, which can guarantee convergence in fixed steps. In\nthis paper, we propose a general distributed implementation of CG, called DCG.\nDCG only needs local communication and local computation, while inheriting the\ncharacteristic of fast convergence. DCG guarantees to converge in $4Hn$ rounds,\nwhere $H$ is the maximum hop number of the network and $n$ is the number of\nnodes. We present the applications of DCG in solving the least square problem\nand network localization problem. The results show the convergence speed of DCG\nis three orders of magnitude faster than the widely used Richardson iteration\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:21:23 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ping", "Haodi", ""], ["Wang", "Yongcai", ""], ["Li", "Deying", ""]]}, {"id": "2107.13843", "submitter": "Gali Sheffi", "authors": "Gali Sheffi, Maurice Herlihy and Erez Petrank", "title": "VBR: Version Based Reclamation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Safe lock-free memory reclamation is a difficult problem. Existing solutions\nfollow three basic methods (or their combinations): epoch based reclamation,\nhazard pointers, and optimistic reclamation. Epoch-based methods are fast, but\ndo not guarantee lock-freedom. Hazard pointer solutions are lock-free but\ntypically do not provide high performance. Optimistic methods are lock-free and\nfast, but previous optimistic methods did not go all the way. While reads were\nexecuted optimistically, writes were protected by hazard pointers. In this work\nwe present a new reclamation scheme called version based reclamation (VBR),\nwhich provides a full optimistic solution to lock-free memory reclamation,\nobtaining lock-freedom and high efficiency. Speculative execution is known as a\nfundamental tool for improving performance in various areas of computer\nscience, and indeed evaluation with a lock-free linked-list, hash-table and\nskip-list shows that VBR outperforms state-of-the-art existing solutions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 09:20:14 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sheffi", "Gali", ""], ["Herlihy", "Maurice", ""], ["Petrank", "Erez", ""]]}, {"id": "2107.13921", "submitter": "Dominik Scheinert", "authors": "Dominik Scheinert, Lauritz Thamsen, Houkun Zhu, Jonathan Will,\n  Alexander Acker, Thorsten Wittkopp, Odej Kao", "title": "Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across\n  Contexts", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed dataflow systems enable the use of clusters for scalable data\nanalytics. However, selecting appropriate cluster resources for a processing\njob is often not straightforward. Performance models trained on historical\nexecutions of a concrete job are helpful in such situations, yet they are\nusually bound to a specific job execution context (e.g. node type, software\nversions, job parameters) due to the few considered input parameters. Even in\ncase of slight context changes, such supportive models need to be retrained and\ncannot benefit from historical execution data from related contexts.\n  This paper presents Bellamy, a novel modeling approach that combines\nscale-outs, dataset sizes, and runtimes with additional descriptive properties\nof a dataflow job. It is thereby able to capture the context of a job\nexecution. Moreover, Bellamy is realizing a two-step modeling approach. First,\na general model is trained on all the available data for a specific scalable\nanalytics algorithm, hereby incorporating data from different contexts.\nSubsequently, the general model is optimized for the specific situation at\nhand, based on the available data for the concrete context. We evaluate our\napproach on two publicly available datasets consisting of execution data from\nvarious dataflow jobs carried out in different environments, showing that\nBellamy outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 11:57:38 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Scheinert", "Dominik", ""], ["Thamsen", "Lauritz", ""], ["Zhu", "Houkun", ""], ["Will", "Jonathan", ""], ["Acker", "Alexander", ""], ["Wittkopp", "Thorsten", ""], ["Kao", "Odej", ""]]}, {"id": "2107.14101", "submitter": "Zoran \\v{S}koda", "authors": "Zoran \\v{S}koda", "title": "Why blockchain and smart contracts need semantic descriptions", "comments": "early draft, to be significantly expanded", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We argue that there is a hierarchy of levels describing to that particular\nlevel relevant features of reality behind the content and behavior of\nblockchain and smart contracts in their realistic deployment.\n  Choice, design, audit and legal control of these systems could be more\ninformed, easier and raised to a higher level, if research on foundations of\nthese descriptions develops and sets the formalisms, tools and standards for\nsuch descriptions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:48:06 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["\u0160koda", "Zoran", ""]]}]