[{"id": "1502.00050", "submitter": "Moumen Hamouma", "authors": "Hamouma Moumen", "title": "Time-Free and Timer-Based Assumptions Can Be Combined to Solve\n  Authenticated Byzantine Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To circumvent the FLP impossibility result in a deterministic way several\nprotocols have been proposed on top of an asynchronous distributed system\nenriched with additional assumptions. In the context of Byzantine failures for\nsystems where at most t processes may exhibit a Byzantine behavior, two\napproaches have been investigated to solve the consensus problem.The first,\nrelies on the addition of synchrony, called Timer-Based, but the second is\nbased on the pattern of the messages that are exchanged, called Time-Free. This\npaper shows that both types of assumptions are not antagonist and can be\ncombined to solve authenticated Byzantine consensus. This combined assumption\nconsiders a correct process pi, called 2t-BW, and a set X of 2t processes such\nthat, eventually, for each query broadcasted by a correct process pj of X, pj\nreceives a response from pi 2 X among the (n- t) first responses to that query\nor both links connecting pi and pj are timely. Based on this combination, a\nsimple hybrid authenticated Byzantine consensus protocol,benefiting from the\nbest of both worlds, is proposed. Whereas many hybrid protocols have been\ndesigned for the consensus problem in the crash model, this is, to our\nknowledge, the first hybrid deterministic solution to the Byzantine consensus\nproblem.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 00:59:39 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Moumen", "Hamouma", ""]]}, {"id": "1502.00068", "submitter": "Ameet Talwalkar", "authors": "Evan R. Sparks, Ameet Talwalkar, Michael J. Franklin, Michael I.\n  Jordan, Tim Kraska", "title": "TuPAQ: An Efficient Planner for Large-scale Predictive Analytic Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of massive datasets combined with the development of\nsophisticated analytical techniques have enabled a wide variety of novel\napplications such as improved product recommendations, automatic image tagging,\nand improved speech-driven interfaces. These and many other applications can be\nsupported by Predictive Analytic Queries (PAQs). A major obstacle to supporting\nPAQs is the challenging and expensive process of identifying and training an\nappropriate predictive model. Recent efforts aiming to automate this process\nhave focused on single node implementations and have assumed that model\ntraining itself is a black box, thus limiting the effectiveness of such\napproaches on large-scale problems. In this work, we build upon these recent\nefforts and propose an integrated PAQ planning architecture that combines\nadvanced model search techniques, bandit resource allocation via runtime\nalgorithm introspection, and physical optimization via batching. The result is\nTuPAQ, a component of the MLbase system, which solves the PAQ planning problem\nwith comparable quality to exhaustive strategies but an order of magnitude more\nefficiently than the standard baseline approach, and can scale to models\ntrained on terabytes of data across hundreds of machines.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 04:51:58 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2015 22:02:24 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Sparks", "Evan R.", ""], ["Talwalkar", "Ameet", ""], ["Franklin", "Michael J.", ""], ["Jordan", "Michael I.", ""], ["Kraska", "Tim", ""]]}, {"id": "1502.00075", "submitter": "Lewis Tseng", "authors": "Lewis Tseng and Nitin Vaidya", "title": "Byzantine Broadcast Under a Selective Broadcast Model for Single-hop\n  Wireless Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores an old problem, {\\em Byzantine fault-tolerant Broadcast}\n(BB), under a new model, {\\em selective broadcast model}. The new model\n\"interpolates\" between the two traditional models in the literature. In\nparticular, it allows fault-free nodes to exploit the benefits of a broadcast\nchannel (a feature from reliable broadcast model) and allows faulty nodes to\nsend mismatching messages to different neighbors (a feature from point-to-point\nmodel) simultaneously. The {\\em selective broadcast} model is motivated by the\npotential for {\\em directional} transmissions on a wireless channel.\n  We provide a collection of results for a single-hop wireless network under\nthe new model. First, we present an algorithm for {\\em Multi-Valued} BB that is\norder-optimal in bit complexity. Then, we provide an algorithm that is designed\nto achieve BB efficiently in terms of message complexity. Third, we determine\nsome lower bounds on both bit and message complexities of BB problems in the\n{\\em selective broadcast model}. Finally, we present a conjecture on an \"exact\"\nlower bound on the bit complexity of BB under the {\\em selective broadcast}\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 06:20:34 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Tseng", "Lewis", ""], ["Vaidya", "Nitin", ""]]}, {"id": "1502.00089", "submitter": "Arnaud Casteigts", "authors": "Arnaud Casteigts, Ralf Klasing, Yessin M. Neggaz, Joseph G. Peters", "title": "Efficiently Testing T-Interval Connectivity in Dynamic Graphs", "comments": "Long version of a CIAC 2015 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many types of dynamic networks are made up of durable entities whose links\nevolve over time. When considered from a {\\em global} and {\\em discrete}\nstandpoint, these networks are often modelled as evolving graphs, i.e. a\nsequence of graphs ${\\cal G}=(G_1,G_2,...,G_{\\delta})$ such that $G_i=(V,E_i)$\nrepresents the network topology at time step $i$. Such a sequence is said to be\n$T$-interval connected if for any $t\\in [1, \\delta-T+1]$ all graphs in\n$\\{G_t,G_{t+1},...,G_{t+T-1}\\}$ share a common connected spanning subgraph. In\nthis paper, we consider the problem of deciding whether a given sequence ${\\cal\nG}$ is $T$-interval connected for a given $T$. We also consider the related\nproblem of finding the largest $T$ for which a given ${\\cal G}$ is $T$-interval\nconnected. We assume that the changes between two consecutive graphs are\narbitrary, and that two operations, {\\em binary intersection} and {\\em\nconnectivity testing}, are available to solve the problems. We show that\n$\\Omega(\\delta)$ such operations are required to solve both problems, and we\npresent optimal $O(\\delta)$ online algorithms for both problems. We extend our\nonline algorithms to a dynamic setting in which connectivity is based on the\nrecent evolution of the network.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 10:26:14 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 14:29:57 GMT"}, {"version": "v3", "created": "Fri, 17 Mar 2017 17:07:07 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Casteigts", "Arnaud", ""], ["Klasing", "Ralf", ""], ["Neggaz", "Yessin M.", ""], ["Peters", "Joseph G.", ""]]}, {"id": "1502.00101", "submitter": "Roman Dovgopol", "authors": "Roman Dovgopol, Matthew Rosonke", "title": "Hybrid Update/Invalidate Schemes for Cache Coherence Protocols", "comments": "Appearing in MSTUCA Scientific Bulletin - August 2015, sec:CS; 10\n  pages, 6 figure", "journal-ref": "MSTUCA Scientific Bulletin 218 (2015)", "doi": null, "report-no": "ISSN 2079-0619", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general when considering cache coherence, write back schemes are the\ndefault. These schemes invalidate all other copies of a data block during a\nwrite. In this paper we propose several hybrid schemes that will switch between\nupdating and invalidating on processor writes at runtime, depending on program\nconditions. We created our own cache simulator on which we could implement our\nschemes, and generated data sets from both commercial benchmarks and through\nartificial methods to run on the simulator. We analyze the results of running\nthe benchmarks with various schemes, and suggest further research that can be\ndone in this area.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 13:05:08 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Dovgopol", "Roman", ""], ["Rosonke", "Matthew", ""]]}, {"id": "1502.00206", "submitter": "Rajiv Ranjan Dr.", "authors": "Khalid Alhamazani, Rajiv Ranjan, Prem Prakash Jayaraman, Karan Mitra,\n  Chang Liu, Fethi Rabhi, Dimitrios Georgakopoulos, Lizhe Wang", "title": "Cross-Layer Multi-Cloud Real-Time Application QoS Monitoring and\n  Benchmarking As-a-Service Framework", "comments": "A revised version of this technical report has been accepted for\n  publication by IEEE Transactions on Cloud Computing on April 29, 2015", "journal-ref": null, "doi": "10.1109/TCC.2015.2441715", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides on-demand access to affordable hardware (multi-core\nCPUs, GPUs, disks, and networking equipment) and software (databases,\napplication servers and data processing frameworks) platforms with features\nsuch as elasticity, pay-per-use, low upfront investment and low time to market.\nThis has led to the proliferation of business critical applications that\nleverage various cloud platforms. Such applications hosted on single or\nmultiple cloud provider platforms have diverse characteristics requiring\nextensive monitoring and benchmarking mechanisms to ensure run-time Quality of\nService (QoS) (e.g., latency and throughput). This paper proposes, develops and\nvalidates CLAMBS:Cross-Layer Multi-Cloud Application Monitoring and\nBenchmarking as-a-Service for efficient QoS monitoring and benchmarking of\ncloud applications hosted on multi-clouds environments. The major highlight of\nCLAMBS is its capability of monitoring and benchmarking individual application\ncomponents such as databases and web servers, distributed across cloud layers,\nspread among multiple cloud providers. We validate CLAMBS using prototype\nimplementation and extensive experimentation and show that CLAMBS efficiently\nmonitors and benchmarks application components on multi-cloud platforms\nincluding Amazon EC2 and Microsoft Azure.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 06:57:47 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 05:28:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Alhamazani", "Khalid", ""], ["Ranjan", "Rajiv", ""], ["Jayaraman", "Prem Prakash", ""], ["Mitra", "Karan", ""], ["Liu", "Chang", ""], ["Rabhi", "Fethi", ""], ["Georgakopoulos", "Dimitrios", ""], ["Wang", "Lizhe", ""]]}, {"id": "1502.00316", "submitter": "Emilio Ferrara", "authors": "Xiaoming Gao, Emilio Ferrara, Judy Qiu", "title": "Parallel clustering of high-dimensional social media data streams", "comments": "IEEE/ACM CCGrid 2015: 15th IEEE/ACM International Symposium on\n  Cluster, Cloud and Grid Computing, 2015", "journal-ref": "Cluster, Cloud and Grid Computing (CCGrid), 2015 15th IEEE/ACM\n  International Symposium on (pp. 323-332). IEEE. 2015", "doi": "10.1109/CCGrid.2015.19", "report-no": null, "categories": "cs.DC cs.DB cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Cloud DIKW as an analysis environment supporting scientific\ndiscovery through integrated parallel batch and streaming processing, and apply\nit to one representative domain application: social media data stream\nclustering. Recent work demonstrated that high-quality clusters can be\ngenerated by representing the data points using high-dimensional vectors that\nreflect textual content and social network information. Due to the high cost of\nsimilarity computation, sequential implementations of even single-pass\nalgorithms cannot keep up with the speed of real-world streams. This paper\npresents our efforts to meet the constraints of real-time social stream\nclustering through parallelization. We focus on two system-level issues. Most\nstream processing engines like Apache Storm organize distributed workers in the\nform of a directed acyclic graph, making it difficult to dynamically\nsynchronize the state of parallel workers. We tackle this challenge by creating\na separate synchronization channel using a pub-sub messaging system. Due to the\nsparsity of the high-dimensional vectors, the size of centroids grows quickly\nas new data points are assigned to the clusters. Traditional synchronization\nthat directly broadcasts cluster centroids becomes too expensive and limits the\nscalability of the parallel algorithm. We address this problem by communicating\nonly dynamic changes of the clusters rather than the whole centroid vectors.\nOur algorithm under Cloud DIKW can process the Twitter 10% data stream in\nreal-time with 96-way parallelism. By natural improvements to Cloud DIKW,\nincluding advanced collective communication techniques developed in our Harp\nproject, we will be able to process the full Twitter stream in real-time with\n1000-way parallelism. Our use of powerful general software subsystems will\nenable many other applications that need integration of streaming and batch\ndata analytics.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:41:13 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Gao", "Xiaoming", ""], ["Ferrara", "Emilio", ""], ["Qiu", "Judy", ""]]}, {"id": "1502.00355", "submitter": "Gang Mei", "authors": "Kunyang Zhao, Gang Mei, Nengxiong Xu, and Jiayin Zhang", "title": "On the Accelerating of Two-dimensional Smart Laplacian Smoothing on the\n  GPU", "comments": "The author declares that this paper has been submitted to the\n  International Conference on Computational Science ICCS 2015. 10 pages, 4\n  figures", "journal-ref": "Journal of Information & Computational Science 12:13 (2015)\n  5133-5143", "doi": "10.12733/jics20106587", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a GPU-accelerated implementation of two-dimensional Smart\nLaplacian smoothing. This implementation is developed under the guideline of\nour paradigm for accelerating Laplacianbased mesh smoothing [13]. Two types of\ncommonly used data layouts, Array-of-Structures (AoS) and Structure-of-Arrays\n(SoA) are used to represent triangular meshes in our implementation. Two\niteration forms that have different choices of the swapping of intermediate\ndata are also adopted. Furthermore, the feature CUDA Dynamic Parallelism (CDP)\nis employed to realize the nested parallelization in Smart Laplacian smoothing.\nExperimental results demonstrate that: (1) our implementation can achieve the\nspeedups of up to 44x on the GPU GT640; (2) the data layout AoS can always\nobtain better efficiency than the SoA layout; (3) the form that needs to swap\nintermediate nodal coordinates is always slower than the one that does not swap\ndata; (4) the version of our implementation with the use of the feature CDP is\nslightly faster than the version where the CDP is not adopted.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 04:32:42 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Zhao", "Kunyang", ""], ["Mei", "Gang", ""], ["Xu", "Nengxiong", ""], ["Zhang", "Jiayin", ""]]}, {"id": "1502.00378", "submitter": "Swan Dubois", "authors": "Swan Dubois (REGAL, UPMC), Mohamed-Hamza Kaaouachi (REGAL, UPMC),\n  Franck Petit (REGAL, UPMC)", "title": "Enabling Minimal Dominating Set in Highly Dynamic Distributed Systems", "comments": "arXiv admin note: text overlap with arXiv:1412.6007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing a Minimal Dominating Set in highly\ndynamic distributed systems. We assume weak connectivity, i.e., the network may\nbe disconnected at each time instant and topological changes are unpredictable.\nWe make only weak assumptions on the communication: every process is infinitely\noften able to communicate with other processes (not necessarily directly). Our\ncontribution is threefold. First, we propose a new definition of minimal\ndominating set suitable for the context of time-varying graphs that seems more\nrelevant than existing ones. Next, we provide a necessary and sufficient\ntopological condition for the existence of a deterministic algorithm for\nminimal dominating set construction in our settings. Finally, we propose a new\nmeasure of time complexity in time-varying graph in order to to allow fair\ncomparison between algorithms. Indeed, this measure takes account of\ncommunication delays attributable to dynamicity of the graph and not to the\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 06:52:57 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Dubois", "Swan", "", "REGAL, UPMC"], ["Kaaouachi", "Mohamed-Hamza", "", "REGAL, UPMC"], ["Petit", "Franck", "", "REGAL, UPMC"]]}, {"id": "1502.00802", "submitter": "Ismail Elouafiq", "authors": "Amine Semma, Ismail Elouafiq", "title": "Algorithm for Achieving Consensus Over Conflicting Rumors: Convergence\n  Analysis and Applications", "comments": "IEEE Student Paper Contest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the large expansion in the study of social networks, this paper\ndeals with the problem of multiple messages spreading over the same network\nusing gossip algorithms. Given two messages distributed over some nodes of the\ngraph, we first investigate the final distribution of the messages given an\ninitial state. Then, an algorithm is presented to achieve consensus over one of\nthe messages. Finally, a game theoretical application and an analogy with\nword-of-mouth marketing are outlined.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 10:31:43 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Semma", "Amine", ""], ["Elouafiq", "Ismail", ""]]}, {"id": "1502.00858", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Distributed Radio Interferometric Calibration", "comments": "MNRAS Accepted 2015 March 13. Received 2015 January 28; in original\n  form 2014 November 6, low resolution figures", "journal-ref": null, "doi": "10.1093/mnras/stv596", "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing data volumes delivered by a new generation of radio\ninterferometers require computationally efficient and robust calibration\nalgorithms. In this paper, we propose distributed calibration as a way of\nimproving both computational cost as well as robustness in calibration. We\nexploit the data parallelism across frequency that is inherent in radio\nastronomical observations that are recorded as multiple channels at different\nfrequencies. Moreover, we also exploit the smoothness of the variation of\ncalibration parameters across frequency. Data parallelism enables us to\ndistribute the computing load across a network of compute agents. Smoothness in\nfrequency enables us reformulate calibration as a consensus optimization\nproblem. With this formulation, we enable flow of information between compute\nagents calibrating data at different frequencies, without actually passing the\ndata, and thereby improving robustness. We present simulation results to show\nthe feasibility as well as the advantages of distributed calibration as opposed\nto conventional calibration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 13:30:08 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 16:10:03 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "1502.01095", "submitter": "Nirmala Suresh", "authors": "A.P. Nirmala, Dr. R. Sridaran", "title": "A Novel architecture for improving performance under virtualized\n  environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though virtualization provides a lot of advantages in cloud computing,\nit does not provide effective performance isolation between the virtualization\nmachines. In other words, the performance may get affected due the\ninterferences caused by co-virtual machines. This can be achieved by the proper\nmanagement of resource allocations between the Virtual Machines running\nsimultaneously. This paper aims at providing a proposed novel architecture that\nis based on Fast Genetic K-means++ algorithm and test results show positive\nimprovements in terms of performance improvements over a similar existing\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 05:21:30 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Nirmala", "A. P.", ""], ["Sridaran", "Dr. R.", ""]]}, {"id": "1502.01435", "submitter": "Quentin Stout", "authors": "Quentin F. Stout", "title": "Optimal component labeling algorithms for mesh-connected computers and\n  VLSI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an undirected graph $G$ of $n$ weighted edges, stored one edge per\nprocessor in a square mesh of $n$ processors, we show how to determine the\nconnected components and a minimal spanning forest in $\\Theta(\\sqrt{n})$ time.\nMore generally, we show how to solve these problems in $\\Theta(n^{1/d})$ time\nwhen the mesh is a $d$-dimensional cube, where the implied constants depend\nupon $d$.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 05:11:47 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Stout", "Quentin F.", ""]]}, {"id": "1502.01633", "submitter": "Petr Kuznetsov", "authors": "Vitaly Aksenov, Vincent Gramoli, Petr Kuznetsov, Srivatsan Ravi, Di\n  Shang", "title": "A Concurrency-Optimal List-Based Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an efficient concurrent data structure is an important challenge\nthat is not easy to meet. Intuitively, efficiency of an implementation is\ndefined, in the first place, by its ability to process applied operations in\nparallel, without using unnecessary synchronization. As we show in this paper,\neven for a data structure as simple as a linked list used to implement the set\ntype, the most efficient algorithms known so far are not concurrency-optimal:\nthey may reject correct concurrent schedules.\n  We propose a new algorithm for the list-based set based on a value-aware\ntry-lock that we show to achieve optimal concurrency: it only rejects\nconcurrent schedules that violate correctness of the implemented set type. We\nshow empirically that reaching optimality does not induce a significant\noverhead. In fact, our implementation of the concurrency-optimal algorithm\noutperforms both the Lazy Linked List and the Harris-Michael state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 16:41:18 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 02:13:40 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 10:43:58 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Aksenov", "Vitaly", ""], ["Gramoli", "Vincent", ""], ["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""], ["Shang", "Di", ""]]}, {"id": "1502.01838", "submitter": "Sean Sedwards", "authors": "Cyrille Jegourel, Axel Legay, Sean Sedwards and Louis-Marie Traonouez", "title": "Distributed Verification of Rare Properties using Importance Splitting\n  Observers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare properties remain a challenge for statistical model checking (SMC) due\nto the quadratic scaling of variance with rarity. We address this with a\nvariance reduction framework based on lightweight importance splitting\nobservers. These expose the model-property automaton to allow the construction\nof score functions for high performance algorithms.\n  The confidence intervals defined for importance splitting make it appealing\nfor SMC, but optimising its performance in the standard way makes distribution\ninefficient. We show how it is possible to achieve equivalently good results in\nless time by distributing simpler algorithms. We first explore the challenges\nposed by importance splitting and present an algorithm optimised for\ndistribution. We then define a specific bounded time logic that is compiled\ninto memory-efficient observers to monitor executions. Finally, we demonstrate\nour framework on a number of challenging case studies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 10:00:23 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 14:41:26 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Jegourel", "Cyrille", ""], ["Legay", "Axel", ""], ["Sedwards", "Sean", ""], ["Traonouez", "Louis-Marie", ""]]}, {"id": "1502.02290", "submitter": "Chinmoy Dutta", "authors": "Chinmoy Dutta and Yashodhan Kanoria and D. Manjunath and Jaikumar\n  Radhakrishnan", "title": "How Hard is Computing Parity with Noisy Communications?", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a tight lower bound of $\\Omega(N \\log\\log N)$ on the number of\ntransmissions required to compute the parity of $N$ input bits with constant\nerror in a noisy communication network of $N$ randomly placed sensors, each\nhaving one input bit and communicating with others using local transmissions\nwith power near the connectivity threshold. This result settles the lower bound\nquestion left open by Ying, Srikant and Dullerud (WiOpt 06), who showed how the\nsum of all the $N$ bits can be computed using $O(N \\log\\log N)$ transmissions.\nThe same lower bound has been shown to hold for a host of other functions\nincluding majority by Dutta and Radhakrishnan (FOCS 2008).\n  Most works on lower bounds for communication networks considered mostly the\nfull broadcast model without using the fact that the communication in real\nnetworks is local, determined by the power of the transmitters. In fact, in\nfull broadcast networks computing parity needs $\\theta(N)$ transmissions. To\nobtain our lower bound we employ techniques developed by Goyal, Kindler and\nSaks (FOCS 05), who showed lower bounds in the full broadcast model by reducing\nthe problem to a model of noisy decision trees. However, in order to capture\nthe limited range of transmissions in real sensor networks, we adapt their\ndefinition of noisy decision trees and allow each node of the tree access to\nonly a limited part of the input. Our lower bound is obtained by exploiting\nspecial properties of parity computations in such noisy decision trees.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 19:32:58 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Dutta", "Chinmoy", ""], ["Kanoria", "Yashodhan", ""], ["Manjunath", "D.", ""], ["Radhakrishnan", "Jaikumar", ""]]}, {"id": "1502.02304", "submitter": "Istvan Szalkai", "authors": "Istvan Szalkai, Gyorgy Dosa", "title": "Online Algorithms for a Generalized Parallel Machine Scheduling Problem", "comments": "International Conference on Recent Achievements in Mechatronics,\n  Automation, Computer Sciences and Robotics at SAPIENTIA University, Tirgu\n  Mures, Romania on 6-7th March, 2015. http://www.macro.ms.sapientia.ro/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider different online algorithms for a generalized scheduling problem\nfor parallel machines, described in details in the first section. This problem\nis the generalization of the classical parallel machine scheduling problem,\nwhen the make-span is minimized; in that case each job contains only one task.\nOn the other hand, the problem in consideration is still a special version of\nthe workflow scheduling problem. We present several heuristic algorithms and\ncompare them by computer tests.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 21:11:36 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Szalkai", "Istvan", ""], ["Dosa", "Gyorgy", ""]]}, {"id": "1502.02358", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Using Heavy Clique Base Coarsening to Enhance Virtual Network Embedding", "comments": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 6, No. 1, 2015", "journal-ref": null, "doi": "10.14569/IJACSA.2015.060118", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network virtualization allows cloud infrastructure providers to accommodate\nmultiple virtual networks on a single physical network. However, mapping\nmultiple virtual network resources to physical network components, called\nvirtual network embedding (VNE), is known to be non-deterministic\npolynomial-time hard (NP-hard). Effective virtual network embedding increases\nthe revenue by increasing the number of accepted virtual networks. In this\npaper, we propose virtual network embedding algorithm, which improves virtual\nnetwork embedding by coarsening virtual networks. Heavy Clique matching\ntechnique is used to coarsen virtual networks. Then, the coarsened virtual\nnetworks are enhanced by using a refined Kernighan-Lin algorithm. The\nperformance of the proposed algorithm is evaluated and compared with existing\nalgorithms using extensive simulations, which show that the proposed algorithm\nimproves virtual network embedding by increasing the acceptance ratio and the\nrevenue.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 04:51:41 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1502.02389", "submitter": "Michel Steuwer", "authors": "Michel Steuwer and Christian Fensch and Christophe Dubach", "title": "Patterns and Rewrite Rules for Systematic Code Generation (From\n  High-Level Functional Patterns to High-Performance OpenCL Code)", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing systems have become increasingly complex with the emergence of\nheterogeneous hardware combining multicore CPUs and GPUs. These parallel\nsystems exhibit tremendous computational power at the cost of increased\nprogramming effort. This results in a tension between achieving performance and\ncode portability. Code is either tuned using device-specific optimizations to\nachieve maximum performance or is written in a high-level language to achieve\nportability at the expense of performance.\n  We propose a novel approach that offers high-level programming, code\nportability and high-performance. It is based on algorithmic pattern\ncomposition coupled with a powerful, yet simple, set of rewrite rules. This\nenables systematic transformation and optimization of a high-level program into\na low-level hardware specific representation which leads to high performance\ncode.\n  We test our design in practice by describing a subset of the OpenCL\nprogramming model with low-level patterns and by implementing a compiler which\ngenerates high performance OpenCL code. Our experiments show that we can\nsystematically derive high-performance device-specific implementations from\nsimple high-level algorithmic expressions. The performance of the generated\nOpenCL code is on par with highly tuned implementations for multicore CPUs and\nGPUs written by experts\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 07:28:22 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Steuwer", "Michel", ""], ["Fensch", "Christian", ""], ["Dubach", "Christophe", ""]]}, {"id": "1502.02426", "submitter": "Fabian Fuchs", "authors": "Fabian Fuchs, Roman Prutkin", "title": "Simple Distributed Delta + 1 Coloring in the SINR Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wireless ad hoc or sensor networks, distributed node coloring is a\nfundamental problem closely related to establishing efficient communication\nthrough TDMA schedules. For networks with maximum degree Delta, a Delta + 1\ncoloring is the ultimate goal in the distributed setting as this is always\npossible. In this work we propose Delta + 1 coloring algorithms for the\nsynchronous and asynchronous setting. All algorithms have a runtime of O(Delta\nlog n) time slots. This improves on the previous algorithms for the SINR model\neither in terms of the number of required colors or the runtime and matches the\nruntime of local broadcasting in the SINR model (which can be seen as a lower\nbound).\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 10:43:01 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Fuchs", "Fabian", ""], ["Prutkin", "Roman", ""]]}, {"id": "1502.02538", "submitter": "Srikanth Sastry", "authors": "Nancy Lynch and Srikanth Sastry", "title": "Consensus using Asynchronous Failure Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FLP result shows that crash-tolerant consensus is impossible to solve in\nasynchronous systems, and several solutions have been proposed for\ncrash-tolerant consensus under alternative (stronger) models. One popular\napproach is to augment the asynchronous system with appropriate failure\ndetectors, which provide (potentially unreliable) information about process\ncrashes in the system, to circumvent the FLP impossibility.\n  In this paper, we demonstrate the exact mechanism by which (sufficiently\npowerful) asynchronous failure detectors enable solving crash-tolerant\nconsensus. Our approach, which borrows arguments from the FLP impossibility\nproof and the famous result from CHT, which shows that $\\Omega$ is a weakest\nfailure detector to solve consensus, also yields a natural proof to $\\Omega$ as\na weakest asynchronous failure detector to solve consensus. The use of I/O\nautomata theory in our approach enables us to model execution in a more\ndetailed fashion than CHT and also addresses the latent assumptions and\nassertions in the original result in CHT.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:22:16 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 20:45:59 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Lynch", "Nancy", ""], ["Sastry", "Srikanth", ""]]}, {"id": "1502.02606", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa and Alina Ene and Huy L. Nguyen and Justin\n  Ward", "title": "The Power of Randomization: Distributed Submodular Maximization on\n  Massive Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. Unfortunately, the resulting\nsubmodular optimization problems are often too large to be solved on a single\nmachine. We develop a simple distributed algorithm that is embarrassingly\nparallel and it achieves provable, constant factor, worst-case approximation\nguarantees. In our experiments, we demonstrate its efficiency in large problems\nwith different kinds of constraints with objective values always close to what\nis achievable in the centralized setting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:04:43 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 17:49:22 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1502.02725", "submitter": "Srivatsan Ravi Mr", "authors": "Petr Kuznetsov, Srivatsan Ravi", "title": "Why Transactional Memory Should Not Be Obstruction-Free", "comments": "Model of Transactional Memory identical with arXiv:1407.6876", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory (TM) is an inherently optimistic abstraction: it allows\nconcurrent processes to execute sequences of shared-data accesses\n(transactions) speculatively, with an option of aborting them in the future.\nEarly TM designs avoided using locks and relied on non-blocking synchronization\nto ensure obstruction-freedom: a transaction that encounters no step contention\nis not allowed to abort. However, it was later observed that obstruction-free\nTMs perform poorly and, as a result, state-of-the-art TM implementations are\nnowadays blocking, allowing aborts because of data conflicts rather than step\ncontention.\n  In this paper, we explain this shift in the TM practice theoretically, via\ncomplexity bounds. We prove a few important lower bounds on obstruction-free\nTMs. Then we present a lock-based TM implementation that beats all of these\nlower bounds. In sum, our results exhibit a considerable complexity gap between\nnon-blocking and blocking TM implementations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 23:14:49 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 09:09:00 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1502.02768", "submitter": "Ashraf Shahin", "authors": "Ashraf A. Shahin", "title": "Virtual Network Embedding Algorithms Based on Best-Fit Subgraph\n  Detection", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.02358", "journal-ref": "Computer and Information Science; Vol. 8, No. 1; 2015", "doi": "10.5539/cis.v8n1p62", "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main objectives of cloud computing providers is increasing the\nrevenue of their cloud datacenters by accommodating virtual network requests as\nmany as possible. However, arrival and departure of virtual network requests\nfragment physical network's resources and reduce the possibility of accepting\nmore virtual network requests. To increase the number of virtual network\nrequests accommodated by fragmented physical networks, we propose two virtual\nnetwork embedding algorithms, which coarsen virtual networks using Heavy Edge\nMatching (HEM) technique and embed coarsened virtual networks on best-fit\nsub-substrate networks. The performance of the proposed algorithms are\nevaluated and compared with existing algorithms using extensive simulations,\nwhich show that the proposed algorithms increase the acceptance ratio and the\nrevenue.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 03:43:40 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Shahin", "Ashraf A.", ""]]}, {"id": "1502.02821", "submitter": "Paul Doyle Dr.", "authors": "Paul Doyle", "title": "Building a scalable global data processing pipeline for large\n  astronomical photometric datasets", "comments": "PhD Thesis, Dublin Institute of Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomical photometry is the science of measuring the flux of a celestial\nobject. Since its introduction, the CCD has been the principle method of\nmeasuring flux to calculate the apparent magnitude of an object. Each CCD image\ntaken must go through a process of cleaning and calibration prior to its use.\nAs the number of research telescopes increases the overall computing resources\nrequired for image processing also increases. Existing processing techniques\nare primarily sequential in nature, requiring increasingly powerful servers,\nfaster disks and faster networks to process data. Existing High Performance\nComputing solutions involving high capacity data centres are complex in design\nand expensive to maintain, while providing resources primarily to high profile\nscience projects. This research describes three distributed pipeline\narchitectures, a virtualised cloud based IRAF, the Astronomical Compute Node\n(ACN), a private cloud based pipeline, and NIMBUS, a globally distributed\nsystem. The ACN pipeline processed data at a rate of 4 Terabytes per day\ndemonstrating data compression and upload to a central cloud storage service at\na rate faster than data generation. The primary contribution of this research\nis NIMBUS, which is rapidly scalable, resilient to failure and capable of\nprocessing CCD image data at a rate of hundreds of Terabytes per day. This\npipeline is implemented using a decentralised web queue to control the\ncompression of data, uploading of data to distributed web servers, and creating\nweb messages to identify the location of the data. Using distributed web queue\nmessages, images are downloaded by computing resources distributed around the\nglobe. Rigorous experimental evidence is presented verifying the horizontal\nscalability of the system which has demonstrated a processing rate of 192\nTerabytes per day with clear indications that higher processing rates are\npossible.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 09:27:58 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Doyle", "Paul", ""]]}, {"id": "1502.02852", "submitter": "Daniel Gregorek", "authors": "Daniel Gregorek, Robert Schmidt, Alberto Garcia-Ortiz", "title": "Transaction Level Analysis for a Clustered and Hardware-Enhanced Task\n  Manager on Homogeneous Many-Core Systems", "comments": "Presented at HIP3ES, 2015 (arXiv: 1501.03064)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2015/03", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing parallelism of many-core systems demands for efficient\nstrategies for the run-time system management. Due to the large number of cores\nthe management overhead has a rising impact to the overall system performance.\nThis work analyzes a clustered infrastructure of dedicated hardware nodes to\nmanage a homogeneous many-core system. The hardware nodes implement a message\npassing protocol and perform the task mapping and synchronization at run-time.\nTo make meaningful mapping decisions, the global management nodes employ a\nworkload status communication mechanism.\n  This paper discusses the design-space of the dedicated infrastructure by\nmeans of task mapping use-cases and a parallel benchmark including\napplication-interference. We evaluate the architecture in terms of application\nspeedup and analyze the mechanism for the status communication. A comparison\nversus centralized and fully-distributed configurations demonstrates the\nreduction of the computation and communication management overhead for our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 10:56:50 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Gregorek", "Daniel", ""], ["Schmidt", "Robert", ""], ["Garcia-Ortiz", "Alberto", ""]]}, {"id": "1502.02908", "submitter": "Stefan Engblom", "authors": "Pavol Bauer, Stefan Engblom, Stefan Widgren", "title": "Fast event-based epidemiological simulations on national scales", "comments": "27 pages, 5 figures", "journal-ref": "Int. J. High Perf. Comput. Appl. 30(4):438--453 (2016)", "doi": "10.1177/1094342016635723", "report-no": null, "categories": "q-bio.PE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational modeling framework for data-driven simulations and\nanalysis of infectious disease spread in large populations. For the purpose of\nefficient simulations, we devise a parallel solution algorithm targeting\nmulti-socket shared memory architectures. The model integrates infectious\ndynamics as continuous-time Markov chains and available data such as animal\nmovements or aging are incorporated as externally defined events. To bring out\nparallelism and accelerate the computations, we decompose the spatial domain\nand optimize cross-boundary communication using dependency-aware task\nscheduling. Using registered livestock data at a high spatio-temporal\nresolution, we demonstrate that our approach not only is resilient to varying\nmodel configurations, but also scales on all physical cores at realistic work\nloads. Finally, we show that these very features enable the solution of inverse\nproblems on national scales.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 13:48:46 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 15:48:54 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 13:57:07 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Bauer", "Pavol", ""], ["Engblom", "Stefan", ""], ["Widgren", "Stefan", ""]]}, {"id": "1502.02921", "submitter": "Albert Saa-Garriga", "authors": "Albert Saa-Garriga, David Castells-Rufas and Jordi Carrabina", "title": "OMP2MPI: Automatic MPI code generation from OpenMP programs", "comments": "Presented at HIP3ES, 2015 (arXiv: 1501.03064)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2015/06", "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present OMP2MPI a tool that generates automatically MPI\nsource code from OpenMP. With this transformation the original program can be\nadapted to be able to exploit a larger number of processors by surpassing the\nlimits of the node level on large HPC clusters. The transformation can also be\nuseful to adapt the source code to execute in distributed memory many-cores\nwith message passing support. In addition, the resulting MPI code can be used\nas an starting point that still can be further optimized by software engineers.\nThe transformation process is focused on detecting OpenMP parallel loops and\ndistributing them in a master/worker pattern. A set of micro-benchmarks have\nbeen used to verify the correctness of the the transformation and to measure\nthe resulting performance. Surprisingly not only the automatically generated\ncode is correct by construction, but also it often performs faster even when\nexecuted with MPI.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 14:32:25 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 08:52:41 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Saa-Garriga", "Albert", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1502.02991", "submitter": "Gal Amram", "authors": "Gal Amram, Lior Mizrahi and Gera Weiss", "title": "Simple Executions of Snapshot Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well known snapshot primitive in concurrent programming allows for\nn-asynchronous processes to write values to an array of single-writer registers\nand, for each process, to take a snapshot of these registers. In this paper we\nprovide a formulation of the well known linearizability condition for snapshot\nalgorithms in terms of the existence of certain mathematical functions. In\naddition, we identify a simplifying property of snapshot implementations we\ncall \"schedule-based algorithms\". This property is natural to assume in the\nsense that as far as we know, every published snapshot algorithm is\nschedule-based. Based on this, we prove that when dealing with schedule-based\nalgorithms, it suffices to consider only a small class of very simple\nexecutions to prove or disprove correctness in terms of linearizability. We\nbelieve that the ideas developed in this paper may help to design automatic\nverification of snapshot algorithms. Since verifying linearizability was\nrecently proved to be EXPSPACE-complete, focusing on unique objects (snapshot\nin our case) can potentially lead to designing restricted, but feasible\nverification methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 17:03:54 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Amram", "Gal", ""], ["Mizrahi", "Lior", ""], ["Weiss", "Gera", ""]]}, {"id": "1502.03032", "submitter": "Jiyan Yang", "authors": "Jiyan Yang, Xiangrui Meng, Michael W. Mahoney", "title": "Implementing Randomized Matrix Algorithms in Parallel and Distributed\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of large-scale data, distributed systems built on top of clusters\nof commodity hardware provide cheap and reliable storage and scalable\nprocessing of massive data. Here, we review recent work on developing and\nimplementing randomized matrix algorithms in large-scale parallel and\ndistributed environments. Randomized algorithms for matrix problems have\nreceived a great deal of attention in recent years, thus far typically either\nin theory or in machine learning applications or with implementations on a\nsingle machine. Our main focus is on the underlying theory and practical\nimplementation of random projection and random sampling algorithms for very\nlarge very overdetermined (i.e., overconstrained) $\\ell_1$ and $\\ell_2$\nregression problems. Randomization can be used in one of two related ways:\neither to construct sub-sampled problems that can be solved, exactly or\napproximately, with traditional numerical methods; or to construct\npreconditioned versions of the original full problem that are easier to solve\nwith traditional iterative algorithms. Theoretical results demonstrate that in\nnear input-sparsity time and with only a few passes through the data one can\nobtain very strong relative-error approximate solutions, with high probability.\nEmpirical results highlight the importance of various trade-offs (e.g., between\nthe time to construct an embedding and the conditioning quality of the\nembedding, between the relative importance of computation versus communication,\netc.) and demonstrate that $\\ell_1$ and $\\ell_2$ regression problems can be\nsolved to low, medium, or high precision in existing distributed systems on up\nto terabyte-sized data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:38:44 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:15:33 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Yang", "Jiyan", ""], ["Meng", "Xiangrui", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1502.03157", "submitter": "EPTCS", "authors": "Javier C\\'amara (Carnegie Mellon University), Jos\\'e Proen\\c{c}a (KU\n  Leuven)", "title": "Proceedings 13th International Workshop on Foundations of Coordination\n  Languages and Self-Adaptive Systems", "comments": null, "journal-ref": "EPTCS 175, 2015", "doi": "10.4204/EPTCS.175", "report-no": null, "categories": "cs.DC cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of FOCLASA 2014, the 13th International\nWorkshop on the Foundations of Coordination Languages and Self-Adaptive\nSystems. FOCLASA 2014 was held in Rome, Italy, on September 9, 2014 as a\nsatellite event of CONCUR 2014, the 25th International Conference on\nConcurrency Theory.\n  Modern software systems are distributed, concurrent, mobile, and often\ninvolve composition of heterogeneous components and stand-alone services.\nService coordination and self-adaptation constitute the core characteristics of\ndistributed and service-oriented systems. Coordination languages and formal\napproaches to modelling and reasoning about self-adaptive behaviour help to\nsimplify the development of complex distributed service-based systems, enable\nfunctional correctness proofs and improve reusability and maintainability of\nsuch systems. The goal of the FOCLASA workshop is to put together researchers\nand practitioners of the aforementioned fields, to share and identify common\nproblems, and to devise general solutions in the context of coordination\nlanguages and self-adaptive systems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:30:30 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["C\u00e1mara", "Javier", "", "Carnegie Mellon University"], ["Proen\u00e7a", "Jos\u00e9", "", "KU\n  Leuven"]]}, {"id": "1502.03158", "submitter": "Haitham Bou Ammar PhD", "authors": "Rasul Tutunov, Haitham Bou Ammar, Ali Jadbabaie", "title": "A Fast Distributed Solver for Symmetric Diagonally Dominant Linear\n  Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fast distributed solver for linear equations\ngiven by symmetric diagonally dominant M-Matrices. Our approach is based on a\ndistributed implementation of the parallel solver of Spielman and Peng by\nconsidering a specific approximated inverse chain which can be computed\nefficiently in a distributed fashion. Representing the system of equations by a\ngraph $\\mathbb{G}$, the proposed distributed algorithm is capable of attaining\n$\\epsilon$-close solutions (for arbitrary $\\epsilon$) in time proportional to\n$n^{3}$ (number of nodes in $\\mathbb{G}$), ${\\alpha}$ (upper bound on the size\nof the R-Hop neighborhood), and $\\frac{{W}_{max}}{{W}_{min}}$ (maximum and\nminimum weight of edges in $\\mathbb{G}$).\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:32:28 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Tutunov", "Rasul", ""], ["Ammar", "Haitham Bou", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1502.03234", "submitter": "Paul Springer", "authors": "Paul Springer", "title": "A Scalable, Linear-Time Dynamic Cutoff Algorithm for Molecular\n  Simulations of Interfacial Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This master thesis introduces the idea of dynamic cutoffs in molecular\ndynamics simulations, based on the distance between particles and the\ninterface, and presents a solution for detecting interfaces in real-time. Our\ndynamic cutoff method (DCM) exhibits a linear-time complexity as well as nearly\nideal weak and strong scaling. The DCM is tailored for massively parallel\narchitectures and for large interfacial systems with millions of particles. We\nimplemented the DCM as part of the LAMMPS open-source molecular dynamics\npackage and demonstrate the nearly ideal weak- and strong-scaling behavior of\nthis method on an IBM BlueGene/Q supercomputer. Our results for a liquid/vapor\nsystem consisting of Lennard-Jones particles show that the accuracy of DCM is\ncomparable to that of the traditional particle-particle particle- mesh (PPPM)\nalgorithm. The performance comparison indicates that DCM is preferable for\nlarge systems due to the limited scaling of FFTs within the PPPM algorithm.\nMoreover, the DCM requires the interface to be identified every other MD\ntimestep. As a consequence, this thesis also presents an interface detection\nmethod which is (1) applicable in real time; (2) parallelizable; and (3) scales\nlinearly with respect to the number of particles.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 09:44:26 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Springer", "Paul", ""]]}, {"id": "1502.03320", "submitter": "Shay Kutten", "authors": "Valerie King, Shay Kutten, Mikkel Thorup", "title": "Construction and impromptu repair of an MST in a distributed network\n  with o(m) communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the CONGEST model, a communications network is an undirected graph whose\n$n$ nodes are processors and whose $m$ edges are the communications links\nbetween processors. At any given time step, a message of size $O(\\log n)$ may\nbe sent by each node to each of its neighbors. We show for the synchronous\nmodel: If all nodes start in the same round, and each node knows its ID and the\nID's of its neighbors, or in the case of MST, the distinct weights of its\nincident edges and knows $n$, then there are Monte Carlo algorithms which\nsucceed w.h.p. to determine a minimum spanning forest (MST) and a spanning\nforest (ST) using $O(n \\log^2 n/\\log\\log n)$ messages for MST and $O(n \\log n\n)$ messages for ST, resp. These results contradict the \"folk theorem\" noted in\nAwerbuch, et.al., JACM 1990 that the distributed construction of a broadcast\ntree requires $\\Omega(m)$ messages. This lower bound has been shown there and\nin other papers for some CONGEST models; our protocol demonstrates the limits\nof these models.\n  A dynamic distributed network is one which undergoes online edge insertions\nor deletions. We also show how to repair an MST or ST in a dynamic network with\nasynchronous communication. An edge deletion can be processed in $O(n\\log n\n/\\log \\log n)$ expected messages in the MST, and $O(n)$ expected messages for\nthe ST problem, while an edge insertion uses $O(n)$ messages in the worst case.\nWe call this \"impromptu\" updating as we assume that between processing of edge\nupdates there is no preprocessing or storage of additional information.\nPrevious algorithms for this problem that use an amortized $o(m)$ messages per\nupdate require substantial preprocessing and additional local storage between\nupdates.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 14:43:28 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["King", "Valerie", ""], ["Kutten", "Shay", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1502.03372", "submitter": "Jelena Marasevic", "authors": "Jelena Marasevic, Cliff Stein, and Gil Zussman", "title": "A Fast Distributed Stateless Algorithm for $\\alpha$-Fair Packing\n  Problems", "comments": "Added structural results for asymptotic cases of \\alpha-fairness\n  (\\alpha approaching 0, 1, or infinity), improved presentation, and revised\n  throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, fair resource allocation problems have received\nconsiderable attention in a variety of application areas. However, little\nprogress has been made in the design of distributed algorithms with convergence\nguarantees for general and commonly used $\\alpha$-fair allocations. In this\npaper, we study weighted $\\alpha$-fair packing problems, that is, the problems\nof maximizing the objective functions (i) $\\sum_j w_j\nx_j^{1-\\alpha}/(1-\\alpha)$ when $\\alpha > 0$, $\\alpha \\neq 1$ and (ii) $\\sum_j\nw_j \\ln x_j$ when $\\alpha = 1$, over linear constraints $Ax \\leq b$, $x\\geq 0$,\nwhere $w_j$ are positive weights and $A$ and $b$ are non-negative. We consider\nthe distributed computation model that was used for packing linear programs and\nnetwork utility maximization problems. Under this model, we provide a\ndistributed algorithm for general $\\alpha$ that converges to an\n$\\varepsilon-$approximate solution in time (number of distributed iterations)\nthat has an inverse polynomial dependence on the approximation parameter\n$\\varepsilon$ and poly-logarithmic dependence on the problem size. This is the\nfirst distributed algorithm for weighted $\\alpha-$fair packing with\npoly-logarithmic convergence in the input size. The algorithm uses simple local\nupdate rules and is stateless (namely, it allows asynchronous updates, is\nself-stabilizing, and allows incremental and local adjustments). We also obtain\na number of structural results that characterize $\\alpha-$fair allocations as\nthe value of $\\alpha$ is varied. These results deepen our understanding of\nfairness guarantees in $\\alpha-$fair packing allocations, and also provide\ninsight into the behavior of $\\alpha-$fair allocations in the asymptotic cases\n$\\alpha\\rightarrow 0$, $\\alpha \\rightarrow 1$, and $\\alpha \\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 16:58:16 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 16:49:27 GMT"}, {"version": "v3", "created": "Tue, 16 Feb 2016 00:02:04 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Marasevic", "Jelena", ""], ["Stein", "Cliff", ""], ["Zussman", "Gil", ""]]}, {"id": "1502.03431", "submitter": "Camille Coti", "authors": "Camille Coti and Sami Evangelista and Kais Klai", "title": "Time Petri Net Models for a New Queuless and Uncentralized Resource\n  Discovery System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we detail the model using Petri Nets of a new fully\ndistributed resource reservation system. The basic idea of the considered\ndistributed system is to let a user reserve a set of resources on a local\nnetwork and to use them, without any specific, central administration component\nsuch as a front-end node. Resources can be, for instance, computing resources\n(cores, nodes, GPUs...) or some memory on a server. In order to verify some\nqualitative and quantitative properties provided by this system, we need to\nmodel it. We detail the algorithms used by this system and the Petri Net models\nwe made of it.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 20:36:30 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Coti", "Camille", ""], ["Evangelista", "Sami", ""], ["Klai", "Kais", ""]]}, {"id": "1502.03504", "submitter": "Matthew Sottile", "authors": "Craig Rasmussen and Matthew Sottile and Daniel Nagle and Soren\n  Rasmussen", "title": "Locally-Oriented Programming: A Simple Programming Model for\n  Stencil-Based Computations on Multi-Level Distributed Memory Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging hybrid accelerator architectures for high performance computing are\noften suited for the use of a data-parallel programming model. Unfortunately,\nprogrammers of these architectures face a steep learning curve that frequently\nrequires learning a new language (e.g., OpenCL). Furthermore, the distributed\n(and frequently multi-level) nature of the memory organization of clusters of\nthese machines provides an additional level of complexity. This paper presents\npreliminary work examining how programming with a local orientation can be\nemployed to provide simpler access to accelerator architectures. A\nlocally-oriented programming model is especially useful for the solution of\nalgorithms requiring the application of a stencil or convolution kernel. In\nthis programming model, a programmer codes the algorithm by modifying only a\nsingle array element (called the local element), but has read-only access to a\nsmall sub-array surrounding the local element. We demonstrate how a\nlocally-oriented programming model can be adopted as a language extension using\nsource-to-source program transformations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 01:34:33 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Rasmussen", "Craig", ""], ["Sottile", "Matthew", ""], ["Nagle", "Daniel", ""], ["Rasmussen", "Soren", ""]]}, {"id": "1502.03513", "submitter": "EPTCS", "authors": "Denis Darquennes, Jean-Marie Jacquet, Isabelle Linden", "title": "On Distributed Density in Tuple-based Coordination Languages", "comments": "In Proceedings FOCLASA 2014, arXiv:1502.03157", "journal-ref": "EPTCS 175, 2015, pp. 36-53", "doi": "10.4204/EPTCS.175.3", "report-no": null, "categories": "cs.PL cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the chemical metaphor, this paper proposes an extension of\nLinda-like languages in the aim of modeling the coordination of complex\ndistributed systems. The new language manipulates finite sets of tuples and\ndistributes a density among them. This new concept adds to the non-determinism\ninherent in the selection of matched tuples a non-determinism to the tell, ask\nand get primitives on the consideration of different tuples. Furthermore,\nthanks to de Boer and Palamidessi's notion of modular embedding, we establish\nthat this new language strictly increases the expressiveness of the Dense Bach\nlanguage introduced earlier and, consequently, Linda-like languages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:15:02 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Darquennes", "Denis", ""], ["Jacquet", "Jean-Marie", ""], ["Linden", "Isabelle", ""]]}, {"id": "1502.03515", "submitter": "EPTCS", "authors": "Ludovic Henrio (Univ. of Nice Sophia Antipolis, CNRS, France),\n  Oleksandra Kulankhina (NRIA Sophia Antipolis Mediterannee, Univ. of Nice\n  Sophia Antipolis, CNRS, France), Dongqian Liu (MoE Engineering Research\n  Center for Software and Hardware Co-design Technology and Application, ECNU,\n  China), Eric Madelaine (NRIA Sophia Antipolis Mediterannee, Univ. of Nice\n  Sophia Antipolis, CNRS, France)", "title": "Verifying the correct composition of distributed components:\n  Formalisation and Tool", "comments": "In Proceedings FOCLASA 2014, arXiv:1502.03157", "journal-ref": "EPTCS 175, 2015, pp. 69-85", "doi": "10.4204/EPTCS.175.5", "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides formal definitions characterizing well-formed\ncomposition of components in order to guarantee their safe deployment and\nexecution. Our work focuses on the structural aspects of component composition;\nit puts together most of the concepts common to many component models, but\nnever formalized as a whole. Our formalization characterizes correct component\narchitectures made of functional and non-functional aspects, both structured as\ncomponent assemblies. Interceptor chains can be used for a safe and controlled\ninteraction between the two aspects. Our well-formed components guarantee a set\nof properties ensuring that the deployed component system has a correct\narchitecture and can run safely. Finally, those definitions constitute the\nformal basis for our Eclipse-based environment for the development and\nspecification of component-based applications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:15:21 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Henrio", "Ludovic", "", "Univ. of Nice Sophia Antipolis, CNRS, France"], ["Kulankhina", "Oleksandra", "", "NRIA Sophia Antipolis Mediterannee, Univ. of Nice\n  Sophia Antipolis, CNRS, France"], ["Liu", "Dongqian", "", "MoE Engineering Research\n  Center for Software and Hardware Co-design Technology and Application, ECNU,\n  China"], ["Madelaine", "Eric", "", "NRIA Sophia Antipolis Mediterannee, Univ. of Nice\n  Sophia Antipolis, CNRS, France"]]}, {"id": "1502.03543", "submitter": "Nithish Divakar Mr.", "authors": "Nithish Divakar", "title": "Primal Dual Affine Scaling on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an implementation of Primal-Dual Affine scaling method to\nsolve linear optimization problem on GPU based systems. Strategies to convert\nthe system generated by complementary slackness theorem into a symmetric system\nare given. A new CUDA friendly technique to solve the resulting symmetric\npositive definite subsystem is also developed. Various strategies to reduce the\nmemory transfer and storage requirements were also explored.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 05:27:30 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Divakar", "Nithish", ""]]}, {"id": "1502.03645", "submitter": "Andreas Kreienbuehl", "authors": "Andreas Kreienbuehl, Arne Naegel, Daniel Ruprecht, Robert Speck,\n  Gabriel Wittum, and Rolf Krause", "title": "Numerical simulation of skin transport using Parareal", "comments": "11 pages, 8 figures", "journal-ref": "Computing and Visualization in Science 17(2), pp. 99-108, 2015", "doi": "10.1007/s00791-015-0246-y", "report-no": null, "categories": "cs.CE cs.DC cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-silico investigation of skin permeation is an important but also\ncomputationally demanding problem. To resolve all scales involved in full\ndetail will not only require exascale computing capacities but also suitable\nparallel algorithms. This article investigates the applicability of the\ntime-parallel Parareal algorithm to a brick and mortar setup, a precursory\nproblem to skin permeation. The C++ library Lib4PrM implementing Parareal is\ncombined with the UG4 simulation framework, which provides the spatial\ndiscretization and parallelization. The combination's performance is studied\nwith respect to convergence and speedup. It is confirmed that anisotropies in\nthe domain and jumps in diffusion coefficients only have a minor impact on\nParareal's convergence. The influence of load imbalances in time due to\ndifferences in number of iterations required by the spatial solver as well as\nspatio-temporal weak scaling is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:21:09 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 13:23:04 GMT"}], "update_date": "2015-10-19", "authors_parsed": [["Kreienbuehl", "Andreas", ""], ["Naegel", "Arne", ""], ["Ruprecht", "Daniel", ""], ["Speck", "Robert", ""], ["Wittum", "Gabriel", ""], ["Krause", "Rolf", ""]]}, {"id": "1502.03942", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Lorenz H\\\"ubschle-Schneider and Peter Sanders and Ingo M\\\"uller", "title": "Communication Efficient Algorithms for Top-k Selection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present scalable parallel algorithms with sublinear per-processor\ncommunication volume and low latency for several fundamental problems related\nto finding the most relevant elements in a set, for various notions of\nrelevance: We begin with the classical selection problem with unsorted input.\nWe present generalizations with locally sorted inputs, dynamic content\n(bulk-parallel priority queues), and multiple criteria. Then we move on to\nfinding frequent objects and top-k sum aggregation. Since it is unavoidable\nthat the output of these algorithms might be unevenly distributed over the\nprocessors, we also explain how to redistribute this data with minimal\ncommunication.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 11:01:29 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 15:28:27 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["H\u00fcbschle-Schneider", "Lorenz", ""], ["Sanders", "Peter", ""], ["M\u00fcller", "Ingo", ""]]}, {"id": "1502.03971", "submitter": "Noy Rotbart", "authors": "Casper Petersen, Noy Rotbart, Jakob Grue Simonsen and Christian\n  Wulff-Nilsen", "title": "Near-optimal adjacency labeling scheme for power-law graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adjacency labeling scheme is a method that assigns labels to the vertices\nof a graph such that adjacency between vertices can be inferred directly from\nthe assigned label, without using a centralized data structure. We devise\nadjacency labeling schemes for the family of power-law graphs. This family that\nhas been used to model many types of networks, e.g. the Internet AS-level\ngraph. Furthermore, we prove an almost matching lower bound for this family. We\nalso provide an asymptotically near- optimal labeling scheme for sparse graphs.\nFinally, we validate the efficiency of our labeling scheme by an experimental\nevaluation using both synthetic data and real-world networks of up to hundreds\nof thousands of vertices.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 13:02:08 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Petersen", "Casper", ""], ["Rotbart", "Noy", ""], ["Simonsen", "Jakob Grue", ""], ["Wulff-Nilsen", "Christian", ""]]}, {"id": "1502.04025", "submitter": "Tilo Wettig", "authors": "Paul Arts, Jacques Bloch, Peter Georg, Benjamin Glaessle, Simon\n  Heybrock, Yu Komatsubara, Robert Lohmayer, Simon Mages, Bernhard Mendl, Nils\n  Meyer, Alessio Parcianello, Dirk Pleiter, Florian Rappl, Mauro Rossi, Stefan\n  Solbrig, Giampietro Tecchiolli, Tilo Wettig, Gianpaolo Zanier", "title": "QPACE 2 and Domain Decomposition on the Intel Xeon Phi", "comments": "plenary talk at Lattice 2014, to appear in the conference proceedings\n  PoS(LATTICE2014), 15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC hep-lat physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of QPACE 2, which is a custom-designed supercomputer\nbased on Intel Xeon Phi processors, developed in a collaboration of Regensburg\nUniversity and Eurotech. We give some general recommendations for how to write\nhigh-performance code for the Xeon Phi and then discuss our implementation of a\ndomain-decomposition-based solver and present a number of benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 15:23:52 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Arts", "Paul", ""], ["Bloch", "Jacques", ""], ["Georg", "Peter", ""], ["Glaessle", "Benjamin", ""], ["Heybrock", "Simon", ""], ["Komatsubara", "Yu", ""], ["Lohmayer", "Robert", ""], ["Mages", "Simon", ""], ["Mendl", "Bernhard", ""], ["Meyer", "Nils", ""], ["Parcianello", "Alessio", ""], ["Pleiter", "Dirk", ""], ["Rappl", "Florian", ""], ["Rossi", "Mauro", ""], ["Solbrig", "Stefan", ""], ["Tecchiolli", "Giampietro", ""], ["Wettig", "Tilo", ""], ["Zanier", "Gianpaolo", ""]]}, {"id": "1502.04246", "submitter": "David Doty", "authors": "David Doty, David Soloveichik", "title": "Stable Leader Election in Population Protocols Requires Linear Time", "comments": "accepted to Distributed Computing special issue of invited papers\n  from DISC 2015; significantly revised proof structure and intuitive\n  explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CC q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A population protocol *stably elects a leader* if, for all $n$, starting from\nan initial configuration with $n$ agents each in an identical state, with\nprobability 1 it reaches a configuration $\\mathbf{y}$ that is correct (exactly\none agent is in a special leader state $\\ell$) and stable (every configuration\nreachable from $\\mathbf{y}$ also has a single agent in state $\\ell$). We show\nthat any population protocol that stably elects a leader requires $\\Omega(n)$\nexpected \"parallel time\" --- $\\Omega(n^2)$ expected total pairwise interactions\n--- to reach such a stable configuration. Our result also informs the\nunderstanding of the time complexity of chemical self-organization by showing\nan essential difficulty in generating exact quantities of molecular species\nquickly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 21:17:03 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 03:36:53 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2015 18:55:40 GMT"}, {"version": "v4", "created": "Sat, 20 Aug 2016 16:43:30 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Doty", "David", ""], ["Soloveichik", "David", ""]]}, {"id": "1502.04281", "submitter": "Michael Borokhovich", "authors": "Ioannis Mitliagkas, Michael Borokhovich, Alexandros G. Dimakis,\n  Constantine Caramanis", "title": "FrogWild! -- Fast PageRank Approximations on Graph Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.IT cs.SI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose FrogWild, a novel algorithm for fast approximation of high\nPageRank vertices, geared towards reducing network costs of running traditional\nPageRank algorithms. Our algorithm can be seen as a quantized version of power\niteration that performs multiple parallel random walks over a directed graph.\nOne important innovation is that we introduce a modification to the GraphLab\nframework that only partially synchronizes mirror vertices. This partial\nsynchronization vastly reduces the network traffic generated by traditional\nPageRank algorithms, thus greatly reducing the per-iteration cost of PageRank.\nOn the other hand, this partial synchronization also creates dependencies\nbetween the random walks used to estimate PageRank. Our main theoretical\ninnovation is the analysis of the correlations introduced by this partial\nsynchronization process and a bound establishing that our approximation is\nclose to the true PageRank vector.\n  We implement our algorithm in GraphLab and compare it against the default\nPageRank implementation. We show that our algorithm is very fast, performing\neach iteration in less than one second on the Twitter graph and can be up to 7x\nfaster compared to the standard GraphLab PageRank implementation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 04:33:00 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Borokhovich", "Michael", ""], ["Dimakis", "Alexandros G.", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1502.04395", "submitter": "Lewis Tseng", "authors": "Lewis Tseng, Alec Benzer and Nitin H. Vaidya", "title": "Application-Aware Consistency: An Application to Social Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work weakens well-known consistency models using graphs that capture\napplications' characteristics. The weakened models not only respect application\nsemantic, but also yield a performance benefit. We introduce a notion of\ndependency graphs, which specify relations between events that are important\nwith respect to application semantic, and then weaken traditional consistency\nmodels (e.g., causal consistency) using these graphs. Particularly, we consider\ntwo types of graphs: intra-process and inter-process dependency graphs, where\nintra-process dependency graphs specify how events in a single process are\nrelated, and inter-process dependency graphs specify how events across multiple\nprocesses are related. Then, based on these two types of graphs, we define new\nconsistency model, namely {\\em application-aware} consistency. We also discuss\nwhy such application-aware consistency can be useful in social network\napplications.\n  This is a work in progress, and we present early ideas regarding\napplication-aware consistency here.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 00:36:47 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 18:25:28 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Tseng", "Lewis", ""], ["Benzer", "Alec", ""], ["Vaidya", "Nitin H.", ""]]}, {"id": "1502.04496", "submitter": "Christian Cachin", "authors": "Marcus Brandenburger, Christian Cachin, Nikola Kne\\v{z}evi\\'c", "title": "Don't Trust the Cloud, Verify: Integrity and Consistency for Cloud\n  Object Stores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services have turned remote computation into a commodity and enable\nconvenient online collaboration. However, they require that clients fully trust\nthe service provider in terms of confidentiality, integrity, and availability.\nTowards reducing this dependency, this paper introduces a protocol for\nverification of integrity and consistency for cloud object storage (VICOS),\nwhich enables a group of mutually trusting clients to detect data-integrity and\nconsistency violations for a cloud object-storage service. It aims at services\nwhere multiple clients cooperate on data stored remotely on a potentially\nmisbehaving service. VICOS enforces the consistency notion of\nfork-linearizability, supports wait-free client semantics for most operations,\nand reduces the computation and communication overhead compared to previous\nprotocols. VICOS is based in a generic way on any authenticated data structure.\nMoreover, its operations cover the hierarchical name space of a cloud object\nstore, supporting a real-world interface and not only a simplistic abstraction.\nA prototype of VICOS that works with the key-value store interface of commodity\ncloud storage services has been implemented, and an evaluation demonstrates its\nadvantage compared to existing systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:15:03 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 14:49:35 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Brandenburger", "Marcus", ""], ["Cachin", "Christian", ""], ["Kne\u017eevi\u0107", "Nikola", ""]]}, {"id": "1502.04511", "submitter": "Juho Hirvonen", "authors": "Laurent Feuilloley, Juho Hirvonen and Jukka Suomela", "title": "Locally Optimal Load Balancing", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies distributed algorithms for locally optimal load-balancing:\nWe are given a graph of maximum degree $\\Delta$, and each node has up to $L$\nunits of load. The task is to distribute the load more evenly so that the loads\nof adjacent nodes differ by at most $1$.\n  If the graph is a path ($\\Delta = 2$), it is easy to solve the fractional\nversion of the problem in $O(L)$ communication rounds, independently of the\nnumber of nodes. We show that this is tight, and we show that it is possible to\nsolve also the discrete version of the problem in $O(L)$ rounds in paths.\n  For the general case ($\\Delta > 2$), we show that fractional load balancing\ncan be solved in $\\operatorname{poly}(L,\\Delta)$ rounds and discrete load\nbalancing in $f(L,\\Delta)$ rounds for some function $f$, independently of the\nnumber of nodes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 12:32:15 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Feuilloley", "Laurent", ""], ["Hirvonen", "Juho", ""], ["Suomela", "Jukka", ""]]}, {"id": "1502.04551", "submitter": "Micha{\\l}  Karpi\\'nski", "authors": "Micha{\\l} Karpi\\'nski, Marek Piotr\\'ow", "title": "Smaller Selection Networks for Cardinality Constraints Encoding", "comments": "Extended version of the paper sent to CP2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection comparator networks have been studied for many years. Recently,\nthey have been successfully applied to encode cardinality constraints for\nSAT-solvers. To decrease the size of generated formula there is a need for\nconstructions of selection networks that can be efficiently generated and\nproduce networks of small sizes for the practical range of their two\nparameters: n - the number of inputs (boolean variables) and k - the number of\nselected items (a cardinality bound). In this paper we give and analyze a new\nconstruction of smaller selection networks that are based on the pairwise\nselection networks introduced by Codish and Zanon-Ivry. We prove also that\nstandard encodings of cardinality constraints with selection networks preserve\narc-consistency.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 14:29:01 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Karpi\u0144ski", "Micha\u0142", ""], ["Piotr\u00f3w", "Marek", ""]]}, {"id": "1502.04798", "submitter": "Nguyen Cuong Ha Huy", "authors": "Ha Huy Cuong Nguyen, Van Thuan Dang, Van Son Le", "title": "Technical solutions to resources allocation for distributed virtual\n  machine systems", "comments": "International Journal of Computer Science and Telecommunications\n  www.ijcst.org, International Journal of Computer Science and Information\n  Security - 2015", "journal-ref": null, "doi": null, "report-no": "ID: 30011511", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual machine is built on group of real servers which are scattered\nglobally and connect together through the telecommunications systems, it has an\nincreasingly important role in the operation, providing the ability to exploit\nvirtual resources. The latest technique helps to use computing resources more\neffectively and has many benefits, such as cost reduction of power, cooling\nand, hence, contributes to the Green Computing. To ensure the supply of these\nresources to demand processes correctly and promptly, avoiding any duplication\nor conflict, especially remote resources, it is necessary to study and propose\na reliable solution appropriate to be the foundation for internal control\nsystems in the cloud. In the scope of this paper, we find a way to produce\nefficient distributed resources which emphasizes solutions preventing deadlock\nand proposing methods to avoid resource shortage issue. With this approach, the\noutcome result is the checklist of re-sources state which has the possibility\nof deadlock and lack of resources, by sending messages to the servers, the\nserver would know the situation and have corresponding reaction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:28:30 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Nguyen", "Ha Huy Cuong", ""], ["Dang", "Van Thuan", ""], ["Le", "Van Son", ""]]}, {"id": "1502.04908", "submitter": "Srivatsan Ravi Mr", "authors": "Petr Kuznetsov and Srivatsan Ravi", "title": "Progressive Transactional Memory in Time and Space", "comments": "Model of Transactional Memory identical with arXiv:1407.6876,\n  arXiv:1502.02725", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory (TM) allows concurrent processes to organize sequences\nof operations on shared \\emph{data items} into atomic transactions. A\ntransaction may commit, in which case it appears to have executed sequentially\nor it may \\emph{abort}, in which case no data item is updated.\n  The TM programming paradigm emerged as an alternative to conventional\nfine-grained locking techniques, offering ease of programming and\ncompositionality. Though typically themselves implemented using locks, TMs hide\nthe inherent issues of lock-based synchronization behind a nice transactional\nprogramming interface.\n  In this paper, we explore inherent time and space complexity of lock-based\nTMs, with a focus of the most popular class of \\emph{progressive} lock-based\nTMs. We derive that a progressive TM might enforce a read-only transaction to\nperform a quadratic (in the number of the data items it reads) number of steps\nand access a linear number of distinct memory locations, closing the question\nof inherent cost of \\emph{read validation} in TMs. We then show that the total\nnumber of \\emph{remote memory references} (RMRs) that take place in an\nexecution of a progressive TM in which $n$ concurrent processes perform\ntransactions on a single data item might reach $\\Omega(n \\log n)$, which\nappears to be the first RMR complexity lower bound for transactional memory.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 15:01:06 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 03:15:27 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Kuznetsov", "Petr", ""], ["Ravi", "Srivatsan", ""]]}, {"id": "1502.04963", "submitter": "Joel Rybicki", "authors": "Joel Rybicki, Jukka Suomela", "title": "Exact bounds for distributed graph colouring", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove exact bounds on the time complexity of distributed graph colouring.\nIf we are given a directed path that is properly coloured with $n$ colours, by\nprior work it is known that we can find a proper 3-colouring in $\\frac{1}{2}\n\\log^*(n) \\pm O(1)$ communication rounds. We close the gap between upper and\nlower bounds: we show that for infinitely many $n$ the time complexity is\nprecisely $\\frac{1}{2} \\log^* n$ communication rounds.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 17:23:35 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 17:40:24 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Rybicki", "Joel", ""], ["Suomela", "Jukka", ""]]}, {"id": "1502.05110", "submitter": "Chuan Qin", "authors": "Mingqiang Li, Chuan Qin, Patrick P. C. Lee", "title": "CDStore: Toward Reliable, Secure, and Cost-Efficient Cloud Storage via\n  Convergent Dispersal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CDStore, which disperses users' backup data across multiple clouds\nand provides a unified multi-cloud storage solution with reliability, security,\nand cost-efficiency guarantees. CDStore builds on an augmented secret sharing\nscheme called convergent dispersal, which supports deduplication by using\ndeterministic content-derived hashes as inputs to secret sharing. We present\nthe design of CDStore, and in particular, describe how it combines convergent\ndispersal with two-stage deduplication to achieve both bandwidth and storage\nsavings and be robust against side-channel attacks. We evaluate the performance\nof our CDStore prototype using real-world workloads on LAN and commercial cloud\ntestbeds. Our cost analysis also demonstrates that CDStore achieves a monetary\ncost saving of 70% over a baseline cloud storage solution using\nstate-of-the-art secret sharing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 04:03:11 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 08:13:30 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Li", "Mingqiang", ""], ["Qin", "Chuan", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1502.05179", "submitter": "Andrey Shchurov", "authors": "Andrey A. Shchurov, Radek Marik", "title": "Dependability Tests Selection Based on the Concept of Layered Networks", "comments": "10 pages, 3 figures, 5 tables", "journal-ref": "International Journal of Scientific and Engineering Research\n  (IJSER) V6(1):1165-1174, January 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the consequences of failure and downtime of distributed systems\nhave become more and more severe. As an obvious solution, these systems\nincorporate protection mechanisms to tolerate faults that could cause systems\nfailures and system dependability must be validated to ensure that protection\nmechanisms have been implemented correctly and the system will provide the\ndesired level of reliable service. This paper presents a systematic approach\nfor identifying (1) characteristic sets of critical system elements for\ndependability testing (single points of failure and recovery groups) based on\nthe concept of layered networks; and (2) the most important combinations of\ncomponents from each recovery group based on a combinatorial technique. Based\non these combinations, we determine a set of test templates to be performed to\ndemonstrate system dependability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 11:13:14 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Shchurov", "Andrey A.", ""], ["Marik", "Radek", ""]]}, {"id": "1502.05183", "submitter": "Ioannis Marcoullis", "authors": "Shlomi Dolev and Chryssis Georgiou and Ioannis Marcoullis and Elad\n  Michael Schiller", "title": "Practically-Self-Stabilizing Virtual Synchrony", "comments": null, "journal-ref": "Journal of Computer and System Sciences 2018,\n  https://doi.org/10.1016/j.jcss.2018.04.003", "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual synchrony is an important abstraction that is proven to be extremely\nuseful when implemented over asynchronous, typically large, message-passing\ndistributed systems. Fault tolerant design is a key criterion for the success\nof such implementations. This is because large distributed systems can be\nhighly available as long as they do not depend on the full operational status\nof every system participant. Namely, they employ redundancy in numbers to\novercome non-optimal behavior of participants and to gain global robustness and\nhigh availability.\n  Self-stabilizing systems can tolerate transient faults that drive the system\nto an arbitrary unpredicted configuration. Such systems automatically regain\nconsistency from any such arbitrary configuration, and then produce the desired\nsystem behavior. Practically self-stabilizing systems ensure the desired system\nbehavior for practically infinite number of successive steps e.g., $2^{64}$\nsteps.\n  We present the first practically self-stabilizing virtual synchrony\nalgorithm. The algorithm is a combination of several new techniques that may be\nof independent interest. In particular, we present a new counter algorithm that\nestablishes an efficient practically unbounded counter, that in turn can be\ndirectly used to implement a self-stabilizing Multiple-Writer Multiple-Reader\n(MWMR) register emulation. Other components include self-stabilizing group\nmembership, self-stabilizing multicast, and self-stabilizing emulation of\nreplicated state machine. As we base the replicated state machine\nimplementation on virtual synchrony, rather than consensus, the system\nprogresses in more extreme asynchronous executions in relation to\nconsensus-based replicated state machine.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 11:36:18 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 16:41:32 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 09:37:02 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Dolev", "Shlomi", ""], ["Georgiou", "Chryssis", ""], ["Marcoullis", "Ioannis", ""], ["Schiller", "Elad Michael", ""]]}, {"id": "1502.05730", "submitter": "Evgeny Nikulchev", "authors": "Evgeniy Pluzhnik, Oleg Lukyanchikov, Evgeny Nikulchev, Simon Payain", "title": "Designing Applications with Distributed Databases in a Hybrid Cloud", "comments": "in WIT Transactions of Information and Communication Technologies,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing applications for use in a hybrid cloud has many features. These\ninclude dynamic virtualization management and an unknown route switching\ncustomers. This makes it impossible to evaluate the query and hence the optimal\ndistribution of data. In this paper, we formulate the main challenges of\ndesigning and simulation offer installation for processing.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 21:25:11 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Pluzhnik", "Evgeniy", ""], ["Lukyanchikov", "Oleg", ""], ["Nikulchev", "Evgeny", ""], ["Payain", "Simon", ""]]}, {"id": "1502.05745", "submitter": "Rati Gelashvili", "authors": "Dan Alistarh, Rati Gelashvili", "title": "Polylogarithmic-Time Leader Election in Population Protocols Using\n  Polylogarithmic States", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population protocols are networks of finite-state agents, interacting\nrandomly, and updating their states using simple rules. Despite their extreme\nsimplicity, these systems have been shown to cooperatively perform complex\ncomputational tasks, such as simulating register machines to compute standard\narithmetic functions. The election of a unique leader agent is a key\nrequirement in such computational constructions. Yet, the fastest currently\nknown population protocol for electing a leader only has linear stabilization\ntime, and, it has recently been shown that no population protocol using a\nconstant number of states per node may overcome this linear bound.\n  In this paper, we give the first population protocol for leader election with\npolylogarithmic stabilization time, using polylogarithmic memory states per\nnode. The protocol structure is quite simple: each node has an associated\nvalue, and is either a leader (still in contention) or a minion (following some\nleader). A leader keeps incrementing its value and \"defeats\" other leaders in\none-to-one interactions, and will drop from contention and become a minion if\nit meets a leader with higher value. Importantly, a leader also drops out if it\nmeets a minion with higher absolute value. While these rules are quite simple,\nthe proof that this algorithm achieves polylogarithmic stabilization time is\nnon-trivial. In particular, the argument combines careful use of concentration\ninequalities with anti-concentration bounds, showing that the leaders' values\nbecome spread apart as the execution progresses, which in turn implies that\nstraggling leaders get quickly eliminated. We complement our analysis with\nempirical results, showing that our protocol stabilizes extremely fast, even\nfor large network sizes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 23:09:20 GMT"}, {"version": "v2", "created": "Sat, 14 Jan 2017 17:11:53 GMT"}, {"version": "v3", "created": "Sun, 16 Apr 2017 17:17:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Alistarh", "Dan", ""], ["Gelashvili", "Rati", ""]]}, {"id": "1502.05786", "submitter": "Arpan Mukhopadhyay", "authors": "Arpan Mukhopadhyay, A. Karthik, Ravi R. Mazumdar", "title": "Randomized Assignment of Jobs to Servers in Heterogeneous Clusters of\n  Shared Servers for Low Delay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.SY math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the job assignment problem in a multi-server system consisting of\n$N$ parallel processor sharing servers, categorized into $M$ ($\\ll N$)\ndifferent types according to their processing capacity or speed. Jobs of random\nsizes arrive at the system according to a Poisson process with rate $N\n\\lambda$. Upon each arrival, a small number of servers from each type is\nsampled uniformly at random. The job is then assigned to one of the sampled\nservers based on a selection rule. We propose two schemes, each corresponding\nto a specific selection rule that aims at reducing the mean sojourn time of\njobs in the system.\n  We first show that both methods achieve the maximal stability region. We then\nanalyze the system operating under the proposed schemes as $N \\to \\infty$ which\ncorresponds to the mean field. Our results show that asymptotic independence\namong servers holds even when $M$ is finite and exchangeability holds only\nwithin servers of the same type. We further establish the existence and\nuniqueness of stationary solution of the mean field and show that the tail\ndistribution of server occupancy decays doubly exponentially for each server\ntype. When the estimates of arrival rates are not available, the proposed\nschemes offer simpler alternatives to achieving lower mean sojourn time of\njobs, as shown by our numerical studies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 06:51:01 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Mukhopadhyay", "Arpan", ""], ["Karthik", "A.", ""], ["Mazumdar", "Ravi R.", ""]]}, {"id": "1502.05831", "submitter": "Shengyun Liu", "authors": "Shengyun Liu, Paolo Viotti, Christian Cachin, Vivien Qu\\'ema and Marko\n  Vukoli\\'c", "title": "XFT: Practical Fault Tolerance Beyond Crashes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite years of intensive research, Byzantine fault-tolerant (BFT) systems\nhave not yet been adopted in practice. This is due to additional cost of BFT in\nterms of resources, protocol complexity and performance, compared with crash\nfault-tolerance (CFT). This overhead of BFT comes from the assumption of a\npowerful adversary that can fully control not only the Byzantine faulty\nmachines, but at the same time also the message delivery schedule across the\nentire network, effectively inducing communication asynchrony and partitioning\notherwise correct machines at will. To many practitioners, however, such strong\nattacks appear irrelevant.\n  In this paper, we introduce cross fault tolerance or XFT, a novel approach to\nbuilding reliable and secure distributed systems and apply it to the classical\nstate-machine replication (SMR) problem. In short, an XFT SMR protocol provides\nthe reliability guarantees of widely used asynchronous CFT SMR protocols such\nas Paxos and Raft, but also tolerates Byzantine faults in combination with\nnetwork asynchrony, as long as a majority of replicas are correct and\ncommunicate synchronously. This allows the development of XFT systems at the\nprice of CFT (already paid for in practice), yet with strictly stronger\nresilience than CFT --- sometimes even stronger than BFT itself.\n  As a showcase for XFT, we present XPaxos, the first XFT SMR protocol, and\ndeploy it in a geo-replicated setting. Although it offers much stronger\nresilience than CFT SMR at no extra resource cost, the performance of XPaxos\nmatches that of the state-of-the-art CFT protocols.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:15:45 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2015 07:40:58 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 17:27:20 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Liu", "Shengyun", ""], ["Viotti", "Paolo", ""], ["Cachin", "Christian", ""], ["Qu\u00e9ma", "Vivien", ""], ["Vukoli\u0107", "Marko", ""]]}, {"id": "1502.05968", "submitter": "Javad Ghaderi", "authors": "Javad Ghaderi, Sanjay Shakkottai, R Srikant", "title": "Scheduling Storms and Streams in the Cloud", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by emerging big streaming data processing paradigms (e.g., Twitter\nStorm, Streaming MapReduce), we investigate the problem of scheduling graphs\nover a large cluster of servers. Each graph is a job, where nodes represent\ncompute tasks and edges indicate data-flows between these compute tasks. Jobs\n(graphs) arrive randomly over time, and upon completion, leave the system. When\na job arrives, the scheduler needs to partition the graph and distribute it\nover the servers to satisfy load balancing and cost considerations.\nSpecifically, neighboring compute tasks in the graph that are mapped to\ndifferent servers incur load on the network; thus a mapping of the jobs among\nthe servers incurs a cost that is proportional to the number of \"broken edges\".\nWe propose a low complexity randomized scheduling algorithm that, without\nservice preemptions, stabilizes the system with graph arrivals/departures; more\nimportantly, it allows a smooth trade-off between minimizing average\npartitioning cost and average queue lengths. Interestingly, to avoid service\npreemptions, our approach does not rely on a Gibbs sampler; instead, we show\nthat the corresponding limiting invariant measure has an interpretation\nstemming from a loss system.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 18:45:45 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Ghaderi", "Javad", ""], ["Shakkottai", "Sanjay", ""], ["Srikant", "R", ""]]}, {"id": "1502.06086", "submitter": "Suyash Gupta", "authors": "Suyash Gupta, Rahul Shrivastava and V. Krishna Nandivada", "title": "DCAFE: Dynamic load-balanced loop Chunking & Aggressive Finish\n  Elimination for Recursive Task Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two symbiotic optimizations to optimize recursive\ntask parallel (RTP) programs by reducing the task creation and termination\noverheads. Our first optimization Aggressive Finish-Elimination (AFE) helps\nreduce the redundant join operations to a large extent. The second optimization\nDynamic Load-Balanced loop Chunking (DLBC) extends the prior work on loop\nchunking to decide on the number of parallel tasks based on the number of\navailable worker threads, at runtime. Further, we discuss the impact of\nexceptions on our optimizations and extend them to handle RTP programs that may\nthrow exceptions. We implemented DCAFE (= DLBC+AFE) in the X10v2.3 compiler and\ntested it over a set of benchmark kernels on two different hardwares (a 16-core\nIntel system and a 64-core AMD system). With respect to the base X10 compiler\nextended with loop-chunking of Nandivada et al [Nandivada et\nal.(2013)Nandivada, Shirako, Zhao, and Sarkar](LC), DCAFE achieved a geometric\nmean speed up of 5.75x and 4.16x on the Intel and AMD system, respectively. We\nalso present an evaluation with respect to the energy consumption on the Intel\nsystem and show that on average, compared to the LC versions, the DCAFE\nversions consume 71.2% less energy.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 08:18:43 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Gupta", "Suyash", ""], ["Shrivastava", "Rahul", ""], ["Nandivada", "V. Krishna", ""]]}, {"id": "1502.06381", "submitter": "Markus Zimmermann", "authors": "Markus Zimmermann (for the ALICE collaboration)", "title": "The ALICE analysis train system", "comments": "5 pages, 3 figures, proceedings of the conference ACAT 2014 (Advanced\n  Computing and Analysis Techniques in physics), Prague, Czech Republic,\n  September 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the ALICE experiment hundreds of users are analyzing big datasets on a\nGrid system. High throughput and short turn-around times are achieved by a\ncentralized system called the LEGO trains. This system combines analysis from\ndifferent users in so-called analysis trains which are then executed within the\nsame Grid jobs thereby reducing the number of times the data needs to be read\nfrom the storage systems. The centralized trains improve the performance, the\nusability for users and the bookkeeping in comparison to single user analysis.\nThe train system builds upon the already existing ALICE tools, i.e. the\nanalysis framework as well as the Grid submission and monitoring\ninfrastructure. The entry point to the train system is a web interface which is\nused to configure the analysis and the desired datasets as well as to test and\nsubmit the train. Several measures have been implemented to reduce the time a\ntrain needs to finish and to increase the CPU efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 10:53:06 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zimmermann", "Markus", "", "for the ALICE collaboration"]]}, {"id": "1502.06461", "submitter": "Pamela Zave", "authors": "Pamela Zave", "title": "How to Make Chord Correct", "comments": "5 figures; 11 two-column pages including references and appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Chord distributed hash table (DHT) is well-known and frequently used to\nimplement peer-to-peer systems. Chord peers find other peers, and access their\ndata, through a ring-shaped pointer structure in a large identifier space.\nDespite claims of proven correctness, i.e., eventual reachability, previous\nwork has shown that the Chord ring-maintenance protocol is not correct under\nits original operating assumptions. It has not, however, discovered whether\nChord could be made correct with reasonable operating assumptions. The\ncontribution of this paper is to provide the first specification of correct\noperations and initialization for Chord, an inductive invariant that is\nnecessary and sufficient to support a proof of correctness, and the proof\nitself. Most of the proof is carried out by automated analysis of an Alloy\nmodel. The inductive invariant reflects the fact that a Chord network must have\na minimum ring size (the minimum being the length of successor lists plus one)\nto be correct. The invariant relies on an assumption that there is a stable\nbase, of the minimum size, of permanent ring members. Because a stable base has\nonly a few members and a Chord network can have millions, we learn that the\nobstacles to provable correctness are anomalies in small networks, and that a\nstable base need not be maintained once a Chord network grows large.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:40:18 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 15:18:30 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Zave", "Pamela", ""]]}, {"id": "1502.06471", "submitter": "Siamak Taati", "authors": "Siamak Taati", "title": "Restricted density classification in one dimension", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DC nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The density classification task is to determine which of the symbols\nappearing in an array has the majority. A cellular automaton solving this task\nis required to converge to a uniform configuration with the majority symbol at\neach site. It is not known whether a one-dimensional cellular automaton with\nbinary alphabet can classify all Bernoulli random configurations almost surely\naccording to their densities. We show that any cellular automaton that washes\nout finite islands in linear time classifies all Bernoulli random\nconfigurations with parameters close to 0 or 1 almost surely correctly. The\nproof is a direct application of a \"percolation\" argument which goes back to\nGacs (1986).\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 15:51:17 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 14:29:33 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Taati", "Siamak", ""]]}, {"id": "1502.06564", "submitter": "Raul Isea", "authors": "Raul Isea, Esther Montes, Antonio J. Rubio-Montero and Rafael Mayo", "title": "Challenges and characterization of a Biological system on Grid by means\n  of the PhyloGrid application", "comments": "8 pages, 3 figures, appears in Proceedings of the First EELA-2\n  Conference, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this work we present a new application that is being developed. PhyloGrid\nis able to perform large-scale phylogenetic calculations as those that have\nbeen made for estimating the phylogeny of all the sequences already stored in\nthe public NCBI database. The further analysis has been focused on checking the\norigin of the HIV-1 disease by means of a huge number of sequences that sum up\nto 2900 taxa. Such a study has been able to be done by the implementation of a\nworkflow in Taverna.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 13:19:29 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Isea", "Raul", ""], ["Montes", "Esther", ""], ["Rubio-Montero", "Antonio J.", ""], ["Mayo", "Rafael", ""]]}, {"id": "1502.06733", "submitter": "Amina Guermouche", "authors": "Amina Guermouche (UVSQ), Nicolas Triquenaux (UVSQ), Benoit Pradelle\n  (UVSQ), William Jalby (UVSQ)", "title": "Minimizing Energy Consumption of MPI Programs in Realistic Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic voltage and frequency scaling proves to be an efficient way of\nreducing energy consumption of servers. Energy savings are typically achieved\nby setting a well-chosen frequency during some program phases. However,\ndetermining suitable program phases and their associated optimal frequencies is\na complex problem. Moreover, hardware is constrained by non negligible\nfrequency transition latencies. Thus, various heuristics were proposed to\ndetermine and apply frequencies, but evaluating their efficiency remains an\nissue. In this paper, we translate the energy minimization problem into a mixed\ninteger program that specifically models most current hardware limitations. The\nproblem solution then estimates the minimal energy consumption and the\nassociated frequency schedule. The paper provides two different formulations\nand a discussion on the feasibility of each of them on realistic applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 09:55:48 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 07:28:58 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Guermouche", "Amina", "", "UVSQ"], ["Triquenaux", "Nicolas", "", "UVSQ"], ["Pradelle", "Benoit", "", "UVSQ"], ["Jalby", "William", "", "UVSQ"]]}, {"id": "1502.07118", "submitter": "Andreas Holzer", "authors": "Andreas Haas, Thomas A. Henzinger, Andreas Holzer, Christoph M.\n  Kirsch, Michael Lippautz, Hannes Payer, Ali Sezgin, Ana Sokolova, Helmut\n  Veith", "title": "Local Linearizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantics of concurrent data structures is usually given by a sequential\nspecification and a consistency condition. Linearizability is the most popular\nconsistency condition due to its simplicity and general applicability.\nNevertheless, for applications that do not require all guarantees offered by\nlinearizability, recent research has focused on improving performance and\nscalability of concurrent data structures by relaxing their semantics.\n  In this paper, we present local linearizability, a relaxed consistency\ncondition that is applicable to container-type concurrent data structures like\npools, queues, and stacks. While linearizability requires that the effect of\neach operation is observed by all threads at the same time, local\nlinearizability only requires that for each thread T, the effects of its local\ninsertion operations and the effects of those removal operations that remove\nvalues inserted by T are observed by all threads at the same time. We\ninvestigate theoretical and practical properties of local linearizability and\nits relationship to many existing consistency conditions. We present a generic\nimplementation method for locally linearizable data structures that uses\nexisting linearizable data structures as building blocks. Our implementations\nshow performance and scalability improvements over the original building blocks\nand outperform the fastest existing container-type implementations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 10:40:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 07:40:31 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 05:24:41 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2016 15:39:28 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Haas", "Andreas", ""], ["Henzinger", "Thomas A.", ""], ["Holzer", "Andreas", ""], ["Kirsch", "Christoph M.", ""], ["Lippautz", "Michael", ""], ["Payer", "Hannes", ""], ["Sezgin", "Ali", ""], ["Sokolova", "Ana", ""], ["Veith", "Helmut", ""]]}, {"id": "1502.07169", "submitter": "Wolf Roediger", "authors": "Wolf Roediger, Tobias Muehlbauer, Alfons Kemper, Thomas Neumann", "title": "High-Speed Query Processing over High-Speed Networks", "comments": "12 pages, accepted at VLDB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern database clusters entail two levels of networks: connecting CPUs and\nNUMA regions inside a single server in the small and multiple servers in the\nlarge. The huge performance gap between these two types of networks used to\nslow down distributed query processing to such an extent that a cluster of\nmachines actually performed worse than a single many-core server. The increased\nmain-memory capacity of the cluster remained the sole benefit of such a\nscale-out.\n  The economic viability of high-speed interconnects such as InfiniBand has\nnarrowed this performance gap considerably. However, InfiniBand's higher\nnetwork bandwidth alone does not improve query performance as expected when the\ndistributed query engine is left unchanged. The scalability of distributed\nquery processing is impaired by TCP overheads, switch contention due to\nuncoordinated communication, and load imbalances resulting from the\ninflexibility of the classic exchange operator model. This paper presents the\nblueprint for a distributed query engine that addresses these problems by\nconsidering both levels of networks holistically. It consists of two parts:\nFirst, hybrid parallelism that distinguishes local and distributed parallelism\nfor better scalability in both the number of cores as well as servers. Second,\na novel communication multiplexer tailored for analytical database workloads\nusing remote direct memory access (RDMA) and low-latency network scheduling for\nhigh-speed communication with almost no CPU overhead. An extensive evaluation\nwithin the HyPer database system using the TPC-H benchmark shows that our\nholistic approach indeed enables high-speed query processing over high-speed\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 14:08:51 GMT"}, {"version": "v2", "created": "Thu, 7 May 2015 13:12:15 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2015 13:34:35 GMT"}, {"version": "v4", "created": "Mon, 2 Nov 2015 14:31:10 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Roediger", "Wolf", ""], ["Muehlbauer", "Tobias", ""], ["Kemper", "Alfons", ""], ["Neumann", "Thomas", ""]]}, {"id": "1502.07241", "submitter": "Frank Hannig", "authors": "Frank Hannig, Dietmar Fey, Anton Lokhmotov", "title": "Proceedings of the DATE Friday Workshop on Heterogeneous Architectures\n  and Design Methods for Embedded Image Systems (HIS 2015)", "comments": "Website of the workshop: https://www12.cs.fau.de/ws/his2015/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers accepted at the DATE Friday Workshop on\nHeterogeneous Architectures and Design Methods for Embedded Image Systems (HIS\n2015), held in Grenoble, France, March 13, 2015. HIS 2015 was co-located with\nthe Conference on Design, Automation and Test in Europe (DATE).\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 16:52:56 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 06:00:09 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Hannig", "Frank", ""], ["Fey", "Dietmar", ""], ["Lokhmotov", "Anton", ""]]}, {"id": "1502.07446", "submitter": "V\\'itor Schwambach", "authors": "V\\'itor Schwambach, S\\'ebastien Cleyet-Merle, Alain Issard, St\\'ephane\n  Mancini", "title": "Estimating the Potential Speedup of Computer Vision Applications on\n  Embedded Multiprocessors", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/01", "categories": "cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision applications constitute one of the key drivers for embedded\nmulticore architectures. Although the number of available cores is increasing\nin new architectures, designing an application to maximize the utilization of\nthe platform is still a challenge. In this sense, parallel performance\nprediction tools can aid developers in understanding the characteristics of an\napplication and finding the most adequate parallelization strategy. In this\nwork, we present a method for early parallel performance estimation on embedded\nmultiprocessors from sequential application traces. We describe its\nimplementation in Parana, a fast trace-driven simulator targeting OpenMP\napplications on the STMicroelectronics' STxP70 Application-Specific\nMultiprocessor (ASMP). Results for the FAST key point detector application show\nan error margin of less than 10% compared to the reference cycle-approximate\nsimulator, with lower modeling effort and up to 20x faster execution time.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:13:47 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Schwambach", "V\u00edtor", ""], ["Cleyet-Merle", "S\u00e9bastien", ""], ["Issard", "Alain", ""], ["Mancini", "St\u00e9phane", ""]]}, {"id": "1502.07447", "submitter": "Mehmet Ali Arslan", "authors": "Mehmet Ali Arslan, Flavius Gruian, Krzysztof Kuchcinski", "title": "A Comparative Study of Scheduling Techniques for Multimedia Applications\n  on SIMD Pipelines", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/02", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel architectures are essential in order to take advantage of the\nparallelism inherent in streaming applications. One particular branch of these\nemploy hardware SIMD pipelines. In this paper, we analyse several scheduling\ntechniques, namely ad hoc overlapped execution, modulo scheduling and modulo\nscheduling with unrolling, all of which aim to efficiently utilize the special\narchitecture design. Our investigation focuses on improving throughput while\nanalysing other metrics that are important for streaming applications, such as\nregister pressure, buffer sizes and code size. Through experiments conducted on\nseveral media benchmarks, we present and discuss trade-offs involved when\nselecting any one of these scheduling techniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:15:23 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Arslan", "Mehmet Ali", ""], ["Gruian", "Flavius", ""], ["Kuchcinski", "Krzysztof", ""]]}, {"id": "1502.07451", "submitter": "Hao Wu", "authors": "Hao Wu, Daniel Lohmann, Wolfgang Schr\\\"oder-Preikschat", "title": "A Graph-Partition-Based Scheduling Policy for Heterogeneous\n  Architectures", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/05", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to improve system performance efficiently, a number of systems\nchoose to equip multi-core and many-core processors (such as GPUs). Due to\ntheir discrete memory these heterogeneous architectures comprise a distributed\nsystem within a computer. A data-flow programming model is attractive in this\nsetting for its ease of expressing concurrency. Programmers only need to define\ntask dependencies without considering how to schedule them on the hardware.\nHowever, mapping the resulting task graph onto hardware efficiently remains a\nchallenge. In this paper, we propose a graph-partition scheduling policy for\nmapping data-flow workloads to heterogeneous hardware. According to our\nexperiments, our graph-partition-based scheduling achieves comparable\nperformance to conventional queue-base approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:19:32 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Wu", "Hao", ""], ["Lohmann", "Daniel", ""], ["Schr\u00f6der-Preikschat", "Wolfgang", ""]]}, {"id": "1502.07608", "submitter": "Jose Gracia", "authors": "Steffen Brinkmann, Jose Gracia", "title": "CppSs -- a C++ Library for Efficient Task Parallelism", "comments": "accepted for publication at INFOCOMP, work-in-progress track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the C++ library CppSs (C++ super-scalar), which provides efficient\ntask-parallelism without the need for special compilers or other software. Any\nC++ compiler that supports C++11 is sufficient. CppSs features different\ndirectionality clauses for defining data dependencies. While the variable\nargument lists of the taskified functions are evaluated at compile time, the\nresulting task dependencies are fixed by the runtime value of the arguments and\nare thus analysed at runtime. With CppSs, we provide task-parallelism using\nmerely native C++.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 15:56:50 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Brinkmann", "Steffen", ""], ["Gracia", "Jose", ""]]}]