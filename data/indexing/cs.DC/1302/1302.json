[{"id": "1302.0210", "submitter": "Ming-Hung Chen", "authors": "Ming-Hung Chen, Shi-Chen Wang, and Cheng-Fu Chou", "title": "Deadline is not Enough: How to Achieve Importance-aware Server-centric\n  Data Centers via a Cross Layer Approach", "comments": "This version is finished on 2012/7/30. New versions which include\n  improved rate-based importance-aware protocol and DCTCP-based\n  importance-aware protocol are considered to be submitted before release", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's datacenters face important challenges for providing low-latency\nhigh-quality interactive services to meet user's expectation. For improving the\napplication throughput, recent research works have embedded application\ndeadline information into design of network flow schedule to meet the latency\nrequirement. Here, arises a critical question: does application-level\nthroughput mean providing better quality service? We note that there are\nusually a set of semantic related responses (or flows) for answering a query;\nand, some responses are highly correlative with the query while others do not.\nThus, this observation motivates us to associate the importance of the contents\nwith the application flows (or responses) in order to enhance the service\nquality. We first model the application importance maximization problem in a\ngeneric network and in a server-centric network. Since both of them are too\ncomplicated to be deployed in the real world, we propose the importance-aware\ndelivery protocol, which is a distributed event-driven rate-based delivery\ncontrol protocol, for server-centric datacenter networks. The proposed protocol\nis able to make use of the multiple disjoin paths of server-centric network,\nand jointly consider flow importance, flow size, and deadline to maximize the\ngoodput of most-related semantic data of a query. Through real-data-based or\nsynthetic simulations, the results show that our proposed protocol\nsignificantly outperforms D3 and MPTCP in terms of the precision at K and the\nsum of application-level importance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 15:44:26 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Chen", "Ming-Hung", ""], ["Wang", "Shi-Chen", ""], ["Chou", "Cheng-Fu", ""]]}, {"id": "1302.0264", "submitter": "Mohsen Ghaffari", "authors": "Mohsen Ghaffari, Bernhard Haeupler, Majid Khabbazian", "title": "A Bound on the Throughput of Radio Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the well-studied radio network model: a synchronous model with a\ngraph G=(V,E) with |V|=n where in each round, each node either transmits a\npacket, with length B=Omega(log n) bits, or listens. Each node receives a\npacket iff it is listening and exactly one of its neighbors is transmitting. We\nconsider the problem of k-message broadcast, where k messages, each with\nTheta(B) bits, are placed in an arbitrary nodes of the graph and the goal is to\ndeliver all messages to all the nodes. We present a simple proof showing that\nthere exist a radio network with radius 2 where for any k, broadcasting k\nmessages requires at least Omega(k log n) rounds. That is, in this network,\nregardless of the algorithm, the maximum achievable broadcast throughput is\nO(1/log n).\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 20:03:24 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Ghaffari", "Mohsen", ""], ["Haeupler", "Bernhard", ""], ["Khabbazian", "Majid", ""]]}, {"id": "1302.0621", "submitter": "Patrick P. C. Lee", "authors": "Chun-Ho Ng and Patrick P. C. Lee", "title": "RevDedup: A Reverse Deduplication Storage System Optimized for Reads to\n  Latest Backups", "comments": "A 7-page version appeared in APSys'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling up the backup storage for an ever-increasing volume of virtual\nmachine (VM) images is a critical issue in virtualization environments. While\ndeduplication is known to effectively eliminate duplicates for VM image\nstorage, it also introduces fragmentation that will degrade read performance.\nWe propose RevDedup, a deduplication system that optimizes reads to latest VM\nimage backups using an idea called reverse deduplication. In contrast with\nconventional deduplication that removes duplicates from new data, RevDedup\nremoves duplicates from old data, thereby shifting fragmentation to old data\nwhile keeping the layout of new data as sequential as possible. We evaluate our\nRevDedup prototype using microbenchmark and real-world workloads. For a 12-week\nspan of real-world VM images from 160 users, RevDedup achieves high\ndeduplication efficiency with around 97% of saving, and high backup and read\nthroughput on the order of 1GB/s. RevDedup also incurs small metadata overhead\nin backup/read operations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 09:09:38 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 12:32:32 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2013 06:04:17 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Ng", "Chun-Ho", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1302.0948", "submitter": "Piotr Skowron", "authors": "Piotr Skowron, Krzysztof Rzadca", "title": "Non-monetary fair scheduling---a cooperative game theory approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-organizational system in which each organization\ncontributes processors to the global pool but also jobs to be processed on the\ncommon resources. The fairness of the scheduling algorithm is essential for the\nstability and even for the existence of such systems (as organizations may\nrefuse to join an unfair system).\n  We consider on-line, non-clairvoyant scheduling of sequential jobs. The\nstarted jobs cannot be stopped, canceled, preempted, or moved to other\nprocessors. We consider identical processors, but most of our results can be\nextended to related or unrelated processors.\n  We model the fair scheduling problem as a cooperative game and we use the\nShapley value to determine the ideal fair schedule. In contrast to the current\nliterature, we do not use money to assess the relative utilities of jobs.\nInstead, to calculate the contribution of an organization, we determine how the\npresence of this organization influences the performance of other\norganizations. Our approach can be used with arbitrary utility function (e.g.,\nflow time, tardiness, resource utilization), but we argue that the utility\nfunction should be strategy resilient. The organizations should be discouraged\nfrom splitting, merging or delaying their jobs. We present the unique (to\nwithin a multiplicative and additive constants) strategy resilient utility\nfunction.\n  We show that the problem of fair scheduling is NP-hard and hard to\napproximate. However, for unit-size jobs, we present an FPRAS. Also, we show\nthat the problem parametrized with the number of organizations is FPT. Although\nfor the large number of the organizations the problem is computationally hard,\nthe presented exponential algorithm can be used as a fairness benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 06:59:03 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 17:37:13 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Skowron", "Piotr", ""], ["Rzadca", "Krzysztof", ""]]}, {"id": "1302.1157", "submitter": "Zaid Towfic", "authors": "Zaid J. Towfic and Jianshu Chen and Ali H. Sayed", "title": "Excess-Risk of Distributed Stochastic Learners", "comments": "32 pages, 5 figures, to appear in IEEE Transactions on Information\n  Theory, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the learning ability of consensus and diffusion distributed\nlearners from continuous streams of data arising from different but related\nstatistical distributions. Four distinctive features for diffusion learners are\nrevealed in relation to other decentralized schemes even under left-stochastic\ncombination policies. First, closed-form expressions for the evolution of their\nexcess-risk are derived for strongly-convex risk functions under a diminishing\nstep-size rule. Second, using these results, it is shown that the diffusion\nstrategy improves the asymptotic convergence rate of the excess-risk relative\nto non-cooperative schemes. Third, it is shown that when the in-network\ncooperation rules are designed optimally, the performance of the diffusion\nimplementation can outperform that of naive centralized processing. Finally,\nthe arguments further show that diffusion outperforms consensus strategies\nasymptotically, and that the asymptotic excess-risk expression is invariant to\nthe particular network topology. The framework adopted in this work studies\nconvergence in the stronger mean-square-error sense, rather than in\ndistribution, and develops tools that enable a close examination of the\ndifferences between distributed strategies in terms of asymptotic behavior, as\nwell as in terms of convergence rates.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2013 19:19:34 GMT"}, {"version": "v2", "created": "Sat, 20 Sep 2014 21:00:33 GMT"}, {"version": "v3", "created": "Sun, 17 Jul 2016 16:40:11 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Towfic", "Zaid J.", ""], ["Chen", "Jianshu", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1302.1306", "submitter": "Giuseppe Lipari", "authors": "Youcheng Sun, Romain Soulat, Giuseppe Lipari, \\'Etienne Andr\\'e,\n  Laurent Fribourg", "title": "Parametric Schedulability Analysis of Fixed Priority Real-Time\n  Distributed Systems", "comments": "Submitted to ECRTS 2013 (http://ecrts.eit.uni-kl.de/ecrts13)", "journal-ref": null, "doi": null, "report-no": "LSV-13-03", "categories": "cs.DC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric analysis is a powerful tool for designing modern embedded systems,\nbecause it permits to explore the space of design parameters, and to check the\nrobustness of the system with respect to variations of some uncontrollable\nvariable. In this paper, we address the problem of parametric schedulability\nanalysis of distributed real-time systems scheduled by fixed priority. In\nparticular, we propose two different approaches to parametric analysis: the\nfirst one is a novel technique based on classical schedulability analysis,\nwhereas the second approach is based on model checking of Parametric Timed\nAutomata (PTA).\n  The proposed analytic method extends existing sensitivity analysis for single\nprocessors to the case of a distributed system, supporting preemptive and\nnon-preemptive scheduling, jitters and unconstrained deadlines. Parametric\nTimed Automata are used to model all possible behaviours of a distributed\nsystem, and therefore it is a necessary and sufficient analysis. Both\ntechniques have been implemented in two software tools, and they have been\ncompared with classical holistic analysis on two meaningful test cases. The\nresults show that the analytic method provides results similar to classical\nholistic analysis in a very efficient way, whereas the PTA approach is slower\nbut covers the entire space of solutions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 10:06:39 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Sun", "Youcheng", ""], ["Soulat", "Romain", ""], ["Lipari", "Giuseppe", ""], ["Andr\u00e9", "\u00c9tienne", ""], ["Fribourg", "Laurent", ""]]}, {"id": "1302.1326", "submitter": "Yu Zhou", "authors": "Yu Zhou", "title": "Cloud Computing framework for Computer Vision Research:An Introduction", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing offers the potential to help scientists to process massive\nnumber of computing resources often required in machine learning application\nsuch as computer vision problems. This proposal would like to show that which\nbenefits can be obtained from cloud in order to help medical image analysis\nusers (including scientists, clinicians, and research institutes). As security\nand privacy of algorithms are important for most of algorithms inventors, these\nalgorithms can be hidden in a cloud to allow the users to use the algorithms as\na package without any access to see/change their inside. In another word, in\nthe user part, users send their images to the cloud and configure the algorithm\nvia an interface. In the cloud part, the algorithms are applied to this image\nand the results are returned back to the user. My proposal has two parts: (1)\ninvestigate the potential of cloud computing for computer vision problems and\n(2) study the components of a proposed cloud-based framework for medical image\nanalysis application and develop them (depending on the length of the\ninternship). The investigation part will involve a study on several aspects of\nthe problem including security, usability (for medical end users of the\nservice), appropriate programming abstractions for vision problems, scalability\nand resource requirements. In the second part of this proposal I am going to\nthoroughly study of the proposed framework components and their relations and\ndevelop them. The proposed cloud-based framework includes an integrated\nenvironment to enable scientists and clinicians to access to the previous and\ncurrent medical image analysis algorithms using a handful user interface\nwithout any access to the algorithm codes and procedures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 11:41:26 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Zhou", "Yu", ""]]}, {"id": "1302.1390", "submitter": "Raphael `kena' Poss", "authors": "Mike Lankamp and Raphael Poss and Qiang Yang and Jian Fu and Irfan\n  Uddin and Chris R. Jesshope", "title": "MGSim - Simulation tools for multi-core processor architectures", "comments": "33 pages, 22 figures, 4 listings, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MGSim is an open source discrete event simulator for on-chip hardware\ncomponents, developed at the University of Amsterdam. It is intended to be a\nresearch and teaching vehicle to study the fine-grained hardware/software\ninteractions on many-core and hardware multithreaded processors. It includes\nsupport for core models with different instruction sets, a configurable\nmulti-core interconnect, multiple configurable cache and memory models, a\ndedicated I/O subsystem, and comprehensive monitoring and interaction\nfacilities. The default model configuration shipped with MGSim implements\nMicrogrids, a many-core architecture with hardware concurrency management.\nMGSim is furthermore written mostly in C++ and uses object classes to represent\nchip components. It is optimized for architecture models that can be described\nas process networks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:00:35 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Lankamp", "Mike", ""], ["Poss", "Raphael", ""], ["Yang", "Qiang", ""], ["Fu", "Jian", ""], ["Uddin", "Irfan", ""], ["Jesshope", "Chris R.", ""]]}, {"id": "1302.1939", "submitter": "R. J. Sobie", "authors": "R.Sobie, A. Agarwal, I. Gable, C. Leavett-Brown, M. Paterson, R.\n  Taylor, A. Charbonneau, R. Impey, W. Podiama", "title": "HTC Scientific Computing in a Distributed Cloud Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the use of a distributed cloud computing system for\nhigh-throughput computing (HTC) scientific applications. The distributed cloud\ncomputing system is composed of a number of separate\nInfrastructure-as-a-Service (IaaS) clouds that are utilized in a unified\ninfrastructure. The distributed cloud has been in production-quality operation\nfor two years with approximately 500,000 completed jobs where a typical\nworkload has 500 simultaneous embarrassingly-parallel jobs that run for\napproximately 12 hours. We review the design and implementation of the system\nwhich is based on pre-existing components and a number of custom components. We\ndiscuss the operation of the system, and describe our plans for the expansion\nto more sites and increased computing capacity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 04:29:16 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Sobie", "R.", ""], ["Agarwal", "A.", ""], ["Gable", "I.", ""], ["Leavett-Brown", "C.", ""], ["Paterson", "M.", ""], ["Taylor", "R.", ""], ["Charbonneau", "A.", ""], ["Impey", "R.", ""], ["Podiama", "W.", ""]]}, {"id": "1302.1954", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and He Zhang and Rainbow Cai", "title": "On a Catalogue of Metrics for Evaluating Commercial Cloud Services", "comments": "10 pages, Proceedings of the 13th ACM/IEEE International Conference\n  on Grid Computing (Grid 2012), pp. 164-173, Beijing, China, September 20-23,\n  2012", "journal-ref": null, "doi": "10.1109/Grid.2012.15", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the continually increasing amount of commercial Cloud services in the\nmarket, evaluation of different services plays a significant role in\ncost-benefit analysis or decision making for choosing Cloud Computing. In\nparticular, employing suitable metrics is essential in evaluation\nimplementations. However, to the best of our knowledge, there is not any\nsystematic discussion about metrics for evaluating Cloud services. By using the\nmethod of Systematic Literature Review (SLR), we have collected the de facto\nmetrics adopted in the existing Cloud services evaluation work. The collected\nmetrics were arranged following different Cloud service features to be\nevaluated, which essentially constructed an evaluation metrics catalogue, as\nshown in this paper. This metrics catalogue can be used to facilitate the\nfuture practice and research in the area of Cloud services evaluation.\nMoreover, considering metrics selection is a prerequisite of benchmark\nselection in evaluation implementations, this work also supplements the\nexisting research in benchmarking the commercial Cloud services.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 07:10:19 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Zhang", "He", ""], ["Cai", "Rainbow", ""]]}, {"id": "1302.1957", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Rainbow Cai and He Zhang", "title": "Towards a Taxonomy of Performance Evaluation of Commercial Cloud\n  Services", "comments": "8 pages, Proceedings of the 5th International Conference on Cloud\n  Computing (IEEE CLOUD 2012), pp. 344-351, Honolulu, Hawaii, USA, June 24-29,\n  2012", "journal-ref": null, "doi": "10.1109/CLOUD.2012.74", "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud Computing, as one of the most promising computing paradigms, has become\nincreasingly accepted in industry. Numerous commercial providers have started\nto supply public Cloud services, and corresponding performance evaluation is\nthen inevitably required for Cloud provider selection or cost-benefit analysis.\nUnfortunately, inaccurate and confusing evaluation implementations can be often\nseen in the context of commercial Cloud Computing, which could severely\ninterfere and spoil evaluation-related comprehension and communication. This\npaper introduces a taxonomy to help profile and standardize the details of\nperformance evaluation of commercial Cloud services. Through a systematic\nliterature review, we constructed the taxonomy along two dimensions by\narranging the atomic elements of Cloud-related performance evaluation. As such,\nthis proposed taxonomy can be employed both to analyze existing evaluation\npractices through decomposition into elements and to design new experiments\nthrough composing elements for evaluating performance of commercial Cloud\nservices. Moreover, through smooth expansion, we can continually adapt this\ntaxonomy to the more general area of evaluation of Cloud Computing.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 07:20:30 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Cai", "Rainbow", ""], ["Zhang", "He", ""]]}, {"id": "1302.2129", "submitter": "Nima Noorshams", "authors": "Nima Noorshams, Martin Wainwright", "title": "Non-Asymptotic Analysis of an Optimal Algorithm for Network-Constrained\n  Averaging with Noisy Links", "comments": null, "journal-ref": "N. Noorshams, M. J. Wainwright, \"Non-Asymptotic Analysis of an\n  Optimal Algorithm for Network-Constrained Averaging with Noisy Links\", IEEE\n  journal of selected topics in signal processing, vol. 5, no. 4, pp. 833-844,\n  Aug. 2011", "doi": "10.1109/JSTSP.2011.2122241", "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of network-constrained averaging is to compute the average of a\nset of values distributed throughout a graph G using an algorithm that can pass\nmessages only along graph edges. We study this problem in the noisy setting, in\nwhich the communication along each link is modeled by an additive white\nGaussian noise channel. We propose a two-phase decentralized algorithm, and we\nuse stochastic approximation methods in conjunction with the spectral graph\ntheory to provide concrete (non-asymptotic) bounds on the mean-squared error.\nHaving found such bounds, we analyze how the number of iterations T_G(n;\n\\delta) required to achieve mean-squared error \\delta\\ scales as a function of\nthe graph topology and the number of nodes n. Previous work provided guarantees\nwith the number of iterations scaling inversely with the second smallest\neigenvalue of the Laplacian. This paper gives an algorithm that reduces this\ngraph dependence to the graph diameter, which is the best scaling possible.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2013 19:46:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Noorshams", "Nima", ""], ["Wainwright", "Martin", ""]]}, {"id": "1302.2197", "submitter": "Zheng Li", "authors": "Zheng Li and He Zhang and Liam O'Brien", "title": "Towards Technology Independent Strategies for SOA Implementations", "comments": "12 pages, Proceedings of the 6th International Conference on\n  Evaluation of Novel Approaches to Software Engineering (ENASE 2011), pp.\n  143-154, Beijing, China, June 08-11, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefiting from the technology based strategies, Service-Oriented\nArchitecture (SOA) has been able to achieve the general goals such as agility,\nflexibility, reusability and efficiency. Nevertheless, technical conditions\nalone cannot guarantee successful SOA implementations. As a valuable and\nnecessary supplement, the space of technology independent strategies should\nalso be explored. Through treating SOA system as an instance of organization\nand identifying the common ground on the similar process of SOA implementation\nand organization design, this paper uses existing work in organization theory\narea to inspire the research into technology independent strategies of SOA\nimplementation. As a result, four preliminary strategies that can be applied to\norganizational area we identify to support SOA implementations. Furthermore,\nthe novel methodology of investigating technology independent strategies for\nimplementing SOA is revealed, which encourages interdisciplinary research\nacross service-oriented computing and organization theory.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 05:34:38 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Li", "Zheng", ""], ["Zhang", "He", ""], ["O'Brien", "Liam", ""]]}, {"id": "1302.2199", "submitter": "Zheng Li", "authors": "Zheng Li and Jacky Keung", "title": "Software Cost Estimation Framework for Service-Oriented Architecture\n  Systems using Divide-and-Conquer Approach", "comments": "8 pages, Proceedings of the 5th International Symposium on\n  Service-Oriented System Engineering (SOSE 2010), pp. 47-54, Nanjing, China,\n  June 4-5, 2010. arXiv admin note: text overlap with arXiv:1302.1912", "journal-ref": null, "doi": "10.1109/SOSE.2010.29", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the complexity of Service-Oriented Architecture (SOA), cost and effort\nestimation for SOA-based software development is more difficult than that for\ntraditional software development. Unfortunately, there is a lack of published\nwork about cost and effort estimation for SOA-based software. Existing cost\nestimation approaches are inadequate to address the complex service-oriented\nsystems. This paper proposes a novel framework based on Divide-and-Conquer\n(D&C) for cost estimation for building SOA-based software. By dealing with\nseparately development parts, the D&C framework can help organizations simplify\nand regulate SOA implementation cost estimation. Furthermore, both cost\nestimation modeling and software sizing work can be satisfied respectively by\nswitching the corresponding metrics within this framework. Given the\nrequirement of developing these metrics, this framework also defines the future\nresearch in four different directions according to the separate cost estimation\nsub-problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 05:47:04 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Li", "Zheng", ""], ["Keung", "Jacky", ""]]}, {"id": "1302.2201", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Jacky Keung and Xiwei Xu", "title": "Effort-Oriented Classification Matrix of Web Service Composition", "comments": "6 pages, Proceedings of the 5th International Conference on Internet\n  and Web Applications and Services (ICIW 2010), pp. 357-362, Barcelona, Spain,\n  May 9-15, 2010", "journal-ref": null, "doi": "10.1109/ICIW.2010.59", "report-no": null, "categories": "cs.SE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the service-oriented computing domain, Web service composition is an\neffective realization to satisfy the rapidly changing requirements of business.\nTherefore, the research into Web service composition has unfolded broadly.\nSince examining all of the related work in this area becomes a mission next to\nimpossible, the classification of composition approaches can be used to\nfacilitate multiple research tasks. However, the current attempts to classify\nWeb service composition do not have clear objectives. Furthermore, the contexts\nand technologies of composition approaches are confused in the existing\nclassifications. This paper proposes an effort-oriented classification matrix\nfor Web service composition, which distinguishes between the context and\ntechnology dimension. The context dimension is aimed at analyzing the\nenvironment influence on the effort of Web service composition, while the\ntechnology dimension focuses on the technique influence on the effort.\nConsequently, besides the traditional classification benefits, this matrix can\nbe used to build the basis of cost estimation for Web service composition in\nfuture research.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 05:53:29 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Keung", "Jacky", ""], ["Xu", "Xiwei", ""]]}, {"id": "1302.2202", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and Rainbow Cai and He Zhang", "title": "Building an Expert System for Evaluation of Commercial Cloud Services", "comments": "8 page, Proceedings of the 2012 International Conference on Cloud and\n  Service Computing (CSC 2012), pp. 168-175, Shanghai, China, November 22-24,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial Cloud services have been increasingly supplied to customers in\nindustry. To facilitate customers' decision makings like cost-benefit analysis\nor Cloud provider selection, evaluation of those Cloud services are becoming\nmore and more crucial. However, compared with evaluation of traditional\ncomputing systems, more challenges will inevitably appear when evaluating\nrapidly-changing and user-uncontrollable commercial Cloud services. This paper\nproposes an expert system for Cloud evaluation that addresses emerging\nevaluation challenges in the context of Cloud Computing. Based on the knowledge\nand data accumulated by exploring the existing evaluation work, this expert\nsystem has been conceptually validated to be able to give suggestions and\nguidelines for implementing new evaluation experiments. As such, users can\nconveniently obtain evaluation experiences by using this expert system, which\nis essentially able to make existing efforts in Cloud services evaluation\nreusable and sustainable.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 06:04:35 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Cai", "Rainbow", ""], ["Zhang", "He", ""]]}, {"id": "1302.2203", "submitter": "Zheng Li", "authors": "Zheng Li and Liam O'Brien and He Zhang and Rainbow Cai", "title": "A Factor Framework for Experimental Design for Performance Evaluation of\n  Commercial Cloud Services", "comments": "8 pages, Proceedings of the 4th International Conference on Cloud\n  Computing Technology and Science (CloudCom 2012), pp. 169-176, Taipei,\n  Taiwan, December 03-06, 2012", "journal-ref": null, "doi": "10.1109/CloudCom.2012.6427525", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the diversity of commercial Cloud services, performance evaluations of\ncandidate services would be crucial and beneficial for both service customers\n(e.g. cost-benefit analysis) and providers (e.g. direction of service\nimprovement). Before an evaluation implementation, the selection of suitable\nfactors (also called parameters or variables) plays a prerequisite role in\ndesigning evaluation experiments. However, there seems a lack of systematic\napproaches to factor selection for Cloud services performance evaluation. In\nother words, evaluators randomly and intuitively concerned experimental factors\nin most of the existing evaluation studies. Based on our previous taxonomy and\nmodeling work, this paper proposes a factor framework for experimental design\nfor performance evaluation of commercial Cloud services. This framework\ncapsules the state-of-the-practice of performance evaluation factors that\npeople currently take into account in the Cloud Computing domain, and in turn\ncan help facilitate designing new experiments for evaluating Cloud services.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 06:18:04 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Li", "Zheng", ""], ["O'Brien", "Liam", ""], ["Zhang", "He", ""], ["Cai", "Rainbow", ""]]}, {"id": "1302.2217", "submitter": "Swan Dubois", "authors": "Swan Dubois (LPD, EPFL), Rachid Guerraoui (LPD, EPFL)", "title": "Introducing Speculation in Self-Stabilization - An Application to Mutual\n  Exclusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-stabilization ensures that, after any transient fault, the system\nrecovers in a finite time and eventually exhibits. Speculation consists in\nguaranteeing that the system satisfies its requirements for any execution but\nexhibits significantly better performances for a subset of executions that are\nmore probable. A speculative protocol is in this sense supposed to be both\nrobust and efficient in practice. We introduce the notion of speculative\nstabilization which we illustrate through the mutual exclusion problem. We then\npresent a novel speculatively stabilizing mutual exclusion protocol. Our\nprotocol is self-stabilizing for any asynchronous execution. We prove that its\nstabilization time for synchronous executions is diam(g)/2 steps (where diam(g)\ndenotes the diameter of the system). This complexity result is of independent\ninterest. The celebrated mutual exclusion protocol of Dijkstra stabilizes in n\nsteps (where n is the number of processes) in synchronous executions and the\nquestion whether the stabilization time could be strictly smaller than the\ndiameter has been open since then (almost 40 years). We show that this is\nindeed possible for any underlying topology. We also provide a lower bound\nproof that shows that our new stabilization time of diam(g)/2 steps is optimal\nfor synchronous executions, even if asynchronous stabilization is not required.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 11:09:22 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Dubois", "Swan", "", "LPD, EPFL"], ["Guerraoui", "Rachid", "", "LPD, EPFL"]]}, {"id": "1302.2227", "submitter": "Sina Esfandiarpoor", "authors": "Sina Esfandiarpoor, Ali Pahlavan, Maziar Goudarzi", "title": "Virtual Machine Consolidation for Datacenter Energy Improvement", "comments": "This is draft version. The finally version will be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth and proliferation of cloud computing services around the world\nhas increased the necessity and significance of improving the energy efficiency\nof could implementations. Virtual machines (VM) comprise the backend of most,\nif not all, cloud computing services. Several VMs are often consolidated on a\nphysical machine to better utilize its resources. We take into account the\ncooling and network structure of the datacenter hosting the physical machines\nwhen consolidating the VMs so that fewer racks and routers are employed,\nwithout compromising the service-level agreements, so that unused routing and\ncooling equipment can be turned off to reduce energy consumption. Our\nexperimental results on four benchmarks shows that our technique improves\nenergy consumption of servers, network equipment, and cooling systems by 2.5%,\n18.8%, and 28.2% respectively, resulting in a total of 14.7% improvement on\naverage in the entire datacenter.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2013 12:23:21 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Esfandiarpoor", "Sina", ""], ["Pahlavan", "Ali", ""], ["Goudarzi", "Maziar", ""]]}, {"id": "1302.2529", "submitter": "Riccardo Murri", "authors": "Tyanko Aleksiev, Simon Barkow, Peter Kunszt, Sergio Maffioletti,\n  Riccardo Murri, Christian Panse", "title": "VM-MAD: a cloud/cluster software for service-oriented academic\n  environments", "comments": "16 pages, 5 figures. Accepted at the International Supercomputing\n  Conference ISC13, June 17--20 Leipzig, Germany", "journal-ref": "Supercomputing, Volume 7905 of the series Lecture Notes in\n  Computer Science (2013) pp 447-461", "doi": "10.1007/978-3-642-38750-0_34", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of powerful computing hardware in IaaS clouds makes cloud\ncomputing attractive also for computational workloads that were up to now\nalmost exclusively run on HPC clusters.\n  In this paper we present the VM-MAD Orchestrator software: an open source\nframework for cloudbursting Linux-based HPC clusters into IaaS clouds but also\ncomputational grids. The Orchestrator is completely modular, allowing flexible\nconfigurations of cloudbursting policies. It can be used with any batch system\nor cloud infrastructure, dynamically extending the cluster when needed. A\ndistinctive feature of our framework is that the policies can be tested and\ntuned in a simulation mode based on historical or synthetic cluster accounting\ndata.\n  In the paper we also describe how the VM-MAD Orchestrator was used in a\nproduction environment at the FGCZ to speed up the analysis of mass\nspectrometry-based protein data by cloudbursting to the Amazon EC2. The\nadvantages of this hybrid system are shown with a large evaluation run using\nabout hundred large EC2 nodes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 16:49:49 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2013 11:21:31 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Aleksiev", "Tyanko", ""], ["Barkow", "Simon", ""], ["Kunszt", "Peter", ""], ["Maffioletti", "Sergio", ""], ["Murri", "Riccardo", ""], ["Panse", "Christian", ""]]}, {"id": "1302.2543", "submitter": "Nitin Vaidya", "authors": "Nitin H. Vaidya, Vijay K. Garg", "title": "Byzantine Vector Consensus in Complete Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a network of n processes each of which has a d-dimensional vector of\nreals as its input. Each process can communicate directly with all the\nprocesses in the system; thus the communication network is a complete graph.\nAll the communication channels are reliable and FIFO (first-in-first-out). The\nproblem of Byzantine vector consensus (BVC) requires agreement on a\nd-dimensional vector that is in the convex hull of the d-dimensional input\nvectors at the non-faulty processes. We obtain the following results for\nByzantine vector consensus in complete graphs while tolerating up to f\nByzantine failures:\n  * We prove that in a synchronous system, n >= max(3f+1, (d+1)f+1) is\nnecessary and sufficient for achieving Byzantine vector consensus.\n  * In an asynchronous system, it is known that exact consensus is impossible\nin presence of faulty processes. For an asynchronous system, we prove that n >=\n(d+2)f+1 is necessary and sufficient to achieve approximate Byzantine vector\nconsensus.\n  Our sufficiency proofs are constructive. We show sufficiency by providing\nexplicit algorithms that solve exact BVC in synchronous systems, and\napproximate BVC in asynchronous systems.\n  We also obtain tight bounds on the number of processes for achieving BVC\nusing algorithms that are restricted to a simpler communication pattern.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 17:25:24 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Vaidya", "Nitin H.", ""], ["Garg", "Vijay K.", ""]]}, {"id": "1302.2570", "submitter": "Jukka Suomela", "authors": "Pierre Fraigniaud, Mika G\\\"o\\\"os, Amos Korman, Jukka Suomela", "title": "What can be decided locally without identifiers?", "comments": "1 + 15 pages, 3 figures", "journal-ref": null, "doi": "10.1145/2484239.2484264", "report-no": null, "categories": "cs.DC cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do unique node identifiers help in deciding whether a network $G$ has a\nprescribed property $P$? We study this question in the context of distributed\nlocal decision, where the objective is to decide whether $G \\in P$ by having\neach node run a constant-time distributed decision algorithm. If $G \\in P$, all\nthe nodes should output yes; if $G \\notin P$, at least one node should output\nno.\n  A recent work (Fraigniaud et al., OPODIS 2012) studied the role of\nidentifiers in local decision and gave several conditions under which\nidentifiers are not needed. In this article, we answer their original question.\nMore than that, we do so under all combinations of the following two critical\nvariations on the underlying model of distributed computing:\n  ($B$): the size of the identifiers is bounded by a function of the size of\nthe input network; as opposed to ($\\neg B$): the identifiers are unbounded.\n  ($C$): the nodes run a computable algorithm; as opposed to ($\\neg C$): the\nnodes can compute any, possibly uncomputable function.\n  While it is easy to see that under ($\\neg B, \\neg C$) identifiers are not\nneeded, we show that under all other combinations there are properties that can\nbe decided locally if and only if identifiers are present. Our constructions\nuse ideas from classical computability theory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 18:51:24 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Fraigniaud", "Pierre", ""], ["G\u00f6\u00f6s", "Mika", ""], ["Korman", "Amos", ""], ["Suomela", "Jukka", ""]]}, {"id": "1302.2749", "submitter": "Mario Pastorelli", "authors": "Mario Pastorelli, Antonio Barbuzzi, Damiano Carra, Matteo Dell'Amico,\n  Pietro Michiardi", "title": "Practical Size-based Scheduling for MapReduce Workloads", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Hadoop Fair Sojourn Protocol (HFSP) scheduler, which\nimplements a size-based scheduling discipline for Hadoop. The benefits of\nsize-based scheduling disciplines are well recognized in a variety of contexts\n(computer networks, operating systems, etc...), yet, their practical\nimplementation for a system such as Hadoop raises a number of important\nchallenges. With HFSP, which is available as an open-source project, we address\nissues related to job size estimation, resource management and study the\neffects of a variety of preemption strategies. Although the architecture\nunderlying HFSP is suitable for any size-based scheduling discipline, in this\nwork we revisit and extend the Fair Sojourn Protocol, which solves problems\nrelated to job starvation that affect FIFO, Processor Sharing and a range of\nsize-based disciplines. Our experiments, in which we compare HFSP to standard\nHadoop schedulers, pinpoint at a significant decrease in average job sojourn\ntimes - a metric that accounts for the total time a job spends in the system,\nincluding waiting and serving times - for realistic workloads that we generate\naccording to production traces available in literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 10:11:29 GMT"}, {"version": "v2", "created": "Fri, 3 May 2013 16:26:13 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Pastorelli", "Mario", ""], ["Barbuzzi", "Antonio", ""], ["Carra", "Damiano", ""], ["Dell'Amico", "Matteo", ""], ["Michiardi", "Pietro", ""]]}, {"id": "1302.2757", "submitter": "Daniel Cederman", "authors": "Daniel Cederman and Anders Gidenstam and Phuong Ha and H{\\aa}kan\n  Sundell and Marina Papatriantafilou and Philippas Tsigas", "title": "Lock-free Concurrent Data Structures", "comments": "To appear in \"Programming Multi-core and Many-core Computing\n  Systems\", eds. S. Pllana and F. Xhafa, Wiley Series on Parallel and\n  Distributed Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent data structures are the data sharing side of parallel programming.\nData structures give the means to the program to store data, but also provide\noperations to the program to access and manipulate these data. These operations\nare implemented through algorithms that have to be efficient. In the sequential\nsetting, data structures are crucially important for the performance of the\nrespective computation. In the parallel programming setting, their importance\nbecomes more crucial because of the increased use of data and resource sharing\nfor utilizing parallelism.\n  The first and main goal of this chapter is to provide a sufficient background\nand intuition to help the interested reader to navigate in the complex research\narea of lock-free data structures. The second goal is to offer the programmer\nfamiliarity to the subject that will allow her to use truly concurrent methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 11:08:26 GMT"}], "update_date": "2013-02-13", "authors_parsed": [["Cederman", "Daniel", ""], ["Gidenstam", "Anders", ""], ["Ha", "Phuong", ""], ["Sundell", "H\u00e5kan", ""], ["Papatriantafilou", "Marina", ""], ["Tsigas", "Philippas", ""]]}, {"id": "1302.2837", "submitter": "Sebastian Nanz", "authors": "Sebastian Nanz, Scott West, Kaue Soares da Silveira, Bertrand Meyer", "title": "Benchmarking Usability and Performance of Multicore Languages", "comments": null, "journal-ref": "Proceedings of the 7th ACM-IEEE International Symposium Empirical\n  Software Engineering and Measurement (ESEM'13), pages 183-192. IEEE, 2013", "doi": "10.1109/ESEM.2013.10", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers face a wide choice of programming languages and libraries\nsupporting multicore computing. Ever more diverse paradigms for expressing\nparallelism and synchronization become available while their influence on\nusability and performance remains largely unclear. This paper describes an\nexperiment comparing four markedly different approaches to parallel\nprogramming: Chapel, Cilk, Go, and Threading Building Blocks (TBB). Each\nlanguage is used to implement sequential and parallel versions of six benchmark\nprograms. The implementations are then reviewed by notable experts in the\nlanguage, thereby obtaining reference versions for each language and benchmark.\nThe resulting pool of 96 implementations is used to compare the languages with\nrespect to source code size, coding time, execution time, and speedup. The\nexperiment uncovers strengths and weaknesses in all approaches, facilitating an\ninformed selection of a language under a particular set of requirements. The\nexpert review step furthermore highlights the importance of expert knowledge\nwhen using modern parallel programming approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2013 16:08:12 GMT"}, {"version": "v2", "created": "Thu, 23 Oct 2014 10:42:56 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Nanz", "Sebastian", ""], ["West", "Scott", ""], ["da Silveira", "Kaue Soares", ""], ["Meyer", "Bertrand", ""]]}, {"id": "1302.3292", "submitter": "EPTCS", "authors": "Aurel Randolph (\\'Ecole Polytechnique de Montr\\'eal), Hanifa Boucheneb\n  (\\'Ecole Polytechnique de Montr\\'eal), Abdessamad Imine (INRIA Grand-Est and\n  Nancy-Universit\\'e), Alejandro Quintero (\\'Ecole Polytechnique de Montr\\'eal)", "title": "On Consistency of Operational Transformation Approach", "comments": "In Proceedings Infinity 2012, arXiv:1302.3105", "journal-ref": "EPTCS 107, 2013, pp. 45-59", "doi": "10.4204/EPTCS.107.5", "report-no": null, "categories": "cs.DC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Operational Transformation (OT) approach, used in many collaborative\neditors, allows a group of users to concurrently update replicas of a shared\nobject and exchange their updates in any order. The basic idea of this approach\nis to transform any received update operation before its execution on a replica\nof the object. This transformation aims to ensure the convergence of the\ndifferent replicas of the object, even though the operations are executed in\ndifferent orders. However, designing transformation functions for achieving\nconvergence is a critical and challenging issue. Indeed, the transformation\nfunctions proposed in the literature are all revealed incorrect.\n  In this paper, we investigate the existence of transformation functions for a\nshared string altered by insert and delete operations. From the theoretical\npoint of view, two properties - named TP1 and TP2 - are necessary and\nsufficient to ensure convergence. Using controller synthesis technique, we show\nthat there are some transformation functions which satisfy only TP1 for the\nbasic signatures of insert and delete operations. As a matter of fact, it is\nimpossible to meet both properties TP1 and TP2 with these simple signatures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 02:26:41 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Randolph", "Aurel", "", "\u00c9cole Polytechnique de Montr\u00e9al"], ["Boucheneb", "Hanifa", "", "\u00c9cole Polytechnique de Montr\u00e9al"], ["Imine", "Abdessamad", "", "INRIA Grand-Est and\n  Nancy-Universit\u00e9"], ["Quintero", "Alejandro", "", "\u00c9cole Polytechnique de Montr\u00e9al"]]}, {"id": "1302.3344", "submitter": "Runhui Li", "authors": "Runhui Li and Jian Lin and Patrick P. C. Lee", "title": "CORE: Augmenting Regenerating-Coding-Based Recovery for Single and\n  Concurrent Failures in Distributed Storage Systems", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data availability is critical in distributed storage systems, especially when\nnode failures are prevalent in real life. A key requirement is to minimize the\namount of data transferred among nodes when recovering the lost or unavailable\ndata of failed nodes. This paper explores recovery solutions based on\nregenerating codes, which are shown to provide fault-tolerant storage and\nminimum recovery bandwidth. Existing optimal regenerating codes are designed\nfor single node failures. We build a system called CORE, which augments\nexisting optimal regenerating codes to support a general number of failures\nincluding single and concurrent failures. We theoretically show that CORE\nachieves the minimum possible recovery bandwidth for most cases. We implement\nCORE and evaluate our prototype atop a Hadoop HDFS cluster testbed with up to\n20 storage nodes. We demonstrate that our CORE prototype conforms to our\ntheoretical findings and achieves recovery bandwidth saving when compared to\nthe conventional recovery approach based on erasure codes.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2013 09:08:38 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 02:49:43 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Li", "Runhui", ""], ["Lin", "Jian", ""], ["Lee", "Patrick P. C.", ""]]}, {"id": "1302.3752", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy, Yves Robert, Fr\\'ed\\'eric Vivien, Dounia Zaidouni", "title": "Checkpointing algorithms and fault prediction", "comments": "Supported in part by ANR Rescue. Published in Journal of Parallel and\n  Distributed Computing. arXiv admin note: text overlap with arXiv:1207.6936", "journal-ref": "Journal of Parallel and Distributed Computing, Available online 7\n  November 2013, ISSN 0743-7315", "doi": "10.1016/j.jpdc.2013.10.010", "report-no": "INRIA RR-8237", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We extend the classical first-order analysis of Young\nand Daly in the presence of a fault prediction system, characterized by its\nrecall and its precision. In this framework, we provide an optimal algorithm to\ndecide when to take predictions into account, and we derive the optimal value\nof the checkpointing period. These results allow to analytically assess the key\nparameters that impact the performance of fault predictors at very large scale.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 13:52:50 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 23:36:21 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Aupy", "Guillaume", ""], ["Robert", "Yves", ""], ["Vivien", "Fr\u00e9d\u00e9ric", ""], ["Zaidouni", "Dounia", ""]]}, {"id": "1302.3828", "submitter": "Francesco Pasquale", "authors": "Andrea Clementi, Pierluigi Crescenzi, Carola Doerr, Pierre Fraigniaud,\n  Marco Isopi, Alessandro Panconesi, Francesco Pasquale, and Riccardo Silvestri", "title": "Rumor Spreading in Random Evolving Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC cs.SI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized gossip is one of the most popular way of disseminating information\nin large scale networks. This method is appreciated for its simplicity,\nrobustness, and efficiency. In the \"push\" protocol, every informed node\nselects, at every time step (a.k.a. round), one of its neighboring node\nuniformly at random and forwards the information to this node. This protocol is\nknown to complete information spreading in $O(\\log n)$ time steps with high\nprobability (w.h.p.) in several families of $n$-node \"static\" networks. The\nPush protocol has also been empirically shown to perform well in practice, and,\nspecifically, to be robust against dynamic topological changes.\n  In this paper, we aim at analyzing the Push protocol in \"dynamic\" networks.\nWe consider the \"edge-Markovian\" evolving graph model which captures natural\ntemporal dependencies between the structure of the network at time $t$, and the\none at time $t+1$. Precisely, a non-edge appears with probability $p$, while an\nexisting edge dies with probability $q$. In order to fit with real-world\ntraces, we mostly concentrate our study on the case where $p=\\Omega(1/n)$ and\n$q$ is constant. We prove that, in this realistic scenario, the Push protocol\ndoes perform well, completing information spreading in $O(\\log n)$ time steps\nw.h.p. Note that this performance holds even when the network is, w.h.p.,\ndisconnected at every time step (e.g., when $p << (\\log n) / n$). Our result\nprovides the first formal argument demonstrating the robustness of the Push\nprotocol against network changes. We also address other ranges of parameters\n$p$ and $q$ (e.g., $p+q=1$ with arbitrary $p$ and $q$, and $p=1/n$ with\narbitrary $q$). Although they do not precisely fit with the measures performed\non real-world traces, they can be of independent interest for other settings.\nThe results in these cases confirm the positive impact of dynamism.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 18:13:15 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Clementi", "Andrea", ""], ["Crescenzi", "Pierluigi", ""], ["Doerr", "Carola", ""], ["Fraigniaud", "Pierre", ""], ["Isopi", "Marco", ""], ["Panconesi", "Alessandro", ""], ["Pasquale", "Francesco", ""], ["Silvestri", "Riccardo", ""]]}, {"id": "1302.3860", "submitter": "M\\'arton Trencs\\'eni", "authors": "M\\'arton Trencs\\'eni, Attila Gazs\\'o", "title": "ScalienDB: Designing and Implementing a Distributed Database using Paxos", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ScalienDB is a scalable, replicated database built on top of the Paxos\nalgorithm. It was developed from 2010 to 2012, when the startup backing it\nfailed. This paper discusses the design decisions of the distributed database,\ndescribes interesting parts of the C++ codebase and enumerates lessons learned\nputting ScalienDB into production at a handful of clients. The source code is\navailable on Github under the AGPL license, but it is no longer developed or\nmaintained.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 20:04:10 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Trencs\u00e9ni", "M\u00e1rton", ""], ["Gazs\u00f3", "Attila", ""]]}, {"id": "1302.4059", "submitter": "Tomasz Jurdzinski", "authors": "Tomasz Jurdzinski, Dariusz R.Kowalski, Grzegorz Stachowiak", "title": "Distributed Deterministic Broadcasting in Uniform-Power Ad Hoc Wireless\n  Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1207.6732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of many futuristic technologies, such as MANET, VANET, iThings,\nnano-devices, depend on efficient distributed communication protocols in\nmulti-hop ad hoc networks. A vast majority of research in this area focus on\ndesign heuristic protocols, and analyze their performance by simulations on\nnetworks generated randomly or obtained in practical measurements of some\n(usually small-size) wireless networks. %some library. Moreover, they often\nassume access to truly random sources, which is often not reasonable in case of\nwireless devices. In this work we use a formal framework to study the problem\nof broadcasting and its time complexity in any two dimensional Euclidean\nwireless network with uniform transmission powers. For the analysis, we\nconsider two popular models of ad hoc networks based on the\nSignal-to-Interference-and-Noise Ratio (SINR): one with opportunistic links,\nand the other with randomly disturbed SINR. In the former model, we show that\none of our algorithms accomplishes broadcasting in $O(D\\log^2 n)$ rounds, where\n$n$ is the number of nodes and $D$ is the diameter of the network. If nodes\nknow a priori the granularity $g$ of the network, i.e., the inverse of the\nmaximum transmission range over the minimum distance between any two stations,\na modification of this algorithm accomplishes broadcasting in $O(D\\log g)$\nrounds.\n  Finally, we modify both algorithms to make them efficient in the latter model\nwith randomly disturbed SINR, with only logarithmic growth of performance.\n  Ours are the first provably efficient and well-scalable, under the two\nmodels, distributed deterministic solutions for the broadcast task.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2013 11:36:51 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Jurdzinski", "Tomasz", ""], ["Kowalski", "Dariusz R.", ""], ["Stachowiak", "Grzegorz", ""]]}, {"id": "1302.4085", "submitter": "Charng-Da Lu", "authors": "Charng-Da Lu", "title": "Comprehensive Resource Measurement and Analysis for HPC Systems with\n  TACC_Stats", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-performance computing (HPC) systems are a complex combination of\nsoftware, processors, memory, networks, and storage systems characterized by\nfrequent disruptive technological advances. Anomalous behavior has to be\nmanually diagnosed and remedied with incomplete and sparse data. It also has\nbeen effort-intensive for users to assess the effectiveness with which they are\nusing the available resources. The data available for system level analyses\nappear from multiple sources and in disparate formats (from Linux \"sysstat\" and\naccounting to scheduler/kernel logs). Sysstat does not resolve its measurements\nby job so that job-oriented analyses require individual measurements. There are\nmany user-oriented performance instrumentation and profiling tools but they\nrequire extensive system knowledge, code changes and recompilation, and thus\nare not widely used. To address this issue, we develop TACC_Stats, a\njob-oriented and logically structured version of the conventional Linux\n\"sysstat/sar\" system-wide performance monitor. We use TACC_Stats-collected data\nfrom a supercomputer \"Ranger\" to demonstrate its effectiveness in two case\nstudies.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2013 16:40:10 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Lu", "Charng-Da", ""]]}, {"id": "1302.4168", "submitter": "Ashwin Kayyoor", "authors": "K. Ashwin Kumar, Amol Deshpande, Samir Khuller", "title": "Data Placement and Replica Selection for Improving Co-location in\n  Distributed Environments", "comments": "12 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing need for large-scale data analytics in a number of application\ndomains has led to a dramatic rise in the number of distributed data management\nsystems, both parallel relational databases, and systems that support\nalternative frameworks like MapReduce. There is thus an increasing contention\non scarce data center resources like network bandwidth; further, the energy\nrequirements for powering the computing equipment are also growing\ndramatically. As we show empirically, increasing the execution parallelism by\nspreading out data across a large number of machines may achieve the intended\ngoal of decreasing query latencies, but in most cases, may increase the total\nresource and energy consumption significantly. For many analytical workloads,\nhowever, minimizing query latencies is often not critical; in such scenarios,\nwe argue that we should instead focus on minimizing the average query span,\ni.e., the average number of machines that are involved in processing of a\nquery, through colocation of data items that are frequently accessed together.\nIn this work, we exploit the fact that most distributed environments need to\nuse replication for fault tolerance, and we devise workload-driven replica\nselection and placement algorithms that attempt to minimize the average query\nspan. We model a historical query workload trace as a hypergraph over a set of\ndata items, and formulate and analyze the problem of replica placement by\ndrawing connections to several well-studied graph theoretic concepts. We\ndevelop a series of algorithms to decide which data items to replicate, and\nwhere to place the replicas. We show effectiveness of our proposed approach by\npresenting results on a collection of synthetic and real workloads. Our\nexperiments show that careful data placement and replication can dramatically\nreduce the average query spans resulting in significant reductions in the\nresource consumption.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 07:09:14 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Kumar", "K. Ashwin", ""], ["Deshpande", "Amol", ""], ["Khuller", "Samir", ""]]}, {"id": "1302.4280", "submitter": "Georg Hager", "authors": "Markus Wittmann and Georg Hager and Thomas Zeiser and Gerhard Wellein", "title": "Asynchronous MPI for the Masses", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple library which equips MPI implementations with truly\nasynchronous non-blocking point-to-point operations, and which is independent\nof the underlying communication infrastructure. It utilizes the MPI profiling\ninterface (PMPI) and the MPI_THREAD_MULTIPLE thread compatibility level, and\nworks with current versions of Intel MPI, Open MPI, MPICH2, MVAPICH2, Cray MPI,\nand IBM MPI. We show performance comparisons on a commodity InfiniBand cluster\nand two tier-1 systems in Germany, using low-level and application benchmarks.\nIssues of thread/process placement and the peculiarities of different MPI\nimplementations are discussed in detail. We also identify the MPI libraries\nthat already support asynchronous operations. Finally we show how our ideas can\nbe extended to MPI-IO.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 14:17:37 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Wittmann", "Markus", ""], ["Hager", "Georg", ""], ["Zeiser", "Thomas", ""], ["Wellein", "Gerhard", ""]]}, {"id": "1302.4332", "submitter": "Paolo Bientinesi", "authors": "Lucas Beyer (1), Paolo Bientinesi (1), ((1) AICES, RWTH Aachen)", "title": "Streaming Data from HDD to GPUs for Sustained Peak Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": "AICES-2013/02-1", "categories": "cs.DC cs.CE cs.MS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the genome-wide association studies (GWAS), one has to\nsolve long sequences of generalized least-squares problems; such a task has two\nlimiting factors: execution time --often in the range of days or weeks-- and\ndata management --data sets in the order of Terabytes. We present an algorithm\nthat obviates both issues. By pipelining the computation, and thanks to a\nsophisticated transfer strategy, we stream data from hard disk to main memory\nto GPUs and achieve sustained peak performance; with respect to a\nhighly-optimized CPU implementation, our algorithm shows a speedup of 2.6x.\nMoreover, the approach lends itself to multiple GPUs and attains almost perfect\nscalability. When using 4 GPUs, we observe speedups of 9x over the\naforementioned implementation, and 488x over a widespread biology library.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 16:03:08 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Beyer", "Lucas", "", "AICES, RWTH Aachen"], ["Bientinesi", "Paolo", "", "AICES, RWTH Aachen"]]}, {"id": "1302.4414", "submitter": "Paul Renaud-Goud", "authors": "Olivier Beaumont (LaBRI, INRIA Bordeaux - Sud-Ouest), Philippe Duchon\n  (LaBRI, INRIA Bordeaux - Sud-Ouest), Paul Renaud-Goud (LaBRI)", "title": "Approximation Algorithms for Energy Minimization in Cloud Service\n  Allocation under Reliability Constraints", "comments": null, "journal-ref": "N&deg; RR-8241 (2013)", "doi": null, "report-no": "RR-8241", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider allocation problems that arise in the context of service\nallocation in Clouds. More specifically, we assume on the one part that each\ncomputing resource is associated to a capacity constraint, that can be chosen\nusing Dynamic Voltage and Frequency Scaling (DVFS) method, and to a probability\nof failure. On the other hand, we assume that the service runs as a set of\nindependent instances of identical Virtual Machines. Moreover, there exists a\nService Level Agreement (SLA) between the Cloud provider and the client that\ncan be expressed as follows: the client comes with a minimal number of service\ninstances which must be alive at the end of the day, and the Cloud provider\noffers a list of pairs (price,compensation), this compensation being paid by\nthe Cloud provider if it fails to keep alive the required number of services.\nOn the Cloud provider side, each pair corresponds actually to a guaranteed\nsuccess probability of fulfilling the constraint on the minimal number of\ninstances. In this context, given a minimal number of instances and a\nprobability of success, the question for the Cloud provider is to find the\nnumber of necessary resources, their clock frequency and an allocation of the\ninstances (possibly using replication) onto machines. This solution should\nsatisfy all types of constraints during a given time period while minimizing\nthe energy consumption of used resources. We consider two energy consumption\nmodels based on DVFS techniques, where the clock frequency of physical\nresources can be changed. For each allocation problem and each energy model, we\nprove deterministic approximation ratios on the consumed energy for algorithms\nthat provide guaranteed probability failures, as well as an efficient\nheuristic, whose energy ratio is not guaranteed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 20:33:15 GMT"}, {"version": "v2", "created": "Thu, 23 May 2013 08:13:01 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Beaumont", "Olivier", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Duchon", "Philippe", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Renaud-Goud", "Paul", "", "LaBRI"]]}, {"id": "1302.4519", "submitter": "Quang-Hung Nguyen", "authors": "Nguyen Quang-Hung, Pham Dac Nien, Nguyen Hoai Nam, Nguyen Huynh Tuong,\n  Nam Thoai", "title": "A Genetic Algorithm for Power-Aware Virtual Machine Allocation in\n  Private Cloud", "comments": "10 pages", "journal-ref": "Information and Communication Technology, Lecture Notes in\n  Computer Science, Vol. 7804, Information Systems and Applications, incl.\n  Internet/Web, and HCI, IFIP-LNCS Volumes, ISBN 978-3-642-36817-2, 2013, XVI,\n  552 p. 170 illus", "doi": null, "report-no": null, "categories": "cs.NE cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Energy efficiency has become an important measurement of scheduling algorithm\nfor private cloud. The challenge is trade-off between minimizing of energy\nconsumption and satisfying Quality of Service (QoS) (e.g. performance or\nresource availability on time for reservation request). We consider resource\nneeds in context of a private cloud system to provide resources for\napplications in teaching and researching. In which users request computing\nresources for laboratory classes at start times and non-interrupted duration in\nsome hours in prior. Many previous works are based on migrating techniques to\nmove online virtual machines (VMs) from low utilization hosts and turn these\nhosts off to reduce energy consumption. However, the techniques for migration\nof VMs could not use in our case. In this paper, a genetic algorithm for\npower-aware in scheduling of resource allocation (GAPA) has been proposed to\nsolve the static virtual machine allocation problem (SVMAP). Due to limited\nresources (i.e. memory) for executing simulation, we created a workload that\ncontains a sample of one-day timetable of lab hours in our university. We\nevaluate the GAPA and a baseline scheduling algorithm (BFD), which sorts list\nof virtual machines in start time (i.e. earliest start time first) and using\nbest-fit decreasing (i.e. least increased power consumption) algorithm, for\nsolving the same SVMAP. As a result, the GAPA algorithm obtains total energy\nconsumption is lower than the baseline algorithm on simulated experimentation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 05:12:24 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Quang-Hung", "Nguyen", ""], ["Nien", "Pham Dac", ""], ["Nam", "Nguyen Hoai", ""], ["Tuong", "Nguyen Huynh", ""], ["Thoai", "Nam", ""]]}, {"id": "1302.4544", "submitter": "Danupon Nanongkai", "authors": "Atish Das Sarma, Danupon Nanongkai, Gopal Pandurangan, Prasad Tetali", "title": "Distributed Random Walks", "comments": "Preprint of an article to appear in Journal of the ACM in February\n  2013. The official journal version has several gramatical corrections.\n  Preliminary versions of this paper appeared in PODC 2009 and PODC 2010. arXiv\n  admin note: substantial text overlap with arXiv:0911.3195, arXiv:1205.5525", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing random walks in networks is a fundamental primitive that has found\napplications in many areas of computer science, including distributed\ncomputing. In this paper, we focus on the problem of sampling random walks\nefficiently in a distributed network and its applications. Given bandwidth\nconstraints, the goal is to minimize the number of rounds required to obtain\nrandom walk samples.\n  All previous algorithms that compute a random walk sample of length $\\ell$ as\na subroutine always do so naively, i.e., in $O(\\ell)$ rounds. The main\ncontribution of this paper is a fast distributed algorithm for performing\nrandom walks. We present a sublinear time distributed algorithm for performing\nrandom walks whose time complexity is sublinear in the length of the walk. Our\nalgorithm performs a random walk of length $\\ell$ in $\\tilde{O}(\\sqrt{\\ell D})$\nrounds ($\\tilde{O}$ hides $\\polylog{n}$ factors where $n$ is the number of\nnodes in the network) with high probability on an undirected network, where $D$\nis the diameter of the network. For small diameter graphs, this is a\nsignificant improvement over the naive $O(\\ell)$ bound. Furthermore, our\nalgorithm is optimal within a poly-logarithmic factor as there exists a\nmatching lower bound [Nanongkai et al. PODC 2011]. We further extend our\nalgorithms to efficiently perform $k$ independent random walks in\n$\\tilde{O}(\\sqrt{k\\ell D} + k)$ rounds. We also show that our algorithm can be\napplied to speedup the more general Metropolis-Hastings sampling.\n  Our random walk algorithms can be used to speed up distributed algorithms in\napplications that use random walks as a subroutine, such as computing a random\nspanning tree and estimating mixing time and related parameters. Our algorithm\nis fully decentralized and can serve as a building block in the design of\ntopologically-aware networks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 08:36:18 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Sarma", "Atish Das", ""], ["Nanongkai", "Danupon", ""], ["Pandurangan", "Gopal", ""], ["Tetali", "Prasad", ""]]}, {"id": "1302.4558", "submitter": "Guillaume Aupy", "authors": "Guillaume Aupy, Yves Robert, Fr\\'ed\\'eric Vivien, Dounia Zaidouni", "title": "Checkpointing strategies with prediction windows", "comments": "35 pages, work supported by ANR Rescue. arXiv admin note: substantial\n  text overlap with arXiv:1207.6936, arXiv:1302.3752", "journal-ref": null, "doi": null, "report-no": "INRIA RR-8239", "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the impact of fault prediction techniques on\ncheckpointing strategies. We suppose that the fault-prediction system provides\nprediction windows instead of exact predictions, which dramatically complicates\nthe analysis of the checkpointing strategies. We propose a new approach based\nupon two periodic modes, a regular mode outside prediction windows, and a\nproactive mode inside prediction windows, whenever the size of these windows is\nlarge enough. We are able to compute the best period for any size of the\nprediction windows, thereby deriving the scheduling strategy that minimizes\nplatform waste. In addition, the results of this analytical evaluation are\nnicely corroborated by a comprehensive set of simulations, which demonstrate\nthe validity of the model and the accuracy of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 09:50:38 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Aupy", "Guillaume", ""], ["Robert", "Yves", ""], ["Vivien", "Fr\u00e9d\u00e9ric", ""], ["Zaidouni", "Dounia", ""]]}, {"id": "1302.4587", "submitter": "Christian Schulz", "authors": "Marcel Birn, Vitaly Osipov, Peter Sanders, Christian Schulz, Nodari\n  Sitchinava", "title": "Efficient Parallel and External Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple algorithm for computing a matching on a graph runs in a\nlogarithmic number of phases incurring work linear in the input size. The\nalgorithm can be adapted to provide efficient algorithms in several models of\ncomputation, such as PRAM, External Memory, MapReduce and distributed memory\nmodels. Our CREW PRAM algorithm is the first O(log^2 n) time, linear work\nalgorithm. Our experimental results indicate the algorithm's high speed and\nefficiency combined with good solution quality.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 12:13:10 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2014 16:09:09 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Birn", "Marcel", ""], ["Osipov", "Vitaly", ""], ["Sanders", "Peter", ""], ["Schulz", "Christian", ""], ["Sitchinava", "Nodari", ""]]}, {"id": "1302.4617", "submitter": "Olivier Coulaud", "authors": "Olivier Coulaud (INRIA Bordeaux - Sud-Ouest), Patrice Bordat (IPREM),\n  Pierre Fayon (IPREM), Vincent Lebris (IPREM), Isabelle Baraille (IPREM), Ross\n  Brown (IPREM)", "title": "Extensions of the siesta dft code for simulation of molecules", "comments": null, "journal-ref": "N&deg; RR-8221 (2013)", "doi": null, "report-no": "RR-8221", "categories": "physics.comp-ph cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe extensions to the siesta density functional theory (dft) code\n[30], for the simulation of isolated molecules and their absorption spectra.\nThe extensions allow for: - Use of a multi-grid solver for the Poisson equation\non a finite dft mesh. Non-periodic, Dirichlet boundary conditions are computed\nby expansion of the electric multipoles over spherical harmonics. - Truncation\nof a molecular system by the method of design atom pseudo- potentials of Xiao\nand Zhang[32]. - Electrostatic potential fitting to determine effective atomic\ncharges. - Derivation of electronic absorption transition energies and\noscillator stren- gths from the raw spectra produced by a recently described,\norder O(N3), time-dependent dft code[21]. The code is furthermore integrated\nwithin siesta as a post-processing option.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 14:29:25 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Coulaud", "Olivier", "", "INRIA Bordeaux - Sud-Ouest"], ["Bordat", "Patrice", "", "IPREM"], ["Fayon", "Pierre", "", "IPREM"], ["Lebris", "Vincent", "", "IPREM"], ["Baraille", "Isabelle", "", "IPREM"], ["Brown", "Ross", "", "IPREM"]]}, {"id": "1302.4760", "submitter": "Lauro Costa", "authors": "Lauro Beltr\\~ao Costa and Abmar Barros and Samer Al-Kiswany and Hao\n  Yang and Emalayan Vairavanathan and Matei Ripeanu", "title": "Predicting Intermediate Storage Performance for Workflow Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": "NetSysLab - TR 2013/02", "categories": "cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Configuring a storage system to better serve an application is a challenging\ntask complicated by a multidimensional, discrete configuration space and the\nhigh cost of space exploration (e.g., by running the application with different\nstorage configurations). To enable selecting the best configuration in a\nreasonable time, we design an end-to-end performance prediction mechanism that\nestimates the turn-around time of an application using storage system under a\ngiven configuration. This approach focuses on a generic object-based storage\nsystem design, supports exploring the impact of optimizations targeting\nworkflow applications (e.g., various data placement schemes) in addition to\nother, more traditional, configuration knobs (e.g., stripe size or replication\nlevel), and models the system operation at data-chunk and control message\nlevel.\n  This paper presents our experience to date with designing and using this\nprediction mechanism. We evaluate this mechanism using micro- as well as\nsynthetic benchmarks mimicking real workflow applications, and a real\napplication.. A preliminary evaluation shows that we are on a good track to\nmeet our objectives: it can scale to model a workflow application run on an\nentire cluster while offering an over 200x speedup factor (normalized by\nresource) compared to running the actual application, and can achieve, in the\nlimited number of scenarios we study, a prediction accuracy that enables\nidentifying the best storage system configuration.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 21:39:16 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2013 04:09:34 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Costa", "Lauro Beltr\u00e3o", ""], ["Barros", "Abmar", ""], ["Al-Kiswany", "Samer", ""], ["Yang", "Hao", ""], ["Vairavanathan", "Emalayan", ""], ["Ripeanu", "Matei", ""]]}, {"id": "1302.4779", "submitter": "Charng-Da Lu", "authors": "Charng-Da Lu", "title": "Failure Data Analysis of HPC Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Continuous availability of HPC systems built from commodity components have\nbecome a primary concern as system size grows to thousands of processors. In\nthis paper, we present the analysis of 8-24 months of real failure data\ncollected from three HPC systems at the National Center for Supercomputing\nApplications (NCSA) during 2001-2004. The results show that the availability is\n98.7-99.8% and most outages are due to software halts. On the other hand, the\ndowntime are mostly contributed by hardware halts or scheduled maintenance. We\nalso used failure clustering analysis to identify several correlated failures.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 00:21:44 GMT"}], "update_date": "2013-02-21", "authors_parsed": [["Lu", "Charng-Da", ""]]}, {"id": "1302.4808", "submitter": "Christian Cachin", "authors": "Christian Cachin and Olga Ohrimenko", "title": "Verifying the Consistency of Remote Untrusted Services with\n  Conflict-Free Operations", "comments": "A predecessor of this paper with a slightly different title appears\n  in the proceedings of OPODIS 2014, Lecture Notes in Computer Science,\n  vol.~8878, Springer, 2014", "journal-ref": null, "doi": "10.1007/978-3-319-14472-6_1", "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A group of mutually trusting clients outsources a computation service to a\nremote server, which they do not fully trust and that may be subject to\nattacks. The clients do not communicate with each other and would like to\nverify the correctness of the remote computation and the consistency of the\nserver's responses. This paper presents the Conflict-free Operation\nverification Protocol (COP) that ensures linearizability when the server is\ncorrect and preserves fork-linearizability in any other case. All clients that\nobserve each other's operations are consistent, in the sense that their own\noperations and those operations of other clients that they see are\nlinearizable. If the server forks two clients by hiding an operation, these\nclients never again see operations of each other. COP supports wait-free client\noperations in the sense that when executed with a correct server,\nnon-conflicting operations can run without waiting for other clients, allowing\nmore parallelism than earlier protocols. A conflict arises when an operation\ncauses a subsequent operation to produce a different output value for the\nclient who runs it. The paper gives a precise model for the guarantees of COP\nand includes a formal analysis that these are achieved.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 05:22:35 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 08:41:17 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 20:37:21 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 13:13:10 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cachin", "Christian", ""], ["Ohrimenko", "Olga", ""]]}, {"id": "1302.5161", "submitter": "Hengfeng Wei", "authors": "Hengfeng Wei, Marzio De Biasi, Yu Huang, Jiannong Cao, Jian Lu", "title": "Verifying PRAM Consistency over Read/Write Traces of Data Replicas", "comments": "11 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data replication technologies enable efficient and highly-available data\naccess, thus gaining more and more interests in both the academia and the\nindustry. However, data replication introduces the problem of data consistency.\nModern commercial data replication systems often provide weak consistency for\nhigh availability under certain failure scenarios. An important weak\nconsistency is Pipelined-RAM (PRAM) consistency. It allows different processes\nto hold different views of data. To determine whether a data replication system\nindeed provides PRAM consistency, we study the problem of Verifying PRAM\nConsistency over read/write traces (or VPC, for short).\n  We first identify four variants of VPC according to a) whether there are\nMultiple shared variables (or one Single variable), and b) whether write\noperations can assign Duplicate values (or only Unique values) for each shared\nvariable; the four variants are labeled VPC-SU, VPC-MU, VPC-SD, and VPC-MD.\nSecond, we present a simple VPC-MU algorithm, called RW-CLOSURE. It constructs\nan operation graph $\\mathcal{G}$ by iteratively adding edges according to three\nrules. Its time complexity is $O(n^5)$, where n is the number of operations in\nthe trace. Third, we present an improved VPC-MU algorithm, called READ-CENTRIC,\nwith time complexity $O(n^4)$. Basically it attempts to construct the operation\ngraph $\\mathcal{G}$ in an incremental and efficient way. Its correctness is\nbased on that of RW-CLOSURE. Finally, we prove that VPC-SD (so is VPC-MD) is\n$\\sf{NP}$-complete by reducing the strongly $\\sf{NP}$-complete problem\n3-PARTITION to it.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 02:45:25 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2013 13:07:44 GMT"}, {"version": "v3", "created": "Fri, 29 Nov 2013 02:25:43 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Wei", "Hengfeng", ""], ["De Biasi", "Marzio", ""], ["Huang", "Yu", ""], ["Cao", "Jiannong", ""], ["Lu", "Jian", ""]]}, {"id": "1302.5192", "submitter": "Kyumars Sheykh Esmaili", "authors": "Kyumars Sheykh Esmaili, Lluis Pamies-Juarez, Anwitaman Datta", "title": "The CORE Storage Primitive: Cross-Object Redundancy for Efficient Data\n  Repair & Access in Erasure Coded Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erasure codes are an integral part of many distributed storage systems aimed\nat Big Data, since they provide high fault-tolerance for low overheads.\nHowever, traditional erasure codes are inefficient on reading stored data in\ndegraded environments (when nodes might be unavailable), and on replenishing\nlost data (vital for long term resilience). Consequently, novel codes optimized\nto cope with distributed storage system nuances are vigorously being\nresearched. In this paper, we take an engineering alternative, exploring the\nuse of simple and mature techniques -juxtaposing a standard erasure code with\nRAID-4 like parity. We carry out an analytical study to determine the efficacy\nof this approach over traditional as well as some novel codes. We build upon\nthis study to design CORE, a general storage primitive that we integrate into\nHDFS. We benchmark this implementation in a proprietary cluster and in EC2. Our\nexperiments show that compared to traditional erasure codes, CORE uses 50% less\nbandwidth and is up to 75% faster while recovering a single failed node, while\nthe gains are respectively 15% and 60% for double node failures.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 06:14:04 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 13:59:05 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2013 12:49:47 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2013 05:32:41 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Esmaili", "Kyumars Sheykh", ""], ["Pamies-Juarez", "Lluis", ""], ["Datta", "Anwitaman", ""]]}, {"id": "1302.5371", "submitter": "Sivaraman Dasarathan", "authors": "Sivaraman Dasarathan, Cihan Tepedelenlioglu, Mahesh Banavar, and\n  Andreas Spanias", "title": "Non-Linear Distributed Average Consensus using Bounded Transmissions", "comments": "24 pages, 8 figures, Submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2013.2282912", "report-no": null, "categories": "cs.DC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed average consensus algorithm in which every sensor transmits\nwith bounded peak power is proposed. In the presence of communication noise, it\nis shown that the nodes reach consensus asymptotically to a finite random\nvariable whose expectation is the desired sample average of the initial\nobservations with a variance that depends on the step size of the algorithm and\nthe variance of the communication noise. The asymptotic performance is\ncharacterized by deriving the asymptotic covariance matrix using results from\nstochastic approximation theory. It is shown that using bounded transmissions\nresults in slower convergence compared to the linear consensus algorithm based\non the Laplacian heuristic. Simulations corroborate our analytical findings.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2013 18:42:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Dasarathan", "Sivaraman", ""], ["Tepedelenlioglu", "Cihan", ""], ["Banavar", "Mahesh", ""], ["Spanias", "Andreas", ""]]}, {"id": "1302.5481", "submitter": "Harshad Prajapati", "authors": "Harshad B. Prajapati and Vipul K. Dabhi", "title": "Classification and Characterization of Core Grid Protocols for Global\n  Grid Computing", "comments": "Survey paper having 23 Pages, 9 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing has attracted many researchers over a few years, and as a\nresult many new protocols have emerged and also evolved since its inception a\ndecade ago. Grid protocols play major role in implementing services that\nfacilitate coordinated resource sharing across diverse organizations. In this\npaper, we provide comprehensive coverage of different core Grid protocols that\ncan be used in Global Grid Computing. We establish the classification of core\nGrid protocols into i) Grid network communication and Grid data transfer\nprotocols, ii) Grid information security protocols, iii) Grid resource\ninformation protocols, iv) Grid management protocols, and v) Grid interface\nprotocols, depending upon the kind of activities handled by these protocols.\nAll the classified protocols are also organized into layers of the Hourglass\nmodel of Grid architecture to understand dependency among these protocols. We\nalso present the characteristics of each protocol. For better understanding of\nthese protocols, we also discuss applied protocols as examples from either\nGlobus toolkit or other popular Grid middleware projects. We believe that our\nclassification and characterization of Grid protocols will enable better\nunderstanding of core Grid protocols and will motivate further research in the\narea of Global Grid Computing.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 04:20:36 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Prajapati", "Harshad B.", ""], ["Dabhi", "Vipul K.", ""]]}, {"id": "1302.5518", "submitter": "Lluis Pamies-Juarez", "authors": "Lluis Pamies-Juarez, Henk D.L. Hollmann, Fr\\'ed\\'erique Oggier", "title": "Locally Repairable Codes with Multiple Repair Alternatives", "comments": "IEEE International Symposium on Information Theory (ISIT 2013)", "journal-ref": null, "doi": "10.1109/ISIT.2013.6620355", "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed storage systems need to store data redundantly in order to\nprovide some fault-tolerance and guarantee system reliability. Different coding\ntechniques have been proposed to provide the required redundancy more\nefficiently than traditional replication schemes. However, compared to\nreplication, coding techniques are less efficient for repairing lost\nredundancy, as they require retrieval of larger amounts of data from larger\nsubsets of storage nodes. To mitigate these problems, several recent works have\npresented locally repairable codes designed to minimize the repair traffic and\nthe number of nodes involved per repair. Unfortunately, existing methods often\nlead to codes where there is only one subset of nodes able to repair a piece of\nlost data, limiting the local repairability to the availability of the nodes in\nthis subset. In this paper, we present a new family of locally repairable codes\nthat allows different trade-offs between the number of contacted nodes per\nrepair, and the number of different subsets of nodes that enable this repair.\nWe show that slightly increasing the number of contacted nodes per repair\nallows to have repair alternatives, which in turn increases the probability of\nbeing able to perform efficient repairs. Finally, we present pg-BLRC, an\nexplicit construction of locally repairable codes with multiple repair\nalternatives, constructed from partial geometries, in particular from\nGeneralized Quadrangles. We show how these codes can achieve practical lengths\nand high rates, while requiring a small number of nodes per repair, and\nproviding multiple repair alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 08:40:12 GMT"}, {"version": "v2", "created": "Wed, 15 May 2013 04:06:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Pamies-Juarez", "Lluis", ""], ["Hollmann", "Henk D. L.", ""], ["Oggier", "Fr\u00e9d\u00e9rique", ""]]}, {"id": "1302.5586", "submitter": "Riyadh Baghdadi", "authors": "Riyadh Baghdadi, Albert Cohen, Serge Guelton, Sven Verdoolaege, Jun\n  Inoue, Tobias Grosser, Georgia Kouveli, Alexey Kravets, Anton Lokhmotov,\n  Cedric Nugteren, Fraser Waters, Alastair F. Donaldson", "title": "PENCIL: Towards a Platform-Neutral Compute Intermediate Language for\n  DSLs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate the design and implementation of a platform-neutral compute\nintermediate language (PENCIL) for productive and performance-portable\naccelerator programming.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 13:43:12 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Cohen", "Albert", ""], ["Guelton", "Serge", ""], ["Verdoolaege", "Sven", ""], ["Inoue", "Jun", ""], ["Grosser", "Tobias", ""], ["Kouveli", "Georgia", ""], ["Kravets", "Alexey", ""], ["Lokhmotov", "Anton", ""], ["Nugteren", "Cedric", ""], ["Waters", "Fraser", ""], ["Donaldson", "Alastair F.", ""]]}, {"id": "1302.5607", "submitter": "Andrea Clementi", "authors": "Andrea Clementi and Miriam di Ianni and Giorgio Gambosi and Emanuele\n  Natale and Riccardo Silvestri", "title": "Distributed Community Detection in Dynamic Graphs", "comments": "Version II", "journal-ref": null, "doi": "10.1007/978-3-319-03578-9_1", "report-no": null, "categories": "cs.SI cs.DC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the increasing interest in self-organizing social opportunistic\nnetworks, we investigate the problem of distributed detection of unknown\ncommunities in dynamic random graphs. As a formal framework, we consider the\ndynamic version of the well-studied \\emph{Planted Bisection Model}\n$\\sdG(n,p,q)$ where the node set $[n]$ of the network is partitioned into two\nunknown communities and, at every time step, each possible edge $(u,v)$ is\nactive with probability $p$ if both nodes belong to the same community, while\nit is active with probability $q$ (with $q<<p$) otherwise. We also consider a\ntime-Markovian generalization of this model.\n  We propose a distributed protocol based on the popular \\emph{Label\nPropagation Algorithm} and prove that, when the ratio $p/q$ is larger than\n$n^{b}$ (for an arbitrarily small constant $b>0$), the protocol finds the right\n\"planted\" partition in $O(\\log n)$ time even when the snapshots of the dynamic\ngraph are sparse and disconnected (i.e. in the case $p=\\Theta(1/n)$).\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 14:32:33 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2013 15:51:55 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Clementi", "Andrea", ""], ["di Ianni", "Miriam", ""], ["Gambosi", "Giorgio", ""], ["Natale", "Emanuele", ""], ["Silvestri", "Riccardo", ""]]}, {"id": "1302.5646", "submitter": "Pranava Jha", "authors": "Pranava K. Jha", "title": "Comments on \"Resource placement in Cartesian product of networks\"\n  [Imani, Sarbazi-Azad and Zomaya, J. Parallel Distrib. Comput., 70 (2010)\n  481-495]", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present note points out a number of errors, omissions, redundancies and\narbitrary deviations from the standard terminology in the paper \"Resource\nplacement in Cartesian product of networks,\" by N. Imani, H. Sarbazi-Azad and\nA.Y. Zomaya [J. Parallel Distrib. Comput. 70 (2010) 481-495].\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 17:51:31 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Jha", "Pranava K.", ""]]}, {"id": "1302.5657", "submitter": "Bernat Gaston", "authors": "Bernat Gast\\'on, Jaume Pujol, Merc\\`e Villanueva", "title": "A realistic distributed storage system: the rack model", "comments": "Submitted to IEEE Transactions on Information Theory. arXiv admin\n  note: text overlap with arXiv:1301.1549", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a realistic distributed storage environment, storage nodes are usually\nplaced in racks, a metallic support designed to accommodate electronic\nequipment. It is known that the communication (bandwidth) cost between nodes\nwhich are in the same rack is much lower than between nodes which are in\ndifferent racks.\n  In this paper, a new model, where the storage nodes are placed in two racks,\nis proposed and analyzed. Moreover, the two-rack model is generalized to any\nnumber of racks. In this model, the storage nodes have different repair costs\ndepending on the rack where they are placed. A threshold function, which\nminimizes the amount of stored data per node and the bandwidth needed to\nregenerate a failed node, is shown. This threshold function generalizes the\nones given for previous distributed storage models. The tradeoff curve obtained\nfrom this threshold function is compared with the ones obtained from the\nprevious models, and it is shown that this new model outperforms the previous\nones in terms of repair cost.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 17:35:50 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Gast\u00f3n", "Bernat", ""], ["Pujol", "Jaume", ""], ["Villanueva", "Merc\u00e8", ""]]}, {"id": "1302.5679", "submitter": "Juliana  Silva", "authors": "Juliana M. N. Silva and Cristina Boeres and L\\'ucia M. A. Drummond and\n  Artur A. Pessoa", "title": "Memory Aware Load Balance Strategy on a Parallel Branch-and-Bound\n  Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest trends in high-performance computing systems show an increasing\ndemand on the use of a large scale multicore systems in a efficient way, so\nthat high compute-intensive applications can be executed reasonably well.\nHowever, the exploitation of the degree of parallelism available at each\nmulticore component can be limited by the poor utilization of the memory\nhierarchy available. Actually, the multicore architecture introduces some\ndistinct features that are already observed in shared memory and distributed\nenvironments. One example is that subsets of cores can share different subsets\nof memory. In order to achieve high performance it is imperative that a careful\nallocation scheme of an application is carried out on the available cores,\nbased on a scheduling model that considers the main performance bottlenecks, as\nfor example, memory contention. In this paper, the {\\em Multicore Cluster\nModel} (MCM) is proposed, which captures the most relevant performance\ncharacteristics in multicores systems such as the influence of memory hierarchy\nand contention. Better performance was achieved when a load balance strategy\nfor a Branch-and-Bound application applied to the Partitioning Sets Problem is\nbased on MCM, showing its efficiency and applicability to modern systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2013 19:17:13 GMT"}], "update_date": "2013-02-25", "authors_parsed": [["Silva", "Juliana M. N.", ""], ["Boeres", "Cristina", ""], ["Drummond", "L\u00facia M. A.", ""], ["Pessoa", "Artur A.", ""]]}, {"id": "1302.5798", "submitter": "EPTCS", "authors": "Simon Gay, Paul Kelly", "title": "Proceedings Fifth Workshop on Programming Language Approaches to\n  Concurrency- and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 109, 2013", "doi": "10.4204/EPTCS.109", "report-no": null, "categories": "cs.PL cs.DC cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PLACES 2012 (full title: Programming Language Approaches to Concurrency- and\nCommunication-Centric Software) is the fifth edition of the PLACES workshop\nseries. After the first PLACES, which was affiliated to DisCoTec in 2008, the\nworkshop has been part of ETAPS every year since 2009 and is now an established\npart of the ETAPS satellite events. PLACES 2012 was held on 31st March in\nTallinn, Estonia.\n  The workshop series was started in order to promote the application of novel\nprogramming language ideas to the increasingly important problem of developing\nsoftware for systems in which concurrency and communication are intrinsic\naspects. This includes software for both multi-core systems and large-scale\ndistributed and/or service-oriented systems. The scope of PLACES includes new\nprogramming language features, whole new programming language designs, new type\nsystems, new semantic approaches, new program analysis techniques, and new\nimplementation mechanisms.\n  This year's call for papers attracted 17 submissions, from which the\nprogramme committee selected 10 papers for presentation at the workshop. After\nthe workshop, all of the authors were invited to produce revised versions of\ntheir papers for inclusion in the EPTCS proceedings. The authors of six papers\naccepted the invitation, and those papers constitute the present volume.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 13:44:36 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Gay", "Simon", ""], ["Kelly", "Paul", ""]]}, {"id": "1302.5851", "submitter": "Matthew Felice Pace", "authors": "Matthew Felice Pace and Alexander Tiskin", "title": "Parallel Suffix Array Construction by Accelerated Sampling", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deterministic BSP algorithm for constructing the suffix array of a given\nstring is presented, based on a technique which we call accelerated sampling.\nIt runs in optimal O(n/p) local computation and communication, and requires a\nnear optimal O(log log p) synchronisation steps. The algorithm provides an\nimprovement over the synchronisation costs of existing algorithms, and\nreinforces the importance of the sampling technique.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2013 22:39:03 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Pace", "Matthew Felice", ""], ["Tiskin", "Alexander", ""]]}, {"id": "1302.5872", "submitter": "K. V. Rashmi", "authors": "K. V. Rashmi, Nihar B. Shah, Kannan Ramchandran", "title": "A Piggybacking Design Framework for Read-and Download-efficient\n  Distributed Storage Codes", "comments": "Extended version of ISIT 2013 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DC cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new 'piggybacking' framework for designing distributed storage\ncodes that are efficient in data-read and download required during node-repair.\nWe illustrate the power of this framework by constructing classes of explicit\ncodes that entail the smallest data-read and download for repair among all\nexisting solutions for three important settings: (a) codes meeting the\nconstraints of being Maximum-Distance-Separable (MDS), high-rate and having a\nsmall number of substripes, arising out of practical considerations for\nimplementation in data centers, (b) binary MDS codes for all parameters where\nbinary MDS codes exist, (c) MDS codes with the smallest repair-locality. In\naddition, we employ this framework to enable efficient repair of parity nodes\nin existing codes that were originally constructed to address the repair of\nonly the systematic nodes. The basic idea behind our framework is to take\nmultiple instances of existing codes and add carefully designed functions of\nthe data of one instance to the other. Typical savings in data-read during\nrepair is 25% to 50% depending on the choice of the code parameters.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2013 05:20:03 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Rashmi", "K. V.", ""], ["Shah", "Nihar B.", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1302.5999", "submitter": "Srimugunthan Dhandapani", "authors": "Srimugunthan, K. Gopinath", "title": "Distributed Wear levelling of Flash Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large scale distributed storage systems, flash memories are an excellent\nchoice because flash memories consume less power, take lesser floor space for a\ntarget throughput and provide faster access to data. In a traditional\ndistributed filesystem, even distribution is required to ensure load-balancing,\nbalanced space utilisation and failure tolerance. In the presence of flash\nmemories, in addition, we should also ensure that the number of writes to these\ndifferent flash storage nodes are evenly distributed, to ensure even wear of\nflash storage nodes, so that unpredictable failures of storage nodes are\navoided. This requires that we distribute updates and do garbage collection,\nacross the flash storage nodes. We have motivated the distributed wearlevelling\nproblem considering the replica placement algorithm for HDFS. Viewing the\nwearlevelling across flash storage nodes as a distributed co-ordination\nproblem, we present an alternate design, to reduce the message communication\ncost across participating nodes. We demonstrate the effectiveness of our design\nthrough simulation\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 05:55:57 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Srimugunthan", "", ""], ["Gopinath", "K.", ""]]}, {"id": "1302.6224", "submitter": "Hammurabi Mendes", "authors": "Hammurabi Mendes, Christine Tasson, Maurice Herlihy", "title": "Distributed Computability in Byzantine Asynchronous Systems", "comments": "Will appear at the Proceedings of the 46th Annual Symposium on the\n  Theory of Computing, STOC 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we extend the topology-based approach for characterizing\ncomputability in asynchronous crash-failure distributed systems to asynchronous\nByzantine systems. We give the first theorem with necessary and sufficient\nconditions to solve arbitrary tasks in asynchronous Byzantine systems where an\nadversary chooses faulty processes. In our adversarial formulation, outputs of\nnon-faulty processes are constrained in terms of inputs of non-faulty processes\nonly. For colorless tasks, an important subclass of distributed problems, the\ngeneral result reduces to an elegant model that effectively captures the\nrelation between the number of processes, the number of failures, as well as\nthe topological structure of the task's simplicial complexes.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 20:50:25 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 19:49:01 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2013 21:30:46 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 14:40:27 GMT"}, {"version": "v5", "created": "Tue, 10 Jun 2014 17:38:42 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Mendes", "Hammurabi", ""], ["Tasson", "Christine", ""], ["Herlihy", "Maurice", ""]]}, {"id": "1302.6243", "submitter": "George Giakkoupis", "authors": "George Giakkoupis", "title": "Tight Bounds for Rumor Spreading with Vertex Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a bound for the classic PUSH-PULL rumor spreading protocol on\narbitrary graphs, in terms of the vertex expansion of the graph. We show that\nO(log^2(n)/\\alpha) rounds suffice with high probability to spread a rumor from\na single node to all n nodes, in any graph with vertex expansion at least\n\\alpha. This bound matches the known lower bound, and settles the question on\nthe relationship between rumor spreading and vertex expansion asked by\nChierichetti, Lattanzi, and Panconesi (SODA 2010). Further, some of the\narguments used in the proof may be of independent interest, as they give new\ninsights, for example, on how to choose a small set of nodes in which to plant\nthe rumor initially, to guarantee fast rumor spreading.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 21:00:47 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Giakkoupis", "George", ""]]}, {"id": "1302.6256", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, David F. Gleich, Assefaw H. Gebremedhin, Md. Mostofa\n  Ali Patwary", "title": "Parallel Maximum Clique Algorithms with Applications to Network Analysis\n  and Storage", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DC cs.DM cs.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast, parallel maximum clique algorithm for large sparse graphs\nthat is designed to exploit characteristics of social and information networks.\nThe method exhibits a roughly linear runtime scaling over real-world networks\nranging from 1000 to 100 million nodes. In a test on a social network with 1.8\nbillion edges, the algorithm finds the largest clique in about 20 minutes. Our\nmethod employs a branch and bound strategy with novel and aggressive pruning\ntechniques. For instance, we use the core number of a vertex in combination\nwith a good heuristic clique finder to efficiently remove the vast majority of\nthe search space. In addition, we parallelize the exploration of the search\ntree. During the search, processes immediately communicate changes to upper and\nlower bounds on the size of maximum clique, which occasionally results in a\nsuper-linear speedup because vertices with large search spaces can be pruned by\nother processes. We apply the algorithm to two problems: to compute temporal\nstrong components and to compress graphs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 21:16:13 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2013 02:00:15 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Gleich", "David F.", ""], ["Gebremedhin", "Assefaw H.", ""], ["Patwary", "Md. Mostofa Ali", ""]]}, {"id": "1302.6267", "submitter": "Shams Zawoad", "authors": "Shams Zawoad, Amit Kumar Dutta, Ragib Hasan", "title": "SecLaaS: Secure Logging-as-a-Service for Cloud Forensics", "comments": "To appear at the 8th ACM Symposium on Information, Computer and\n  Communications Security (ASIACCS), 2013. (Acceptance rate: 16.2%)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing has emerged as a popular computing paradigm in recent years.\nHowever, today's cloud computing architectures often lack support for computer\nforensic investigations. Analyzing various logs (e.g., process logs, network\nlogs) plays a vital role in computer forensics. Unfortunately, collecting logs\nfrom a cloud is very hard given the black-box nature of clouds and the\nmulti-tenant cloud models, where many users share the same processing and\nnetwork resources. Researchers have proposed using log API or cloud management\nconsole to mitigate the challenges of collecting logs from cloud\ninfrastructure. However, there has been no concrete work, which shows how to\nprovide cloud logs to investigator while preserving users' privacy and\nintegrity of the logs. In this paper, we introduce Secure-Logging-as-a-Service\n(SecLaaS), which stores virtual machines' logs and provides access to forensic\ninvestigators ensuring the confidentiality of the cloud users. Additionally,\nSeclaaS preserves proofs of past log and thus protects the integrity of the\nlogs from dishonest investigators or cloud providers. Finally, we evaluate the\nfeasibility of the scheme by implementing SecLaaS for network access logs in\nOpenStack - a popular open source cloud platform.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 22:36:06 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Zawoad", "Shams", ""], ["Dutta", "Amit Kumar", ""], ["Hasan", "Ragib", ""]]}, {"id": "1302.6312", "submitter": "Shams Zawoad", "authors": "Shams Zawoad, Ragib Hasan", "title": "Cloud Forensics: A Meta-Study of Challenges, Approaches, and Open\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, cloud computing has become popular as a cost-effective and\nefficient computing paradigm. Unfortunately, today's cloud computing\narchitectures are not designed for security and forensics. To date, very little\nresearch has been done to develop the theory and practice of cloud forensics.\nMany factors complicate forensic investigations in a cloud environment. First,\nthe storage system is no longer local. Therefore, even with a subpoena, law\nenforcement agents cannot confiscate the suspect's computer and get access to\nthe suspect's files. Second, each cloud server contains files from many users.\nHence, it is not feasible to seize servers from a data center without violating\nthe privacy of many other users. Third, even if the data belonging to a\nparticular suspect is identified, separating it from other users' data is\ndifficult. Moreover, other than the cloud provider's word, there is usually no\nevidence that links a given data file to a particular suspect. For such\nchallenges, clouds cannot be used to store healthcare, business, or national\nsecurity related data, which require audit and regulatory compliance. In this\npaper, we systematically examine the cloud forensics problem and explore the\nchallenges and issues in cloud forensics. We then discuss existing research\nprojects and finally, we highlight the open problems and future directions in\ncloud forensics research area. We posit that our systematic approach towards\nunderstanding the nature and challenges of cloud forensics will allow us to\nexamine possible secure solution approaches, leading to increased trust on and\nadoption of cloud computing, especially in business, healthcare, and national\nsecurity. This in turn will lead to lower cost and long-term benefit to our\nsociety as a whole.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 04:55:53 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Zawoad", "Shams", ""], ["Hasan", "Ragib", ""]]}, {"id": "1302.6329", "submitter": "EPTCS", "authors": "Peter Calvert (University of Cambridge Computer Laboratory), Alan\n  Mycroft (University of Cambridge Computer Laboratory)", "title": "Mapping the Join Calculus to Heterogeneous Hardware", "comments": "In Proceedings PLACES 2012, arXiv:1302.5798", "journal-ref": "EPTCS 109, 2013, pp. 7-12", "doi": "10.4204/EPTCS.109.2", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As modern architectures introduce additional heterogeneity and parallelism,\nwe look for ways to deal with this that do not involve specialising software to\nevery platform. In this paper, we take the Join Calculus, an elegant model for\nconcurrent computation, and show how it can be mapped to an architecture by a\nCartesian-product-style construction, thereby making use of the calculus'\ninherent non-determinism to encode placement choices. This unifies the concepts\nof placement and scheduling into a single task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 06:48:47 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Calvert", "Peter", "", "University of Cambridge Computer Laboratory"], ["Mycroft", "Alan", "", "University of Cambridge Computer Laboratory"]]}, {"id": "1302.6331", "submitter": "EPTCS", "authors": "Marco Carbone (IT University of Copenhagen), Fabrizio Montesi (IT\n  University of Copenhagen)", "title": "Merging Multiparty Protocols in Multiparty Choreographies", "comments": "In Proceedings PLACES 2012, arXiv:1302.5798", "journal-ref": "EPTCS 109, 2013, pp. 21-27", "doi": "10.4204/EPTCS.109.4", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreography-based programming is a powerful paradigm for defining\ncommunication-based systems from a global viewpoint. A choreography can be\nchecked against multiparty protocol specifications, given as behavioural types,\nthat may be instantiated indefinitely at runtime. Each protocol instance is\nstarted with a synchronisation among the involved peers.\n  We analyse a simple transformation from a choreography with a possibly\nunbounded number of protocol instantiations to a choreography instantiating a\nsingle protocol, which is the merge of the original ones. This gives an\neffective methodology for obtaining new protocols by composing existing ones.\nMoreover, by removing all synchronisations required for starting protocol\ninstances, our transformation reduces the number of communications and\nresources needed to execute a choreography.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2013 06:49:03 GMT"}], "update_date": "2013-02-27", "authors_parsed": [["Carbone", "Marco", "", "IT University of Copenhagen"], ["Montesi", "Fabrizio", "", "IT\n  University of Copenhagen"]]}, {"id": "1302.6911", "submitter": "Oskar Schirmer", "authors": "Oskar Schirmer", "title": "Using Virtual Addresses with Communication Channels", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While for single processor and SMP machines, memory is the allocatable\nquantity, for machines made up of large amounts of parallel computing units,\neach with its own local memory, the allocatable quantity is a single computing\nunit. Where virtual address management is used to keep memory coherent and\nallow allocation of more than physical memory is actually available, virtual\ncommunication channel references can be used to make computing units stay\nconnected across allocation and swapping.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2013 20:34:05 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["Schirmer", "Oskar", ""]]}, {"id": "1302.7193", "submitter": "Eike Hermann M\\\"uller", "authors": "Eike Mueller, Xu Guo, Robert Scheichl, Sinan Shi", "title": "Matrix-free GPU implementation of a preconditioned conjugate gradient\n  solver for anisotropic elliptic PDEs", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in geophysical and atmospheric modelling require the fast\nsolution of elliptic partial differential equations (PDEs) in \"flat\" three\ndimensional geometries. In particular, an anisotropic elliptic PDE for the\npressure correction has to be solved at every time step in the dynamical core\nof many numerical weather prediction models, and equations of a very similar\nstructure arise in global ocean models, subsurface flow simulations and gas and\noil reservoir modelling. The elliptic solve is often the bottleneck of the\nforecast, and an algorithmically optimal method has to be used and implemented\nefficiently. Graphics Processing Units have been shown to be highly efficient\nfor a wide range of applications in scientific computing, and recently\niterative solvers have been parallelised on these architectures. We describe\nthe GPU implementation and optimisation of a Preconditioned Conjugate Gradient\n(PCG) algorithm for the solution of a three dimensional anisotropic elliptic\nPDE for the pressure correction in NWP. Our implementation exploits the strong\nvertical anisotropy of the elliptic operator in the construction of a suitable\npreconditioner. As the algorithm is memory bound, performance can be improved\nsignificantly by reducing the amount of global memory access. We achieve this\nby using a matrix-free implementation which does not require explicit storage\nof the matrix and instead recalculates the local stencil. Global memory access\ncan also be reduced by rewriting the algorithm using loop fusion and we show\nthat this further reduces the runtime on the GPU. We demonstrate the\nperformance of our matrix-free GPU code by comparing it to a sequential CPU\nimplementation and to a matrix-explicit GPU code which uses existing libraries.\nThe absolute performance of the algorithm for different problem sizes is\nquantified in terms of floating point throughput and global memory bandwidth.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2013 13:56:18 GMT"}], "update_date": "2013-03-01", "authors_parsed": [["Mueller", "Eike", ""], ["Guo", "Xu", ""], ["Scheichl", "Robert", ""], ["Shi", "Sinan", ""]]}]